machine learning machine learning a probabilistic perspective kevin p. murphy today s web-enabled deluge of electronic data calls for automated methods of data analysis. machine learning provides these developing methods that can automatically detect patterns in data and use the uncovered patterns to predict future data. this textbook offers a comprehensive and self-contained introduction to the field of machine learning a unified probabilistic approach. the coverage combines breadth and depth offering necessary background material on such topics as probability optimization and linear algebra as well as discussion of recent developments in the field including conditional random fields regularization and deep learning. the book is written in an informal accessible style complete with pseudo-code for the most important algorithms. all topics are copiously illustrated with color images and worked examples drawn from such application domains as biology text processing computer vision and robotics. rather than providing a cookbook of different heuristic methods the book stresses a principled model-based approach often using the language of graphical models to specify models in a concise and intuitive way. almost all the models described have been implemented in a matlab software package pmtk modeling toolkit that is freely available online. the book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students. kevin p. murphy is a research scientist at google. previously he was associate professor of computer science and statistics at the university of british columbia. adaptive computation and machine learning series an astonishing machine learning book intuitive full of examples fun to read but still comprehensive strong and deep! a great starting point for any university student and a must-have for anybody in the field. jan peters darmstadt university of technology max-planck institute for intelligent systems kevin murphy excels at unraveling the complexities of machine learning methods while motivating the reader with a stream of illustrated examples and real-world case studies. the accompanying software package includes source code for many of the figures making it both easy and very tempting to dive in and explore these methods for yourself. a must-buy for anyone interested in machine learning or curious about how to extract useful knowledge from big data. john winn microsoft research this is a wonderful book that starts with basic topics in statistical modeling culminating in the most advanced topics. it provides both the theoretical foundations of probabilistic machine learning as well as practical tools in the form of matlab code. the book should be on the shelf of any student interested in the topic and any practitioner working in the field. yoram singer google research this book will be an essential reference for practitioners of modern machine learning. it covers the basic concepts needed to understand the field as a whole and the powerful modern methods that build on those concepts. in machine learning the language of probability and statistics reveals important connections between seemingly disparate algorithms and strategies. thus its readers will become articulate in a holistic view of the state-of-the-art and poised to build the next generation of machine learning algorithms. david blei princeton university the mit press massachusetts institute of technology cambridge massachusetts httpmitpress.mit.edu the cover image is based on sequential bayesian updating of a gaussian distribution. see figure for details. machine learning a probabilistic perspective kevin p. murphy machine learning a probabilistic perspective machine learning a probabilistic perspective kevin p. murphy the mit press cambridge massachusetts london england massachusetts institute of technology all rights reserved. no part of this book may be reproduced in any form by any electronic or mechanical means photocopying recording or information storage and retrieval without permission in writing from the publisher. for information about special quantity discounts please email special_salesmitpress.mit.edu this book was set in the latex programming language by the author. printed and bound in the united states of america. library of congress cataloging-in-publication information murphy kevin p. machine learning a probabilistic perspective kevin p. murphy. p. cm. computation and machine learning series includes bibliographical references and index. isbn alk. paper machine learning. probabilities. i. title. this book is dedicated to alessandro michael and stefano and to the memory of gerard joseph murphy. contents preface xxvii introduction types of machine learning classification regression discovering clusters discovering latent factors discovering graph structure matrix completion machine learning what and why? supervised learning unsupervised learning some basic concepts in machine learning parametric vs non-parametric models a simple non-parametric classifier k-nearest neighbors the curse of dimensionality parametric models for classification and regression linear regression logistic regression overfitting model selection no free lunch theorem probability introduction a brief review of probability theory discrete random variables fundamental rules bayes rule independence and conditional independence continuous random variables viii contents the binomial and bernoulli distributions the multinomial and multinoulli distributions the poisson distribution the empirical distribution quantiles mean and variance some common discrete distributions some common continuous distributions gaussian distribution degenerate pdf the laplace distribution the gamma distribution the beta distribution pareto distribution joint probability distributions transformations of random variables monte carlo approximation information theory covariance and correlation the multivariate gaussian multivariate student t distribution dirichlet distribution linear transformations general transformations central limit theorem entropy kl divergence mutual information example change of variables the mc way example estimating by monte carlo integration accuracy of monte carlo approximation generative models for discrete data likelihood prior posterior posterior predictive distribution a more complex prior introduction bayesian concept learning the beta-binomial model likelihood prior posterior posterior predictive distribution contents ix likelihood prior posterior posterior predictive the dirichlet-multinomial model naive bayes classifiers model fitting using the model for prediction the log-sum-exp trick feature selection using mutual information classifying documents using bag of words notation basics mle for an mvn maximum entropy derivation of the gaussian quadratic discriminant analysis linear discriminant analysis two-class lda mle for discriminant analysis strategies for preventing overfitting regularized lda diagonal lda nearest shrunken centroids classifier gaussian models introduction gaussian discriminant analysis inference in jointly gaussian distributions linear gaussian systems digression the wishart distribution inverse wishart distribution visualizing the wishart distribution inferring the parameters of an mvn posterior distribution of posterior distribution of posterior distribution of and sensor fusion with unknown precisions statement of the result examples information form proof of the result statement of the result examples proof of the result x contents map estimation credible intervals inference for a difference in proportions bayesian occam s razor computing the marginal likelihood bayes factors jeffreys-lindley paradox uninformative priors jeffreys priors robust priors mixtures of conjugate priors bayesian statistics introduction summarizing posterior distributions bayesian model selection priors hierarchical bayes empirical bayes bayesian decision theory bayes estimators for common loss functions the false positive vs false negative tradeoff other topics example beta-binomial model example gaussian-gaussian model example modeling related cancer rates frequentist statistics bootstrap large sample theory for the mle bayes risk minimax risk admissible estimators introduction sampling distribution of an estimator frequentist decision theory desirable properties of estimators empirical risk minimization consistent estimators unbiased estimators minimum variance estimators the bias-variance tradeoff regularized risk minimization structural risk minimization estimating the risk using cross validation upper bounding the risk using statistical learning theory contents xi surrogate loss functions pathologies of frequentist statistics counter-intuitive behavior of confidence intervals p-values considered harmful the likelihood principle why isn t everyone a bayesian? derivation of the mle geometric interpretation convexity linear regression introduction model specification maximum likelihood estimation squares robust linear regression ridge regression bayesian linear regression basic idea numerically stable computation connection with pca regularization effects of big data computing the posterior computing the posterior predictive bayesian inference when is unknown eb for linear regression procedure logistic regression introduction model specification model fitting mle steepest descent newton s method iteratively reweighted least squares quasi-newton metric methods regularization multi-class logistic regression bayesian logistic regression online learning and stochastic optimization laplace approximation derivation of the bic gaussian approximation for logistic regression approximating the posterior predictive residual analysis detection online learning and regret minimization xii stochastic optimization and risk minimization the lms algorithm the perceptron algorithm a bayesian view generative vs discriminative classifiers pros and cons of each approach dealing with missing data fisher s linear discriminant analysis contents generalized linear models and the exponential family definition examples log partition function mle for the exponential family bayes for the exponential family maximum entropy derivation of the exponential family introduction the exponential family generalized linear models probit regression basics ml and map estimation bayesian inference generalized linear mixed models learning to rank the pointwise approach the pairwise approach the listwise approach loss functions for ranking directed graphical models nets introduction chain rule conditional independence mlmap estimation using gradient-based optimization latent variable interpretation ordinal probit regression multinomial probit models multi-task learning hierarchical bayes for multi-task learning application to personalized email spam filtering application to domain adaptation other kinds of prior example semi-parametric glmms for medical data computational issues contents xiii graphical models graph terminology directed graphical models naive bayes classifiers markov and hidden markov models medical diagnosis genetic linkage analysis directed gaussian graphical models examples inference learning conditional independence properties of dgms plate notation learning from complete data learning with missing andor latent variables d-separation and the bayes ball algorithm markov properties other markov properties of dgms markov blanket and full conditionals influence diagrams mixture models and the em algorithm mixture models latent variable models unidentifiability computing a map estimate is non-convex mixtures of gaussians mixture of multinoullis using mixture models for clustering mixtures of experts parameter estimation for mixture models the em algorithm basic idea em for gmms em for mixture of experts em for dgms with hidden variables em for the student distribution em for probit regression theoretical basis for em online em other em variants model selection for latent variable models model selection for probabilistic models model selection for non-probabilistic methods fitting models with missing data xiv contents em for the mle of an mvn with missing data fa is a low rank parameterization of an mvn inference of the latent factors unidentifiability mixtures of factor analysers em for factor analysis models fitting fa models with missing data classical pca statement of the theorem proof singular value decomposition probabilistic pca em algorithm for pca latent linear models factor analysis principal components analysis choosing the number of latent dimensions pca for categorical data pca for paired and multi-view data supervised pca factor regression partial least squares canonical correlation analysis independent component analysis maximum likelihood estimation the fastica algorithm using em other estimation principles model selection for fappca model selection for pca sparse linear models introduction bayesian variable selection regularization basics regularization algorithms coordinate descent the spike and slab model from the bernoulli-gaussian model to regularization algorithms why does regularization yield sparse solutions? optimality conditions for lasso comparison of least squares lasso ridge and subset selection regularization path model selection bayesian inference for linear models with laplace priors contents xv lars and other homotopy methods proximal and gradient projection methods em for lasso regularization extensions group lasso fused lasso elastic net and lasso combined non-convex regularizers bridge regression hierarchical adaptive lasso other hierarchical priors automatic relevance determination bayesian learning sparse coding learning a sparse coding dictionary results of dictionary learning from image patches compressed sensing image inpainting and denoising ard for linear regression whence sparsity? connection to map estimation algorithms for ard ard for logistic regression kernels rbf kernels kernels for comparing documents mercer definite kernels linear kernels matern kernels string kernels pyramid match kernels kernels derived from probabilistic generative models introduction kernel functions using kernels inside glms kernel machines rvms and other sparse vector machines the kernel trick support vector machines kernelized nearest neighbor classification kernelized k-medoids clustering kernelized ridge regression kernel pca svms for regression svms for classification xvi choosing c summary of key points a probabilistic interpretation of svms comparison of discriminative kernel methods kernels for building generative models smoothing kernels kernel density estimation from kde to knn kernel regression locally weighted regression predictions using noise-free observations predictions using noisy observations effect of the kernel parameters estimating the kernel parameters computational and numerical issues semi-parametric gps gaussian processes introduction gps for regression gps meet glms connection with other methods gp latent variable model approximation methods for large datasets linear models compared to gps linear smoothers compared to gps svms compared to gps and rvms compared to gps neural networks compared to gps smoothing splines compared to gps rkhs methods compared to gps binary classification multi-class classification gps for poisson regression contents adaptive basis function models introduction classification and regression trees generalized additive models basics growing a tree pruning a tree pros and cons of trees random forests cart compared to hierarchical mixture of experts contents xvii a bayesian view backfitting computational efficiency multivariate adaptive regression splines forward stagewise additive modeling adaboost logitboost boosting as functional gradient descent sparse boosting multivariate adaptive regression trees boosting why does boosting work so well? feedforward neural networks perceptrons ensemble learning stacking error-correcting output codes ensemble learning is not equivalent to bayes model averaging experimental comparison interpreting black-box models convolutional neural networks other kinds of neural networks a brief history of the field the backpropagation algorithm identifiability regularization bayesian inference low-dimensional features high-dimensional features markov and hidden markov models introduction markov models transition matrix application language modeling stationary distribution of a markov chain application google s pagerank algorithm for web page ranking hidden markov models applications of hmms inference in hmms types of inference problems for temporal models the forwards algorithm the forwards-backwards algorithm the viterbi algorithm forwards filtering backwards sampling xviii learning for hmms training with fully observed data em for hmms baum-welch algorithm bayesian methods for fitting hmms discriminative training model selection generalizations of hmms variable duration hmms hierarchical hmms input-output hmms auto-regressive and buried hmms factorial hmm coupled hmm and the influence model dynamic bayesian networks contents ssms for object tracking robotic slam online parameter learning using recursive least squares ssm for time series forecasting state space models introduction applications of ssms inference in lg-ssm learning for lg-ssm approximate online inference for non-linear non-gaussian ssms identifiability and numerical stability training with fully observed data em for lg-ssm subspace methods bayesian methods for fitting lg-ssms the kalman filtering algorithm the kalman smoothing algorithm extended kalman filter unscented kalman filter assumed density filtering hybrid discretecontinuous ssms inference application data association and multi-target tracking application fault diagnosis application econometric forecasting undirected graphical models random fields introduction conditional independence properties of ugms key properties contents xix an undirected alternative to d-separation comparing directed and undirected graphical models the hammersley-clifford theorem representing potential functions ising model hopfield networks potts model gaussian mrfs markov logic networks parameterization of mrfs examples of mrfs learning conditional random fields structural svms training maxent models using gradient methods training partially observed maxent models approximate methods for computing the mles of mrfs pseudo likelihood stochastic maximum likelihood feature induction for maxent models iterative proportional fitting chain-structured crfs memms and the label-bias problem applications of crfs crf training ssvms a probabilistic view ssvms a non-probabilistic view cutting plane methods for fitting ssvms online algorithms for fitting ssvms latent structural svms exact inference for graphical models serial protocol parallel protocol gaussian bp other bp variants introduction belief propagation for trees the variable elimination algorithm the junction tree algorithm message passing on a junction tree the generalized distributive law computational complexity of ve a weakness of ve computational complexity of jta creating a junction tree xx contents jta generalizations computational intractability of exact inference in the worst case approximate inference variational inference alternative interpretations of the variational objective forward or reverse kl? derivation of the mean field update equations example mean field for the ising model example factorial hmm introduction variational inference the mean field method structured mean field variational bayes variational bayes em variational message passing and vibes local variational bounds example vb for a univariate gaussian example vb for linear regression example vbem for mixtures of gaussians motivating applications bohning s quadratic bound to the log-sum-exp function bounds for the sigmoid function other bounds and approximations to the log-sum-exp function variational inference based on upper bounds more variational inference a brief history lbp on pairwise models lbp on a factor graph convergence accuracy of lbp other speedup tricks for lbp introduction loopy belief propagation algorithmic issues loopy belief propagation theoretical issues ugms represented in exponential family form the marginal polytope exact inference as a variational optimization problem mean field as a variational optimization problem lbp as a variational optimization problem loopy bp vs mean field extensions of belief propagation generalized belief propagation contents xxi convex belief propagation expectation propagation ep as a variational inference problem optimizing the ep objective using moment matching ep for the clutter problem lbp is a special case of ep ranking players using trueskill other applications of ep map state estimation linear programming relaxation max-product belief propagation graphcuts experimental comparison of graphcuts and bp dual decomposition monte carlo inference using the cdf sampling from a gaussian method introduction sampling from standard distributions rejection sampling basic idea example application to bayesian statistics adaptive rejection sampling rejection sampling in high dimensions importance sampling particle filtering rao-blackwellised particle filtering sequential importance sampling the degeneracy problem the resampling step the proposal distribution application robot localization application visual object tracking application time series forecasting rbpf for switching lg-ssms application tracking a maneuvering target application fast slam basic idea handling unnormalized distributions importance sampling for a dgm likelihood weighting sampling importance resampling markov chain monte carlo inference xxii introduction gibbs sampling basic idea example gibbs sampling for the ising model example gibbs sampling for inferring the parameters of a gmm collapsed gibbs sampling gibbs sampling for hierarchical glms bugs and jags the imputation posterior algorithm blocking gibbs sampling metropolis hastings algorithm contents basic idea gibbs sampling is a special case of mh proposal distributions adaptive mcmc initialization and mode hopping reversible jump mcmc why mh works speed and accuracy of mcmc auxiliary variable mcmc the burn-in phase mixing rates of markov chains practical convergence diagnostics accuracy of mcmc how many chains? auxiliary variable sampling for logistic regression slice sampling swendsen wang hybridhamiltonian mcmc annealing methods simulated annealing annealed importance sampling parallel tempering approximating the marginal likelihood the candidate method harmonic mean estimate annealed importance sampling clustering introduction measuring evaluating the output of clustering methods dirichlet process mixture models from finite to infinite mixture models the dirichlet process contents xxiii applying dirichlet processes to mixture modeling fitting a dp mixture model affinity propagation spectral clustering graph laplacian normalized graph laplacian example hierarchical clustering agglomerative clustering divisive clustering choosing the number of clusters bayesian hierarchical clustering clustering datapoints and features multi-view clustering biclustering graphical model structure learning directed or undirected tree? chow-liu algorithm for finding the ml tree structure finding the map forest approximating the marginal likelihood when we have missing data structural em discovering hidden variables case study google s rephil structural equation models relevance networks dependency networks markov equivalence exact structural inference scaling up to larger graphs introduction structure learning for knowledge discovery learning tree structures mixtures of trees learning dag structures learning dag structure with latent variables learning causal dags learning undirected gaussian graphical models causal interpretation of dags using causal dags to resolve simpson s paradox learning causal dag structures mle for a ggm graphical lasso bayesian inference for ggm structure handling non-gaussian data using copulas xxiv learning undirected discrete graphical models graphical lasso for mrfscrfs thin junction trees latent variable models for discrete data introduction distributed state lvms for discrete data contents mixture models exponential family pca lda and mpca gap model and non-negative matrix factorization basics unsupervised discovery of topics quantitatively evaluating lda as a language model fitting using gibbs sampling example fitting using batch variational inference fitting using online variational inference determining the number of topics latent dirichlet allocation extensions of lda lvms for graph-structured data stochastic block model mixed membership stochastic block model relational topic model lvms for relational data infinite relational model probabilistic matrix factorization for collaborative filtering restricted boltzmann machines correlated topic model dynamic topic model lda-hmm supervised lda varieties of rbms learning rbms applications of rbms deep learning introduction deep generative models deep directed networks deep boltzmann machines deep belief networks greedy layer-wise learning of dbns deep neural networks contents deep multi-layer perceptrons deep auto-encoders stacked denoising auto-encoders applications of deep networks handwritten digit classification using dbns data visualization and feature discovery using deep auto-encoders information retrieval using deep auto-encoders hashing learning audio features using convolutional dbns learning image features using convolutional dbns xxv discussion notation bibliography indexes index to code index to keywords preface introduction with the ever increasing amounts of data in electronic form the need for automated methods for data analysis continues to grow. the goal of machine learning is to develop methods that can automatically detect patterns in data and then to use the uncovered patterns to predict future data or other outcomes of interest. machine learning is thus closely related to the fields of statistics and data mining but differs slightly in terms of its emphasis and terminology. this book provides a detailed introduction to the field and includes worked examples drawn from application domains such as molecular biology text processing computer vision and robotics. target audience this book is suitable for upper-level undergraduate students and beginning graduate students in computer science statistics electrical engineering econometrics or any one else who has the appropriate mathematical background. specifically the reader is assumed to already be familiar with basic multivariate calculus probability linear algebra and computer programming. prior exposure to statistics is helpful but not necessary. a probabilistic approach this books adopts the view that the best way to make machines that can learn from data is to use the tools of probability theory which has been the mainstay of statistics and engineering for centuries. probability theory can be applied to any problem involving uncertainty. in machine learning uncertainty comes in many forms what is the best prediction decision given some data? what is the best model given some data? what measurement should i perform next? etc. including inferring parameters of statistical models is sometimes called a bayesian approach. however this term tends to elicit very strong reactions positive or negative depending on who you ask so we prefer the more neutral term probabilistic approach besides we will often use techniques such as maximum likelihood estimation which are not bayesian methods but certainly fall within the probabilistic paradigm. the systematic application of probabilistic reasoning to all inferential problems rather than describing a cookbook of different heuristic methods this book stresses a principled model-based approach to machine learning. for any given model a variety of algorithms xxviii preface can often be applied. conversely any given algorithm can often be applied to a variety of models. this kind of modularity where we distinguish model from algorithm is good pedagogy and good engineering. we will often use the language of graphical models to specify our models in a concise and intuitive way. in addition to aiding comprehension the graph structure aids in developing efficient algorithms as we will see. however this book is not primarily about graphical models it is about probabilistic modeling in general. a practical approach nearly all of the methods described in this book have been implemented in a matlab software package called pmtk which stands for probabilistic modeling toolkit. this is freely available from digit refers to the third edition of the toolkit which is the one used in this version of the book. there are also a variety of supporting files written by other people available at pmtksupport.googlecode.com. these will be downloaded automatically if you follow the setup instructions described on the pmtk website. matlab is a high-level interactive scripting language ideally suited to numerical computation and data visualization and can be purchased from www.mathworks.com. some of the code requires the statistics toolbox which needs to be purchased separately. there is also a free version of matlab called octave available at httpwww.gnu.orgsoftwareoctave which supports most of the functionality of matlab. some not all of the code in this book also works in octave. see the pmtk website for details. pmtk was used to generate many of the figures in this book the source code for these figures is included on the pmtk website allowing the reader to easily see the effects of changing the data or algorithm or parameter settings. the book refers to files by name e.g. naivebayesfit. in order to find the corresponding file you can use two methods within matlab you can type which naivebayesfit and it will return the full path to the file or if you do not have matlab but want to read the source code anyway you can use your favorite search engine which should return the corresponding file from the website. details on how to use pmtk can be found on the website which will be udpated over time. details on the underlying theory behind these methods can be found in this book. acknowledgments a book this large is obviously a team effort. i would especially like to thank the following people my wife margaret for keeping the home fires burning as i toiled away in my office for the last six years matt dunham who created many of the figures in this book and who wrote much of the code in pmtk baback moghaddam who gave extremely detailed feedback on every page of an earlier draft of the book chris williams who also gave very detailed feedback cody severinski and wei-lwun lu who assisted with figures generations of ubc students who gave helpful comments on earlier drafts daphne koller nir friedman and chris manning for letting me use their latex style files stanford university google research and skyline college for hosting me during part of my sabbatical and various canadian funding agencies crc and cifar who have supported me financially over the years. in addition i would like to thank the following people for giving me helpful feedback on preface xxix parts of the book andor for sharing figures code exercises or even some cases text david blei hannes bretschneider greg corrado arnaud doucet mario figueiredo nando de freitas mark girolami gabriel goh tom griffiths katherine heller geoff hinton aapo hyvarinen tommi jaakkola mike jordan charles kemp emtiyaz khan bonnie kirkpatrick daphne koller zico kolter honglak lee julien mairal andrew mcpherson tom minka ian nabney arthur pope carl rassmussen ryan rifkin ruslan salakhutdinov mark schmidt daniel selsam david sontag erik sudderth josh tenenbaum kai yu martin wainwright yair weiss. kevin patrick murphy palo alto california june introduction machine learning what and why? we are drowning in information and starving for knowledge. john naisbitt. we are entering the era of big data. for example there are about trillion web one hour of video is uploaded to youtube every second amounting to years of content every the genomes of of people each of which has a length of base pairs have been sequenced by various labs walmart handles more than transactions per hour and has databases containing more than petabytes of information and so on. this deluge of data calls for automated methods of data analysis which is what machine learning provides. in particular we define machine learning as a set of methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data or to perform other kinds of decision making under uncertainty as planning how to collect more data!. this books adopts the view that the best way to solve such problems is to use the tools of probability theory. probability theory can be applied to any problem involving uncertainty. in machine learning uncertainty comes in many forms what is the best prediction about the future given some past data? what is the best model to explain some data? what measurement should i perform next? etc. the probabilistic approach to machine learning is closely related to the field of statistics but differs slightly in terms of its emphasis and we will describe a wide variety of probabilistic models suitable for a wide variety of data and tasks. we will also describe a wide variety of algorithms for learning and using such models. the goal is not to develop a cook book of ad hoc techiques but instead to present a unified view of the field through the lens of probabilistic modeling and inference. although we will pay attention to computational efficiency details on how to scale these methods to truly massive datasets are better described in other books such as and ullman bekkerman et al. source httpwww.youtube.comtpress_statistics. rob tibshirani a statistician at stanford university has created an amusing comparison between machine learning and statistics available at chapter introduction it should be noted however that even when one has an apparently massive data set the effective number of data points for certain cases of interest might be quite small. in fact data across a variety of domains exhibits a property known as the long tail which means that a few things words are very common but most things are quite rare section for details. for example of google searches each day have never been seen this means that the core statistical issues that we discuss in this book concerning generalizing from relatively small samples sizes are still very relevant even in the big data era. types of machine learning in the predictive or supervised machine learning is usually divided into two main types. learning approach the goal is to learn a mapping from inputs x to outputs y given a labeled set of input-output pairs d yin here d is called the training set and n is the number of training examples. in the simplest setting each training input xi is a d-dimensional vector of numbers representing say the height and weight of a person. these are called features attributes or covariates. in general however xi could be a complex structured object such as an image a sentence an email message a time series a molecular shape a graph etc. similarly the form of the output or response variable can in principle be anything but most methods assume that yi is a categorical or nominal variable from some finite set yi c as male or female or that yi is a real-valued scalar as income level. when yi is categorical the problem is known as classification or pattern recognition and when yi is real-valued the problem is known as regression. another variant known as ordinal regression occurs where label space y has some natural ordering such as grades a f. the second main type of machine learning is the descriptive or unsupervised learning approach. here we are only given inputs d and the goal is to find interesting patterns in the data. this is sometimes called knowledge discovery. this is a much less well-defined problem since we are not told what kinds of patterns to look for and there is no obvious error metric to use supervised learning where we can compare our prediction of y for a given x to the observed value. there is a third type of machine learning known as reinforcement learning which is somewhat less commonly used. this is useful for learning how to act or behave when given occasional reward or punishment signals. example consider how a baby learns to walk. unfortunately rl is beyond the scope of this book although we do discuss decision theory in section which is the basis of rl. see e.g. et al. sutton and barto russell and norvig szepesvari wiering and van otterlo for more information on rl. httpcertifiedknowledge.orgblogare-search-queries-becoming-even-more-unique-statistic s-from-google. supervised learning figure left some labeled training examples of colored shapes along with unlabeled test cases. right representing the training data as an n d design matrix. row i represents the feature vector xi. the last column is the label yi based on a figure by leslie kaelbling. supervised learning we begin our investigation of machine learning by discussing supervised learning which is the form of ml most widely used in practice. classification in this section we discuss classification. here the goal is to learn a mapping from inputs x to outputs y where y c with c being the number of classes. if c this is called binary classification which case we often assume y if c this is called multiclass classification. if the class labels are not mutually exclusive somebody may be classified as tall and strong we call it multi-label classification but this is best viewed as predicting multiple related binary class labels so-called multiple output model. when we use the term classification we will mean multiclass classification with a single output unless we state otherwise. one way to formalize the problem is as function approximation. we assume y f for some unknown function f and the goal of learning is to estimate the function f given a labeled training set and then to make predictions using y f use the hat symbol to denote an estimate. our main goal is to make predictions on novel inputs meaning ones that we have not seen before is called generalization since predicting the response on the training set is easy can just look up the answer. example as a simple toy example of classification consider the problem illustrated in figure we have two classes of object which correspond to labels and the inputs are colored shapes. these have been described by a set of d features or attributes which are stored in an n d design matrix x shown in figure the input features x can be discrete continuous or a combination of the two. in addition to the inputs we have a vector of training labels y. in figure the test cases are a blue crescent a yellow circle and a blue arrow. none of these have been seen before. thus we are required to generalize beyond the training set. a chapter introduction reasonable guess is that blue crescent should be y since all blue shapes are labeled in the training set. the yellow circle is harder to classify since some yellow things are labeled y and some are labeled y and some circles are labeled y and some y consequently it is not clear what the right label should be in the case of the yellow circle. similarly the correct label for the blue arrow is unclear. the need for probabilistic predictions to handle ambiguous cases such as the yellow circle above it is desirable to return a probability. the reader is assumed to already have some familiarity with basic concepts in probability. if not please consult chapter for a refresher if necessary. we will denote the probability distribution over possible labels given the input vector x and training set d by pyxd. in general this represents a vector of length c. there are just two classes it is sufficient to return the single number py since py py in our notation we make explicit that the probability is conditional on the test input x as well as the training set d by putting these terms on the right hand side of the conditioning bar we are also implicitly conditioning on the form of model that we use to make predictions. when choosing between different models we will make this assumption explicit by writing pyxd m where m denotes the model. however if the model is clear from context we will drop m from our notation for brevity. given a probabilistic output we can always compute our best guess as to the true label using y f c argmax py cxd this corresponds to the most probable class label and is called the mode of the distribution pyxd it is also known as a map estimate stands for maximum a posteriori. using the most probable label makes intuitive sense but we will give a more formal justification for this procedure in section now consider a case such as the yellow circle where p yxd is far from in such a case we are not very confident of our answer so it might be better to say i don t know instead of returning an answer that we don t really trust. this is particularly important in domains such as medicine and finance where we may be risk averse as we explain in section another application where it is important to assess risk is when playing tv game shows such as jeopardy. in this game contestants have to solve various word puzzles and answer a variety of trivia questions but if they answer incorrectly they lose money. in ibm unveiled a computer system called watson which beat the top human jeopardy champion. watson uses a variety of interesting techniques et al. but the most pertinent one for our present purposes is that it contains a module that estimates how confident it is of its answer. the system only chooses to buzz in its answer if sufficiently confident it is correct. similarly google has a system known as smartass selection system that predicts the probability you will click on an ad based on your search history and other user and ad-specific features this probability is known as the click-through rate or ctr and can be used to maximize expected profit. we will discuss some of the basic principles behind systems such as smartass later in this book. supervised learning s t n e m u c o d words figure subset of size x of the data. we only show rows for clarity. each row is a document as a bag-of-words bit vector each column is a word. the red lines separate the classes which are descending order comp rec sci talk are the titles of usenet groups. we can see that there are subsets of words whose presence or absence is indicative of the class. the data is available from httpcs.nyu.eduroweisdata.html. figure generated by newsgroupsvisualize. real-world applications classification is probably the most widely used form of machine learning and has been used to solve many interesting and often difficult real-world problems. we have already mentioned some important applciations. we give a few more examples below. document classification and email spam filtering in document classification the goal is to classify a document such as a web page or email message into one of c classes that is to compute py cxd where x is some representation of the text. a special case of this is email spam filtering where the classes are spam y or ham y most classifiers assume that the input vector x has a fixed size. a common way to represent variable-length documents in feature-vector format is to use a bag of words representation. this is explained in detail in section but the basic idea is to define xij iff word j occurs in document i. if we apply this transformation to every document in our data set we get a binary document word co-occurrence matrix see figure for an example. essentially the document classification problem has been reduced to one that looks for subtle changes in the pattern of bits. for example we may notice that most spam messages have a high probability of containing the words buy cheap viagra etc. in exercise and exercise you will get hands-on experience applying various classification techniques to the spam filtering problem. chapter introduction figure three types of iris flowers setosa versicolor and virginica. source httpwww.statlab.u ni-heidelberg.dedatairis used with kind permission of dennis kramb and signa. sepal length sepal width petal length petal width h t g n e l l a p e s h t i d w l a p e s h t g n e l l a t e p t h d w i l a t e p figure visualization of the iris data as a pairwise scatter plot. the diagonal plots the marginal histograms of the features. the off diagonals contain scatterplots of all possible pairs of features. red circle setosa green diamond versicolor blue star virginica. figure generated by fisheririsdemo. classifying flowers figure gives another example of classification due to the statistician ronald fisher. the goal is to learn to distinguish three different kinds of iris flower called setosa versicolor and virginica. fortunately rather than working directly with images a botanist has already extracted useful feature features or characteristics sepal length and width and petal length and width. extraction is an important but difficult task. most machine learning methods use features chosen by some human. later we will discuss some methods that can learn good features from the data. if we make a scatter plot of the iris data as in figure we see that it is easy to distinguish setosas circles from the other two classes by just checking if their petal length supervised learning true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class true class first test mnist gray-scale images. figure same as but with the features permuted randomly. classification performance is identical on both versions of the data the training data is permuted in an identical way. figure generated by shuffleddigitsdemo. or width is below some threshold. however distinguishing versicolor from virginica is slightly harder any decision will need to be based on at least two features. is always a good idea to perform exploratory data analysis such as plotting the data before applying a machine learning method. image classification and handwriting recognition now consider the harder problem of classifying images directly where a human has not preprocessed the data. we might want to classify the image as a whole e.g. is it an indoors or outdoors scene? is it a horizontal or vertical photo? does it contain a dog or not? this is called image classification. in the special case that the images consist of isolated handwritten letters and digits for example in a postal or zip code on a letter we can use classification to perform handwriting recognition. a standard dataset used in this area is known as mnist which stands for modified national institute of standards term modified is used because the images have been preprocessed to ensure the digits are mostly in the center of the image. this dataset contains training images and test images of the digits to as written by various people. the images are size and have grayscale values in the range see figure for some example images. many generic classification methods ignore any structure in the input features such as spatial layout. consequently they can also just as easily handle data that looks like figure which is the same data except we have randomly permuted the order of all the features. will verify this in exercise this flexibility is both a blessing the methods are general purpose and a curse the methods ignore an obviously useful source of information. we will discuss methods for exploiting structure in the input features later in the book. available from httpyann.lecun.comexdbmnist. chapter introduction figure example of face detection. input image family photo taken august used with kind permission of bernard diedrich of sherwood studios. output of classifier which detected faces at different poses. this was produced using the online demo at httpdemo.pittpatt.com. the classifier was trained on of manually labeled images of faces and non-faces and then was applied to a dense set of overlapping patches in the test image. only the patches whose probability of containing a face was sufficiently high were returned. used with kind permission of pittpatt.com face detection and recognition a harder problem is to find objects within an image this is called object detection or object localization. an important special case of this is face detection. one approach to this problem is to divide the image into many small overlapping patches at different locations scales and orientations and to classify each such patch based on whether it contains face-like texture or not. this is called a sliding window detector. the system then returns those locations where the probability of face is sufficiently high. see figure for an example. such face detection systems are built-in to most modern digital cameras the locations of the detected faces are used to determine the center of the auto-focus. another application is automatically blurring out faces in google s streetview system. having found the faces one can then proceed to perform face recognition which means estimating the identity of the person figure in this case the number of class labels might be very large. also the features one should use are likely to be different than in the face detection problem for recognition subtle differences between faces such as hairstyle may be important for determining identity but for detection it is important to be invariant to such details and to just focus on the differences between faces and non-faces. for more information about visual object detection see e.g. regression regression is just like classification except the response variable is continuous. figure shows a simple example we have a single real-valued input xi r and a single real-valued response yi r. we consider fitting two models to the data a straight line and a quadratic function. explain how to fit such models below. various extensions of this basic problem can arise such as having high-dimensional inputs outliers non-smooth responses etc. we will discuss ways to handle such problems later in the book. unsupervised learning degree degree figure figure generated by linregpolyvsdegree. linear regression on some data. same data with polynomial regression here are some examples of real-world regression problems. predict tomorrow s stock market price given current market conditions and other possible side information. predict the age of a viewer watching a given video on youtube. predict the location in space of a robot arm end effector given control signals sent to its various motors. predict the amount of prostate specific antigen in the body as a function of a number of different clinical measurements. predict the temperature at any location inside a building using weather data time door sensors etc. unsupervised learning we now consider unsupervised learning where we are just given output data without any inputs. the goal is to discover interesting structure in the data this is sometimes called knowledge discovery. unlike supervised learning we are not told what the desired output is instead we will formalize our task as one of density estimation that is we for each input. want to build models of the form pxi there are two differences from the supervised case. first we have written pxi instead of pyixi that is supervised learning is conditional density estimation whereas unsupervised learning is unconditional density estimation. second xi is a vector of features so we need to create multivariate probability models. by contrast in supervised learning yi is usually just a single variable that we are trying to predict. this means that for most supervised learning problems we can use univariate probability models input-dependent parameters which significantly simplifies the problem. will discuss multi-output classification in chapter where we will see that it also involves multivariate probability models. unsupervised learning is arguably more typical of human and animal learning. it is also more widely applicable than supervised learning since it does not require a human expert to chapter introduction t i h g e w height t i h g e w height figure the height and weight of some people. figure generated by kmeansheightweight. a possible clustering using k clusters. manually label the data. labeled data is not only expensive to but it also contains relatively little information certainly not enough to reliably estimate the parameters of complex models. geoff hinton who is a famous professor of ml at the university of toronto has said when we re learning to see nobody s telling us what the right answers are we just look. every so often your mother says that s a dog but that s very little information. you d be lucky if you got a few bits of information even one bit per second that way. the brain s visual system has neural connections. and you only live for seconds. so it s no use learning one bit per second. you need more like bits per second. and there s only one place you can get that much information from the input itself. geoffrey hinton in below we describe some canonical examples of unsupervised learning. discovering clusters as a canonical example of unsupervised learning consider the problem of clustering data into groups. for example figure plots some data representing the height and weight of a group of people. it seems that there might be various clusters or subgroups although it is not clear how many. let k denote the number of clusters. our first goal is to estimate the distribution over the number of clusters pkd this tells us if there are subpopulations within the data. for simplicity we often approximate the distribution pkd by its mode arg maxk pkd. in the supervised case we were told that there are two classes k and female but in the unsupervised case we are free to choose as many or few clusters as we like. picking a model of the right complexity is called model selection and will be discussed in detail below. our second goal is to estimate which cluster each point belongs to. let zi k is an example of a hidden or represent the cluster to which data point i is assigned. the advent of crowd sourcing web sites such as mechanical turk which outsource data processing tasks to humans all over the world has reduced the cost of labeling data. nevertheless the amount of unlabeled data is still orders of magnitude larger than the amount of labeled data. unsupervised learning figure a set of points that live on a linear subspace embedded in the solid red line is the first principal component direction. the dotted black line is the second pc direction. representation of the data. figure generated by latent variable since it is never observed in the training set. we can infer which cluster each i argmaxk pzi kxid. this is illustrated in data point belongs to by computing z figure where we use different colors to indicate the assignments assuming k in this book we focus on model based clustering which means we fit a probabilistic model to the data rather than running some ad hoc algorithm. the advantages of the model-based approach are that one can compare different kinds of models in an objective way terms of the likelihood they assign to the data we can combine them together into larger systems etc. here are some real world applications of clustering. in astronomy the autoclass system et al. discovered a new type of star based on clustering astrophysical measurements. in e-commerce it is common to cluster users into groups based on their purchasing or web-surfing behavior and then to send customized targeted advertising to each group e.g. in biology it is common to cluster flow-cytometry data into groups to discover different sub-populations of cells e.g. et al. discovering latent factors when dealing with high dimensional data it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the essence of the data. this is called dimensionality reduction. a simple example is shown in figure where we project some data down to a plane. the approximation is quite good since most points lie close to this subspace. reducing to would involve projecting points onto the red line in figure this would be a rather poor approximation. will make this notion precise in chapter the motivation behind this technique is that although the data may appear high dimensional there may only be a small number of degrees of variability corresponding to latent factors. for example when modeling the appearance of face images there may only be a few underlying latent factors which describe most of the variability such as lighting pose identity etc as illustrated in figure chapter introduction figure a randomly chosen pixel images from the olivetti face database. and the first three principal component basis vectors figure generated by pcaimagedemo. the mean when used as input to other statistical models such low dimensional representations often result in better predictive accuracy because they focus on the essence of the object filtering out inessential features. also low dimensional representations are useful for enabling fast nearest neighbor searches and two dimensional projections are very useful for visualizing high dimensional data. the most common approach to dimensionality reduction is called principal components analysis or pca. this can be thought of as an unsupervised version of linear regression where we observe the high-dimensional response y but not the low-dimensional cause z. thus the model has the form z y we have to invert the arrow and infer the latent low-dimensional z from the observed high-dimensional y. see section for details. dimensionality reduction and pca in particular has been applied in many different areas. some examples include the following in biology it is common to use pca to interpret gene microarray data to account for the fact that each measurement is usually the result of many genes which are correlated in their behavior by the fact that they belong to different biological pathways. in natural language processing it is common to use a variant of pca called latent semantic analysis for document retrieval section in signal processing of acoustic or neural signals it is common to use ica is a variant of pca to separate signals into their different sources section in computer graphics it is common to project motion capture data to a low dimensional space and use it to create animations. see section for one way to tackle such problems. unsupervised learning figure a sparse undirected gaussian graphical model learned using graphical lasso applied to some flow cytometry data et al. which measures the phosphorylation status of proteins. figure generated by ggmlassodemo. discovering graph structure sometimes we measure a set of correlated variables and we would like to discover which ones are most correlated with which others. this can be represented by a graph g in which nodes represent variables and edges represent direct dependence between variables will make this precise in chapter when we discuss graphical models. we can then learn this graph structure from data i.e. we compute g argmax pgd. as with unsupervised learning in general there are two main applications for learning sparse graphs to discover new knowledge and to get better joint probability density estimators. we now give somes example of each. much of the motivation for learning sparse graphical models comes from the systems biology community. for example suppose we measure the phosphorylation status of some proteins in a cell et al. figure gives an example of a graph structure that was learned from this data methods discussed in section as another example smith et al. showed that one can recover the neural wiring diagram of a certain kind of bird from time-series eeg data. the recovered structure closely matched the known functional connectivity of this part of the bird brain. in some cases we are not interested in interpreting the graph structure we just want to use it to model correlations and to make predictions. one example of this is in financial portfolio management where accurate models of the covariance between large numbers of different stocks is important. carvalho and west show that by learning a sparse graph and then using this as the basis of a trading strategy it is possible to outperform make more money than methods that do not exploit sparse graphs. another example is predicting traffic jams on the freeway. horvitz et al. describe a deployed system called jambayes for predicting traffic flow in the seattle area predictions are made using a graphical model whose structure was learned from data. chapter introduction figure a noisy image with an occluder. an estimate of the underlying pixel intensities based on a pairwise mrf model. source figure of and huttenlocher used with kind permission of pedro felzenszwalb. matrix completion sometimes we have missing data that is variables whose values are unknown. for example we might have conducted a survey and some people might not have answered certain questions. or we might have various sensors some of which fail. the corresponding design matrix will then have holes in it these missing entries are often represented by nan which stands for not a number the goal of imputation is to infer plausible values for the missing entries. this is sometimes called matrix completion. below we give some example applications. image inpainting an interesting example of an imputation-like task is known as image inpainting. the goal is to fill in holes due to scratches or occlusions in an image with realistic texture. this is illustrated in figure where we denoise the image as well as impute the pixels hidden behind the occlusion. this can be tackled by building a joint probability model of the pixels given a set of clean images and then inferring the unknown variables given the known variables this is somewhat like masket basket analysis except the data is real-valued and spatially structured so the kinds of probability models we use are quite different. see sections and for some possible choices. collaborative filtering another interesting example of an imputation-like task is known as collaborative filtering. a common example of this concerns predicting which movies people will want to watch based on how they and other people have rated movies which they have already seen. the key idea is that the prediction is not based on features of the movie or user it could be but merely on a ratings matrix. more precisely we have a matrix x where xm u is the rating unsupervised learning figure example of movie-rating data. training data is in red test data is denoted by empty cells are unknown. an integer between and where is dislike and is like by user u of movie m. note that most of the entries in x will be missing or unknown since most users will not have rated most movies. hence we only observe a tiny subset of the x matrix and we want to predict in particular for any given user u we might want to predict which of the a different subset. unrated movies heshe is most likely to want to watch. launched in with a usd prize httpnetflixprize.com. in order to encourage research in this area the dvd rental company netflix created a competition in particular they provided a large matrix of ratings on a scale of to for movies created by users. the full matrix would have entries but only about of the entries are observed so the matrix is extremely sparse. a subset of these are used for training and the rest for testing as shown in figure the goal of the competition was to predict more accurately than netflix s existing system. on september the prize was awarded to a team of researchers known as bellkor s pragmatic chaos section discusses some of their methodology. further details on the teams and their methods can be found at market basket analysis in commercial data mining there is much interest in a task called market basket analysis. the data consists of a very large but sparse binary matrix where each column represents an item or product and each row represents a transaction. we set xij if item j was purchased on the i th transaction. many items are purchased together bread and butter so there will be correlations amongst the bits. given a new partially observed bit vector representing a subset of items that the consumer has bought the goal is to predict which other bits are likely to turn on representing other items the consumer might be likely to buy. collaborative filtering we often assume there is no missing data in the training data since we know the past shopping behavior of each customer. this task arises in other domains besides modeling purchasing patterns. for example similar techniques can be used to model dependencies between files in complex software systems. in this case the task is to predict given a subset of files that have been changed which other ones need to be updated to ensure consistency e.g. et al. it is common to solve such tasks using frequent itemset mining which create association rules e.g. et al. sec for details. alternatively we can adopt a probabilistic approach and fit a joint density model xd to the bit vectors see e.g. et al. chapter introduction illustration of a k-nearest neighbors classifier in for k the nearest neighbors figure of test point have labels and so we predict py k the nearest neighbors of test point have labels and so we predict py k illustration of the voronoi tesselation induced by based on figure of et al. figure generated by knnvoronoi. such models often have better predictive acccuracy than association rules although they may be less interpretible. this is typical of the difference between data mining and machine learning in data mining there is more emphasis on interpretable models whereas in machine learning there is more emphasis on accurate models. some basic concepts in machine learning in this section we provide an introduction to some key ideas in machine learning. we will expand on these concepts later in the book but we introduce them briefly here to give a flavor of things to come. parametric vs non-parametric models in this book we will be focussing on probabilistic models of the form pyx or px depending on whether we are interested in supervised or unsupervised learning respectively. there are many ways to define such models but the most important distinction is this does the model have a fixed number of parameters or does the number of parameters grow with the amount of training data? the former is called a parametric model and the latter is called a nonparametric model. parametric models have the advantage of often being faster to use but the disadvantage of making stronger assumptions about the nature of the data distributions. nonparametric models are more flexible but often computationally intractable for large datasets. we will give examples of both kinds of models in the sections below. we focus on supervised learning for simplicity although much of our discussion also applies to unsupervised learning. a simple non-parametric classifier k-nearest neighbors a simple example of a non-parametric classifier is the k nearest neighbor classifier. this simply looks at the k points in the training set that are nearest to the test input x some basic concepts in machine learning train predicted label figure probability of class map estimate of class label. figure generated by knnclassifydemo. some synthetic training data in probability of class for knn with k counts how many members of each class are in this set and returns that empirical fraction as the estimate as illustrated in figure more formally i nk py cxd k k iyi c where nkxd are the of the k nearest points to x in d and ie is the indicator function defined as follows if e is true if e is false ie this method is an example of memory-based learning or instance-based learning. it can be derived from a probabilistic framework as explained in section the most common chapter introduction s e b u c f o h t g n e l e g d e fraction of data in neighborhood figure illustration of the curse of dimensionality. we embed a small cube of side s inside a larger unit cube. we plot the edge length of a cube needed to cover a given volume of the unit cube as a function of the number of dimensions. based on figure from et al. figure generated by cursedimensionality. distance metric to use is euclidean distance limits the applicability of the technique to data which is real-valued although other metrics can be used. figure gives an example of the method in action where the input is two dimensional we have three classes and k discuss the effect of k below. panel plots the training data. panel plots py where x is evaluated on a grid of points. panel plots py we do not need to plot py since probabilities sum to one. panel plots the map estimate yx argmaxcy cxd. a knn classifier with k induces a voronoi tessellation of the points figure this is a partition of space which associates a region v with each point xi in such a way that all points in v are closer to xi than to any other point. within each cell the predicted label is the label of the corresponding training point. the curse of dimensionality the knn classifier is simple and can work quite well provided it is given a good distance metric and has enough labeled training data. in fact it can be shown that the knn classifier can come within a factor of of the best possible performance if n and hart however the main problem with knn classifiers is that they do not work well with high dimensional inputs. the poor performance in high dimensional settings is due to the curse of dimensionality. to explain the curse we give some examples from et al. consider applying a knn classifier to data where the inputs are uniformly distributed in the d-dimensional unit cube. suppose we estimate the density of class labels around a test point x by growing a hyper-cube around x until it contains a desired fraction f of the data points. the expected edge length of this cube will be edf f if d and we want to base our estimate on some basic concepts in machine learning pdf a gaussian pdf with mean and variance figure generated by gaussplotdemo. figure visualization of the conditional density model pyx n the density falls off exponentially fast as we move away from the regression line. figure generated by of the data we have so we need to extend the cube along each dimension around x. even if we only use of the data we find see figure since the entire range of the data is only along each dimension we see that the method is no longer very local despite the name nearest neighbor the trouble with looking at neighbors that are so far away is that they may not be good predictors about the behavior of the input-output function at a given point. parametric models for classification and regression the main way to combat the curse of dimensionality is to make some assumptions about the nature of the data distribution pyx for a supervised problem or px for an unsupervised problem. these assumptions known as inductive bias are often embodied in the form of a parametric model which is a statistical model with a fixed number of parameters. below we briefly describe two widely used examples we will revisit these and other models in much greater depth later in the book. linear regression one of the most widely used models for regression is known as linear regression. this asserts that the response is a linear function of the inputs. this can be written as follows yx wt x wjxj where wt x represents the inner or scalar product between the input vector x and the model s weight vector and is the residual error between our linear predictions and the true response. in statistics it is more common to denote the regression weights by chapter introduction degree degree figure polynomial of degrees and fit by least squares to data points. figure generated by linregpolyvsdegree. we often assume that has a or normal distribution. we denote this by n where is the mean and is the variance chapter for details. when we plot this distribution we get the well-known bell curve shown in figure to make the connection between linear regression and gaussians more explicit we can rewrite the model in the following form pyx n this makes it clear that the model is a conditional probability density. in the simplest case we assume is a linear function of x so wt x and that the noise is fixed in this case are the parameters of the model. for example suppose the input is dimensional. we can represent the expected response as follows wt x where is the intercept or bias term is the slope and where we have defined the vector x x. a constant term to an input vector is a common notational trick which if is positive allows us to combine the intercept term with the other terms in the model. it means we expect the output to increase as the input increases. this is illustrated in in figure a more conventional plot of the mean response vs x is shown in figure linear regression can be made to model non-linear relationships by replacing x with some non-linear function of the inputs that is we use pyx n this is known as basis function expansion. for example figure illustrates the case where x xd for d and d this is known as polynomial regression. we will consider other kinds of basis functions later in the book. in fact many popular machine learning methods such as support vector machines neural networks classification and regression trees etc. can be seen as just different ways of estimating basis functions from data as we discuss in chapters and carl friedrich gauss was a german mathematician and physicist. some basic concepts in machine learning figure the sigmoid or logistic function. we have sigm and sigm figure generated by sigmoidplot. logistic regression for sat scores. solid black dots are the data. the open red circles are the predicted probabilities. the green crosses denote two students with the same sat score of hence same input representation x but with different training labels student passed y the other failed y hence this data is not perfectly separable using just the sat feature. figure generated by logregsatdemo. logistic regression we can generalize linear regression to the classification setting by making two changes. first we replace the gaussian distribution for y with a bernoulli is more appropriate for the case when the response is binary y that is we use pyx w bery where e second we compute a linear combination of the inputs as before but then we pass this through a function that ensures by defining sigmwt x where sigm refers to the sigmoid function also known as the logistic or logit function. this is defined as sigm exp e e the term sigmoid means s-shaped see figure for a plot. it is also known as a squashing function since it maps the whole real line to which is necessary for the output to be interpreted as a probability. putting these two steps together we get pyx w berysigmwt x this is called logistic regression due to its similarity to linear regression it is a form of classification not regression!. daniel bernoulli was a dutch-swiss mathematician and physicist. chapter introduction a simple example of logistic regression is shown in figure where we plot pyi w where xi is the score of student i and yi is whether they passed or failed a class. the solid black dots show the training data and the red circles plot py w where w are the parameters estimated from the training data discuss how to compute these estimates in section if we threshold the output probability at we can induce a decision rule of the form yx py by looking at figure we see that for x x we can imagine drawing a vertical line at x x this is known as a decision boundary. everything to the left of this line is classified as a and everything to the right of the line is classified as a we notice that this decision rule has a non-zero error rate even on the training set. this is because the data is not linearly separable i.e. there is no straight line we can draw to separate the from the we can create models with non-linear decision boundaries using basis function expansion just as we did with non-linear regression. we will see many examples of this later in the book. overfitting when we fit highly flexible models we need to be careful that we do not overfit the data that is we should avoid trying to model every minor variation in the input since this is more likely to be noise than true signal. this is illustrated in figure where we see that using a high degree polynomial results in a curve that is very wiggly it is unlikely that the true function has such extreme oscillations. thus using such a model might result in accurate predictions of future outputs. as another example consider the knn classifier. the value of k can have a large effect on the behavior of this model. when k the method makes no errors on the training set we just return the labels of the original training points but the resulting prediction surface is very wiggly figure therefore the method may not work well at predicting future in figure we see that using k results in a smoother prediction surface data. because we are averaging over a larger neighborhood. as k increases the predictions becomes smoother until in the limit of k n we end up predicting the majority label of the whole data set. below we discuss how to pick the right value of k. model selection when we have a variety of models of different complexity linear or logistic regression models with different degree polynomials or knn classifiers with different values of k how should we pick the right one? a natural approach is to compute the misclassification rate on sat stands for scholastic aptitude test this is a standardized test for college admissions used in the united states data in this example is from and albert some basic concepts in machine learning predicted label predicted label figure prediction surface for knn on the data in figure figure generated by knnclassifydemo. the training set for each method. this is defined as follows errfd n if yi where f is our classifier. in figure we plot this error rate vs k for a knn classifier blue line. we see that increasing k increases our error rate on the training set because we are over-smoothing. as we said above we can get minimal error on the training set by using k since this model is just memorizing the data. however what we care about is generalization error which is the expected value of the misclassification rate when averaged over future data section for details. this can be approximated by computing the misclassification rate on a large independent test set not used during model training. we plot the test error vs k in figure in solid red curve. now we see a u-shaped curve for complex models k the method overfits and for simple models k the method underfits. therefore an obvious way to pick k is to pick the value with the minimum error on the test set this example any value between and should be fine. unfortunately when training the model we don t have access to the test set assumption so we cannot use the test set to pick the model of the right however we can create a test set by partitioning the training set into two the part used for training the model and a second part called the validation set used for selecting the model complexity. we then fit all the models on the training set and evaluate their performance on the validation set and pick the best. once we have picked the best we can refit it to all the available data. if we have a separate test set we can evaluate performance on this in order to estimate the accuracy of our method. discuss this in more detail in section often we use about of the data for the training set and for the validation set. but if the number of training cases is small this technique runs into problems because the model in academic settings we usually do have access to the test set but we should not use it for model fitting or model selection otherwise we will get an unrealistically optimistic estimate of performance of our method. this is one of the golden rules of machine learning research. e t a r n o i t a c i f i s s a c s m i l chapter introduction train test k misclassification rate vs k in a k-nearest neighbor classifier. on the left where k is figure small the model is complex and hence we overfit. on the right where k is large the model is simple and we underfit. dotted blue line training set solid red line test set schematic of cross validation. figure generated by knnclassifydemo. won t have enough data to train on and we won t have enough data to make a reliable estimate of the future performance. a simple but popular solution to this is to use cross validation the idea is simple we split the training data into k folds then for each fold k k we train on all the folds but the k th and test on the k th in a round-robin fashion as sketched in figure we then compute the error averaged over all the folds and use this as a proxy for the test error. that each point gets predicted only once although it will be used for training k times. it is common to use k this is called cv. if we set k n then we get a method called leave-one out cross validation or loocv since in fold i we train on all the data cases except for i and then test on i. exercise asks you to compute the cv estimate of the test error vs k and to compare it to the empirical test error in figure choosing k for a knn classifier is a special case of a more general problem known as model selection where we have to choose between models with different degrees of flexibility. crossvalidation is widely used for solving such problems although we will discuss other approaches later in the book. no free lunch theorem all models are wrong but some models are useful. george box and draper much of machine learning is concerned with devising different models and different algorithms to fit them. we can use methods such as cross validation to empirically choose the best method for our particular problem. however there is no universally best model this is sometimes called the no free lunch theorem the reason for this is that a set of assumptions that works well in one domain may work poorly in another. george box is a retired statistics professor at the university of wisconsin. some basic concepts in machine learning as a consequence of the no free lunch theorem we need to develop many different types of models to cover the wide variety of data that occurs in the real world. and for each model there may be many different algorithms we can use to train the model which make different speed-accuracy-complexity tradeoffs. it is this combination of data models and algorithms that we will be studying in the subsequent chapters. exercises exercise knn classifier on shuffled mnist data run and verify that the misclassification rate the first test cases of mnist of a classifier is you run it all on all test cases the error rate is modify the code so that you first randomly permute the features of the training and test design matrices as in shuffleddigitsdemo and then apply the classifier. verify that the error rate is not changed. exercise approximate knn classifiers use the matlabc code at httppeople.cs.ubc.camariusmindex.phpflannflann to perform approximate nearest neighbor search and combine it with to classify the mnist data set. how much speedup do you get and what is the drop any in accuracy? exercise cv for knn use knnclassifydemo to plot the cv estimate of the misclassification rate on the test set. compare this to figure discuss the similarities and differences to the test error rate. probability introduction probability theory is nothing but common sense reduced to calculation. pierre laplace in the previous chapter we saw how probability can play a useful role in machine learning. in this chapter we discuss probability theory in more detail. we do not have to space to go into great detail for that you are better off consulting some of the excellent textbooks available on this topic such as bertsekas and tsitsiklis wasserman but we will briefly review many of the key ideas you will need in later chapters. before we start with the more technical material let us pause and ask what is probability? we are all familiar with the phrase the probability that a coin will land heads is but what does this mean? there are actually at least two different interpretations of probability. one is called the frequentist interpretation. in this view probabilities represent long run frequencies of events. for example the above statement means that if we flip the coin many times we expect it to land heads about half the the other interpretation is called the bayesian interpretation of probability. in this view probability is used to quantify our uncertainty about something hence it is fundamentally related to information rather than repeated trials in the bayesian view the above statement means we believe the coin is equally likely to land heads or tails on the next toss. one big advantage of the bayesian interpretation is that it can be used to model our uncertainty about events that do not have long term frequencies. for example we might want to compute the probability that the polar ice cap will melt by ce. this event will happen zero or one times but cannot happen repeatedly. nevertheless we ought to be able to quantify our uncertainty about this event based on how probable we think this event is we will take appropriate actions section for a discussion of optimal decision making under uncertainty. to give some more machine learning oriented examples we might have received a specific email message and want to compute the probability it is spam. or we might have observed a blip on our radar screen and want to compute the probability distribution over the location of the corresponding target it a bird plane or missile. in all these cases the idea of repeated trials does not make sense but the bayesian interpretation is valid and indeed actually the stanford statistician former professional magician persi diaconis has shown that a coin is about likely to land facing the same way up as it started due to the physics of the problem et al. chapter probability a uniform distribution on with px k a degenerate distribution figure px if x and px if x figure generated by discreteprobdistfig. quite natural. we shall therefore adopt the bayesian interpretation in this book. fortunately the basic rules of probability theory are the same no matter which interpretation is adopted. a brief review of probability theory this section is a very brief review of the basics of probability theory and is merely meant as a refresher for readers who may be rusty readers who are already familiar with these basics may safely skip this section. discrete random variables the expression pa denotes the probability that the event a is true. for example a might be the logical expression it will rain tomorrow we require that pa where pa means the event definitely will not happen and pa means the event definitely will happen. we write pa to denote the probability of the event not a this is defined to pa pa. we will often write a to mean the event a is true and a to mean the event a is false. we can extend the notion of binary events by defining a discrete random variable x which can take on any value from a finite or countably infinite set x we denote the probability of the event that x x by px x or just px for short. here p is called a probability mass function or pmf. this satisfies the properties px and x x px figure shows two pmf s defined on the finite state space x on the left we have a uniform distribution px and on the right we have a degenerate distribution px ix where i is the binary indicator function. this distribution represents the fact that x is always equal to the value in other words it is a constant. fundamental rules in this section we review the basic rules of probability. a brief review of probability theory probability of a union of two events given two events a and b we define the probability of a or b as follows pa b a b pa b pa b if a and b are mutually exclusive joint probabilities we define the probability of the joint event a and b as follows pa b pa b pabpb this is sometimes called the product rule. given a joint distribution on two events pa b we define the marginal distribution as follows pab bpb b pa pa b b b where we are summing over all possible states of b. we can define pb similarly. this is sometimes called the sum rule or the rule of total probability. the product rule can be applied multiple times to yield the chain rule of probability where we introduce the matlab-like notation to denote the set d. conditional probability we define the conditional probability of event a given that event b is true as follows pab pa b pb if pb bayes rule combining the definition of conditional probability with the product and sum rules yields bayes rule also called bayes px xpy yx x px yx px xy y px x y y py y example medical diagnosis as an example of how to use this rule consider the following medical diagonsis problem. suppose you are a woman in your and you decide to have a medical test for breast cancer called a mammogram. if the test is positive what is the probability you have cancer? that obviously depends on how reliable the test is. suppose you are told the test has a sensitivity thomas bayes was an english mathematician and presbyterian minister. chapter probability of which means if you have cancer the test will be positive with probability in other words px where x is the event the mammogram is positive and y is the event you have breast cancer. many people conclude they are therefore likely to have cancer. but this is false! it ignores the prior probability of having breast cancer which fortunately is quite low py ignoring this prior is called the base rate fallacy. we also need to take into account the fact that the test may be a false positive or false alarm. unfortunately such false positives are quite likely current screening technology px combining these three terms using bayes rule we can compute the correct answer as follows py px px px where py py about a chance of actually having breast in other words if you test positive you only have example generative classifiers we can generalize the medical diagonosis example to classify feature vectors x of arbitrary type as follows py cx py c c py this is called a generative classifier since it specifies how to generate the data using the classconditional density pxy c and the class prior py c. we discuss such models in detail in chapters and an alternative approach is to directly fit the class posterior py cx this is known as a discriminative classifier. we discuss the pros and cons of the two approaches in section independence and conditional independence we say x and y are unconditionally independent or marginally independent denoted x y if we can represent the joint as the product of the two marginals figure i.e. x y px y pxpy a brief review of probability theory figure computing px y pxpy where x y here x and y are discrete random variables x has possible states and y has possible states. a general joint distribution on two such variables would require parameters to define it subtract because of the sum-to-one constraint. by assuming independence we only need parameters to define px y. in general we say a set of variables is mutually independent if the joint can be written as a product of marginals. unfortunately unconditional independence is rare because most variables can influence most other variables. however usually this influence is mediated via other variables rather than being direct. we therefore say x and y are conditionally independent given z iff the conditional joint can be written as a product of conditional marginals x y px y pxzpy when we discuss graphical models in chapter we will see that we can write this assumption as a graph x z y which captures the intuition that all the dependencies between x and y are mediated via z. for example the probability it will rain tomorrow x is independent of whether the ground is wet today y given knowledge of whether it is raining today z. intuitively this is because z causes both x and y so if we know z we do not need to know about y in order to predict x or vice versa. we shall expand on this concept in chapter another characterization of ci is this theorem x y iff there exist function g and h such that px yz gx zhy z for all x y z such that pz these numbers are from based on this analysis the us government decided not to recommend annual mammogram screening to women in their the number of false alarms would cause needless worry and stress amongst women and result in unnecesssary expensive and potentially harmful followup tests. see section for the optimal way to trade off risk reverse reward in the face of uncertainty. chapter probability see exercise for the proof. ci assumptions allow us to build large probabilistic models from small pieces. we will see many examples of this throughout the book. in particular in section we discuss naive bayes classifiers in section we discuss markov models and in chapter we discuss graphical models all of these models heavily exploit ci properties. continuous random variables so far we have only considered reasoning about uncertain discrete quantities. we will now show how to extend probability to reason about uncertain continuous quantities. suppose x is some uncertain continuous quantity. the probability that x lies in any interval a x b can be computed as follows. define the events a a b b and w x b. we have that b a w and since a and w are mutually exclusive the sum rules gives pb pa w and hence pw pb pa define the function f px q. this is called the cumulative distribution function or cdf of x. this is obviously a monotonically increasing function. see figure for an example. using this notation we have pa x b f f now define f d dx f assume this derivative exists this is called the probability density function or pdf. see figure for an example. given a pdf we can compute the probability of a continuous variable being in a finite interval as follows b p x b f a as the size of the interval gets smaller we can write p x x dx pxdx we require px but it is possible for px for any given x so long as the density integrates to as an example consider the uniform distribution unifa b unifxa b b a if we set a and b ia x b we havep x for any x a brief review of probability theory cdf figure plot of the cdf for the standard normal n corresponding pdf. the shaded regions each contain of the probability mass. therefore the nonshaded region contains of the probability mass. if the distribution is gaussian n then the leftmost cutoff point is where if is the cdf of the gaussian. by symmetry the rightost cutoff point is the central interval is and the left cutoff is and the right is figure generated by quantiledemo. quantiles since the cdf f is a monotonically increasing function it has an inverse let us denote this by is the value of x such that p x this is if f is the cdf of x then f f is the median of the distribution with half of called the quantile of f the value f the probability mass on the left and half on the right. the values f are the lower and upper quartiles. the cdf of the gaussian distribution n then points to the left of probability mass as illustrated in figure by symmetry points to the right of also contain of the mass. hence the central interval of the mass. if we set the central interval is covered by the range we can also use the inverse cdf to compute tail area probabilities. for example if is contain contains and f if the distribution is n then the interval becomes this is sometimes approximated by writing mean and variance the most familiar property of a distribution is its mean orexpected value denoted by for discrete rv s it is defined as e x x x px and for continuous rv s it is defined as e x x pxdx. if this integral is not finite the mean is not defined will see some examples of this later. the variance is a measure of the spread of a distribution denoted by this is defined chapter probability pxdx x xpxdx e as follows var e from which we derive the useful result e x the standard deviation is defined as std var this is useful since it has the same units as x itself. some common discrete distributions in this section we review some commonly used parametric distributions defined on discrete state spaces both finite and countably infinite. the binomial and bernoulli distributions suppose we toss a coin n times. let x n be the number of heads. if the probability of heads is then we say x has a binomial distribution written as x binn the pmf is given by n k binkn n k n! k!k! k is the number of ways to choose k items from n is known as the binomial coefficient and is pronounced n choose k see figure for some examples of the binomial distribution. this distribution has the following mean and variance mean var n now suppose we toss a coin only once. let x be a binary random variable with probability of success or heads of we say that x has a bernoulli distribution. this is written as x ber where the pmf is defined as berx in other words berx if x if x this is obviously just a special case of a binomial distribution with n some common discrete distributions illustration of the binomial distribution with n and figure generated figure by binomdistplot. the multinomial and multinoulli distributions the binomial distribution can be used to model the outcomes of coin tosses. to model the outcomes of tossing a k-sided die we can use the multinomial distribution. this is defined as let x xk be a random vector where xj is the number of times side j of follows the die occurs. then x has the following pmf xj j n xk muxn n xk n! x k! where j is the probability that side j shows up and is the multinomial coefficient number of ways to divide a set of size n subsets with sizes up to xk. xk into now suppose n this is like rolling a k-sided dice once so x will be a vector of and bit vector in which only one bit can be turned on. specifically if the dice shows up as face k then the k th bit will be on. in this case we can think of x as being a scalar categorical random variable with k states and x is its dummy encoding that is x ix k. for example if k we encode the states and as and this is also called a one-hot encoding since we imagine that only one of the k wires is hot or on. in this case the pmf becomes ixj j see figure for an example. this very common special case is known as a categorical or discrete distribution. lacerda suggested we call it the multinoulli distribution by analogy with the binomial bernoulli distinction a term which we shall adopt in this book. we chapter probability name multinomial multinoulli binomial bernoulli n k x x nk x x n x xk n xk encoding table summary of the multinomial and related distributions. a t a g c c g g t a c g g c a t t a g c t g c a a c c g c a t c a g c c a c t a g a g c a a t a a c c g c g a c c g c a t t a g c c g c t a a g g t a t a a g c c t c g t a c g t a t t a g c c g t t a c g g c c a t a t c c g g t a c a g t a a t a g c a g g t a c c g a a a c a t c c g t g a c g g a a s t i b sequence position figure seqlogodemo. some aligned dna sequences. the corresponding sequence logo. figure generated by will use the following notation for this case catx in otherwords if x cat then px j j. see table for a summary. application dna sequence motifs an interesting application of multinomial models arises in biosequence analysis. suppose we have a set of dna sequences such as in figure where there are rows and columns along the genome. we see that several locations are conserved by evolution because they are part of a gene coding region since the corresponding columns tend to be pure for example column is all g s. one way to visually summarize the data is by using a sequence logo see figure we plot the letters a c g and t with a fontsize proportional to their empirical probability and with the most probable letter on the top. the empirical probability distribution at location t t is gotten by normalizing the vector of counts equation nt ixit ixit ixit ixit t ntn this distribution is known as a motif. we can also compute the most probable letter in each location this is called the consensus sequence. some common discrete distributions poi poi figure illustration of some poisson distributions for we have truncated the x-axis to for clarity but the support of the distribution is over all the non-negative integers. figure generated by poissonplotdemo. the poisson distribution we say that x has a poisson distribution with parameter written x poi if its pmf is x x! poix the first term is just the normalization constant required to ensure the distribution sums to the poisson distribution is often used as a model for counts of rare events like radioactive decay and traffic accidents. see figure for some plots. the empirical distribution given a set of data d xn we define the empirical distribution also called the empirical measure as follows xi pempa n xa if x a if x a where xa is the dirac measure defined by in general we can associate weights with each sample px wi xi where we require wi and wi we can think of this as a histogram with spikes at the data points xi where wi determines the height of spike i. this distribution assigns probability to any point not in the data set. chapter probability some common continuous distributions in this section we present some commonly used univariate continuous probability distributions. gaussian distribution the most widely used distribution in statistics and machine learning is the gaussian or normal distribution. its pdf is given by n e here e is the mean mode and var is the variance. normalization constant needed to ensure the density integrates to exercise is the if x n we say x follows a standard normal distribution. see figure for a plot of this pdf this is sometimes called the bell curve. we write x n to denote that px x we will often talk about the precision of a gaussian by which we mean the inverse variance a high precision means a narrow distribution variance centered on density at its center x we have n px note that since this is a pdf we can have px to see this consider evaluating the we have so if x the cumulative distribution function or cdf of the gaussian is defined as n see figure for a plot of this cdf when this integral has no closed form expression but is built in to most software packages. in particular we can compute it in terms of the error function erfz where z and x erfx e dt the gaussian distribution is the most widely used distribution in statistics. there are several reasons for this. first it has two parameters which are easy to interpret and which capture some of the most basic properties of a distribution namely its mean and variance. second the central limit theorem tells us that sums of independent random variables have an approximately gaussian distribution making it a good choice for modeling residual errors or noise third the gaussian distribution makes the least number of assumptions the symbol will have many different meanings in this book in order to be consistent with the rest of the literature. the intended meaning should be clear from context. some common continuous distributions maximum entropy subject to the constraint of having a specified mean and variance as we show in section this makes it a good default choice in many cases. finally it has a simple mathematical form which results in easy to implement but often highly effective methods as we will see. see ch for a more extensive discussion of why gaussians are so widely used. degenerate pdf in the limit that the gaussian becomes an infinitely tall and infinitely thin spike centered at where is called a dirac delta function and is defined as n lim if x if x such that from a sum or integral a useful property of delta functions is the sifting property which selects out a single term f f since the integrand is only non-zero if x one problem with the gaussian distribution is that it is sensitive to outliers since the logprobability only decays quadratically with distance from the center. a more robust distribution is the student t its pdf is as follows t x where is the mean is the scale parameter and is called the degrees of freedom. see figure for some plots. for later reference we note that the distribution has the following properties mean mode var this distribution has a colourful etymology. it was first published in by william sealy gosset who worked at the guinness brewery in dublin. since his employer would not allow him to use his own name he called it the student distribution. the origin of the term t seems to have arisen in the context of tables of the student distribution used by fisher when developing the basis of classical statistical inference. see for more historical details. gauss student laplace chapter probability gauss student laplace the pdf s for a n t and figure the mean is and the variance is for both the gaussian and laplace. the mean and variance of the student is undefined when log of these pdf s. note that the student distribution is not log-concave for any parameter value unlike the laplace distribution which is always log-concave log-convex... nevertheless both are unimodal. figure generated by studentlaplacepdfplot. gaussian student t laplace gaussian student t laplace figure illustration of the effect of outliers on fitting gaussian student and laplace distributions. no outliers gaussian and student curves are on top of each other. with outliers. we see that the gaussian is more affected by outliers than the student and laplace distributions. based on figure of figure generated by robustdemo. the variance is only defined if the mean is only defined if as an illustration of the robustness of the student distribution consider figure on the left we show a gaussian and a student fit to some data with no outliers. on the right we add some outliers. we see that the gaussian is affected a lot whereas the student distribution hardly changes. this is because the student has heavier tails at least for small figure if this distribution is known as the cauchy or lorentz distribution. this is notable to ensure finite variance we require for having such heavy tails that the integral that defines the mean does not converge. it is common to use which gives good performance in a range of problems et al. for the student distribution rapidly approaches a gaussian distribution and loses its robustness properties. some common continuous distributions gamma distributions if a the mode is at otherwise it is as figure some gaa b distributions. we increase the rate b we reduce the horizontal scale thus squeezing everything leftwards and upwards. figure generated by gammaplotdemo. an empirical pdf of some rainfall data with a fitted gamma distribution superimposed. figure generated by gammarainfalldemo. the laplace distribution another distribution with heavy tails is the laplace also known as the double sided exponential distribution. this has the following pdf lapx b exp b here is a location parameter and b is a scale parameter. see figure for a plot. this distribution has the following properties mean mode var its robustness to outliers is illustrated in figure it also put mores probability density at than the gaussian. this property is a useful way to encourage sparsity in a model as we will see in section the gamma distribution the gamma distribution is a flexible distribution for positive real valued rv s x defined in terms of two parameters called the shape a and the rate b it is gatshape a rate b ba t a t b pierre-simon laplace was a french mathematician who played a key role in creating the field of bayesian statistics. there is an alternative parameterization where we use the scale parameter instead of the rate gasta b gata this version is the one used by matlab s gampdf although in this book will use the rate parameterization unless otherwise specified. chapter probability where is the gamma function ux udu a b see figure for some plots. for later reference we note that the distribution has the following properties mean a b mode var a there are several distributions which are just special cases of the gamma which we discuss erlang distribution this is the same as the gamma distribution where a is an integer. below. exponential distribution this is defined by exponx where is the rate parameter. this distribution describes the times between events in a poisson process i.e. a process in which events occur continuously and independently at a constant average rate it is common to fix a yielding the one-parameter erlang distribution erlangx where is the rate parameter. this is the distribution of the sum of squared gaussian random variables. more precisely if zi n and s chi-squared distribution this is defined by gax i then s z that another useful result is the following x iga b whereig is the inverse gamma distribution defined by igxshape a scale b x bx ba if x gaa b then one can show the distribution has these properties mean b a mode b a var the mean only exists if a the variance only exists if a we will see applications of these distributions later on. the beta distribution the beta distribution has support over the interval and is defined as follows betaxa b ba b xa xb here bp q is the beta function ba b b see figure for plots of some beta distributions. we require a b to ensure the distribution if is integrable to ensure ba b exists. if a b we get the uniform distirbution. some common continuous distributions beta distributions figure some beta distributions. figure generated by betaplotdemo. a and b are both less than we get a bimodal distribution with spikes at and if a and b are both greater than the distribution is unimodal. for later reference we note that the distribution has the following properties mean a a b mode pareto distribution a a b var ab b the pareto distribution is used to model the distribution of quantities that exhibit long tails also called heavy tails. for example it has been observed that the most frequent word in english the occurs approximately twice as often as the second most frequent word of which occurs twice as often as the fourth most frequent word etc. if we plot the frequency of words vs their rank we will get a power law this is known as zipf s law. wealth has a similarly skewed distribution especially in plutocracies such as the the pareto pdf is defined as follow paretoxk m kmkx ix m this density asserts that x must be greater than some constant m but not too much greater where k controls what is too much as k the distribution approaches m. see if we plot the distibution on a log-log scale it forms a straight figure for some plots. line of the form log px a log x c for some constants a and c. see figure for an illustration is known as a power law. this distribution has the following properties mean km k if k mode m var if k americans have more wealth than half of in the usa see and pierson for a political analysis of how such an extreme distribution of income has arisen in a democratic country. all americans combined. chapter probability pareto distribution k on log scale the pareto distribution paretoxm k for m the pdf on a log-log scale. figure figure generated by paretoplot. joint probability distributions so far we have been mostly focusing on modeling univariate probability distributions. in this section we start our discussion of the more challenging problem of building joint probability distributions on multiple related random variables this will be a central topic in this book. a joint probability distribution has the form xd for a set ofd variables and models the relationships between the variables. if all the variables are discrete we can represent the joint distribution as a big multi-dimensional array with one variable per dimension. however the number of parameters needed to define such a model is ok d where k is the number of states for each variable. we can define high dimensional joint distributions using fewer parameters by making conditional independence assumptions as we explain in chapter in the case of continuous distributions an alternative approach is to restrict the form of the pdf to certain functional forms some of which we will examine below. covariance and correlation the covariance between two rv s x and y measures the degree to which x and y are related. covariance is defined as cov y e e e e e e joint probability distributions figure several sets of y points with the correlation coefficient of x and y for each set. note that the correlation reflects the noisiness and direction of a linear relationship row but not the slope of that relationship nor many aspects of nonlinear relationships n.b. the figure in the center has a slope of but in that case the correlation coefficient is undefined because the variance of y is zero. source httpen.wikipedia.orgwikifilecorrelation_examples.png if x is a d-dimensional random vector its covariance matrix is defined to be the following symmetric positive definite matrix cov e e e var t cov cov var cov cov cov xd cov xd var covariances can be between and infinity. sometimes it is more convenient to work with a normalized measure with a finite upper bound. the correlation coefficient between x and y is defined as corr y cov y var var a correlation matrix has the form corr r corr corr xd corr xd corr corr one can show that corr y hence in a correlation matrix each entry on the diagonal is and the other entries are between and one can also show that corr y if and only if y ax b for some parameters a and b i.e. if there is a linear relationship between x and y exercise intuitively one chapter probability might expect the correlation coefficient to be related to the slope of the regression line i.e. the coefficient a in the expression y ax b. however as we show in equation later the regression coefficient is in fact given by a cov y a better way to think of the correlation coefficient is as a degree of linearity see figure if x and y are independent meaning px y xpy section then cov y and hence corr y so they are uncorrelated. however the converse is not true uncorrelated does not imply independent. for example let x u and y x clearly y is dependent on x fact y is uniquely determined by x yet one can show that corr y some striking examples of this fact are shown in figure this shows several data sets where there is clear dependendence between x and y and yet the correlation coefficient is a more general measure of dependence between random variables is mutual information discussed in section this is only zero if the variables truly are independent. the multivariate gaussian the multivariate gaussian or multivariate normal is the most widely used joint probability density function for continuous variables. we discuss mvns in detail in chapter here we just give some definitions and plots. the pdf of the mvn in d dimensions is defined by the following n exp where e r d is the mean vector and cov is the d d covariance matrix. sometimes we will work in terms of the precision matrix or concentration matrix instead. this is just the inverse covariance matrix the normalization constant just ensures that the pdf integrates to exercise figure plots some mvn densities in for three different kinds of covariance matrices. a full covariance matrix has dd parameters divide by since is symmetric. a diagonal covariance matrix has d parameters and has in the off-diagonal terms. a spherical or isotropic covariance has one free parameter. multivariate student t distribution a more robust alternative to the mvn is the multivariate student t distribution whose pdf is given by t v v where is called the scale matrix it is not exactly the covariance matrix and v this has fatter tails than a gaussian. the smaller is the fatter the tails. as the joint probability distributions full spherical diagonal spherical figure we show the level sets for gaussians. a full covariance matrix has elliptical contours. a diagonal covariance matrix is an axis aligned ellipse. a spherical covariance matrix has a circular shape. surface plot for the spherical gaussian in figure generated by distribution tends towards a gaussian. the distribution has the following properties mean mode cov dirichlet distribution a multivariate generalization of the beta distribution is the dirichlet which has support over the probability simplex defined by xk x k k ix sk sk xk the pdf is defined as follows dirx b johann dirichlet was a german mathematician chapter probability p figure the dirichlet distribution when k defines a distribution over the simplex which can be represented by the triangular surface. points on this surface satisfy k and k figure generated by visdirichletgui by jonathan huang. comb-like structure on the edges is a plotting artifact. figure generated by plot of the dirichlet density when samples from dir samples from dir figure this results in very sparse distributions with many results in more uniform dense distributions. figure generated by dirichlethistogramdemo. samples from a symmetric dirichlet distribution for different parameter values. this where b k is the natural generalization of the beta function to k variables transformations of random variables b where k k. figure shows some plots of the dirichlet when k and figure for some sampled probability vectors. we see that k controls the strength of the distribution peaked it is and the k control where the peak occurs. for example is a uniform distribution is a broad distribution centered at and is a narrow distribution centered at if k for all k we get spikes at the corner of the simplex. for future reference the distribution has these properties e mode var k k k k where case the mean becomes and the variance becomes var k increases the precision the variance of the distribution. k k. often we use a symmetric dirichlet prior of the form k in this so increasing k transformations of random variables if x p is some random variable and y f what is the distribution of y? this is the question we address in this section. linear transformations suppose f is a linear function y f ax b in this case we can easily derive the mean and covariance of y as follows. first for the mean we have e e b a b where e this is called the linearity of expectation. if f is a scalar-valued function f at x b the corresponding result is e at x b at b for the covariance we have cov cov b a at where cov we leave the proof of this as an exercise. if f is scalar valued the result becomes var var at x b at a chapter probability we will use both of these results extensively in later chapters. note however that the mean and covariance only completely define the distribution of y if x is gaussian. in general we must use the techniques described below to derive the full distribution of y as opposed to just its first two moments. general transformations if x is a discrete rv we can derive the pmf for y by simply summing up the probability mass for all the x s such that f pyy pxx xf for example if f if x is even and f otherwise and pxx is uniform on the set then x pxx and similarly. note that in this example f is a many-to-one function. if x is continuous we cannot use equation since pxx is a density not a pmf and we cannot sum up densities. instead we work with cdf s and write pyy p y p y p y we can derive the pdf of y by differentiating the cdf. in the case of monotonic and hence invertible functions we can write pyy p y p f pxf taking derivatives we get pxf d dy pyy d pyy dy we can think of dx as a measure of volume in the x-space similarly dy dy measures the change in volume. since the sign of this where x f measures volume in y space. thus dx change is not important we take the absolute value to get the general expression pxx dx dy dx dy d dx pxx dx dy pyy pxx this is called change of variables formula. we can understand this result more intuitively as follows. observations falling in the range x x will get transformed into y y where pxx x pyy y. hence pyy pxx x y. for example suppose x u and y x then pyy y see also exercise multivariate change of variables we can extend the previous results to multivariate distributions as follows. let f be a function that maps r n and let y f then its jacobian matrix j is given by n to r jx y yn xn yn xn yn xn transformations of random variables det j measures how much a unit cube changes in volume when we apply f. jacobian of the inverse mapping y x if f is an invertible mapping we can define the pdf of the transformed variables using the det x y pxx det jy x pyy pxx in exercise you will use this formula to derive the normalization constant for a multivariate gaussian. as a simple example consider transforming a density from cartesian coordinates x to polar coordinates y wherex r cos and r sin then cos r sin r cos sin jy x r r and det j r hence pyy xx det j pr cos r sin to see this geometrically notice that the area of the shaded patch in figure is given by p r r dr d r in the limit this is equal to the density at the center of the patch pr times the size of the patch r dr d hence pr cos r sin dr d central limit theorem now consider n random variables with pdf s necessarily gaussian pxi each with mean and variance we assume each variable is independent and identically distributed or iid for short. let sn xi be the sum of the rv s. this is a simple but widely used transformation of rv s. one can show that as n increases the distribution of this sum approaches psn s n exp n hence the distribution of the quantity zn sn n n x n converges to the standard normal where x n the central limit theorem. see e.g. or for a proof. xi is the sample mean. this is called in figure we give an example in which we compute the mean of rv s drawn from a beta distribution. we see that the sampling distribution of the mean value rapidly converges to a gaussian distribution. chapter probability figure change of variables from polar to cartesian. the area of the shaded patch is r dr d based on figure n n figure the central limit theorem in pictures. we plot a histogram of for j as n the distribution tends towards a gaussian. n n based on figure of figure generated by centrallimitdemo. xij where xij n monte carlo approximation in general computing the distribution of a function of an rv using the change of variables formula can be difficult. one simple but powerful alternative is as follows. first we generate s samples from the distribution call them xs. are many ways to generate such samples one popular method for high dimensional distributions is called markov chain monte carlo or mcmc this will be explained in chapter given the samples we can approximate the distribution of f by using the empirical distribution of this is called a monte carlo approximation named after a city in europe known for its plush gambling casinos. monte carlo techniques were first developed in the area of statistical physics in particular during development of the atomic bomb but are now widely used in statistics and machine learning as well. we can use monte carlo to approximate the expected value of any function of a random monte carlo approximation figure computing the distribution of y where px is uniform the analytic result is shown in the middle and the monte carlo approximation is shown on the right. figure generated by variable. we simply draw samples and then compute the arithmetic mean of the function applied to the samples. this can be written as follows f s e where xs px. this is called monte carlo integration and has the advantage over numerical integration is based on evaluating the function at a fixed grid of points that the function is only evaluated in places where there is non-negligible probability. f by varying the function f we can approximate many quantities of interest such as x s xs e var s c p c s xs median x we give some examples below and will see many more in later chapters. example change of variables the mc way in section we discussed how to analytically compute the distribution of a function of a random variable y f a much simpler approach is to use a monte carlo approximation. for example suppose x unif and y we can approximate py by drawing many samples from px squaring them and computing the resulting empirical distribution. see figure for an illustration. we will use this technique extensively in later chapters. see also figure chapter probability figure estimating by monte carlo integration. blue points are inside the circle red crosses are outside. figure generated by mcestimatepi. example estimating by monte carlo integration mc approximation can be used for many applications not just statistical ones. suppose we want to estimate we know that the area of a circle with radius r is but it is also equal to the following definite integral r r i r r hence let f y be an indicator function that is for points inside the circle and outside and let px and py be uniform distributions on r r sop x y then let us approximate this by monte carlo integration. i f ypxpydxdy s f ys f ypxpydxdy we find with standard error section for a discussion of standard errors. we can plot the points that are accepted rejected as in figure accuracy of monte carlo approximation the accuracy of an mc approximation increases with sample size. this is illustrated in figure on the top line we plot a histogram of samples from a gaussian distribution. on the bottom line we plot a smoothed version of these samples created using a kernel density estimate this smoothed distribution is then evaluated on a dense grid of points monte carlo approximation samples samples samples samples figure and samples from a gaussian distribution n solid red line is true pdf. top line histogram of samples. bottom line kernel density estimate derived from samples in dotted blue solid red line is true pdf. based on figure of figure generated by mcaccuracydemo. and plotted. note that this smoothing is just for the purposes of plotting it is not used for the monte carlo estimate itself. if we denote the exact mean by e and the mc approximation by one can show that with independent samples n s where var e e f this is a consequence of the central-limit theorem. of course is unknown in the above expression but it can also be estimated by mc s then we have p s s chapter probability s is called the or empirical standard error and is an estimate of our the term uncertainty about our estimate of section for more discussion on standard errors. if we want to report an answer which is accurate to within with probability at least we can approximate we need to use a number of samples s which satisfies the factor by yielding s information theory information theory is concerned with representing data in a compact fashion task known as data compression or source coding as well as with transmitting and storing it in a way that is robust to errors task known as error correction or channel coding. at first this seems far removed from the concerns of probability theory and machine learning but in fact there is an intimate connection. to see this note that compactly representing data requires allocating short codewords to highly probable bit strings and reserving longer codewords to less probable bit strings. this is similar to the situation in natural language where common words as a the and are generally much shorter than rare words. also decoding messages sent over noisy channels requires having a good probability model of the kinds of messages that people tend to send. in both cases we need a model that can predict which kinds of data are likely and which unlikely which is also a central problem in machine learning for more details on the connection between information theory and machine learning. obviously we cannot go into the details of information theory here e.g. and thomas if you are interested to learn more. however we will introduce a few basic concepts that we will need later in the book. entropy the entropy of a random variable x with distribution p denoted by h or sometimes h is a measure of its uncertainty. in particular for a discrete variable with k states it is defined by h px k px k usually we use log base in which case the units are called bits for binary digits. if we use log base e the units are called nats. for example if x with histogram distribution p we find h the discrete distribution with maximum entropy is the uniform distribution section for a proof. hence for a k-ary random variable the entropy is maximized if px k in this case h k. conversely the distribution with minimum entropy is zero is any delta-function that puts all its mass on one state. such a distribution has no uncertainty. in figure where we plotted a dna sequence logo the height of each bar is defined to be h where h is the entropy of that distribution and is the maximum possible entropy. thus a bar of height corresponds to a uniform distribution whereas a bar of height corresponds to a deterministic distribution. information theory x h px figure entropy of a bernoulli random variable as a function of the maximum entropy is figure generated by bernoullientropyfig. for the special case of binary random variables x we can write px and px hence the entropy becomes h px px px px this is called the binary entropy function and is also written h we plot this in figure we see that the maximum value of occurs when the distribution is uniform kl divergence one way to measure the dissimilarity of two probability distributions p and q is known as the kullback-leibler divergence divergence orrelative entropy. this is defined as follows kl pk log pk qk k k pk log pk pk log qk k where the sum gets replaced by an integral for we can rewrite this as kl pk log qk h q where h q is called the cross entropy h q one can show and thomas that the cross entropy is the average number of bits needed to encode data coming from a source with distribution p when we use model q to the kl divergence is not a distance since it is asymmetric. one symmetric version of the kl divergence is the jensen-shannon divergence defined as whereq chapter probability define our codebook. hence the regular entropy h h p defined in section is the expected number of bits if we use the true model so the kl divergence is the difference between these. in other words the kl divergence is the average number of extra bits needed to encode the data due to the fact that we used distribution q to encode the data instead of the true distribution p. the extra number of bits interpretation should make it clear that kl and that the kl is only equal to zero iff q p. we now give a proof of this important result. theorem inequality kl with equality iff p q. proof. to prove the theorem we need to use jensen s inequality. this states that for any convex function f we have that f ixi if i this is clearly true for n definition of convexity and let us now prove the main theorem following and thomas let a where i and can be proved by induction for n px be the support of px. then px qx kl px log x a x a log log x a px log px qx px log qx x a qx log x x qx px where the first inequality follows from jensen s. since logx is a strictly concave function we have equality in equation iff px for some c. we have equality in equation x x qx which implies c hence kl iff px qx iff for all x. x a qx one important consequence of this result is that the discrete distribution with the maximum entropy is the uniform distribution. more precisely h log where is the number of states for x with equality iff px is uniform. to see this let ux then kl x px log px ux px log ux h log x px log px x this is a formulation of laplace s principle of insufficient reason which argues in favor of using uniform distributions when there are no other reasons to favor one distribution over another. see section for a discussion of how to create distributions that satisfy certain constraints but otherwise are as least-commital as possible. example the gaussian satisfies first and second moment constraints but otherwise has maximum entropy. information theory mutual information consider two random variables x and y suppose we want to know how much knowing one variable tells us about the other. we could compute the correlation coefficient but this is only defined for real-valued random variables and furthermore this is a very limited measure of dependence as we saw in figure a more general approach is to determine how similar the joint distribution px y is to the factored distribution pxpy this is called the mutual information or mi and is defined as follows i y kl y px y log px y pxpy x y we have i y with equality iff px y pxpy that is the mi is zero iff the variables are independent. to gain insight into the meaning of mi it helps to re-express it in terms of joint and conditional entropies. one can show that the above expression is equivalent to the following i y h h h h where h is the conditional entropy defined as h x pxh x. thus we can interpret the mi between x and y as the reduction in uncertainty about x after observing y or by symmetry the reduction in uncertainty about y after observing x. we will encounter several applications of mi later in the book. see also exercises and for the connection between mi and correlation coefficients. a quantity which is closely related to mi is the pointwise mutual information or pmi. for two events random variables x and y this is defined as pmix y log px y pxpy log pxy px pyx py log this measures the discrepancy between these events occuring together compared to what would be expected by chance. clearly the mi of x and y is just the expected value of the pmi. interestingly we can rewrite the pmi as follows pxy px pyx py pmix y log this is the amount we learn from updating the prior px into the posterior pxy or equivalently updating the prior py into the posterior pyx. log mutual information for continuous random variables the above formula for mi is defined for discrete random variables. for continuous random variables it is common to first discretize or quantize them by dividing the ranges of each variable into bins and computing how many values fall in each histogram bin we can then easily compute the mi using the formula above mutualinfoallpairsmixed for some code and mimixeddemo for a demo. unfortunately the number of bins used and the location of the bin boundaries can have a significant effect on the results. one way around this is to try to estimate the mi directly chapter probability figure left correlation coefficient vs maximal information criterion for all pairwise relationships in the who data. right scatter plots of certain pairs of variables. the red lines are non-parametric smoothing regressions fit separately to each trend. source figure of et al. used with kind permission of david reshef and the american association for the advancement of science. without first performing density estimation another approach is to try many different bin sizes and locations and to compute the maximum mi achieved. this statistic appropriately normalized is known as the maximal information coefficient et al. more precisely define maxg gxy i y mx y log minx y where gx y is the set of grids of size x y and xg y represents a discretization of the variables onto this grid. maximization over bin locations can be performed efficiently using dynamic programming et al. now define the mic as mic max xyxyb mx y where b is some sample-size dependent bound on the number of bins we can use and still reliably estimate the distribution et al. suggest b n it can be shown that the mic lies in the range where represents no relationship between the variables and represents a noise-free relationship of any form not just linear. figure gives an example of this statistic in action. the data consists of variables measuring a variety of social economic health and political indicators collected by the world health organization on the left of the figure we see the correlation coefficient plotted against the mic for all variable pairs. on the right of the figure we see scatter plots for particular pairs of variables which we now discuss the point marked c has a low cc and a low mic. the corresponding scatter plot makes it information theory clear that there is no relationship between these two variables of lives lost to injury and density of dentists in the population. the points marked d and h have high cc absolute value and high mic because they represent nearly linear relationships. the points marked e f and g have low cc but high mic. this is because they correspond to non-linear sometimes as in the case of e and f non-functional i.e. one-to-many relationships between the variables. in summary we see that statistics as mic based on mutual information can be used to discover interesting relationships between variables in a way that simpler measures such as correlation coefficients cannot. for this reason the mic has been called a correlation for the century exercises exercise probabilities are sensitive to the form of the question that was used to generate the answer minka. my neighbor has two children. assuming that the gender of a child is like a coin flip it is most likely a priori that my neighbor has one boy and one girl with probability the other possibilities two boys or two girls have probabilities and a. suppose i ask him whether he has any boys and he says yes. what is the probability that one child is a girl? b. suppose instead that i happen to see one of his children run by and it is a boy. what is the probability that the other child is a girl? exercise legal reasoning peter lee. suppose a crime has been committed. blood is found at the scene for which there is no innocent explanation. it is of a type which is present in of the population. a. the prosecutor claims there is a chance that the defendant would have the crime blood type if he were innocent. thus there is a chance that he guilty this is known as the prosecutor s fallacy. what is wrong with this argument? b. the defender claims the crime occurred in a city of people. the blood type would be found in approximately people. the evidence has provided a probability of just in that the defendant is guilty and thus has no relevance. this is known as the defender s fallacy. what is wrong with this argument? exercise variance of a sum show that the variance of a sum is var y var var y where cov y is the covariance between x and y exercise bayes rule for medical diagnosis koller. after your yearly checkup the doctor has bad news and good news. the bad news is that you tested positive for a serious disease and that the test is accurate the probability of testing positive given that you have the disease is as is the probability of tetsing negative given that you don t have the disease. the good news is that this is a rare disease striking only one in people. what are the chances that you actually have the disease? your calculations as well as giving the final result. chapter probability exercise the monty hall problem mackay. on a game show a contestant is told the rules as follows there are three doors labelled a single prize has been hidden behind one of them. you get to select one door. initially your chosen door will not be opened. instead the gameshow host will open one of the other two doors and he will do so in such a way as not to reveal the prize. for example if you first choose door he will then open one of doors and and it is guaranteed that he will choose which one to open so that the prize will not be revealed. at this point you will be given a fresh choice of door you can either stick with your first choice or you can switch to the other closed door. all the doors will then be opened and you will receive whatever is behind your final choice of door. imagine that the contestant chooses door first then the gameshow host opens door revealing nothing behind the door as promised. should the contestant stick with door or switch to door or does it make no difference? you may assume that initially the prize is equally likely to be behind any of the doors. hint use bayes rule. exercise conditional independence koller. a. let h k be a discrete random variable and let and be the observed values of two other random variables and suppose we wish to calculate the vector p which of the following sets of numbers are sufficient for the calculation? i. p p p p ii. p p p iii. p p p b. now suppose we now assume and are conditionally independent given h. which of the above sets are sufficent now? show your calculations as well as giving the final result. hint use bayes rule. exercise pairwise independence does not imply mutual independence we say that two random variables are pairwise independent if and hence we say that n random variables are mutually independent if pxixs pxi s n and hence pxi show that pairwise independence between all pairs of variables does not necessarily imply mutual independence. it suffices to give a counter example. information theory exercise conditional independence iff joint factorizes in the text we said x y iff px yz pxzpyz for all x y z such that pz now prove the following alternative definition x y iff there exist function g and h such that px yz gx zhy z for all x y z such that pz exercise conditional independence koller. are the following properties true? prove or disprove. note that we are not restricting attention to distributions that can be represented by a graphical model. a. true or false? wz y y y wz b. true or false? y y y w exercise deriving the inverse gamma density let x gaa b i.e. gaxa b ba xa xb let y show that y iga b i.e. igxshape a scale b ba x bx hint use the change of variables formula. exercise normalization constant for a gaussian the normalization constant for a zero-mean gaussian is given by b z exp a dx where a and b to compute this consider its square b b z exp a a dxdy let us change variables from cartesian y to polar using x r cos and y r sin since dxdy rdrd and we have z r exp drd evaluate this integral and hence show z two terms the first of which d dudr re hint separate the integral into a product of then is constant so is easy. hint if u e so the second integral is also easy u ur. chapter probability exercise expressing mutual information in terms of entropies show that ix y hx hxy hy hy exercise mutual information for correlated normals and thomas find the mutual information where x has a bivariate normal distribution n evaluate for and and comment. hint the entropy of a d-dimensional gaussian is hx ed det in the case this becomes e hx hint exercise a measure of correlation mutual information and thomas let x and y be discrete random variables which are identically distributed hx hy but not necessarily independent. define r hy hx hx a. show r ixy b. show r c. when is r d. when is r exercise mle minimizes kl divergence to the empirical distribution let pempx be the empirical distribution and let qx be some model. show that argminq kl is obtained by qx where is the mle. hint use non-negativity of the kl divergence. exercise mean mode variance for the beta distribution suppose betaa b. derive the mean mode and variance. exercise expected value of the minimum suppose x y are two points sampled independently and uniformly at random from the interval what is the expected location of the left most point? generative models for discrete data introduction in section we discussed how to classify a feature vector x by applying bayes rule to a generative classifier of the form py cx pxy c c the key to using such models is specifying a suitable form for the class-conditional density pxy c which defines what kind of data we expect to see in each class. in this chapter we focus on the case where the observed data are discrete symbols. we also discuss how to infer the unknown parameters of such models. bayesian concept learning consider how a child learns to understand the meaning of a word such as dog presumably the child s parents point out positive examples of this concept saying such things as look at the cute dog! or mind the doggy etc. however it is very unlikely that they provide negative examples by saying look at that non-dog certainly negative examples may be obtained during an active learning process the child says look at the dog and the parent says that s a cat dear not a dog but psychological research has shown that people can learn concepts from positive examples alone and tenenbaum we can think of learning the meaning of a word as equivalent to concept learning which in turn is equivalent to binary classification. to see this define f if x is an example of the concept c and f otherwise. then the goal is to learn the indicator function f which just defines which elements are in the set c. by allowing for uncertainty about the definition of f or equivalently the elements of c we can emulate fuzzy set theory but using standard probability calculus. note that standard binary classification techniques require positive and negative examples. by contrast we will devise a way to learn from positive examples alone. for pedagogical purposes we will consider a very simple example of concept learning called the number game based on part of josh tenenbaum s phd thesis the game proceeds as follows. i choose some simple arithmetical concept c such as prime number or a number between and i then give you a series of randomly chosen positive examples d xn drawn from c and ask you whether some new test case x belongs to c i.e. i ask you to classify x. chapter generative models for discrete data examples figure empirical predictive distribution averaged over humans in the number game. first two rows after seeing d and d this illustrates diffuse similarity. third row after seeing d this illustrates rule-like behavior of bottom row after seeing d this illustrates focussed similarity near source figure of used with kind permission of josh tenenbaum. suppose for simplicity that all numbers are integers between and now suppose i tell you is a positive example of the concept. what other numbers do you think are positive? it s hard to tell with only one example so your predictions will be quite vague. presumably numbers that are similar in some sense to are more likely. but similar in what way? is similar because it is close by is similar because it has a digit in common is similar because it is also even and a power of but does not seem similar. thus some numbers are more likely than others. we can represent this as a probability distribution p xd which is the probability that x c given the data d for any x this is called the posterior predictive distribution. figure shows the predictive distribution of people derived from a lab experiment. we see that people predict numbers that are similar to under a variety of kinds of similarity. now suppose i tell you that and are also positive examples. now you may guess that the hidden concept is powers of two this is an example of induction. given this hypothesis the predictive distribution is quite specific and puts most of its mass on powers of as shown if instead i tell you the data is d you will get a in figure row. different kind of generalization gradient as shown in figure how can we explain this behavior and emulate it in a machine? the classic approach to induction is to suppose we have a hypothesis space of concepts h such as odd numbers even numbers all numbers between and powers of two all numbers ending in j bayesian concept learning j etc. the subset of h that is consistent with the data d is called the version space. as we see more examples the version space shrinks and we become increasingly certain about the concept however the version space is not the whole story. after seeing d there are many consistent rules how do you combine them to predict if x c? also after seeing d why did you choose the rule powers of two and not say all even numbers or powers of two except for both of which are equally consistent with the evidence? we will now provide a bayesian explanation for this. likelihood we must explain why we chose htwo powers of two and not say heven even numbers after seeing d given that both hypotheses are consistent with the evidence. the key intuition is that we want to avoid suspicious coincidences. if the true concept was even numbers how come we only saw numbers that happened to be powers of two? to formalize this let us assume that examples are sampled uniformly at random from the extension of a concept. extension of a concept is just the set of numbers that belong to it e.g. the extension of heven is the extension of numbers ending in is tenenbaum calls this the strong sampling assumption. given this assumption the probability of independently sampling n items replacement from h is given by pdh sizeh this crucial equation embodies what tenenbaum calls the size principle which means the model favors the simplest hypothesis consistent with the data. this is more commonly known as occam s to see how it works let d then pdhtwo since there are only powers of two less than but pdheven since there are even numbers. so the likelihood that h htwo is higher than if h heven. after examples the likelihood of htwo whereas the likelihood of heven is is this is a likelihood ratio of almost in favor of htwo. this quantifies our earlier intuition that d would be a very suspicious coincidence if generated by heven. prior suppose d given this data the concept more likely than h powers of two since is missing from the set of examples. powers of two except is does not need to explain the coincidence that however the hypothesis powers of two except seems conceptually unnatural we can capture such intution by assigning low prior probability to unnatural concepts. of course your prior might be different than mine. this subjective aspect of bayesian reasoning is a source of much controversy since it means for example that a child and a math professor william of occam spelt ockham was an english monk and philosopher chapter generative models for discrete data will reach different answers. in fact they presumably not only have different priors but also different hypothesis spaces. however we can finesse that by defining the hypothesis space of the child and the math professor to be the same and then setting the child s prior weight to be zero on certain advanced concepts. thus there is no sharp distinction between the prior and the hypothesis space. although the subjectivity of the prior is controversial it is actually quite useful. if you are told the numbers are from some arithmetic rule then given and you may think is likely but is unlikely. but if you are told that the numbers are examples of healthy cholesterol levels you would probably think is unlikely and is likely. thus we see that the prior is the mechanism by which background knowledge can be brought to bear on a problem. without this rapid learning from small samples sizes is impossible. so what prior should we use? for illustration purposes let us use a simple prior which puts uniform probability on simple arithmetical concepts such as even numbers odd numbers prime numbers numbers ending in etc. to make things more interesting we make the concepts even and odd more likely apriori. we also include two unnatural concepts namely powers of plus and powers of except but give them low prior weight. see figure for a plot of this prior. we will consider a slightly more sophisticated prior later on. posterior the posterior is simply the likelihood times the prior normalized. in this context we have phd pdhph h pd phid hhn h where id h is iff and only if all the data are in the extension of the hypothesis h. figure plots the prior likelihood and posterior after seeing d we see that the posterior is a combination of prior and likelihood. in the case of most of the concepts the prior is uniform so the posterior is proportional to the likelihood. however the unnatural concepts of powers of plus and powers of except have low posterior support despite having high likelihood due to the low prior. conversely the concept of odd numbers has low posterior support despite having a high prior due to the low likelihood. figure plots the prior likelihood and posterior after seeing d now the likelihood is much more peaked on the powers of two concept so this dominates the posterior. essentially the learner has an aha moment and figures out the true concept. we see the need for the low prior on the unnatural concepts otherwise we would have overfit the data and picked powers of except for in general when we have enough data the posterior phd becomes peaked on a single concept namely the map estimate i.e. phd hm ap where hm ap argmaxh phd is the posterior mode and where is the dirac measure defined by xa if x a if x a bayesian concept learning data even odd squares mult of mult of mult of mult of mult of mult of mult of mult of ends in ends in ends in ends in ends in ends in ends in ends in ends in powers of powers of powers of powers of powers of powers of powers of powers of powers of all powers of powers of prior lik post figure prior likelihood and posterior for d based on figure generated by numbersgame. note that the map estimate can be written as pdhph argmax hm ap argmax h h pdh log ph since the likelihood term depends exponentially on n and the prior stays constant as we get more and more data the map estimate converges towards the maximum likelihood estimate or mle hmle argmax h pdh argmax log pdh h in other words if we have enough data we see that the data overwhelms the prior. in this chapter generative models for discrete data data even odd squares mult of mult of mult of mult of mult of mult of mult of mult of ends in ends in ends in ends in ends in ends in ends in ends in ends in powers of powers of powers of powers of powers of powers of powers of powers of powers of all powers of powers of prior lik x post figure prior likelihood and posterior for d based on figure generated by numbersgame. case the map estimate converges towards the mle. if the true hypothesis is in the hypothesis space then the map ml estimate will converge upon this hypothesis. thus we say that bayesian inference ml estimation are consistent estimators section for details. we also say that the hypothesis space is identifiable in the limit meaning we can recover the truth in the limit of infinite data. if our hypothesis class is not rich enough to represent the truth will usually be the case we will converge on the hypothesis that is as close as possible to the truth. however formalizing this notion of closeness is beyond the scope of this chapter. bayesian concept learning powers of powers of ends in squares even mult of mult of all powers of powers of ph figure posterior over hypotheses and the corresponding predictive distribution after seeing one example d a dot means this number is consistent with this hypothesis. the graph phd on the right is the weight given to hypothesis h. by taking a weighed sum of dots we get p x cd based on figure of figure generated by numbersgame. posterior predictive distribution the posterior is our internal belief state about the world. the way to test if our beliefs are justified is to use them to predict objectively observable quantities is the basis of the scientific method. specifically the posterior predictive distribution in this context is given by p x cd py x hphd h this is just a weighted average of the predictions of each individual hypothesis and is called bayes model averaging et al. this is illustrated in figure the dots at the bottom show the predictions from each hypothesis the vertical curve on the right shows the weight associated with each hypothesis. if we multiply each row by its weight and add up we get the distribution at the top. when we have a small andor ambiguous dataset the posterior phd is vague which induces a broad predictive distribution. however once we have figured things out the posterior becomes a delta function centered at the map estimate. in this case the predictive distribution chapter generative models for discrete data becomes p x cd h p xh hh p x h this is called a plug-in approximation to the predictive density and is very widely used due to its simplicity. however in general this under-represents our uncertainty and our predictions will not be as smooth as when using bma. we will see more examples of this later in the book. although map learning is simple it cannot explain the gradual shift from similarity-based for reasoning uncertain posteriors to rule-based reasoning certain posteriors. example suppose we observe d if we use the simple prior above the minimal consistent hypothesis is all powers of so only and get a non-zero probability of being predicted. this is of course an example of overfitting. given d the map hypothesis is all powers of two thus the plug-in predictive distribution gets broader stays the same as we see more data it starts narrow but is forced to broaden as it seems more data. in contrast in the bayesian approach we start broad and then narrow down as we learn more in particular given d there are many hypotheses which makes more intuitive sense. with non-negligible posterior support so the predictive distribution is broad. however when we see d the posterior concentrates its mass on one hypothesis so the predictive distribution becomes narrower. so the predictions made by a plug-in approach and a bayesian approach are quite different in the small sample regime although they converge to the same answer as we see more data. a more complex prior to model human behavior tenenbaum used a slightly more sophisticated prior which was derived by analysing some experimental data of how people measure similarity between numbers see for details. the result is a set of arithmetical concepts similar to those mentioned above plus all intervals between n and m for n m that these hypotheses are not mutually exclusive. thus the prior is a mixture of two priors one over arithmetical rules and one over intervals intervalh rulesh ph the only free parameter in the model is the relative weight given to these two parts of the prior. the results are not very sensitive to this value so long as reflecting the fact that people are more likely to think of concepts defined by rules. the predictive distribution of the model using this larger hypothesis space is shown in figure it is strikingly similar to the human predictive distribution shown in figure even though it was not fit to human data the choice of hypothesis space. the beta-binomial model the number game involved inferring a distribution over a discrete variable drawn from a finite hypothesis space h h given a series of discrete observations. this made the computations particularly simple we just needed to sum multiply and divide. however in many applications k where the unknown parameters are continuous so the hypothesis space is subset of r the beta-binomial model examples figure predictive distributions for the model using the full hypothesis space. compare to figure the predictions of the bayesian model are only plotted for those values of x for which human data is available this is why the top line looks sparser than figure source figure of used with kind permission of josh tenenbaum. k is the number of parameters. this complicates the mathematics since we have to replace sums with integrals. however the basic ideas are the same. we will illustrate this by considering the problem of inferring the probability that a coin shows up heads given a series of observed coin tosses. although this might seem trivial it turns out that this model forms the basis of many of the methods we will consider later in this book including naive bayes classifiers markov models etc. it is historically important since it was the example which was analyzed in bayes original paper of analysis was subsequently generalized by pierre-simon laplace creating what we now call bayes rule see for further historical details. we will follow our now-familiar recipe of specifying the likelihood and prior and deriving the posterior and posterior predictive. likelihood suppose xi ber where xi represents heads xi represents tails and is the rate parameter of heads. if the data are iid the likelihood has the form pd chapter generative models for discrete data ixi heads and where we haven ixi tails. these two counts are called the sufficient statistics of the data since this is all we need to know about d to infer alternative set of sufficient statistics are and n more formally we say sd is a sufficient statistic for data d if p if we use a uniform prior this is equivalent to saying pd psd consequently if we have two datasets with the same sufficient statistics we will infer the same value for now suppose the data consists of the count of the number of heads observed in a fixed number n of trials. in this case we have binn where bin represents the binomial distribution which has the following pmf binkn n k n k k is a constant independent of the likelihood for the binomial sampling model is the since same as the likelihood for the bernoulli model. so any inferences we make about will be the same whether we observe the counts d n or a sequence of trials d xn. prior we need a prior which has support over the interval to make the math easier it would convenient if the prior had the same form as the likelihood i.e. if the prior looked like p for some prior parameters and posterior by simply adding up the exponents if this were the case then we could easily evaluate the p pd when the prior and the posterior have the same form we say that the prior is a conjugate prior for the corresponding likelihood. conjugate priors are widely used because they simplify computation and are easy to interpret as we see below. in the case of the bernoulli the conjugate prior is the beta distribution which we encountered in section beta b a the parameters of the prior are called hyper-parameters. we can set them in order to encode our prior beliefs. for example to encode our beliefs that has mean and standard deviation we set a and b or to encode our beliefs that has mean and that we think it lives in the interval with probability then we find a and b if we know nothing about except that it lies in the interval we can use a uniform prior which is a kind of uninformative prior section for details. the uniform distribution can be represented by a beta distribution with a b the beta-binomial model prior lik post prior lik post figure updating a prior with a binomial likelihood with sufficient statistics to yield a posterior. likelihood with sufficient statistics to yield a posterior. figure generated by binomialbetaposteriordemo. updating a prior with a binomial posterior if we multiply the likelihood by the beta prior we get the following posterior equation p bbeta a b in particular the posterior is obtained by adding the prior hyper-parameters to the empirical counts. for this reason the hyper-parameters are known as pseudo counts. the strength of the prior also known as the effective sample size of the prior is the sum of the pseudo counts a b this plays a role analogous to the data set size n figure gives an example where we update a weak prior with a peaked likelihood function corresponding to a large sample size we see that the posterior is essentially identical to the likelihood since the data has overwhelmed the prior. figure gives an example where we update a strong prior with a peaked likelihood function now we see that the posterior is a compromise between the prior and likelihood. to see this suppose we have two data sets da and db with sufficient statistics n a n b n b datasets. in batch mode we have note that updating the posterior sequentially is equivalent to updating in a single batch. and be the sufficient statistics of the combined and n a let n a n b n b n a p b beta a b in sequential mode we have p pdb binn b n b beta n a n b n b a a n a n b a n a b b this makes bayesian inference particularly well-suited to online learning as we will see later. chapter generative models for discrete data posterior mean and mode from equation the map estimate is given by m ap a a b n if we use a uniform prior then the map estimate reduces to the mle which is just the empirical fraction of heads n m le this makes intuitive sense but it can also be derived by applying elementary calculus to maximize the likelihood function in equation by contrast the posterior mean is given by a a b n this difference between the mode and the mean will prove important later. we will now show that the posterior mean is convex combination of the prior mean and the mle which captures the notion that the posterior is a compromise between what we previously believed and what the data is telling us. let a b be the equivalent sample size of the prior which controls its strength and let the prior mean be a then the posterior mean is given by n n e where n is the ratio of the prior to posterior equivalent sample size. so the weaker the prior the smaller is and hence the closer the posterior mean is to the mle. one can show similarly that the posterior mode is a convex combination of the prior mode and the mle and that it too converges to the mle. m le n n n posterior variance the mean and mode are point estimates but it is useful to know how much we can trust them. the variance of the posterior is one way to measure this. the variance of the beta posterior is given by var b b we can simplify this formidable expression in the case that n a b to get var n n n n var where is the mle. hence the error bar in our estimate the posterior standard deviation is given by n the beta-binomial model we see that the uncertainty goes down at a rate of n note however that the uncertainty is maximized when and is minimized when is close to or this means it is easier to be sure that a coin is biased than to be sure that it is fair. posterior predictive distribution so far we have been focusing on inference of the unknown parameters. let us now turn our attention to prediction of future observable data. consider predicting the probability of heads in a single future trial under a betaa b poste rior. we have p x px beta bd e a a b thus we see that the mean of the posterior predictive distribution is equivalent this case to plugging in the posterior mean parameters p xd ber xe overfitting and the black swan paradox suppose instead that we plug-in the mle i.e. we use p xd ber x m le. unfortunately this approximation can perform quite poorly when the sample size is small. for example suppose we have seen n tails in a row. the mle is since this makes the observed data as probable as possible. however using this estimate we predict that heads are impossible. this is called the zero count problem or the sparse data problem and frequently occurs when estimating counts from small amounts of data. one might think that in the era of big data such concerns are irrelevant but note that once we partition the data based on certain criteria such as the number of times a specific person has engaged in a specific activity the sample sizes can become much smaller. this problem arises for example when trying to perform personalized recommendation of web pages. thus bayesian methods are still useful even in the big data regime the zero-count problem is analogous to a problem in philosophy called the black swan paradox. this is based on the ancient western conception that all swans were white. in that context a black swan was a metaphor for something that could not exist. swans were discovered in australia by european explorers in the century. the term black swan paradox was first coined by the famous philosopher of science karl popper the term has also been used as the title of a recent popular book this paradox was used to illustrate the problem of induction which is the problem of how to draw general conclusions about the future from specific observations from the past. let us now derive a simple bayesian solution to the problem. we will use a uniform prior so a b in this case plugging in the posterior mean gives laplace s rule of succession p x this justifies the common practice of adding to the empirical counts normalizing and then plugging them in a technique known as add-one smoothing. that plugging in the map chapter generative models for discrete data parameters would not have this smoothing effect since the mode has the form n which becomes the mle if a b predicting the outcome of multiple future trials suppose now we were interested in predicting the number of heads x in m future trials. this is given by m x pxd m binx m bd x a we recognize the integral as the normalization constant for a betaax m xb distribution. ba b x a bx a m x b thus we find that the posterior predictive is given by the following known as the beta-binomial distribution bbxa b m m x bx a m x b ba b this distribution has the following mean and variance a m ab b m e var a b if m and hence x we see that the mean becomes e px a ab which is consistent with equation a b this process is illustrated in figure we start with a prior and plot the posterior predictive density after seeing heads and tails. figure plots a plug-in approximation using a map estimate. we see that the bayesian prediction has longer tails spreading its probablity mass more widely and is therefore less prone to overfitting and blackswan type paradoxes. the dirichlet-multinomial model in the previous section we discussed how to infer the probability that a coin comes up heads. in this section we generalize these results to infer the probability that a dice with k sides comes up as face k. this might seem like another toy exercise but the methods we will study are widely used to analyse text data biosequence data etc. as we will see later. the dirichlet-multinomial model posterior predictive plugin predictive figure figure generated by betabinompostpreddemo. posterior predictive distributions after seeing plugin approximation. likelihood suppose we observe n dice rolls d xn where xi k. the data is iid the likelihood has the form if we assume pd nk k iyi k is the number of times event k occured are the sufficient where nk statistics for this model. the likelihood for the multinomial model has the same form up to an irrelevant constant factor. prior since the parameter vector lives in the k-dimensional probability simplex we need a prior that has support over this simplex. ideally it would also be conjugate. fortunately the dirichlet distribution satisfies both criteria. so we will use the following prior dir b posterior k k ix sk multiplying the likelihood by the prior we find that the posterior is also dirichlet p pd k k nk knk dir k nk k k chapter generative models for discrete data we see that the posterior is obtained by adding the prior hyper-parameters k to the empirical counts nk. we can derive the mode of this posterior the map estimate by using calculus. however k k we can do this by using a lagrange we must enforce the constraint that multiplier. the constrained objective function or lagrangian is given by the log likelihood plus log prior plus the constraint nk log k k log k k k k k k nk k taking derivatives with respect to yields to simplify notation we define the original constraint k k taking derivatives with respect to k yields k k k k k k n k k where given by k nk k n k k k we can solve for using the sum-to-one constraint k is the equivalent sample size of the prior. thus the map estimate is which is consistent with equation if we use a uniform prior k we recover the mle k nkn this is just the empirical fraction of times face k shows up. we do not need to explicitly enforce the constraint that k since the gradient of the objective has the form nk k so negative values would reduce the objective rather than maximize it. course this does not preclude setting k and indeed this is the optimal solution if nk and k the dirichlet-multinomial model posterior predictive the posterior predictive distribution for a single multinoulli trial expression is given by the following px jd px j px j j jp jdd j e jd p j jdd j d j j nj k k nk j nj n where j are all the components of except j. see also exercise the above expression avoids the zero-count problem just as we saw in section in fact this form of bayesian smoothing is even more important in the multinomial case than the binary case since the likelihood of data sparsity increases once we start partitioning the data into many categories. worked example language models using bag of words one application of bayesian smoothing using the dirichlet-multinomial model is to language modeling which means predicting which words might occur next in a sequence. here we will take a very simple-minded approach and assume that the i th word xi k is sampled independently from all the other words using a cat distribution. this is called the bag of words model. given a past sequence of words how can we predict which one is likely to come next? for example suppose we observe the following sequence of a children s nursery rhyme mary had a little lamb little lamb little lamb mary had a little lamb its fleece as white as snow furthermore suppose our vocabulary consists of the following words mary lamb little big fleece white black snow rain unk here unk stands for unknown and represents all other words that do not appear elsewhere on the list. to encode each line of the nursery rhyme we first strip off punctuation and remove any stop words such as a as the etc. we can also perform stemming which means reducing words to their base form such as stripping off the final s in plural words or the ing from verbs running becomes run. in this example no words need stemming. finally we replace each word by its index into the vocabulary to get we now ignore the word order and count how often each word occurred resulting in a histogram of word counts chapter generative models for discrete data token word mary count lamb little big fleece white black snow rain unk just denote the above counts by nj. p x jd e jd if we use a dir prior for the posterior predictive is j nj nj if we set j we get p x jd the modes of the predictive distribution are x lamb and x unk note that the words big black and rain are predicted to occur with non-zero probability in the future even though they have never been seen before. later on we will see more sophisticated language models. naive bayes classifiers in this section we discuss how to classify vectors of discrete-valued features x kd where k is the number of values for each feature and d is the number of features. we will use a generative approach. this requires us to specify the class conditional distribution pxy c. the simplest approach is to assume the features are conditionally independent given the class label. this allows us to write the class conditional density as a product of one dimensional densities pxy c pxjy c jc the resulting model is called a naive bayes classifier the model is called naive since we do not expect the features to be independent even conditional on the class label. however even if the naive bayes assumption is not true it often results in classifiers that work well and pazzani one reason for this is that the model is quite simple only has ocd parameters for c classes and d features and hence it is relatively immune to overfitting. the form of the class-conditional density depends on the type of each feature. we give some possibilities below jc where jc is the mean of feature j in objects of class c and in the case of real-valued features we can use the gaussian distribution pxy c n jc jc is its variance. in the case of binary features xj we can use the bernoulli distribution pxy berxj jc where jc is the probability that feature j occurs in class c. c this is sometimes called the multivariate bernoulli naive bayes model. we will see an application of this below. naive bayes classifiers in the case of categorical features xj k we can model use the multinoulli distribution pxy c catxj jc where jc is a histogram over the k possible values for xj in class c. obviously we can handle other kinds of features or use different distributional assumptions. also it is easy to mix and match features of different types. model fitting we now discuss how to train a naive bayes classifier. this usually means computing the mle or the map estimate for the parameters. however we will also discuss how to compute the full posterior p mle for nbc the probability for a single data case is given by pxi yi pyi pxij j iyic c pxij jciyic j c j c hence the log-likelihood is given by log pd nc log c log pxij jc iyic we see that this expression decomposes into a series of terms one concerning and dc terms containing the jc s. hence we can optimize all these parameters separately. from equation the mle for the class prior is given by c nc n i iyi c is the number of examples in class c. where nc the mle for the likelihood depends on the type of distribution we choose to use for each feature. for simplicity let us suppose all features are binary so xjy c ber jc. in this case the mle becomes jc njc nc it is extremely simple to implement this model fitting procedure see algorithm for some pseudo-code naivebayesfit for some matlab code. this algorithm obviously takes on d time. the method is easily generalized to handle features of mixed type. this simplicity is one reason the method is so widely used. figure gives an example where we have classes and binary features representing the presence or absence of words in a bag-of-words model. the plot visualizes the c vectors for the two classes. the big spike at index corresponds to the word subject which occurs in both classes with probability section we discuss how to filter out such uninformative features. chapter generative models for discrete data algorithm fitting a naive bayes classifier to binary features nc njc for i do c yi class label of i th example nc nc for j do if xij then njc njc c nc n jc njc n figure class conditional densities pxj c for two document classes corresponding to x windows and ms windows figure generated by naivebayesbowdemo. bayesian naive bayes the trouble with maximum likelihood is that it can overfit. for example consider the example in figure the feature corresponding to the word subject it feature j always occurs in both classes so we estimate jc what will happen if we encounter a new email which does not have this word in it? our algorithm will crash and burn since we will find that py cx for both classes! this is another manifestation of the black swan paradox discussed in section a simple solution to overfitting is to be bayesian. for simplicity we will use a factored prior p p jc we will use a dir prior for and a beta prior for each jc. often we just take and corresponding to add-one or laplace smoothing. naive bayes classifiers combining the factored likelihood in equation with the factored prior above gives the following factored posterior p p nc c p jcd betanc njc njc p jcd c in other words to compute the posterior we just update the prior counts with the empirical counts from the likelihood. it is straightforward to modify algorithm to handle this version of model fitting using the model for prediction at test time the goal is to compute py cxd py cd pxjy cd the correct bayesian procedure is to integrate out the unknown parameters py cxd caty c berxjy c jcp jcd in particular from equafortunately this is easy to do at least if the posterior is dirichlet. tion we know the posterior predictive density can be obtained by simply plugging in the posterior mean parameters hence py cxd c jcixj jcixj njc nc nc c n jk c c c. where if we have approximated the posterior by a single point p where may be the ml or map estimate then the posterior predictive density is obtained by simply plugging in the parameters to yield a virtually identical rule py cxd c jcixj jcixj chapter generative models for discrete data the only difference is we replaced the posterior mean with the posterior mode or mle however this small difference can be important in practice since the posterior mean will result in less overfitting section the log-sum-exp trick we now discuss one important practical detail that arises when using generative classifiers of any kind. we can compute the posterior over class labels using equation using the appropriate class-conditional density a plug-in approximation. unfortunately a naive implementation of equation can fail due to numerical underflow. the problem is that pxy c is often a very small number especially if x is a high-dimensional vector. this is because we require x pxy so the probability of observing any particular high-dimensional vector is that small. the obvious solution is to take logs when applying bayes rule as follows log py cx c log bc log pxy c log py c however this requires evaluating the following expression log py x log px log and we can t add up in the log domain. fortunately we can factor out the largest term and just represent the remaining numbers relative to that. for example loge e log e e e c ebc log ebc beb log in general we have log c ebc b b c where b maxc bc. this is called the log-sum-exp trick and is widely used. the function logsumexp for an implementation. this trick is used in algorithm which gives pseudo-code for using an nbc to compute pyixi see naivebayespredict for the matlab code. note that we do not need the log-sum-exp trick if we only want to compute yi since we can just maximize the unnormalized quantity log pyi c log pxiy c. feature selection using mutual information since an nbc is fitting a joint distribution over potentially many features it can suffer from overfitting. in addition the run-time cost is od which may be too high for some applications. one common approach to tackling both of these problems is to perform feature selection to remove irrelevant features that do not help much with the classification problem. the simplest approach to feature selection is to evaluate the relevance of each feature separately and then naive bayes classifiers algorithm predicting with a naive bayes classifier for binary features for i do for c do lic log c for j do pic explic logsumexpli yi argmaxc pic if xij then lic lic log jc else lic lic jc take the top k where k is chosen based on some tradeoff between accuracy and complexity. this approach is known as variable ranking filtering orscreening. one way to measure relevance is to use mutual information between feature xj and the class label y xj y ix y pxj y log pxj y pxjpy the mutual information can be thought of as the reduction in entropy on the label distribution once we observe the value of feature j. if the features are binary it is easy to show that the mi can be computed as follows c ij jc c log jc j jc c log jc j where c py c jc pxj c and j pxj quantities can be computed as a by-product of fitting a naive bayes classifier. c c jc. figure illustrates what happens if we apply this to the binary bag of words dataset used in figure we see that the words with highest mutual information are much more discriminative than the words which are most probable. for example the most probable word in both classes is subject which always occurs because this is newsgroup data which always has a subject line. but obviously this is not very discriminative. the words with highest mi with the class label are decreasing order windows microsoft dos and motif which makes sense since the classes correspond to microsoft windows and x windows. classifying documents using bag of words of these document classification is the problem of classifying text documents into different categories. one simple approach is to represent each document as a binary vector which records whether each word is present or not so xij iff word j occurs in document i otherwise xij we can then use the following class conditional density pxiyi c berxij jc ixij jc xij chapter generative models for discrete data class subject this with but you prob class subject windows this with but prob highest mi windows microsoft dos motif window mi table we list the most likely words for class windows and class windows. we also show the words with highest mutual information with class label. produced by naivebayesbowdemo this is called the bernoulli product model or thebinary independence model. however ignoring the number of times each word occurs in a document loses some information and nigam a more accurate representation counts the number let xi be a vector of counts for document i so of occurrences of each word. specifically xij ni where ni is the number of terms in document i xij ni. for the class conditional densities we can use a multinomial distribution xij jc pxiyi c muxini c xij! where we have implicitly assumed that the document length ni is independent of the class. here jc is the probability of generating word j in documents of class c these parameters satisfy the constraint that jc for each class although the multinomial classifier is easy to train and easy to use at test time it does not work particularly well for document classification. one reason for this is that it does not take into account the burstiness of word usage. this refers to the phenomenon that most words never appear in any given document but if they do appear once they are likely to appear more than once i.e. words occur in bursts. the multinomial model cannot capture the burstiness phenomenon. to see why note that jc and since jc for rare words it becomes increasingly equation has the form nij unlikely to generate many of them. for more frequent words the decay rate is not as fast. to see why intuitively note that the most frequent words are function words which are not specific to the class such as and the and but the chance of the word and occuring is pretty much the same no matter how many time it has previously occurred document length so the independence assumption is more reasonable for common words. however since rare words are the ones that matter most for classification purposes these are the ones we want to model the most carefully. various ad hoc heuristics have been proposed to improve the performance of the multinomial document classifier et al. we now present an alternative class conditional density that performs as well as these ad hoc methods yet is probabilistically sound et al. since equation models each word independently this model is often called a naive bayes classifier although technically the features xij are not independent because of the constraint j xij ni. naive bayes classifiers suppose we simply replace the multinomial class conditional density with the dirichlet compound multinomial or dcm density defined as follows muxini cdir c cd c pxiyi c xij! bxi c b c equation is derived in equation surprisingly this simple change is all that is needed to capture the burstiness phenomenon. the intuitive reason for this is as follows after seeing one occurence of a word say word j the posterior counts on j gets updated making another occurence of word j more likely. by contrast if j is fixed then the occurences of each word are independent. the multinomial model corresponds to drawing a ball from an urn with k colors of ball recording its color and then replacing it. by contrast the dcm model corresponds to drawing a ball recording its color and then replacing it with one additional copy this is called the polya urn. using the dcm as the class conditional density gives much better results than using the multinomial and has performance comparable to state of the art methods as described in et al. the only disadvantage is that fitting the dcm model is more complex see elkan for the details. exercises exercise mle for the bernoulli binomial model derive equation by optimizing the log of the likelihood in equation exercise marginal likelihood for the beta-bernoulli model in equation we showed that the marginal likelihood is the ratio of the normalizing constants pd z z n we will now derive an alternative derivation of this fact. by the chain rule of probability in section we showed that the posterior predictive distribution is px nk k i ni i nk k n where k and is the data seen so far. now suppose d h t t h h or d then pd n show how this reduces to equation by using the fact that for integers chapter generative models for discrete data exercise posterior predictive for beta-binomial model recall from equation that the posterior predictive for the beta-binomial is given by pxn d bx n n x b n x prove that this reduces to p x when n hence x i.e. show that hint use the fact that exercise beta updating from censored likelihood gelman. suppose we toss a coin n times. let x be the number of heads. we observe that there are fewer than heads but we don t know exactly how many. let the prior probability of heads be p beta compute the posterior p up to normalization constants i.e. derive an expression proportional to p x hint the answer is a mixture distribution. exercise uninformative prior for log-odds ratio let logit log show that if p then p beta hint use the change of variables formula. exercise mle for the poisson distribution the poisson pmf is defined as poix x parameter. derive the mle. x! for x where is the rate exercise bayesian analysis of the poisson distribution in exercise we defined the poisson distribution with rate and derived its mle. here we perform a conjugate bayesian analysis. a. derive the posterior p assuming a conjugate prior p ga b a b. hint the b. what does the posterior mean tend to as a and b that the mean of a gaa b posterior is also a gamma distribution. distribution is ab. exercise mle for the uniform distribution kaelbling. consider a uniform distribution centered on with width the density function is given by px ix a a naive bayes classifiers a. given a data set xn what is the maximum likelihood estimate of a it a? b. what probability would the model assign to a new data point using a? c. do you see any problem with the above approach? briefly suggest words a better approach. exercise bayesian analysis of the uniform distribution consider the uniform distribution the maximum likelihood estimate is maxd as we saw in exercise but this is unsuitable for predicting future data since it puts zero probability mass outside the training data. in this exercise we will perform a bayesian analysis of the uniform distribution the conjugate prior is the pareto distribution p pareto k defined in section given a pareto prior the joint distribution of and d xn is pd kbk i maxd n let m maxd. the evidence probability that all n samples came from the same uniform distribution is pd kbk d if m b if m b m n k kbk derive the posterior p and show that if can be expressed as a pareto distribution. exercise taxicab problem suppose you arrive in a new city and see a taxi numbered how many taxis are there in this city? let us assume taxis are numbered sequentially as integers starting from up to some unknown upper bound number taxis from for simplicity we can also count from without changing the analysis. hence the likelihood function is px the uniform distribution. the goal is to estimate we will use the bayesian analysis from exercise a. suppose we see one taxi numbered so d m n using an non-informative prior on of the form p p a what is the posterior p b. compute the posterior mean mode and median number of taxis in the city if such quantities exist. c. rather than trying to compute a point estimate of the number of taxis we can compute the predictive density over the next taxicab number using where k are the hyper-parameters n k are the updated hyper-parameters. now consider the case d and using equation write down an expression for pxd as above use a non-informative prior b k d. use the predictive density formula to compute the probability that the next taxi you will see the next day has number or i.e. compute px px px e. briefly describe sentences some ways we might make the model more accurate at prediction. chapter generative models for discrete data exercise bayesian analysis of the exponential distribution a lifetime x of a machine is modeled by an exponential distribution with unknown parameter the likelihood is px e x for x a. show that the mle is where x n b. suppose we observe lifetimes years of different iid machines. xi. what is the mle given this data? c. assume that an expert believes should have a prior distribution that is also exponential p expon choose the prior parameter call it such that e hint recall that the gamma distribution has the form ga b a b and its mean is ab. d. what is the posterior p e. f. what is the posterior mean e is the exponential prior conjugate to the exponential likelihood? g. explain why the mle and posterior mean differ. which is more reasonable in this example? exercise map estimation for the bernoulli with non-conjugate priors prior p beta we know that with this prior the map estimate is given by in the book we discussed bayesian inference of a bernoulli rate parameter with the jaakkola. n p where is the number of heads is the number of tails and n is the total number of trials. a. now consider the following prior that believes the coin is fair or is slightly biased towards tails if if otherwise derive the map estimate under this prior as a function of and n b. suppose the true parameter is which prior leads to a better estimate when n is small? which prior leads to a better estimate when n is large? exercise posterior predictive distribution for a batch of data with the dirichlet-multinomial model in equation we gave the the posterior predictive distribution for a single multinomial trial using a dirichlet prior. now consider predicting a batch of new data d xm consisting of m single multinomial trials of predicting the next m words in a sentence assuming they are drawn iid. derive an expression for p dd naive bayes classifiers your answer should be a function of and the old and new counts statistics defined as n old k n new k ixi k ixi k i d i d hint recall that for a vector of counts the marginal likelihood is given by pd k where k k and n k k k nk. exercise posterior predictive for dirichlet-multinomial koller.. a. suppose we compute the empirical distribution over letters of the roman alphabet plus the space character distribution over values from samples. suppose we see the letter e times. what is ed if we assume dir where k for all k? in the samples we saw e times a times and p times. what is p ad if we assume dir where k for all k? show your work. b. suppose exercise setting the beta hyper-parameters suppose and we believe that e m and var v. using equation solve for and in terms of m and v. what values do you get if m and v exercise setting the beta hyper-parameters ii draper. suppose and we believe that e and u write a program that can solve for and in terms of m and u. hint write as a function of and m so the pdf only has one unknown then write down the probability mass contained in the interval as an integral and minimize its squared discrepancy from what values do you get if m and u what is the equivalent sample size of this prior? exercise marginal likelihood for beta-binomial under uniform prior suppose we toss a coin n times and observe heads. let binn and show that the marginal likelihood is hint x! if x is an integer. exercise bayes factor for coin tossing suppose we toss a coin n times and observe heads. let the null hypothesis be that the coin is fair and the alternative be that the coin can have any bias so p derive the bayes factor in favor of the biased coin hypothesis. what if n and hint see exercise exercise irrelevant features with naive bayes jaakkola. let xiw if word w occurs in document i and xiw otherwise. let cw be the estimated probability that word w occurs in documents of class c. then the log-likelihood that document chapter generative models for discrete data x belongs to class c is log pxic log cw xiw xiw xiw log cw xiw cw xiw log cw cw cw w where w is the number of words in the vocabulary. we can write this more succintly as log pxic c where xi xiw is a bit vector and c cw cw cwt w we see that this is a linear classifier since the class-conditional density is a linear function inner product of the parameters c. a. assuming pc write down an expression for the log posterior odds ratio in terms of the features and the parameters and b. intuitively words that occur in both classes are not very discriminative and therefore should not affect our beliefs about the class label. consider a particular word w. state the conditions on and equivalently the conditions on under which the presence or absence of w in a test document will have no effect on the class posterior a word will be ignored by the classifier. hint using your previous result figure out when the posterior odds ratio is c. the posterior mean estimate of using a prior is given by cw i c xiw c where the sum is over the nc documents in class c. consider a particular word w and suppose it always occurs in every document of class. let there be documents of class and be the number of documents in class where e.g. we get much more non-spam than if we use the above estimate for cw will word w be spam this is an example of class imbalance. ignored by our classifier? explain why or why not. d. what other ways can you think of which encourage irrelevant words to be ignored? exercise class conditional densities for binary data consider a generative classifier for c classes with class conditional density pxy and uniform class prior py. suppose all the d features are binary xj if we assume all the features are conditionally independent naive bayes assumption we can write pxy c berxj jc this requires dc parameters. naive bayes classifiers a. now consider a different model which we will call the full model in which all the features are fully dependent we make no factorization assumptions. how might we represent pxy c in this case? how many parameters are needed to represent pxy c? b. assume the number of features d is fixed. let there be n training cases. if the sample size n is very small which model bayes or full is likely to give lower test set error and why? if the sample size n is very large which model bayes or full is likely to give lower test set error and why? c. d. what is the computational complexity of fitting the full and naive bayes models as a function of n the model here means computing the mle or map parameter and d? use big-oh notation. estimates. you may assume you can convert a d-bit vector to an array index in od time. e. what is the computational complexity of applying the full and naive bayes models at test time to a single test case? f. suppose the test case has missing data. let xv be the visible features of size v and xh be the hidden features of size h where v h d. what is the computational complexity of computing pyxv for the full and naive bayes models as a function of v and h? exercise mutual information for naive bayes classifiers with binary features derive equation exercise fitting a naive bayes spam filter by hand daphne koller.. consider a naive bayes model bernoulli version for spam classification with the vocabulary vsecret we have the following example spam messages dollar offer offer today is secret and normal messages price for valued customer secret sports today is healthy price pizza. give the mles for the following parameters spam secretspam secretnon-spam sportsnon-spam dollarspam. gaussian models introduction in this chapter we discuss the multivariate gaussian or multivariate normal which is the most widely used joint probability density function for continuous variables. it will form the basis for many of the models we will encounter in later chapters. unfortunately the level of mathematics in this chapter is higher than in many other chapters. in particular we rely heavily on linear algebra and matrix calculus. this is the price one must pay in order to deal with high-dimensional data. beginners may choose to skip sections marked with a in addition since there are so many equations in this chapter we have put a box around those that are particularly important. notation let us briefly say a few words about notation. we denote vectors by boldface lower case letters such as x. we denote matrices by boldface upper case letters such as x. we denote entries in a matrix by non-bold upper case letters such as xij. all vectors are assumed to be column vectors unless noted otherwise. we use xd to denote a column vector created by stacking d scalars. similarly if we write x xd where the left hand side is a tall column vector we mean to stack the xi along the rows this is usually written as x dt but that is rather ugly. if we write x xd where the left hand side is a matrix we mean to stack the xi along the columns creating a matrix. xt basics recall from section that the pdf for an mvn in d dimensions is defined by the following n exp chapter gaussian models x figure visualization of a dimensional gaussian density. the major and minor axes of the ellipse are defined by the first two eigenvectors of the covariance matrix namely and based on figure of the expression inside the exponent is the mahalanobis distance between a data vector x and the mean vector we can gain a better understanding of this quantity by performing an eigendecomposition of that is we write u ut whereu is an orthonormal matrix of eigenvectors satsifying ut u i and is a diagonal matrix of eigenvalues. using the eigendecomposition we have that u t u i uiut i where ui is the i th column of u containing the i th eigenvector. hence we can rewrite the mahalanobis distance as follows i uiut i i uiut i i i where yi ut i recall that the equation for an ellipse in is hence we see that the contours of equal probability density of a gaussian lie along ellipses. this is illustrated in figure the eigenvectors determine the orientation of the ellipse and the eigenvalues determine how elogonated it is. in general we see that the mahalanobis distance corresponds to euclidean distance in a transformed coordinate system where we shift by and rotate by u. introduction mle for an mvn we now describe one way to estimate the parameters of an mvn using mle. in later sections we will discuss bayesian inference for the parameters which can mitigate overfitting and can provide a measure of confidence in our estimates. theorem for a gaussian. if we have n iid samples xi n then the mle for the parameters is given by mle mle n n xi x xxi xt n i x xt xixt that is the mle is just the empirical mean and empirical covariance. in the univariate case we get the following familiar results xi x i n i i i n n proof to prove this result we will need several results from matrix algebra which we summarize in the equations a and b are vectors and a and b are matrices. also the notation below. tra refers to the trace of a matrix which is the sum of its diagonals tra i aii. a a aa a b at a a trba t log a t trabc trcab trbca the last equation is called the cyclic permutation property of the trace operator. using this we can derive the widely used trace trick which reorders the scalar inner product xt ax as follows xt ax trxt ax trxxt a traxxt chapter gaussian models proof. we can now begin with the proof. the log-likelihood is log pd n log where is the precision matrix. using the substitution yi xi and the chain rule of calculus we have yi i yt yi t so the mle of is just the empirical mean. now we can use the trace-trick to rewrite the log-likelihood for as follows trxi hence n xi x i tr n n log log where s is the scatter matrix centered on taking derivatives of this expression with respect to yields n t t st n s so n if we plug-in the mle x which is just the empirical covariance matrix centered on both parameters must be simultaneously optimized we get the standard equation for the mle of a covariance matrix. gaussian discriminant analysis maximum entropy derivation of the gaussian in this section we show that the multivariate gaussian is the distribution with maximum entropy subject to having a specified mean and covariance also section this is one reason the gaussian is so widely used the first two moments are usually all that we can reliably estimate from data so we want a distribution that captures these properties but otherwise makes as few addtional assumptions as possible. to simplify notation we will assume the mean is zero. the pdf has the form z exp xt px for i j d we see that this is in if we define fijx ixj and ij the same form as equation the entropy of this distribution log base e is given by hn ln ed we now show the mvn has maximum entropy amongst all distributions with a specified covariance qxxixj ij. let p n then theorem let qx be any density satisfying hq hp. proof. and thomas we have kl hq hq hq p qx log qx px dx qx log pxdx px log pxdx where the key step in equation with a follows since q and p yield the same moments for the quadratic form encoded by log px. gaussian discriminant analysis one important application of mvns is to define the the class conditional densities in a generative classifier i.e. pxy c n c c the resulting technique is called discriminant analysis or gda though it is a generative not discriminative classifier see section for more on this distinction. if c is diagonal this is equivalent to naive bayes. chapter gaussian models t i h g e w red female bluemale height t i h g e w red female bluemale height figure heightweight data. visualization of gaussians fit to each class. of the probability mass is inside the ellipse. figure generated by gaussheightweight. we can classify a feature vector using the following decision rule derived from equation yx argmax c py c log px c when we compute the probability of x under each class conditional density we are measuring the distance from x to the center of each class c using mahalanobis distance. this can be thought of as a nearest centroids classifier. as an example figure shows two gaussian class-conditional densities in representing the height and weight of men and women. we can see that the features are correlated as is to be expected people tend to weigh more. the ellipses for each class contain of the probability mass. if we have a uniform prior over classes we can classify a new test vector as follows yx argmin c ct c c quadratic discriminant analysis the posterior over class labels is given by equation we can gain further insight into this model by plugging in the definition of the gaussian density as follows py cx c exp exp ct c c thresholding this results in a quadratic function of x. the result is known as quadratic discriminant analysis figure gives some examples of what the decision boundaries look like in gaussian discriminant analysis parabolic boundary some linear some quadratic figure quadratic decision boundaries in for the and class case. discrimanalysisdboundariesdemo. figure generated by softmax distribution s where at different temperatures t when the figure temperature is high the distribution is uniform whereas when the temperature is low the distribution is spiky with all its mass on the largest element. figure generated by linear discriminant analysis py cx c exp we now consider a special case in which the covariance matrices are tied or shared across classes c in this case we can simplify equation as follows c c t exp since the quadratic term xt is independent of c it will cancel out in the numerator and denominator. if we define xt xt c t c c log c t c t exp c c c log c t c c chapter gaussian models c x c e t e t x s cx c and s is the softmax function defined as follows then we can write py cx where t s x t e e the softmax function is so-called since it acts a bit like the max function. to see this let us divide each c by a constant t called the temperature. then as t we find s if c otherwise in other words at low temperatures the distribution spends essentially all of its time in the most probable state whereas at high temperatures it visits all states uniformly. see figure for an illustration. note that this terminology comes from the area of statistical physics where it is common to use the boltzmann distribution which has the same form as the softmax function. py cx an interesting property of equation is that if we take logs we end up with a linear reason it is linear is because the xt cancels from the numerator function of x. and denominator. thus the decision boundary between any two classes say c and will be a straight line. hence this technique is called linear discriminant analysis or lda. we can derive the form of this line as follows y x xt c see figure for some examples. an alternative to fitting an lda model and then deriving the class posterior is to directly fit pyx w catywx for some c d weight matrix w. this is called multi-class logistic regression or multinomial logistic we will discuss this model in detail in section the difference between the two approaches is explained in section t c x c t two-class lda to gain further insight into the meaning of these equations let us consider the binary case. in this case the posterior is given by e t py x x e t e t x x e x sigm the abbreviation lda could either stand for linear discriminant analysis or latent dirichlet allocation we hope the meaning is clear from text. in the language modeling community this model is called a maximum entropy model for reasons explained in section gaussian discriminant analysis linear boundary all linear boundaries figure discrimanalysisdboundariesdemo. linear decision boundaries in for the and class case. figure generated by figure geometry of lda in the class case where i. where sigm refers to the sigmoid function now so if we define log t t log w log chapter gaussian models then we have wt and hence py sigmwt which is half way between the means. is closely related to logistic regression which we will discuss in section so the final decision rule is as follows shift x by project onto the line w and see if the result is positive or negative. if then w is in the direction of so we classify the point based on whether its projection is closer to or this is illustrated in figure furthemore if then if we make then gets closer to so more of the line belongs to class a priori. conversely if the boundary shifts right. thus we see that the class prior c just changes the decision threshold and not the overall geometry as we claimed above. similar argument applies in the multi-class case. the magnitude of w determines the steepness of the logistic function and depends on how well-separated the means are relative to the variance. in psychology and signal detection theory it is common to define the discriminability of a signal from the background noise using a quantity called d-prime where is the mean of the signal and is the mean of the noise and is the standard deviation of the noise. if is large the signal will be easier to discriminate from the noise. mle for discriminant analysis we now discuss how to fit a discriminant analysis model. the simplest way is to use maximum likelihood. the log-likelihood function is as follows log pd iyi c log c iyic log n c c we see that this factorizes into a term for and c terms for each c and c. hence we can estimate these parameters separately. for the class prior we have c nc n as with naive bayes. for the class-conditional densities we just partition the data based on its class label and compute the mle for each gaussian cxi ct iyic c nc xi c nc iyic see discrimanalysisfit for a matlab implementation. once the model has been fit you can make predictions using discrimanalysispredict which uses a plug-in approximation. strategies for preventing overfitting the speed and simplicity of the mle method is one of its greatest appeals. however the mle can badly overfit in high dimensions. in particular the mle for a full covariance matrix is singular if nc d. and even when nc d the mle can be ill-conditioned meaning it is close to singular. there are several possible solutions to this problem gaussian discriminant analysis use a diagonal covariance matrix for each class which assumes the features are conditionally independent this is equivalent to using a naive bayes classifier use a full covariance matrix but force it to be the same for all classes c this is an example of parameter tying or parameter sharing and is equivalent to lda use a diagonal covariance matrix and forced it to be shared. this is called diagonal covariance lda and is discussed in section use a full covariance matrix but impose a prior and then integrate it out. if we use a conjugate prior this can be done in closed form using the results from section this is analogous to the bayesian naive bayes method in section see for details. fit a full or diagonal covariance matrix by map estimation. we discuss two different kinds of prior below. project the data into a low dimensional subspace and fit the gaussians there. see sec tion for a way to find the best discriminative linear projection. we discuss some of these options below. regularized lda suppose we tie the covariance matrices so c as in lda and furthermore we perform map estimation of using an inverse wishart prior of the form iwdiag mle section then we have diag mle mle when we evaluate the class conditional densities we need to compute where controls the amount of regularization which is related to the strength of the prior section for details. this technique is known as regularized discriminant analysis or rda et al. mle which is impossible to compute if d n however we can use the svd of x to get around this as we show below. that this trick cannot be applied to qda which is a nonlinear function of x. let x udvt be the svd of the design matrix where v is d n u is an n n orthogonal matrix and d is a diagonal matrix of size n furthermore define the n n matrix z ud this is like a design matrix in a lower dimensional space we assume n d. also define z vt as the mean of the data in this reduced space we can recover the original mean using v z since vt v vvt i. with these definitions we can and hence chapter gaussian models rewrite the mle as follows mle n n n v xt x t zv zt vzt zvt v z t n zt z z t z z vt v zvt where z is the empirical covariance of z. hence we can rewrite the map estimate as map v zvt z diag z z note however that we never need to actually compute the d d matrix map. this is because equation tells us that to classify using lda all we need to compute is py cx exp c where c xt c c c we can compute the crucial c term for rda without inverting the d d matrix as follows c c log c c c t c map c zvt c v z vt c v z zc where zc vt c is the mean of the z matrix for data belonging to class c. see rdafit for the code. diagonal lda a simple alternative to rda is to tie the covariance matrices so c as in lda and then to use a diagonal covariance matrix for each class. this is called the diagonal lda model and is equivalent to rda with the corresponding discriminant function is as follows to equation cx log px y c typically we set cj xcj and across classes defined by j log c j j which is the pooled empirical variance of feature j j iyicxij n c in high dimensional settings this model can work much better than lda and rda and levina gaussian discriminant analysis number of genes r o r r e n o i t a c i f i s s a c s m i l test train cv figure error versus amount of shrinkage for nearest shrunken centroid classifier applied to the srbct gene expression data. figure generated by shrunkencentroidssrbctdemo. based on figure of et al. nearest shrunken centroids classifier one drawback of diagonal lda is that it depends on all of the features. in high dimensional problems we might prefer a method that only depends on a subset of the features for reasons of accuracy and interpretability. one approach is to use a screening method perhaps based on mutual information as in section we now discuss another approach to this problem known as the nearest shrunken centroids classifier et al. the basic idea is to perform map estimation for diagonal lda with a sparsity-promoting prior section more precisely define the class-specific feature mean cj in terms of the class-independent feature mean mj and a class-specific offset cj. thus we have cj mj cj we will then put a prior on the cj terms to encourage them to be strictly zero and compute a map estimate. if for feature j we find that cj for all c then feature j will play no role in the classification decision cj will be independent of c. thus features that are not discriminative are automatically ignored. the details can be found in et al. and and park see shrunkencentroidsfit for some code. let us give an example of the method in action based on et al. consider the problem of classifying a gene expression dataset which genes classes training samples and test samples. using a diagonal lda classifier produces errors on the test set. using the nearest shrunken centroids classifier produced errors on the test set for a range of values see figure more importantly the model is sparse and hence more interpretable figure plots an unpenalized estimate of the difference dcj in gray as well as the shrunken estimates are computed using the value of estimated by cv. estimates cj in blue. we see that only genes are used out of the original now consider an even harder problem with genes a training set of patients a test set of patients and different types of cancer et al. hastie et al. et al. report that nearest shrunken centroids produced errors on the test class class chapter gaussian models class class figure profile of ure this selects genes. based on figure of et al. shrunkencentroidssrbctdemo. the shrunken centroids corresponding to optimal in figfigure generated by set using genes and that rda produced errors on the test set using all genes. the pmtk function cancerhighdimclassifdemo can be used to reproduce these numbers. inference in jointly gaussian distributions given a joint distribution it is useful to be able to compute marginals and conditionals we discuss how to do this below and then give some applications. these operations take time in the worst case. see section for faster methods. inference in jointly gaussian distributions statement of the result theorem and conditionals of an mvn. suppose x is jointly gaussian with parameters then the marginals are given by and the posterior conditional is given by n equation is of such crucial importance in this book that we have put a box around it so you can easily find it. for the proof see section we see that both the marginal and conditional distributions are themselves gaussian. for the marginals we just extract the rows and columns corresponding to or for the conditional we have to do a bit more work. however it is not that complicated the conditional mean is just a linear function of and the conditional covariance is just a constant matrix that is independent of we give three different equivalent expressions for the posterior mean and two different equivalent expressions for the posterior covariance each one is useful in different circumstances. examples below we give some examples of these equations in action which will make them seem more intuitive. marginals and conditionals of a gaussian let us consider a example. the covariance matrix is the marginal is a gaussian obtained by projecting the joint distribution onto the line n chapter gaussian models x x x figure a joint gaussian distribution with a correlation coefficient of we plot the contour and the principal axes. the unconditional marginal the conditional n obtained by slicing at height figure generated by if we get suppose we observe the conditional is obtained by slicing the joint distribution through the line figure in figure we show an example where and we see that e which makes sense since means that we believe that if increases by its mean then increases by we also see var this also makes sense our uncertainty about has gone down since we n if we get have learned something about by observing since conveys no information about if they are uncorrelated hence independent. interpolating noise-free data suppose we want to estimate a function defined on the interval t such that yi f for n observed points ti. we assume for now that the data is noise-free so we want to interpolate it that is fit a function that goes exactly through the data. section for the noisy data case. the question is how does the function behave in between the observed data points? it is often reasonable to assume that the unknown function is smooth. in chapter we shall see how to encode priors over functions and how to update such a prior with observed values to get a posterior over functions. but in this section we take a simpler approach which is adequate for map estimation of functions defined on inputs. we follow the presentation of and somersalo we start by discretizing the problem. first we divide the support of the function into d equal subintervals. we then define xj f sj jh h j d t d inference in jointly gaussian distributions figure interpolating noise-free data using a gaussian with prior precision see also figure based on figure of and somersalo figure generated by gaussinterpdemo. we can encode our smoothness prior by assuming that xj is an average of its neighbors xj and plus some gaussian noise j j d xj where n the precision term controls how much we think the function will vary a large corresponds to a belief that the function is very smooth a small corresponds to a belief that the function is quite wiggly in vector form the above equation can be written as follows where l is the d second order finite difference matrix lx l the corresponding prior has the form exp px n l we will henceforth assume we have scaled l by so we can ignore the term and just write lt l for the precision matrix. note that although x is d-dimensional the precision matrix only has rank d thus this is an improper prior known as an intrinsic gaussian random field section for chapter gaussian models more information. however providing we observe n data points the posterior will be proper. now let be the n noise-free observations of the function and be the d n unknown function values. without loss of generality assume that the unknown variables are ordered first then the known variables. then we can partition the l matrix as follows l r n r we can also partition the precision matrix of the joint distribution lt l lt lt lt lt using equation we can write the conditional distribution as follows lt note that we can compute the mean by solving the following system of linear equations this is efficient since is tridiagonal. figure gives an illustration of these equations. we see that the posterior mean equals the observed data at the specified points and smoothly interpolates in between as desired. it is also interesting to plot the pointwise marginal credibility intervals j shown in grey. we see that the variance goes up as we move away from the data. we also see that the variance goes up as we decrease the precision of the prior interestingly has no effect on the posterior mean since it cancels out when multiplying and by contrast when we consider noisy data in section we will see that the prior precision affects the smoothness of posterior mean estimate. the marginal credibility intervals do not capture the fact that neighboring locations are correlated. we can represent that by drawing complete functions vectors x from the posterior and plotting them. these are shown by the thin lines in figure these are not quite as smooth as the posterior mean itself. this is because the prior only penalizes first-order differences. see section for further discussion of this point. data imputation suppose we are missing some entries in a design matrix. if the columns are correlated we can use the observed entries to predict the missing entries. figure shows a simple example. we sampled some data from a dimensional gaussian and then deliberately hid of the data in each row. we then inferred the missing entries given the observed entries using the true model. more precisely for each row i we compute pxhi where hi and vi are the indices of the hidden and visible entries in case i. from this we compute the marginal we then plot the mean of this distribution distribution of each missing variable pxhij xij e this represents our best guess about the true value of that entry in the inference in jointly gaussian distributions observed imputed truth figure illustration of data imputation. left column visualization of three rows of the data matrix with missing entries. middle column mean of the posterior predictive based on partially observed data in that row but the true model parameters. right column figure generated by gaussimputationdemo. true values. xhijxvi sense that it minimizes our expected squared error section for details. figure shows that the estimates are quite close to the truth. course if j vi the expected value is equal to the observed value xij xij. we can use var as a measure of confidence in this guess although this is not this is called multiple shown. alternatively we could draw multiple samples from pxhi imputation. in addition to imputing the missing entries we may be interested in computing the likelihood of each partially observed row in the table pxvi which can be computed using equation this is useful for detecting outliers observations. information form suppose x n one can show that e is the mean vector and cov is the covariance matrix. these are called the moment parameters of the distribution. however it is sometimes useful to use the canonical parameters or natural parameters defined as we can convert back to the moment parameters using using the canonical parameters we can write the mvn in information form in exponential family form defined in section x t ncx exp where we use the notation nc to distinguish from the moment parameterization n it is also possible to derive the marginalization and conditioning formulas in information form. we find chapter gaussian models thus we see that marginalization is easier in moment form and conditioning is easier in information form. another operation that is significantly easier in information form is multiplying two gaussians. one can show that nc f f g g c f g f g however in moment form things are much messier n f f g g n g g f f g g f g g g proof of the result we now prove theorem readers who are intimidated by heavy matrix algebra can safely skip this section. we first derive some results that we will need here and elsewhere in the book. we will return to the proof at the end. inverse of a partitioned matrix using schur complements the key tool we need is a way to invert a partitioned matrix. this can be done using the following result. theorem of a partitioned matrix. consider a general partitioned matrix m e f g h where we assume e and h are invertible. we have m h e e h h e where mh e fh me h ge we say that mh is the schur complement of m wrt h. equation is called the partitioned inverse formula. proof. if we could block diagonalize m it would be easier to invert. to zero out the top right block of m we can pre-multiply as follows i fh i e f g h e fh h g similarly to zero out the bottom left we can post-multiply as follows inference in jointly gaussian distributions e fh h g putting it all together we get i fh i e f g h i h i i h i e fh h e fh h x m z w taking the inverse of both sides yields z w and hence substituting in the definitions we get e f g h m zw h h h i i h i fh i h i fh i alternatively we could have decomposed the matrix m in terms of e and me ge yielding h h e e e e f g h e f g h the matrix inversion lemma we now derive some useful corollaries of the above result. corollary inversion lemma. consider a general partitioned matrix m where we assume e and h are invertible. we have fh e e ge fh e ge fh ge chapter gaussian models the first two equations are s known as the matrix inversion lemma or the shermanmorrison-woodbury formula. the third equation is known as the matrix determinant lemma. a typical application in machine learning statistics is the following. let e be a n n diagonal matrix let f gt x of size n d where n d and let h i. then we have xxt xt the lhs takes on time to compute the rhs takes time to compute. another application concerns computing a rank one update of an inverse matrix. h scalar f u column vector and g vt row vector. then we have let uvt e e vt e e e e vt e e this is useful when we incrementally add a data vector to a design matrix and want to update our sufficient statistics. can derive an analogous formula for removing a data vector. proof. to prove equation we simply equate the top left block of equation and equation to prove equation we simple equate the top right blocks of equations and the proof of equation is left as an exercise. proof of gaussian conditioning formulas we can now return to our original goal which is to derive equation let us factor the joint as as follows e exp e exp using equation the above exponent becomes i i i exp this is of the form expquadratic form in expquadratic form in i exp linear gaussian systems hence we have successfully factorized the joint as n where the parameters of the conditional distribution can be read off from the above equations using we can also use the fact that to check the normalization constants are correct where and we leave the proof of the other forms of the result in equation as an exercise. linear gaussian systems suppose we have two variables x and y. let x r a noisy observation of x. let us assume we have the following prior and likelihood dx be a hidden variable and y r dy be px n x x pyx n b y where a is a matrix of size dy dx. this is an example of a linear gaussian system. we can represent this schematically as x y meaning x generates y. in this section we show how to invert the arrow that is how to infer x from y. we state the result below then give several examples and finally we derive the result. we will see many more applications of these results in later chapters. statement of the result theorem rule for linear gaussian systems. given a linear gaussian system as in equation the posterior pxy is given by the following pxy n xy xy x at xy y a b xy xyat y x x chapter gaussian models in addition the normalization constant py is given by py n x b y a xat for the proof see section examples in this section we give some example applications of the above result. inferring an unknown scalar from noisy measurements suppose we make n noisy measurements yi of some underlying quantity x let us assume the measurement noise has fixed precision y so the likelihood is pyix y now let us use a gaussian prior for the value of the unknown source px we want to compute yn we can convert this to a form that lets us apply n n row vector of s bayes rule for gaussians by defining y yn a and y diag yi. then we get pxy n n n n y n n yy n n y n y y n y these equations are quite intuitive the posterior precision n is the prior precision plus n units of measurement precision y. also the posterior mean n is a convex combination of the mle y and the prior mean this makes it clear that the posterior mean is a compromise if the prior is weak relative to the signal strength is between the mle and the prior. small relative to y we put more weight on the mle. if the prior is strong relative to the signal strength is large relative to y we put more weight on the prior. this is illustrated in figure which is very similar to the analogous results for the beta-binomial model in figure note that the posterior mean is written in terms of n yy so having n measurements each of precision y is like having one measurement with value y and precision n y. we can rewrite the results in terms of the posterior variance rather than posterior precision linear gaussian systems prior variance prior lik post prior variance prior lik post figure inference about x given a noisy observation y strong prior n the posterior weak prior n the posterior mean is mean is shrunk towards the prior mean which is similar to the mle. figure generated by as follows pxd n n n n n n n n y n n n is the posterior variance. n n y where is the prior variance and we can also compute the posterior sequentially by updating after each observation. if n we can rewrite the posterior after seeing a single observation as follows we define y to be the variances of the likelihood prior and posterior and y y pxy y y y we can rewrite the posterior mean in different ways y y y y y y y y chapter gaussian models the first equation is a convex combination of the prior and the data. the second equation is the prior mean adjusted towards the data. the third equation is the data adjusted towards the prior mean this is called shrinkage. these are all equivalent ways of expressing the tradeoff between likelihood and prior. if is small relative to y corresponding to a strong prior the amount of shrinkage is large figure whereas if is large relative to y corresponding to a weak prior the amount of shrinkage is small figure another way to quantify the amount of shrinkage is in terms of the signal-to-noise ratio which is defined as follows snr e x e where x n is the true signal y x is the observed signal and n y is the noise term. y inferring an unknown vector from noisy measurements now consider n vector-valued observations yi n y and a gaussian prior x n setting a i b and using y for the effective observation with precision n y we have yn n n n n n n y y y see figure for a example. we can think of x as representing the true but unknown location of an object in space such as a missile or airplane and the yi as being noisy observations such as radar blips as we receive more blips we are better able to localize the source. in section we will see how to extend this example to track moving objects using the famous kalman filter algorithm. now suppose we have multiple measuring devices and we want to combine them together this is known as sensor fusion. if we have multiple observations with different covariances to sensors with different reliabilities the posterior will be an appropriate weighted average of the data. consider the example in figure we use an uninformative prior on x namely px n n we get noisy observations n and n we then compute in figure we set so both sensors are equally reliable. in this case the posterior mean is half way between the two observations and in figure we set and so sensor is more reliable than sensor in this case the posterior mean is closer to in figure we set so sensor is more reliable in the component direction and sensor is more in this case the posterior mean uses s reliable in the component direction. vertical component and s horizontal component. linear gaussian systems post after obs data prior figure illustration of bayesian inference for the mean of a gaussian. the data is generated from yi n y where x and y we assume the sensor noise covariance y is known but x is unknown. the black cross represents x. the prior is px n we show the posterior after data points have been observed. figure generated by figure we observe cross and cross and infer e equally reliable sensors so the posterior mean estimate is in between the two circles. cross. sensor is more reliable so the estimate shifts more towards the green circle. sensor is more reliable in the vertical direction sensor is more reliable in the horizontal direction. the estimate is an appropriate combination of the two measurements. figure generated by note that this technique crucially relies on modeling our uncertainty of each sensor computing an unweighted average would give the wrong result. however we have assumed the sensor precisions are known. when they are not we should model out uncertainty about and as well. see section for details. interpolating noisy data we now revisit the example of section this time we no longer assume noise-free let us assume that we obtain n noisy observations yi without loss observations. of generality assume these correspond to xn we can model this setup as a linear instead chapter gaussian models gaussian system y ax where n y y is the observation noise and a is a n d projection matrix that selects out the observed elements. for example if n and d we have a we can easily compute the posterior using the same improper prior as before x l mean and variance. in figure we plot the posterior mean posterior variance and some posterior samples. now we see that the prior precision effects the posterior mean as well as the posterior variance. in particular for a strong prior the estimate is very smooth and the uncertainty is low. but for a weak prior the estimate is wiggly and the uncertainty from the data is high. the posterior mean can also be computed by solving the following optimization problem min x xj where we have defined and xd for notational simplicity. we recognize this as a discrete approximation to the following problem f min where is the first derivative of f. the first term measures fit to the data and the second term penalizes functions that are too wiggly this is an example of tikhonov regularization which is a popular approach to functional data analysis. see chapter for more sophisticated approaches which enforce higher order smoothness the resulting samples look less jagged proof of the result we now derive equation the basic idea is to derive the joint distribution px y pxpyx and then to use the results from section for computing pxy. in more detail we proceed as follows. the log of the joint distribution is as follows irrelevant constants log px y xt x x ax bt y ax b this is clearly a joint gaussian distribution since it is the exponential of a quadratic form. expanding out the quadratic terms involving x and y and ignoring linear and constant terms we have q y y y a at y y t y ax y x y yt x x xt x at x y y a x y x y digression the wishart distribution interpolating noisy data variance using a gaussian with prior precision see also figure based on figure of and somersalo figure figure generated by gaussinterpnoisydemo. see also splinebasisdemo. where the precision matrix of the joint is defined as x at y a at y y a y xx xy yx yy from equation and using the fact that y a x b we have pxy xy xy xy xx xy xy xy x at y a xx x xyy y y b x at digression the wishart distribution the wishart distribution is the generalization of the gamma distribution to positive definite matrices. press has said the wishart distribution ranks next to the normal distribution in order of importance and usefuleness in multivariate statistics we will mostly use it to model our uncertainty in covariance matrices or their inverses the pdf of the wishart is defined as follows wi d exp tr s zwi here is called the degrees of freedom and s is the scale matrix shall get more intuition for these parameters shortly. the normalization constant for this distribution chapter gaussian models requires integrating over all symmetric pd matrices is the following formidable expression zwi d where da is the multivariate gamma function dx dd hence and i d the normalization constant only exists hence the pdf is only well defined if d in particular let xi n then the scatter matrix s i has a wishart distribution s wi hence e n more generally one can show that the mean and mode of wis are given by there is a connection between the wishart distribution and the gaussian. xixt mean s mode d where the mode only exists if d if d the wishart reduces to the gamma distribution wi ga s inverse wishart distribution recall that we showed that if gaa b then that iga b. similarly if wis then iws d where iw is the inverse wishart the multidimensional generalization of the inverse gamma. it is defined as follows for d and s iw exp ziw d ziw trs one can show that the distribution has these properties mean s d mode s d if d this reduces to the inverse gamma iw ig inferring the parameters of an mvn s figure visualization of the wishart distribution. left some samples from the wishart distribution wis where s and right plots of the marginals are gamma and the approximate marginal on the correlation coefficient. if there is a lot of uncertainty about the value of the correlation coefficient the almost uniform distribution on the sampled matrices are highly variable and some are nearly singular. as increases the sampled matrices are more concentrated on the prior s. figure generated by wiplotdemo. visualizing the wishart distribution since the wishart is a distribution over matrices it is hard to plot as a density function. however we can easily sample from it and in the case we can use the eigenvectors of the resulting matrix to define an ellipse as explained in section see figure for some examples. for higher dimensional matrices we can plot marginals of the distribution. the diagonals of a wishart distributed matrix have gamma distributions so are easy to plot. it is hard in general to work out the distribution of the off-diagonal elements but we can sample matrices from the distribution and then compute the distribution empirically. in particular we can convert each sampled matrix to a correlation matrix and thus compute a monte carlo approximation to the expected correlation coefficients e s r where wi and r converts matrix into a correlation matrix rij ii jj we can then use kernel density estimation to produce a smooth approximation to the univariate density e for plotting purposes. see figure for some examples. inferring the parameters of an mvn so far we have discussed inference in a gaussian assuming the parameters are known. we now discuss how to infer the parameters themselves. we will assume the data has chapter gaussian models the form xi n for i and is fully observed so we have no missing data section for how to estimate parameters of an mvn in the presence of missing values. to simplify the presentation we derive the posterior in three parts first we compute p then we compute p finally we compute the joint p posterior distribution of we have discussed how to compute the mle for we now discuss how to compute its posterior which is useful for modeling our uncertainty about its value. the likelihood has the form pd n n for simplicity we will use a conjugate prior which in this case is a gaussian. in particular if p n then we can derive a gaussian posterior for based on the results in section we get p vn n v n v mn vn x this is exactly the same process as inferring the location of an object based on noisy radar blips except now we are inferring the mean of a distribution based on noisy samples. a bayesian there is no difference between uncertainty about parameters and uncertainty about anything else. we can model an uninformative prior by setting i. in this case we have p n n so the posterior mean is equal to the mle. we also see that the posterior variance goes down as which is a standard result from frequentist statistics. posterior distribution of we now discuss how to compute p the likelihood has the form pd n exp trs the corresponding conjugate prior is known as the inverse wishart distribution recall that this has the following pdf iw exp here d is the degrees of freedom and is a symmetric pd matrix. we see that s plays the role of the prior scatter matrix and d controls the strength of the prior and hence plays a role analogous to the sample size n inferring the parameters of an mvn l e u a v n e g e i true mle k map true mle map true mle map l e u a v n e g e i l e u a v n e g e i figure estimating a covariance matrix in d dimensions using n samples. we plot the eigenvalues in descending order for the true covariance matrix black the mle blue and the map estimate red using equation with we also list the condition number of each matrix in the legend. based on figure of and strimmer figure generated by shrinkcovdemo. multiplying the likelihood and prior we find that the posterior is also inverse wishart p n tr exp tr exp n iw n exp tr n n s n s in words this says that the posterior strength n is the prior strength plus the number of observations n and the posterior scatter matrix sn is the prior scatter matrix plus the data scatter matrix s map estimation we see from equation that mle is a rank minn d matrix. if n d this is not full rank and hence will be uninvertible. and even if n d it may be the case that is ill-conditioned it is nearly singular. to solve these problems we can use the posterior mode mean. one can show techniques analogous to the derivation of the mle that the map estimate is given by map sn n d s n if we use an improper uniform prior corresponding to and we recover the mle. chapter gaussian models let us now consider the use of a proper informative prior which is necessary whenever dn is large bigger than let x so s sx. then we can rewrite the map estimate as a convex combination of the prior mode and the mle. to see this let be the prior mode. then the posterior mode can be rewritten as map sx n n n n s n mle where controls the amount of shrinkage towards the prior. this begs the question where do the parameters of the prior come from? it is common to set by cross validation. alternatively we can use the closed-form formula provided in and wolf schaefer and strimmer which is the optimal frequentist estimate if we use squared loss. this is arguably not the most natural loss function for covariance matrices it ignores the postive definite constraint but it results in a simple estimator which is implemented in the pmtk function shrinkcov. we discuss bayesian ways of estimating later. as for the prior covariance matrix it is common to use the following dependent prior diag mle. in this case the map estimate is given by mapi j mlei j mlei j if i j otherwise thus we see that the diagonal entries are equal to their ml estimates and the off diagonal elements are shrunk somewhat towards this technique is therefore called shrinkage estimation orregularized estimation. the benefits of map estimation are illustrated in figure we consider fitting a dimensional gaussian to n n and n data points. we see that the map estimate is always well-conditioned unlike the mle. in particular we see that the eigenvalue spectrum of the map estimate is much closer to that of the true matrix than the mle s. the eigenvectors however are unaffected. the importance of regularizing the estimate of will become apparent in later chapters when we consider fitting covariance matrices to high dimensional data. univariate posterior in the case the likelihood has the form pd exp the standard conjugate prior is the inverse gamma distribution which is just the scalar version of the inverse wishart ig exp inferring the parameters of an mvn prior iw true figure sequential updating of the posterior for starting from an uninformative prior. the data was generated from a gaussian with known mean and unknown variance figure generated by multiplying the likelihood and the prior we see that the posterior is also ig p ig bn an bn see figure for an illustration. this arises because iw ig the form of the posterior is not quite as pretty as the multivariate case because of the factors of another problem with using the distribution is that the strength of the prior is encoded in both and to avoid both of these problems it is common the statistics literature to use an alternative parameterization of the ig distribution known as the inverse chi-squared distribution. this is defined as follows ig exp here controls the strength of the prior and prior the posterior becomes p n n n n n n encodes the value of the prior. with this we see that the posterior dof n is the prior dof plus n and the posterior sum of squares n n is the prior sum of squares we can emulate an uninformative prior p by setting which makes plus the data sum of squares. intuitive sense it corresponds to a zero virtual sample size. chapter gaussian models posterior distribution of and we now discuss how to compute p these results are a bit complex but will prove useful later on in this book. feel free to skip this section on a first reading. likelihood the likelihood is given by pd n n exp now one can show that tr hence we can rewrite the likelihood as follows pd n n n exp tr exp xt x n we will use this form below. prior the obvious prior to use is the following p unfortunately this is not conjugate to the likelihood. to see why note that and appear together in a non-factorized way in the likelihood hence they will also be coupled together in the posterior. the above prior is sometimes called semi-conjugate or conditionally conjugate since both conditionals p and p are individually conjugate. to create a full conjugate prior we need to use a prior where and are dependent on each other. we will use a joint distribution of the form p looking at the form of the likelihood equation equation we see that a natural conjugate inferring the parameters of an mvn prior has the form of a normal-inverse-wishart or niw distribution defined as follows niw iw tr n zn iw exp exp zn iw exp zn iw d tr where da is the multivariate gamma function. the parameters of the niw can be interpreted as follows is our prior mean for and is how strongly we believe this prior and is to our prior mean for and is how strongly we believe this one can show that the uninformative prior has the form lim k n k d niw in practice it is often better to use a weakly informative data-dependent prior. a common and raftery is to use choice e.g. diagsxn and d to ensure e and to set x and to some small number such as et al. although this prior has four parameters there are really only three free parameters since our uncertainty in the mean is proportional to the variance. in particular if we believe that the variance is large then our uncertainty in must be large too. this makes sense intuitively since if the data has large spread it may be hard to pin down its mean. see also exercise where we will see the three free parameters more explicitly. if we want separate control over our confidence in and we must use a semi-conjugate prior. posterior chapter gaussian models the posterior can be shown to be niw with updated parameters p niw n n sn n x mn n n n x n n n n n sn sx n s xixt n mn mt i as the uncentered sum-of-squares matrix is easier n where we have defined s to update incrementally than the centered version. this result is actually quite intuitive the posterior mean is a convex combination of the prior mean and the mle with strength n and the posterior scatter matrix sn is the prior scatter matrix plus the empirical scatter matrix sx plus an extra term due to the uncertainty in the mean creates its own virtual scatter matrix. posterior mode the mode of the joint distribution has the following form argmax p sn n d if we set this reduces to argmax p sx n d the corresponding estimate is almost the same as equation but differs by in the denominator because this is the mode of the joint not the mode of the marginal. posterior marginals the posterior marginal for is simply p p iw n the mode and mean of this marginal are given by n d n d e map sn sn one can show that the posterior marginal for has a multivariate student t distribution p p t sn n d n n d this follows from the fact that the student distribution can be represented as a scaled mixture of gaussians equation inferring the parameters of an mvn is the prior variance and is how strongly we believe this. figure the n i believe this notice that the contour plot the surface is shaped like a squashed egg increase the strength of our belief in the mean so it gets narrower we increase the strength of our belief in the variance so it gets narrower figure generated by distribution. is the prior mean and is how strongly we we posterior predictive the posterior predictive is given by pxd pxd pd so it can be easily evaluated in terms of a ratio of marginal likelihoods. it turns out that this ratio has the form of a multivariate student-t distribution pxd n n n sn d t n n n d sn n d the student-t has wider tails than a gaussian which takes into account the fact that is unknown. however this rapidly becomes gaussian-like. posterior for scalar data we now specialise the above results to the case where xi is these results are widely used in the statistics literature. as in section it is conventional not to use the normal inverse chapter gaussian models wishart but to use the normal inverse chi-squared or nix distribution defined by n i n exp see figure for some plots. along the axis the distribution is shaped like a gaussian and along the axis the distribution is shaped like a the contours of the joint density have interestingly we see that the contours for are more peaked a squashed egg appearance. for small values of which makes sense since if the data is low variance we will be able to estimate its mean more reliably. one can show that the posterior is given by p i n n n n x mn n n n n n n n n n let us see how these results look if we use the following uninformative prior p p n i with this prior the posterior has the form p i x n n n n n where n n n mle is the the sample standard deviation. estimate of the variance. hence the marginal posterior for the mean is given by section we show that this is an unbiased p t n n the posterior marginal for is just p with the posterior mean given by e p n n n n n mixture representation of the student p p t with the posterior mean given by e mn n n n the posterior marginal for has a student t distribution which follows from the scale inferring the parameters of an mvn and the posterior variance of is n n n n n n n var var s n the square root of this is called the standard error of the mean thus an approximate posterior credible interval for the mean is x s n credible intervals are discussed in more detail in section they are contrasted with frequentist confidence intervals in section bayesian t-test suppose we want to test the hypothesis that for some known value given values xi n this is called a two-sided one-sample t-test. a simple way to perform such a test is just to check if if it is not then we can be sure that a more common scenario is when we want to test if two paired samples have the same mean. more precisely suppose yi n and zi n we want to determine if using xi yi zi as our data. we can evaluate this quantity as follows p p this is called a one-sided paired t-test. the difference in binomial proportions see section a similar approach to unpaired tests comparing to calculate the posterior we must specify a prior. suppose we use an uninformative prior. as we showed above we find that the posterior marginal on has the form p t n n now let us define the following t statistic t x s n where the denominator is the standard error of the mean. we see that p fn where f is the cdf of the standard student t distribution t a more complex approach is to perform bayesian model comparison. that is we compute the bayes factor in section where is the point null hypothesis that and is the alternative hypothesis that see et al. rouder et al. for details. chapter gaussian models connection with frequentist statistics if we use an uninformative prior it turns out that the above bayesian analysis gives the same result as derived using frequentist methods. discuss frequentist statistics in chapter specifically from the above results we see that sn tn this has the same form as the sampling distribution of the mle sn tn the reason is that the student distribution is symmetric in its first two arguments so t t hence statements about the posterior for have the same form as statements about the sampling distribution of x. consequently the p-value in section returned by a frequentist test is the same as p returned by the bayesian method. see bayesttestdemo for an example. despite the superficial similarity these two results have a different interpretation in the bayesian approach is unknown and x is fixed whereas in the frequentist approach x is unknown and is fixed. more equivalences between frequentist and bayesian inference in simple models using uninformative priors can be found in and tiao see also section sensor fusion with unknown precisions in this section we apply the results in section to the problem of sensor fusion in the case where the precision of each measurement device is unknown. this generalizes the results of section where the measurement model was assumed to be gaussian with known precision. the unknown precision case turns out to give qualitatively different results yielding a potentially multi-modal posterior as we will see. our presentation is based on suppose we want to pool data from multiple sources to estimate some quantity r but the reliability of the sources is unknown. specifically suppose we have two different measurement devices x and y with different precisions xi n y we make two independent measurements with each device which turn out to be x and yi n we will use a non-informative prior for p which we can emulate using an infinitely broad gaussian p if the x and y terms were known then the posterior would be gaussian p x y n n nx x ny y mn xnxx ynyy nx x ny y inferring the parameters of an mvn xi and y ny where nx is the number of x measurements ny is the number of y measurements x yi this result follows because the posterior nx precision is the sum of the measurement precisions and the posterior mean is a weighted sum of the prior mean is and the data means. however the measurement precisions are not known. initially we will estimate them by maximum likelihood. the log-likelihood is given by x y log x x log y y i i the mle is obtained by solving the following simultaneous equations x y xnxx ynyy x nx y ny this gives x y nx xx ny yy nx x ny y nx i ny i we notice that the mle for has the same form as the posterior mean mn we can solve these equations by fixed point iteration. let us initialize by estimating x x and y using this we get so p x y if we now iterate we converge to x y p x y n and y where x nx y ny the plug-in approximation to the posterior is plotted in figure this weights each sensor according to its estimated precision. since sensor y was estimated to be much less reliable than sensor x we havee x so we effectively ignore the y sensor. x y now we will adopt a bayesian approach and integrate out the unknown precisions rather than trying to estimate them. that is we compute p p pdx xp x x we will use uninformative jeffrey s priors p p x x and p y y. pdy yp y y chapter gaussian models since the x and y terms are symmetric we will just focus on one of them. the key integral is i pdx xp x x x nx xx nx exp x x d x exploiting the fact that nx this simplifies to i x x exp xx xd x we recognize this as proportional to the integral of an unnormalized gamma density ga b a b x. hence the integral is proportional to the normalizing where a and b constant of the gamma distribution a so we get x pdx xp x x i x and the posterior becomes p x y the exact posterior is plotted in figure we see that it has two modes one near x and one near y these correspond to the beliefs that the x sensor is more reliable than the y one and vice versa. the weight of the first mode is larger since the data from the x sensor agree more with each other so it seems slightly more likely that the x sensor is the reliable one. obviously cannot both be reliable since they disagree on the values that they are reporting. however the bayesian solution keeps open the possibility that the y sensor is the more reliable one from two measurements we cannot tell and choosing just the x sensor as the plug-in approximation does results in over confidence posterior that is too narrow. exercises exercise uncorrelated does not imply independent let x u and y x clearly y is dependent on x fact y is uniquely determined if x u b then ex and by x. however show that y hint var exercise uncorrelated and gaussian does not imply independent unless jointly gaussian let x n and y w x where pw pw it is clear that x and y are not independent since y is a function of x. a. show y n inferring the parameters of an mvn figure posterior for sensorfusionunknownprec. plug-in approximation. exact posterior. figure generated by b. show cov y thus x and y are uncorrelated but dependent even though they are gaussian. hint use the definition of covariance cov y e e e and the rule of iterated expectation e e exercise correlation coefficient is between and prove that y exercise correlation coefficient for linearly related variables is show that if y ax b for some parameters a and b then y similarly show that if a then y exercise normalization constant for a multidimensional gaussian prove that the normalization constant for a d-dimensional gaussian is given by exp hint diagonalize and use the fact that i i to write the joint pdf as a product of d onedimensional gaussians in a transformed coordinate system. will need the change of variables formula. finally use the normalization constant for univariate gaussians. exercise bivariate gaussian let x n where x r and where is the correlation coefficient. show that the pdf is given by exp chapter gaussian models raw standarized whitened figure heightweight data for the men. standardized. whitened. exercise conditioning a bivariate gaussian consider a bivariate gaussian distribution where where the correlation coefficient is given by a. what is p simplify your answer by expressing it in terms of and b. assume what is p now? exercise whitening vs standardizing a. load the heightweight data using rawdata dlmread heightweightdata.txt the first column is the class label the second column is height the third weight. extract the heightweight data corresponding to the males. fit a gaussian to the male data using the empirical mean and covariance. plot your gaussian as an ellipse superimposing on your scatter plot. it should look like figure where have labeled each datapoint by its index. turn in your figure and code. b. standardizing the data means ensuring the empirical variance along each dimension is this can be where j is the empirical std of dimension j. standardize the data and done by computing replot. it should look like figure axis equal turn in your figure and code. xij xj j c. whitening or sphereing the data means ensuring its empirical covariance matrix is proportional to i so the data is uncorrelated and of equal variance along each dimension. this can be done by computing ut x for each data vector x where u are the eigenvectors and the eigenvalues of x. whiten the data and replot. it should look like figure note that whitening rotates the data so people move to counter-intuitive locations in the new coordinate system e.g. person who moves from the right hand side to the left. exercise sensor fusion with known variances in suppose we have two sensors with known different variances and but unknown the same i n from the first sensor and observations mean suppose we observe observations inferring the parameters of an mvn i n from the second sensor. example suppose is the true temperature outside and sensor is a precise variance digital thermosensing device and sensor is an imprecise variance mercury thermometer. let d represent all the data from both sensors. what is the posterior p assuming a non-informative prior for we can simulate using a gaussian with a precision of give an explicit expression for the posterior mean and variance. exercise derivation of information form formulae for marginalizing and conditioning derive the information form results of section exercise derivation of the niw posterior derive equation hint one can show that n n mn mn n this is a matrix generalization of an operation called completing the derive the corresponding result for the normal-wishart model. exercise bic for gaussians jaakkola. the bayesian information criterion is a penalized log-likelihood function that can be used for model selection section it is defined as bic log pd m l d logn where d is the number of free parameters in the model and n is the number of samples. in this question we will see how to use this to choose between a full covariance gaussian and a gaussian with a diagonal covariance. obviously a full covariance gaussian has higher likelihood but it may not be worth the extra parameters if the improvement over a diagonal covariance matrix is too small. so we use the bic score to choose the model. following section we can write log pd n tr s n s n log xxi xt where s is the scatter matrix covariance the trace of a matrix is the sum of its diagonals and we have used the trace trick. a. derive the bic score for a gaussian in d dimensions with full covariance matrix. simplify your answer as much as possible exploiting the form of the mle. be sure to specify the number of free parameters d. b. derive the bic score for a gaussian in d dimensions with a diagonal covariance matrix. be sure to specify the number of free parameters d. hint for the digaonal case the ml estimate of is the same as m l except the off-diagonal terms are zero diag diag m m ld d in the scalar case completing the square means rewriting as ax w where a b and w chapter gaussian models exercise gaussian posterior credible interval degroot. let x n where is unknown but has prior n seeing n samples is n n n. confidence interval. how big does n have to be to ensure the posterior after is called a credible interval and is the bayesian analog of a n ud where u is an interval on n of width and d is the data. hint recall that of the probability mass of a gaussian is within of the mean. exercise map estimation for gaussians jaakkola. consider samples xn from a gaussian random variable with known variance and unknown mean we further assume a prior distribution gaussian over the mean n with fixed mean m and fixed variance thus the only unknown is a. calculate the map estimate m ap you can state the result without proof. alternatively with a lot more work you can compute derivatives of the log posterior set to zero and solve. b. show that as the number of samples n increase the map estimate converges to the maximum likelihood estimate. c. suppose n is small and fixed. what does the map estimator converge to if we increase the prior variance d. suppose n is small and fixed. what does the map estimator converge to if we decrease the prior variance exercise sequential updating of et al. the unbiased estimates for the covariance of a d-dimensional gaussian based on n samples is given by cn n mnxi mnt it is clear that it takes time to compute cn. efficient to incrementally update these estimates than to recompute from scratch. if the data points arrive one at a time it is more a. show that the covariance can be sequentially udpated as follows mnt cn n n n b. how much time does it take per sequential update? big-o notation. c. show that we can sequentially update the precision matrix using c n n n c c n mnt c n mnt c n mn n hint notice that the update to consists of adding a rank-one matrix namely uut where u mn. use the matrix inversion lemma for rank-one updates which we repeat here for convenience uvt e e e t e inferring the parameters of an mvn d. what is the time complexity per update? exercise likelihood ratio for gaussians source source alpaydin ex consider a binary classifier where the k class conditional densities are mvn pxy j n j j. by bayes rule we have py py log log pxy pxy log py py in other words the log posterior ratio is the log likelihood ratio plus the log prior ratio. for each of the cases in the table below derive an expression for the log likelihood ratio log simplifying as much as possible. form of j arbitrary shared shared axis-aligned j with ij for i j shared spherical cov j j j num parameters kdd dd d exercise ldaqda on heightweight data the function discrimanalysisheightweightdemo fits an lda and qda model to the heightweight data. compute the misclassification rate of both of these models on the training set. turn in your numbers and code. exercise naive bayes with mixed features consider a class naive bayes classifier with one binary feature and one gaussian feature y muy c c c n c c let the parameter vectors be as follows a. compute result should be a vector of numbers that sums to b. compute c. compute d. explain any interesting patterns you see in your results. hint look at the parameter vector exercise decision boundary for lda with semi tied covariances consider a generative classifier with class conditional densities of the form n c c. in lda we assume c and in qda each c is arbitrary. here we consider the class case in which k for k that is the gaussian ellipsoids have the same shape but the one for class is wider derive an expression for py simplifying as much as possible. give a geometric interpretation of your result if possible. exercise logistic regression vs ldaqda jaakkola. suppose we train the following binary classifiers via maximum likelihood. matrices set to i matrix i.e. pxy c n c i. we assume py is uniform. a. gaussi a generative classifier where the class conditional densities are gaussian with both covariance b. gaussx as for gaussi but the covariance matrices are unconstrained i.e. pxy c n c c. chapter gaussian models c. linlog a logistic regression model with linear features. d. quadlog a logistic regression model using linear and quadratic features polynomial basis function expansion of degree after training we compute the performance of each model m on the training set as follows log pyixi m lm n that this is the conditional log-likelihood pyx and not the joint log-likelihood py x we now want to compare the performance of each model. we will write lm if model m must have lower equal log likelihood the training set than for any training set other words m is worse than at least as far as training set logprob is concerned. for each of the following model pairs state whether lm or whether no such statement can be made m might sometimes be better than and sometimes worse also for each question briefly sentences explain why. lm a. gaussi linlog. b. gaussx quadlog. c. linlog quadlog. d. gaussi quadlog. e. now suppose we measure performance in terms of the average misclassification rate on the training set rm n iyi yxi is it true in general that lm implies that rm explain why or why not. exercise gaussian decision boundaries et al. let pxy j j j where j and let the class priors be equal py a. find the decision region px px sketch the result. hint draw the curves and find where they intersect. find both solutions of the equation px px hint recall that to solve a quadratic equation bx c we use b x b. now suppose all other parameters remain the same. what is in this case? inferring the parameters of an mvn exercise qda with classes consider a three category classification problem. let the prior probabilites p the class-conditional densities are multivariate normal densities with parameters classify the following points a. x b. x exercise scalar qda you can solve this exercise by hand or using a computer r whatever. in either case show your work. consider the following training set of heights x inches and gender y of some us college students x y m m f f f a. fit a bayes classifier to this data using maximum likelihood estimation i.e. estimate the parameters of the class conditional likelihoods pxy c n c c and the class prior py c c what are your values of c c c for c m f show your work you can get partial credit if you make an arithmetic error. b. compute py mx where x and are the mle parameters. is called a plug-in prediction. c. what would be a simple way to extend this technique if you had multiple attributes per person such as height and weight? write down your proposed model as an equation. bayesian statistics introduction we have now seen a variety of different probability models and we have discussed how to i.e. we have discussed how to compute map parameter estimates fit them to data argmax p using a variety of different priors. we have also discussed how to compute the full posterior p as well as the posterior predictive density pxd for certain special cases in later chapters we will discuss algorithms for the general case. using the posterior distribution to summarize everything we know about a set of unknown variables is at the core of bayesian statistics. in this chapter we discuss this approach to statistics in more detail. in chapter we discuss an alternative approach to statistics known as frequentist or classical statistics. summarizing posterior distributions the posterior p summarizes everything we know about the unknown quantities in this section we discuss some simple quantities that can be derived from a probability distribution such as a posterior. these summary statistics are often easier to understand and visualize than the full joint. map estimation we can easily compute a point estimate of an unknown quantity by computing the posterior mean median or mode. in section we discuss how to use decision theory to choose between these methods. typically the posterior mean or median is the most appropriate choice for a realvalued quantity and the vector of posterior marginals is the best choice for a discrete quantity. however the posterior mode aka the map estimate is the most popular choice because it reduces to an optimization problem for which efficient algorithms often exist. futhermore map estimation can be interpreted in non-bayesian terms by thinking of the log prior as a regularizer section for more details. although this approach is computationally appealing it is important to point out that there are various drawbacks to map estimation which we briefly discuss below. this will provide motivation for the more thoroughly bayesian approach which we will study later in this chapter elsewhere in this book. chapter bayesian statistics figure a bimodal distribution in which the mode is very untypical of the distribution. the thin blue vertical line is the mean which is arguably a better summary of the distribution since it is near the majority of the probability mass. figure generated by bimodaldemo. a skewed distribution in which the mode is quite different from the mean. figure generated by gammaplotdemo. no measure of uncertainty the most obvious drawback of map estimation and indeed of any other point estimate such as the posterior mean or median is that it does not provide any measure of uncertainty. in many applications it is important to know how much one can trust a given estimate. we can derive such confidence measures from the posterior as we discuss in section plugging in the map estimate can result in overfitting in machine learning we often care more about predictive accuracy than in interpreting the parameters of our models. however if we don t model the uncertainty in our parameters then our predictive distribution will be overconfident. we saw several examples of this in chapter and we will see more examples later. overconfidence in predictions is particularly problematic in situations where we may be risk averse see section for details. the mode is an untypical point choosing the mode as a summary of a posterior distribution is often a very poor choice since the mode is usually quite untypical of the distribution unlike the mean or median. this is illustrated in figure for a continuous space. the basic problem is that the mode is a point of measure zero whereas the mean and median take the volume of the space into account. another example is shown in figure here the mode is but the mean is non-zero. such skewed distributions often arise when inferring variance parameters especially in hierarchical models. in such cases the map estimate hence the mle is obviously a very bad estimate. how should we summarize a posterior if the mode is not a good choice? the answer is to use decision theory which we discuss in section the basic idea is to specify a loss function where l is the loss you incur if the truth is and your estimate is if we use loss l i then the optimal estimate is the posterior mode. loss means you only get points if you make no errors otherwise you get nothing there is no partial credit under summarizing posterior distributions py g px figure example of the transformation of a density under a nonlinear transform. note how the mode of the transformed distribution is not the transform of the original mode. based on exercise of figure generated by bayeschangeofvar. this loss function! for continuous-valued quantities we often prefer to use squared error loss l the corresponding optimal estimator is then the posterior mean as we show in section or we can use a more robust loss function l which gives rise to the posterior median. map estimation is not invariant to reparameterization a more subtle problem with map estimation is that the result we get depends on how we parameterize the probability distribution. changing from one representation to another equivalent representation changes the result which is not very desirable since the units of measurement are arbitrary when measuring distance we can use centimetres or inches. to understand the problem suppose we compute the posterior for x. if we define y f the distribution for y is given by equation which we repeat here for convenience dx dy pyy pxx the dx dy term is called the jacobian and it measures the change in size of a unit volume passed through f. let x argmaxx pxx be the map estimate for x. in general it is not the case that y argmaxy pyy is given by f x. for example let x n and y f where f exp x we can derive the distribution of y using monte carlo simulation section the result is shown in figure we see that the original gaussian has become squashed by the sigmoid nonlinearity. in particular we see that the mode of the transformed distribution is not equal to the transform of the original mode. chapter bayesian statistics to see how this problem arises in the context of map estimation consider the following example due to michael jordan. the bernoulli distribution is typically parameterized by its mean so py where y suppose we have a uniform prior on the unit interval p if there is no data the map estimate is just the mode of the prior which can be anywhere between and we will now show that different parameterizations can pick different points in this interval arbitrarily. first let so the new prior is d d p p for so the new mode is m ap arg max now let the new prior is d for so the new mode is p p m ap arg max d thus the map estimate depends on the parameterization. the mle does not suffer from this since the likelihood is a function not a probability density. bayesian inference does not suffer from this problem either since the change of measure is taken into account when integrating over the parameter space. one solution to the problem is to optimize the following objective function pd argmax here i is the fisher information matrix associated with px section this estimate is parameterization independent for reasons explained in druilhet and marin unfortunately optimizing equation is often difficult which minimizes the appeal of the whole approach. credible intervals in addition to point estimates we often want a measure of confidence. a standard measure of confidence in some quantity is the width of its posterior distribution. this can be measured using a credible interval which is a region c u for lower and upper which contains of the posterior probability mass i.e. c u p ud there may be many such intervals so we choose one such that there is mass in each tail this is called a central interval. summarizing posterior distributions figure central interval and hpd region for a posterior. the ci is and the hpd is based on figure of figure generated by betahpd. if the posterior has a known functional form we can compute the posterior central interval using f and u f where f is the cdf of the posterior. for example if the posterior is gaussian p n and then we have and u where denotes the cdf of the gaussian. this is illustrated in figure this justifies the common practice of quoting a credible interval in the form of where represents the posterior mean represents the posterior standard deviation and is a good approximation to of course the posterior is not always gaussian. for example in our coin example if we use a uniform prior and we observe heads out of n trials then the posterior is a beta distribution p we find the posterior credible interval is betacredibleint for the one line of matlab code we used to compute this. if we don t know the functional form but we can draw samples from the posterior then we can use a monte carlo approximation to the posterior quantiles we simply sort the s samples and find the one that occurs at location along the sorted list. as s this converges to the true quantile. see mcquantiledemo for a demo. people often confuse bayesian credible intervals with frequentist confidence intervals. however they are not the same thing as we discuss in section in general credible intervals are usually what people want to compute but confidence intervals are usually what they actually compute because most people are taught frequentist statistics but not bayesian statistics. fortunately the mechanics of computing a credible interval is just as easy as computing a confidence interval e.g. betacredibleint for how to do it in matlab. highest posterior density regions a problem with central intervals is that there might be points outside the ci which have higher probability density. this is illustrated in figure where we see that points outside the left-most ci boundary have higher density than those just inside the right-most ci boundary. this motivates an alternative quantity known as the highest posterior density or hpd region. this is defined as the of most probable points that in total constitute of the chapter bayesian statistics pmin figure figure of et al. figure generated by postdensityintervals. central interval and hpd region for a hypothetical multimodal posterior. based on probability mass. more formally we find the threshold p p and then define the hpd as c p p on the pdf such that in the hpd region is sometimes called a highest density interval or hdi. for example figure shows the hdi of a distribution which is we see that this is narrower than the ci even though it still contains of the mass furthermore every point inside of it has higher density than every point outside of it. for a unimodal distribution the hdi will be the narrowest interval around the mode containing of the mass. to see this imagine water filling in reverse where we lower the level until of the mass is revealed and only is submerged. this gives a simple algorithm for computing hdis in the case simply search over points such that the interval contains of the mass and has minimal width. this can be done by numerical optimization if we know the inverse cdf of the distribution or by search over the sorted data points if we have a bag of samples betahpd for a demo. if the posterior is multimodal the hdi may not even be a connected region see figure for an example. however summarizing multimodal posteriors is always difficult. inference for a difference in proportions sometimes we have multiple parameters and we are interested in computing the posterior distribution of some function of these parameters. for example suppose you are about to buy something from amazon.com and there are two sellers offering it for the same price. seller has positive reviews and negative reviews. seller has positive reviews and negative reviews. who should you buy this example is from see also lingpipe-blog.c bayesian model selection p p f d p exact posteriors p idi. monte carlo approximation to p we use kernel density figure estimation to get a smooth plot. the vertical lines enclose the central interval. figure generated by amazonsellerdemo on the face of it you should pick seller but we cannot be very confident that seller is better since it has had so few reviews. in this section we sketch a bayesian analysis of this problem. similar methodology can be used to compare rates or proportions across groups for a variety of other settings. let and be the unknown reliabilities of the two sellers. since we don t know much about them we ll endow them both with uniform priors i the posteriors are p and p let us define as the difference in the rates. we might want to work in terms of the log-odds ratio. we can compute the desired quantity using numerical integration we want to compute p for convenience p i beta we find p which means you are better off buying from seller see amazonsellerdemo for the code. is also possible to solve the integral analytically a simpler way to solve the problem is to approximate the posterior p by monte carlo sampling. this is easy since and are independent in the posterior and both have beta distributions which can be sampled from using standard methods. the distributions p idi are shown in figure and a mc approximation to p together with a hpd is shown figure an mc approximation to p is obtained by counting the fraction of samples where this turns out to be which is very close to the exact value. amazonsellerdemo for the code. bayesian model selection in figure we saw that using too high a degree polynomial results in overfitting and using too low a degree results in underfitting. similarly in figure we saw that using too small chapter bayesian statistics a regularization parameter results in overfitting and too large a value results in underfitting. in general when faced with a set of models families of parametric distributions of different complexity how should we choose the best one? this is called the model selection problem. one approach is to use cross-validation to estimate the generalization error of all the candiate models and then to pick the model that seems the best. however this requires fitting each model k times where k is the number of cv folds. a more efficient approach is to compute the posterior over models from this we can easily compute the map model m argmax pmd. this is called bayesian model selection. if we use a uniform prior over models pm this amounts to picking the model which pmd pdmpm m m pmd maximizes pdm pd this quantity is called the marginal likelihood the integrated likelihood or the evidence for model m. the details on how to perform this integral will be discussed in section but first we give an intuitive interpretation of what this quantity means. bayesian occam s razor one might think that using pdm to select models would always favor the model with the most parameters. this is true if we use pd m to select models where m is the mle or map estimate of the parameters for model m because models with more parameters will fit the data better and hence achieve higher likelihood. however if we integrate out the parameters rather than maximizing them we are automatically protected from overfitting models with more parameters do not necessarily have higher marginal likelihood. this is called the bayesian occam s razor effect murray and ghahramani named after the principle known as occam s razor which says one should pick the simplest model that adequately explains the data. one way to understand the bayesian occam s razor is to notice that the marginal likelihood can be rewritten as follows based on the chain rule of probability pd where we have dropped the conditioning on x for brevity. this is similar to a leave-one-out cross-validation estimate of the likelihood since we predict each future point given all the previous ones. course the order of the data does not matter in the above expression. if a model is too complex it will overfit the early examples and will then predict the remaining ones poorly. another way to understand the bayesian occam s razor effect is to note that probabilities must where the sum is over all possible data sets. complex sum to one. hence models which can predict many things must spread their probability mass thinly and hence will not obtain as large a probability for any given data set as simpler models. this is sometimes bayesian model selection figure a schematic illustration of the bayesian occam s razor. the broad curve corresponds to a complex model the narrow curve to a simple model and the middle curve is just right. based on figure of see also and ghahramani figure for a similar plot produced on real data. called the conservation of probability mass principle and is illustrated in figure on the horizontal axis we plot all possible data sets in order of increasing complexity in some abstract sense. on the vertical axis we plot the predictions of possible models a simple one a medium one and a complex one we also indicate the actually observed data by a vertical line. model is too simple and assigns low probability to model also assigns relatively low probability because it can predict many data sets and hence it spreads its probability quite widely and thinly. model is just right it predicts the observed data with a reasonable degree of confidence but does not predict too many other things. hence model is the most probable model. as a concrete example of the bayesian occam s razor consider the data in figure we plot polynomials of degrees and fit to n data points. it also shows the posterior over models where we use a gaussian prior section for details. there is not enough data to justify a complex model so the map model is d figure shows what happens when n now it is clear that d is the right model data was in fact generated from a quadratic. as another example figure plots log pd vs log for the polynomial ridge regression model where ranges over the same set of values used in the cv experiment. we see that the maximum evidence occurs at roughly the same point as the minimum of the test mse which also corresponds to the point chosen by cv. when using the bayesian approach we are not restricted to evaluating the evidence at a argmax pd finite grid of values. instead we can use numerical optimization to find this technique is called empirical bayes or type ii maximum likelihood section for details. an example is shown in figure we see that the curve has a similar shape to the cv estimate but it can be computed more efficiently. logev eb logev eb chapter bayesian statistics d m p methodeb m logev eb we plot polynomials of degrees and fit to n data points using empirical figure bayes. the solid green curve is the true function the dashed red curve is the prediction blue lines represent around the mean. we plot the posterior over models pdd assuming a uniform prior pd based on a figure by zoubin ghahramani. figure generated by linregebmodelselvsn. computing the marginal likelihood when discussing parameter inference for a fixed model we often wrote p m p m thus ignoring the normalization constant pdm. this is valid since pdm is constant wrt however when comparing models we need to know how to compute the marginal likelihood pdm. in general this can be quite hard since we have to integrate over all possible parameter values but when we have a conjugate prior it is easy to compute as we now show. let p be our prior where q is an unnormalized distribution and is the normalization constant of the prior. let pd be the likelihood where contains any constant factors in the likelihood. finally let p q be our poste bayesian model selection logev eb logev eb d m p logev eb methodeb m figure same as figure except now n figure generated by linregebmodelselvsn. rior where q qd is the unnormalized posterior and zn is the normalization constant of the posterior. we have p q zn pd pd pd qd zn so assuming the relevant normalization constants are tractable we have an easy way to compute the marginal likelihood. we give some examples below. pd pd n ba b pd ba b a n n n pd ba b ba b ba b so ba b pd n chapter bayesian statistics beta-binomial model let us apply the above result to the beta-binomial model. since we know p beta where b we know the normalization constant of the posterior is a and hence p pd the marginal likelihood for the beta-bernoulli model is the same as above except it is missing the term. dirichlet-multinoulli model by the same reasoning as the beta-bernoulli case one can show that the marginal likelihood for the dirichlet-multinoulli model is given by pd bn b where b k k k hence we can rewrite the above result in the following form which is what is usually presented in the literature k k k k pd k k k we will see many applications of this equation later. gaussian-gaussian-wishart model consider the case of an mvn with a conjugate niw prior. let be the normalizer for the prior zn be normalizer for the posterior and let zl be the normalizer for the bayesian model selection likelihood. then it is easy to see that pd zn n n d n d n d n d n n n this equation will prove useful later. bic approximation to log marginal likelihood in general computing the integral in equation can be quite difficult. one simple but popular approximation is known as the bayesian information criterion or bic which has the following form bic log pd dof log n log pd where dof is the number of degrees of freedom in the model and is the mle for the we see that this has the form of a penalized log likelihood where the penalty term depends on the model s complexity. see section for the derivation of the bic score. as an example consider linear regression. as we show in section the mle is given by w the corresponding y and rssn where rss wt x log likelihood is given by log pd n n hence the bic score is as follows constant terms bic n log d logn where d is the number of variables in the model. use an alternative definition of bic which we call the bic cost we want to minimize it in the statistics literature it is common to bic-cost log pd dof log n log pd in the context of linear regression this becomes bic-cost n log logn traditionally the bic score is defined using the ml estimate so it is independent of the prior. however for models such as mixtures of gaussians the ml estimate can be poorly behaved so it is better to evaluate the bic score using the map estimate as in and raftery chapter bayesian statistics the bic method is very closely related to the minimum description length or mdl principle which characterizes the score for a model in terms of how well it fits the data minus how complex the model is to define. see and yu for details. there is a very similar expression to bic mdl called the akaike information criterion or aic defined as aicmd log pd m le dofm this is derived from a frequentist framework and cannot be interpreted as an approximation to the marginal likelihood. nevertheless the form of this expression is very similar to bic. we see that the penalty for aic is less than for bic. this causes aic to pick more complex models. however this can result in better predictive accuracy. see e.g. et al. sec for further discussion on such information criteria. effect of the prior sometimes it is not clear how to set the prior. when we are performing posterior inference the details of the prior may not matter too much since the likelihood often overwhelms the prior anyway. but when computing the marginal likelihood the prior plays a much more important role since we are averaging the likelihood over all possible parameter settings as weighted by the prior. in figures and where we demonstrated model selection for linear regression we used a prior of the form pw n here is a tuning parameter that controls how strong the prior is. this parameter can have a large effect as we discuss in section intuitively if is large the weights are forced to be small so we need to use a complex model with many small parameters a high degree polynomial to fit the data. conversely if is small we will favor simpler models since each parameter is allowed to vary in magnitude by a lot. if the prior is unknown the correct bayesian procedure is to put a prior on the prior. that is we should put a prior on the hyper-parameter as well as the parametrs w. to compute the marginal likelihood we should integrate out all unknowns i.e. we should compute pdm pdwpw mp of course this requires specifying the hyper-prior. fortunately the higher up we go in the bayesian hierarchy the less sensitive are the results to the prior settings. so we can usually make the hyper-prior uninformative. a computational shortcut is to optimize rather than integrating it out. that is we use pdm pdwpw mdw pd m argmax where argmax pdwpw mdw this approach is called empirical bayes and is discussed in more detail in section this is the method used in figures and bayesian model selection bayes factor bf bf bf bf bf bf bf bf bf interpretation decisive evidence for strong evidence for moderate evidence for weak evidence for weak evidence for moderate evidence for strong evidence for decisive evidence for table jeffreys scale of evidence for interpreting bayes factors. bayes factors suppose our prior on models is uniform pm then model selection is equivalent to picking the model with the highest marginal likelihood. now suppose we just have two models we are considering call them the null hypothesis and the alternative hypothesis define the bayes factor as the ratio of marginal likelihoods is like a likelihood ratio except we integrate out the parameters which allows us to if then we prefer model otherwise we compare models of different complexity. prefer model of course it might be that is only slightly greater than in that case we are not very confident that model is better. jeffreys proposed a scale of evidence for interpreting the magnitude of a bayes factor which is shown in table this is a bayesian alternative to the frequentist concept of a alternatively we can just convert the bayes factor to a posterior over models. if we have example testing if a coin is fair suppose we observe some coin tosses and want to decide if the data was generated by a fair coin or a potentially biased coin where could be any value in let us denote the first model by and the second model by the marginal likelihood under is simply a p-value is defined as the probability the null hypothesis of observing some test statistic f as the chi-squared statistic that is as large or larger than that actually observed i.e. pvalued p d f d note that has almost nothing to do with what we really want to know which is chapter bayesian statistics bic approximation to figure log marginal likelihood for the coins example. bic approximation. figure generated by coinsmodelseldemo. where n is the number of coin tosses. the marginal likelihood under using a beta prior is b b pd we plot log vs the number of heads in figure assuming n and shape of the curve is not very sensitive to and as long as if we observe or heads the unbiased coin hypothesis is more likely than since is a simpler model has no free parameters it would be a suspicious coincidence if the coin were biased but happened to produce almost exactly headstails. however as the counts become more extreme we favor the biased coin hypothesis. note that if we plot the log bayes factor log it will have exactly the same shape since log is a constant. see also exercise in figure shows the bic approximation to log for our biased coin example from section we see that the curve has approximately the same shape as the exact log marginal likelihood which is all that matters for model selection purposes since the absolute scale is irrelevant. in particular it favors the simpler model unless the data is overwhelmingly in support of the more complex model. jeffreys-lindley paradox problems can arise when we use improper priors priors that do not integrate to for model selection hypothesis testing even though such priors may be acceptable for other purposes. for example consider testing the hypotheses vs to define the marginal density on we use the following mixture model p priors this is only meaningful if p and p are proper density functions. in this case the posterior is given by pd pd pd now suppose we use improper priors p and p then pd pd pd pd is the integrated or marginal likelihood for model i. now let i where hence thus we can change the posterior arbitrarily by choosing and as we please. note that using proper but very vague priors can cause similar problems. in particular the bayes factor will always favor the simpler model since the probability of the observed data under a complex model with a very diffuse prior will be very small. this is called the jeffreys-lindley paradox. thus it is important to use proper priors when performing model selection. note however that if and share the same prior over a subset of the parameters this part of the prior can be improper since the corresponding normalization constant will cancel out. priors the most controversial aspect of bayesian statistics is its reliance on priors. bayesians argue this is unavoidable since nobody is a tabula rasa or blank slate all inference must be done conditional on certain assumptions about the world. nevertheless one might be interested in minimizing the impact of one s prior assumptions. we briefly discuss some ways to do this below. uninformative priors if we don t have strong beliefs about what should be it is common to use an uninformative or non-informative prior and to let the data speak for itself the issue of designing uninformative priors is actually somewhat tricky. as an example of the difficulty consider a bernoulli parameter one might think that the most uninformative prior would be the uniform distribution but the posterior mean in this case is e hence one could argue that the prior wasn t completely uninformative after all. whereas the mle is chapter bayesian statistics clearly by decreasing the magnitude of the pseudo counts we can lessen the impact of the prior. by the above argument the most non-informative prior is betac c lim c which is a mixture of two equal point masses at and and lu this is also called the haldane prior. note that the haldane prior is an improper prior meaning it does not integrate to however as long as we see at least one head and at least one tail the posterior will be proper. in section we will argue that the right uninformative prior is in fact beta clearly the difference in practice between these three priors is very likely negligible. in general it is advisable to perform some kind of sensitivity analysis in which one checks how much one s conclusions or predictions change in response to change in the modeling assumptions which includes the choice of prior but also the choice of likelihood and any kind of data preprocessing. if the conclusions are relatively insensitive to the modeling assumptions one can have more confidence in the results. jeffreys priors harold designed a general purpose technique for creating non-informative priors. the result is known as the jeffreys prior. the key observation is that if p is non-informative then any re-parameterization of the prior such as h for some function h should also be non-informative. now by the change of variables formula p p d d so the prior will in general change. however let us pick p i e where i is the fisher information d log px d this is a measure of curvature of the expected negative log likelihood and hence a measure of stability of the mle section now d log px d d log px d d d squaring and taking expectations over x we have i i e i i d log px d d d d d harold jeffreys was an english mathematician statistician geophysicist and astronomer. priors so we find the transformed prior is p d d d d i so p and p are the same. some examples will make this clearer. example jeffreys prior for the bernoulli and multinoulli suppose x ber the log likelihood for a single sample is log px x log x the score function is just the gradient of the log-likelihood s d d log px x x the observed information is the second derivative of the log-likelihood j d log px x x the fisher information is the expected information i hence jeffreys prior is p beta now consider a multinoulli random variable with k states. one can show that the jeffreys prior is given by p dir note that this is different from the more obvious choices of dir k k or example jeffreys prior for location and scale parameters one can show that the jeffreys prior for a location parameter such as the gaussian mean is p thus is an example of a translation invariant prior which satisfies the property that the probability mass assigned to any interval b is the same as that assigned to any other shifted interval of the same width such as c b c. that is b c b p c c b p a a c chapter bayesian statistics this can be achieved using p which we can approximate by using a gaussian with infinite variance p note that this is an improper prior since it does not integrate to using improper priors is fine as long as the posterior is proper which will be the case provided we have seen n data points since we can nail down the location as soon as we have seen a single data point. similarly one can show that the jeffreys prior for a scale parameter such as the gaussian variance is p this is an example of a scale invariant prior which satisfies the property that the probability mass assigned to any interval b is the same as that assigned to any other interval bc which is scaled in size by some constant factor c example if we change units from meters to feet we do not want that to affect our inferences. this can be achieved by using ps bc to see this note that psds s ac bc ac logbc logac b logb loga psds we can approximate this using a degenerate gamma distribution ps the prior ps is also improper but the posterior is proper as soon as we have seen n data points we need at least two data points to estimate a variance. a robust priors in many cases we are not very confident in our prior so we want to make sure it does not have an undue influence on the result. this can be done by using robust priors and ruggeri which typically have heavy tails which avoids forcing things to be too close to the prior mean. let us consider an example from suppose x n we observe that x and we want to estimate the mle is of course which seems reasonable. the posterior mean under a uniform prior is also but now suppose we know that the prior median is and the prior quantiles are at and so p p let us also assume the prior is smooth and unimodal. it is easy to show that a gaussian prior of the form n satisfies these prior constraints. but in this case the posterior mean is given by which doesn t seem very satisfactory. now suppose we use as a cauchy prior t this also satisfies the prior constraints of our example. but this time we find numerical method integration see robustpriordemo for the code that the posterior mean is about which seems much more reasonable. mixtures of conjugate priors robust priors are useful but can be computationally expensive to use. conjugate priors simplify the computation but are often not robust and not flexible enough to encode our prior knowl priors edge. however it turns out that a mixture of conjugate priors is also conjugate and can approximate any kind of prior and hall diaconis and ylvisaker thus such priors provide a good compromise between computational convenience and flexibility. for example suppose we are modeling coin tosses and we think the coin is either fair or is biased towards heads. this cannot be represented by a beta distribution. however we can model it using a mixture of two beta distributions. for example we might use p beta beta if comes from the first distribution the coin is fair but if it comes from the second it is biased towards heads. we can represent a mixture by introducing a latent indicator variable z where z k means that comes from mixture component k. the prior has the form p pz kp k k where each p k is conjugate and pz k are called the mixing weights. one can show that the posterior can also be written as a mixture of conjugate distributions as follows p pz kdp z k k where pz kd are the posterior mixing weights given by pz kd pz kpdz k pz here the quantity pdz k is the marginal likelihood for mixture component k section example suppose we use the mixture prior p where and and we observe heads and tails. the posterior becomes p pz z if heads and tails then using equation the posterior becomes p beta beta see figure for an illustration. chapter bayesian statistics mixture of beta distributions prior posterior figure a mixture of two beta distributions. figure generated by mixbetademo. application finding conserved regions in dna and protein sequences we mentioned that dirichlet-multinomial models are widely used in biosequence analysis. let us give a simple example to illustrate some of the machinery that has developed. specifically consider the sequence logo discussed in section now suppose we want to find locations which represent coding regions of the genome. such locations often have the same letter across all sequences because of evolutionary pressure. so we need to find columns which are pure or nearly so in the sense that they are mostly all as mostly all ts mostly all cs or mostly all gs. one approach is to look for low-entropy columns these will be ones whose distribution is nearly deterministic but suppose we want to associate a confidence measure with our estimates of purity. this in this case we can let can be useful if we believe adjacent locations are conserved together. if location t is conserved and let zt otherwise. we can then add a dependence between adjacent zt variables using a markov chain see chapter for details. in any case we need to define a likelihood model pntzt where nt is the vector of counts for column t. it is natural to make this be a multinomial distribution with parameter t. since each column has a different distribution we will want to integrate out t and thus compute the marginal likelihood pnt tp tztd t but what prior should we use for t? when zt we can use a uniform prior p but what should we use if zt after all if the column is conserved it could be a pure column of as cs gs or ts. a natural approach is to use a mixture of dirichlet priors each one of which is tilted towards the appropriate corner of the simplex e.g. pntzt p dir dir since this is conjugate we can easily compute pntzt. see et al. for an hierarchical bayes application of these ideas to a real bio-sequence problem. hierarchical bayes a key requirement for computing the posterior p is the specification of a prior p where are the hyper-parameters. what if we don t know how to set in some cases we can use uninformative priors we we discussed above. a more bayesian approach is to put a prior on our priors! in terms of graphical models we can represent the situation as follows d this is an example of a hierarchical bayesian model also called a multi-level model since there are multiple levels of unknown quantities. we give a simple example below and we will see many others later in the book. example modeling related cancer rates consider the problem of predicting cancer rates in various cities example is from and albert in particular suppose we measure the number of people in various cities ni and the number of people who died of cancer in these cities xi. we assume xi binni i and we want to estimate the cancer rates i. one approach is to estimate them all separately but this will suffer from the sparse data problem of the rate of cancer due to small ni. another approach is to assume all the i are the same this is called parameter tying. the resulting pooled mle is just i ni but the assumption that all the cities have the same rate is a rather strong one. a compromise approach is to assume that the i are similar but that there may be city-specific variations. this can be modeled by assuming the i are drawn from some common distribution say i betaa b. the full joint distribution can be written as pd binxini ibeta i i where b. note that it is crucial that we infer b from the data if we just clamp it to a constant the i will be conditionally independent and there will be no information flow between them. by contrast by treating as an unknown variable we allow the data-poor cities to borrow statistical strength from data-rich ones. suppose we compute the joint posterior p from this we can get the posterior marginals p id. in figure we plot the posterior means e id as blue bars as well as the population level mean e bd shown as a red line represents the average of the i s. we see that the posterior mean is shrunk towards the pooled estimate more strongly for cities with small sample sizes ni. for example city and city both have a observed cancer incidence rate but city has a smaller population so its rate is shrunk more towards the population-level estimate it is closer to the horizontal red line than city figure shows the posterior credible intervals for i. we see that city which has a very large population people has small posterior uncertainty. consequently this city chapter bayesian statistics number of people with cancer at pop of city at linepooled mle posterior linepop mean credible interval on theta x figure results of fitting the model using the data from and albert first row number of cancer incidents xi in cities in missouri. second row population size ni. the largest city has a population of and incidents but we truncate the vertical axes of the first two rows so that the differences between the other cities are visible. third row mle i. the red line is the pooled mle. fourth row posterior mean e id. the red line is e bd the population-level mean. posterior credible intervals on the cancer rates. figure generated by cancerrateseb has the largest impact on the posterior estimate of which in turn will impact the estimate of the cancer rates for other cities. cities and which have the highest mle also have the highest posterior uncertainty reflecting the fact that such a high estimate is in conflict with the prior is estimated from all the other cities. in the above example we have one parameter per city modeling the probability the response is on. by making the bernoulli rate parameter be a function of covariates i sigmwt i x we can model multiple correlated logistic regression tasks. this is called multi-task learning and will be discussed in more detail in section empirical bayes in hierarchical bayesian models we need to compute the posterior on multiple levels of latent variables. for example in a two-level model we need to compute p pd in some cases we can analytically marginalize out this leaves is with the simpler problem of just computing p as a computational shortcut we can approximate the posterior on the hyper-parameters with a point-estimate p where argmax p since is typically much smaller than in dimensionality it is less prone to overfitting so we can safely use a uniform prior on then the estimate becomes argmax pd argmax pd empirical bayes where the quantity inside the brackets is the marginal or integrated likelihood sometimes called the evidence. this overall approach is called empirical bayes ortype-ii maximum likelihood. in machine learning it is sometimes called the evidence procedure. empirical bayes violates the principle that the prior should be chosen independently of the data. however we can just view it as a computationally cheap approximation to inference in a hierarchical bayesian model just as we viewed map estimation as an approximation to inference in the one level model d. in fact we can construct a hierarchy in which the more integrals one performs the more bayesian one becomes method maximum likelihood map estimation ml-ii bayes map-ii full bayes definition argmax pd argmax pd argmax argmax p pd pd argmax pd pd argmax pd note that eb can be shown to have good frequentist properties e.g. and louis efron so it is widely used by non-bayesians. for example the popular james-stein estimator discussed in section can be derived using eb. example beta-binomial model let us return to the cancer rates model. we can analytically integrate out the i s and write down the marginal likelihood directly as follows i pda b binxini ibeta ia bd i ba xi b ni xi ba b i various ways of maximizing this wrt a and b are discussed in having estimated a and b we can plug in the hyper-parameters to compute the posterior p i a bd in the usual way using conjugate analysis. the net result is that the posterior mean of each i is a weighted average of its local mle and the prior means which depends on b but since is estimated based on all the data each i is influenced by all the data. example gaussian-gaussian model we now study another example that is analogous to the cancer rates example except the data is real-valued. we will use a gaussian likelihood and a gaussian prior. this will allow us to write down the solution analytically. in particular suppose we have data from multiple related groups. for example xij could be the test score for student i in school j for j and i j. we want to estimate the mean score for each school j. however since the sample size nj may be small for chapter bayesian statistics some schools we can regularize the problem by using a hierarchical bayesian model where we assume j come from a common prior n the joint distribution has the following form p n j n j where we assume is known for simplicity. relax this assumption in exercise we explain how to estimate below. once we have estimated we can compute the posteriors over the j s. to do that it simplifies matters to rewrite the joint distribution in the following form exploiting the fact that nj gaussian measurements with values xij and variance j are equivalent to one measurement of value xj this yields xij with variance nj p n j j j from this it follows from the results of section that the posteriors are given by p jd j bj bjxj bj j bj j j where x and will be defined below. the quantity bj controls the degree of shrinkage towards the overall mean if the data is reliable for group j because the sample size nj is large then j will be small relative to hence bj will be small and we will put more weight on xj when we estimate j. however groups with small sample sizes will get regularized towards the overall mean more heavily. we will see an example of this below. if j for all groups j the posterior mean becomes j bx bxj x bxj x this has exactly the same form as the james stein estimator discussed in section example predicting baseball scores we now give an example of shrinkage applied to baseball batting averages from and morris we observe the number of hits for d players during the first t games. call the number of hits bi. we assume bj bint j where j is the true batting average for player j. the goal is to estimate the j. the mle is of course j xj where xj bjt is the empirical batting average. however we can use an eb approach to do better. gaussian xj n j for known to apply the gaussian shrinkage approach described above we require that the likelihood be drop the i subscript since we assume nj empirical bayes mle and shrinkage estimates mse mle mse shrunk e s m player number true shrunk mle figure mle parameters and corresponding shrunken estimates we plot the true parameters the posterior mean estimate and the mles for of the players. figure generated by shrinkagedemobaseball. since xj already represents the average for player j. however binomial likelihood. while this has the right mean e j the variance is not constant in this example we have a var t var t j t t so let us apply a variance stabilizing to xj to better match the gaussian assumption yj f now we have approximately yj n j n j we use gaussian shrinkage to estimate the j using equation with and we then transform back to get t j j the results are shown in figure in we plot the mle j and the posterior mean j. we see that all the estimates have shrunk towards the global mean in we plot the true value j the mle j and the posterior mean j. true values of j are estimated from a large number of independent games. we see that on average the shrunken estimate is much closer to the true parameters than the mle is. specifically the mean squared error j is over three times smaller using the shrinkage estimates defined by mse n j than using the mles j. estimating the hyper-parameters in this section we give an algorithm for estimating suppose initially that j is the same for all groups. in this case we can derive the eb estimate in closed form as we now show. from equation we have pxj n j j j n suppose e and var let y f then a taylor series expansions gives y f a variance stabilizing transformation is a function f such that hence var is independent of chapter bayesian statistics hence the marginal likelihood is pd n thus we can estimate the hyper-parameters using the usual mles for a gaussian. for we have xj x d d which is the overall mean. for the variance we can use moment matching is equivalent to the mle for a gaussian we simply equate the model variance to the empirical variance so since we know must be positive it is common to use the following revised estimate hence the shrinkage factor is b in the case where the j s are different we can no longer derive a solution in closed form. exercise discusses how to use the em algorithm to derive an eb estimate and exercise discusses how to perform full bayesian inference in this hierarchical model. bayesian decision theory we have seen how probability theory can be used to represent and updates our beliefs about the state of the world. however ultimately our goal is to convert our beliefs into actions. in this section we discuss the optimal way to do this. we can formalize any given statistical decision problem as a game against nature opposed to a game against other strategic players which is the topic of game theory see e.g. and leyton-brown for details. in this game nature picks a state or parameter or label y y unknown to us and then generates an observation x x which we get to see. we then have to make a decision that is we have to choose an action a from some action space a. finally we incur some loss ly a which measures how compatible our action a is with nature s hidden state y. for example we might use misclassification loss ly a a or squared loss ly a we will see some other examples below. bayesian decision theory our goal is to devise a decision procedure or policy x a which specifies the optimal action for each possible input. by optimal we mean the action that minimizes the expected loss argmin a a e a in economics u a ly a. thus the above rule becomes it is more common to talk of a utility function this is just negative loss argmax a a e a this is called the maximum expected utility principle and is the essence of what we mean by rational behavior. note that there are two different interpretations of what we mean by expected in the bayesian version which we discuss below we mean the expected value of y given the data we have seen so far. in the frequentist version which we discuss in section we mean the expected value of y and x that we expect to see in the future. in the bayesian approach to decision theory the optimal action having observed x is defined as the action a that minimizes the posterior expected loss epyx a ly apyx y y is continuous when we want to estimate a parameter vector we should replace the sum with an integral. hence the bayes estimator also called the bayes decision rule is given by arg min a a bayes estimators for common loss functions in this section we show how to construct bayes estimators for the loss functions most commonly arising in machine learning. map estimate minimizes loss the loss is defined by ly a iy a if a y if a y this is commonly used in classification problems where y is the true class label and a y is the estimate. for example in the two class case we can write the loss matrix as follows y y y y chapter bayesian statistics figure for some regions of input space where the class posteriors are uncertain we may prefer not to choose class or instead we may prefer the reject option. based on figure of section we generalize this loss function so it penalizes the two kinds of errors on the off-diagonal differently. the posterior expected loss is pa yx pyx hence the action that minimizes the expected loss is the posterior mode or map estimate y arg max y y pyx reject option in classification problems where pyx is very uncertain we may prefer to choose a reject action in which we refuse to classify the example as any of the specified classes and instead say don t know such ambiguous cases can be handled by e.g. a human expert. see figure for an illustration. this is useful in risk averse domains such as medicine and finance. we can formalize the reject option as follows. let choosing a c correspond to picking the reject action and choosing a c correspond to picking one of the classes. suppose we define the loss function as if i j and i j c if i c otherwise ly j a i r s where r is the cost of the reject action and s is the cost of a substitution error. in exercise you will show that the optimal action is to pick the reject action if the most probable class has a probability below r s otherwise you should just pick the most probable class. bayesian decision theory plots of the ly a aq vs a for q q and q figure figure generated by lossfunctionfig. posterior mean minimizes loss for continuous parameters a more appropriate loss function is squared error loss or quadratic loss defined as ly a e the posterior expected loss is given by hence the optimal estimate is the posterior mean y e ypyxdy a this is often called the minimum mean squared error estimate or mmse estimate. in a linear regression problem we have pyx n w in this case the optimal estimate given some training data d is given by e xt e that is we just plug-in the posterior mean parameter estimate. note that this is the optimal thing to do no matter what prior we use for w. posterior median minimizes loss the loss penalizes deviations from the truth quadratically and thus is sensitive to outliers. a more robust alternative is the absolute or loss ly a a figure the optimal estimate is the posterior median i.e. a value a such that p ax p ax see exercise for a proof. supervised learning consider a prediction function x y and suppose we have some cost function which gives the cost of predicting when the truth is y. we can define the loss incurred by chapter bayesian statistics taking action using this predictor when the unknown state of nature is parameters of the data generating mechanism as follows l exy pxy ly y x y this is known as the generalization error. our goal is to minimize the posterior expected loss given by p this should be contrasted with the frequentist risk which is defined in equation the false positive vs false negative tradeoff in this section we focus on binary decision problems such as hypothesis testing two-class classification object event detection etc. there are two types of error we can make a false positive false alarm which arises when we estimate y but the truth is y or a false negative missed detection which arises when we estimate y but the truth is y the loss treats these two kinds of errors equivalently. however we can consider the following more general loss matrix y y y lf p y lf n where lf n is the cost of a false negative and lf p is the cost of a false positive. the posterior expected loss for the two possible actions is given by y f n py y f p py hence we should pick class y iff y y py py lf p lf n if lf n clf p it is easy to show that we should pick y iff py where c also et al. for example if a false negative costs twice as much as false positive so c then we use a decision threshold of before declaring a positive. below we discuss roc curves which provide a way to study the fp-fn tradeoff without having to choose a specific threshold. roc curves and all that suppose we are solving a binary decision problem such as classification hypothesis testing object detection etc. also assume we have a labeled data set d yi. let bayesian decision theory truth estimate n t p f n n f p t n n t p f p f n t n n t p f p n f n t n tp fn fp tn table quantities derivable from a confusion matrix. n is the true number of positives n is the called number of positives n is the true number of negatives n is the called number of negatives. y t pntprsensitivityrecall y y f nnfnrmiss ratetype ii y f pn i t nn table estimating p yy from a confusion matrix. abbreviations fnr false negative rate fpr false positive rate tnr true negative rate tpr true positive rate. if be our decision rule where f is a measure of confidence that y should be monotonically related to py but does not need to be a probability and is some threshold parameter. for each given value of we can apply our decision rule and count the number of true positives false positives true negatives and false negatives that occur as shown in table this table of errors is called a confusion matrix. from this table we can compute the true positive rate also known as the sensitivity recall or hit rate by using t p r t pn p y we can also compute the false positive rate also called the false alarm rate or the type i error rate by using f p r f pn p y these and other definitions are summarized in tables and we can combine these errors in any way we choose to compute a loss function. however rather than than computing the tpr and fpr for a fixed threshold we can run our detector for a set of thresholds and then plot the tpr vs fpr as an implicit function of this is called a receiver operating characteristic or roc curve. see figure for an example. any system can achieve the point on the bottom left p r t p r by setting and thus classifying everything as negative similarly any system can achieve the point on the top right p r t p r by setting and thus classifying everything as positive. if a system is performing at chance level then we can achieve any point on the diagonal line t p r f p r by choosing an appropriate threshold. a system that perfectly separates the positives from negatives has a threshold that can achieve the top left corner p r t p r by varying the threshold such a system will hug the left axis and then the top axis as shown in figure the quality of a roc curve is often summarized as a single number using the area under the curve or auc. higher auc scores are better the maximum is obviously another summary statistic that is used is the equal error rate or eer also called the cross over rate defined as the value which satisfies f p r f n r. since f n r t p r we can compute the eer by drawing a line from the top left to the bottom right and seeing where it intersects the roc curve points a and b in figure lower eer scores are better the minimum is obviously chapter bayesian statistics r p t a b fpr b a i i n o s c e r p recall figure roc curves for two hypothetical classification systems. a is better than b. we plot the true positive rate vs the false positive rate as we vary the threshold we also indicate the equal error rate with the red and blue dots and the area under the curve for classifier b. a precision-recall curve for two hypothetical classification systems. a is better than b. figure generated by prhand. y y y y t p nprecisionppv f p nfdp t n n f n n table estimating py y from a confusion matrix. abbreviations fdp false discovery probability npv negative predictive value ppv positive predictive value precision recall curves when trying to detect a rare event as retrieving a relevant document or finding a face in an image the number of negatives is very large. hence comparing t p r t pn to f p r f pn is not very informative since the fpr will be very small. hence all the action in the roc curve will occur on the extreme left. in such cases it is common to plot the tpr versus the number of false positives rather than vs the false positive rate. however in some cases the very notion of negative is not well-defined. for example when detecting objects in images section if the detector works by classifying patches then the number of patches examined and hence the number of true negatives is a parameter of the algorithm not part of the problem definition. so we would like to use a measure that only talks about positives. the precision is defined as t p n py y and the recall is defined as t pn p y precision measures what fraction of our detections are actually positive and recall measures what fraction of the positives we actually detected. if yi is the predicted label and yi is the true label we can estimate precision and recall using i yi i yi i yi i yi p r a precision recall curve is a plot of precision vs recall as we vary the threshold see figure hugging the top right is the best one can do. this curve can be summarized as a single number using the mean precision over bayesian decision theory class y y y y class y y y y pooled y y y y illustration of the difference between macro- and micro-averaging. y is the true label and y table is the called label. in this example the macro-averaged precision is the micro-averaged precision is based on table of et al. recall values which approximates the area under the curve. alternatively one can quote the precision for a fixed recall level such as the precision of the first k entities recalled. this is called the average precision at k score. this measure is widely used when evaluating information retrieval systems. f-scores for a fixed threshold one can compute a single precision and recall value. these are often combined into a single statistic called the f score or score which is the harmonic mean of precision and recall r r p using equation we can write this as yi yi yi yi this is a widely used measure in information retrieval systems. to understand why we use the harmonic mean instead of the arithmetic mean consider the following scenario. suppose we recall all entries so r the precision will be the given by the prevalence py suppose the prevalence is low say py by contrast the arithmetic mean of p and r is given by harmonic mean of this strategy is only in the multi-class case for document classification problems there are two ways to generalize scores. the first is called macro-averaged and is defined as where is the score obtained on the task of distinguishing class c from all the others. the other is called micro-averaged and is defined as the score where we pool all the counts from each class s contingency table. table gives a worked example that illustrates the difference. we see that the precision of class is and of class is the macro-averaged precision is therefore whereas the micro-averaged precision is the latter is much closer to the precision of class than to the precision of class since class is five times larger than class to give equal weight to each class use macro-averaging. chapter bayesian statistics false discovery rates suppose we are trying to discover a rare phenomenon using some kind of high throughput measurement device such as a gene expression micro array or a radio telescope. we will need to make many binary decisions of the form pyi whered and n may be large. this is called multiple hypothesis testing. note that the difference from standard binary classification is that we are classifying yi based on all the data not just based on xi. so this is a simultaneous classification problem where we might hope to do better than a series of individual classification problems. how should we set the threshold a natural approach is to try to minimize the expected number of false positives. in the bayesian approach this can be computed as follows f d i pi pr. error ipi discovery where pi pyi is your belief that this object exhibits the phenomenon in question. we then define the posterior expected false discovery rate as follows f dr f d where n i ipi is the number of discovered items. given a desired fdr tolerance say one can then adapt to achieve this this is called the direct posterior probability approach to controlling the fdr et al. muller et al. in order to control the fdr it is very helpful to estimate the pi s jointly using a hierarchical bayesian model as in section rather than independently. this allows the pooling of statistical strength and thus lower fdr. see e.g. and hochberg for more information. other topics in this section we briefly mention a few other topics related to bayesian decision theory. we do not have space to go into detail but we include pointers to the relevant literature. contextual bandits a one-armed bandit is a colloquial term for a slot machine found in casinos around the world. the game is this you insert some money pull an arm and wait for the machine to stop if you re lucky you win some money. now imagine there is a bank of k such machines to choose from. which one should you use? this is called a multi-armed bandit and can be modeled using bayesian decision theory there are k possible actions and each action has an unknown reward function rk. by maintaining a belief state k prkd one can devise an optimal policy this can be compiled into a series of gittins indices this optimally solves the exploration-exploitation tradeoff which specifies how many times one should try each action before deciding to go with the winner. now consider an extension where each arm and the player has an associated feature vector call all these features x. this is called a contextual bandit e.g. scott li et al. for example the arms could represent ads or news articles which we want to show to the user and the features could represent properties of these ads or articles such bayesian decision theory as a bag of words as well as properties of the user such as demographics. if we assume a linear model for reward rk t k x we can maintain a distribution over the parameters of each arm p kd where d is a series of tuples of the form x r which specifies which arm was pulled what its features were and what the resulting outcome was r if the user clicked on the ad and r otherwise. we discuss ways to compute p kd from linear and logistic regression models in later chapters. given the posterior we must decide what action to take. one common heuristic known as ucb stands for upper confidence bound is to take the action which maximizes k k argmax k k k var and is a tuning parameter that trades off exploration where k e and exploitation. the intuition is that we should pick actions about which we believe are good k is large and or actions about which we are uncertain k is large. an even simpler method known as thompson sampling is as follows. at each step we pick action k with a probability that is equal to its probability of being the optimal action ie x max e x pk we can approximate this by drawing a single sample from the posterior t p and then choosing k despite its simplicity this has been shown to work quite argmaxk e well and li rx k t utility theory suppose we are a doctor trying to decide whether to operate on a patient or not. we imagine there are states of nature the patient has no cancer the patient has lung cancer or the patient has breast cancer. since the action and state space is discrete we can represent the loss function l a as a loss matrix such as the following surgery no surgery no cancer lung cancer breast cancer these numbers reflects the fact that not performing surgery when the patient has cancer is very bad of or depending on the type of cancer since the patient might die not performing surgery when the patient does not have cancer incurs no loss performing surgery when the patient does not have cancer is wasteful of and performing surgery when the patient does have cancer is painful but necessary it is natural to ask where these numbers come from. ultimately they represent the personal preferences or values of a fictitious doctor and are somewhat arbitrary just as some people prefer chocolate ice cream and others prefer vanilla there is no such thing as the right loss utility function. however it can be shown e.g. that any set of consistent preferences can be converted to a scalar loss utility function. note that utility can be measured on an arbitrary scale such as dollars since it is only relative values that people are often squeamish about talking about human lives in monetary terms but all decision making requires chapter bayesian statistics sequential decision theory so far we have concentrated on one-shot decision problems where we only have to make one decision and then the game ends. in setion we will generalize this to multi-stage or sequential decision problems. such problems frequently arise in many business and engineering settings. this is closely related to the problem of reinforcement learning. however further discussion of this point is beyond the scope of this book. exercises exercise proof that a mixture of conjugate priors is indeed conjugate derive equation exercise optimal threshold on classification probability consider a case where we have learned a conditional probability distribution p suppose there are only two classes and let p and p consider the loss matrix below predicted label y true label y a. show that the decision y that minimizes the expected loss is equivalent to setting a probability threshold and predicting y if and y if what is as a function of and your work. b. show a loss matrix where the threshold is your work. exercise reject option in classifiers et al. in many classification problems one has the option either of assigning x to class j or if you are too uncertain of choosing the reject option. if the cost for rejects is less than the cost of falsely classifying the object it may be the optimal action. let i mean you choose action i for i c where c is the number of classes and c is the reject action. let y j be the true unknown state of nature. define the loss function as follows iy j if i j and i j c r s if i c otherwise in otherwords you incur loss if you correctly classify you incur r loss if you choose the reject option and you incur s loss if you make a substitution error tradeoffs and one needs to use some kind of currency to compare different courses of action. insurance companies do this all the time. ross schachter a decision theorist at stanford university likes to tell a story of a school board who rejected a study on absestos removal from schools because it performed a cost-benefit analysis which was considered inhumane because they put a dollar value on children s health the result of rejecting the report was that the absestos was not removed which is surely more inhumane in medical domains one often measures utility in terms of qaly or quality-adjusted life-years instead of dollars but it s the same idea. of course even if you do not explicitly specify how much you value different people s lives your behavior will reveal your implicit values preferences and these preferences can then be converted to a real-valued scale such as dollars or qaly. inferring a utility function from behavior is called inverse reinforcement learning. bayesian decision theory decision y predict predict reject true label y a. show that the minimum risk is obtained if we decide y j if py jx py kx for all k j is the most probable class and if py jx r s otherwise we decide to reject. b. describe qualitatively what happens as r s is increased from to the relative cost of rejection increases. exercise more reject options in many applications the classifier is allowed to reject a test example rather than classifying it into one of the classes. consider for example a case in which the cost of a misclassification is but the cost of having a human manually make the decison is only we can formulate this as the following loss matrix a. suppose p is predicted to be which decision minimizes the expected loss? b. now suppose p now which decision minimizes the expected loss? c. show that in general for this loss matrix but for any posterior distribution there will be two thresholds and such that the optimal decisionn is to predict if reject if and predict if py what are these thresholds? exercise newsvendor problem consider the following classic problem in decision theory economics. suppose you are trying to decide how much quantity q of some product newspapers to buy to maximize your profits. the optimal amount will depend on how much demand d you think there is for your product as well as its cost to you c and its selling price p suppose d is unknown but has pdf f and cdf f we can evaluate the expected profit by considering two cases if d q then we sell all q items and make profit cq but if d q we only sell d items at profit cd but have wasted cq d on the unsold items. so the expected profit if we buy quantity q is cdf cqf cq df q q e q simplify this expression and then take derivatives wrt q to show that the optimal quantity q maximizes the expected profit satisfies f p c p exercise bayes factors and roc curves let b be the bayes factor in favor of model suppose we plot two roc curves one computed by thresholding b and the other computed by thresholding will they be the same or different? explain why. exercise bayes model averaging helps predictive accuracy let be a quantity that we want to predict let d be the observed data and m be a finite set of models. suppose our action is to provide a probabilistic prediction p and the loss function is l p chapter bayesian statistics log p we can either perform bayes model averaging and predict using pbm a p m m or we could predict using any single model plugin approximation show that for all models m m the posterior expected loss using bma is lower i.e. pm p e l pbm a e pm where the expectation over is with respect to p p m m hint use the non-negativity of the kl divergence. exercise mle and model selection for a discrete distribution jaakkola. let x denote the result of a coin toss for tails x for heads. the coin is potentially biased so that heads occurs with probability suppose that someone else observes the coin flip and reports to you the outcome y. but this person is unreliable and only reports the result correctly with probability i.e. pyx is given by x x y y assume that is independent of x and a. write down the joint probability distribution px y as a table in terms of b. suppose have the following dataset x y what are the mles for and justify your answer. hint note that the likelihood function factorizes px y what is pd where denotes this model? fractional form if you wish. may leave your answer in c. now consider a model with parameters representing px y xy. of these parameters are free to vary since they must sum to one. what is the mle of what is pd where denotes this model? d. suppose we are not sure which model is correct. we compute the leave-one-out cross validated log likelihood of the model and the model as follows lm log pxi yim i and i denotes the mle computed on d excluding row i. which model will cv pick and why? hint notice how the table of counts changes when you omit each training case one at a time. bayesian decision theory e. recall that an alternative to cv is to use the bic score defined as bicmd log pd m le dofm log n where dofm is the number of free parameters in the model compute the bic scores for both models log base e. which model does bic prefer? exercise posterior median is optimal estimate under loss prove that the posterior median is optimal estimate under loss. exercise decision rule for trading off fps and fns if lf n clf p show that we should pick y iff py where c frequentist statistics introduction the approach to statistical inference that we described in chapter is known as bayesian statistics. perhaps surprisingly this is considered controversial by some people whereas the application of bayes rule to non-statistical problems such as medical diagnosis spam filtering or airplane tracking is not controversial. the reason for the objection has to do with a misguided distinction between parameters of a statistical model and other kinds of unknown attempts have been made to devise approaches to statistical inference that avoid treating parameters like random variables and which thus avoid the use of priors and bayes rule. such approaches are known as frequentist statistics classical statistics or orthodox statistics. instead of being based on the posterior distribution they are based on the concept of a sampling distribution. this is the distribution that an estimator has when applied to multiple data sets sampled from the true but unknown distribution see section for details. it is this notion of variation across repeated trials that forms the basis for modeling uncertainty used by the frequentist approach. by contrast in the bayesian approach we only ever condition on the actually observed data there is no notion of repeated trials. this allows the bayesian to compute the probability of one-off events as we discussed in section perhaps more importantly the bayesian approach avoids certain paradoxes that plague the frequentist approach section nevertheless it is important to be familiar with frequentist statistics section since it is widely used in machine learning. sampling distribution of an estimator in frequentist statistics a parameter estimate is computed by applying an estimator to some data d so the parameter is viewed as fixed and the data as random which is the exact opposite of the bayesian approach. the uncertainty in the parameter estimate can be measured by computing the sampling distribution of the estimator. to understand this parameters are sometimes considered to represent true unknown physical quantities which are therefore not random. however we have seen that it is perfectly reasonable to use a probability distribution to represent one s uncertainty about an unknown constant. chapter frequentist statistics boot true mle se boot true mle se figure a bootstrap approximation to the sampling distribution of for a bernoulli distribution. we use b bootstrap samples. the n datacases were generated from ber mle with n mle with n figure generated by bootstrapdemober. i where xs i p concept imagine sampling many different data sets ds from some true model p i.e. let ds is the true parameter. here s indexes the sampled data set and n is the size of each such dataset. now apply the estimator to each ds to get a set of estimates as we let s the distribution induced on is the sampling distribution of the estimator. we will discuss various ways to use the sampling distribution in later sections. but first we sketch two approaches for computing the sampling distribution itself. and bootstrap the bootstrap is a simple monte carlo technique to approximate the sampling distribution. this is particularly useful in cases where the estimator is a complex function of the true parameters. the idea is simple. if we knew the true parameters we could generate many s fake i p datasets each of size n from the true distribution xs for s s i n we could then compute our estimator from each sample s f and use the empirical distribution of the resulting samples as our estimate of the sampling distribution. since is unknown the idea of the parametric bootstrap is to generate the samples using instead. an alternative called the non-parametric bootstrap is to sample the xs i replacement from the original data d and then compute the induced distribution as before. some methods for speeding up the bootstrap when applied to massive data sets are discussed in et al. figure shows an example where we compute the sampling distribution of the mle for a bernoulli using the parametric bootstrap. using the non-parametric bootstrap are essentially the same. we see that the sampling distribution is asymmetric and therefore quite far from gaussian when n when n the distribution looks more gaussian as theory suggests below. computed by the bootstrap and parameter values sampled from the posterior s p a natural question is what is the connection between the parameter estimates s sampling distribution of an estimator conceptually they are quite different. but in the common case that that the prior is not very strong they can be quite similar. for example figure shows an example where we compute the posterior using a uniform prior and then sample from it. we see that the posterior and the sampling distribution are quite similar. so one can think of the bootstrap distribution as a poor man s posterior see et al. for details. however perhaps surprisingly bootstrap can be slower than posterior sampling. the reason is that the bootstrap has to fit the model s times whereas in posterior sampling we usually only fit the model once find a local mode and then perform local exploration around the mode. such local exploration is usually much faster than fitting a model from scratch. large sample theory for the mle in some cases the sampling distribution for some estimators can be computed analytically. in particular it can be shown that under certain conditions as the sample size tends to infinity the sampling distribution of the mle becomes gaussian. informally the requirement for this result to hold is that each parameter in the model gets to see an infinite amount of data and that the model be identifiable. unfortunately this excludes many of the models of interest to machine learning. nevertheless let us assume we are in a simple setting where the theorem holds. the center of the gaussian will be the mle but what about the variance of this gaussian? intuitively the variance of the estimator will be related to the amount of curvature of the likelihood surface at its peak. if the curvature is large the peak will be sharp and the variance low in this case the estimate is well determined by contrast if the curvature is small the peak will be nearly flat so the variance is high. likelihood evaluated at some point let us now formalize this intuition. define the score function as the gradient of the log s log pd define the observed information matrix as the gradient of the negative score function or equivalently the hessian of the nll j s log pd in this becomes j d d log pd this is just a measure of curvature of the log-likelihood function at since we are studying the sampling distribution d xn is a set of random variables. the fisher information matrix is defined to be the expected value of the observed information e in j this is not the usual definition but is equivalent to it under standard assumptions. more precisely the standard definition is as follows just give the scalar case to simplify notation i that is the variance of the score function. if is the mle it is easy to see that e d log px var d d log px d chapter frequentist statistics f often n where e is the expected value of the function f when applied to data sampled from representing the true parameter that generated the data is assumed known so we just write in in for short. furthermore it is easy to see that in because the log-likelihood for a sample of size n is just n times steeper than the log-likelihood for a sample of size so we can drop the subscript and just write i this is the notation that is usually used. now let mled be the mle where d n in it can be shown that as n e.g. for a proof. we say that the sampling distribution of the mle is asymptotically normal. in the mle? unfortunately distribution. however we can approximate the sampling distribution by replacing consequently the approximate standard errors of k are given by what about the variance of the mle which can be used as some measure of confidence is unknown so we can t evaluate the variance of the sampling with sek in kk for example from equation we know that the fisher information for a binomial sampling model is i so the approximate standard error of the mle is se n i n in where n under a uniform prior. i xi. compare this to equation which is the posterior standard deviation frequentist decision theory in frequentist or classical decision theory there is a loss function and a likelihood but there is no prior and hence no posterior or posterior expected loss. thus there is no automatic way of deriving an optimal estimator unlike the bayesian case. instead in the frequentist approach we are free to choose any estimator or decision procedure x a we the gradient must be zero at a maximum so the variance reduces to the expected square of the score function d log px i d d log px e is a much more intuitive quantity than the variance of the score. in practice the frequentist approach is usually only applied to one-shot statistical decision problems such as classification regression and parameter estimation since its non-constructive nature makes it difficult to apply to sequential decision problems which adapt to data online. so now the fisher information reduces to the expected second derivative of the nll which d log px d that e it can be shown frequentist decision theory having chosen an estimator we define its expected loss or risk as follows r ep d l dp d l d d where d is data sampled from nature s distribution which is represented by parameter in other words the expectation is wrt the sampling distribution of the estimator. compare this to the bayesian posterior expected loss ep a we see that the bayesian approach averages over is unknown and conditions on d is known whereas the frequentist approach averages over d ignoring the observed data and conditions on not only is the frequentist definition unnatural it cannot even be computed because is unknown. consequently we cannot compare different estimators in terms of their frequentist risk. we discuss various solutions to this below. l ap is unknown. bayes risk how do we choose amongst estimators? we need some way to convert r into a single measure of quality r which does not depend on knowing one approach is to put a prior on and then to define bayes risk or integrated risk of an estimator as follows rb ep r a bayes estimator or bayes decision rule is one which minimizes the expected risk b argmin rb note that the integrated risk is also called the preposterior risk since it is before we have seen the data. minimizing this can be useful for experiment design. we will now prove a very important theorem that connects the bayesian and frequentist approaches to decision theory. theorem a bayes estimator can be obtained by minimizing the posterior expected loss for each x. proof. by switching the order of integration we have y x x y y x x rb px ly y p ly y ly px chapter frequentist statistics r r r figure risk functions for two decision procedures and since has lower worst case risk it is the minimax estimator even though has lower risk for most values of thus minimax estimators are overly conservative. to minimize the overall expectation we just minimize the term inside for each x so our decision rule is to pick bx argmin a a hence we see that the picking the optimal action on a case-by-case basis in the bayesian approach is optimal on average in the frequentist approach. in other words the bayesian approach provides a good way of achieving frequentist goals. in fact one can go further and prove the following. theorem every admissable decision rule is a bayes decision rule with respect to some possibly improper prior distribution. this theorem shows that the best way to minimize frequentist risk is to be bayesian! see and smith for further discussion of this point. minimax risk obviously some frequentists dislike using bayes risk since it requires the choice of a prior this is only in the evaluation of the estimator not necessarily as part of its construction. an alternative approach is as follows. define the maximum risk of an estimator as rmax max r a minimax rule is one which minimizes the maximum risk mm argmin rmax frequentist decision theory for example in figure we see that has lower worst-case risk than ranging over all possible values of so it is the minimax estimator section for an explanation of how to compute a risk function for an actual model. minimax estimators have a certain appeal. however computing them can be hard. and furthermore they are very pessimistic. in fact one can show that all minimax estimators are equivalent to bayes estimators under a least favorable prior. in most statistical situations game theoretic ones assuming nature is an adversary is not a reasonable assumption. admissible estimators the basic problem with frequentist decision theory is that it relies on knowing the true distribution p in order to evaluate the risk. however it might be the case that some estimators in particular if r r for all are worse than others regardless of the value of then we say that dominates the domination is said to be strict if the inequality is strict for some an estimator is said to be admissible if it is not strictly dominated by any other estimator. example let us give an example based on and smith consider the problem of estimating the mean of a gaussian. we assume the data is sampled from xi n and use quadratic loss l the corresponding risk function is the mse. some possible decision rules or estimators are as follows x the sample mean x the sample median a fixed value the posterior mean under a n prior n n x n wx w for we consider a weak prior and a stronger prior the prior mean is some fixed value. we assume is known. is the same as with an infinitely strong prior we know the true parameter into squared bias plus variance can do this since in this toy example in section we show that the mse can be decomposed let us now derive the risk functions analytically. m se var the sample mean is unbiased so its risk is m se var n chapter frequentist statistics risk functions for mle median fixed risk functions for mle median fixed r r figure risk functions for estimating the mean of a gaussian using data sampled n the solid dark blue horizontal line is the mle the solid light blue curved line is the posterior mean when left n samples. right n samples. based on figure of and smith figure generated by riskfngauss. the sample median is also unbiased. one can show that the variance is approximately so m se for the variance is zero so m se finally for the posterior mean we have m se w w n e n these functions are plotted in figure for n we see that in general the best estimator depends on the value of is very close to then which is unknown. if is within some reasonable range around then the just predicts is best. posterior mean which combines the prior guess of with the actual data is best. if is far from the mle is best. none of this should be suprising a small amount of shrinkage the posterior mean with a weak prior is usually desirable assuming our prior mean is sensible. what is more surprising is that the risk of decision rule median is always higher than that of mean for every value of consequently the sample median is an if frequentist decision theory inadmissible estimator for this particular problem the data is assumed to come from a gaussian. in practice the sample median is often better than the sample mean because it is more robust to outliers. one can show that the median is the bayes estimator squared loss if we assume the data comes from a laplace distribution which has heavier tails than a gaussian. more generally we can construct robust estimators by using flexible models of our data such as mixture models or non-parametric density estimators and then computing the posterior mean or median. stein s paradox suppose we have n iid random variables xi n i and we want to estimate the i. the obvious estimator is the mle which in this case sets i xi. it turns out that this is an inadmissible estimator under quadratic loss when n to show this it suffices to construct an estimator that is better. the james-stein estimator is one such estimator and is defined as follows i bx bxi x bxi x xi and b is some tuning constant. this estimate shrinks the derive this estimator using an empirical bayes approach in where x n i towards the overall mean. section it can be shown that this shrinkage estimator has lower frequentist risk than the mle mean for n this is known as stein s paradox. the reason it is called a paradox is illustrated by the following example. suppose i is the true iq of student i and xi is his test score. why should my estimate of i depend on the global mean x and hence on some other student s scores? one can create even more paradoxical examples by making the different dimensions be qualitatively different e.g. is my iq is the average rainfall in vancouver etc. the solution to the paradox is the following. if your goal is to estimate just i you cannot do better than using xi but if the goal is to estimate the whole vector and you use squared error as your loss function then shrinkage helps. to see this suppose we want to estimate from a single sample x n i. a simple estimate is but this will overestimate the result since e e i i n i consequently we can reduce our risk by pooling information even from unrelated sources and shrinking towards the overall mean. in section we give a bayesian explanation for this. see also and morris admissibility is not enough it seems clear that we can restrict our search for good estimators to the class of admissible estimators. but in fact it is easy to construct admissible estimators as we show in the following example. chapter frequentist statistics then r and r theorem let x n and consider estimating under squared loss. let a constant independent of the data. this is an admissible estimator. proof. suppose not. then there is some other estimator with smaller risk so r r where the inequality must be strict for some suppose the true parameter is since r r for all and r we have r and hence thus the only way can avoid having higher risk than at some specific point is by being equal to hence there is no other estimator with strictly lower risk so is admissible. desirable properties of estimators since frequentist decision theory does not provide an automatic way to choose the best estimator we need to come up with other heuristics for choosing amongst them. in this section we discuss some properties we would like estimators to have. unfortunately we will see that we cannot achieve all of these properties at the same time. consistent estimators an estimator is said to be consistent if it eventually recovers the true parameters that generated as the arrow the data as the sample size goes to infinity i.e. denotes convergence in probability. of course this concept only makes sense if the data actually comes from the specified model with parameters which is not usually the case with real data. nevertheless it can be a useful theoretical property. p it can be shown that the mle is a consistent estimator. the intuitive reason is that maxi is the true where p mizing likelihood is equivalent to minimizing kl distribution and p is our estimate. we can achieve kl divergence iff unbiased estimators the bias of an estimator is defined as bias epd where is the true parameter value. if the bias is zero the estimator is called unbiased. this means the sampling distribution is centered on the true parameter. for example the mle for a gaussian mean is unbiased bias e e n xi n n if the model is unidentifiable the mle may select a set of parameters that is different from the true parameters but for which the induced distribution p is the same as the exact distribution. such parameters are said to be likelihood equivalent. desirable properties of estimators however the mle for a gaussian variance is not an unbiased estimator of in fact one can show that e n n however the following estimator n n n n is an unbiased estimator which we can easily prove as follows e n e n n n n n n in matlab varx returns n the difference will be negligible. n whereas returns mle. for large enough although the mle may sometimes be a biased estimator one can show that asymptotically it is always unbiased. is necessary for the mle to be a consistent estimator. although being unbiased sounds like a desirable property this is not always true. see sec tion and for discussion of this point. minimum variance estimators it seems intuitively reasonable that we want our estimator to be unbiased we shall give some arguments against this claim below. however being unbiased is not enough. for example suppose we want to estimate the mean of a gaussian from d xn. the estimator that just looks at the first data point is an unbiased estimator but will generally be further from than the empirical mean x is also unbiased. so the variance of an estimator is also important. a natural question is how long can the variance go? a famous result called the cramerrao lower bound provides a lower bound on the variance of any unbiased estimator. more precisely theorem inequality. let xn px and xn be an unbiased estimator of then under various smoothness assumptions on px we have var ni where i is the fisher information matrix section a proof can be found e.g. in it can be shown that the mle achieves the cramer rao lower bound and hence has the smallest asymptotic variance of any unbiased estimator. thus mle is said to be asymptotically optimal. chapter frequentist statistics the bias-variance tradeoff e although using an unbiased estimator seems like a good idea this is not always the case. to see why suppose we use quadratic loss. as we showed above the corresponding risk is the mse. we now derive a very useful decomposition of the mse. expectations and variances are wrt the true distribution pd but we drop the explicit conditioning for notational brevity. let denote the estimate and e denote the expected value of the estimate we vary d. then we have var e e e in words mse variance this is called the bias-variance tradeoff e.g. et al. what it means is that it might be wise to use a biased estimator so long as it reduces our variance assuming our goal is to minimize squared error. example estimating a gaussian mean let us give an example based on suppose we want to estimate the mean of a gaussian from x xn we assume the data is sampled from xi n an obvious estimate is the mle. this has a bias of and a variance of var n but we could also use a map estimate. in section we show that the map estimate under a gaussian prior of the form n is given by x n wx w x n n where w controls how much we trust the mle compared to our prior. is also the posterior mean since the mean and mode of a gaussian are the same. the bias and variance are given by e x w w w var x n desirable properties of estimators sampling distribution truth prior n mse of postmean mse of mle sample size e s m e v i t l a e r left sampling distribution of the map estimate with different prior strengths figure mle corresponds to right mse relative to that of the mle versus sample size. based on figure of figure generated by samplingdistgaussshrinkage. let us assume that our prior is slightly misspecified so we use whereas the truth is in figure we see that the sampling distribution of the map estimate for so although the map estimate is biased w it has lower variance. is biased away from the truth but has lower variance narrower than that of the mle. in figure we plot mse xmsex vs n we see that the map estimate has lower mse than the mle especially for small sample size for the case corresponds to the mle and the case corresponds to a strong prior which hurts performance because the prior mean is wrong. it is clearly important to tune the strength of the prior a topic we discuss later. example ridge regression another important example of the bias variance tradeoff arises in ridge regression which we discuss in section in brief this corresponds to map estimation for linear regression under a gaussian prior pw n the zero-mean prior encourages the weights to be small which reduces overfitting the precision term controls the strength of this prior. setting results in the mle using results in a biased estimate. to illustrate the effect on the variance consider a simple example. figure on the left plots each individual fitted curve and on the right plots the average fitted curve. we see that as we increase the strength of the regularizer the variance decreases but the bias increases. bias-variance tradeoff for classification if we use loss instead of squared error the above analysis breaks down since the frequentist risk is no longer expressible as squared bias plus variance. in fact one can show of et al. that the bias and variance combine multiplicatively. if the estimate is on chapter frequentist statistics ln ln ln ln figure illustration of bias-variance tradeoff for ridge regression. we generate data sets from the true function shown in solid green. left we plot the regularized fit for different data sets. we use linear regression with a gaussian rbf expansion with centers evenly spread over the interval. right we plot the average of the fits averaged over all datasets. top row strongly regularized we see that the individual fits are similar to each other variance but the average is far from the truth bias. bottom row lightly regularized we see that the individual fits are quite different from each other variance but the average is close to the truth bias. based on figure figure generated by the correct side of the decision boundary then the bias is negative and decreasing the variance will decrease the misclassification rate. but if the estimate is on the wrong side of the decision boundary then the bias is positive so it pays to increase the variance this little known fact illustrates that the bias-variance tradeoff is not very useful for classification. it is better to focus on expected loss below not directly on bias and variance. we can approximate the expected loss using cross validatinon as we discuss in section empirical risk minimization frequentist decision theory suffers from the fundamental problem that one cannot actually compute the risk function since it relies on knowing the true data distribution. contrast the bayesian posterior expected loss can always be computed since it conditions on the the data rather than conditioning on however there is one setting which avoids this problem and that is where the task is to predict observable quantities as opposed to estimating hidden variables or parameters. that is instead of looking at loss functions of the form l where is the true but unknown parameter and is our estimator let us look at loss empirical risk minimization functions of the form ly where y is the true but unknown response and is our prediction given the input x. in this case the frequentist risk becomes rp exy p ly y x y where p represents nature s distribution of course this distribution is unknown but a simple approach is to use the empirical distribution derived from some training data to approximate p i.e. p y pempx y n xi yi we then define the empirical risk as follows rempdd rpemp n lyi in the case of loss ly iy this becomes the misclassification rate. in the case of squared error loss ly this becomes the mean squared error. we define the task of empirical risk minimization or erm as finding a decision procedure a classification rule to minimize the empirical risk erm argmin rempd in the unsupervised case we eliminate all references to y and replace ly with lx where for example lx which measures the reconstruction error. we can define the decision rule using encodex as in vector quantization or pca finally we define the empirical risk as lxi rempd n of course we can always trivially minimize this risk by setting x so it is critical that the encoder-decoder go via some kind of bottleneck. regularized risk minimization note that the empirical risk is equal to the bayes risk if our prior about nature s distribution is that it is exactly equal to the empirical distribution e pemp rempd therefore minimizing the empirical risk will typically result in overfitting. necessary to add a complexity penalty to the objective function it is therefore often rempd c chapter frequentist statistics where c measures the complexity of the prediction function and controls the strength of the complexity penalty. this approach is known as regularized risk minimization note that if the loss function is negative log likelihood and the regularizer is a negative log prior this is equivalent to map estimation. the two key issues in rrm are how do we measure complexity and how do we pick for a linear model we can define the complexity of in terms of its degrees of freedom discussed in section for more general models we can use the vc dimension discussed in section to pick we can use the methods discussed in section structural risk minimization the regularized risk minimization principle says that we should fit the model for a given complexity penalty by using argmin c but how should we pick we cannot using the training set since this will underestimate the true risk a problem known as optimism of the training error. as an alternative we can use the following rule known as the structural risk minimization principle argmin r where r is an estimate of the risk. there are two widely used estimates cross validation and theoretical upper bounds on the risk. we discuss both of these below. estimating the risk using cross validation we can estimate the risk of some estimator using a validation set. if we don t have a separate validation set we can use cross validation as we briefly discussed in section more precisely cv is defined as follows. let there be n data cases in the training set. denote the data in the k th test fold by dk and all the other data by d k. stratified cv these folds are chosen so the class proportions discrete labels are present are roughly equal in each fold. let f be a learning algorithm or fitting function that takes a dataset and a model index m could a discrete index such as the degree of a polynomial or a continuous index such as the strength of a regularizer and returns a parameter vector m fd m finally let p be a prediction function that takes an input and a parameter vector and returns a prediction y px f thus the combined fit-predict cycle is denoted as fmxd pxfd m empirical risk minimization i dk the k-fold cv estimate of the risk of fm is defined by rmd k n l k m mx k m be note that we can call the fitting algorithm once per fold. let f k the function that was trained on all the data except for the test data in fold k. then we can rewrite the cv estimate as n l yi f k mxi l yi f ki m rmd k i dk n where ki is the fold in which i is used as test data. model that was trained on data that does not contain xi. in other words we predict yi using a of k n the method is known as leave one out cross validation or loocv. in this case n the estimated risk becomes rmd n n yi f i m l mx pxfd i m. this requires fitting the model n times where for f i where f i m we omit the i th training case. fortunately for some model classes and loss functions linear models and quadratic loss we can fit the model once and analytically remove the effect of the i th training case. this is known as generalized cross validation or gcv. example using cv to pick for ridge regression i dk as a concrete example consider picking the strength of the regularizer in penalized linear regression. we use the following rule r k arg min min max where min max is a finite range of values that we search over and r k is the k-fold cv estimate of the risk of using given by r k xt w k is the prediction function trained on data excluding fold k and is the map estimate. figure gives an example where f k w arg minw n llwd of a cv estimate of the risk vs log where the loss function is squared error. lyi f k when performing classification we usually use loss. in this case we optimize a convex upper bound on the empirical risk to estimate w m but we optimize cv estimate of the risk itself to estimate we can handle the non-smooth loss function when estimating because we are using brute-force search over the entire space. when we have more than one or two tuning parameters this approach becomes infeasible. in such cases one can use empirical bayes which allows one to optimize large numbers of hyper-parameters using gradient-based optimizers instead of brute-force search. see section for details. chapter frequentist statistics fold cross validation ntrain mean squared error train mse test mse e s m log lambda log lambda figure mean squared error for penalized degree polynomial regression vs log regularizer. same as in figures except now we have n training points instead of the stars correspond to the values used to plot the functions in figure cv estimate. the vertical scale is truncated for clarity. the blue line corresponds to the value chosen by the one standard error rule. figure generated by linregpolyvsregdemo. the one standard error rule the above procedure estimates the risk but does not give any measure of uncertainty. a standard frequentist measure of uncertainty of an estimate is the standard error of the mean defined by n n se where is an estimate of the variance of the loss li lyi f ki m l n n li note that measures the intrinsic variability of li across samples whereas se measures our uncertainty about the mean l. suppose we apply cv to a set of models and compute the mean and se of their estimated risks. a common heuristic for picking a model from these noisy estimates is to pick the value which corresponds to the simplest model whose risk is no more than one standard error above the risk of the best model this is called the one-standard error rule et al. for example in figure we see that this heuristic does not choose the lowest point on the curve but one that is slightly to its right since that corresponds to a more heavily regularized model with essentially the same empirical performance. empirical risk minimization cv for model selection in non-probabilistic unsupervised learning if we are performing unsupervised learning we must use a loss function such as lx which measures reconstruction error. here is some encode-decode scheme. however as we discussed in section we cannot use cv to determine the complexity of since we will always get lower loss with a more complex model even if evaluated on the test set. this is because more complex models will compress the data less and induce less distortion. consequently we must either use probabilistic models or invent other heuristics. upper bounding the risk using statistical learning theory the principle problem with cross validation is that it is slow since we have to fit the model multiple times. this motivates the desire to compute analytic approximations or bounds to the generalization error. this is the studied in the field of statistical learning theory more precisely slt tries to bound the risk rp h for any data distribution p and hypothesis h h in terms of the empirical risk rempd h the sample size n and the size of the hypothesis space h. let us initially consider the case where the hypothesis space is finite with size dimh in other words we are selecting a model hypothesis from a finite list rather than optimizing real-valued parameters then we can prove the following. theorem for any data distribution p and any dataset d of size n drawn from p the probability that our estimate of the error rate will be more than wrong in the worst case is upper bounded as follows h rp h dimhe p max h h proof. to prove this we need two useful results. first hoeffding s inequality which states that if xn ber then for any p where x n events then p d xi. second the union bound which says that if ad are a set of finally for notational brevity let rh rh p be the true risk and rn rempd h p be the empirical risk. using these results we have p max h h rn rh p h h h h h h p rn rh rn rh dimhe chapter frequentist statistics ths bound tells us that the optimism of the training error increases with dimh but decreases with n as is to be expected. if the hypothesis space h is infinite we have real-valued parameters we cannot use dimh instead we can use a quantity called the vapnik-chervonenkis or vc dimension of the hypothesis class. see for details. stepping back from all the theory the key intuition behind statistical learning theory is quite if the hypothesis space h is very simple. suppose we find a model with low empirical risk. big relative to the data size then it is quite likely that we just got lucky and were given a data set that is well-modeled by our chosen function by chance. however this does not mean that such a function will have low generalization error. but if the hypothesis class is sufficiently constrained in size andor the training set is sufficiently large then we are unlikely to get lucky in this way so a low empirical risk is evidence of a low true risk. note that optimism of the training error does not necessarily increase with model complexity but it does increase with the number of different models that are being searched over. the advantage of statistical learning theory compared to cv is that the bounds on the risk are quicker to compute than using cv. the disadvantage is that it is hard to compute the vc dimension for many interesting models and the upper bounds are usually very loose see and langford one can extend statistical learning theory by taking computational complexity of the learner into account. this field is called computational learning theory or colt. most of this work focuses on the case where h is a binary classifier and the loss function is loss. if we observe a low empirical risk and the hypothesis space is suitably small then we can say that our estimated function is probably approximately correct or pac. a hypothesis space is said to be efficiently pac-learnable if there is a polynomial time algorithm that can identify a function that is pac. see and vazirani for details. surrogate loss functions minimizing the loss in the erm rrm framework is not always easy. for example we might want to optimize the auc or scores. or more simply we might just want to minimize the loss as is common in classification. unfortunately the risk is a very non-smooth objective and hence is hard to optimize. one alternative is to use maximum likelihood estimation instead since log-likelihood is a smooth convex upper bound on the risk as we show below. to see this consider binary logistic regression and let yi suppose our decision function computes the log-odds ratio py w py w f log wt xi i then the corresponding probability distribution on the output label is pyixi w sigmyi i let us define the log-loss as as l nlly log pyx w e y pathologies of frequentist statistics s s o l hinge logloss figure y the vertical axis is the loss. the log loss uses log base figure generated by hingelossplot. illustration of various loss functions for binary classification. the horizontal axis is the margin now consider computing the most probable label which is equivalent to using y if it is clear that minimizing the average log-loss is equivalent to maximizing the likelihood. i and y if i the loss of our function becomes iy y iy figure plots these two loss functions. we see that the nll is indeed an upper bound on the loss. log-loss is an example of a surrogate loss function. another example is the hinge loss l hingey y see figure for a plot. we see that the function looks like a door hinge hence its name. this loss function forms the basis of a popular classification method known as support vector machines which we will discuss in section the surrogate is usually chosen to be a convex upper bound since convex functions are easy to minimize. see e.g. et al. for more information. pathologies of frequentist statistics i believe that it would be very difficult to persuade an intelligent person that current statistical practice was sensible but that there would be much less difficulty with an approach via likelihood and bayes theorem. george box frequentist statistics exhibits various forms of weird and undesirable behaviors known as pathologies. we give a few examples below in order to caution the reader these and other examples are explained in more detail in lindley and phillips lindley berger jaynes minka chapter frequentist statistics counter-intuitive behavior of confidence intervals a confidence interval is an interval derived from the sampling distribution of an estimator a bayesian credible interval is derived from the posterior of a parameter as we discussed in section more precisely a frequentist confidence interval for some parameter is defined by the following un-natural expression u d u d d that is if we sample hypothetical future data d from then d u d is a confidence interval if the parameter lies inside this interval percent of the time. in bayesian statistics we condition on what is known namely the observed data d and average over what is not known namely the parameter in frequentist statistics we do exactly the opposite we condition on what is unknown namely the true parameter value and average over hypothetical future data sets d. this counter-intuitive definition of confidence intervals can lead to bizarre results. consider the following example from suppose we draw two integers d from let us step back for a moment and think about what is going on. px if x if x otherwise if we would expect the following outcomes each with probability let m and define the following confidence interval ud m for the above samples this yields hence equation is clearly a ci since is contained in of these intervals. however if d then p so we know that must be yet we only have confidence in this fact. another less contrived example is as follows. suppose we want to estimate the parameter of a bernoulli distribution. let x xi be the sample mean. the mle is x. an n xn is approximate confidence interval for a bernoulli parameter is x called a wald interval and is based on a gaussian approximation to the binomial distribution compare to equation now consider a single trial where n and the mle is which overfits as we saw in section but our confidence interval is also which seems even worse. it can be argued that the above flaw is because we approximated the true sampling distribution with a gaussian or because the sample size was to small or the parameter too extreme however the wald interval can behave badly even for large n and non-extreme parameters et al. pathologies of frequentist statistics p-values considered harmful suppose we want to decide whether to accept or reject some baseline model which we will call the null hypothesis. we need to define some decision rule. in frequentist statistics it is standard to first compute a quantity called the p-value which is defined as the probability the null of observing some test statistic f as the chi-squared statistic that is as large or larger than that actually pvalued p d f d this quantity relies on computing a tail area probability of the sampling distribution we give an example of how to do this below. given the p-value we define our decision rule as follows we reject the null hypothesis iff the p-value is less than some threshold such as if we do reject it we say the difference between the observed test statistic and the expected test statistic is statistically significant at level this approach is known as null hypothesis significance testing ornhst. this procedure guarantees that our expected type i positive error rate is at most this is sometimes interpreted as saying that frequentist hypothesis testing is very conservative since it is unlikely to accidently reject the null hypothesis. but in fact the opposite is the case because this method only worries about trying to reject the null it can never gather evidence in favor of the null no matter how large the sample size. because of this p-values tend to overstate the evidence against the null and are thus very trigger happy in general there can be huge differences between p-values and the quantity that we really care about which is the posterior probability of the null hypothesis given the data in particular sellke et al. show that even if the p-value is as slow as the posterior probability of is at least and often much higher. so frequentists often claim to have significant evidence of an effect that cannot be explained by the null hypothesis whereas bayesians are usually more conservative in their claims. for example p-values have been used to prove that esp perception is real et al. even though esp is clearly very improbable. for this reason p-values have been banned from certain medical journals another problem with p-values is that their computation depends on decisions you make about when to stop collecting data even if these decisions don t change the data you actually observed. for example suppose i toss a coin n times and observe s successes and f failures so n s f. in this case n is fixed and s hence f is random. the relevant sampling model is the binomial binsn s n s let the null hypothesis be that the coin is fair where is the probability of success the one-sided p-value using test statistic ts s is p s the reason we cannot just compute the probability of the observed value of the test statistic is that this will have probability zero under a pdf. the p-value is defined in terms of the cdf so is always a number between and chapter frequentist statistics the two-sided p-value is in either case the p-value is larger than the magical threshold so a frequentist would not reject the null hypothesis. now suppose i told you that i actually kept tossing the coin until i observed f tails. in this case f is fixed and n hence s n f is random. the probability model becomes the negative binomial distribution given by negbinomsf s f f where f n s. note that the term which depends on is the same in equations and so the posterior over would be the same in both cases. however these two interpretations of the same data give different p-values. in particular under the negative binomial model we get p s so the p-value is and suddenly there seems to be significant evidence of bias in the coin! obviously this is ridiculous the data is the same so our inferences about the coin should be the same. after all i could have chosen the experimental protocol at random. it is the outcome of the experiment that matters not the details of how i decided which one to run. although this might seem like just a mathematical curiosity this also has significant practical in particular the fact that the stopping rule affects the computation of the pimplications. value means that frequentists often do not terminate experiments early even when it is obvious what the conclusions are lest it adversely affect their statistical analysis. if the experiments are costly or harmful to people this is obviously a bad idea. perhaps it is not surprising then that the us food and drug administration which regulates clinical trials of new drugs has recently become supportive of bayesian since bayesian methods are not affected by the stopping rule. the likelihood principle the fundamental reason for many of these pathologies is that frequentist inference violates the likelihood principle which says that inference should be based on the likelihood of the observed data not based on hypothetical future data that you have not observed. bayes obviously satisfies the likelihood principle and consequently does not suffer from these pathologies. a compelling argument in favor of the likelihood principle was presented in who showed that it followed automatically from two simpler principles. the first of these is the sufficiency principle which says that a sufficient statistic contains all the relevant information see ian. pathologies of frequentist statistics about an unknown parameter this is true by definition. the second principle is known as weak conditionality which says that inferences should be based on the events that happened not which might have happened. to motivate this consider an example from suppose we need to analyse a substance and can send it either to a laboratory in new york or in california. the two labs seem equally good so a fair coin is used to decide between them. the coin comes up heads so the california lab is chosen. when the results come back should it be taken into account that the coin could have come up tails and thus the new york lab could have been used? most people would argue that the new york lab is irrelevant since the tails event didn t happen. this is an example of weak conditionality. given this principle one can show that all inferences should only be based on what was observed which is in contrast to standard frequentist procedures. see and wolpert for further details on the likelihood principle. why isn t everyone a bayesian? given these fundamental flaws of frequentist statistics and the fact that bayesian methods do not have such flaws an obvious question to ask is why isn t everyone a bayesian? the statistician bradley efron wrote a paper with exactly this title his short paper is well worth reading for anyone interested in this topic. below we quote his opening section the title is a reasonable question to ask on at least two counts. first of all everone used to be a bayesian. laplace wholeheatedly endorsed bayes s formulation of the inference problem and most scientists followed suit. this included gauss whose statistical work is usually presented in frequentist terms. a second and more important point is the cogency of the bayesian argument. modern statisticians following the lead of savage and de finetti have advanced powerful theoretical arguments for preferring bayesian inference. a byproduct of this work is a disturbing catalogue of inconsistencies in the frequentist point of view. nevertheless everyone is not a bayesian. the current era is the first century in which statistics has been widely used for scientific reporting and in fact statistics is mainly non-bayesian. however lindley predicts a change for the century. time will tell whether lindley was right.... exercises exercise pessimism of loocv suppose we have a completely random labeled dataset the features x tell us nothing about the class labels y with examples of class and examples of class where what is the best misclassification rate any method can achieve? what is the estimated misclassification rate of the same method using loocv? exercise james stein estimator for gaussian means consider the stage model yi i n i and i n we observe the following data points i suppose is known and chapter frequentist statistics a. find the ml-ii estimates of and b. find the posterior estimates e iyi and var iyi for i other terms c. give a credible interval for p iyi for i do you trust this interval the i are computed similarly. gaussian assumption is reasonable? i.e. is it likely to be too large or too small or just right? d. what do you expect would happen to your estimates if were much smaller you do not need to compute the numerical answer just briefly explain what would happen qualitatively and why. exercise show that m le n m le is biased is a biased estimator of i.e. show n n hint note that xn are independent and use the fact that the expectation of a product of independent random variables is the product of the expectations. exercise estimation of when is known suppose we sample xn n where is a known constant. derive an expression for the mle for in this case. is it unbiased? linear regression introduction linear regression is the work horse of statistics and machine learning. when augmented with kernels or other forms of basis function expansion it can model also nonlinear relationships. and when the gaussian output is replaced with a bernoulli or multinoulli distribution it can be used for classification as we will see below. so it pays to study this model in detail. model specification as we discussed in section linear regression is a model of the form pyx n x linear regression can be made to model non-linear relationships by replacing x with some pyx n non-linear function of the inputs that is we use this is known as basis function expansion. that the model is still linear in the parameters w so it is still called linear regression the importance of this will become clear below. a simple example are polynomial basis functions where the model has the form x xd figure illustrates the effect of changing d increasingly complex functions. we can also apply linear regression to more than input. for example consider modeling temperature as a function of location. figure plots e and figure plots e increasing the degree d allows us to create maximum likelihood estimation squares a common way to esitmate the parameters of a statistical model is to compute the mle which is defined as arg max log pd chapter linear regression figure linear regression applied to data. vertical axis is temperature horizontal axes are location within a room. data was collected by some remote sensing motes at intel s lab in berkeley ca courtesy of romain thibaux. temperature data is fitted with a quadratic of the form f produced by surfacefitdemo. the fitted plane has the form f it is common to assume the training examples are independent and identically distributed commonly abbreviated to iid. this means we can write the log-likelihood as follows log pd log pyixi nll instead of maximizing the log-likelihood we can equivalently minimize the negative log likelihood or nll log pyixi the nll formulation is sometimes more convenient since many optimization software packages are designed to find the minima of functions rather than maxima. now let us apply the method of mle to the linear regression setting. inserting the definition of the gaussian into the above we find that the log likelihood is given by log exp wt rssw n rss stands for residual sum of squares and is defined by rssw wt the rss is also called the sum of squared errors or sse and ssen is called the mean squared error or mse. it can also be written as the square of the norm of the vector of maximum likelihood estimation squares sum of squares error contours for linear regression prediction truth w figure in linear least squares we try to minimize the sum of squared distances from each training point by a red circle to its approximation by a blue cross that is we minimize the sum of the lengths of the little vertical blue lines. the red diagonal line represents yx which is the least squares regression line. note that these residual lines are not perpendicular to the least squares line in contrast to figure figure generated by residualsdemo. contours of the rss error surface for the same example. the red cross represents the mle w figure generated by contoursssedemo. residual errors rssw i where wt xi. we see that the mle for w is the one that minimizes the rss so this method is known as least squares. this method is illustrated in figure the training data yi are shown as red circles the estimated values yi are shown as blue crosses and the residuals yi yi are shown as vertical blue lines. the goal is to find the setting of the parameters slope and intercept such that the resulting red line minimizes the sum of squared residuals lengths of the vertical blue lines. in figure we plot the nll surface for our linear regression example. we see that it is a quadratic bowl with a unique minimum which we now derive. this is true even if we use basis function expansion such as polynomials because the nll is still linear in the parameters w even if it is not linear in the inputs x. derivation of the mle first we rewrite the objective in a form that is more amenable to differentiation nllw xwt xw wt xw wt y where xt x xixt i id is the sum of squares matrix and xt y xiyi. using results from equation we see that the gradient of this is given by gw xw xt y xiwt xi yi equating to zero we get xt xw xt y chapter linear regression this is known as the normal equation. the corresponding solution w to this linear system of equations is called the ordinary least squares or ols solution which is given by wols x y geometric interpretation this equation has an elegant geometrical intrepretation as we now explain. we assume n d so we have more examples than features. the columns of x define a linear subspace of dimensionality d which is embedded in n dimensions. let the j th column be xj which is d which represents the i th data a vector in r n for example suppose we have n examples in d case. similarly y is a vector in r dimensions should not be confused with xi r y x n these vectors are illustrated in figure we seek a vector y r n that lies in this linear subspace and is as close as possible to y i.e. we want to find argmin y span xd since y spanx there exists some weight vector w such that y wd xd xw maximum likelihood estimation squares x y y figure graphical interpretation of least squares for n examples and d features. and but does not lie on this are vectors in r plane. the orthogonal projection of y onto this plane is denoted y. the red line from y to y is the residual whose norm we want to minimize. for visual clarity all vectors have been converted to unit norm. figure generated by leastsquaresprojection. together they define a plane. y is also a vector in r to minimize the norm of the residual y y we want the residual vector to be orthogonal to every column of x so xt j y for j hence j y xt xw w x xt hence our projected value of y is given by y x w xxt x y y this corresponds to an orthogonal projection of y onto the column space of x. the projection matrix p xxt x is called the hat matrix since it puts the hat on y convexity when discussing least squares we noted that the nll had a bowl shape with a unique minimum. the technical term for functions like this is convex. convex functions play a very important role in machine learning. let us define this concept more precisely. we say a set s is convex if for any s we have s chapter linear regression figure illustration of a convex set. illustration of a nonconvex set. x y a b illustration of a convex function. we see that the chord joining f to f lies figure above the function. a function that is neither convex nor concave. a is a local minimum b is a global minimum. figure generated by convexfnhand. that is if we draw a line from to for an illustration of a convex set and figure for an illustration of a non-convex set. all points on the line lie inside the set. see figure a function f is called convex if its epigraph set of points above the function defines a convex set. equivalently a function f is called convex if it is defined on a convex set and if for any s and for any we have f f see figure for a example. a function is called strictly convex if the inequality is strict. a function f is concave if f is convex. examples of scalar convex functions include e and log examples of scalar concave functions include log and intuitively a convex function has a bowl shape and hence has a unique global minimum corresponding to the bottom of the bowl. hence its second derivative must be positive everywhere d d f a twice-continuously differentiable multivariate function f is convex iff its hessian is positive definite for all in the machine learning context the function f often corresponds to the nll. recall that the hessian is the matrix of second partial derivatives defined by hjk matrix h is positive definite iff vt hv for any non-zero vector v. f j k also recall that a huber robust linear regression linear data with noise and outliers least squares laplace figure illustration of robust linear regression. figure generated by linregrobustdemocombined. illustration of and huber loss functions. figure generated by huberlossdemo. models where the nll is convex are desirable since this means we can always find the globally optimal mle. we will see many examples of this later in the book. however many models of interest will not have concave likelihoods. in such cases we will discuss ways to derive locally optimal parameter estimates. robust linear regression it is very common to model the noise in regression models using a gaussian distribution with zero mean and constant variance n where yi wt xi. in this case maximizing likelihood is equivalent to minimizing the sum of squared residuals as we have seen. however if we have outliers in our data this can result in a poor fit as illustrated in figure outliers are the points on the bottom of the figure. this is because squared error penalizes deviations quadratically so points far from the line have more affect on the fit than points near to the line. one way to achieve robustness to outliers is to replace the gaussian distribution for the response variable with a distribution that has heavy tails. such a distribution will assign higher likelihood to outliers without having to perturb the straight line to explain them. one possibility is to use the laplace distribution introduced in section if we use this as our observation model for regression we get the following likelihood pyx w b lapywt x b exp b the robustness arises from the use of wt x instead of wt for simplicity we will assume b is fixed. let ri yi wt xi be the i th residual. the nll has the form wt x i chapter linear regression likelihood gaussian gaussian gaussian laplace student name prior uniform least squares gaussian laplace uniform robust regression uniform robust regression ridge lasso section exercise table summary of various likelihoods and priors used for linear regression. the likelihood refers to the distributional form of pyx w and the prior refers to the distributional form of pw. map estimation with a uniform distribution corresponds to mle. unfortunately this is a non-linear objective function which is hard to optimize. fortunately we can convert the nll to a linear objective subject to linear constraints using the following split variable trick. first we define and then we impose the linear inequality constraints that r constrained objective becomes i r i s.t. i r r i wt xi r i r i yi i and r i now the ri r i r i min wrr i this is an example of a linear program with d unknowns and constraints. since this is a convex optimization problem it has a unique solution. to solve an lp we must first write it in standard form which as follows min f t s.t. a b aeq beq l u f a b aeq i i in our current example r r beq y l u this can be solved by any lp solver e.g. and vandenberghe see figure for an example of the method in action. an alternative to using nll under a laplace likelihood is to minimize the huber loss function defined as follows if if lh this is equivalent to for errors that are smaller than and is equivalent to for larger errors. see figure the advantage of this loss function is that it is everywhere differentiable drr signr if r we can also check that the function is using the fact that d continuous since the gradients of the two parts of the function match at r namely dr lh consequently optimizing the huber loss is much faster than using the laplace likelihood since we can use standard smooth optimization methods as quasinewton instead of linear programming. d figure gives an illustration of the huber loss function. the results are qualitatively fact it turns out that the huber method also has a similiar to the probabilistic methods. probabilistic interpretation although it is rather unnatural et al. ridge regression ln lambda ln lambda figure degree polynomial fit to n data points with increasing amounts of regularization. data was generated from noise with variance the error bars representing the noise variance get wider as the fit gets smoother since we are ascribing more of the data variation to the noise. figure generated by linregpolyvsregdemo. ridge regression one problem with ml estimation is that it can result in overfitting. in this section we discuss a way to ameliorate this problem by using map estimation with a gaussian prior. for simplicity we assume a gaussian likelihood rather than a robust likelihood. basic idea the reason that the mle can overfit is that it is picking the parameter values that are the best for modeling the training data but if the data is noisy such parameters often result in complex functions. as a simple example suppose we fit a degree polynomial to n data points using least squares. the resulting curve is very wiggly as shown in figure the corresponding least squares coefficients are as follows we see that there are many large positive and negative numbers. these balance out exactly to make the curve wiggle in just the right way so that it almost perfectly interpolates the data. but this situation is unstable if we changed the data a little the coefficients would change a lot. we can encourage the parameters to be small thus resulting in a smoother curve by using a zero-mean gaussian prior n pw j argmax w where controls the strength of the prior. the corresponding map estimation problem becomes log n wt xi log n chapter linear regression negative log marg. likelihood cv estimate of mse mean squared error train mse test mse log lambda log lambda figure training error blue and test error red for a degree polynomial fit by ridge regression plotted vs log data was generated from noise with variance set has size n note models are ordered from complex regularizer on the left to simple regularizer on the right. the stars correspond to the values used to plot the functions in figure estimate of performance using training set. dotted blue cross-validation estimate of future mse. solid black negative log marginal likelihood log pd both curves have been vertically rescaled to to make them comparable. figure generated by linregpolyvsregdemo. it is a simple exercise to show that this is equivalent to minimizing the following jw n wt where and j wt w is the squared two-norm. here the first term is the mse nll as usual and the second term is a complexity penalty. the corresponding solution is given by j wridge id xt x y this technique is known as ridge regression or penalized least squares. in general adding a gaussian prior to the parameters of a model to encourage them to be small is called regularization or weight decay. note that the offset term is not regularized since this just affects the height of the function not its complexity. by penalizing the sum of the magnitudes of the weights we ensure the function is simple w corresponds to a straight line which is the simplest possible function corresponding to a constant. we illustrate this idea in figure where we see that increasing results in smoother we functions. the resulting coefficients also become smaller. for example using have ridge regression in figure we plot the mse on the training and test sets vs log we see that as we increase the model becomes more constrained the error on the training set increases. for the test set we see the characteristic u-shaped curve where the model overfits and then underfits. in section we will discuss a more probabilistic approach. it is common to use cross validation to pick as shown in figure we will consider a variety of different priors in this book. each of these corresponds to a different form of regularization. this technique is very widely used to prevent overfitting. numerically stable computation interestingly ridge regression which works better statistically is also easier to fit numerically since id xt x is much better conditioned hence more likely to be invertible than xt x at least for suitable largy nevertheless inverting matrices is still best avoided for reasons of numerical stability. if you write winvx xx in matlab it will give you a warning. we now describe a useful trick for fitting ridge regression models hence by extension computing vanilla ols estimates that is more numerically robust. we assume the prior has the form pw n where is the precision matrix. in the case of ridge regression to avoid penalizing the term we should center the data first as explained in exercise first let us augment the original data with some virtual data coming from the prior x x y y where where the extra rows represent pseudo-data from the prior. t is a cholesky decomposition of we see that x is d d we now show that the nll on this expanded data is equivalent to penalized nll on the original data f y xwt y xw x y xw w w xw w xwt xw wt xwt xw t w w x w y hence the map estimate is given by wridge xt x xt y as we claimed. now let x qr chapter linear regression be the qr decomposition of x where q is orthonormal qt q qqt i and r is upper triangular. then xt x qt qr r r t hence wridge r t rt qt y r y note that r is easy to invert since it is upper triangular. this gives us a way to compute the ridge estimate while avoiding having to invert xt x. we can use this technique to find the mle by simply computing the qr decomposition of the unaugmented matrix x and using the original y. this is the method of choice for solving least squares problems. fact it is so sommon that it can be implemented in one line of matlab using the backslash operator wxy. note that computing the qr decomposition of an n d matrix takes on time and is numerically very stable. if d n we should first perform an svd decomposition. in particular let x usvt be the svd of x where vt v in uut ut u in and s is a diagonal n n matrix. now let z ud be an n n matrix. then we can rewrite the ridge estimate thus wridge vzt z in y in other words we can replace the d-dimensional vectors xi with the n vectors zi and perform our penalized fit as before. we then transform the n solution to the d-dimensional solution by multiplying by v. geometrically we are rotating to a new coordinate system in which all but the first n coordinates are zero. this does not affect the solution since the spherical gaussian prior is rotationally invariant. the overall time is now odn operations. connection with pca in this section we discuss an interesting connection between ridge regression and pca which gives further insight into why ridge regression works well. our discussion is based on et al. let x usvt be the svd of x. from equation we have wridge i y hence the ridge predictions on the training set are given by y x wridge usvt i y u sut y uj sjjut j y ridge regression ml estimate map estimate prior mean figure geometry of ridge regression. the likelihood is shown as an ellipse and the prior is shown as a circle centered on the origin. based on figure of figure generated by geomridge where sjj i j j and j are the singular values of x. hence y x wridge uj j j ut j y in contrast the least squares prediction is y x wls y uut y ujut j y j is small compared to then direction uj will not have much effect on the prediction. in if view of this we define the effective number of degrees of freedom of the model as follows dof j j let us try to understand why this behavior is desirable. when dof and as dof in section we show that cov x if we use a uniform prior for w. thus the directions in which we are most uncertain about w are determined by the eigenvectors of this matrix with the smallest eigenvalues as shown in figure furthermore in section we show that the squared singular values j are equal to the eigenvalues of xt x. hence small singular values j correspond to directions with high posterior variance. it is these directions which ridge shrinks the most. chapter linear regression this process is illustrated in figure the horizontal parameter is not-well determined by the data high posterior variance but the vertical parameter is well-determined. hence wmap is shifted strongly towards the prior mean which is to figure which illustrated sensor fusion with sensors of different reliabilities. in this way ill-determined parameters are reduced in size towards this is called shrinkage. is close to wmle but wmap there is a related but different technique called principal components regression. the idea is this first use pca to reduce the dimensionality to k dimensions and then use these low dimensional features as input to regression. however this technique does not work as well as ridge in terms of predictive accuracy et al. the reason is that in pc regression only the first k dimensions are retained and the remaining d k dimensions are entirely ignored. by contrast ridge regression uses a soft weighting of all the dimensions. regularization effects of big data regularization is the most common way to avoid overfitting. however another effective approach which is not always available is to use lots of data. it should be intuitively obvious that the more training data we have the better we will be able to so we expect the test set error to decrease to some plateau as n increases. this is illustrated in figure where we plot the mean squared error incurred on the test set achieved by polynomial regression models of different degrees vs n plot of error vs training set size is known as a learning curve. the level of the plateau for the test error consists of two terms an irreducible component that all models incur due to the intrinsic variability of the generating process is called the noise floor and a component that depends on the discrepancy between the generating process truth and the model this is called structural error. in figure the truth is a degree polynomial and we try fitting polynomials of degrees and to this data. call the models and we see that the structural error for models and is zero since both are able to capture the true generating process. however the structural error for is substantial which is evident from the fact that the plateau occurs high above the noise floor. for any model that is expressive enough to capture the truth one with small structural error the test error will go to the noise floor as n however it will typically go to zero faster for simpler models since there are fewer parameters to estimate. in particular for finite training sets there will be some discrepancy between the parameters that we estimate and the best parameters that we could estimate given the particular model class. this is called approximation error and goes to zero as n but it goes to zero faster for simpler models. this is illustrated in figure see also exercise in domains with lots of data simple methods can work surprisingly well et al. however there are still reasons to study more sophisticated learning methods because there will always be problems for which we have little data. for example even in such a data-rich domain as web search as soon as we want to start personalizing the results the amount of data available for any given user starts to look small again to the complexity of the problem. this assumes the training data is randomly sampled and we don t just get repetitions of the same examples. having informatively sampled data can help even more this is the motivation for an approach known as active learning where you get to choose your training data. truthdegree model degree train test bayesian linear regression e s m truthdegree model degree train test e s m size of training set size of training set truthdegree model degree train test truthdegree model degree train test e s m e s m size of training set size of training set figure mse on training and test sets vs size of training set for data generated from a degree polynomial with gaussian noise of variance we fit polynomial models of varying degree to this data. degree degree degree degree note that for small training set sizes the test error of the degree polynomial is higher than that of the degree polynomial due to overfitting but this difference vanishes once we have enough data. note also that the degree polynomial is too simple and has high test error even given large amounts of training data. figure generated by linregpolyvsn. in such cases we may want to learn multiple related models at the same time which is known as multi-task learning. this will allow us to borrow statistical strength from tasks with lots of data and to share it with tasks with little data. we will discuss ways to do later in the book. bayesian linear regression although ridge regression is a useful way to compute a point estimate sometimes we want to compute the full posterior over w and for simplicity we will initially assume the noise variance is known so we focus on computing pwd then in section we consider chapter linear regression the general case where we compute pw we assume throughout a gaussian likelihood model. performing bayesian inference with a robust likelihood is also possible but requires more advanced techniques exercise computing the posterior in linear regression the likelihood is given by pyx w xw exp xwt xw where is an offset term. if the inputs are centered so i xij for each j the mean of the output is equally likely to be positive or negative. so let us put an improper prior on of the form p and then integrate it out to get pyx w exp yi is the empirical mean of the output. for notational simplicity we shall where y assume the output has been centered and write y for y n the conjugate prior to the above gaussian likelihood is also a gaussian which we will denote by pw n using bayes rule for gaussians equation the posterior is given by pwx y n n vn vn xt y wn vn v n v v vn xt x xt x if and then the posterior mean reduces to the ridge estimate if we define this is because the mean and mode of a gaussian are the same. to gain insight into the posterior distribution not just its mode let us consider a example yx w where the true parameters are and in figure we plot the prior the likelihood the posterior and some samples from the posterior predictive. in particular the right hand column plots the function yx ws where x ranges over and ws n vn is a sample from the parameter posterior. initially when we sample from the prior row our predictions are all over the place since our prior is uniform. after we see one data point row our posterior becomes constrained by the corresponding likelihood and our predictions pass close to the observed data. however we see that the posterior has a ridge-like shape reflecting the fact that there are many possible solutions with different bayesian linear regression likelihood priorposterior data space y y y y x x x x sequential bayesian updating of a linear regression model pyx n figure row represents the prior row represents the first data point row represents the second data point row represents the data point left column likelihood function for current data point. middle column posterior given data so far the first line is the prior. right column samples from the current priorposterior predictive distribution. the white cross in columns and represents the true parameter value we see that the mode of the posterior rapidly samples converges to this point. the blue circles in column are the observed data points. based on figure of figure generated by slopesintercepts. this makes sense since we cannot uniquely infer two parameters from one observation. after we see two data points row the posterior becomes much narrower and our predictions all have similar slopes and intercepts. after we observe data points row the posterior is essentially a delta function centered on the true value indicated by a white cross. estimate converges to the truth since the data was generated from this model and because bayes is a consistent estimator see section for discussion of this point. computing the posterior predictive it s tough to make predictions especially about the future. yogi berra chapter linear regression in machine learning we often care more about predictions than about interpreting the parameters. using equation we can easily show that the posterior predictive distribution at a test point x is also gaussian pyxd n w vn n n x n xt vn x n the variance in this prediction n depends on two terms the variance of the observation noise and the variance in the parameters vn the latter translates into variance about observations in a way which depends on how close x is to the training data d. this is illustrated in figure where we see that the error bars get larger as we move away from the training points representing increased uncertainty. this is important for applications such as active learning where we want to model what we don t know as well as what we do. by contrast the plugin approximation has constant sized error bars since n w wwdw pyx w pyxd see figure bayesian inference when is unknown in this section we apply the results in section to the problem of computing pw for a linear regression model. this generalizes the results from section where we assumed was known. in the case where we use an uninformative prior we will see some interesting connections to frequentist statistics. conjugate prior as usual the likelihood has the form pyx w n by analogy to section one can show that the natural conjugate prior has the following form pw nigw n exp v posterior predictive variance prediction training data functions sampled from posterior bayesian linear regression plugin approximation prediction training data functions sampled from plugin approximation to posterior figure plug-in approximation to predictive density plug in the mle of the parameters. posterior predictive density obtained by integrating out the parameters. black curve is posterior mean error bars are standard deviations of the posterior predictive density. samples from the plugin approximation to posterior predictive. samples from the posterior predictive. figure generated by linregpostpreddemo. with this prior and likelihood one can show that the posterior has the following form pw nigw vn an bn xt y xt x wn vn vn an bn v wt yt y wt n v n wn the expressions for wn and vn are similar to the case where is known. the expression for an is also intuitive since it just updates the counts. the expression for bn can be interpreted chapter linear regression it is the prior sum of squares plus the empirical sum of squares yt y plus a as follows term due to the error in the prior on w. the posterior marginals are as follows p igan bn pwd bn an vn we give a worked example of using these equations in section by analogy to section the posterior predictive distribution is a student t distribution. in particular given m new test inputs x we have p y xd y xwn bn an xvn xt the predictive variance has two components due to the measurement noise and xvn xt due to the uncertainty in w. this latter terms varies depending on how close the test inputs are to the training data. it is common to set corresponding to an uninformative prior for and to set for any positive value g. this is called zellner s g-prior and gxt x here g plays a role analogous to in ridge regression. however the prior covariance is rather than i. this ensures that the posterior is invariant to scaling proportional to x of the inputs see also exercise we will see below that if we use an uninformative prior the posterior precision given n measurements is v n xt x. the unit information prior is defined to contain as much information as one sample and wasserman to create a unit information prior for linear regression we need to use v n xt x which is equivalent to the g-prior with g n uninformative prior an uninformative prior can be obtained by considering the uninformative limit of the conjugate g-prior which corresponds to setting g this is equivalent to an improper nig prior with i and which gives pw alternatively we can start with the semi-conjugate prior pw wp and take each term to its uninformative limit individually which gives pw this is equivalent to an improper nig prior with i and the corresponding posterior is given by pw nigw vn an bn y wn wmle x vn x n d an bn x wmlet x wmle bayesian linear regression wj e var sig ci table posterior mean standard deviation and credible intervals for a linear regression model with an uninformative prior fit to the caterpillar data. produced by linregbayescaterpillar. the marginal distribution of the weights is given by pwd t w c n d where c x n d and w is the mle. we discuss the implications of these equations below. an example where bayesian and frequentist inference coincide the use of a uninformative prior is interesting because the resulting posterior turns out to be equivalent to the results from frequentist statistics also section in particular from equation we have pwjd t wj n d n d this is equivalent to the sampling distribution of the mle which is given by the following e.g. and berger wj wj sj where sj tn d n d is the standard error of the estimated parameter. section for a discussion of sampling distributions. consequently the frequentist confidence interval and the bayesian marginal credible interval for the parameters are the same in this case. as a worked example of this consider the caterpillar dataset from and robert details of what the data mean don t matter for our present purposes. we can compute chapter linear regression the posterior mean and standard deviation and the credible intervals for the regression coefficients using equation the results are shown in table it is easy to check that these credible intervals are identical to the confidence intervals computed using standard frequentist methods linregbayescaterpillar for the code. we can also use these marginal posteriors to compute if the coefficients are significantly different from an informal way to do this using decision theory is to check if its ci excludes from table we see that the cis for coefficients are all significant by this measure so we put a little star by them. it is easy to check that these results are the same as those produced by standard frequentist software packages which compute p-values at the level. although the correspondence between the bayesian and frequentist results might seem appealing to some readers recall from section that frequentist inference is riddled with pathologies. also note that the mle does not even exist when n d so standard frequentist inference theory breaks down in this setting. bayesian inference theory still works although it requires the use of proper priors. and george for one extension of the g-prior to the case where d n eb for linear regression procedure so far we have assumed the prior is known. in this section we describe an empirical bayes procedure for picking the hyper-parameters. more precisely we choose to maximize the marignal likelihood where be the precision of the observation noise and is the precision of the prior pw this is known as the evidence procedure see section for the algorithmic details. the evidence procedure provides an alternative to using cross validation. for example in likelihood for different values of as well as the figure we plot the log marginal maximum value found by the optimizer. we see that in this example we get the same result kept fixed in both methods to make them as shown in figure comparable. the principle practical advantage of the evidence procedure over cv will become apparent in section where we generalize the prior by allowing a different j for every feature. this can be used to perform feature selection using a technique known as automatic relevancy determination or ard. by contrast it would not be possible to use cv to tune d different hyper-parameters. the evidence procedure is also useful when comparing different kinds of models since it provides a good approximation to the evidence pdm max pdw mpwm pdw mpwm it is important to least approximately integrate over rather than setting it arbitrarily for reasons discussed in section indeed this is the method we used to evaluate the marginal alternatively we could integrate out analytically as shown in section and just optimize and weigend however it turns out that this is less accurate than optimizing both and bayesian linear regression e s m fold cross validation ntrain log evidence log lambda log alpha figure estimate of test mse produced by cross-validation vs log the smallest value is indicated by the vertical line. note the vertical scale is in log units. log marginal likelihood vs log the largest value is indicated by the vertical line. figure generated by linregpolyvsregdemo. likelihood for the polynomial regression models in figures and for a more bayesian approach in which we model our uncertainty about rather than computing point estimates see section exercises exercise behavior of training set error with increasing sample size the error on the test will always decrease as we get more training data since the model will be better estimated. however as shown in figure for sufficiently complex models the error on the training set can increase we we get more training data until we reach some plateau. explain why. exercise multi-output linear regression jaakkola. when we have multiple independent outputs in linear regression the model becomes pyx w n j xi j since the likelihood factorizes across dimensions so does the mle. thus w wm where wj x in this exercise we apply this result to a model with dimensional response vector yi r have some binary input data xi the training data is as follows suppose we chapter linear regression x y let us embed each xi into using the following basis function the model becomes y wt where w is a matrix. compute the mle for w from the above data. exercise centering and ridge regression assume that x so the input data has been centered. show that the optimizer of jw xw xw wt w is y w x i y exercise mle for for linear regression show that the mle for the error variance in linear regression is given by n xt i this is just the empirical variance of the residual errors when we plug in our estimate of w. exercise mle for the offset term in linear regression linear regression has the form e wt x. it is common to include a column of s in the design matrix so we can solve for the offset term term and the other parameters w at the same time using the normal equations. however it is also possible to solve for w and separately. show that n yi n i w y xt w xt i i so models the difference in the average output from the average predicted output. also show that w c xc c yc xxi xt yxi x i xi x along its rows and yc y y is where xc is the centered input matrix containing xc the centered output vector. thus we can first compute w on centered data and then estimate using y xt w. bayesian linear regression exercise mle for simple linear regression simple linear regression refers to the case where the input is scalar so d show that the mle in this case is given by the following equations which may be familiar from basic statistics classes i xiyi n x y i y x e ixi xyi y ixi i n cov y var see for a demo. exercise sufficient statistics for online linear regression not keep the original data xi yi but we do have the following functions of the data jaakkola. consider fitting the model y using least squares. unfortunately we did xn c xx n n xi yn n c xy n xyi y c yy n yi a. what are the minimal set of statistics that we need to estimate see equation b. what are the minimal set of statistics that we need to estimate see equation c. suppose a new data point arrives and we want to update our sufficient statistics without is useful for online learning. show that we looking at the old data which we have not stored. can this for x as follows. this has the form new estimate is old estimate plus correction. we see that the size of the correction diminishes over time as we get more samples. derive a similar expression to update y d. show that one can update c xy recursively using c xy n nc xy nxnyn derive a similar expression to update cxx. implement the online learning algorithm i.e. write a function of the form linregupdatessss x y where x and y are scalars and ss is a structure containing the sufficient statistics. e. f. plot the coefficients over time using the dataset in use polydatamake sampling thibaux check that they converge to the solution given by the batch learner ordinary least squares. your result should look like figure turn in your derivation code and plot. exercise bayesian linear regression in with known bolstad. consider fitting a model of the form pyx n to the data shown below n xn xi n nxn xn n chapter linear regression s t h g e w i online linear regression batch batch time figure regression coefficients over time. produced by exercise x y a. compute an unbiased estimate of using n denominator is n since we have inputs namely the offset term and x. here yi and w is the mle. b. now assume the following prior on w pw use an uniform prior on and a n prior on show that this can be written as a gaussian prior of the form pw n what are and c. compute the marginal posterior of the slope where d is the data above and is the unbiased estimate computed above. what is e show your work. can use matlab if you like. hint the posterior variance is a very small number! and var d. what is a credible interval for exercise generative model for linear regression linear regression is the problem of estimating ey using a linear function of the form wt x. typically we assume that the conditional distribution of y given x is gaussian. we can either estimate this conditional gaussian directly discriminative approach or we can fit a gaussian to the joint distribution of x y and then derive ey x. in exercise we showed that the discriminative approach leads to these equations ey wt x y xt w w c xc c yc bayesian linear regression where xc x x is the centered input matrix and x replicates x across the rows. similarly yc y y is the centered output vector and y replicates y across the rows. a. by finding the maximum likelihood estimates of xx xy x and y derive the above equations by fitting a joint gaussian to x y and using the formula for conditioning a gaussian section show your work. b. what are the advantages and disadvantages of this approach compared to the standard discriminative approach? exercise bayesian linear regression using the g-prior show that when we use the g-prior pw nigw gxt x following form the posterior has the pw nigw vn an bn vn g g g x wmle wn g an bn wt mlext x wmle logistic regression introduction one way to build a probabilistic classifier is to create a joint model of the form py x and then to condition on x thereby deriving pyx. this is called the generative approach. an alternative approach is to fit a model of the form pyx directly. this is called the discriminative approach and is the approach we adopt in this chapter. in particular we will assume discriminative models which are linear in the parameters. this will turn out to significantly simplify model fitting as we will see. in section we compare the generative and discriminative approaches and in later chapters we will consider non-linear and non-parametric discriminative models. model specification as we discussed in section logistic regression corresponds to the following binary classification model pyx w berysigmwt x a example is shown in figure logistic regression can easily be extended to higherdimensional inputs. for example figure shows plots of py w sigmwt x for input and different weight vectors w. if we threshold these probabilities at we induce a linear decision boundary whose normal is given by w. model fitting in this section we discuss algorithms for estimating the parameters of a logistic regression model. chapter logistic regression w w w x x w x x w x x x x w w x x w x x w x x x x x x w x x plots of here w defines the normal to the decision figure boundary. points to the right of this have sigmwt x and points to the left have sigmwt x based on figure of figure generated by mle the negative log-likelihood for logistic regression is given by nllw log i log i yi i this is also called the cross-entropy error function section another way of writing this is as follows. suppose yi instead of yi we x hence have py wt x and py exp yiwt xi n llw unlike linear regression we can no longer write down the mle in closed form. instead we need to use an optimization algorithm to compute it. for this we need to derive the gradient and hessian. in the case of logistic regression one can show that the gradient and hessian model fitting figure gradient descent on a simple function starting from for steps using a fixed learning rate size the global minimum is at figure generated by steepestdescentdemo. of this are given by the following g h d dw d dw f gwt i yixi xt y i w ixt i i i ixixt i xt sx where s diag i. one can also show that h is positive definite. hence the nll is convex and has a unique global minimum. below we discuss some methods for finding this minimum. steepest descent perhaps the simplest algorithm for unconstrained optimization is gradient descent also known as steepest descent. this can be written as follows k kgk where k is the step size or learning rate. the main issue in gradient descent is how should we set the step size? this turns out to be quite tricky. if we use a constant learning rate but make it too small convergence will be very slow but if we make it too large the method can fail to converge at all. this is illustrated in figure where we plot the following function f we arbitrarily decide to start from in figure we use a fixed step size of we see that it moves slowly along the valley. in figure we use a fixed step size of we see that the algorithm starts oscillating up and down the sides of the valley and never converges to the optimum. chapter logistic regression exact line searching figure steepest descent on the same function as figure starting from using line search. figure generated by steepestdescentdemo. illustration of the fact that at the end of a line search of picture the local gradient of the function will be perpendicular to the search direction. based on figure of et al. let us develop a more stable method for picking the step size so that the method is guaranthis property is called global teed to converge to a local optimum no matter where we start. convergence which should not be confused with convergence to the global optimum! by taylor s theorem we have f d f gt d where d is our descent direction. so if is chosen small enough then f d f since the gradient will be negative. but we don t want to choose the step size too small or we will move very slowly and may not reach the minimum. so let us pick to minimize f k dk this is called line minimization or line search. there are various methods for solving this optimization problem see and wright for details. figure demonstrates that line search does indeed work for our simple problem. however we see that the steepest descent path with exact line searches exhibits a characteristic zig-zag behavior. to see why note that an exact line search satisfies k arg min a necessary condition for the optimum is t g where g d is the gradient at the end of the step. so we either have g which means we have found a stationary point or g d which means that exact search stops at a point where the local gradient is perpendicular to the search direction. hence consecutive directions will be orthogonal figure this explains the zig-zag behavior. one simple heuristic to reduce the effect of zig-zagging is to add a momentum term k by the chain rule k as follows k kgk k k k model fitting where k controls the importance of the momentum term. community this is known as the heavy ball method e.g. in the optimization an alternative way to minimize zig-zagging is to use the method of conjugate gradients e.g. and wright ch or and van loan sec this is the method of choice for quadratic objectives of the form f t a which arise when solving linear systems. however non-linear cg is less popular. newton s method algorithm newton s method for minimizing a strictly convex function initialize for k until convergence do evaluate gk f k evaluate hk k solve hkdk gk for dk use line search to find stepsize k along dk k kdk one can derive faster optimization methods by taking the curvature of the space the into account. these are called second order optimization metods. the primary hessian example is newton s algorithm. this is an iterative algorithm which consists of updates of the form k kh k gk the full pseudo-code is given in algorithm this algorithm can be derived as follows. consider making a second-order taylor series approximation of f around k k k fquad fk gt kt hk k let us rewrite this as fquad t a bt c where a hk b gk hk k c fk gt k k t k hk k the minimum of fquad is at a k h k gk thus the newton step dk h order approximation of f around k. see figure for an illustration. k gk is what should be added to k to minimize the second chapter logistic regression fx f quad fx f quad x k x k k x k x k k illustration of newton s method for minimizing a function. figure the solid curve is the function f the dotted line fquadx is its second order approximation at xk. the newton step dk is what must be added to xk to get to the minimum of fquadx. based on figure of figure generated by newtonsmethodminquad. illustration of newton s method applied to a nonconvex function. we fit a quadratic around the current point xk and move to its stationary point xk dk. unfortunately this is a local maximum not minimum. this means we need to be careful about the extent of our quadratic approximation. based on figure of figure generated by newtonsmethodnonconvex. in its simplest form listed newton s method requires that hk be positive definite which will hold if the function is strictly convex. if not the objective function is not convex then hk may not be positive definite so dk h k gk may not be a descent direction figure for an example. in this case one simple strategy is to revert to steepest descent dk gk. the levenberg marquardt algorithm is an adaptive way to blend between newton steps and steepest descent steps. this method is widely used when solving nonlinear least squares problems. an alternative approach is this rather than computing dk h k gk directly we can solve the linear system of equations hkdk gk for dk using conjugate gradient if hk is not positive definite we can simply truncate the cg iterations as soon as negative curvature is detected this is called truncated newton. iteratively reweighted least squares let us now apply newton s algorithm to find the mle for binary logistic regression. the newton update at iteration k for this model is as follows k since the hessian is exact wk h wk skx skx skx skx k skxwk xt k y k skzk where we have defined the working response as zk xwk s k k model fitting equation is an example of a weighted least squares problem which is a minimizer of skizki wt since sk is a diagonal matrix we can rewrite the targets in component form each case i as zki wt k xi yi ki ki this algorithm is known as iteratively reweighted least squares or irls for short since at each iteration we solve a weighted least squares problem where the weight matrix sk changes at each iteration. see algorithm for some pseudocode. algorithm iteratively reweighted least squares w y repeat i wt xi i sigm i si i zi i yi i s w sx si sz until converged quasi-newton metric methods the mother of all second-order optimization algorithm is newton s algorithm which we discussed in section unfortunately it may be too expensive to compute h explicitly. quasinewton methods iteratively build up an approximation to the hessian using information gleaned from the gradient vector at each step. the most common method is called bfgs after its inventors broyden fletcher goldfarb and shanno which updates the approximation to the hessian bk hk as follows ykyt k yt k sk sk k k yk gk gk bk st k bksk this is a rank-two update to the matrix and ensures that the matrix remains positive definite certain restrictions on the step size. we typically start with a diagonal approximation i. thus bfgs can be thought of as a diagonal plus low-rank approximation to the hessian. chapter logistic regression alternatively bfgs can iteratively update an approximation to the inverse hessian ck h k as follows i skyt k yt k sk ck i ykst k yt k sk skst k yt k sk since storing the hessian takes space for very large problems one can use limited memory bfgs orl-bfgs where hk or h is approximated by a diagonal plus low rank matrix. in particular the product h k gk can be obtained by performing a sequence of inner products with sk and yk using only the m most recent yk pairs and ignoring older information. the storage requirements are therefore omd. typically m suffices for good performance. see and wright for more information. l-bfgs is often the method of choice for most unconstrained smooth optimization problems that arise in machine learning see section k regularization just as we prefer ridge regression to linear regression so we should prefer map estimation for logistic regression to computing the mle. in fact regularization is important in the classification setting even if we have lots of data. to see why suppose the data is linearly separable. in this case the mle is obtained when corresponding to an infinitely steep sigmoid function iwt x also known as a linear threshold unit. this assigns the maximal amount of probability mass to the training data. however such a solution is very brittle and will not generalize well. to prevent this we can use regularization just as we did with ridge regression. we note that the new objective gradient and hessian have the following forms nllw w t w w w i it is a simple matter to pass these modified equations into any gradient-based optimizer. multi-class logistic regression now we consider multinomial logistic regression sometimes called a maximum entropy classifier. this is a model of the form py cx w expwt c x expwt x a slight variant known as a conditional logit model normalizes over a different set of classes for each data case this can be useful for modeling choices that users make between different sets of items that are offered to them. let us now introduce some notation. let ic pyi cxi w s ic where i wt xi is a c vector. also let yic iyi c be the one-of-c encoding of yi thus yi is a bit vector in which the c th bit turns on iff yi c. following et al. let us model fitting set wc to ensure identifiability and define w vecw to be a d column vector. with this the log-likelihood can be written as yic ic yic log ic log yicwt c xi expwt xi log define the nll as f we now proceed to compute the gradient and hessian of this expression. since w is blockit helps to define a b structured the notation gets a bit heavy but the ideas are simple. be the kronecker product of matrices a and b. if a is an m n matrix and b is a p q matrix then a b is the mp nq block matrix amnb a b returning to the task at hand one can show that the gradient is given by gw f i yi xi where yi iyi c and iw w pyi c w are column vectors of length c for example if we have d feature dimensions and c classes this becomes i gw wc f ic yicxi in other words for each class c the derivative for the weights in the c th column is i this has the same form as in the binary logistic regression case namely an error term times xi. turns out to be a general property of distributions in the exponential family as we will see in section chapter logistic regression one can also show that the hessian is the following block structured dc dc matrix hw for example if we have features and classes this becomes i i t i i i i in other words the block c i ic i submatrix is given by hw where xi xixt i this is also a positive definite matrix so there is a unique mle. now consider minimizing log pdw log pw where pw c n the new objective its gradient and hessian are given by w c v c c wc wcv wc this can be passed to any gradient-based optimizer to find the map estimate. note however that the hessian has size o which is c times more row and columns than in the binary case so limited memory bfgs is more appropriate than newton s method. see logregfit for some matlab code. bayesian logistic regression it is natural to want to compute the full posterior over the parameters pwd for logistic regression models. this can be useful for any situation where we want to associate confidence intervals with our predictions this is necessary when solving contextual bandit problems discussed in section unfortunately unlike the linear regression case this cannot be done exactly since there is no convenient conjugate prior for logistic regression. we discuss one simple approximation below some other approaches include mcmc variational inference expectation propagation and rasmussen etc. for notational simplicity we stick to binary logistic regression. bayesian logistic regression laplace approximation in this section we discuss how to make a gaussian approximation to a posterior distribution. the approximation works as follows. suppose r d. let p e e z where e is called an energy function and is equal to the negative log of the unnormalized log posterior e log p with z pd being the normalization constant. performing a taylor series expansion around the mode the lowest energy state we get h where g is the gradient and h is the hessian of the energy function evaluated at the mode g e e h t g e since is the mode the gradient term is zero. hence p z e e exp n h h z pd p e e the last line follows from normalization constant of the multivariate gaussian. equation is known as the laplace approximation to the marginal likelihood. therefore equation is sometimes called the the laplace approximation to the posterior. however in the statistics community the term laplace approximation refers to a more sophisticated method e.g. it may therefore be better to use the term gaussian approximation to refer to equation a gaussian approximation is often a reasonable approximation since posteriors often become more gaussian-like as the sample size increases for reasons analogous to the central physics there is an analogous technique known as a saddle point approximation. et al. for details. limit theorem. derivation of the bic we can use the gaussian approximation to write the log marginal likelihood as follows dropping irrelevant constants log pd log pd log p log the penalization terms which are added to the log pd are sometimes called the occam factor and are a measure of model complexity. if we have a uniform prior p we can drop the second term and replace with the mle chapter logistic regression we now focus on approximating the third term. we have h log pdi let us approximate each hi by a fixed matrix h. then we have hi where hi log log h logn d h d log n log h where d dim and we have assumed h is full rank. we can drop the log h term since it is independent of n and thus will get overwhelmed by the likelihood. putting all the pieces together we recover the bic score log pd log pd d log n gaussian approximation for logistic regression now let us apply the gaussian approximation to logistic regression. we will use a a gaussian prior of the form pw n just as we did in map estimation. the approximate posterior is given by pwd n w h where w arg minw ew ew pdw log pw and h w. as an example consider the linearly separable data in figure there are many parameter settings that correspond to lines that perfectly separate the training data we show examples. the likelihood surface is shown in figure where we see that the likelihood is unbounded as we move up and to the right in parameter space along a ridge where is indicated by the diagonal line. the reasons for this is that we can maximize the likelihood by driving to infinity to being on this line since large regression weights make the sigmoid function very steep turning it into a step function. consequently the mle is not well defined when the data is linearly separable. to regularize the problem let us use a vague spherical prior centered at the origin n multiplying this spherical prior by the likelihood surface results in a highly skewed posterior shown in figure posterior is skewed because the likelihood function chops off regions of parameter space a soft fashion which disagree with the data. the map estimate is shown by the blue dot. unlike the mle this is not at infinity. the gaussian approximation to this posterior is shown in figure we see that this is a symmetric distribution and therefore not a great approximation. of course it gets the mode correct construction and it at least represents the fact that there is more uncertainty along the southwest-northeast direction corresponds to uncertainty about the orientation of separating lines than perpendicular to this. although a crude approximation this is surely better than approximating the posterior by a delta function which is what map estimation does. approximating the posterior predictive given the posterior we can compute credible intervals perform hypothesis tests etc. just as we did in section in the case of linear regression. but in machine learning interest usually focusses on prediction. the posterior predictive distribution has the form pyxd pyx wpwddw bayesian logistic regression data log likelihood log unnormalised posterior laplace approximation to posterior figure two-class data in log-likelihood for a logistic regression model. the line is drawn from the origin in the direction of the mle is at infinity. the numbers correspond to points in parameter space corresponding to the lines in unnormalized log posterior vague spherical prior. laplace approximation to posterior. based on a figure by mark girolami. figure generated by logreglaplacegirolamidemo. unfortunately this integral is intractable. the simplest approximation is the plug-in approximation which in the binary case takes the form py py e where e is the posterior mean. in this context e is called the bayes point. of course such a plug-in estimate underestimates the uncertainty. we discuss some better approximations below. wmap decision boundary for sampled w chapter logistic regression numerical approx of mc approx of figure posterior predictive distribution for a logistic regression model in top left contours of py wmap. top right samples from the posterior predictive distribution. bottom left averaging over these samples. bottom right moderated output approximation. based on a figure by mark girolami. figure generated by logreglaplacegirolamidemo. monte carlo approximation a better approach is to use a monte carlo approximation as follows py s sigmwst x where ws pwd are samples from the posterior. technique can be trivially extended to the multi-class case. if we have approximated the posterior using monte carlo we can reuse these samples for prediction. if we made a gaussian approximation to the posterior we can draw independent samples from the gaussian using standard methods. figure shows samples from the posteiror predictive for our example. figure bayesian logistic regression sigmoid probit figure posterior predictive density for sat data. the red circle denotes the posterior mean the blue cross the posterior median and the blue lines denote the and percentiles of the predictive the logistic function sigmx in distribution. figure generated by logregsatdemobayes. solid red with the rescaled probit function x in dotted blue superimposed. here which was chosen so that the derivatives of the two curves match at x based on figure of figure generated by probitplot. figure generated by probitregdemo. shows the average of these samples. by averaging over multiple predictions we see that the uncertainty in the decision boundary splays out as we move further from the training data. so although the decision boundary is linear the posterior predictive density is not linear. note also that the posterior mean decision boundary is roughly equally far from both classes this is the bayesian analog of the large margin principle discussed in section figure shows an example in the red dots denote the mean of the posterior predictive evaluated at the training data. the vertical blue lines denote credible intervals for the posterior predictive the small blue star is the median. we see that with the bayesian approach we are able to model our uncertainty about the probability a student will pass the exam based on his sat score rather than just getting a point estimate. probit approximation output if we have a gaussian approximation to the posterior pwd n vn we can also compute a deterministic approximation to the posterior predictive distribution at least in the binary case. we proceed as follows py sigmwt xpwddw sigman a ada a wt x a e mt n x e a var pwdwt n xt vn x chapter logistic regression thus we see that we need to evaluate the expectation of a sigmoid with respect to a gaussian. this can be approximated by exploiting the fact that the sigmoid function is similar to the probit function which is given by the cdf of the standard normal n a figure plots the sigmoid and probit functions. we have rescaled the axes so that sigma has the same slope as a at the origin where the advantage of using the probit is that one can convolve it with a gaussian analytically an a we now plug in the approximation sigma a to both sides of this equation to get sigman sigm applying this to the logistic regression model we get the following expression suggested in and lauritzen py sigm a a figure indicates that this gives very similar results to the monte carlo approximation. the plug-in estimate. to see this note that and hence using equation is sometimes called a moderated output since it is less extreme than sigm sigm py w where the inequality is strict if if we have py w but the moderated prediction is always closer to so it is less confident. however the decision boundary occurs whenever py sigm which implies wt x hence the decision boundary for the moderated approximation is the same as for the plug-in approximation. so the number of misclassifications will be the same for the two methods but the log-likelihood will not. that in the multiclass case taking into account posterior covariance gives different answers than the plug-in approach see exercise of and williams residual analysis detection it is sometimes useful to detect data cases which are outliers this is called residual analysis or case analysis. in a regression setting this can be performed by computing ri yi yi where yi wt xi. these values should follow a n distribution if the modelling assumptions are correct. this can be assessed by creating a qq-plot where we plot the n theoretical quantiles of a gaussian distribution against the n empirical quantiles of the ri. points that deviate from the straightline are potential outliers. online learning and stochastic optimization classical methods based on residuals do not work well for binary data because they rely on asymptotic normality of the test statistics. however adopting a bayesian approach we can just define outliers to be points which which pyi yi is small where we typically use yi sigm wt xi. note that w was estimated from all the data. a better method is to exclude yi from the estimate of w when predicting yi. that is we define outliers to be points which have low probability under the cross-validated posterior predictive distribution defined by pyixi x i y i pyixi w wpwdw this can be efficiently approximated by sampling methods for further discussion of residual analysis in logistic regression models see e.g.johnson and albert sec online learning and stochastic optimization traditionally machine learning is performed offline which means we have a batch of data and we optimize an equation of the following form f n f zi where zi yi in the supervised case or just xi in the unsupervised case and f zi is some kind of loss function. for example we might use f zi log pyixi in which case we are trying to maximize the likelihood. alternatively we might use f zi lyi hxi where hxi is a prediction function and ly y is some other loss function such as squared error or the huber loss. in frequentist decision theory the average loss is called the risk section so this overall approach is called empirical risk minimization or erm section for details. however if we have streaming data we need to perform online learning so we can update our estimates as each new data point arrives rather than waiting until the end may never occur. and even if we have a batch of data we might want to treat it like a stream if it is too large to hold in main memory. below we discuss learning methods for this kind of a simple implementation trick can be used to speed up batch learning algorithms when applied to data sets that are too large to hold in memory. first note that the naive implementation makes a pass over the data file from the beginning to end accumulating the sufficient statistics and gradients as it goes then an update is performed and the process repeats. unfortunately at the end of each pass the data from the beginning of the file will have been evicted from the cache are are assuming it cannot all fit into memory. rather than going back to the beginning of the file and reloading it we can simply work backwards from the end of the file which is already in memory. we then repeat this forwards-backwards pattern over the data. this simple trick is known as rocking. chapter logistic regression online learning and regret minimization suppose that at each step nature presents a sample zk and the learner must respond with a parameter estimate k. in the theoretical machine learning community the objective used in online learning is the regret which is the averaged loss incurred relative to the best we could have gotten in hindsight using a single fixed parameter value regretk k f t zt min k f zt for example imagine we are investing in the stock-market. let j be the amount we invest in stock j and let zj be the return on this stock. our loss function is f z t z. the regret is how much better worse we did by trading at each step rather than adopting a buy and hold strategy using an oracle to choose which stocks to buy. is as follows at each step k update the parameters using one simple algorithm for online learning is online gradient descent which proj k kgk where projv argminw v is the projection of vector v onto space v gk f k zk is the gradient and k is the step size. projection step is only needed if d. see section for the parameter must be constrained to live in a certain subset of r details. below we will see how this approach to regret minimization relates to more traditional objectives such as mle. there are a variety of other approaches to regret minimization which are beyond the scope of this book e.g. cesa-bianchi and lugosi for details. stochastic optimization and risk minimization now suppose that instead of minimizing regret with respect to the past we want to minimize expected loss in the future as is more common in statistical learning theory. that is we want to minimize f e z where the expectation is taken over future data. optimizing functions where some of the variables in the objective are random is called stochastic suppose we receive an infinite stream of samples from the distribution. one way to optimize stochastic objectives such as equation is to perform the update in equation at each step. this is called stochastic gradient descent or sgd and yudin since we typically want a single parameter estimate we can use a running average t k k note that in stochastic optimization the objective is stochastic and therefore the algorithms will be too. however it is also possible to apply stochastic optimization algorithms to deterministic objectives. examples include simulated annealing and stochastic gradient descent applied to the empirical risk minimization problem. there are some interesting theoretical connections between online learning and stochastic optimization and lugosi but this is beyond the scope of this book. online learning and stochastic optimization this is called polyak-ruppert averaging and can be implemented recursively as follows k k k k k see e.g. kushner and yin for details. setting the step size we now discuss some sufficient conditions on the learning rate to guarantee convergence of sgd. these are known as the robbins-monro conditions k k the set of values of k over time is called the learning rate schedule. various formulas are used such as k or the following bach and moulines k k where slows down early iterations of the algorithm and controls the rate at which old values of are forgotten. the need to adjust these tuning parameters is one of the main drawback of stochastic optimization. one simple heuristic is as follows store an initial subset of the data and try a range of values on this subset then choose the one that results in the fastest decrease in the objective and apply it to all the rest of the data. note that this may not result in convergence but the algorithm can be terminated when the performance improvement on a hold-out set plateaus is called early stopping. per-parameter step sizes one drawback of sgd is that it uses the same step size for all parameters. we now briefly present a method known as adagrad for adaptive gradient et al. which is similar in spirit to a diagonal hessian approximation. also et al. for a similar in particular if ik is parameter i at time k and gik is its gradient then we approach. make an update as follows ik ik gik sik where the diagonal step size vector is the gradient vector squared summed over all time steps. this can be recursively updated as follows sik sik the result is a per-parameter step size that adapts to the curvature of the loss function. this method was original derived for the regret minimization case but it can be applied more generally. chapter logistic regression sgd compared to batch learning if we don t have an infinite data stream we can simulate one by sampling data points at random from our training set. essentially we are optimizing equation by treating it as an expectation with respect to the empirical distribution. algorithm stochastic gradient descent initialize repeat randomly permute data for i n do g f zi proj g update until converged in theory we should sample with replacement although in practice it is usually better to randomly permute the data and sample without replacement and then to repeat. a single such pass over the entire data set is called an epoch. see algorithm for some pseudocode. in this offline case it is often better to compute the gradient of a mini-batch of b data cases. if b this is standard sgd and if b n this is standard steepest descent. typically b is used. although a simple first-order method sgd performs surprisingly well on some problems especially ones with large data sets the intuitive reason for this is that one can get a fairly good estimate of the gradient by looking at just a few examples. carefully evaluating precise gradients using large datasets is often a waste of time since the algorithm will have to recompute the gradient again anyway at the next step. it is often a better use of computer time to have a noisy estimate and to move rapidly through parameter space. as an extreme example suppose we double the training set by duplicating every example. batch methods will take twice as long but online methods will be unaffected since the direction of the gradient has not changed the size of the data changes the magnitude of the gradient but that is irrelevant since the gradient is being scaled by the step size anyway. in addition to enhanced speed sgd is often less prone to getting stuck in shallow local minima because it adds a certain amount of noise consequently it is quite popular in the machine learning community for fitting models with non-convex objectives such as neural networks and deep belief networks the lms algorithm as an example of sgd let us consider how to compute the mle for linear regression in an online fashion. we derived the batch gradient in equation the online gradient at iteration k is given by gk xi t k xi yi online learning and stochastic optimization w black line lms trajectory towards ls soln cross rss vs iteration figure illustration of the lms algorithm. left we start from and slowly converging to the least squares solution of cross. right plot of objective function over time. note that it does not decrease monotonically. figure generated by lmsdemo. where i ik is the training example to use at iteration k. if the data set is streaming we use ik k we shall assume this from now on for notational simplicity. equation is easy to interpret it is the feature vector xk weighted by the difference between what we predicted yk t k xk and the true response yk hence the gradient acts like an error signal. after computing the gradient we take a step along it as follows k k yk ykxk is no need for a projection step since this is an unconstrained optimization problem. this algorithm is called the least mean squares or lms algorithm and is also known as the delta rule or thewidrow-hoff rule. start at and converge the sense that k k of figure shows the results of applying this algorithm to the data shown in figure we drops below a threshold in about iterations. note that lms may require multiple passes through the data to find the optimum. by contrast the recursive least squares algorithm which is based on the kalman filter and which uses second-order information finds the optimum in a single pass section see also exercise the perceptron algorithm now let us consider how to fit a binary logistic regression model in an online manner. the batch gradient was given in equation in the online case the weight update has the simple form k k kgi k k i yixi where i pyi k e k. we see that this has exactly the same form as the lms algorithm. indeed this property holds for all generalized linear models chapter logistic regression we now consider an approximation to this algorithm. specifically let y pyxi yi arg max represent the most probable class label. we replace i py sigm t xi in the gradient expression with yi. thus the approximate gradient becomes gi yi yixi it will make the algebra simpler if we assume y rather than y case our prediction becomes yi sign t xi in this then if yiyi we have made an error but if yiyi we guessed the right label. at each step we update the weight vector by adding on the gradient. the key observation is that if we predicted correctly then yi yi so the gradient is zero and we do not change the weight vector. but if xi is misclassified we update the weights as follows if yi but yi then the negative gradient is yi yixi and if yi but yi then the negative gradient is yi yixi we can absorb the factor of into the learning rate and just write the update in the case of a misclassification as k k kyixi since it is only the sign of the weights that matter not the magnitude we will set k see algorithm for the pseudocode. one can show that this method known as the perceptron algorithm will converge provided the data is linearly separable i.e. that there exist parameters such that predicting with sign t x achieves error on the training set. however if the data is not linearly separable the algorithm will not converge and even if it does converge it may take a long time. there are much better ways to train logistic regression models as using proper sgd without the gradient approximation or irls discussed in section however the perceptron algorithm is historically important it was one of the first machine learning algorithms ever derived frank rosenblatt in and was even implemented in analog hardware. in addition the algorithm can be used to fit models where computing marginals pyix is more expensive than computing the map output arg maxy pyx this arises in some structured-output classification problems. see section for details. a bayesian view another approach to online learning is to adopt a bayesian view. this is conceptually quite simple we just apply bayes rule recursively p pdk this has the obvious advantage of returning a posterior instead of just a point estimate. it also allows for the online adaptation of hyper-parameters which is important since cross-validation cannot be used in an online setting. finally it has the obvious advantage that it can be generative vs discriminative classifiers d yi for i n algorithm perceptron algorithm input linearly separable data set xi r initialize k repeat k k i k mod n if yi yi then k yixi else no-op until converged quicker than sgd. to see why note that by modeling the posterior variance of each parameter in addition to its mean we effectively associate a different learning rate for each parameter freitas et al. which is a simple way to model the curvature of the space. these variances can then be adapted using the usual rules of probability theory. by contrast getting second-order optimization methods to work online is more tricky e.g. et al. sunehag et al. bordes et al. as a simple example in section we show how to use the kalman filter to fit a linear regression model online. unlike the lms algorithm this converges to the optimal answer in a single pass over the data. an extension which can learn a robust non-linear regression model in an online fashion is described in et al. for the glm case we can use an assumed density filter where we approximate the posterior by a gaussian with a diagonal covariance the variance terms serve as a per-parameter step-size. see section for details. another approach is to use particle filtering this was used in et al. for sequentially learning a kernelized linearlogistic regression model. generative vs discriminative classifiers in section we showed that the posterior over class labels induced by gaussian discriminant analysis has exactly the same form as logistic regression namely py sigmwt x. the decision boundary is therefore a linear function of x in both cases. note however that many generative models can give rise to a logistic regression posterior e.g. if each class-conditional density is poisson pxy c poix c. so the assumptions made by gda are much stronger than the assumptions made by logistic regression. a further difference between these models is the way they are trained. when fitting a log pyixi whereas log pyi xi inative model we usually maximize the conditional log likelihood when fitting a generative model we usually maximize the joint log likelihood it is clear that these can in general give different results exercise when the gaussian assumptions made by gda are correct the model will need less training data than logistic regression to achieve a certain level of performance but if the gaussian chapter logistic regression assumptions are incorrect logistic regression will do better and jordan this is because discriminative models do not need to model the distribution of the features. this is illustrated in figure we see that the class conditional densities are rather complex in particular pxy is a multimodal distribution which might be hard to estimate. however the class posterior py cx is a simple sigmoidal function centered on the threshold value of this suggests that in general discriminative methods will be more accurate since their job is in some sense easier. however accuracy is not the only important factor when choosing a method. below we discuss some other advantages and disadvantages of each approach. pros and cons of each approach easy to fit? as we have seen it is usually very easy to fit generative classifiers. for example in sections and we show that we can fit a naive bayes model and an lda model by simple counting and averaging. by contrast logistic regression requires solving a convex optimization problem section for the details which is much slower. fit classes separately? in a generative classifier we estimate the parameters of each class conditional density independently so we do not have to retrain the model when we add more classes. in contrast in discriminative models all the parameters interact so the whole model must be retrained if we add a new class. is also the case if we train a generative model to maximize a discriminative objective salojarvi et al. handle missing features easily? sometimes some of the inputs of x are not observed. in a generative classifier there is a simple method for dealing with this as we discuss in section however in a discriminative classifier there is no principled solution to this problem since the model assumes that x is always available to be conditioned on see for some heuristic approaches. can handle unlabeled training data? there is much interest in semi-supervised learning which uses unlabeled data to help solve a supervised task. this is fairly easy to do using generative models e.g. et al. liang et al. but is much harder to do with discriminative models. symmetric in inputs and outputs? we can run a generative model backwards and infer probable inputs given the output by computing pxy. this is not possible with a discriminative model. the reason is that a generative model defines a joint distribution on x and y and hence treats both inputs and outputs symmetrically. can handle feature preprocessing? a big advantage of discriminative methods is that they allow us to preprocess the input in arbitrary ways e.g. we can replace x with which could be some basis function expansion as illustrated in figure it is often hard to define a generative model on such pre-processed data since the new features are correlated in complex ways. well-calibrated probabilities? some generative models such as naive bayes make strong independence assumptions which are often not valid. this can result in very extreme posterior class probabilities near or discriminative models such as logistic regression are usually better calibrated in terms of their probability estimates. we see that there are arguments for and against both kinds of models. it is therefore useful to have both kinds in your toolbox see table for a summary of the classification and generative vs discriminative classifiers linear multinomial logistic regression kernel rbf multinomial logistic regression figure multinomial logistic regression for classes in the original feature space. after basis function expansion using rbf kernels with a bandwidth of and using all the data points as centers. figure generated by logregmultinomkerneldemo. s e i t i s n e d l a n o i t i d n o c s s a c l x x figure the class-conditional densities pxy c may be more complex than the class posteriors py cx figure generated by generativevsdiscrim. based on figure of regression techniques we cover in this book. dealing with missing data sometimes some of the inputs of x are not observed this could be due to a sensor failure or a failure to complete an entry in a survey etc. this is called the missing data problem and rubin the ability to handle missing data in a principled way is one of the biggest advantages of generative models. to formalize our assumptions we can associate a binary response variable ri that specifies whether each value xi the joint model has the form pxi ri prixi where are the parameters controlling whether the item is observed or not. chapter logistic regression model discriminant analysis naive bayes classifier tree-augmented naive bayes classifier linear regression logistic regression sparse linear logistic regression mixture of experts multilayer perceptron neural network conditional random field k nearest neighbor classifier mixture discriminant analysis classification and regression trees boosted model sparse kernelized linlogreg relevance vector machine support vector machine gaussian processes smoothing splines classifregr classif classif classif regr classif both both both classif classif classif both both both both both both regr gendiscr gen gen gen discrim discrim discrim discrim discrim discrim gen gen discrim discrim discrim discrim discrim discrim discrim paramnon param param param param param param param param param non non non non non non non non non section sec. sec. sec. sec. sec. ch. sec. ch. sec. sec. sec. sec. sec. sec. sec. sec. ch. section is the model suitable for classification regression or both table list of various models for classification and regression which we discuss in this book. columns are as follows model name is the model generative or discriminative is the model parametric or non-parametric list of sections in book which discuss the model. see also tsupervised.html for the pmtk equivalents of these models. any generative probabilistic model hmms boltzmann machines bayesian networks etc. can be turned into a classifier by using it as a class conditional density. is observed or not. if we assume prixi pri we say the data is missing completely at random or mcar. if we assume prixi prixo i is the observed part of xi we say the data is missing at random or mar. if neither of these assumptions hold we say the data is not missing at random or nmar. in this case we have to model the missing data mechanism since the pattern of missingness is informative about the values of the missing data and the corresponding parameters. this is the case in most collaborative filtering problems for example. see e.g. for further discussion. we will henceforth assume the data is mar. i where xo when dealing with missing data it is helpful to distinguish the cases when there is missingness only at test time the training data is complete data from the harder case when there is missingness also at training time. we will discuss these two cases below. note that the class label is always missing at test time by definition if the class label is also sometimes missing at training time the problem is called semi-supervised learning. generative vs discriminative classifiers missing data at test time in a generative classifier we can handle features that are mar by marginalizing them out. for example if we are missing the value of we can compute py py c c py c c if we make the naive bayes assumption the marginalization can be performed as follows c pxj jc pxj jc c hence in a naive bayes classifier we where we exploited the fact that can simply ignore missing features at test time. similarly in discriminant analysis no matter what regularization method was used to estimate the parameters we can always analytically marginalize out the missing variables section c n missing data at training time missing data at training time is harder to deal with. in particular computing the mle or map estimate is no longer a simple optimization problem for reasons discussed in section however soon we will study are a variety of more sophisticated algorithms as em algorithm in section for finding approximate ml or map estimates in such cases. fisher s linear discriminant analysis discriminant analysis is a generative approach to classification which requires fitting an mvn to the features. as we have discussed this can be problematic in high dimensions. an alternative approach is to reduce the dimensionality of the features x r d and then fit an mvn to the resulting low-dimensional features z r l. the simplest approach is to use a linear projection matrix z wx where w is a l d matrix. one approach to finding w would be to use pca the result would be very similar to rda since svd and pca are essentially equivalent. however pca is an unsupervised technique that does not take class labels into account. thus the resulting low dimensional features are not necessarily optimal for classification as illustrated in figure an alternative approach is to find the matrix w such that the low-dimensional data can be classified as well as possible using a gaussian class-conditional density model. the assumption of gaussianity is reasonable since we are computing linear combinations of non-gaussian features. this approach is called fisher s linear discriminant analysis orflda. flda is an interesting hybrid of discriminative and generative techniques. the drawback of this technique is that it is restricted to using l c dimensions regardless of d for reasons that we will explain below. in the two-class case this means we are seeking a single vector w onto which we can project the data. below we derive the optimal w in the two-class case. we chapter logistic regression means fisher pca fisher pca figure example of fisher s linear discriminant. two class data in dashed green line first principal basis vector. dotted red line fisher s linear discriminant vector. solid black line joins the class-conditional means. projection of points onto pca vector shows poor class separation. figure generated by fisherldademo. projection of points onto fisher s vector shows good class separation. generative vs discriminative classifiers then generalize to the multi-class case and finally we give a probabilistic interpretation of this technique. derivation of the optimal projection we now derive this optimal direction w for the two-class case following the presentation of sec define the class-conditional means as xi xi let mk wt k be the projection of each mean onto the line w. also let zi wt xi be the projection of the data onto the line. the variance of the projected points is proportional to k iyik the goal is to find w such that we maximize the distance between the means while also ensuring the projected clusters are tight jw we can rewrite the right hand side of the above in terms of w as follows jw wt sbw wt sw w where sb is the between-class scatter matrix given by sb and sw is the within-class scatter matrix given by sw to see this note that wt sbw wt w and wt sw w wt w wt equation is a ratio of two scalars we can take its derivative with respect to w and equate to zero. one can show that that jw is maximized when sbw sw w chapter logistic regression figure pca projection of vowel data to flda projection of vowel data to we see there is better class separation in the flda case. based on figure of et al. figure generated by fisherdiscrimvoweldemo by hannes bretschneider. where wt sbw wt sw w equation is called a generalized eigenvalue problem. if sw is invertible we can convert it to a regular eigenvalue problem s w sbw w however in the two class case there is a simpler solution. in particular since sbw w then from equation we have w s w s w w since we only care about the directionality and not the scale factor we can just set w s w this is the optimal solution in the two-class case. if sw i meaning the pooled covariance matrix is isotropic then w is proportional to the vector that joins the class means. this is an intuitively reasonable direction to project onto as shown in figure extension to higher dimensions and multiple classes we can extend the above idea to multiple classes and to higher dimensional subspaces by finding a projection matrix w which maps from d to l so as to maximize jw bwt w wt generative vs discriminative classifiers where b w c c c nc c c nc n c nc n cxi ct iyic the solution can be shown to be w w u w assuming w is non-singular. it where u are the l leading eigenvectors of is singular we can first perform pca on all the data. w b figure gives an example of this method applied to some d dimensional speech data representing c different vowel sounds. we see that flda gives better class separation than pca. note that flda is restricted to finding at most a l c dimensional linear subspace no matter how large d because the rank of the between class covariance matrix b is c term arises because of the term which is a linear function of the c. this is a rather severe restriction which limits the usefulness of flda. probabilistic interpretation of flda to find a valid probabilistic interpretation of flda we follow the approach of and andreo zhou et al. they proposed a model known as heteroscedastic lda which works as follows. let w be a d d invertible matrix and let zi wxi be a transformed version of the data. we now fit full covariance gaussians to the transformed data one per class but with the constraint that only the first l components will be class-specific the remaining h d l components will be shared across classes and will thus not be discriminative. that is we use where is the shared h dimensional mean and is the shared h h covariace. the pdf of the original data is given by pxiyi c w n c c n sc n for fixed w it is easy to derive the mle for one can then optimize pzi yi c c c c sc c where w w using gradient methods. wl wh chapter logistic regression in the special case that the c are diagonal there is a closed-form solution for w and in the special case the c are all equal we recover classical lda et al. in view of this this result it should be clear that hlda will outperform lda if the class covariances are not equal within the discriminative subspace if the assumption that c is independent of c is a poor assumption. this is easy to demonstrate on synthetic data and is also the case on more challenging tasks such as speech recognition and andreo furthermore we can extend the model by allowing each class to use its own projection matrix this is known as multiple lda exercises exercise spam classification using logistic regression consider the email spam data set discussed on of et al. this consists of email messages from which features have been extracted. these are as follows features in giving the percentage of words in a given message which match a given word on the list. the list contains words such as business free george etc. data was collected by george forman so his name occurs quite a lot. features in giving the percentage of characters in the email that match a given character on the list. the characters are feature the average length of an uninterrupted sequence of capital letters is mean is feature the length of the longest uninterrupted sequence of capital letters is mean is feature the sum of the lengts of uninterrupted sequence of capital letters is mean is load the data from spamdata.mat which contains a training set size and a test set size one can imagine performing several kinds of preprocessing to this data. try each of the following separately a. standardize the columns so they all have mean and unit variance. b. transform the features using logxij c. binarize the features using ixij for each version of the data fit a logistic regression model. use cross validation to choose the strength of the regularizer. report the mean error rate on the training and test sets. you should get numbers similar to this method stnd log binary train test precise values will depend on what regularization value you choose. turn in your code and numerical results. also exercise exercise spam classification using naive bayes we will re-examine the dataset from exercise generative vs discriminative classifiers a. use naivebayesfit and naivebayespredict on the binarized spam data. what is the training and can try different settings of the pseudocount if you like corresponds to the test error? beta prior each jc although the default of is probably fine. turn in your error rates. b. modify the code so it can handle real-valued features. use a gaussian density for each feature fit it with maximum likelihood. what are the training and test error rates on the standardized data and the log transformed data? turn in your error rates and code. exercise gradient and hessian of log-likelihood for logistic regression a. let a be the sigmoid function. show that d da log likelihood b. using the previous result and the chain rule of calculus derive an expression for the gradient of the c. the hessian can be written as h xt sx where s diag n. show may assume that i so the elements of s will be strictly that h is positive definite. positive and that x is full rank. exercise gradient and hessian of log-likelihood for multinomial logistic regression a. let ik s ik. prove that the jacobian of the softmax is ik ij ik kj ij where kj ik j. b. hence show that wc icxi hint use the chain rule and the fact that c yic c. show that the block submatrix of the hessian for classes c and is given by ic i i i exercise symmetric version of regularized multinomial logistic regression ex of et al. multiclass logistic regression has the form py cx w wt c x wt k x where w is a c weight matrix. we can arbitrarily define wc for one of the classes say c c since py cx w py cx w. in this case the model has the form c x py cx w wt wt k x chapter logistic regression if we don t clamp one of the vectors to some constant value the parameters will be unidentifiable. however suppose we don t clamp wc so we are using equation but we add regularization by optimizing log pyixi w show that at the optimum we have still need to enforce that to ensure identifiability of the offset. wcj for j the unregularized terms we exercise elementary properties of regularized logistic regression jaaakkola.. consider minimizing jw where i d log i w is the average log-likelihood on data set d for yi answer the following true false questions. a. jw has multiple locally optimal solutions tf? b. let w arg minw jw be a global optimum. w is sparse many zero entries tf? c. d. wdtrain always increases as we increase tf? e. wdtest always increases as we increase tf? if the training data is linearly separable then some weights wj might become infinite if tf? exercise regularizing separate terms in logistic regression jaaakkola. a. consider the data in figure where we fit the model py w suppose we fit the model by maximum likelihood i.e. we minimize jw where is the log likelihood on the training set. sketch a possible decision boundary corresponding to w. answer on your copy since you will need multiple versions of this figure. boundary unique? how many classification errors does your method make on the training set? the figure first rough sketch is enough and then superimpose your is your answer b. now suppose we regularize only the parameter i.e. we minimize w suppose is a very large number so we regularize all the way to but all other parameters are unregularized. sketch a possible decision boundary. how many classification errors does your method make on the training set? hint consider the behavior of simple linear regression when c. now suppose we heavily regularize only the parameter i.e. we minimize w sketch a possible decision boundary. how many classification errors does your method make on the training set? generative vs discriminative classifiers figure data for logistic regression question. d. now suppose we heavily regularize only the parameter. sketch a possible decision boundary. how many classification errors does your method make on the training set? generalized linear models and the exponential family introduction we have now encountered a wide variety of probability distributions the gaussian the bernoulli the student t the uniform the gamma etc. it turns out that most of these are members of a broader class of distributions known as the exponential in this chapter we discuss various properties of this family. this allows us to derive theorems and algorithms with very broad applicability. we will see how we can easily use any member of the exponential family as a class-conditional density in order to make a generative classifier. in addition we will discuss how to build discriminative models where the response variable has an exponential family distribution whose mean is a linear function of the inputs this is known as a generalized linear model and generalizes the idea of logistic regression to other kinds of response variables. the exponential family before defining the exponential family we mention several reasons why it is important it can be shown that under certain regularity conditions the exponential family is the only family of distributions with finite-sized sufficient statistics meaning that we can compress the data into a fixed-sized summary without loss of information. this is particularly useful for online learning as we will see later. the exponential family is the only family of distributions for which conjugate priors exist which simplifies the computation of the posterior section the exponential family can be shown to be the family of distributions that makes the least set of assumptions subject to some user-chosen constraints section the exponential family is at the core of generalized linear models as discussed in section the exponential family is at the core of variational inference as discussed in section the exceptions are the student t which does not have the right form and the uniform distribution which does not have fixed support independent of the parameter values. chapter generalized linear models and the exponential family definition a pdf or pmf px for x xm x m and r exponential family if it is of the form d is said to be in the px z hx exp t hx exp t a x m hx exp t where z a log z here are called the natural parameters or canonical parameters r d is called a vector of sufficient statistics z is called the partition function a is called the log partition function or cumulant function and hx is the a scaling constant often if x we say it is a natural exponential family. equation can be generalized by writing px hx exp a where is a function that maps the parameters to the canonical parameters if dim dim it is called a curved exponential family which means we have more if the model is said to be in canonical form. sufficient statistics than parameters. we will assume models are in canonical form unless we state otherwise. examples let us consider some examples to make things clearer. bernoulli the bernoulli for x can be written in exponential family form as follows berx x expx log x exp where ix and however this representation is over-complete since there is a linear dependendence between the features ix consequently is not uniquely identifiable. it is common to require that the representation be minimal which means there is a unique associated with the distribution. in this case we can just define berx exp x log catx k exp xk k k k exp exp exp xk log k xk log xk log k k xk log k k k j k k k k xk log k where k catx exp t a k k log k ix k k. we can write this in exponential family form as follows the exponential family now we have log can recover the mean parameter from the canonical parameter using which is the log-odds ratio and z we sigm e multinoulli we can represent the multinoulli as a minimal exponential family as follows xk ix k we can recover the mean parameters from the canonical parameters using k e k e j from this we find k e j and hence a log e j k e k e j if we define k we can write s and a log softmax function in equation e k where s is the chapter generalized linear models and the exponential family univariate gaussian the univariate gaussian can be written in exponential family form as follows n exp exp x z exp t x exp log where z a non-examples not all distributions of interest belong to the exponential family. for example the uniform distribution x unifa b does not since the support of the distribution depends on the parameters. also the student t distribution does not belong since it does not have the required form. log partition function an important property of the exponential family is that derivatives of the log partition function for this reason a is can be used to generate cumulants of the sufficient sometimes called a cumulant function. we will prove this for a distribution this can be generalized to a k-parameter distribution in a straightforward way. for the first the first and second cumulants of a distribution are its mean e and variance var whereas the first and second moments are its mean e and e x the exponential family derivative we have log exp exp exp exp expa exp a e da d d d d d for the second derivative we have d exp a hx e var e where we used the fact that da in the multivariate case we have that d e i j e ix jx e ix e jx and hence cov since the covariance is positive definite we see that a is a convex function section example the bernoulli distribution for example consider the bernoulli distribution. we have a e so the mean is given by da d e e e sigm the variance is given by d d d e e e e e e e chapter generalized linear models and the exponential family mle for the exponential family the likelihood of an exponential family model has the form hxi g exp we see that the sufficient statistics are n and kxi pd for example for the bernoulli model we have gaussian we have i xi i i also need to know the sample size n i ixi and for the univariate the pitman-koopman-darmois theorem states that under certain regularity conditions the exponential family is the only family of distributions with finite sufficient statistics. finite means of a size independent of the size of the data set. one of the conditions required in this theorem is that the support of the distribution not be dependent on the parameter. for a simple example of such a distribution consider the uniform distribution px u x the likelihood is given by pd n maxxi so the sufficient statistics are n and sd maxi xi. this is finite in size but the uniform distribution is not in the exponential family because its support set x depends on the parameters. n iid data points d xn the log-likelihood is we now descibe how to compute the mle for a canonical exponential family model. given log pd t n a since a is concave in and t is linear in we see that the log likelihood is concave and hence has a unique global maximum. to derive this maximum we use the fact that the derivative of the log partition function yields the expected value of the sufficient statistic vector log pd n e setting this gradient to zero we see that at the mle the empirical average of the sufficient statistics must equal the model s theoretical expected sufficient statistics i.e. must satisfy e n the exponential family this is called moment matching. for example in the bernoulli distribution we have ix so the mle satisfies e px n ixi bayes for the exponential family we have seen that exact bayesian analysis is considerably simplified if the prior is conjugate to the likelihood. informally this means that the prior p has the same form as the likelihood pd for this to make sense we require that the likelihood have finite sufficient statistics so that we can write pd psd this suggests that the only family of distributions for which conjugate priors exist is the exponential family. we will derive the form of the prior and posterior below. likelihood the likelihood of the exponential family is given by pd g exp sn pd expn t s n a where sn sxi. in terms of the canonical parameters this becomes where s n sn prior the natural conjugate prior has the form p g exp let us write to separate out the size of the prior pseudo-data from the mean of the sufficient statistics on this pseudo-data in canonical form the prior becomes p exp t posterior the posterior is given by p p n n p n sn so we see that we just update the hyper-parameters by adding. in canonical form this becomes p exp t n s n p n n s n so we see that the posterior hyper-parameters are a convex combination of the prior mean hyper-parameters and the average of the sufficient statistics. chapter generalized linear models and the exponential family posterior predictive density let us derive a generic expression for the predictive density for future observables given past data d xn as follows. for notational brevity we will combine the sufficient statistics with the size of the data as follows sd sd and so the prior becomes p z g exp the likelihood and posterior have a similar form. hence h xi z sd z sd z sd k k k h xi exp g d d sk xi skxi if n this becomes the marginal likelihood of normalizer of the posterior divided by the normalizer of the prior multiplied by a constant. which reduces to the familiar form of example bernoulli distribution as a simple example let us revisit the beta-bernoulli model in our new notation. the likelihood is given by pd exp log hence the conjugate prior is given by p exp log i xi if we define and we see that this is a beta distribution. i ixi is the sufficient statistic we can derive the posterior as follows where s p s n n n we can derive the posterior predictive distribution as follows. assume p beta and let s sd be the number of heads in the past data. we can predict the probability of a the exponential family given sequence of future heads as follows n nd n n n n n n n n where nm nm nm nm nm nm n nm n s s maximum entropy derivation of the exponential family xm with sufficient statistic i xi although the exponential family is convenient is there any deeper justification for its use? it turns out that there is it is the distribution that makes the least number of assumptions about the data subject to a specific set of user-specified constraints as we explain below. in particular suppose all we know is the expected values of certain features or functions fkxpx fk x where fk are known constants and fkx is an arbitrary function. the principle of maximum entropy or maxent says we should pick the distribution with maximum entropy to uniform subject to the constraints that the moments of the distribution match the empirical moments of the specified functions. px and jp to maximize entropy subject to the constraints in equation and the constraints that x px we need to use lagrange multipliers. the lagrangian is given by px log px kfk px pxfkx x x k x we can use the calculus of variations to take derivatives wrt the function p but we will adopt a simpler approach and treat p as a fixed length vector we are assuming x is discrete. then we have k j px log px setting j px yields exp z k px kfkx kfkx chapter generalized linear models and the exponential family w xi g g i i i figure a visualization of the various features of a glm. based on figure of where z using the sum to one constraint we have x px z exp x k kfkx hence the normalization constant is given by z exp kfkx x k thus the maxent distribution px has the form of the exponential family also known as the gibbs distribution. generalized linear models linear and logistic regression are examples of generalized linear models or glms and nelder these are models in which the output density is in the exponential family and in which the mean parameters are a linear combination of the inputs passed through a possibly nonlinear function such as the logistic function. we describe glms in more detail below. we focus on scalar outputs for notational simplicity. excludes multinomial logistic regression but this is just to simplify the presentation. basics to understand glms let us first consider the case of an unconditional dstribution for a scalar response variable pyi exp cyi yi a where is the dispersion parameter set to is the natural parameter a is the partition function and c is a normalization constant. for example in the case of logistic regression is the log-odds ratio log where e y is the mean parameter section to convert from the mean parameter to the natural parameter generalized linear models distrib. n binn poi link g identity logit log log log sigm e e table canonical link functions and their inverses for some common glms. we can use a function so this function is uniquely determined by the form of the exponential family distribution. in fact this is an invertible mapping so we have furthermore we know from section that the mean is given by the derivative of the partition function so we have now let us add inputs covariates. we first define a linear function of the inputs i wt xi we now make the mean of the distribution be some invertible monotonic function of this linear combination. by convention this function known as the mean function is denoted by g so i g i g xi see figure for a summary of the basic model. the inverse of the mean function namely g is called the link function. we are free to choose almost any function we like for g so long as it is invertible and so long as g has the appropriate range. for example in logistic regression we set i g i sigm i. one particularly simple form of link function is to use g this is called the canonical link function. in this case i i wt xi so the model becomes pyixi w exp cyi yiwt xi awt xi in table we list some distributions and their canonical link functions. we see that for the bernoulli binomial distribution the canonical link is the logit function g log whose inverse is the logistic function sigm based on the results in section we can show that the mean and variance of the response variable are as follows yxi w yxi w e var i i i i to make the notation clearer let us consider some simple examples. for linear regression we have log pyixi w yi i i i where yi r and i i wt xi here a soe i and var chapter generalized linear models and the exponential family for binomial regression we have log pyixi w i log i i i i log ni yi where yi ni i sigmwt xi i log i wt xi and here a ni e so e ni i i var ni i. for poisson regression we have log pyixi w i log i i logyi! where yi i expwt xi i log i wt xi and here a so e var i. poisson regression is widely used in bio-statistical applications where yi might represent the number of diseases of a given person or place or the number of reads at a genomic location in a high-throughput sequencing context e.g. et al. ml and map estimation one of the appealing properties of glms is that they can be fit using exactly the same methods that we used to fit logistic regression. in particular the log-likelihood has the following form log pdw iyi a i we can compute the gradient vector using the chain rule as follows dwj d i d i d i i d i d i i d i d i d i dwj d i d i d i d i xij d i d i xij if we use a canonical link i i this simplifies to ixi which is a sum of the input vectors weighted by the errors. this can be used inside a gradient descent procedure discussed in section however for improved efficiency we should use a second-order method. if we use a canonical link the hessian is given by h d i d i xixt i xt sx probit regression name logistic probit log-log complementary log-log formula g sigm e g g exp exp g exp exp table summary of some possible mean functions for binary regression. where s diag d d irls algorithm specifically we have the following newton update is a diagonal weighting matrix. this can be used inside the d n d n stx zt t s stzt t t where t xwt and t g t. if we extend the derivation to handle non-canonical links we find that the hessian has another term. however it turns out that the expected hessian is the same as in equation using the expected hessian as the fisher information matrix instead of the actual hessian is known as the fisher scoring method. it is straightforward to modify the above procedure to perform map estimation with a gaussian prior we just modify the objective gradient and hessian just as we added regularization to logistic regression in section bayesian inference bayesian inference for glms is usually conducted using mcmc possible methods include metropolis hastings with an irls-based proposal gibbs sampling using adaptive rejection sampling for each full-conditional and smith etc. see e.g. et al. for futher information. it is also possible to use the gaussian approximation or variational inference probit regression in logistic regression we use a model of the form py w sigmwt xi. in general we can write py w g xi for any function g that maps to several possible mean functions are listed in table in this section we focus on the case where g where is the cdf of the standard normal. this is known as probit regression. the probit function is very similar to the logistic function as shown in figure however this model has some advantages over logistic regression as we will see. chapter generalized linear models and the exponential family mlmap estimation using gradient-based optimization we can find the mle for probit regression using standard gradient methods. let i wt xi and let yi then the gradient of the log-likelihod for a specific case is given by gi d dw log p yiwt xi d i dw d d i log p yiwt xi xi yi i yi i where is the standard normal pdf and is its cdf. similarly the hessian for a single case is given by hi d log p yiwt xi xi xt i yi i i yi i yi we can modify these expressions to compute the map estimate in a straightforward manner. in particular if we use the prior pw the gradient and hessian of the penalized these expressions can be log likelihood have the form passed to any gradient-based optimizer. see probitregdemo for a demo. i hi i gi w and latent variable interpretation we can interpret the probit logistic model as follows. first let us associate each item xi with two latent utilities and corresponding to the possible choices of yi and yi we then assume that the observed choice is whichever action has larger utility. more precisely the model is as follows wt xi wt xi yi where s are error terms representing all the other factors that might be relevant in decision making that we have chosen not to are unable to model. this is called a random utility model or rum train since it is only the difference in utilities that matters let us define zi where if the s have a gaussian distribution then so does thus we can write zi wt xi n yi izi following and fruhwirth we call this the difference rum or drum model. when we marginalize out zi we recover the probit model pyi w izi xi pwt xi p wt xi wt xi xi probit regression where we used the symmetry of the this latent variable interpretation provides an alternative way to fit the model as discussed in section interestingly if we use a gumbel distribution for the s we induce a logistic distibution for and the model reduces to logistic regression. see section for further details. ordinal probit regression one advantage of the latent variable interpretation of probit regression is that it is easy to extend to the case where the response variable is ordinal that is it can take on c discrete values which can be ordered in some way such as low medium and high. this is called ordinal regression. the basic idea is as follows. we introduce c thresholds j and set if j zi j yi j where c. for identifiability reasons we set and c for example if c this reduces to the standard binary probit model whereby zi produces yi and zi produces yi if c we partition the real line into intervals we can vary the parameter to ensure the right relative amount of probability mass falls in each interval so as to match the empirical frequencies of each class label. finding the mles for this model is a bit trickier than for binary probit regression since we need to optimize for w and and the latter must obey an ordering constraint. see e.g. and largey for an approach based on em. it is also possible to derive a simple gibbs sampling algorithm for this model e.g. multinomial probit models now consider the case where the response variable can take on c unordered categorical values yi c. the multinomial probit model is defined as follows zic wt xic n r yi arg max zic c and endersby scott fruhwirth-schnatter and fruhwirth for see e.g. more details on the model and its connection to multinomial logistic regression. defining w wc and xic xi we can recover the more familiar formulation zic xt i wc. since only relative utilities matter we constrain r to be a correlation matrix. if instead of setting yi argmaxc zic we use yic izic we get a model known as multivariate probit which is one way to model c correlated binary outcomes e.g. et al. note that the assumption that the gaussian noise term is zero mean and unit variance is made without loss of generality. to see why suppose we used some other mean and variance then we could easily rescale w and add an offset term without changing the likelihood. since p wt x p x chapter generalized linear models and the exponential family multi-task learning sometimes we want to fit many related classification or regression models. it is often reasonable to assume the input-output mapping is similar across these different models so we can get better performance by fitting all the parameters at the same time. in machine learning this setup is often called multi-task learning transfer learning et al. or learning to learn and pratt in statistics this is usually tackled using hierarchical bayesian models and heskes as we discuss below although there are other possible methods e.g. hierarchical bayes for multi-task learning let yij be the response of the i th item in groupj for i j and j for example j might index schools i might index students within a school and yij might be the test score as in section or j might index people and i might index purchaes and yij might be the identity of the item that was purchased is known as discrete choice modeling let xij be a feature vector associated with yij. the goal is to fit the models pyjxj for all j. although some groups may have lots of data there is often a long tail where the majority of groups have little data. thus we can t reliably fit each model separately but we don t want to use the same model for all groups. as a compromise we can fit a separate model for each group but encourage the model parameters to be similar across groups. more precisely suppose e gxt ij j where g is the link function for the glm. furthermore suppose j n in this model groups with small sample size borrow statistical strength from the groups with larger sample size because the j s are correlated via the latent common parents section for further discussion of this point. the term j controls how much group j depends on the common parents and the term controls the strength of the overall prior. j i and that n i. suppose for simplicity that and that j and are all known they could be set by cross validation. the overall log probability has the form log pd log p log pdj j j j j we can perform map estimation of using standard gradient methods. alternatively we can perform an iterative optimization scheme alternating between optimizing the j and the since the likelihood and prior are convex this is guaranteed to converge to the global optimum. note that once the models are trained we can discard and use each model separately. application to personalized email spam filtering an interesting application of multi-task learning is personalized spam filtering. suppose we want to fit one classifier per user j. since most users do not label their email as spam or not it will be hard to estimate these models independently. so we will let the j have a common prior representing the parameters of a generic user. multi-task learning in this case we can emulate the behavior of the above model with a simple trick attenberg et al. weinberger et al. we make two copies of each feature xi one concatenated with the user id and one not. the effect will be to learn a predictor of the form e u wj iu iu jxi where u is the user id. in other words e u j t wjt xi thus will be estimated from everyone s email whereas wj will just be estimated from user j s email. to see the correspondence with the above hierarchical bayesian model define wj j then the log probability of the original model can be rewritten as log pdj wj j j if we assume j the effect is the same as using the augmented feature trick with the same regularizer strength for both wj and however one typically gets better performance by not requiring that j be equal to and manning application to domain adaptation domain adaptation is the problem of training a set of classifiers on data drawn from different distributions such as email and newswire text. this problem is obviously a special case of multi-task learning where the tasks are the same. and manning used the above hierarchical bayesian model to perform domain adaptation for two nlp tasks namely named entity recognition and parsing. they report reasonably large improvements over fitting separate models to each dataset and small improvements over the approach of pooling all the data and fitting a single model. other kinds of prior in multi-task learning it is common to assume that the prior is gaussian. however sometimes other priors are more suitable. for example consider the task of conjoint analysis which requires figuring out which features of a product customers like best. this can be modelled using the same hierarchical bayesian setup as above but where we use a sparsity-promoting prior on j rather than a gaussian prior. this is called multi-task feature selection. see e.g. et al. argyriou et al. for some possible approaches. it is not always reasonable to assume that all tasks are all equally similar. if we pool the parameters across tasks that are qualitatively different the performance will be worse than not using pooling because the inductive bias of our prior is wrong. indeed it has been found experimentally that sometimes multi-task learning does worse than solving each task separately is called negative transfer. chapter generalized linear models and the exponential family one way around this problem is to use a more flexible prior such as a mixture of gaussians. such flexible priors can provide robustness against prior mis-specification. see e.g. et al. jacob et al. for details. one can of course combine mixtures with sparsity-promoting priors et al. many other variants are possible. generalized linear mixed models suppose we generalize the multi-task learning scenario to allow the response to include information at the group level xj as well as at the item level xij. similarly we can allow the parameters to vary across groups j or to be tied across groups this gives rise to the following model j e xj g j where the k are basis functions. this model can be represented pictorially as shown in figures will be explained in chapter note that the number of j figure parameters grows with the number of groups whereas the size of is fixed. frequentists call the terms j random effects since they vary randomly across groups but they call a fixed effect since it is viewed as a fixed but unknown constant. a model with both fixed and random effects is called a mixed model. if pyx is a glm the overall model is called a generalized linear mixed effects model or glmm. such models are widely used in statistics. example semi-parametric glmms for medical data consider the following example from suppose yij is the amount of spinal bone mineral density for person j at measurement i. let xij be the age of person and let xj be their ethnicity which can be one of white asian black or hispanic. the primary goal is to determine if there are significant differences in the mean sbmd among the four ethnic groups after accounting for age. the data is shown in the light gray lines in figure we see that there is a nonlinear effect of sbmd vs age so we will use a semi-parametric model which combines linear regression with non-parametric regression et al. we also see that there is variation across individuals within each group so we will use a mixed effects model. specifically we will use to account for the random effect of each person since no other coefficients are person-specific where bk is the k th spline basis functions section to account for the nonlinear effect of age and w ixj a ixj b ixj h to account for the effect of the different ethnicities. furthermore we use a linear link function. the overall model is therefore e xj j t bxij ij w ixj w where n y. contains the non-parametric part of the model related to age contains the parametric part of the model related to ethnicity and j is a random offset for person j. we endow all of these regression coefficients with separate gaussian priors. we can then perform posterior inference to compute p section for hixj h aixj a bixj b generalized linear mixed models y j y ij xij nj j xj hispanic white m c g y t i s n e d l i a r e n m e n o b l a n p s i asian black age in years figure directed graphical model for generalized linear mixed effects model with j groups. spinal bone mineral density vs age for four different ethnic groups. raw data is shown in the light gray lines. fitted model shown in black is the posterior predicted mean dotted is the posterior predictive variance. from figure of used with kind permission of matt wand chapter generalized linear models and the exponential family computational details. after fitting the model we can compute the prediction for each group. see figure for the results. we can also perform significance testing by computing p g wd for each ethnic group g relative to some baseline white as we did in section computational issues the principle problem with glmms is that they can be difficult to fit for two reasons. first pyij may not be conjugate to the prior p where second there are two levels of unknowns in the model namely the regression coefficients and the means and variances of the priors one approach is to adopt fully bayesian inference methods such as variational bayes et al. or mcmc and hill we discuss vb in section and mcmc in section an alternative approach is to use empirical bayes which we discuss in general terms in section in the context of a glmm we can use the em algorithm where in the e step we compute p and in the m step we optimize if the linear regression setting the e step can be performed exactly but in general we need to use approximations. traditional methods use numerical quadrature or monte carlo e.g. and clayton a faster approach is to use variational em see and mcauliffe for an application of variational em to a multi-level discrete choice modeling problem. in frequentist statistics there is a popular method for fitting glmms called generalized estimating equations or gee and hilbe however we do not recommend this approach since it is not as statistically efficient as likelihood-based methods section in addition it can only provide estimates of the population parameters but not the random effects j which are sometimes of interest in themselves. learning to rank in this section we discuss the learning to rank or letor problem. that is we want to learn a function that can rank order a set of items will be more precise below. the most common application is to information retrieval. specifically suppose we have a query q and a set of documents dm that might be relevant to q all documents that contain the string q. we would like to sort these documents in decreasing order of relevance and show the top k to the user. similar problems arise in other areas such as collaborative filtering. players in a game or tournament setting is a slightly different kind of problem see section below we summarize some methods for solving this problem following the presentation of this material is not based on glms but we include it in this chapter anyway for lack of a better place. a standard way to measure the relevance of a document d to a query q is to use a probabilistic n language model based on a bag of words model. that is we define simq d pqd is the i th word or term and pqid is a multinoulli distribution pqid where qi estimated from document d. in practice we need to smooth the estimated distribution for example by using a dirichlet prior representing the overall frequency of each word. this can be learning to rank estimated from all documents in the system. more precisely we can use ptd tft d lend ptbackground where tft d is the frequency of term t in document d lend is the number of words in d and is a smoothing parameter e.g. zhai and lafferty for details. however there might be many other signals that we can use to measure relevance. for example the pagerank of a web document is a measure of its authoritativeness derived from the web s link structure section for details. we can also compute how often and where the query occurs in the document. below we discuss how to learn how to combine all these the pointwise approach suppose we collect some training data representing the relevance of a set of documents for each query. specifically for each query q suppose that we retrieve m possibly relevant documents dj for j for each query document pair we define a feature vector xq d. for example this might contain the query-document similarity score and the page rank score of the document. furthermore suppose we have a set of labels yj representing the degree of relevance of document dj to query q. such labels might be binary relevant or irrelevant or they may represent a degree of relevance very relevant somewhat relevant irrelevant. such labels can be obtained from query logs by thresholding the number of times a document was clicked on for a given query. if we have binary relevance labels we can solve the problem using a standard binary classification scheme to estimate py d. if we have ordered relevancy labels we can use ordinal regression to predict the rating py rxq d. in either case we can then sort the documents by this scoring metric. this is called the pointwise approach to letor and is widely used because of its simplicity. however this method does not take into account the location of each document in the list. thus it penalizes errors at the end of the list just as much as errors at the beginning which is often not the desired behavior. in addition each decision about relevance is made very myopically. the pairwise approach there is evidence et al. that people are better at judging the relative relevance of two items rather than absolute relevance. consequently the data might tell us that dj is more relevant than dk for a given query or vice versa. we can model this kind of data using a binary classifier of the form pyjkxq dj xq dk where we set yjk if reldj q reldk q and yjk otherwise. one way to model such a function is as follows pyjk xk sigmf f rather surprisingly google does not at least did not as of using such learning methods in its search engine. source peter norvig quoted in rone-to-catastrophic-errors-than-machine-learned-models.html. chapter generalized linear models and the exponential family where f is a scoring function often taken to be linear f t x. this is a special kind of neural network known as ranknet et al. section for a general discussion of neural networks. we can find the mle of w by maximizing the log likelihood or equivalently by minimizing the cross entropy loss given by l lijk lijk iyijk log pyijk xik w log pyijk xik w this can be optimized using gradient descent. a variant of ranknet is used by microsoft s bing search the listwise approach the pairwise approach suffers from the problem that decisions about relevance are made just based on a pair of items rather than considering the full context. we now consider methods that look at the entire list of items at the same time. we can define a total order on a list by specifying a permutation of its indices to model our uncertainty about we can use the plackett-luce distribution which derives its name from independent work by and this has the following form p uj su where sj s is the score of the document ranked at the j th position. to understand equation let us consider a simple example. suppose b c. then we have that p is the probability of a being ranked first times the probability of b being ranked second given that a is ranked first times the probabilty of c being ranked third given that a and b are ranked first and second. in other words sb sa sb sc p to incorporate features we can define sd d where we often take f to be a linear function f t x. this is known as the listnet model et al. to train this model let yi be the relevance scores of the documents for query i. we then minimize the cross entropy term sc sc sb sc sa of course as stated this is intractable since the i th term needs to sum over mi! permutations. to make this tractable we can consider permutations over the top k positions only i p log p p su source eatures-and-the-science-behind-bing.aspx. learning to rank there are only m!m k! such permutations. entropy term its derivative in om time. if we set k we can evaluate each cross in the special case where only one document from the presented list is deemed relevant say yi c we can instead use multinomial logistic regression pyi cx expsc this often performs at least as well as ranking methods at least in the context of collaborative filtering et al. loss functions for ranking there are a variety of ways to measure the performance of a ranking system which we summarize below. mean reciprocal rank for a query q let the rank position of its first relevant document be denoted by rq. then we define the mean reciprocal rank to be this is a very simple performance measure. mean average precision in the case of binary relevance labels we can define the precision at k of some ordering as follows pk num. relevant documents in the top k positions of k we then define the average precision as follows k pk ik num. relevant documents ap where ik is iff document k is relevant. for example y then the ap is precision as the ap averaged over all queries. if we have the relevancy labels finally we define the mean average normalized discounted cumulative gain suppose the relevance labels have multiple levels. we can define the discounted cumulative gain of the first k items in an ordering as follows dcgkr ri i where ri is the relevance of item i and the term is used to discount items later in the list. table gives a simple numerical example. an alternative definition that places stronger emphasis on retrieving relevant documents uses dcgkr i the trouble with dcg is that it varies in magnitude just because the length of a returned list may vary. it is therefore common to normalize this measure by the ideal dcg which is chapter generalized linear models and the exponential family i ri i i ri na table illustration of how to compute ndcg from httpen.wikipedia.orgwikidiscounted the value ri is the relevance score of the item in position i. from this we see that the maximum dcg is obtained using the ordering with scores hence the ideal dcg is and so the normalized dcg is the dcg obtained by using the optimal ordering idcgkr argmax dcgkr. this can be easily computed by sorting and then computing dcgk. finally we define the normalized discounted cumulative gain or ndcg as dcgidcg. table gives a simple numerical example. the ndcg can be averaged over queries to give a measure of performance. rank correlation. we can measure the correlation between the ranked list and the relevance judegment using a variety of methods. one approach known as the kendall s statistics is defined in terms of the weighted pairwise inconsistency between the two lists uv wuv sgn u vsgn u v uv wuv a variety of other measures are commonly used. these loss functions can be used in different ways. in the bayesian approach we first fit the model using posterior inference this depends on the likelihood and prior but not the loss. we then choose our actions at test time to minimize the expected future loss. one way to do this is to sample parameters from the posterior s p and then evaluate say the precisionk for different thresholds averaging over s. see et al. for an example of such an approach. in the frequentist approach we try to minimize the empirical loss on the training set. the problem is that these loss functions are not differentiable functions of the model parameters. we can either use gradient-free optimization methods or we can minimize a surrogate loss function instead. cross entropy loss negative log likelihood is an example of a widely used surrogate loss function. another loss known as weighted approximate-rank pairwise or warp loss proposed in et al. and extended in et al. provides a better approximation to the precisionk loss. warp is defined as follows warpf y lrankf y rankf y if f y lk j with learning to rank here f f is the vector of scores for each possible output label or in ir terms for each possible document corresponding to input query x. the expression rankf y measures the rank of the true label y assigned by this scoring function. finally l transforms the integer rank into a real-valued penalty. using and would optimize the proportion of top-ranked correct labels. setting to be non-zero values would optimize the top k in the ranked list which will induce good performance as measured by map or precisionk. as it stands warp loss is still hard to optimize but it can be further approximated by monte carlo sampling and then optimized by gradient descent as described in et al. exercises exercise conjugate prior for univariate gaussian in exponential family form derive the conjugate prior for and for a univariate gaussian using the exponential family by analogy to section by suitable reparameterization show that the prior has the form p n and thus only has free parameters. exercise the mvn is in the exponential family show that we can write the mvn in exponential family form. hint use the information form defined in section directed graphical models nets introduction i basically know of two principles for treating complicated systems in simple ways the first is the principle of modularity and the second is the principle of abstraction. i am an apologist for computational probability in machine learning because i believe that probability theory implements these two principles in deep and intriguing ways namely through factorization and through averaging. exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. michael jordan in suppose we observe multiple correlated variables such as words in a document pixels in an image or genes in a microarray. how can we compactly represent the joint distribution px how can we use this distribution to infer one set of variables given another in a reasonable amount of computation time? and how can we learn the parameters of this distribution with a reasonable amount of data? these questions are at the core of probabilistic modeling inference and learning and form the topic of this chapter. chain rule by the chain rule of probability we can always represent a joint distribution as follows using any ordering of the variables pxv where v is the number of variables the matlab-like notation denotes the set v and where we have dropped the conditioning on the fixed parameters for brevity. the problem with this expression is that it becomes more and more complicated to represent the conditional distributions as t gets large. for example suppose all the variables have k states. we can represent as a table of ok numbers representing a discrete distribution are actually only k free parameters due to the sum-to-one constraint but we write ok for simplicity. similarly we can represent as a table of ok numbers by writing i tij we j tij for all rows i say that t is a stochastic matrix since it satisfies the constraint and tij for all entries. similarly we can represent as a table with chapter directed graphical models nets ok numbers. these are called conditional probability tables or cpts. we see that there are ok v parameters in the model. we would need an awful lot of data to learn so many parameters. one solution is to replace each cpt with a more parsimonius conditional probability distribution or cpd such as multinomial logistic regression i.e. pxt the total number of parameters is now only ok making this a compact density model frey this is adequate if all we want to do is evaluate the probability of a fully observed vector for example we can use this model to define a class-conditional density pxy c thus making a generative classifier and bengio however this model is not useful for other kinds of prediction tasks since each variable depends on all the previous variables. so we need another approach. conditional independence the key to efficiently representing large joint distributions is to make some assumptions about conditional independence recall from section that x and y are conditionally independent given z denoted x y if and only if the conditional joint can be written as a product of conditional marginals i.e. x y px y pxzpy let us see why this might help. suppose we assume that or in words the future is independent of the past given the present this is called the order markov assumption. using this assumption plus the chain rule we can write the joint distribution as follows pxtxt this is called a markov chain. they can be characterized by an initial distribution over states i plus a state transition matrix pxt jxt i. see section for more information. graphical models although the first-order markov assumption is useful for defining distributions on sequences how can we define distributions on images or videos or in general arbitrary collections of variables as genes belonging to some biological pathway? this is where graphical models come in. a graphical model is a way to represent a joint distribution by making ci assumptions. in particular the nodes in the graph represent random variables and the of edges represent ci assumptions. better name for these models would in fact be independence diagrams but the term graphical models is now entrenched. there are several kinds of graphical model depending on whether the graph is directed undirected or some combination of directed and undirected. in this chapter we just study directed graphs. we consider undirected graphs in chapter introduction figure are the leaves. a simple dag on nodes numbered in topological order. node is the root nodes and a simple undirected graph with the following maximal cliques graph terminology before we continue we must define a few basic terms most of which are very intuitive. a graph g consists of a set of nodes or vertices v v and a set of edges e t s t v. we can represent the graph by its adjacency matrix in which we write gs t to denote t e that is if s t is an edge in the graph. if gs t iff gt s we say the graph is undirected otherwise it is directed. we usually assume gs s which means there are no self loops. here are some other terms we will commonly use parent for a directed graph the parents of a node is the set of all nodes that feed into it pas gt s chs gs t pas. child for a directed graph the children of a node is the set of all nodes that feed out of it family for a directed graph the family of a node is the node and its parents fams root for a directed graph a root is a node with no parents. leaf for a directed graph a leaf is a node with no children. ancestors for a directed graph the ancestors are the parents grand-parents etc of a node. that is the ancestors of t is the set of nodes that connect to t via a trail anct s t. descendants for a directed graph the descendants are the children grand-children etc of a node. that is the descendants of s is the set of nodes that can be reached via trails from s descs s t. neighbors for any graph we define the neighbors of a node as the set of all immediately connected nodes nbrs gs t gt s for an undirected graph we chapter directed graphical models nets write s t to indicate that s and t are neighbors t e is an edge in the graph. degree the degree of a node is the number of neighbors. for directed graphs we speak of the in-degree and out-degree which count the number of parents and children. cycle or loop for any graph we define a cycle or loop to be a series of nodes such that we can get back to where we started by following edges sn n if the graph is directed we may speak of a directed cycle. for example in figure there are no directed cycles but is an undirected cycle. dag a directed acyclic graph or dag is a directed graph with no directed cycles. see figure for an example. topological ordering for a dag a topological ordering or total ordering is a numbering of the nodes such that parents have lower numbers than their children. for example in figure we can use etc. path or trail a path or trail s t is a series of directed edges leading from s to t. tree an undirected tree is an undirectecd graph with no cycles. a directed tree is a dag in which there are no directed cycles. if we allow a node to have multiple parents we call it a polytree otherwise we call it a moral directed tree. forest a forest is a set of trees. subgraph a subgraph ga is the graph created by using the nodes in a and their corresponding edges ga clique for an undirected graph a clique is a set of nodes that are all neighbors of each other. a maximal clique is a clique which cannot be made any larger without losing the clique property. for example in figure is a clique but it is not maximal since in fact the maximal cliques are as we can add and still maintain the clique property. follows directed graphical models a directed graphical model or dgm is a gm whose graph is a dag. these are more commonly known as bayesian networks. however there is nothing inherently bayesian about bayesian networks they are just a way of defining probability distributions. these models are also called belief networks. the term belief here refers to subjective probability. once again there is nothing inherently subjective about the kinds of probability distributions represented by dgms. finally these models are sometimes called causal networks because the directed arrows are sometimes interpreted as representing causal relations. however there is nothing inherently causal about dgms. section for a discussion of causal dgms. for these reasons we use the more neutral less glamorous term dgm. the key property of dags is that the nodes can be ordered such that parents come before children. this is called a topological ordering and it can be constructed from any dag. given such an order we define the ordered markov property to be the assumption that a node only depends on its immediate parents not on all predecessors in the ordering i.e. xs xpredspasxpas where pas are the parents of node s and preds are the predecessors of node s in the ordering. this is a natural generalization of the first-order markov property to from chains to general dags. examples y y figure a naive bayes classifier represented as a dgm. we assume there are d features for simplicity. shaded nodes are observed unshaded nodes are hidden. tree-augmented naive bayes classifier for d features. in general the tree topology can change depending on the value of y. for example the dag in figure encodes the following joint distribution in general we have pxtxpat where each term pxtxpat is a cpd. we have written the distribution as pxg to emphasize that this equation only holds if the ci assumptions encoded in dag g are correct. however if each node has of parents and we will usual drop this explicit conditioning for brevity. k states the number of parameters in the model is ov k f which is much less than the ok v needed by a model which makes no ci assumptions. examples in this section we show a wide variety of commonly used probabilistic models can be conveniently represented as dgms. naive bayes classifiers in section we introduced the naive bayes classifier. this assumes the features are conditionally independent given the class label. this assumption is illustrated in figure this allows us to write the joint distirbution as follows py x py pxjy the naive bayes assumption is rather naive since it assumes the features are conditionally independent. one way to capture correlation between the features is to use a graphical model. in particular if the model is a tree the method is known as a tree-augmented naive bayes chapter directed graphical models nets figure a first and second order markov chain. zt xt figure a first-order hmm. classifier or tan model et al. this is illustrated in figure the reason to use a tree as opposed to a generic graph is two-fold. first it is easy to find the optimal tree structure using the chow-liu algorithm as explained in section second it is easy to handle missing features in a tree-structured model as we explain in section markov and hidden markov models figure illustrates a first-order markov chain as a dag. of course the assumption that the immediate past xt captures everything we need to know about the entire history is a bit strong. we can relax it a little by adding a dependence from xt to xt as well this is called a second order markov chain and is illustrated in figure the corresponding joint has the following form pxtxt xt we can create higher-order markov models in a similar way. see section for a more detailed discussion of markov models. unfortunately even the second-order markov assumption may be inadequate if there are longrange correlations amongst the observations. we can t keep building ever higher order models since the number of parameters will blow up. an alternative approach is to assume that there is an underlying hidden process that can be modeled by a first-order markov chain but that the data is a noisy observation of this process. the result is known as a hidden markov model or hmm and is illustrated in figure here zt is known as a hidden variable at time t and xt is the observed variable. put time in quotation marks since these models can be applied to any kind of sequence data such as genomics or language where t represents location rather than time. the cpd pztzt is the transition model and the cpd pxtzt is the observation model. examples p p table noisy-or cpd for parents augmented with leak node. we have omitted the t subscript for brevity. the hidden variables often represent quantities of interest such as the identity of the word that someone is currently speaking. the observed variables are what we measure such as the acoustic waveform. what we would like to do is estimate the hidden state given the data i.e. to compute this is called state estimation and is just another form of probabilistic inference. see chapter for further details on hmms. medical diagnosis consider modeling the relationship between various variables that are measured in an intensive care unit such as the breathing rate of a patient their blood pressure etc. the alarm network in figure is one way to represent these et al. this model has variables and parameters. since this model was created by hand by a process called knowledge engineering it is known as a probabilistic expert system. in section we discuss how to learn the parameters of dgms from data assuming the graph structure is known and in chapter we discuss how to learn the graph structure itself. a different kind of medical diagnosis network known as the quick medical reference or qmr network et al. is shown in figure this was designed to model infectious diseases. the qmr model is a bipartite graph structure with diseases at the top and symptoms or findings at the bottom. all nodes are binary. we can write the distribution as follows pv h phs pvthpat s t where hs represent the hidden nodes and vt represent the visible nodes the cpd for the root nodes are just bernoulli distributions representing the prior probability of that disease. representing the cpds for the leaves using cpts would require too many parameters because the fan-in of parents of many leaf nodes is very high. a natural alternative is to use logistic regression to model the cpd pvt sigmwt t hpat. dgm in which the cpds are logistic regression distributions is known as a sigmoid belief net however since the parameters of this model were created by hand an alternative cpd known as the noisy-or model was used. the noisy-or model assumes that if a parent is on then the child will usually also be on it is an or-gate but occasionally the links from parents to child may fail independently at random. in this case even if the parent is on the child may be off. to model this more precisely let st qst be the probability that the s t link fails so qst st pvt chapter directed graphical models nets minvolset disconnect ventmach pulm embolus intubation venttube kinked tube pap shunt press ventlung hypo volemia anaphy laxis stroke volume lvfailure tpr co minvol ventalv pvsat insuff anesth catechol history cvp lved volume pcwp bp errlow output hrbp hr errcauter hrsat hrekg figure network. the alarm network. figure generated by visualizealarmnetwork. the qmr examples gp gm px a a a a b b b o o o a b o a b o a b o px b px o px ab table cpt which encodes a mapping from genotype to phenotype this is a deterministic but many-to-one mapping. h s is the probability that s can activate t on its own causal power the only way for the child to be off is if all the links from all parents that are on fail independently at random. thus pvt st s pat obviously pvt pvt if we observe that vt but all its parents are off then this contradicts the model. such a data case would get probability zero under the model which is problematic because it is possible that someone exhibits a symptom but does not have any of the specified diseases. to handle this we add a dummy leak node which is always on this represents all other causes the parameter represents the probability that the background leak can cause the effect on its own. the modified cpd becomes pvt st see table for a numerical example. s pat hs if we define wst log st we can rewrite the cpd as pvt exp hswst s we see that this is similar to a logistic regression model. bipartite models with noisy-or cpds are called models. it is relatively easy to set the st parameters by hand based on domain expertise. however it is also possible to learn them from data e.g meek and heckerman noisy-or cpds have also proved useful in modeling human causal learning and tenenbaum as well as general binary classification settings and zheng genetic linkage analysis another important historically very early application of dgms is to the problem of genetic linkage analysis. we start with a pedigree graph which is a dag that representing the relationship between parents and children as shown in figure we then convert this to a dgm as we explain below. finally we perform probabilistic inference in the resulting model. chapter directed graphical models nets family tree circles are females squares are males. figure left individuals with the disease of interest are highlighted. right dgm for two loci. blue nodes xij is the observed phenotype for individual i at locus j. all other nodes are hidden. orange nodes gpm is the paternal maternal allele. small red nodes zpm are the paternal maternal selection switching variables. these are linked across loci ij zp ij zm zm the founder nodes do not have any parents and hence do no need switching variables. based on figure from et al. ij ijl and zp examples in more detail for each person animal i and location or locus j along the genome we the observed marker xij can be a property such as blood type create three nodes or just a fragment of dna that can be measured and two hidden alleles gm ij one inherited from i s mother allele and the other from i s father allele. together the ordered pair gij ij constitutes i s hidden genotype at locus j. ij xij arcs representing the fact that genotypes obviously we must add gm cause phenotypes manifestations of genotypes. the cpd pxijgm ij is called the penetrance model. as a very simple example suppose xij b o ab represents ij b o is their genotype. we can repreperson i s observed bloodtype and gm sent the penetrance model using the deterministic cpd shown in table for example a dominates o so if a person has genotype ao or oa their phenotype will be a. ij gp ij xij and gp ij and gp ij gp ij gp in addition we add arcs from i s mother and father into gij reflecting the mendelian inheritance of genetic material from one s parents. more precisely let mi k be i s mother. then gm kj that is i s maternal allele is a copy of one of its mother s two alleles. let z m ij be a hidden variable than specifies the choice. we can model this using the following cpd known as the inheritance model ij could either be equal to gm kj or gp igm igm ij gm kj ij gp kj if z m if z m ij m ij p pgm ijgm kj gp kj z m ij ijgm kj z p kj gp we can define pgp are said to specify the phase of the genotype. the values of gp the haplotype of person i at locus ij similarly where k pi is i s father. the values of the zij ij constitute ij and z m ij gm ij z p next we need to specify the prior for the root nodes pgm ij. this is called the founder model and represents the overall prevalence of difference kinds of alleles in the population. we usually assume independence between the loci for these founder alleles. ij and pgp finally we need to specify priors for the switch variables that control the inheritance process. these variables are spatially correlated since adjacent sites on the genome are typically inherited together events are rare. we can model this by imposing a two-state markov chain on the z s where the probability of switching state at locus j is given by j e where dj is the distance between loci j and j this is called the recombination model. the resulting dgm is shown in figure it is a series of replicated pedigree dags augmented with switching z variables which are linked using markov chains. is a related model known as phylogenetic hmm and haussler which is used to model evolution amongst phylogenies. as a simplified example of how this model can be used suppose we only have one locus corresponding to blood type. for brevity we will drop the j index. suppose we observe xi a. then there are possible genotypes gi is a o or a. there is ambiguity because the genotype to phenotype mapping is many-to-one. we want to reverse this mapping. this is known as an inverse problem. fortunately we can use the blood types of relatives to help disambiguate the evidence. information will flow from the other s up to their s then across to i s gi via the pedigree dag. thus we can combine our local evidence pxigi sometimes the observed marker is equal to the unphased genotype which is the unordered set ij however the phased or hidden genotype is not directly measurable. ij gm chapter directed graphical models nets with an informative prior pgix i conditioned on the other data to get a less entropic local posterior pgix pxigipgix i. in practice the model is used to try to determine where along the genome a given diseasecausing gene is assumed to lie this is the genetic linkage analysis task. the method works as follows. first suppose all the parameters of the model including the distance between all the marker loci are known. the only unknown is the location of the disease-causing gene. if there are l marker loci we construct l models in model we postulate that the disease gene comes after marker for l we can estimate the markov switching parameter and hence the distance between the disease gene and its nearest known locus. we measure the quality of that model using its likelihood pd we then can then pick the model with highest likelihood is equivalent to the map model under a uniform prior. note however that computing the likelihood requires marginalizing out all the hidden z and g variables. see and geiger and the references therein for some exact methods for this task these are based on the variable elimination algorithm which we discuss in section unfortunately for reasons we explain in section exact methods can be computationally intractable if the number of individuals andor loci is large. see et al. for an approximate method for computing the likelihood this is based on a form of variational inference which we will discuss in section directed gaussian graphical models consider a dgm where all the variables are real-valued and all the cpds have the following form pxtxpat n t wt t xpat t this is called a linear gaussian cpd. as we show below multiplying all these cpds together results in a large joint gaussian distribution of the form px this is called a directed ggm or a gaussian bayes net. we now explain how to derive and from the cpd parameters following and kenley app. b. for convenience we will rewrite the cpds in the following form s pat xt t wtsxs s tzt where zt n t is the conditional standard deviation of xt given its parents wts is the strength of the s t edge and t is the local it is easy to see that the global mean is just the concatenation of the local means d. we now derive the global covariance let s diag be a diagonal matrix containing the standard deviations. we can rewrite equation in matrix-vector form as follows wx if we do not subtract off the parent s mean if we use xt t is much messier as can be seen by looking at s pat wtsxs tzt the derivation of inference now let e be a vector of noise terms e sz we can rearrange this to get e wx since w is lower triangular wts if t s in the topological ordering we have that i w is lower triangular with on the diagonal. hence xd d ed wdd since i w is always invertible we can write x w ue usz where we defined u w decomposition of as we now show cov cov cov us cov sut inference thus the regression weights correspond to a cholesky we have seen that graphical models provide a compact way to define joint probability distributions. given such a joint distribution what can we do with it? the main use for such a joint distribution is to perform probabilistic inference. this refers to the task of estimating unknown quantities from known quantities. for example in section we introduced hmms and said that one of the goals is to estimate the hidden states words from the observations speech signal. and in section we discussed genetic linkage analysis and said that one of the goals is to estimate the likelihood of the data under various dags corresponding to different hypotheses about the location of the disease-causing gene. in general we can pose the inference problem as follows. suppose we have a set of correlated random variables with joint distribution this section we are assuming the parameters of the model are known. we discuss how to learn the parameters in section let us partition this vector into the visible variables xv which are observed and the hidden variables xh which are unobserved. inference refers to computing the posterior distribution of the unknowns given the knowns pxh xv pxh xv pxv pxhxv h xv h essentially we are conditioning on the data by clamping the visible variables to their observed values xv and then normalizing to go from pxh xv to pxhxv. the normalization constant pxv is the likelihood of the data also called the probability of the evidence. chapter directed graphical models nets sometimes only some of the hidden variables are of interest to us. so let us partition the hidden variables into query variables xq whose value we wish to know and the remaining nuisance variables xn which we are not interested in. we can compute what we are interested in by marginalizing out the nuisance variables pxqxv pxq xnxv xn in section we saw how to perform all these operations for a multivariate gaussian in ov time where v is the number of variables. what if we have discrete random variables with say k states each? if the joint distribution is represented as a multi-dimensional table we can always perform these operations exactly but this will take ok v time. in chapter we explain how to exploit the factorization encoded by the gm to perform these operations in ov k time where w is a quantity known as the treewidth of the graph. this measures if the graph is a tree a chain we have w so for these how tree-like the graph is. models inference takes time linear in the number of nodes. unfortunately for more general graphs exact inference can take time exponential in the number of nodes as we explain in section we will therefore examine various approximate inference schemes later in the book. learning in the graphical models literature it is common to distinguish between inference and learning. inference means computing of pxhxv where v are the visible nodes h are the hidden nodes and are the parameters of the model assumed to be known. learning usually means computing a map estimate of the parameters given data log pxiv log p argmax where xiv are the visible variables in case i. if we have a uniform prior p this reduces to the mle as usual. if we adopt a bayesian view the parameters are unknown variables and should also be inferred. thus to a bayesian there is no distinction between inference and learning. in fact we can just add the parameters as nodes to the graph condition on d and then infer the values of all the nodes. discuss this in more detail below. in this view the main difference between hidden variables and parameters is that the number of hidden variables grows with the amount of training data there is usually a set of hidden variables for each observed data case whereas the number of parameters in usually fixed least in a parametric model. this means that we must integrate out the hidden variables to avoid overfitting but we may be able to get away with point estimation techniques for parameters which are fewer in number. plate notation when inferring parameters from data we often assume the data is iid. we can represent this assumption explicitly using a graphical model as shown in figure this illustrates the learning xn xi n left data points xi are conditionally independent given right plate notation. this figure represents the same model as the one on the left except the repeated xi nodes are inside a box known as a plate the number in the lower right hand corner n specifies the number of repetitions of the xi node. assumption that each data case was generated independently but from the same distribution. notice that the data cases are only independent conditional on the parameters marginally the data cases are dependent. nevertheless we can see that in this example the order in which the data cases arrive makes no difference to our beliefs about since all orderings will have the same sufficient statistics. hence we say the data is exchangeable. to avoid visual clutter it is common to use a form of syntactic sugar called plates we simply draw a little box around the repeated variables with the convention that nodes within the box will get repeated when the model is unrolled. we often write the number of copies or repetitions in the bottom right corner of the box. see figure for a simple example. the corresponding joint distribution has the form p p pxi this dgm represents the ci assumptions behind the models we considered in chapter a slightly more complex example is shown in figure on the left we show a naive bayes classifier that has been unrolled for d features but uses a plate to represent repetition over cases i the version on the right shows the same model using nested plate notation. when a variable is inside two plates it will have two sub-indices. for example we write jc to represent the parameter for feature j in class-conditional density c. note that plates can be nested or crossing. notational devices for modeling more complex parameter tying patterns can be devised et al. but these are not widely used. what is not clear from the figure is that jc is used to generate xij iff yi c otherwise it is ignored. this is an example of context specific independence since the ci relationship xij jc only holds if yi c. chapter directed graphical models nets yi xid n cd c yi xij n jc c d figure naive bayes classifier as a dgm. with single plates. with nested plates. learning from complete data if all the variables are fully observed in each case so there is no missing data and there are no hidden variables we say the data is complete. for a dgm with complete data the likelihood is given by pd pxi pxitxipat t pdt t where dt is the data associated with node t and its parents i.e. the t th family. this is a product of terms one per cpd. we say that the likelihood decomposes according to the graph structure. now suppose that the prior factorizes as well p p t then clearly the posterior also factorizes p pd pdt tp t this means we can compute the posterior of each cpd independently. in other words factored prior plus factored likelihood implies factored posterior let us consider an example where all cpds are tabular thus extending the earlier results of secion where discussed bayesian naive bayes. we have a separate row a separate multinoulli distribution for each conditioning case i.e. for each combination of parent values as in table formally we can write the t th cpt as xtxpat c cat tc where tck pxt kxpat c for k kt c ct and t t here kt is the number learning of states for node t ct number of nodes. obviously s pat ks is the number of parent combinations and t is the k tck for each row of each cpt. let us put a separate dirichlet prior on each row of each cpt i.e. tc dir tc. then we can compute the posterior by simply adding the pseudo counts to the empirical counts to get tcd dirntc tc wheren tck is the number of times that node t is in state k while its parents are in state c ntck ixit k xipat c from equation the mean of this distribution is given by the following tck ntck tck for example consider the dgm in figure suppose the training data consists of the following cases below we list all the sufficient statistics ntck and the posterior mean parameters ick under a dirichlet prior with ick to add-one smoothing for the t node it is easy to show that the mle has the same form as equation except without the tck terms i.e. tck of course the mle suffers from the zero-count problem discussed in section so it is important to use a prior to regularize the estimation problem. learning with missing andor latent variables if we have missing data andor hidden variables the likelihood no longer factorizes and indeed it is no longer convex as we explain in detail in section this means we will usually can only compute a locally optimal ml or map estimate. bayesian inference of the parameters is even harder. we discuss suitable approximate inference techniques in later chapters. chapter directed graphical models nets conditional independence properties of dgms we say that g is an i-map map for p or that p is markov wrt g at the heart of any graphical model is a set of conditional indepence assumptions. we write xa g xbxc if a is independent of b given c in the graph g using the semantics to be defined below. let ig be the set of all such ci statements encoded by the graph. iff ig ip where ip is the set of all ci statements that hold for distribution p. in other words the graph is an i-map if it does not make any assertions of ci that are not true of the distribution. this allows us to use the graph as a safe proxy for p when reasoning about p s ci properties. this is helpful for designing algorithms that work for large classes of distributions regardless of their specific numerical parameters note that the fully connected graph is an i-map of all distributions since it makes no ci assertions at all it is not missing any edges. we therefore say g is a minimal i-map of p if g is an i-map of p and if there is no g which is an i-map of p. it remains to specify how to determine if xa g xbxc. deriving these independencies for undirected graphs is easy section but the dag situation is somewhat complicated because of the need to respect the orientation of the directed edges. we give the details below. d-separation and the bayes ball algorithm markov properties first we introduce some definitions. we say an undirected path p is d-separated by a set of nodes e the evidence iff at least one of the following conditions hold p contains a chain s m t or s m t wherem e p contains a tent or fork s t wherem e p contains a collider or v-structure s t where m is not in e and nor is any descendant of m. next we say that a set of nodes a is d-separated from a different set of nodes b given a third observed set e iff each undirected path from every node a a to every node b b is d-separated by e. finally we define the ci properties of a dag as follows xa g xbxe a is d-separated from b given e the bayes ball algorithm is a simple way to see if a is d-separated from b given e based on the above definition. the idea is this. we shade all nodes in e indicating that they are observed. we then place balls at each node in a let them bounce around according to some rules and then ask if any of the balls reach any of the nodes in b. the three main rules are shown in figure notice that balls can travel opposite to edge directions. we see that a ball can pass through a chain but not if it is shaded in the middle. similarly a ball can pass through a fork but not if it is shaded in the middle. however a ball cannot pass through a v-structure unless it is shaded in the middle. we can justify the rules of bayes ball as follows. first consider a chain structure x y z which encodes px y z pxpyxpzy conditional independence properties of dgms y x z z x y z x z y x y x y x z z y figure bayes ball rules. a shaded node is one we condition on. if there is an arrow hitting a bar it means the ball cannot pass through otherwise the ball can pass through. based on when we condition on y arex and z independent? we have px zy pxpyxpzy px ypzy and therefore x zy. so observing the middle node of chain breaks it in two in a markov chain. py py pxypzy now consider the tent structure x y z. the joint is px y z pypxypzy chapter directed graphical models nets x y x y x z y figure bayes ball boundary conditions. example of why we need boundary conditions. is an observed child of y rendering y effectively observed so the ball bounces back up on its way from x to z. when we condition on y arex and z independent? we have px zy px y z pypxypzy pxypzy and therefore x zy. so observing a root node separates its children in a naive bayes classifier see section py py finally consider a v-structure x y z. the joint is px y z pxpzpyx z when we condition on y arex and z independent? we have px zy pxpzpyx z py px z pxpz so x zy. however in the unconditional distribution we have so we see that x and z are marginally independent. so we see that conditioning on a common child at the bottom of a v-structure makes its parents become dependent. this important effect is called explaining away inter-causal reasoning or berkson s paradox. as an example of explaining away suppose we toss two coins representing the binary numbers and and we observe the sum of their values. a priori the coins are independent but once we observe their sum they become coupled if the sum is and the first coin is then we know the second coin is finally bayes ball also needs the boundary conditions shown in figure to understand where these rules come from consider figure suppose y is a noise-free copy of y then if we observe y we effectively observe y as well so the parents x and z have to compete to explain this. so if we send a ball down x y y it should bounce back up along y y z. however if y and all its children are hidden the ball does not bounce back. conditional independence properties of dgms figure a dgm. for example in figure we see that since the path is blocked by is observed the path is blocked by is hidden and the path is blocked by is hidden. however we also see that since now the path is no longer blocked by is observed. exercise gives you some more practice in determining ci relationships for dgms. other markov properties of dgms from the d-separation criterion one can conclude that t ndt patpat where the non-descendants of a node ndt are all the nodes except for its descendants ndt t desct. equation is called the directed local markov property. for example in figure we have and a special case of this property is when we only look at predecessors of a node according to some topological ordering. we have t predt patpat which follows since predt ndt. this is called the ordered markov property which justifies equation for example in figure if we use the ordering we find and we have now described three markov properties for dags the directed global markov property g in equation the ordered markov property o in equation and the directed local it is obvious that g l o. what is less markov property l in equation obvious but nevertheless true is that o l g e.g. and friedman for the proof. hence all these properties are equivalent. furthermore any distribution p that is markov wrt g can be factorized as in equation this is called the factorization property f. it is obvious that o f but one can show that the converse also holds e.g. and friedman for the proof. markov blanket and full conditionals the set of nodes that renders a node t conditionally independent of all the other nodes in the graph is called t s markov blanket we will denote this by mbt. one can show that the markov blanket of a node in a dgm is equal to the parents the children and the co-parents chapter directed graphical models nets i.e. other nodes who are also parents of its children mbt cht pat copat for example in figure we have where is a co-parent of because they share a common child namely to see why the co-parents are in the markov blanket note that when we derive pxtx t pxt x tpx t all the terms that do not involve xt will cancel out between numerator and denominator so we are left with a product of cpds which contain xt in their scope. hence pxtx t pxtxpat pxsxpas s cht for example in figure we have the resulting expression is called t s full conditional and will prove to be important when we study gibbs sampling influence diagrams we can represent multi-stage decision problems by using a graphical notation known as a decision diagram or an influence diagram and matheson kjaerulff and madsen this extends directed graphical models by adding decision nodes called action nodes represented by rectangles and utility nodes called value nodes represented by diamonds. the original random variables are called chance nodes and are represented by ovals as usual. figure gives a simple example illustrating the famous oil wild-catter in this problem you have to decide whether to drill an oil well or not. you have two possible actions d means drill d means don t drill. you assume there are states of nature o means the well is dry o means it is wet some oil and o means it is soaking a lot of oil. suppose your prior beliefs are po finally you must specify the utility function u o. since the states and actions are discrete we can represent it as a table to a cpt in a dgm. suppose we use the following numbers in dollars o o o d d we see that if you don t drill you incur no costs but also make no money. if you drill a dry well you lose if you drill a wet well you gain and if you drill a soaking well you gain your prior expected utility if you drill is given by eu pou o this example is originally from our presentation is based on some notes by daphne koller. influence diagrams oil oil sound drill drill utility test utility oil sound cost drill utility figure influence diagram for basic oil wild catter problem. an extension in which we have an information arc from the sound chance node to the drill decision node. an extension in which we get to decide whether to perform the test or not. your expected utility if you don t drill is so your maximum expected utility is m eu maxeu eu and therefore the optimal action is to drill d arg maxeu eu now let us consider a slight extension to the model. suppose you perform a sounding to estimate the state of the well. the sounding observation can be in one of states s is a diffuse reflection pattern suggesting no oil s is an open reflection pattern suggesting some oil and s is a closed reflection pattern indicating lots of oil. since s is caused by o we add an o s arc to our model. in addition we assume that the outcome of the sounding test will be available before we decide whether to drill or not hence we add an information arc from s to d. this is illustrated in figure pso let us model the reliability of our sensor using the following conditional distribution for chapter directed graphical models nets s s s o o o suppose we do the sounding test and we observe s the posterior over the oil state is pos now your posterior expected utility of performing action d is eu pos d if d this gives s eu however if d then eu since not drilling incurs no cost. so if we observe s we are better off not drilling which makes sense. now suppose we do the sounding test and we observe s by similar reasoning one can show that eu which is higher than eu similarly if we observe s we have eu which is much higher than eu hence the optimal policy d if s choose d and get and get and if s choose d and get if s choose d is as follows you can compute your expected profit or maximum expected utility as follows m eu pseu this is the expected utility given possible outcomes of the sounding test assuming you act optimally given the outcome. the prior marginal on the outcome of the test is ps popso o hence your maximum expected utility is m eu now suppose you can choose whether to do the test or not. this can be modelled as shown in figure where we add a new test node t if t we do the test and s can enter of states determined by o exactly as above. if t we don t do the test and s enters a special unknown state. there is also some cost associated with performing the test. is it worth doing the test? this depends on how much our meu changes if we know the if you don t do the test we have m eu outcome of the test the state of s. if you do the test you have m eu from equation so the from equation improvement in utility if you do the test act optimally on its outcome is this is influence diagrams xt zt at at xt rt rt figure a pomdp shown as an influence diagram. zt are hidden world states. we implicitly make the no forgetting assumption which effectively means that at has arrows coming into it from all previous observations an mdp shown as an influence diagram. called the value of perfect information so we should do the test as long as it costs less than in terms of graphical models the vpi of a variable t can be determined by computing the meu for the base influence diagram i and then computing the meu for the same influence diagram where we add information arcs from t to the action nodes and then computing the difference. in other words vpi meui t d meui where d is the decision node and t is the variable we are measuring. it is possible to modify the variable elimination algorithm so that it computes the optimal policy given an influence diagram. these methods essentially work backwards from the final time-step computing the optimal decision at each step assuming all following actions are chosen optimally. see e.g. and nilsson kjaerulff and madsen for details. we could continue to extend the model in various ways. for example we could imagine a dynamical system in which we test observe outcomes perform actions move on to the next oil well and continue drilling polluting in this way. in fact many problems in robotics business medicine public policy etc. can be usefully formulated as influence diagrams unrolled over time lauritzen and nilsson kjaerulff and madsen a generic model of this form is shown in figure this is known as a partially observed markov decision process or pomdp pom-d-p this is basically a hidden markov model augmented with action and reward nodes. this can be used to model the perception-action cycle that all intelligent agents use e.g. et al. for details. a special case of a pomdp in which the states are fully observed is called a markov decision process or mdp shown in figure this is much easier to solve since we only have to compute a mapping from observed states to actions. this can be solved using dynamic programming e.g. and barto for details. in the pomdp case the information arc from xt to at is not sufficient to uniquely determine chapter directed graphical models nets g d b h f i e c a b a d e c f h i g j figure some dgms. the best action since the state is not fully observed. instead we need to choose actions based on our belief state since the belief updating process is deterministic section we can compute a belief state mdp. for details on to compute the policies for such models see e.g. et al. spaan and vlassis exercises exercise marginalizing a node in a dgm koller. consider the dag g in figure assume it is a minimal i-map for pa b c d e f x. now consider marginalizing out x. construct a new dag which is a minimal i-map for pa b c d e f specify justify which extra edges need to be added. exercise bayes ball jordan. here we compute some global independence statements from some directed graphical models. you can use the bayes ball algorithm the d-separation criterion or the method of converting to an undirected graph should give the same results. a. consider the dag in figure list all variables that are independent of a given evidence on b. b. consider the dag in figure list all variables that are independent of a given evidence on j. exercise markov blanket for a dgm prove that the full conditional for node i in a dgm is given by pyjp ayj pxix i pxip axi yj chxi where chxi are the children of xi and p ayj are the parents of yj. exercise hidden variables in dgms consider the dgms in figure which both define where we number empty nodes left to right top to bottom. the graph on the left defines the joint as h h influence diagrams figure weather bn. fishing bn. where we have marginalized over the hidden variable h. the graph on the right defines the joint as a. b. c. points assuming all nodes h are binary and all cpds are tabular prove that the model on the left has free parameters. points assuming all nodes are binary and all cpds are tabular prove that the model on the right has free parameters. points suppose we have a data set d x n for n n where we observe the xs but not h and we want to estimate the parameters of the cpds using maximum likelihood. for which model is this easier? explain your answer. exercise bayes nets for a rainy day nando de freitas.. in this question you must model a problem with binary variables g gray v vancouver r rain and s sad consider the directed graphical model describing the relationship between these variables shown in figure a. write down an expression for p in terms of b. write down an expression for p is this the same or different to p explain why. c. find maximum likelihood estimates of using the following data set where each row is a training case. may state your answers without proof. v g r s exercise fishing nets the following variables et al. consider the bayes net shown in figure here the nodes represent spring summer autumn sea bass medium dark thin chapter directed graphical models nets figure a qmr-style network with some hidden leaves. removing the barren nodes. the corresponding conditional probability tables are note that in the rows represent and the columns each row sums to one and represents the child of the cpd. thus sea bass salmon etc. answer the following queries. you may use matlab or do it by hand. in either case show your work. a. suppose the fish was caught on december the end of autumn and the beginning of winter and thus let instead of the above prior. is called soft evidence since we do not know the exact value of but we have a distribution over it. suppose the lightness has not been measured but it is known that the fish is thin. classify the fish as salmon or sea bass. b. suppose all we know is that the fish is thin and medium lightness. what season is it now most likely? use exercise removing leaves in networks a. consider the qmr network where only some of the symtpoms are observed. for example in figure and are hidden. show that we can safely remove all the hidden leaf nodes without affecting the posterior over the disease nodes i.e. prove that we can compute using the network in figure this is called barren node removal and can be applied to any dgm. b. now suppose we partition the leaves into three groups on off and unknown. clearly we can remove the unknown leaves since they are hidden and do not affect their parents. show that we can analytically remove the leaves that are in the off state by absorbing their effect into the prior of the parents. trick only works for noisy-or cpds. exercise handling negative findings in the qmr network consider the qmr network. let d be the hidden diseases f be the negative findings nodes that are be the positive findings nodes that are on. we can compute the posterior pdf in off and f pdpf then absorb the positive findings two steps first absorb the negative findings pdf pdf f pdf show that the first step can be done in odf time where is the number of dieases and is the number of negative findings. for simplicity you can ignore leak nodes. the reason for this is that there is no correlation induced amongst the parents when the finding is off since there is no explaining away. influence diagrams exercise moralization does not introduce new independence statements recall that the process of moralizing a dag means connecting together all unmarried parents that share a common child and then dropping all the arrows. let m be the moralization of dag g. show that cim cig where ci are the set of conditional independence statements implied by the model. mixture models and the em algorithm latent variable models in chapter we showed how graphical models can be used to define high-dimensional joint probability distributions. the basic idea is to model dependence between two variables by adding an edge between them in the graph. the graph represents conditional independence but you get the point. an alternative approach is to assume that the observed variables are correlated because they arise from a hidden common cause model with hidden variables are also known as latent variable models or lvms. as we will see in this chapter such models are harder to fit than models with no latent variables. however they can have significant advantages for two main reasons. first lvms often have fewer parameters than models that directly represent correlation in the visible space. this is illustrated in figure if all nodes h are binary and all cpds are tabular the model on the left has free parameters whereas the model on the right has free parameters. second the hidden variables in an lvm can serve as a bottleneck which computes a compressed representation of the data. this forms the basis of unsupervised learning as we will see. figure illustrates some generic lvm structures that can be used for this purpose. in general there are l latent variables zil and d visible variables xid where usually d l. if we have l there are many latent factors contributing to each if l we we only have a single latent observation so we have a many-to-many mapping. in this case zi is usually discrete and we have a one-to-many mapping. we can variable also have a many-to-one mapping representing different competing factors or causes for each observed variable such models form the basis of probabilistic matrix factorization discussed in section finally we can have a one-to-one mapping which can be represented as zi xi. by allowing zi andor xi to be vector-valued this representation can subsume all the others. depending on the form of the likelihood pxizi and the prior pzi we can generate a variety of different models as summarized in table mixture models the simplest form of lvm is when zi k representing a discrete latent state. we will use a discrete prior for this pzi cat for the likelihood we use pxizi k pkxi chapter mixture models and the em algorithm figure a dgm with and without hidden variables. the leaves represent medical symptoms. the roots represent primary causes such as smoking diet and exercise. the hidden variable can represent mediating factors such as heart disease which might not be directly visible. zi zil xid xid zil xi zi xi figure a latent variable model represented as a dgm. many-to-one. one-to-one. many-to-many. one-to-many. where pk is the k th base distribution for the observations this can be of any type. the overall model is known as a mixture model since we are mixing together the k base distributions as follows pxi kpkxi this is a convex combination of the pk s since we are taking a weighted sum where the mixing weights k satisfy k and k we give some examples below. mixture models pxizi mvn prod. discrete prod. gaussian prod. gaussian prod. discrete prod. discrete prod. noisy-or prod. bernoulli name mixture of gaussians mixture of multinomials factor analysis probabilistic pca probabilistic ica sparse coding pzi discrete discrete prod. gaussian prod. laplace prod. gaussian multinomial pca dirichlet prod. bernoulli prod. bernoulli latent dirichlet allocation qmr sigmoid belief net section table prod. discrete in the likelihood means a factored distribution of the form gaussian means a factored distribution of the form analysis ica stands for indepedendent components analysis summary of some popular directed latent variable models. here prod means product so j catxijzi and prod. j n pca stands for principal components figure a mixture of gaussians in we show the contours of constant probability for each component in the mixture. a surface plot of the overall density. based on figure of figure generated by mixgaussplotdemo. mixtures of gaussians the most widely used mixture model is the mixture of gaussians also called a gaussian mixture model or gmm. in this model each base distribution in the mixture is a multivariate gaussian with mean k and covariance matrix k. thus the model has the form pxi kn k k figure shows a mixture of gaussians in each mixture component is represented by a different set of eliptical contours. given a sufficiently large number of mixture components a gmm can be used to approximate any density defined on r d. chapter mixture models and the em algorithm mixture of multinoullis we can use mixture models to define density models on many kinds of data. for example suppose our data consist of d-dimensional bit vectors. in this case an appropriate classconditional density is a product of bernoullis pxizi k berxij jk jk xij xij where jk is the probability that bit j turns on in cluster k. the latent variables do not have to any meaning we might simply introduce latent variables in order to make the model more powerful. for example one can show that the mean and covariance of the mixture distribution are given by k e cov k k k k k t k e e t k where k diag jk. so although the component distributions are factorized the joint distribution is not. thus the mixture distribution can capture correlations between variables unlike a single product-of-bernoullis model. using mixture models for clustering there are two main applications of mixture models. the first is to use them as a black-box density model pxi. this can be useful for a variety of tasks such as data compression outlier detection and creating generative classifiers where we model each class-conditional density pxy c by a mixture distribution section the second and more common application of mixture models is to use them for clustering. we discuss this topic in detail in chapter but the basic idea is simple. we first fit the mixture model and then compute pzi kxi which represents the posterior probability that point i belongs to cluster k. this is known as the responsibility of cluster k for point i and can be computed using bayes rule as follows rik pzi kxi pzi k k pzi this procedure is called soft clustering and is identical to the computations performed when using a generative classifier. the difference between the two models only arises at training time in the mixture case we never observe zi whereas with a generative classifier we do observe yi plays the role of zi. we can represent the amount of uncertainty in the cluster assignment by using maxk rik. assuming this is small it may be reasonable to compute a hard clustering using the map estimate given by z i arg max k rik arg max k log pxizi k log pzi k mixture models s e n e g yeast microarray data k means centroids time figure centers produced by k-means. figure generated by kmeansyeastdemo. some yeast gene expression data plotted as a time series. visualizing the cluster figure we fit a mixture of bernoullis to the binarized mnist digit data. we show the mle for the corresponding cluster means k. the numbers on top of each image represent the mixing weights k. no labels were used when training the model. figure generated by mixbermnistem. hard clustering using a gmm is illustrated in figure where we cluster some data representing the height and weight of people. the colors represent the hard assignments. note that the identity of the labels used is immaterial we are free to rename all the clusters without affecting the partitioning of the data this is called label switching. another example is shown in figure here the data vectors xi r represent the expression levels of different genes at different time points. we clustered them using a gmm. we see that there are several kinds of genes such as those whose expression level goes up monotonically over time response to a given stimulus those whose expression level goes down monotonically and those with more complex response patterns. we have clustered the series into k groups. section for details on how to choose k. for example we can represent each cluster by a prototype or centroid. this is shown in figure as an example of clustering binary data consider a binarized version of the mnist handwritten digit dataset figure where we ignore the class labels. we can fit a mixture of chapter mixture models and the em algorithm bernoullis to this using k and then visualize the resulting centroids k as shown in figure we see that the method correctly discovered some of the digit classes but overall the results aren t great it has created multiple clusters for some digits and no clusters for others. there are several possible reasons for these errors the model is very simple and does not capture the relevant visual characteristics of a digit. for example each pixel is treated independently and there is no notion of shape or a stroke. although we think there should be clusters some of the digits actually exhibit a fair degree of visual variety. for example there are two ways of writing s and without the cross bar. figure illustrates some of the range in writing styles. thus we need k clusters to adequately model this data. however if we set k to be large there is nothing in the model or algorithm preventing the extra clusters from being used to create multiple versions of the same digit and indeed this is what happens. we can use model selection to prevent too many clusters from being chosen but what looks visually appealing and what makes a good density estimator may be quite different. the likelihood function is not convex so we may be stuck in a local optimum as we explain in section this example is typical of mixture modeling and goes to show one must be very cautious a little bit of trying to interpret any clusters that are discovered by the method. supervision or using informative priors can help a lot. mixtures of experts section described how to use mixture models in the context of generative classifiers. we can also use them to create discriminative models for classification and regression. for example consider the data in figure it seems like a good model would be three different linear regression functions each applying to a different part of the input space. we can model this by allowing the mixing weights and the mixture densities to be input-dependent pyixi zi k k xi k pzixi catzisvt xi see figure for the dgm. this model is called a mixture of experts or moe and jacobs the idea is that each submodel is considered to be an expert in a certain region of input space. the function pzi kxi is called a gating function and decides which expert to use depending on the input values. for example figure shows how the three experts have carved up the input space figure shows the predictions of each expert individually this case the experts are just linear regression models and figure shows the overall prediction of the model obtained using pyixi pzi kxi zi k k we discuss how to fit this model in section mixture models expert predictions fixed mixing gating functions fixed mixing predicted mean and var fixed mixing figure some data fit with three separate regression lines. gating functions for three different experts the conditionally weighted average of the three expert predictions. figure generated by mixexpdemo. xi zi yi xi i i yi figure a mixture of experts. a hierarchical mixture of experts. chapter mixture models and the em algorithm forwards problem expert predictions prediction mean mode figure some data from a simple forwards model. some data from the inverse model fit the with a mixture of linear regressions. training points are color coded by their responsibilities. predictive mean cross and mode square. based on figures and of figure generated by mixexpdemoonetomany. it should be clear that we can plug in any model for the expert. for example we can use neural networks to represent both the gating functions and the experts. the result is known as a mixture density network. such models are slower to train but can be more flexible than mixtures of experts. see for details. it is also possible to make each expert be itself a mixture of experts. this gives rise to a model known as the hierarchical mixture of experts. see figure for the dgm and section for further details. application to inverse problems mixtures of experts are useful in solving inverse problems. these are problems where we have to invert a many-to-one mapping. a typical example is in robotics where the location of the end effector y is uniquely determined by the joint angles of the motors x. however for any given location y there are many settings of the joints x that can produce it. thus the inverse mapping x f is not unique. another example is kinematic tracking of people from video et al. where the mapping from image appearance to pose is not unique due to self occlusion etc. parameter estimation for mixture models z zn xn x z zi xi x n figure a lvm represented as a dgm. left model is unrolled for n examples. right same model using plate notation. a simpler example for illustration purposes is shown in figure we see that this defines a function y f since for every value x along the horizontal axis there is a unique response y. this is sometimes called the forwards model. now consider the problem of computing x f the corresponding inverse model is shown in figure this is obtained by simply interchanging the x and y axes. now we see that for some values along the horizontal axis there are multiple possible outputs so the inverse is not uniquely defined. for example if y then x could be or consequently the predictive distribution pxy is multimodal. we can fit a mixture of linear experts to this data. figure shows the prediction of each expert and figure shows plugin approximation to the posterior predictive mode and mean. note that the posterior mean does not yield good predictions. in fact any model which is trained to minimize mean squared error even if the model is a flexible nonlinear model such as neural network will work poorly on inverse problems such as this. however the posterior mode where the mode is input dependent provides a reasonable approximation. parameter estimation for mixture models we have seen how to compute the posterior over the hidden variables given the observed variables assuming the parameters are known. in this section we discuss how to learn the parameters. in section we showed that when we have complete data and a factored prior the posterior over the parameters also factorizes making computation very simple. unfortunately this is no longer true if we have hidden variables andor missing data. the reason is apparent from looking at figure if the zi were observed then by d-separation we see that z xd and hence the posterior will factorize. but since in an lvm the zi are hidden the parameters are no longer independent and the posterior does not factorize making it much harder to chapter mixture models and the em algorithm figure left n data points sampled from a mixture of gaussians in with k k and right likelihood surface pd with all other parameters set to their true values. we see the two symmetric modes reflecting the unidentifiability of the parameters. figure generated by mixgaussliksurfacedemo. compute. this also complicates the computation of map and ml estimates as we discus below. unidentifiability the main problem with computing p for an lvm is that the posterior may have multiple modes. to see why consider a gmm. if the zi were all observed we would have a unimodal posterior for the parameters p dir niw k kd consequently we can easily find the globally optimal map estimate hence globally optimal mle. but now suppose the zi s are hidden. in this case for each of the possible ways of filling in the zi s we get a different unimodal likelihood. thus when we marginalize out over the zi s we get a multi-modal posterior for p these modes correspond to different labelings of the clusters. this is illustrated in figure where we plot the likelihood function pd for a gmm with k for the data is shown in figure we see two peaks one corresponding to the case where and the other to the case where we say the parameters are not identifiable since there is not a unique mle. therefore there cannot be a unique map estimate the prior does not rule out certain labelings and hence the posterior must be multimodal. the question of how many modes there do not confuse multimodality of the parameter posterior p with the multimodality defined by the model px in the latter case if we have k clusters we would expect to only get k peaks although it is theoretically possible to get more than k at least if d and williams parameter estimation for mixture models unidentifiability can cause a problem for bayesian inference. are in the parameter posterior is hard to answer. there are k! possible labelings but some of the peaks might get merged. nevertheless there can be an exponential number since finding the optimal mle for a gmm is np-hard et al. drineas et al. for example suppose we draw some samples from the posterior p and then average them to try to approximate the posterior mean kind of monte carlo approach is s explained in more detail in chapter if the samples come from different modes the average will be meaningless. note however that it is reasonable to average the posterior predictive distributions px px since the likelihood function is invariant to which mode the parameters came from. s a variety of solutions have been proposed to the unidentifiability problem. these solutions depend on the details of the model and the inference algorithm that is used. for example see for an approach to handling unidentifiability in mixture models using mcmc. the approach we will adopt in this chapter is much simpler we just compute a single say approximate since finding local mode i.e. we perform approximate map estimation. the globally optimal mle and hence map estimate is np-hard at least for mixture models et al. this is by far the most common approach because of its simplicity. it is also a reasonable approximation at least if the sample size is sufficiently large. to see why consider figure we see that there are n latent variables each of which gets to see one data point each. however there are only two latent parameters each of which gets to see n data points. so the posterior uncertainty about the parameters is typically much less than the posterior uncertainty about the latent variables. this justifies the common strategy of computing pzixi but not bothering to compute p in section we will study hierarchical bayesian models which essentially put structure on top of the parameters. in such models it is important to model p so that the parameters can send information between themselves. if we used a point estimate this would not be possible. computing a map estimate is non-convex in the previous sections we have argued rather heuristically that the likelihood function has multiple modes and hence that finding an map or ml estimate will be hard. in this section we show this result by more algebraic means which sheds some additional insight into the problem. our presentation is based in part on consider the log-likelihood for an lvm pxi zi log pd log i zi unfortunately this objective is hard to maximize. since we cannot push the log inside the sum. this precludes certain algebraic simplications but does not prove the problem is hard. now suppose the joint probability distribution pzi xi is in the exponential family which means it can be written as follows px z z exp t z chapter mixture models and the em algorithm where z are the sufficient statistics and z is the normalization constant section for more details. it can be shown that the mvn is in the exponential family as are nearly all of the distributions we have encountered so far including dirichlet multinomial gamma wishart etc. student distribution is a notable exception. furthermore mixtures of exponential families are also in the exponential family providing the mixing indicator variables are observed with this assumption the complete data log likelihood can be written as follows log pxi zi t zi n z i i the first term is clearly linear in one can show that z is a convex function and vandenberghe so the overall objective is concave to the minus sign and hence has a unique maximum. now consider what happens when we have missing data. the observed data log likelihood is given by log i zi pxi zi log i zi e t n log z one can show that the log-sum-exp function is convex and vandenberghe and we know that z is convex. however the difference of two convex functions is not in general convex. so the objective is neither convex nor concave and has local optima. the disadvantage of non-convex functions is that it is usually hard to find their global optimum. most optimization algorithms will only find a local optimum which one they find depends on where they start. there are some algorithms such as simulated annealing or genetic algorithms that claim to always find the global optimum but this is only under unrealistic assumptions if they are allowed to be cooled infinitely slowly or allowed to run infinitely long in practice we will run a local optimizer perhaps using multiple random restarts to increase out chance of finding a good local optimum. of course careful initialization can help a lot too. we give examples of how to do this on a case-by-case basis. note that a convex method for fitting mixtures of gaussians has been proposed. the idea is to assign one cluster per data point and select from amongst them using a convex penalty rather than trying to optimize the locations of the cluster centers. see and golland for details. this is essentially an unsupervised version of the approach used in sparse kernel logistic regression which we will discuss in section note however that the penalty although convex is not necessarily a good way to promote sparsity as discussed in chapter in fact as we will see in that chapter some of the best sparsity-promoting methods use non-convex penalties and use em to optimie them! the moral of the story is do not be afraid of non-convexity. the em algorithm for many models in machine learning and statistics computing the ml or map parameter estimate is easy provided we observe all the values of all the relevant random variables i.e. if the em algorithm model mix. gaussians mix. experts factor analysis student t probit regression dgm with hidden variables mvn with missing data hmms shrinkage estimates of gaussian means section exercise table some models discussed in this book for which em can be easily applied to find the ml map parameter estimate. we have complete data. however if we have missing data andor latent variables then computing the mlmap estimate becomes hard. one approach is to use a generic gradient-based optimizer to find a local minimum of the negative log likelihood or nll given by nll n log pd however we often have to enforce constraints such as the fact that covariance matrices must be positive definite mixing weights must sum to one etc. which can be tricky exercise in such cases it is often much simpler not always faster to use an algorithm called expectation maximization or em for short et al. meng and van dyk mclachlan and krishnan this is a simple iterative algorithm often with closed-form updates at each step. furthermore the algorithm automatically enforce the required constraints. em exploits the fact that if the data were fully observed then the ml map estimate would be easy to compute. in particular em is an iterative algorithm which alternates between inferring the missing values given the parameters step and then optimizing the parameters given the filled in data step. we give the details below followed by several examples. we end with a more theoretical discussion where we put the algorithm in a larger context. see table for a summary of the applications of em in this book. basic idea let xi be the visible or observed variables in case i and let zi be the hidden or missing variables. the goal is to maximize the log likelihood of the observed data log pxi pxi zi log zi unfortunately this is hard to optimize since the log cannot be pushed inside the sum. chapter mixture models and the em algorithm em gets around this problem as follows. define the complete data log likelihood to be log pxi zi this cannot be computed since zi is unknown. so let us define the expected complete data log likelihood as follows q t e t where t is the current iteration number. q is called the auxiliary function. the expectation is taken wrt the old parameters t and the observed data d. the goal of the e stepis to compute q t or rather the terms inside of it which the mle depends on these are known as the expected sufficient statistics or ess. in the m step we optimize the q function wrt t arg max q t to perform map estimation we modify the m step as follows t argmax q t log p the e step remains unchanged. in section we show that the em algorithm monotonically increases the log likelihood of the observed data the log prior if doing map estimation or it stays the same. so if the objective ever goes down there must be a bug in our math or our code. is a surprisingly useful debugging tool! below we explain how to perform the e and m steps for several simple models that should make things clearer. em for gmms in this section we discuss how to fit a mixture of gaussians using em. fitting other kinds of mixture models requires a straightforward modification see exercise we assume the number of mixture components k is known section for discussion of this point. the em algorithm auxiliary function the expected complete data log likelihood is given by i q e e k k i i i log pxi zi kpxi kizik log e k log kpxi k pzi kxi t log kpxi k rik log pxi k rik log k where rik pzi kxi is the responsibility that cluster k takes for data point i. this is computed in the e step described below. k k i i e step the e step has the following simple form which is the same for any mixture model rik m step kpxi pxi k in the m step we optimize q wrt and the k. for we obviously have i n rik rk n k where rk i rik is the weighted number of points assigned to cluster k. to derive the m step for the k and k terms we look at the parts of q that depend on k and k. we see that the result is k k rik log pxi k i i k rik log k kt k k this is just a weighted version of the standard problem of computing the mles of an mvn section one can show that the new parameter estimates are given by k k i rikxi rk i rikxi kxi kt rk i rikxixt i rk k t k chapter mixture models and the em algorithm these equations make intuitive sense the mean of cluster k is just the weighted average of all points assigned to cluster k and the covariance is proportional to the weighted empirical scatter matrix. after computing the new estimates we set t k k k for k and go to the next e step. example an example of the algorithm in action is shown in figure we start with i i. we color code points such that blue points come from cluster and red points from cluster more precisely we set the color to colori so ambiguous points appear purple. after iterations the algorithm has converged on a good clustering. data was standardized by removing the mean and dividing by the standard deviation before processing. this often helps convergence. k-means algorithm there is a popular variant of the em algorithm for gmms known as the k-means algorithm which we now discuss. consider a gmm in which we make the following assumptions k is fixed and k is fixed so only the cluster centers k r d have to be estimated. now consider the following delta-function approximation to the posterior computed during the e step pzi kxi ik z i where zi argmaxk pzi kxi this is sometimes called hard em since we are making a hard assignment of points to clusters. since we assumed an equal spherical covariance matrix for each cluster the most probable cluster for xi can be computed by finding the nearest prototype z i arg min k hence in each e step we must find the euclidean distance between n data points and k cluster centers which takes on kd time. however this can be sped up using various techniques such as applying the triangle inequality to avoid some redundant computations given the hard cluster assignments the m step updates each cluster center by computing the mean of all points assigned to it k nk xi izik see algorithm for the pseudo-code. the em algorithm figure illustration of the em for a gmm applied to the old faithful data. initial values of the parameters. posterior responsibility of each point computed in the first e step. the degree of redness indicates the degree to which the point belongs to the red cluster and similarly for blue this purple points have a roughly uniform posterior over clusters. we show the updated parameters after the first m step. after iterations. after iterations. after iterations. based on figure figure generated by mixgaussdemofaithful. chapter mixture models and the em algorithm algorithm k-means algorithm initialize mk repeat assign each data point to its closest cluster center zi arg mink update each cluster center by computing the mean of all points assigned to it k nk izik xi until converged figure an image compressed using vector quantization with a codebook of size k. k k figure generated by vqdemo. vector quantization since k-means is not a proper em algorithm it is not maximizing likelihood. instead it can be interpreted as a greedy algorithm for approximately minimizing a loss function related to data compression as we now explain. suppose we want to perform lossy compression of some real-valued vectors xi r d. a very simple approach to this is to use vector quantization or vq. the basic idea is to replace each real-valued vector xi r d with a discrete symbol zi k which is an index into a codebook of k prototypes k r d. each data vector is encoded by using the index of the most similar prototype where similarity is measured in terms of euclidean distance encodexi arg min k we can define a cost function that measures the quality of a codebook by computing the reconstruction error or distortion it induces j zk x n n zi where decodek k. the k-means algorithm can be thought of as a simple iterative scheme for minimizing this objective. of course we can achieve zero distortion if we assign one prototype to every data vector but that takes on dc space where n is the number of real-valued data vectors each of the em algorithm length d and c is the number of bits needed to represent a real-valued scalar quantization accuracy. however in many data sets we see similar vectors repeatedly so rather than storing them many times we can store them once and then create pointers to them. hence we can reduce the space requirement to on k kdc the on k term arises because each of the n data vectors needs to specify which of the k codewords it is using pointers and the okdc term arises because we have to store each codebook entry each of which is a d-dimensional vector. typically the first term dominates the second so we can approximate the rate of the encoding scheme of bits needed per object as k which is typically much less than odc. one application of vq is to image compression. consider the n pixel image in figure this is gray-scale so d if we use one byte to represent each pixel gray-scale intensity of to then c so we need n c bits to represent the image. for the compressed image we need n k kc bits. for k this is about a factor of compression. for k this is about a factor of compression at negligible perceptual loss figure greater compression could be achieved if we modelled spatial correlation between the pixels e.g. if we encoded blocks used by jpeg. this is because the residual errors from the model s predictions would be smaller and would take fewer bits to encode. initialization and avoiding local minima both k-means and em need to be initialized. it is common to pick k data points at random and to make these be the initial cluster centers. or we can pick the centers sequentially so as to try to cover the data. that is we pick the initial point uniformly at random. then each subsequent point is picked from the remaining points with probability proportional to its squared distance to the points s closest cluster center. this is known as farthest point clustering or k-means and vassilvitskii bahmani et al. surprisingly this simple trick can be shown to guarantee that the distortion is never more than olog k worse than optimal and vassilvitskii an heuristic that is commonly used in the speech recognition community is to incrementally grow gmms we initially give each cluster a score based on its mixture weight after each round of training we consider splitting the cluster with the highest score into two with the new centroids being random perturbations of the original centroid and the new scores being half of the old scores. if a new cluster has too small a score or too narrow a variance it is removed. we continue in this way until the desired number of clusters is reached. see and jain for a similar incremental approach. map estimation as usual the mle may overfit. the overfitting problem is particularly severe in the case of gmms. to understand the problem suppose for simplicity that k ki and that k it is possible to get an infinite likelihood by assigning one of the centers say to a single data point say since then the term makes the following contribution to the likelihood n x p x chapter mixture models and the em algorithm s l i a f m m g r o f m e s e m i t f o n o i t c a r f mle map dimensionality figure illustration of how singularities can arise in the likelihood function of gmms. based on figure figure generated by mixgausssingularity. illustration of the benefit of map estimation vs ml estimation when fitting a gaussian mixture model. we plot the fraction of times of random trials each method encounters numerical problems vs the dimensionality of the problem for n samples. solid red curve mle. dotted black curve map. figure generated by mixgaussmlvsmap. hence we can drive this term to infinity by letting as shown in figure we will call this the collapsing variance problem an easy solution to this is to perform map estimation. the new auxiliary function is the expected complete data log-likelihood plus the log prior old rik log ik i k i k rik log pxi k log p k log p note that the e step remains unchanged but the m step needs to be modified as we now explain. for the prior on the mixture weights it is natural to use a dirichlet prior dir since this is conjugate to the categorical distribution. the map estimate is given by k rk k k k k n if we use a uniform prior k this reduces to equation the prior on the parameters of the class conditional densities p k depends on the form of the class conditional densities. we discuss the case of gmms below and leave map estimation for mixtures of bernoullis to exercise for simplicity let us consider a conjugate prior of the form p k k niw k the em algorithm from section the map estimate is given by k xk k sk rkxk rk i rikxi rk sk rk d rikxi xkxi xkt i we now illustrate the benefits of using map estimation instead of ml estimation in the context of gmms. we apply em to some synthetic data in d dimensions using either ml or map estimation. we count the trial as a failure if there are numerical issues involving singular matrices. for each dimensionality we conduct random trials. the results are illustrated in figure using n we see that as soon as d becomes even moderately large ml estimation crashes and burns whereas map estimation never encounters numerical problems. when using map estimation we need to specify the hyper-parameters. here we mention some simple heuristics for setting them and raftery we can set so that the k are unregularized since the numerical problems only arise from k. in this case the map estimates simplify to k xk and k which is not quite so scary-looking. now we discuss how to set one possibility is to use d k is the pooled variance for dimension j. where sj reason term is that the resulting volume of each ellipsoid is then given by for the d. the parameter controls how strongly we believe this prior. the weakest prior we can use while still being proper is to set d so this is a common choice. em for mixture of experts we can fit a mixture of experts model using em in a straightforward manner. the expected complete data log likelihood is given by q old rik log ikn k xi k ik svt xik rik old ik n i wold k old k so the e step is the same as in a standard mixture model except we have to replace k with ik when computing rik. chapter mixture models and the em algorithm in the m step we need to maximize q old wrt wk k and v. for the regression parameters for model k the objective has the form q k old rik wt k xi k if rik is we recognize this as a weighted least squares problem which makes intuitive sense small then data point i will be downweighted when estimating model k s parameters. from section we can immediately write down the mle as wk rkx rky where rk diagrk. the mle for the variance is given by rikyi wt rik k k we replace the estimate of the unconditional mixing weights with the estimate of the gating parameters v. the objective has the form rik log ik i k we recognize this as equivalent to the log-likelihood for multinomial logistic regression in equation except we replace the hard encoding yi with the soft encoding ri. thus we can estimate v by fitting a logistic regression model to soft target labels. em for dgms with hidden variables we can generalize the ideas behind em for mixtures of experts to compute the mle or map estimate for an arbitrary dgm. we could use gradient-based methods et al. but it is much simpler to use em in the e step we just estimate the hidden variables and in the m step we will compute the mle using these filled-in values. we give the details below. for simplicity of presentation we will assume all cpds are tabular. based on section let us write each cpt as follows pxitxipat t ixitixipatc tck the log-likelihood of the complete data is given by log pd ntck log tck where ntck complete data log-likelihood has the form e pd t c k ixit i xipat c are the empirical counts. hence the expected n tck log tck the em algorithm where pxit k xipat cdi e n tck ixit i xipat c where di are all the visible variables in case i. i the quantity pxit xipatdi is known as a family marginal and can be computed using any gm inference algorithm. the n tjk are the expected sufficient statistics and constitute the output of the e step. given these ess the m step has the simple form tck n n k tjk this can be proved by adding lagrange multipliers enforce the constraint to the expected complete data log likelihood and then optimizing each parameter vector tc separately. we can modify this to perform map estimation with a dirichlet prior by simply adding pseudo counts to the expected counts. em for the student distribution one problem with the gaussian distribution is that it is sensitive to outliers since the logprobability only decays quadratically with distance from the center. a more robust alternative is the student t distribution as discussed in section unlike the case of a gaussian there is no closed form formula for the mle of a student even if we have no missing data so we must resort to iterative optimization methods. the easiest one to use is em since it automatically enforces the constraints that is positive and that is symmetric positive definite. in addition the resulting algorithm turns out to have a simple intuitive form as we see below. at first blush it might not be apparent why em can be used since there is no missing data. the key idea is to introduce an artificial hidden or auxiliary variable in order to simplify the algorithm. in particular we will exploit the fact that a student distribution can be written as a gaussian scale mixture t n exercise for a proof of this in the case. this can be thought of as an infinite mixture of gaussians each one with a slightly different covariance matrix. treating the zi as missing data we can write the complete data log likelihood as n log gazi d zi zi d log zi log zi i log log chapter mixture models and the em algorithm where we have defined the mahalanobis distance to be i we can partition this into two terms one involving and and the other involving we have dropping irrelevant constants ln n g n log lg n log zi i n log zi zi em with known let us first derive the algorithm with assumed known for simplicity. in this case we can ignore the lg term so we only need to figure out how to compute e wrt the old parameters. now if zi gaa b then e ab. hence the e step at iteration t is from section we have pzixi gazi d i i e zt zixi d i the m step is obtained by maximizing e to yield i zt i xi i zt n i n i i zt zt i xixt i i zt i these results are quite intuitive the quantity zi is the precision of measurement i so if it is small the corresponding data point is down-weighted when estimating the mean and covariance. this is how the student achieves robustness to outliers. em with unknown to compute the mle for the degrees of freedom we first need to compute the expectation of lg which involves zi and log zi. now if zi gaa b then one can show that i e log zi log b the em algorithm errors using gauss bankrupt solvent errors using student bankrupt solvent figure mixture modeling on the bankruptcy data set. left gaussian class conditional densities. right student class conditional densities. points that belong to class are shown as triangles points that belong to class are shown as circles the estimated labels based on the posterior probability of belonging to each mixture component are computed. if these are incorrect the point is colored red otherwise it is colored blue. data is in black. figure generated by mixstudentbankruptcydemo. where d dx log is the digamma function. hence from equation we have i d log i logzt i d log d substituting into equation we have e n log n log i i zt i the gradient of this expression is equal to n e n d d this has a unique solution in the interval which can be found using a constrained i zt i log n i optimizer. performing a gradient-based optimization in the m step rather than a closed-form update is an example of what is known as the generalized em algorithm. one can show that em will still converge to a local optimum even if we only perform a partial improvement to the parameters in the m step. mixtures of student distributions it is easy to extend the above methods to fit a mixture of student distributions. see exercise for the details. let us consider a small example from we have a n d data set regarding the bankrupty patterns of certain companies. the first feature specifies the ratio chapter mixture models and the em algorithm of retained earnings to total assets and the second feature specifies the ratio of earnings before interests and taxes to total assets. we fit two models to this data ignoring the class labels a mixture of gaussians and a mixture of students. we then use each fitted model to classify the data. we compute the most probable cluster membership and treat this as yi. we then compare yi to the true labels yi and compute an error rate. if this is more than we permute the latent labels we consider cluster to represent class and vice versa and then recompute the error rate. points which are misclassified are then shown in red. the result is shown in figure we see that the student model made errors the gaussian model made this is because the class-conditional densities contain some extreme values causing the gaussian to be a poor choice. em for probit regression in section we described the latent variable interpretation of probit regression. recall that this has the form pyi zi where zi n xi is latent. we now show how to fit this model using em. it is possible to fit probit regression models using gradient based methods as shown in section this em-based approach has the advantage that it generalized to many other kinds of models as we will see later on. the complete data log likelihood has the following form assuming a n prior on w log pyz log n i log n xwt xw log pyizi w const wt v i the posterior in the e step is a truncated gaussian pziyi xi w n xi n xi if yi if yi in equation we see that w only depends linearly on z so we just need to compute e xi w. exercise asks you to show that the posterior mean is given by e xi i i i i i i i i i i i i if yi if yi where i wt xi. in the m step we estimate w using ridge regression where e is the output we are trying to predict. specifically we have w xt x the em algorithm is simple but can be much slower than direct gradient methods as illustrated in figure this is because the posterior entropy in the e step is quite high since we only observe that z is positive or negative but are given no information from the likelihood about its magnitude. using a stronger regularizer can help speed convergence because it constrains the range of plausible z values. in addition one can use various speedup tricks such as data augmentation dyk and meng but we do not discuss that here. the em algorithm probit regression with regularizer of em minfunc l l n d e z i l a n e p iter figure by probitregdemo. fitting a probit regression model in using a quasi-newton method or em. figure generated theoretical basis for em in this section we show that em monotonically increases the observed data log likelihood until it reaches a local maximum saddle point although such points are usually unstable. our derivation will also serve as the basis for various generalizations of em that we will discuss later. expected complete data log likelihood is a lower bound consider an arbitrary distribution qzi over the hidden variables. the observed data log likelihood can be written as follows pxi zi pxi zi log qzi log zi zi qzi now logu is a concave function so from jensen s inequality we have the following lower bound i zi qizi log pxi zi qizi let us denote this lower bound as follows q q eqi pxi zi h i where h is the entropy of qi. the above argument holds for any positive distribution q. which one should we choose? intuitively we should pick the q that yields the tightest lower bound. the lower bound is a sum chapter mixture models and the em algorithm over i of terms of the following form l qi qizi log qizi log zi zi pxi zi qizi pzixi qizi pzixi qizi log kl log pxi qizi zi zi qizi log pxi the pxi term is independent of qi so we can maximize the lower bound by setting qizi pzixi of course is unknown so instead we use qt i pzixi t where t is our estimate of the parameters at iteration t. this is the output of the e step. plugging this in to the lower bound we get q qt pxi zi h qt i eqt i i we recognize the first term as the expected complete data log likelihood. the second term is a constant wrt so the m step becomes arg max q t arg max pxi zi eqt i i as usual. zero so l t qi log pxi t and hence now comes the punchline. since we used qt i pzixi t the kl divergence becomes q t t log pxi t t i we see that the lower bound is tight after the e step. since the lower bound touches the function maximizing the lower bound will also push up on the function itself. that is the m step is guaranteed to modify the parameters so as to increase the likelihood of the observed data it is already at a local maximum. this process is sketched in figure the dashed red curve is the original function observed data log-likelihood. the solid blue curve is the lower bound evaluated at t this touches the objective function at t. we then set to the maximum of the lower bound curve and fit a new bound at that point green curve. the maximum of this new bound becomes etc. this to newton s method in figure which repeatedly fits and then optimizes a quadratic approximation. em monotonically increases the observed data log likelihood we now prove that em monotonically increases the observed data log likelihood until it reaches a local optimum. we have q t q t t t the em algorithm q t q l t figure illustration of em as a bound optimization algorithm. based on figure of figure generated by emloglikelihoodmax. where the first inequality follows since q is a lower bound on the second inequality follows since by definition q t max q t q t t and the final equality follows equation as a consequence of this result if you do not observe monotonic increase of the observed you are performing data log likelihood you must have an error in your math andor code. map estimation you must add on the log prior term to the objective. this is a surprisingly powerful debugging tool. online em when dealing with large or streaming datasets it is important to be able to learn online as we discussed in section there are two main approaches to online em in the literature. the first approach known as incremental em and hinton optimizes the lower bound q qn one qi at a time however this requires storing the expected sufficient statistics for each data case. the second approach known as stepwise em and ishii cappe and mouline cappe is based on stochastic approximation theory and only requires constant memory use. we explain both approaches in more detail below following the presentation of and klein liang and klein. batch em review before explaining online em we review batch em in a more abstract setting. let z be a vector of sufficient statistics for a single data case. example for a mixture of multinoullis this would be the count vector aj which is the number of cluster j was used in z plus the matrix bj v which is of the number of times the hidden state was j and the observed letter z pzxi z be the expected sufficient statistics for case i and was v. let si si be the sum of the ess. given we can derive an ml or map estimate of the parameters in the m step we will denote this operation by example in the case of mixtures of multinoullis we just need to normalize a and each row of b. with this notation under our belt the pseudo code for batch em is as shown in algorithm chapter mixture models and the em algorithm algorithm batch em algorithm initialize repeat new for each example i do z pzxi z si new new si new until converged incremental em in incremental em and hinton we keep track of as well as the si. when we come to a data case we swap out the old si and replace it with the new snew as shown in the code in algorithm note that we can exploit the sparsity of snew to speedup the computation of since most components of wil not have changed. i i i si algorithm incremental em algorithm initialize si for i n repeat for each example i in a random order do z pzxi z i si snew i snew si snew i until converged this can be viewed as maximizing the lower bound q qn by optimizing then then then etc. as such this method is guaranteed to monotonically converge to a local maximum of the lower bound and to the log likelihood itself. stepwise em in stepwise em whenever we compute a new si we move towards it as shown in algorithm at iteration k the stepsize has value k which must satisfy the robbins-monro conditions in for equation we can get somewhat better behavior by using a minibatch of size m before it is possible to optimize m and to maximize the training set likelihood by each update. and klein liang and klein use k k for example a detail as written the update for does not exploit the sparsity of si. we can fix this by storing m instead of and then using the sparse update m m since scaling the counts by a global constant has no effect. j j si. this will not affect the results k the em algorithm figure uated_optimization. illustration of deterministic annealing. based on httpen.wikipedia.orgwikigrad trying different values in parallel for an initial trial period this can significantly speed up the algorithm. algorithm stepwise em algorithm initialize k repeat for each example i in a random order do z pzxi z si k ksi k k until converged and klein liang and klein compare batch em incremental em and stepwise em on four different unsupervised language modeling tasks. they found that stepwise em and m was faster than incremental em and both were much faster than batch em. in terms of accuracy stepwise em was usually as good or sometimes even better than batch em incremental em was often worse than either of the other methods. other em variants em is one of the most widely used algorithms in statistics and machine learning. not surprisingly many variations have been proposed. we briefly mention a few below some of which we will use in later chapters. see and krishnan for more information. annealed em in general em will only converge to a local maximum. to increase the chance of finding the global maximum we can use a variety of methods. one approach is to use a method known as deterministic annealing the basic idea is to smooth the posterior landscape by raising it to a temperature and then gradually cooling it all the while slowly tracking the global maximum. see figure for a sketch. stochastic version chapter mixture models and the em algorithm true log likelihood lower bound true log likelihood lower bound training time training time figure illustration of possible behaviors of variational em. the lower bound increases at each iteration and so does the likelihood. in this case the algorithm is closing the gap between the approximate and true posterior. this can have a regularizing effect. based on figure of et al. figure generated by varembound. the lower bound increases but the likelihood decreases. of this algorithm is described in section an annealed version of em is described in and nakano variational em in section we showed that the optimal thing to do in the e step is to i zixi t. in this case make qi be the exact posterior over the latent variables qt the lower bound on the log likelihood will be tight so the m step will push up on the log-likelihood itself. however sometimes it is computationally intractable to perform exact inference in the e step but we may be able to perform approximate inference. if we can ensure that the e step is performing inference based on a a lowerbound to the likelihood then the m step can be seen as monotonically increasing this lower bound figure this is called variational em and hinton see chapter for some variational inference methods that can be used in the e step. monte carlo em another approach to handling an intractable e step is to use a monte carlo approximation to the expected sufficient statistics. that is we draw samples from the i pzixi t and then compute the sufficient statistics for each completed posterior zs vector zs i and then average the results. this is called monte carlo em or mcem we only draw a single sample it is called stochastic em and and tanner diebolt one way to draw samples is to use mcmc chapter however if we have to wait for mcmc to converge inside each e step the method becomes very slow. an alternative is to use stochastic approximation and only perform brief sampling in the e step followed by a partial parameter update. this is called stochastic approximation em et al. and tends to work better than mcem. another alternative is to apply mcmc to infer the parameters as well as the latent variables fully bayesian approach thus eliminating the distinction between e and m steps. see chapter for details. generalized em sometimes we can perform the e step exactly but we cannot perform the m step exactly. however we can still monotonically increase the log likelihood by performing a partial m step in which we merely increase the expected complete data log likelihood rather than maximizing it. for example we might follow a few gradient steps. this is called the em algorithm k i l g o l em iterations k i l g o l em iterations figure illustration of adaptive over-relaxed em applied to a mixture of gaussians in dimensions. we show the algorithm applied to two different datasets randomly sampled from a mixture of gaussians. we plot the convergence for different update rates using gives the same results as regular em. the actual running time is printed in the legend. figure generated by mixgaussoverrelaxedemdemo. the generalized em or gem algorithm. ways to generalize em.... is an unfortunate term since there are many ecme algorithm the ecm algorithm stands for expectation conditional maximization and refers to optimizing the parameters in the m step sequentially if they turn out to be dependent. the ecme algorithm which stands for ecm either and rubin is a variant of ecm in which we maximize the expected complete data log likelihood q function as usual or the observed data log likelihood during one or more of the conditional maximization steps. the latter can be much faster since it ignores the results of the e step and directly optimizes the objective of interest. a standard example of this is when fitting the student t distribution. for fixed we can update as usual but then to update we replace the standard update of the form arg max q t with arg max log pd see and krishnan for more information. over-relaxed em vanilla em can be quite slow especially if there is lots of missing data. the adaptive overrelaxed em algorithm and roweis performs an update of the form t t t where is a step-size parameter and m t is the usual update computed during the m step. obviously this reduces to standard em if but using larger values of can result in faster convergence. see figure for an illustration. unfortunately using too large a value of can cause the algorithm to fail to converge. finally note that em is in fact just a special case of a larger class of algorithms known as bound optimization or mm algorithms stands for minorize-maximize. see and lange for further discussion. chapter mixture models and the em algorithm model selection for latent variable models when using lvms we must specify the number of latent variables which controls the model in the case of mixture models we must specify k the number complexity. of clusters. choosing these parameters is an example of model selection. we discuss some approaches below. in particuarl model selection for probabilistic models the optimal bayesian approach discussed in section is to pick the model with the largest marginal likelihood k argmaxk pdk. there are two problems with this. likelihood for lvms is quite difficult. in practice simple approximations such as bic can be used e.g. and raftery alternatively we can use the cross-validated likelihood as a performance measure although this can be slow since it requires fitting each model f times where f is the number of cv folds. first evaluating the marginal the second issue is the need to search over a potentially large number of models. the usual approach is to perform exhaustive search over all candidate values of k. however sometimes we can set the model to its maximal size and then rely on the power of the bayesian occam s razor to kill off unwanted components. an example of this will be shown in section when we discuss variational bayes. an alternative approach is to perform stochastic sampling in the space of models. traditional approaches such as lunn et al. are based on reversible jump mcmc and use birth moves to propose new centers and death moves to kill off old centers. however this can be slow and difficult to implement. a simpler approach is to use a dirichlet process mixture model which can be fit using gibbs sampling but still allows for an unbounded number of mixture components see section for details. perhaps surprisingly these sampling-based methods can be faster than the simple approach of evaluating the quality of each k separately. the reason is that fitting the model for each k is often slow. by contrast the sampling methods can often quickly determine that a certain value of k is poor and thus they need not waste time in that part of the posterior. model selection for non-probabilistic methods ed k i d what if we are not using a probabilistic model? for example how do we choose k for the kmeans algorithm? since this does not correspond to a probability model there is no likelihood so none of the methods described above can be used. struction error of a data set d using model complexity k as follows an obvious proxy for the likelihood is the reconstruction error. define the squared recon in the case of k-means the reconstruction is given by xi zi where zi argmink as explained in section figure plots the reconstruction error on the test set for k-means. we notice that the error decreases with increasing model complexity! the reason for this behavior is as follows model selection for latent variable models mse on test vs k for k means nll on test set vs k for gmm figure test set performance vs k for data generated from a mixture of gaussians in is shown in figure mse on test set for k-means. negative log likelihood on test set for gmm. figure generated by xtrain figure data looks essentially the same. gmm density model estimated by em for for the same values of k. synthetic data generated from a mixture of gaussians in histogram of training data. centroids estimated by k-means for k figure generated by chapter mixture models and the em algorithm when we add more and more centroids to k-means we can tile the space more densely as shown in figure hence any given test vector is more likely to find a close prototype to accurately represent it as k increases thus decreasing reconstruction error. however if we use a probabilistic model such as the gmm and plot the negative log-likelihood we get the usual u-shaped curve on the test set as shown in figure in supervised learning we can always use cross validation to select between non-probabilistic models of different complexity but this is not the case with unsupervised learning. although this is not a novel observation it is mentioned in passing in et al. one of the standard references in this field it is perhaps not as widely appreciated as it should be. in fact it is one of the more compelling arguments in favor of probabilistic models. given that cross validation doesn t work and supposing one is unwilling to use probabilistic models some bizarre reason... how can one choose k? the most common approach is to plot the reconstruction error on the training set versus k and to try to identify a knee or kink in the curve. the idea is that for k k is the true number of clusters the rate of decrease in the error function will be high since we are splitting apart things that should not be grouped together. however for k k we are splitting apart natural clusters which does not reduce the error by as much. where k this kink-finding process can be automated by use of the gap statistic et al. nevertheless identifying such kinks can be hard as shown in figure since the loss function usually drops off gradually. a different approach to kink finding is described in section fitting models with missing data suppose we want to fit a joint density model by maximum likelihood but we have holes in our data matrix due to missing data represented by nans. more formally let oij if component j of data case i is observed and let oij otherwise. let xv oij be the visible data and xh oij be the missing or hidden data. our goal is to compute argmax pxv o under the missing at random assumption section we have pxv o pxiv where xiv is a vector created from row i and the columns indexed by the set oij hence the log-likelihood has the form log pxv log pxiv where pxiv xih i pxiv xih fitting models with missing data and xih is the vector of hidden variables for case i discrete for notational simplicity. substituting in we get log pxv pxiv xih log i xih unfortunately this objective is hard to maximize. since we cannot push the log inside the sum. however we can use the em algorithm to compute a local optimum. we give an example of this below. em for the mle of an mvn with missing data suppose we want to fit an mvn by maximum likelihood but we have missing data. we can use em to find a local maximum of the objective as we explain below. getting started to get the algorithm started we can compute the mle based on those rows of the data matrix that are fully observed. if there are no such rows we can use some ad-hoc imputation procedures and then compute an initial mle. e step once we have t we can compute the expected complete data log likelihood at iteration t as follows q t log n t e i tr e i tr e n n n log log log n d t where e e drop the conditioning of the expectation on d and t for brevity. we see that we need to compute these are the expected sufficient statistics. t e xixt i i e and i e xixt i i i where components v are observed and components h are unobserved. we have to compute these quantities we use the results from section specifically consider case xihxiv n vi mi h hv vi hh hv vv v vv vh chapter mixture models and the em algorithm hence the expected sufficient statistics are e xiv xiv where we have assumed loss of generality that the unobserved variables come before the observed variables in the node ordering. we use the result that cov e xihxt e ih xiv e xt ih xt iv xih xiv t e e xt xxt e xt iv xivxt iv to compute e xixt i e xixt i e hence e xihxt ih e e t vi m step by solving q we can show that the m step is equivalent to plugging these ess into the usual mle equations to get t t n n i e xixt i e i t tt thus we see that em is not equivalent to simply replacing variables by their expectations and applying the standard mle formula that would ignore the posterior variance and would result in an incorrect estimate. instead we must compute the expectation of the sufficient statistics and plug that into the usual equation for the mle. we can easily modify the algorithm to perform map estimation by plugging in the ess into the equation for the map estimate. for an implementation see gaussmissingfitem. example as an example of this procedure in action let us reconsider the imputation problem from section which had n data cases with missing data. let us fit the parameters using em. call the resulting parameters we can use our model for predictions by computing e figure indicates that the results obtained using the learned parameters are almost as good as with the true parameters. not surprisingly performance improves with more data or as the fraction of missing data is reduced. xihxiv extension to the gmm case it is straightforward to fit a mixture of gaussians in the presence of partially observed data vectors xi. we leave the details as an exercise. exercises exercise student t as infinite mixture of gaussians derive equation for simplicity assume a one-dimensional distribution. fitting models with missing data imputation with true params imputation with em d e t u p m i d e t u p m i truth truth d e t u p m i d e t u p m i truth truth d e t u p m i d e t u p m i truth truth d e t u p m i d e t u p m i truth truth figure illustration of data imputation. ing true parameters. gaussimputationdemo. scatter plot of true values vs imputed values usb same as but using parameters estimated with em. figure generated by exercise em for mixtures of gaussians show that the m step for ml estimation of a mixture of gaussians is given by k k i rikxi rk i rikxi kxi kt rk i rk k t i rikxixt rk k exercise em for mixture of student distributions derive the em algorithm for ml estimation of a mixture of multivariate student t distributions. exercise gradient descent for fitting gmm consider the gaussian mixture model kn k k px k define the log likelihood as log pxn exercise em for mixtures of bernoullis show that the m step for ml estimation of a mixture of bernoullis is given by i rik i i rikxij i rik kj kj show that the m step for map estimation of a mixture of bernoullis with a prior is given by chapter mixture models and the em algorithm p q jn kn xn j m n k l figure a mixture of gaussians with two discrete latent indicators. jn specifies which mean to use and kn specifies which variance to use. define the posterior responsibility that cluster k has for datapoint n as follows rnk pzn kxn kn k k a. show that the gradient of the log-likelihood wrt k is n d d k rnk k k k d dwk n rnk k b. derive the gradient of the log-likelihood wrt k. now ignore any constraints on k. c. one way to handle the constraint that k is to reparameterize using the softmax function here wk r are unconstrained parameters. show that may be a constant factor missing in the above expression... hint use the chain rule and the fact that d j dwk j j k if j k if j k which follows from exercise d. derive the gradient of the log-likelihood wrt k. now ignore any constraints on k. e. one way to handle the constraint that k be a symmetric positive definite matrix is to reparamek r where r is an upper-triangular but otherwise terize using a cholesky decomposition k rt unconstrained matrix. derive the gradient of the log-likelihood wrt rk. exercise em for a finite scale mixture of gaussians jaakkola.. consider the graphical model in figure which defines the following pxn pj qkn j k fitting models with missing data where pm m ql l are all the parameters. here pj p j and qk p k are the equivalent of mixture weights. we can think of this as a mixture of m non-gaussian components where each component distribution is a scale mixture pxj k combining gaussians with different variances qkn j we will now derive a generalized em algorithm for this model. partial update in the m step rather than finding the exact maximum. a. derive an expression for the responsibilities p j kn kxn needed for the e step. b. write out a full expression for the expected complete log-likelihood that in generalized em we do a q new old e old log p kn xn new c. solving the m-step would require us to jointly optimize the means m and the variances l it will turn out to be simpler to first solve for the j s given fixed j s and subsequently j s given the new values of j s. for brevity we will just do the first part. derive an solve for expression for the maximizing j s given fixed i.e. solve q new exercise manual calculation of the m step for a gmm de freitas. in this question we consider clustering data with a mixture of gaussians using the em algorithm. you are given the data points x suppose the output of the e step is the following matrix r where entry ric is the probability of obervation xi belonging to cluster c responsibility of cluster c for data point i. you just have to compute the m step. you may state the equations for maximum likelihood estimates of these quantities you should know without proof you just have to apply the equations to this data set. you may leave your answer in fractional form. show your work. a. write down the likelihood function you are trying to optimize. b. after performing the m step for the mixing weights what are the new values? c. after performing the m step for the means and what are the new values? exercise moments of a mixture of gaussians consider a mixture of k gaussians px kn k k a. show that e k k k chapter mixture models and the em algorithm figure some data points in circles represent the initial guesses for and b. show that cov k k k k t k e e e e xxt hint use the fact that cov e exercise k-means clustering by hand jaakkola. in figure we show some data points which lie on the integer grid. that the x-axis has been compressed distances should be measured using the actual grid coordinates. suppose we apply the kmeans algorithm to this data using k and with the centers initialized at the two circled data points. draw the final clusters obtained after k-means converges the approximate location of the new centers and group together all the points assigned to each center. hint think about shortest euclidean distance. exercise deriving the k-means cost function show that izik nk izik jw hint note that for any i i i where n i x x x nx since i i i x x x xi nx xnx nx exercise visible mixtures of gaussians are in the exponential family show that the joint distribution px z for a gmm can be represented in exponential family form. fitting models with missing data e m i t l a v v r u s i regression with censored data red x censored green predicted em ols inverse temperature figure example of censored linear regression. black circles are observed training points red crosses are observed but censored training points. green stars are predicted values of the censored training points. we also show the lines fit by least squares censoring and by em. based on figure of figure generated by linregcensoredschmeehahndemo written by hannes bretschneider. exercise em for robust linear regression with a student t likelihood consider a model of the form pyixi w t xi derive an em algorithm to compute the mle for w. you may assume and are fixed for simplicity. hint see section exercise em for eb estimation of gaussian shrinkage model extend the results of section to the case where the j are not equal are known. hint treat the j as hidden variables and then to integrate them out in the e step and maximize in the m step. exercise em for censored linear regression censored regression refers to the case where one knows the outcome is at least at most a certain value but the precise value is unknown. this arises in many different settings. for example suppose one is trying to learn a model that can predict how long a program will take to run for different settings of its parameters. one may abort certain runs if they seem to be taking too long the resulting run times are said to be right censored. for such runs all we know is that yi ci where ci is the censoring time that is yi minzi ci where zi is the true running time and yi is the observed running time. we can also define left censored and interval censored derive an em algorithm for fitting a linear regression model to right-censored data. hint use the results from exercise see figure for an example based on the data from and hahn we notice that the em line is tilted upwards more since the model takes into account the fact that the truncated values are actually higher than the observed values. there is a closely related model in econometrics called the tobit model in which yi maxzi so we only get to observe positive outcomes. an example of this is when zi represents desired investment and yi is actual investment. probit regression is another example. chapter mixture models and the em algorithm exercise posterior mean and variance of a truncated gaussian let zi i where n sometimes such as in probit regression or censored regression we do not observe zi but we observe the fact that it is above some threshold namely we observe the event e izi ci ci i exercise for details on censored regression and section for probit regression. show that e ci i h ci i and ci i i ci e i ih where we have defined hu and where is the pdf of a standard gaussian and is its cdf. hint we have pie pie pe wheree is some event of interest. hint it can be shown that n wn and hence d dw c b wn n n latent linear models factor analysis one problem with mixture models is that they only use a single latent variable to generate the observations. in particular each observation can only come from one of k prototypes. one can think of a mixture model as using k hidden binary variables representing a one-hot encoding of the cluster identity. but because these variables are mutually exclusive the model is still limited in its representational power. to use is a gaussian will consider other choices later an alternative is to use a vector of real-valued latent variables zi r pzi n if the observations are also continuous so xi r d we may use a gaussian for the likelihood. just as in linear regression we will assume the mean is a linear function of the inputs thus yielding l. the simplest prior pxizi n where w is a d l matrix known as the factor loading matrix and is a d d covariance matrix. we take to be diagonal since the whole point of the model is to force zi to explain the correlation rather than baking it in to the observation s covariance. this overall model is called factor analysis or fa. the special case in which is called probabilistic principal components analysis or ppca. the reason for this name will become apparent later. the generative process where l d and is diagonal is illustrated in figure we take an isotropic gaussian spray can and slide it along the line defined by wzi this induces an ellongated hence correlated gaussian in fa is a low rank parameterization of an mvn fa can be thought of as a way of specifying a joint density model on x using a small number of parameters. to see this note that from equation the induced marginal distribution pxi is a gaussian pxi n n w chapter latent linear models figure d observed dimensions. based on figure of illustration of the ppca generative process where we have l latent dimension generating from this we see that we can set without loss of generality since we can always absorb w into similarly we can set i without loss of generality because we can always emulate a correlated prior by using defining a new weight matrix w w then we find cov wt e we thus see that fa approximates the covariance matrix of the visible vector using a low-rank wwt decomposition c cov wwt this only uses old parameters which allows a flexible compromise between a full covariance gaussian with parameters and a diagonal covariance with od parameters. note that if we did not restrict to be diagonal we could trivially set to a full covariance matrix then we could set w in which case the latent factors would not be required. inference of the latent factors although fa can be thought of as just a way to define a density on x it is often used because we hope that the latent factors z will reveal something interesting about the data. to do this we need to compute the posterior over the latent factors. we can use bayes rule for gaussians to give pzixi n i wt i mi iwt note that in the fa model i is actually independent of i so we can denote it by computing this matrix takes time and computing each mi e takes ld time. the mi are sometimes called the latent scores or latent factors. factor analysis rotationnone t n e n o p m o c width wheelbase length weight engine cylinders gmc yukon xl slt nissan pathfinder armada se horsepower kia sorento lx mercedes benz saturn honda insight citympg highwaympg retail dealer mercedes benz porsche component figure projection of cars data based on factor analysis. the blue text are the names of cars corresponding to certain chosen points. figure generated by fabiplotdemo. let us give a simple example based we consider a dataset of d variables and n cases describing various aspects of cars such as the engine size the number of cylinders the miles per gallon the price etc. we first fit a l dimensional model. we can plot the mi scores as points in r to visualize the data as shown in figure to get a better understanding of the meaning of the latent factors we can project unit vectors corresponding to each of the feature dimensions etc. into the low dimensional space. these are shown as blue lines in figure this is known as a biplot. we see that the horizontal axis represents price corresponding to the features labeled dealer and retail with expensive cars on the right. the vertical axis represents fuel efficiency in terms of mpg versus size heavy vehicles are less efficient and are higher up whereas light vehicles are more efficient and are lower down. we can verify this interpretation by clicking on some points and finding the closest exemplars in the training set and printing their names as in figure however in general interpreting latent variable models is fraught with difficulties as we discuss in section unidentifiability just like with mixture models fa is also unidentifiable. to see this suppose r is an arbitrary orthogonal rotation matrix satisfying rrt i. let us define w wr then the likelihood chapter latent linear models function of this modified matrix is the same as for the unmodified matrix since cov we zzt wt e wrrt wt wwt geometrically multiplying w by an orthogonal matrix is like rotating z before generating x but since z is drawn from an isotropic gaussian this makes no difference to the likelihood. consequently we cannot unique identify w and therefore cannot uniquely identify the latent factors either. to ensure a unique solution we need to remove ll degrees of freedom since that in total the fa model has d ld is the number of orthonormal matrices of size l ll free parameters the mean where the first term arises from obviously we require this to be less than or equal to dd which is the number of parameters in an unconstrained symmetric covariance matrix. this gives us an upper bound on l as follows lmax for example d implies l but we usually never choose this upper bound since it would result in overfitting discussion in section on how to choose l. unfortunately even if we set l lmax we still cannot uniquely identify the parameters since the rotational ambiguity still exists. non-identifiability does not affect the predictive performance of the model. however it does affect the loading matrix and hence the interpretation of the latent factors. since factor analysis is often used to uncover structure in the data this problem needs to be addressed. here are some commonly used solutions forcing w to be orthonormal perhaps the cleanest solution to the identifiability problem is to force w to be orthonormal and to order the columns by decreasing variance of the corresponding latent factors. this is the approach adopted by pca which we will discuss in section the result is not necessarily more interpretable but at least it is unique. forcing w to be lower triangular one way to achieve identifiability which is popular in the bayesian community and west is to ensure that the first visible feature is only generated by the first latent factor the second visible feature is only generated by the first two latent factors and so on. for example if l and d the correspond factor loading matrix is given by w we also require that wjj for j the total number of parameters in this constrained matrix is d dl ll which is equal to the number of uniquely identifiable parameters. the disadvantage of this method is that the first l visible variables to see this note that there are l free parameters in r in the first column the column vector must be normalized to unit length there are l free parameters in the second column must be orthogonal to the first and so on. factor analysis qi zi xi n k w k k figure mixture of factor analysers as a dgm. known as the founder variables affect the interpretation of the latent factors and so must be chosen carefully. sparsity promoting priors on the weights instead of pre-specifying which entries in w are zero we can encourage the entries to be zero using regularization et al. ard archambeau and bach or spike-and-slab priors et al. this is called sparse factor analysis. this does not necessarily ensure a unique map estimate but it does encourage interpretable solutions. see section choosing an informative rotation matrix there are a variety of heuristic methods that try to find rotation matrices r which can be used to modify w hence the latent factors so as to try to increase the interpretability typically by encouraging them to be sparse. one popular method is known as varimax use of non-gaussian priors for the latent factors in section we will dicuss how replacing pzi with a non-gaussian distribution can enable us to sometimes uniquely identify w as well as the latent factors. this technique is known as ica. mixtures of factor analysers the fa model assumes that the data lives on a low dimensional linear manifold. in reality most data is better modeled by some form of low dimensional curved manifold. we can approximate a curved manifold by a piecewise linear manifold. this suggests the following model let the k th linear subspace of dimensionality lk be represented by wk for k suppose we have a latent indicator qi k specifying which subspace we should use to generate the data. we then sample zi from a gaussian prior and pass it through the wk matrix k qi and add noise. more precisely the model is as follows pxizi qi k k wkzi pzi i pqi catqi chapter latent linear models figure mixture of mixppcademonetlab. ppcas fit to a dataset for k figure generated by this is called a mixture of factor analysers et al. the ci assumptions are represented in figure another way to think about this model is as a low-rank version of a mixture of gaussians. in particular this model needs okld parameters instead of the parameters needed for a mixture of full covariance gaussians. this can reduce overfitting. in fact mfa is a good generic density model for high-dimensional real-valued data. em for factor analysis models using the results from chapter it is straightforward to derive an em algorithm to fit an fa model. with just a little more work we can fit a mixture of fas. below we state the results without proof. the derivation can be found in and hinton however deriving these equations yourself is a useful exercise if you want to become proficient at the math. to obtain the results for a single factor analyser just set ric and c in the equations below. in section we will see a further simplification of these equations that arises when fitting a ppca model where the results will turn out to have a particularly simple and elegant intepretation. in the e step we compute the posterior responsibility of cluster c for data point i using ric pqi cxi cn c wcwt c the conditional posterior for zi is given by pzixi qi c ic c c wc c c c ic wt mic icwt in the m step it is easiest to estimate c and wc at the same time by defining wc principal components analysis c z also define bic e zxi qi c zztxi qi c z ztxi qi c cic e e qi c then the m step is as follows wc ricxibt ic riccic e t i xi wcbic ric xt i i n c n diag ic ric e qi c note that these updates are for vanilla em. a much faster version of this algorithm based on ecm is described in and yu fitting fa models with missing data in many applications such as collaborative filtering we have missing data. one virtue of the em approach to fitting an fappca model is that it is easy to extend to this case. however overfitting can be a problem if there is a lot of missing data. consequently it is important to perform map estimation or to use bayesian inference. see e.g. and raiko for details. principal components analysis consider the fa model where we constrain and w to be orthonormal. it can be shown and bishop that as this model reduces to classical principal components analysis pca also known as the karhunen loeve transform. the version where is known as probabilistic pca and bishop or sensible pca equivalent result was derived independently from a different perspective in and pentland to make sense of this result we first have to learn about classical pca. we then connect pca to the svd. and finally we return to discuss ppca. classical pca statement of the theorem the synthesis view of classical pca is summarized in the forllowing theorem. theorem suppose we want to find an orthogonal set of l linear basis vectors wj r d and the corresponding scores zi r l such that we minimize the average reconstruction error jw z n chapter latent linear models figure an illustration of pca and ppca where d and l circles are the original data points crosses are the reconstructions. the red star is the data mean. pca. the points are orthogonally projected onto the line. figure generated by ppca. the projection is no longer orthogonal the reconstructions are shrunk towards the data mean star. based on figure of figure generated by where xi wzi subject to the constraint that w is orthonormal. equivalently we can write this objective as follows jw z where z is an n l matrix with the zi in its rows and is the frobenius norm of matrix a defined by f trat a ij the optimal solution is obtained by setting w vl where vl contains the l eigenvectors with largest eigenvalues of the empirical covariance matrix i assume the n xi have zero mean for notational simplicity. furthermore the optimal low-dimensional encoding of the data is given by zi wt xi which is an orthogonal projection of the data onto the column space spanned by the eigenvectors. xixt an example of this is shown in figure for d and l the diagonal line is the vector this is called the first principal component or principal direction. the data points xi r are orthogonally projected onto this line to get zi r. this is the best approximation to the data. will discuss figure later. in general it is hard to visualize higher dimensional data but if the data happens to be a set of images it is easy to do so. figure shows the first three principal vectors reshaped as images as well as the reconstruction of a specific image using a varying number of basis vectors. discuss how to choose l in section below we will show that the principal directions are the ones along which the data shows maximal variance. this means that pca can be misled by directions in which the variance is high merely because of the measurement scale. figure shows an example where the vertical axis uses a large range than the horizontal axis resulting in a line that it is therefore standard practice to standardize the data first or looks somewhat unnatural principal components analysis mean principal basis reconstructed with bases reconstructed with bases principal basis principal basis reconstructed with bases reconstructed with bases figure the mean and the first three pc basis vectors based on images of the digit the mnist dataset. reconstruction of an image based on and all the basis vectors. figure generated by pcaimagedemo. t i h g e w height t i h g e w height figure effect of standardization on pca applied to the height weight dataset. left pca of raw data. right pca of standardized data. figure generated by pcademoheightweight. equivalently to work with correlation matrices instead of covariance matrices. the benefits of this are apparent from figure proof proof. we use wj r high-dimensional observation zi r zj r vectors. points r d to denote the j th principal direction xi r d to denote the i th l to denote the i th low-dimensional representation and n to denote the zn j which is the j th component of all the low-dimensional let us start by estimating the best solution r d and the corresponding projected n we will find the remaining bases etc. later. the reconstruction error is given by n n n chapter latent linear models n i xi xi i xi xi the orthonormality assumption. taking derivatives wrt and equating since wt to zero gives n xi wt xi so the optimal reconstruction weights are obtained by orthogonally projecting the data onto the first principal direction figure plugging back in gives n i xi const n now the variance of the projected coordinates is given by n xt i e t var e since e e because the data has been centered. from this we see that minimizing the reconstruction error is equivalent to maximizing the variance of the projected data i.e. arg min arg max var this is why it is often said that pca finds the directions of maximal variance. this is called the analysis view of pca. the variance of the projected data can be written as wt xixt i wt i xixt i is the empirical covariance matrix correlation matrix if the n n where n data is standardized. principal components analysis we can trivially maximize the variance of the projection hence minimize the reconstruction error by letting so we impose the constraint and instead maximize wt where is the lagrange multiplier. taking derivatives and equating to zero we have hence the direction that maximizes the variance is an eigenvector of the covariance matrix. left multiplying by using wt we find that the variance of the projected data is wt since we want to maximize the variance we pick the eigenvector which corresponds to the largest eigenvalue. now let us find another direction to further minimize the reconstruction error subject to and wt the error is wt n optimizing wrt and gives the same solution as before. exercise asks you to show that j in other words the second principal encoding is gotten by projecting onto the second principal direction. substituting in yields yields wt xi. n i xi wt xixt i wt xixt i const wt dropping the constant term and adding the constraints yields wt exercise asks you to show that the solution is given by the eigenvector with the second largest eigenvalue the proof continues in this way. one can use induction. chapter latent linear models singular value decomposition we have defined the solution to pca in terms of eigenvectors of the covariance matrix. however there is another way to obtain the solution based on the singular value decomposition or svd. this basically generalizes the notion of eigenvectors from square matrices to any kind of matrix. in particular any n d matrix x can be decomposed as follows n d n n n d d d where u is an n n matrix whose columns are orthornormal ut u in v is d d matrix whose rows and columns are orthonormal vt v vvt id and s is a n d matrix containing the r minn d singular values i on the main diagonal with filling the rest of the matrix. the columns of u are the left singular vectors and the columns of v are the right singular vectors. see figure for an example. since there are at most d singular values n d the last n d columns of u are irrelevant since they will be multiplied by the economy sized svd orthin svd avoids computing these unnecessary elements. let us denote this decomposition by u s v. if n d we have n d n d d d d d as in figure if n d we have n d n n n n n d computing the economy-sized svd takes on d minn d time and van loan the connection between eigenvectors and singular vectors is the following. for an arbitrary real matrix x ifx usvt we have xt x vst ut usvt vst svt vdvt where d is a diagonal matrix containing the squares singular values. hence xv vd so the eigenvectors of xt x are equal to v the right singular vectors of x and the eigenvalues of xt x are equal to d the squared singular values. similarly xxt usvt vst ut usst usst ud so the eigenvectors of xxt are equal to u the left singular vectors of x. also the eigenvalues of xxt are equal to the squared singular values. we can summarize all this as follows u evecxxt v evecxt x evalxxt evalxt x principal components analysis d d n d n x u d d s d d v t n d x l l l d l u l sl v t l figure svd decomposition of non-square matrices x usvt the shaded parts of s and all the off-diagonal terms are zero. the shaded entries in u and s are not computed in the economy-sized version since they are not needed. truncated svd approximation of rank l. since the eigenvectors are unaffected by linear scaling of a matrix we see that the right singular vectors of x are equal to the eigenvectors of the empirical covariance furthermore the eigenvalues of are a scaled version of the squared singular values. this means we can perform pca using just a few lines of code pcapmtk. however the connection between pca and svd goes deeper. from equation we can represent a rank r matrix as follows vt x vt r ur r if the singular values die off quickly as in figure we can produce a rank l approximation to the matrix as follows x vt this is called a truncated svd figure the total number of parameters needed to represent an n d matrix using a rank l approximation is n l ld l ln d chapter latent linear models rank rank rank rank figure low rank approximations to an image. top left the original image is of size so has rank subsequent images have ranks and figure generated by svdimagedemo. original randomized i g o l i figure first log singular values for the clown image red line and for a data matrix obtained by randomly shuffling the pixels green line. figure generated by svdimagedemo. principal components analysis as an example consider the pixel image in figure left. this has numbers in it. we see that a rank approximation with only numbers is a very good approximation. one can show that the error in this approximation is given by xlf furthermore one can show that the svd offers the best rank l approximation to a matrix in the sense of minimizing the above frobenius norm. let us connect this back to pca. let x usvt be a truncated svd of x. we know that w v and that z x w so z usvt v us furthermore the optimal reconstruction is given by x z wt so we find x usvt this is precisely the same as a truncated svd approximation! this is another illustration of the fact that pca is the best low rank approximation to the data. probabilistic pca we are now ready to revisit ppca. one can show the following remarkable result. theorem and bishop consider a factor analysis model in which and w is orthogonal. the observed data log likelihood is given by i c n xt log pxw n lnc trc where c wwt and s n data for notational simplicity. the maxima of the log-likelihood are given by i x. are assuming centered lnc xixt w v r where r is an arbitrary l l orthogonal matrix v is the d l matrix whose columns are the first l eigenvectors of s and is the corresponding diagonal matrix of eigenvalues. without loss of generality we can set r i. furthermore the mle of the noise variance is given by j d l which is the average variance associated with the discarded dimensions. thus as we have w v as in classical pca. what about z? it is easy to see that the posterior over the latent factors is given by pzixi f wt xi f f wt w chapter latent linear models not confuse f wt w with c wwt hence as we find w v f i and zi vt xi. thus the posterior mean is obtained by an orthogonal projection of the data onto the column space of v as in classical pca. note however that if the posterior mean is not an orthogonal projection since it is shrunk somewhat towards the prior mean as illustrated in figure this sounds like an undesirable property but it means that the reconstructions will be closer to the overall data mean x. em algorithm for pca although the usual way to fit a pca model uses eigenvector methods or the svd we can also use em which will turn out to have some advantages that we discuss below. em for pca relies on the probabilistic formulation of pca. however the algorithm continues to work in the zero noise limit as shown by let z be a l n matrix storing the posterior means representations let x xt store the original data along its columns. from along its columns. similarly equation when we have z w x this constitutes the e step. notice that this is just an orthogonal projection of the data. from equation the m step is given by w xie t e e t i i where we exploited the fact that cov when it is worth comparing this expression to the mle for multi-output linear regression which has the form thus we see that the m step is like linear regression where we w replace the observed inputs by the expected values of the latent variables. i xixt i i yixt i in summary here is the entire algorithm e step z w m stepw x zt z zt x and bishop showed that the only stable fixed point of the em algorithm is the globally optimal solution. that is the em algorithm converges to a solution where w spans the same linear subspace as that defined by the first l eigenvectors. however if we want w to be orthogonal and to contain the eigenvectors in descending order of eigenvalue we have to orthogonalize the resulting matrix can be done quite cheaply. alternatively we can modify em to give the principal basis directly and oh this algorithm has a simple physical analogy in the case d and l attached by springs to a rigid rod whose orientation is defined by a consider some points in r vector w. let zi be the location where the i th spring attaches to the rod. in the e step we hold the rod fixed and let the attachment points slide around so as to minimize the spring energy is proportional to the sum of squared residuals. in the m step we hold the attachment principal components analysis e step m step e step m step illustration of em for pca when d and l green stars are the original data points figure black circles are their reconstructions. the weight vector w is represented by blue line. we start with a random initial guess of w. the e step is represented by the orthogonal projections. we update the rod w in the m step keeping the projections onto the rod circles fixed. another e step. the black circles can slide along the rod but the rod stays fixed. another m step. based on figure of figure generated by pcaemstepbystep. points fixed and let the rod rotate so as to minimize the spring energy. see figure for an illustration. apart from this pleasing intuitive interpretation em for pca has the following advantages over eigenvector methods em can be faster. in particular assuming n d l the dominant cost of em is the projection operation in the e step so the overall time is ot ln d where t is the number of chapter latent linear models figure illustration of estimating the effective dimensionalities in a mixture of factor analysers using vbem. the blank columns have been forced to via the ard mechanism. the data was generated from clusters with intrinsic dimensionalities of which the method has successfully estimated. source figure of used with kind permission of matt beal. iterations. showed experimentally that the number of iterations is usually very small mean was regardless of n or d. results depends on the ratio of eigenvalues of the empirical covariance matrix. this is much faster than the ominn dn time required by straightforward eigenvector methods although more sophisticated eigenvector methods such as the lanczos algorithm have running times comparable to em. em can be implemented in an online fashion i.e. we can update our estimate of w as the data streams in. em can handle missing data in a simple way section em can be extended to handle mixtures of ppca fa models. em can be modified to variational em or to variational bayes em to fit more complex models. choosing the number of latent dimensions in section we discussed how to choose the number of components k in a mixture model. in this section we discuss how to choose the number of latent dimensions l in a fapca model. model selection for fappca argmaxl pld. however if we use a probabilistic model we can in principle compute l there are two problems with this. first evaluating the marginal likelihood for lvms is quite difficult. lower bounds section can be used also alternatively we can use the cross-validated likelihood as a performance measure although this can be slow since it requires fitting each model f times where f is the number of cv folds. in practice simple approximations such as bic or variational the second issue is the need to search over a potentially large number of models. the usual approach is to perform exhaustive search over all candidate values of l. however sometimes we can set the model to its maximal size and then use a technique called automatic relevancy determination combined with em to automatically prune out irrelevant weights. choosing the number of latent dimensions number of points per cluster intrinsic dimensionalities figure we show the estimated number of clusters and their estimated dimensionalities as a function of sample size. the vbem algorithm found two different solutions when n note that more clusters with larger effective dimensionalities are discovered as the sample sizes increases. source table of used with kind permission of matt beal. this technique will be described in a supervised context in chapter but can be adapted to the context as shown in ghahramani and beal figure illustrates this approach applied to a mixture of fas fit to a small synthetic dataset. the figures visualize the weight matrices for each cluster using hinton diagrams where where the size of the square is proportional to the value of the entry in the we see that many of them are sparse. figure shows that the degree of sparsity depends on the amount of training data in accord with the bayesian occam s razor. in particular when the sample size is small the method automatically prefers simpler models but as the sample size gets sufficiently large the method converges on the correct solution which is one with subspaces of dimensionality and although the ard em method is elegant it still needs to perform search over k. this is done using birth and death moves and beal an alternative approach is to perform stochastic sampling in the space of models. traditional approaches such as and west are based on reversible jump mcmc and also use birth and death moves. however this can be slow and difficult to implement. more recent approaches use non-parametric priors combined with gibbs sampling see e.g. and carin model selection for pca since pca is not a probabilistic model we cannot use any of the methods described above. an obvious proxy for the likelihood is the reconstruction error ed l i d in the case of pca the reconstruction is given by by xi wzi where zi wt and w and are estimated from dtrain. geoff hinton is an english professor of computer science at the university of toronto. chapter latent linear models train set reconstruction error test set reconstruction error e s m r num pcs e s m r num pcs figure reconstruction error on mnist vs number of latent dimensions used by pca. training set. test set. figure generated by pcaoverfitdemo. figure plots edtrain l vs l on the mnist training data in figure we see that it drops off quite quickly indicating that we can capture most of the empirical correlation of the pixels with a small number of factors as illustrated qualitatively in figure exercise asks you to prove that the residual error from only using l terms is given by the sum of the discarded eigenvalues j therefore an alternative to plotting the error is to plot the retained eigenvalues in decreasing order. this is called a scree plot because the plot looks like the side of a mountain and scree refers to the debris fallen from a mountain and lying at its base this will have the same shape as the residual error plot. a related quantity is the fraction of variance explained defined as edtrain l f l j this captures the same information as the scree plot. of course if we use l rankx we get zero reconstruction error on the training set. to avoid overfitting it is natural to plot reconstruction error on the test set. this is shown in figure here we see that the error continues to go down even as the model becomes more complex! thus we do not get the usual u-shaped curve that we typically expect to see. what is going on? the problem is that pca is not a proper generative model of the data. if you give it more latent dimensions it will be able to it is merely a compression technique. approximate the test data more accurately. by contrast a probabilistic model enjoys a bayesian occam s razor effect in that it gets punished if it wastes probability mass on parts of the space where there is little data. this is illustrated in figure which plots the quotation from choosing the number of latent dimensions x train set negative loglik k i l l g o g e n num pcs x test set negative loglik k i l l g o g e n num pcs figure negative log likelihood on mnist vs number of latent dimensions used by ppca. training set. test set. figure generated by pcaoverfitdemo. negative log likelihood computed using ppca vs l. here on the test set we see the usual u-shaped curve. these results are analogous to those in section where we discussed the issue of choosing k in the k-means algorithm vs using a gmm. profile likelihood although there is no u-shape there is sometimes a regime change in the plots from relatively large errors to relatively small. one way to automate the detection of this is described in and ghodsi the idea is this. let k be some measure of the error incurred by a model of size k such that lmax. in pca these are the eigenvalues but the method can also be applied to k-means. now consider partitioning these values into two groups depending on whether k l or k l where l is some threshold which we will determine. to measure the quality of l we will use a simple change-point model where k n if k l and k n if k l. is important that be the same in both models to prevent overfitting in the case where one regime has less data than the other. within each of the two regimes we assume the k are iid which is obviously incorrect but is adequate for our present purposes. we can fit this model for each l lmax by partitioning the data and computing the mles using a pooled estimate of the variance k l k l k l k kl k n l n kl k we can then evaluate the profile log likelihood log n k log n k finally we choose l arg max this is illustrated in figure on the left we plot the scree plot which has the same shape as in figure on the right we plot the profile chapter latent linear models l e u a v n e g e i x scree plot num pcs d o o h i l e k i l g o l e l i f o r p num pcs figure scree plot for training set corresponding to figure generated by pcaoverfitdemo. profile likelihood. figure likelihood. rather miraculously we see a fairly well-determined peak. pca for categorical data in this section we consider extending the factor analysis model to the case where the observed data is categorical rather than real-valued. that is the data has the form yij c where j is the number of observed response variables. we assume each yij is generated from a latent variable zi r l with a gaussian prior which is passed through the softmax function as follows pzi i pyizi catyirswt r zi l m is the factor loading matrix for response j and r where wr r m is the offset term for response r and need an explicit offset term since clamping one element of zi to can cause problems when computing the posterior covariance. as in factor analysis we have defined the prior mean to be and the prior covariance i since we can capture non-zero mean by changing and non-identity covariance by changing wr. we will call this categorical pca. see chapter for a discussion of related models. it is interesting to study what kinds of distributions we can induce on the observed variables by varying the parameters. for simplicity we assume there is a single ternary response variable so yi lives in the probability simplex. figure shows what happens when we vary the parameters of the prior and which is equivalent to varying the parameters of the likelihood and we see that this can define fairly complex distributions over the simplex. this induced distribution is known as the logistic normal distribution we can fit this model to data using a modified version of em. the basic idea is to infer a gaussian approximation to the posterior pziyi in the e step and then to maximize in the m step. the details for the multiclass case can be found in et al. pca for categorical data some examples of the logistic normal distribution defined on the simplex. figure diagonal covariance and non-zero mean. positive correlation between states and source figure of and lafferty used with kind permission of david blei. negative correlation between states and figure left synthetic dimensional bit vectors. right the embedding learned by binary pca using variational em. we have color coded points by the identity of the true prototype that generated them. figure generated by binaryfademotipping. also section the details for the binary case for the the sigmoid link can be found in exercise and for the probit link in exercise one application of such a model is to visualize high dimensional categorical data. figure shows a simple example where we have bit vectors. it is clear that each sample is just a noisy copy of one of three binary prototypes. we fit a catfa to this model yielding approximate mles in figure we plot e we see that there are three distinct clusters as is to be expected. zixi in et al. we show that this model outperforms finite mixture models on the task of imputing missing entries in design matrices consisting of real and categorical data. this is useful for analysing social science survey data which often has missing data and variables of mixed type. chapter latent linear models wy w x w x w y zi zx i zs i yi xi n bx yi n xi w x w y zx i zs i zy i bx xi yi n by figure gaussian latent factor models for paired data. canonical correlation analysis. supervised pca. partial least squares. pca for paired and multi-view data it is common to have a pair of related datasets e.g. gene expression and gene copy number or movie ratings by users and movie reviews. it is natural to want to combine these together into a low-dimensional embedding. this is an example of data fusion. in some cases we might want to predict one element of the pair say from the other one via the low-dimensional bottleneck below we discuss various latent gaussian models for these tasks following the presentation of the models easily generalize from pairs to sets of data xim for m we focus on the case where xim r in this case the joint distribution is multivariate gaussian so we can easily fit the models using em or gibbs sampling. dm. we can generalize the models to handle discrete and count data by using the exponential family as a response distribution instead of the gaussian as we explain in section however this will require the use of approximate inference in the e step an analogous modification to mcmc. pca for paired and multi-view data supervised pca factor regression consider the following model illustrated in figure pzi il pyizi y zi y y pxizi x xid in et al. this is called supervised pca. in this is called bayesian factor regression. this model is like pca except that the target variable yi is taken into account when learning the low dimensional embedding. since the model is jointly gaussian we have yixi n i w y wt y cwy x so although this is a g j contains where w joint density model of xi we can infer the implied conditional distribution. xid and c i wt we now show an interesting connection to zellner s g-prior. suppose pwy n and let x rvt be the svd of x where vt v i and rt r diag the squared singular values. then one can show that pw n gv t n gxt x so the dependence of the prior for w on x arises from the fact that w is derived indirectly by a joint model of x and y. the above discussion focussed on regression. generalizes cca to the exponential family which is more appropriate if xi andor yi are discrete. although we can no longer compute the conditional pyixi in closed form the model has a similar interpretation to the regression case namely that we are predicting the response via a latent bottleneck in particular we might want to find an encoding distribution pzx such that we minimize the basic idea of compressing xi to predict yi can be formulated using information theory. i z i y where is some parameter controlling the tradeoff between compression and predictive accuracy. this is known as the information bottleneck et al. often z is taken to be discrete as in clustering. however in the gaussian case ib is closely related to cca et al. we can easily generalize cca to the case where yi is a vector of responses to be predicted as in multi-label classification. et al. williamson and ghahramani used this model to perform collaborative filtering where the goal is to predict yij the rating person i gives to movie j where the side information xi takes the form of a list of i s friends. the intuition behind this approach is that knowledge of who your friends are as well as the ratings of all other users should help predict which movies you will like. in general any setting where the tasks are correlated could benefit from cca. once we adopt a probabilistic view various extensions are straightforward. for example we can easily generalize to the semi-supervised case where we do not observe yi for all i et al. chapter latent linear models discriminative supervised pca one problem with this model is that it puts as much weight on predicting the inputs xi as the outputs yi. this can be partially alleviated by using a weighted objective of the following form et al. pyi iy y pxi ix x where the m control the relative importance of the data sources and im wmzi. for gaussian data we can see that m just controls the noise variance i xxt yyt i exp exp i i this interpretation holds more generally for the exponential family. note however that it is hard to estimate the m parameters because changing them changes the normalization constant of the likelihood. we give an alternative approach to weighting y more heavily below. partial least squares the technique of partial least squares sun et al. is an asymmetric or more discriminative form of supervised pca. the key idea is to allow some of the in the input features to be explained by its own subspace zx i and to let the rest of the subspace zs i be shared between input and output. the model has the form pzi ils i ilx pyizi pxizi i y i bxzx i x see figure the corresponding induced distribution on the visible variables has the form pvi n idzi n wwt where vi yi y x and wy wx bx wywt y wxwt x wxwt wxwt x x bxbt x w wwt we should choose l large enough so that the shared subspace does not capture covariatespecific variation. this model can be easily generalized to discrete data using the exponential family independent component analysis canonical correlation analysis canonical correlation analysis or cca is like a symmetric unsupervised version of pls it allows each view to have its own private subspace but there is also a shared subspace. if we have two observed variables xi and yi then we have three latent variables zs which is i r shared zx ly which are private. we can write the model as follows and jordan lx and zy i r i r pzi pxizi pyizi ils i wxzs i wyzs i ilx i ily i x i y see figure the corresponding observed joint distribution has the form pvi where w wwt n idzi n wwt wx bx wy wxwt by x bxbt x wxwt y wywt y wywt y bybt y one can compute the mle for this model using em. and jordan show that the resulting mle is equivalent to rotation and scaling to the classical non-probabilistic view. however the advantages of the probabilistic view are many we can trivially generalize to m observed variables we can create mixtures of cca et al. we can create sparse versions of cca using ard and bach we can generalize to the exponential family et al. we can perform bayesian inference of the parameters klami and kaski we can handle non-parametric sparsity-promoting priors for w and b and daume and so on. independent component analysis consider the following situation. you are in a crowded room and many people are speaking. your ears essentially act as two microphones which are listening to a linear combination of the different speech signals in the room. your goal is to deconvolve the mixed signals into their constituent parts. this is known as the cocktail party problem and is an example of blind signal separation or blind source separation where blind means we know nothing about the source of the signals. besides the obvious applications to acoustic signal processing this problem also arises when analysing eeg and meg signals financial data and any other dataset necessarily temporal where latent sources or factors get mixed together in a linear way. at time t and zt r we can formalize the problem as follows. let xt r l be the vector of source signals. we assume that d be the observed signal at the sensors xt wzt chapter latent linear models truth observed signals pca estimate ica estimate figure illustration of ica applied to iid samples of a source signal. observations. pca estimate. ica estimate. figure generated by icademo written by aapo hyvarinen. latent signals. where w is an d l matrix and n in this section we treat each time point as an independent observation i.e. we do not model temporal correlation we could replace the t index with i but we stick with t to be consistent with much of the ica literature. the goal is to infer the source signals pztxt as illustrated in figure in this context w is called the mixing matrix. if l d of sources number of sensors it will be a square matrix. often we will assume the noise level is zero for simplicity. so far the model is identical to factor analysis pca if there is no noise except we don t in general require orthogonality of w. however we will use a different prior for pzt. in pca we assume each source is independent and has a gaussian distribution pzt n we will now relax this gaussian assumption and let the source distributions be any non-gaussian independent component analysis uniform data uniform data after linear mixing pca applied to mixed data from uniform source ica applied to mixed data from uniform source figure distribution. icademouniform written by aapo hyvarinen. latent signals. observations. illustration of ica and pca applied to iid samples of a source signal with a uniform ica estimate. figure generated by pca estimate. distribution pzt pjztj without loss of generality we can constrain the variance of the source distributions to be because any other variance can be modelled by scaling the rows of w appropriately. the resulting model is known as independent component analysis or ica. the reason the gaussian distribution is disallowed as a source prior in ica is that it does not permit unique recovery of the sources as illustrated in figure this is because the pca likelihood is invariant to any orthogonal transformation of the sources zt and mixing matrix w. pca can recover the best linear subspace in which the signals lie but cannot uniquely recover the signals themselves. chapter latent linear models to illustrate this suppose we have two independent sources with uniform distributions as shown in figure now suppose we have the following mixing matrix w then we observe the data shown in figure no noise. if we apply pca followed by scaling to this we get the result in figure this corresponds to a whitening of the data. to uniquely recover the sources we need to perform an additional rotation. the trouble is there is no information in the symmetric gaussian posterior to tell us which angle to rotate by. in a sense pca solves half of the problem since it identifies the linear subspace all that ica has to do is then to identify the appropriate rotation. we see that ica is not that different from methods such as varimax which seek good rotations of the latent factors to enhance interpretability. ica requires that w is square and hence invertible. figure shows that ica can recover the source up to a permutation of the indices and possible sign change. in the non-square case where we have more sources than sensors we cannot uniquely recover the true signal but we can compute the posterior pztxt w which represents our beliefs about the source. in both cases we need to estimate w as well as the source distributions pj. we discuss how to do this below. maximum likelihood estimation in this section we discuss ways to estimate square mixing matrices w for the noise-free ica model. as usual we will assume that the observations have been centered hence we can also assume z is zero-mean. in addition we assume the observations have been whitened which can be done with pca. if the data is centered and whitened we have e xxt i. but in the noise free case we also have cov e xxt we zzt wt wwt hence we see that w must be orthogonal. this reduces the number of parameters we have to estimate from to dd it will also simplify the math and the algorithms. let v w these are often called the recognition weights as opposed to w which are the generative since x wz we have from equation pxwzt pzzt detw pzvxt detv hence we can write the log-likelihood assuming t iid samples as follows t log pdv log detv t log pjvt j xt in the literature it is common to denote the generative weights by a and the recognition weights by w but we are trying to be consistent with the notation used earlier in this chapter. independent component analysis where vj is the j th row of v. since we are constraining v to be orthogonal the first term is a constant so we can drop it. we can also replace the average over the data with an expectation operator to get the following objective nllv e j x and gjz log pjz. we want to minimize this subject to the constraint where zj vt that the rows of v are orthogonal. we also want them to be unit norm since this ensures which is that the variance of the factors is unity with whitened data e necessary to fix the scale of the weights. in otherwords v should be an orthonormal matrix. it is straightforward to derive a gradient descent algorithm to fit this model however it is rather slow. one can also derive a faster algorithm that follows the natural gradient see e.g. ch for details. a popular alternative is to use an approximate newton method which we discuss in section another approach is to use em which we discuss in section vt j x the fastica algorithm we now describe the fast ica algorithm based on and oja which we will show is an approximate newton method for fitting ica models. for simplicity of presentation we initially assume there is only one latent factor. in addition we initially assume all source distributions are known and are the same so we can just write gz log pz. let gz d dz gz. the constrained objective and its gradient and hessian are given by vt v v i where is a lagrange multiplier. let us make the approximation f f hv e x e x this makes the hessian very easy to invert giving rise to the following newton update gvt x xgvt x xxt x e x v xgvt x e x xxt xxt e v v e one can rewrite this in the following way v e xgvt x x v e practice the expectations can be replaced by monte carlo estimates from the training set which gives an efficient online learning algorithm. after performing this update one should project back onto the constraint surface using vnew v chapter latent linear models gaussian laplace uniform figure illustration of gaussian sub-gaussian and super-gaussian distributions in and figure generated by subsupergaussplot written by kevin swersky. to the sign ambiguity of v the values of v one iterates this algorithm until convergence. may not converge but the direction defined by this vector should converge so one can assess convergence by monitoring vnew which should approach since the objective is not convex there are multiple local optima. we can use this fact to learn multiple different weight vectors or features. we can either learn the features sequentially and then project out the part of vj that lies in the subspace defined by earlier features or we can learn them in parallel and orthogonalize v in parallel. this latter approach is usually preferred since unlike pca the features are not ordered in any way. so the first feature is not more important than the second and hence it is better to treat them symmetrically. independent component analysis modeling the source densities so far we have assumed that gz log pz is known. what kinds of models might be reasonable as signal priors? we know that using gaussians correspond to quadratic functions for g won t work. so we want some kind of non-gaussian distribution. in general there are several kinds of non-gaussian distributions such as the following super-gaussian distributions these are distributions which have a big spike at the mean and hence order to ensure unit variance have heavy tails. the laplace distribution is a classic example. see figure formally we say a distribution is super-gaussian or leptokurtic lepto coming from the greek for thin if kurtz where kurtz is the kurtosis of the distribution defined by kurtz where is the standard deviation and k is the k th central moment or moment about the mean k e is the mean and is the variance. it is conventional to subtract in the definition of kurtosis to make the kurtosis of a gaussian variable equal to zero. e sub-gaussian distributions a sub-gaussian or platykurtic platy coming from the greek for broad distribution has negative kurtosis. these are distributions which are much flatter than a gaussian. the uniform distribution is a classic example. see figure skewed distributions another way to be non-gaussian is to be asymmetric. one measure of this is skewness defined by skewz an example of a skewed distribution is the gamma distribution figure when one looks at the empirical distribution of many natural signals such as images and speech when passed through certain linear filters they tend to be very super-gaussian. this result holds both for the kind of linear filters found in certain parts of the brain such as the simple cells found in the primary visual cortex as well as for the kinds of linear filters used in signal processing such as wavelet transforms. one obvious choice for modeling natural signals with ica is therefore the laplace distribution. for mean zero and variance this has a log pdf given by log pz log since the laplace prior is not differentiable at the origin it is more common to use other smoother super-gaussian distributions. one example is the logistic distribution. the corresponding log pdf for the case where the mean is zero and the variance is and s is given by the following log pz log cosh z log chapter latent linear models dk dk qtd ztd xtd t w figure modeling the source distributions using a mixture of univariate gaussians independent factor analysis model of et al. attias various ways of estimating gz log pz are discussed in the seminal paper and garrat however when fitting ica by maximum likelihood it is not critical that the exact shape of the source distribution be known it is important to know whether it is sub z or gz log coshz or super gaussian. consequently it is common to just use gz instead of the more complex expressions above. using em an alternative to assuming a particular form for gz or equivalently for pz is to use a flexible non-parametric density estimator such as a mixture of gaussians pqj k k pzjqj k jk jk pxz this approach was proposed in et al. attias and the corresponding graphical model is shown in figure it is possible to derive an exact em algorithm for this model. the key observation is that it is possible to compute e exactly by summing over all k l combinations of the qt variables where k is the number of mixture components per source. this is too expensive one can use a variational mean field approximation we can then estimate all the source distributions in parallel by fitting a standard gmm to e when the source gmms are independent component analysis known we can compute the marginals pjzj very easily using pjzj jkn jk jk given the pj s we can then use an ica algorithm to estimate w. of course these steps should be interleaved. the details can be found in other estimation principles it is quite common to estimate the parameters of ica models using methods that seem different to maximum likelihood. we will review some of these methods below because they give additional insight into ica. however we will also see that these methods in fact are equivalent to maximum likelihood after all. our presentation is based on and oja maximizing non-gaussianity an early approach to ica was to find a matrix v such that the distribution z vx is as far from gaussian as possible. is a related approach in statistics called projection pursuit. one measure of non-gaussianity is kurtosis but this can be sensitive to outliers. another measure is the negentropy defined as negentropyz h h where e and var since the gaussian is the maximum entropy distribution this measure is always non-negative and becomes large for distributions that are highly nongaussian. we can define our objective as maximizing jv negentropyzj h j j j h j where z vx. will be i independently of v so the first term is a constant. hence if we fix v to be orthogonal and if we whiten the data the covariance of z jv h const e pzj const j j which we see is equal to a sign change and irrelevant constants to the log-likelihood in equation minimizing mutual information one measure of dependence of a set of random variables is the multi-information pz pzj j j iz kl hzj hz chapter latent linear models we would like to minimize this since we are trying to find independent components. put another way we want the best possible factored approximation to the joint distribution. now since z vx we have hzj hvx iz j if we constrain v to be orthogonal we can drop the last term since then hvx hx multiplying by v does not change the shape of the distribution and hx is a constant which is is solely determined by the empirical distribution. hence we have iz j hzj. minimizing this is equivalent to maximizing the negentropy which is equivalent to maximum likelihood. maximizing mutual information instead of trying to minimize the mutual information between the components of z let us imagine a neural network where x is the input and yj j x is the noisy output where is some nonlinear scalar function and n it seems reasonable to try to maximize the information flow through this system a principle known as infomax. and sejnowski that is we want to maximize the mutual information between y internal neural representation and x observed input signal. we have ix y hy hyx where the latter term is constant if we assume the noise has constant variance. one can show that we can approximate the former term as follows log j x log detv e hy where as usual we can drop the last term if v is orthogonal. if we define to be a cdf then is its pdf and the above expression is equivalent to the log likelihood. in particular if we use a logistic nonlinearity sigmz then the corresponding pdf is the logistic distribution and log log coshz irrelevant constants. thus we see that infomax is equivalent to maximum likelihood. exercises exercise m step for fa for the fa model show that the mle in the m step for w is given by equation exercise map estimation for the fa model derive the m step for the fa model using conjugate priors for the parameters. let the empirical covariance matrix have eigenvalues i is a good measure of whether exercise heuristic for assessing applicability of pca d explain why the variance of the evalues d or not pca would be useful for analysing the data higher the value of the more useful pca. independent component analysis exercise deriving the second principal component a. let n show that j yields vt xi. b. show that the value of that minimizes vt is given by the eigenvector of c with the second largest eigenvalue. hint recall that and xt ax x at exercise deriving the residual error for pca a. prove that i xi xt vt j xixt i vj j vj and vt j vk for k j. also hint first consider the case k use the fact that vt recall zij xt b. now show that i vj. jk n vt j xixt i vj n i xi xt i xi xt j j cvj jvt hint recall vt if k d there is no truncation so jd use this to show that the error from only using k d terms is given by j vj j. c. jk j hint partition the sum j into j and j. exercise derivation of fisher s linear discriminant show that the maximum of jw wt sb w where wt sb w wt sw w where f and gx. also recall that d dx d dx d dx wt sw w is given by sbw sw w hint recall that the derivative of a ratio of two scalars is given by d dx xt ax at f gx f exercise pca via successive deflation let k be the first k eigenvectors with largest eigenvalues of c n basis vectors. these satisfy if j k if j k j vk vt xt x i.e. the principal we will construct a method for finding the vj sequentially. chapter latent linear models as we showed in class is the first principal eigenvector of c and satisfies now define xi as the orthogonal projection of xi onto the space orthogonal to xi p xi define x xn as the deflated matrix of rank d which is obtained by removing from the d dimensional data the component that lies in the direction of the first principal direction x x a. using the facts that xt n hence vt xt x n and vt show that the covariance of the deflated matrix is given by c n xt x n xt x b. let u be the principal eigenvector of c. explain why u may assume u is unit norm. c. suppose we have a simple method for finding the leading eigenvector and eigenvalue of a pd matrix denoted by u f write some pseudo code for finding the first k principal basis vectors of x that only uses the special f function and simple vector arithmetic i.e. your code should not use svd or theeig function. hint this should be a simple iterative routine that takes lines to write. the input is c k and the function f the output should be vj and j for j k. do not worry about being syntactically correct. exercise latent semantic indexing de freitas.. in this exercise we study a technique called latent semantic indexing which applies svd to a document by term matrix to create a low-dimensional embedding of the data that is designed to capture semantic similarity of words. the file lsidocuments.pdf contains documents on various topics. a list of all the unique wordsterms that occur in these documents is in lsiwords.txt. a document by term matrix is in lsimatrix.txt. a. let x be the transpose of lsimatrix so each column represents a document. compute the svd of x and make an approximation to it x using the first singular values vectors. plot the low dimensional representation of the documents in you should get something like figure b. consider finding documents that are about alien abductions. if if you look at lsiwords.txt there are versions of this word term abducted term abduction and term abductions suppose we want to find documents containing the word abducted documents and contain it but document does not. however document is clearly related to this topic. thus lsi should also find document create a test document q containing the one word abducted and project it into the subspace to make q. now compute the cosine similarity between q and the low dimensional representation of all the documents. what are the top closest matches? exercise imputation in a fa model derive an expression for pxhxv for a fa model. exercise efficiently evaluating the ppca density derive an expression for px w for the ppca model based on plugging in the mles and using the matrix inversion lemma. independent component analysis figure projection of documents into dimensions. figure generated by lsicode. exercise ppca vs fa exercise of et al. due to hinton.. generate observations from the following model where zi n i fit a fa and pca model with latent factor. hence show that the corresponding weight vector w aligns with the maximal variance direction in the pca case but with the maximal correlation direction in the case of fa. sparse linear models introduction we introduced the topic of feature selection in section where we discussed methods for finding input variables which had high mutual information with the output. the trouble with this approach is that it is based on a myopic strategy that only looks at one variable at a time. this can fail if there are interaction effects. for example if y then neither nor on its own can predict the response but together they perfectly predict the response. for a real-world example of this consider genetic association studies sometimes two genes on their own may be harmless but when present together they cause a recessive disease in this chapter we focus on selecting sets of variables at a time using a model-based approach. if the model is a generalized linear model of the form pyx yf x for some link function f then we can perform feature selection by encouraging the weight vector w to be sparse i.e. to have lots of zeros. this approach turns out to offer significant computational advantages as we will see below. here are some applications where feature selection sparsity is useful in many problems we have many more dimensions d than training cases n the corresponding design matrix is short and fat rather than tall and skinny. this is called the small n large d problem. this is becoming increasingly prevalent as we develop more high throughput measurement devices for example with gene microarrays it is common to measure the expression levels of d genes but to only get n such examples. is perhaps a sign of the times that even our data seems to be getting fatter... we may want to find the smallest set of features that can accurately predict the response growth rate of the cell in order to prevent overfitting to reduce the cost of building a diagnostic device or to help with scientific insight into the problem. in chapter we will use basis functions centered on the training examples so xn where is a kernel function. the resulting design matrix has size n n feature selection in this context is equivalent to selecting a subset of the training examples which can help reduce overfitting and computational cost. this is known as a sparse kernel machine. in terms of in signal processing it is common to represent signals speech etc. wavelet basis functions. to save time and space it is useful to find a sparse representation chapter sparse linear models of the signals in terms of a small number of such basis functions. this allows us to estimate signals from a small number of measurements as well as to compress the signal. see section for more information. note that the topic of feature selection and sparsity is currently one of the most active areas in this chapter we only have space to give an overview of the of machine learning statistics. main results. bayesian variable selection a natural way to pose the variable selection problem is as follows. let j if feature j is relevant and let j otherwise. our goal is to compute the posterior over models p e f e f where f is the cost function f pd log p for example suppose we generate n samples from a d dimensional linear regression model yi n xi in particular we use w and we enumerate all models and compute p for each one give the equations for this below. we order the models in gray code order which ensures consecutive vectors differ by exactly bit reasons for this are computational and are discussed in section in which k elements of w are non-zero. the resulting set of bit patterns is shown in figure the cost of each model f is shown in figure we see that this objective function is extremely bumpy the results are easier to interpret if we compute the posterior distribution over models p this is shown in figure the top models are listed below model prob members the true model is however the coefficients associated with features and are very small to so these variables are harder to detect. given enough data the method will converge on the true model the data is generated from a linear model but for finite data sets there will usually be considerable posterior uncertainty. interpreting the posterior over a large number of models is quite difficult so we will seek various summary statistics. a natural one is the posterior mode or map estimate argmax p argmin f bayesian variable selection pmodeldata log pmodel data pgammajdata figure all possible models. marginal inclusion probabilities. figure generated by linregallsubsetsgraycodedemo. all possible bit vectors of length enumerated in gray code order. score function for posterior over all models. vertical scale has been truncated at for clarity. however the mode is often not representative of the full posterior mass section a better summary is the median model and berger carvahlo and lawrence computed using p j this requires computing the posterior marginal inclusion probabilities p j these are shown in figure we see that the model is confident that variables and are included if we lower the decision threshold to we would add and as well. however if we wanted to capture variable we would incur two false positives and this tradeoff between false positives and false negatives is discussed in more detail in section the above example illustrates the gold standard for variable selection the problem was sufficiently small variables that we were able to compute the full posterior exactly. of course variable selection is most useful in the cases where the number of dimensions is large. since there are possible models vectors it will be impossible to compute the full posterior in general and even finding summaries such as the map estimate or marginal chapter sparse linear models inclusion probabilities will be intractable. we will therefore spend most of this chapter focussing on algorithmic speedups. but before we do that we will explain how we computed p in the above example. the spike and slab model the posterior is given by p p we first consider the prior then the likelihood. it is common to use the following prior on the bit vector p ber j where is the probability a feature is relevant and j is the pseudo-norm that is the number of non-zero elements of the vector. for comparison with later models it is useful to write the log prior as follows log p log const const where log we can write the likelihood as follows pd pyx controls the sparsity of the model. pyx w for notational simplicity we have assumed the response is centered y so we can ignore any offset term we now discuss the prior pw if j feature j is irrelevant so we expect wj if j we expect wj to be non-zero. if we standardize the inputs a reasonable prior is n w controls how big we expect the coefficients associated with the relevant variables to be is scaled by the overall noise level we can summarize this prior as follows w where n w if j if j w the distribution pwj j approaches the first term is a spike at the origin. as a uniform distribution which can be thought of as a slab of constant height. hence this is called the spike and slab model and beauchamp we can drop the coefficients wj for which wj from the model since they are clamped to zero under the prior. hence equation becomes the following a gaussian likelihood pwj j pd n w wid d bayesian variable selection where d is the number of non-zero elements in in what follows we will generalize this slightly by defining a prior of the form pw for any positive definite matrix given these priors we can now compute the marginal likelihood. if the noise variance is known we can write down the marginal likelihood equation as follows pd n w n c c xt if the noise is unknown we can put a prior on it and integrate it out. it is common to use p ig b some guidelines on setting a b can be found in et al. if we use a b we recover the jeffrey s prior p when we integrate out the noise we get the following more complicated expression for the marginal likelihood et al. pd py w d where s is the rss x s s yt y yt x x y see also exercise when the marginal likelihood cannot be computed in closed form if we are using logistic regression or a nonlinear model we can approximate it using bic which has the form log pd log pyx w where w is the ml or map estimate based on x and is the degrees of freedom of the model et al. adding the log prior the overall objective becomes log n log p log pyx w log n const we see that there are two complexity penalties one arising from the bic approximation to the marginal likelihood and the other arising from the prior on p obviously these can be combined into one overall complexity parameter which we will denote by from the bernoulli-gaussian model to regularization another model that is sometimes used and mallick zhou et al. soussen et al. is the following yixi w n jwjxij j j ber wj n w for reasons explained in section also it is common to use a g-prior of the form gxt exercise various approaches have been proposed for setting g including cross validation empirical bayes george and foster hierarchical bayes et al. etc. x chapter sparse linear models et al. this is called the bernoulliin the signal processing literature gaussian model although we could also call it the binary mask model since we can think of the j variables as masking out the weights wj. unlike the spike and slab model we do not integrate out the irrelevant coefficients they always exist. in addition the binary mask model has the form j y wj whereas the spike and slab model has the form j wj y. in the binary mask model only the product jwj can be identified from the likelihood. one interesting aspect of this model is that it can be used to derive an objective function that is widely used in the subset selection literature. first note that the joint prior has the form p w n wi hence the scaled unnormalized negative log posterior has the form f w log p w yx x const w where log let us split w into two subvectors w and w indexed by the zero and non-zero entries of respectively. since x w x w we can just set w w so we do not regularize the non-zero weights there is no complexity penalty coming from the marginal likelihood or its bic approximation. in this case the objective becomes now consider the case where f w x w this is similar to the bic objective above. instead of keeping track of the bit vector we can define the set of relevant variables to be the support or set of non-zero entries of w. then we can rewrite the above equation as follows f this is called regularization. we have converted the discrete optimization problem into a continuous one w r d however the pseudo-norm makes the objective very non smooth so this is still hard to optimize. we will discuss different solutions to this in the rest of this chapter. algorithms since there are models we cannot explore the full posterior or find the globally optimal model. instead we will have to resort to heuristics of one form or another. all of the methods we will discuss involve searching through the space of models and evaluating the cost f at bayesian variable selection all subsets on prostate cancer r o r r e t e s i g n n a r t i subset size figure a lattice of subsets of residual sum of squares versus subset size on the prostate cancer data set. the lower envelope is the best rss achievable for any set of a given size. based on figure of et al. figure generated by prostatesubsets. each point. this requires fitting the model computing argmax pdw or evaluating its pdwpwdw at each step. this is sometimes called marginal likelihood computing the wrapper method since we wrap our search for the best model set of good models around a generic model-fitting procedure. in order to make wrapper methods efficient it is important that we can quickly evaluate the score function for some new model given the score of a previous model this can be done provided we can efficiently update the sufficient statistics needed to compute f this is possible provided only differs from in one bit to adding or removing a single variable and provided f only depends on the data via x in this case we can use rank-one matrix updates downdates to efficiently compute xt x these updates are usually applied to the qr decomposition of x. see e.g. schniter et al. for details. x from xt greedy search suppose we want to find the map model. if we use the objective in equation we can exploit properties of least squares to derive various efficient greedy forwards search methods some of which we summarize below. for further details see soussen et al. single best replacement the simplest method is to use greedy hill climbing where at each step we define the neighborhood of the current model to be all models than can be reached by flipping a single bit of i.e. for each variable if it is currently out of the model we consider adding it and if it is currently in the model we consider removing it. in et al. they call this the single best replacement since we are expecting a sparse solution we can start with the empty set we are essentially moving through the lattice of subsets shown in figure we continue adding or removing until no improvement is possible. orthogonal least squares if we set in equation so there is no complexity penalty there will be no reason to perform deletion steps. in this case the sbr algorithm is equivalent to orthogonal least squares and wigger which in turn is equivalent chapter sparse linear models to greedy forwards selection. in this algorithm we start with the empty set and add the best feature at each step. the error will go down monotonically with as shown in figure we can pick the next best feature j to add to the current set t by solving j t min w arg min t we then update the active set by setting to choose the next feature to add at step t we need to solve d dt least squares problems at step t where dt t is the cardinality of the current active set. having chosen the best feature to add we need to solve an additional least squares problem to compute orthogonal matching pursuits orthogonal least squares is somewhat expensive. a simplification is to freeze the current weights at their current value and then to pick the next feature to add by solving j arg min t min xwt where rt this inner optimization is easy to solve we simply set xt y xwt is the current residual vector. if the columns are unit norm we have j arg max xt so we are just looking for the column that is most correlated with the current residual. we then update the active set and compute the new least squares estimate using x this method is called orthogonal matching pursuits or omp et al. this only requires one least squares calculation per iteration and so is faster than orthogonal least squares but is not quite as accurate and davies matching pursuits an even more aggressive approximation is to just greedily add the feature that is most correlated with the current residual. this is called matching pursuits and zhang this is also equivalent to a method known as least squares boosting backwards selection backwards selection starts with all variables in the model socalled saturated model and then deletes the worst one at each step. this is equivalent to performing a greedy search from the top of the lattice downwards. this can give better results than a bottom-up search since the decision about whether to keep a variable or not is made in the context of all the other variables that might depende on it. however this method is typically infeasible for large problems since the saturated model will be too expensive to fit. foba the forwards-backwards algorithm of is similar to the single best replacement algorithm presented above except it uses an omp-like approximation when choosing the next move to make. a similar dual-pass algorithm was described in et al. bayesian matching pursuit the algorithm of et al. is similiar to omp except it uses a bayesian marginal likelihood scoring criterion a spike and slab model instead of a least squares objective. in addition it uses a form of beam search to explore multiple paths through the lattice at once. regularization basics stochastic search if we want to approximate the posterior rather than just computing a mode because we want to compute marginal inclusion probabilities one option is to use mcmc. the standard approach is to use metropolis hastings where the proposal distribution just flips single bits. this enables us to efficiently compute p given p the probability of a state configuration is estimated by counting how many times the random walk visits this state. see hara and sillanpaa for a review of such methods and and richardson for a very recent method based on evolutionary mcmc. however in a discrete state space mcmc is needlessly inefficient since we can compute the probability of a state directly using p exp f thus there is no need to ever revisit a state. a much more efficient alternative is to use some kind of stochastic search algorithm to generate a set s of high scoring models and then to make the following approximation p e f s e f see and scott for a review of recent methods of this kind. em and variational inference it is tempting to apply em to the spike and slab model which has the form j wj y. we can compute p j in the e step and optimize w in the m step. however this will not work because when we compute p j we are comparing a delta-function with a gaussian pdf n w. we can replace the delta function with a narrow gaussian and then the e step amounts to classifying wj under the two possible gaussian models. however this is likely to suffer from severe local minima. an alternative is to apply em to the bernoulli-gaussian model which has the form j y wj. in this case the posterior p w is intractable to compute because all the bits become correlated due to explaining away. however it is possible to derive a mean field approximation of the form j q jqwj et al. rattray et al. regularization basics when we have many variables it is computationally difficult to find the posterior mode of p and although greedy algorithms often work well e.g. for a theoretical analysis they can of course get stuck in local optima. part of the problem is due to the fact that the j variables are discrete j in the optimization community it is common to relax hard constraints of this form by replacing discrete variables with continuous variables. we can do this by replacing the spike-and-slab style prior that assigns finite probability mass to the event that wj to continuous priors that encourage wj by putting a lot of probability density near the origin such as a zero-mean laplace distribution. this was first introduced in section in the context of robust linear regression. there we exploited the fact that the laplace has heavy tails. here we exploit the fact chapter sparse linear models figure illustration of vs regularization of a least squares problem. based on figure of et al. that it has a spike near more precisely consider a prior of the form pw e we will use a uniform prior on the offset term let us perform map estimation with this prior. the penalized negative log likelihood has the form f log pdw log pw nllw is the norm of w. for suitably large the estimate w will be where sparse for reasons we explain below. indeed this can be thought of as a convex approximation to the non-convex objective nllw argmin w in the case of linear regression the objective becomes f wt rssw where this method is known as basis pursuit denoising or bpdn et al. the reason for this term will become clear later. in general the technique of putting a zero-mean laplace prior on the parameters and performing map estimation is called regularization. it can be combined with any convex or non-convex nll term. many different algorithms have been devised for solving such problems some of which we review in section why does regularization yield sparse solutions? we now explain why regularization results in sparse solutions whereas regularization does not. we focus on the case of linear regression although similar arguments hold for logistic regression and other glms. regularization basics the objective is the following non-smooth objective function rssw min w we can rewrite this as a constrained but smooth objective quadratic function with linear constraints min w rssw s.t. b where b is an upper bound on the of the weights a small bound b corresponds to a large penalty and vice equation is known as lasso which stands for least absolute shrinkage and selection operator we will see why it has this name later. similarly we can write ridge regression rssw min w or as a bound constrained form rssw min s.t. b w in figure we plot the contours of the rss objective function as well as the contours of the and constraint surfaces. from the theory of constrained optimization we know that the optimal solution occurs at the point where the lowest level set of the objective function intersects the constraint surface the constraint is active. it should be geometrically clear that as we relax the constraint b we grow ball until it meets the objective the corners of the ball are more likely to intersect the ellipse than one of the sides especially in high dimensions because the corners stick out more. the corners correspond to sparse solutions which lie on the coordinate axes. by contrast when we grow the ball it can intersect the objective at any point there are no corners so there is no preference for sparsity. to see this another away notice that with ridge regression the prior cost of a sparse solution such as w is the same as the cost of a dense solution such as w as long as they have the same norm since however for lasso setting w is cheaper than setting w the most rigorous way to see that regularization results in sparse solutions is to examine conditions that hold at the optimum. we do this in section optimality conditions for lasso the lasso objective has the form f rss equation is an example of a quadratic program or qp since we have a quadratic objective subject to linear inequality constraints. its lagrangian is given by equation chapter sparse linear models c c fx cx x figure figure generated by subgradientplot. illustration of some sub-derivatives of a function at point based on a figure at http unfortunately the term is not differentiable whenever wj this is an example of a non-smooth optimization problem. to handle non-smooth functions we need to extend the notion of a derivative. we define a subderivative or subgradient of a function f i r at a point to be a scalar g such that f f g i where i is some interval containing see figure for an we define the set of subderivatives as the interval b where a and b are the one-sided limits f f a lim b lim f f the set b of all subderivatives is called the subdifferential of the function f at and is denoted f for example in the case of the absolute value function f the subderivative is given by if if if f if the function is everywhere differentiable then f df calculus result one can show that the point is a local minimum of f iff f d by analogy to the standard in general for a vector valued function we say that g is a subgradient of f at if for all vectors f f g so g is a linear lower bound to the function at regularization basics figure left soft thresholding. the flat region is the interval right hard thresholding. let us apply these concepts to the lasso problem. let us initially ignore the non-smooth penalty term. one can show that rssw jwj cj wj aj cj ij xijyi wt jxi j where w j is w without component j and similarly for xi j. we see that cj is to the correlation between the j th feature xj and the residual due to the other features r j y x jw j. hence the magnitude of cj is an indication of how relevant feature j is for predicting y to the other features and the current parameters. adding in the penalty term we find that the subderivative is given by wj f cj wj cj cj cj cj if wj if wj if wj we can write this in a more compact fashion as follows xt yj if wj if wj if wj depending on the value of cj the solution to wj of wj as follows f can occur at different values chapter sparse linear models if cj so the feature is strongly negatively correlated with the residual then the subgradient is zero at wj cj aj if cj so the feature is only weakly correlated with the residual then the subgradient is zero at wj if cj so the feature is strongly positively correlated with the residual then the subgra dient is zero at wj cj aj in summary we have we can write this as follows wjcj if cj if cj if cj wj soft cj aj aj where soft signa and x maxx is the positive part of x. this is called soft thresholding. this is illustrated in figure where we plot wj vs cj. the dotted line is the line wj cjaj corresponding to the least squares fit. the solid line which represents the regularized estimate wjcj shifts the dotted line down up by except when cj in which case it sets wj by contrast in figure we illustrate hard thresholding. this sets values of wj to if cj but it does not shrink the values of wj outside of this interval. the slope of the soft thresholding line does not coincide with the diagonal which means that even large coefficients are shrunk towards zero consequently lasso is a biased estimator. this is undesirable since if the likelihood indicates cj that the coefficient wj should be large we do not want to shrink it. we will discuss this issue in more detail in section now we finally can understand why tibshirani invented the term lasso in it stands for least absolute selection and shrinkage operator since it selects a subset of the variables and shrinks all the coefficients by penalizing the absolute values. if we get the ols solution minimal norm. if max we get w where max y max xj this value is computed using the fact that is optimal if yj for all j. in general the maximum penalty for an regularized objective is j max max j jn regularization basics comparison of least squares lasso ridge and subset selection we can gain further insight into regularization by comparing it to least squares and and regularized least squares. for simplicity assume all the features of x are orthonormal so xt x i. in this case the rss is given by rssw yt y wt xt xw xt y const k wkxikyi k k i so we see this factorizes into a sum of terms one per dimension. hence we can write down the map and ml estimates analytically as follows mle the ols solution is given by wols xt k where xk is the k th column of x. this follows trivially from equation we see that wols is just the orthogonal projection of feature k onto the response vector section k ridge one can show that the ridge estimate is given by lasso from equation and using the fact that ak and wols k wridge k k wols wlasso k sign wols k wols k we have this corresponds to soft thresholding shown in figure subset selection if we pick the best k features using subset selection the parameter estimate is as follows wss k wols k if rankwols otherwise k k where rank refers to the location in the sorted list of weight magnitudes. this corresponds to hard thresholding shown in figure figure plots the mse vs for lasso for a degree polynomial and figure plots the mse vs polynomial order. we see that lasso gives similar results to the subset selection method. as another example consider a data set concerning prostate cancer. we have d features and n training cases the goal is to predict the log prostate-specific antigen levels et al. for more biological details. table shows that lasso gives better prediction accuracy least on this particular data set than least squares ridge and best subset regression. each case the strength of the regularizer was chosen by cross validation. lasso also gives rise to a sparse solution. of course for other problems ridge may give better predictive accuracy. in practice a combination of lasso and ridge known as the elastic net often performs best since it provides a good combination of sparsity and regularization section chapter sparse linear models train test e s m lambda e s m performance of mle train test degree figure mse vs for lasso for a degree polynomial. note that decreases as we move to the right. figure generated by linregpolylassodemo. mse versus polynomial degree. note that the model order increases as we move to the right. see figure for a plot of some of these polynomial regression models. figure generated by linregpolyvsdegree. term intercept lcavol lweight age lbph svi lcp gleason test error ls best subset ridge lasso table results of different methods on the prostate cancer data which has features and training cases. methods are ls least squares subset best subset regression ridge lasso. rows represent the coefficients we see that subset regression and lasso give sparse solutions. bottom row is the mean squared error on the test set cases. based on table of et al. figure generated by prostatecomparison. regularization path as we increase the solution vector w will tend to get sparser although not necessarily monotonically. we can plot the values wj vs for each feature j this is known as the regularization path. this is illustrated for ridge regression in figure where we plot wj as the regularizer decreases. we see that when all the coefficients are zero. but for any finite value of all coefficients are non-zero furthermore they increase in magnitude as is decreased. in figure we plot the analogous result for lasso. as we move to the right the upper bound on the penalty b increases. when b all the coefficients are zero. as we increase regularization basics lcavol lweight age lbph svi lcp gleason lcavol lweight age lbph svi lcp gleason profiles of ridge coefficients for the prostate cancer example vs bound on norm of w figure so small t is on the left. the vertical line is the value chosen by cv using the rule. based on figure of et al. figure generated by ridgepathprostate. profiles of lasso coefficients for the prostate cancer example vs bound on norm of w so small t is on the left. based on figure of et al. figure generated by lassopathprostate. lcavol lweight age lbph svi lcp gleason lcavol lweight age lbph svi lcp gleason lars step figure illustration of piecewise linearity of regularization path for lasso on the prostate cancer example. we plot wjb vs b for the critical values of b. we plot vs steps of the lars algorithm. figure generated by lassopathprostate. b the coefficients gradually turn on but for any value between and bmax the solution is remarkably it can be shown that the solution path is a piecewise linear function of b et al. that is there are a set of critical values of b where the active set of non-zero coefficients changes. for values of b between these critical values each non-zero coefficient increases or decreases in a linear fashion. this is illustrated in figure furthermore one can solve for these critical values analytically. this is the basis of the lars algorithm et al. which stands for least angle regression and shrinkage section for details. remarkably lars can compute the entire regularization path for roughly the same it is common to plot the solution versus the shrinkage factor defined as sb max rather than against b. this merely affects the scale of the horizontal axis not the shape of the curves. chapter sparse linear models original number of nonzeros reconstruction lambda mse debiased minimum norm solution figure example of recovering a sparse signal using lasso. see text for details. based on figure of et al. figure generated by sparsesensingdemo written by mario figueiredo. computational cost as a single least squares fit ominn dn in figure we plot the coefficients computed at each critical value of b. now the piecewise linearity is more evident. below we display the actual coefficient values at each step along the regularization path last line is the least squares solution listing output of lassopathprostate by changing b from to bmax we can go from a solution in which all the weights are zero to a solution in which all weights are non-zero. unfortunately not all subset sizes are achievable using lasso. one can show that if d n the optimal solution can have at most n variables in it before reaching the complete set corresponding to the ols solution of minimal norm. in section we will see that by using an regularizer as well as an regularizer method known as the elastic net we can achieve sparse solutions which contain more variables than training cases. this lets us explore model sizes between n and d. regularization basics model selection it is tempting to use regularization to estimate the set of relevant variables. in some cases we can recover the true sparsity pattern of w the parameter vector that generated the data. a method that can recover the true model in the n limit is called model selection consistent. the details on which methods enjoy this property and when are beyond the scope of this book see e.g. and van de geer for details. instead of going into a theoretical discussion we will just show a small example. we first of size d consisting of randomly placed spikes. generate a sparse signal w next we generate a random design matrix x of size n d where n finally we where n we then estimate w from generate a noisy observation y xw y and x. is shown in the first row of figure the second row is the estimate using max. we see that this has spikes in the right places but they are too small. the third row is the least squares estimate of the coefficients which are estimated to be non-zero based on supp this is called debiasing and is necessary because lasso shrinks the relevant coefficients as well as the irrelevant ones. the last row is the least squares estimate for all the coefficients jointly ignoring sparsity. we see that the sparse estimate is an excellent estimate of the original signal. by contrast least squares without the sparsity assumption performs very poorly. the original w of course to perform model selection we have to pick it is common to use cross validation. however it is important to note that cross validation is picking a value of that results in good predictive accuracy. this is not usually the same value as the one that is likely to recover the true model. to see why recall that regularization performs selection and shrinkage that is the chosen coefficients are brought closer to in order to prevent relevant coefficients from being shrunk in this way cross validation will tend to pick a value of that is not too large. of course this will result in a less sparse model which contains irrelevant variables positives. indeed it was proved in and buhlmann that the prediction-optimal value of does not result in model selection consistency. in section we will discuss some adaptive mechanisms for automatically tuning on a per-dimension basis that does result in model selection consistency. a downside of using regularization to select variables is that it can give quite different results if the data is perturbed slightly. the bayesian approach which estimates posterior marginal inclusion probabilities p j is much more robust. a frequentist solution to this is to use bootstrap resampling section and to rerun the estimator on different versions of the data. by computing how often each variable is selected across different trials we can approximate the posterior inclusion probabilities. this method is known as stability selection and b ijhlmann we can threshold the stability selection inclusion probabilities at some level say and thus derive a sparse estimator. this is known as bootstrap lasso or bolasso it will include a variable if it occurs in at least of sets returned by lasso a fixed this process of intersecting the sets is a way of eliminating the false positives that vanilla lasso produces. the theoretical results in prove that bolasso is model selection consistent under a wider range of conditions than vanilla lasso. as an illustration we reproduced the experiments in in particular we created chapter sparse linear models x e d n i l e b a i r a v lasso on sign inconsistent data log x e d n i l e b a i r a v bolasso on sign inconsistent data bootstraps lasso vs bolasso on sign inconsistent data nbootstraps t r o p p u s t c e r r o c p lasso bolasso log log figure probability of selection of each variable large probabilities black small probabilities vs. regularization parameter for lasso. as we move from left to right we decrease the amount of regularization and therefore select more variables. same as but for bolasso. probability of correct sign estimation vs. regularization parameter. bolasso dashed and lasso plain the number of bootstrap replications is in based on figures of figure generated by bolassodemo. datasets of size n with d variables of which are relevant. see for more detail on the experimental setup. for dataset n variable j and sparsity level k define sj k n i wj kdn now definep k be the average of sj k n over the in figure we plot p vs log for lasso and bolasso. we see that for datasets. bolasso there is a large range of where the true variables are selected but this is not the case for lasso. this is emphasized in figure where we plot the empirical probability that the correct set of variables is recovered for lasso and for bolasso with an increasing number of bootstrap samples. of course using more samples takes longer. in practice bootstraps seems to be a good compromise between speed and accuracy. with bolasso there is the usual issue of picking obviously we could use cross validation but plots such as figure suggest another heuristic shuffle the rows to create a large black block and then pick to be in the middle of this region. of course operationalizing this intuition may be tricky and will require various ad-hoc thresholds is reminiscent of the find the knee in the curve heuristic discussed in section when discussing how to pick k for mixture models. a bayesian approach provides a more principled method for selecting bayesian inference for linear models with laplace priors we have been focusing on map estimation in sparse linear models. it is also possible to perform bayesian inference e.g. and casella seeger however the posterior mean and median as well as samples from the posterior are not sparse only the mode is sparse. this is another example of the phenomenon discussed in section where we said that the map estimate is often untypical of the bulk of the posterior. another argument in favor of using the posterior mean comes from equation which showed that that plugging in the posterior mean rather than the posterior mode is the optimal thing to do if we want to minimize squared prediction error. et al. shows experimentally and and yavnch shows theoretically that using the posterior mean with a spike-and-slab prior results in better prediction accuracy than using the posterior mode with a laplace prior albeit at slightly higher computational cost. regularization algorithms regularization algorithms in this section we give a brief review of some algorithms that can be used to solve regularized estimation problems. we focus on the lasso case where we have a quadratic loss. however most of the algorithms can be extended to more general settings such as logistic regression et al. for a comprehensive review of regularized logistic regression. note that this area of machine learning is advancing very rapidly so the methods below may not be state of the art by the time you read this chapter. et al. yaun et al. yang et al. for some recent surveys. coordinate descent sometimes it is hard to optimize all the variables simultaneously but it easy to optimize them one by one. in particular we can solve for the j th coefficient with all the others held fixed w j argmin z f zej f where ej is the j th unit vector. we can either cycle through the coordinates in a deterministic fashion or we can sample them at random or we can choose to update the coordinate for which the gradient is steepest. the coordinate descent method is particularly appealing if each one-dimensional optimization problem can be solved analytically for example the shooting algorithm wu and lange for lasso uses equation to compute the optimal value of wj given all the other coefficients. see algorithm for the pseudo code lassoshooting for some matlab code. see et al. for some extensions of this method to the logistic regression case. the resulting algorithm was the fastest method in their experimental comparison which concerned document classification with large sparse feature vectors bags of words. other types of data dense features andor regression problems might call for different algorithms. y algorithm coordinate descent for lasso shooting algorithm initialize w x i repeat for j d do ij xijyi wt xi wjxij cj aj aj aj cj wj soft until converged lars and other homotopy methods the problem with coordinate descent is that it only updates one variable at a time so can be slow to converge. active set methods update many variables at a time. unfortunately they are chapter sparse linear models more complicated because of the need to identify which variables are constrained to be zero and which are free to be updated. active set methods typically only add or remove a few variables at a time so they can take a long if they are started far from the solution. but they are ideally suited for generating a set of solutions for different values of starting with the empty set i.e. for generating regularization path. these algorithms exploit the fact that one can quickly compute w k from w k if k k this is known as warm starting. in fact even if we only want the solution for a single value of call it it can sometimes be computationally more efficient to compute a set of solutions from max down to using warm-starting this is called a continuation method or homotopy method. this is often much faster than directly cold-starting at this is particularly true if is small. perhaps the most well-known example of a homotopy method in machine learning is the lars algorithm which stands for least angle regression and shrinkage et al. similar algorithm was independently invented in et al. this can compute w for all possible values of in an efficient manner. lars works as follows. it starts with a large value of such that only the variable that is most correlated with the response vector y is chosen. then is decreased until a second variable is found which has the same correlation terms of magnitude with the current residual as the first variable where the residual at step k is defined as rk y xfk wk where fk is the current active set equation remarkably one can solve for this new value of analytically by using a geometric argument the term least angle this allows the algorithm to quickly jump to the next point on the regularization path where the active set changes. this repeats until all the variables are added. it is necessary to allow variables to be removed from the active set if we want the sequence of solutions to correspond to the regularization path of lasso. if we disallow variable removal we get a slightly different algorithm called lar which tends to be faster. in particular lar costs the same as a single ordinary least squares fit namely on d minn d which is on if n d and on if d n lar is very similar to greedy forward selection and a method known as least squares boosting section there have been many attempts to extend the lars algorithm to compute the full regularization path for regularized glms such as logistic regression. in general one cannot analytically solve for the critical values of instead the standard approach is to start at max and then slowly decrease tracking the solution as we go this is called a continuation method or homotopy method. these methods exploit the fact that we can quickly compute w k from w k if k k this is known as warm starting. even if we don t want the full path this method is often much faster than directly cold-starting at the desired value of is particularly true if is small. the method described in et al. combines coordinate descent with this warmstarting strategy and computes the full regularization path for any regularized glm. this has been implemented in the glmnet package which is bundled with pmtk. proximal and gradient projection methods in this section we consider some methods that are suitable for very large scale problems where homotopy methods made be too slow. these methods will also be easy to extend to other kinds regularization algorithms of regularizers beyond as we will see later. our presentation in this section is based on yang et al. consider a convex objective of the form f l where l the loss is convex and differentiable and r the regularizer is convex but not necessarily differentiable. for example l rss and r corresponds to the bpdn problem. as another example the lasso problem can be formulated as follows l rss and r c where c b and ic is the indicator function of a convex set c defined as ic c otherwise in some cases it is easy to optimize functions of the form in equation for example suppose l rss and the design matrix is simply x i. then the obective becomes f r the minimizer of this is given by proxry which is the proximal operator for the convex function r defined by proxry argmin rz intuitively we are returning a point that minimizes r but which is also close to y. in general we will use this operator inside an iterative optimizer in which case we want to stay close to the previous iterate. in this case we use proxr k argmin rz z z the key issues are how do we efficiently compute the proximal operator for different regularizers r and how do we extend this technique to more general loss functions l? we discuss these issues below. proximal operators if r the proximal operator is given by componentwise soft-thresholding proxr soft as we showed in section if r the proximal operator is given by componentwise hard-thresholding where hardu a uiu a. proxr hard if r ic the proximal operator is given by the projection onto the set c proxr argmin z c projc chapter sparse linear models illustration of projected gradient descent. the step along the negative gradient to k gk figure if we project that point onto the closest point in the set we get takes us outside the feasible set. proj k gk. we can then derive the implicit update direction using dk k. used with kind permission of mark schmidt. for some convex sets it is easy to compute the projection operator. for example to project onto the rectangular set defined by the box constraints c j uj we can use j uj j j uj j uj projc projc to project onto the euclidean ball c we can use to project onto the ball c we can use projc soft where if and otherwise is the solution to the equation max j we can implement the whole procedure in od time as explained in et al. we will see an application of these different projection methods in section proximal gradient method we now discuss how to use the proximal operator inside of a gradient descent routine. the basic idea is to minimize a simple quadratic approximation to the loss function centered on the regularization algorithms k z rz k t argmin where gk l k is the gradient of the loss tk is a constant discussed below and the last term arises from a simple approximation to the hessian of the loss of the form k i. dropping terms that are independent of z and multiplying by tk we can rewrite the above tk k k expression in terms of a proximal operator as follows argmin tkrz proxtkruk z uk k tkgk gk l k if r this is equivalent to gradient descent. if r c the method is equivalent if r the method is to projected gradient descent sketched in figure known as iterative soft thresholding. mation to the hessian we require that there are several ways to pick tk or equivalently k given that ki is an approxi k k k gk gk in the least squares sense. hence k argmin k k gk k k gk k k k k this is known as the barzilai-borwein or spectral stepsize and borwein fletcher raydan this stepsize can be used with any gradient method whether proximal or not. it does not lead to monotonic decrease of the objective but it is much faster than standard line search techniques. ensure convergence we require that the objective decrease on average where the average is computed over a sliding window of size m when we combine the bb stepsize with the iterative soft thresholding technique r plus a continuation method that gradually reduces we get a fast method for the bpdn problem known as the sparsa algorithm which stands for sparse reconstruction by separable approximation et al. however we will call it the iterative shrinkage and thresholding algorithm. see algorithm for some pseudocode and sparsa for some matlab code. see also exercise for a related approach based on projected gradient descent. nesterov s method a faster version of proximal gradient descent can be obtained by epxanding the quadratic approximation around a point other than the most recent parameter value. in particular consider performing updates of the form proxtkr k tkgk gk l k k k k k k k chapter sparse linear models n d y r n parameters m s algorithm iterative shrinkage-thresholding algorithm input x r initialize r y repeat t maxsxt r adapt the regularizer repeat g l u g softu t update using bb stepsize in equation until f increased too much within the past m steps r y x update residual until t j wj yi xi d n figure representing lasso using a gaussian scale mixture prior. this is known as nesterov s method tseng as before there are a variety of ways of setting tk typically one uses line search. when this method is combined with the iterative soft thresholding technique r plus a continuation method that gradually reduces we get a fast method for the bpdn problem known as the fast iterative shrinkage thesholding algorithm or fista and teboulle regularization algorithms em for lasso in this section we show how to solve the lasso problem using lasso. at first sight this might seem odd since there are no hidden variables. the key insight is that we can represent the laplace distribution as a gaussian scale mixture and mallows west as follows e n j j j j ga thus the laplace is a gsm where the mixing distibution on the variances is the exponential j distribution expon using this decomposition we can represent the lasso model as shown in figure the corresponding joint distribution has the form py w n d ig b ga j j where d diag j and where we have assumed for notational simplicity that x is standardized and that y is centered we can ignore the offset term expanding out we get py w exp wt d w exp exp b exp j j below we describe how to apply the em algorithm to the model in figure in brief in the e step we infer j and and in the m step we estimate w. the resulting estimate w is the same as the lasso estimator. this approach was first proposed in also and brown caron and doucet ding and harrison for some extensions. why em? before going into the details of em it is worthwhile asking why we are presenting this approach at all given that there are a variety of other much faster algorithms that directly solve the map estimation problem for an empirical comparison. the reason is that the latent variable perspective brings several advantages such as the following it provides an easy way to derive an algorithm to find parameter estimates for a variety of other models such as robust linear regression or probit regression to ensure the posterior is unimodal one can follow and casella and slightly modify the model by making the prior variance for the weights depend on the observation noise pwj j the em algorithm is easy to modify. j chapter sparse linear models it suggests trying other priors on the variances besides ga various extensions below. it makes it clear how we can compute the full posterior pwd rather than just a map estimate. this technique is known as the bayesian lasso and casella hans j we will consider the objective function from equation the complete data penalized log likelihood is as follows terms that do not depend on w wt w const where diag j is the precision matrix for w. the e step j the key is to compute e we can derive the full posterior which is given by the following and casella we can derive this directly exercise alternatively j j that the inverse gaussian distribution is also known as the wald distribution. hence e j let diage d e denote the result of this e step. we also need to infer it is easy to show that that the posterior is p w iga b hence x wt x w igan bn e an bn the m step the m step consists of computing w argmax w wt w this is just map estimation under a gaussian prior w xt x y regularization extensions however since we expect many wj we will have j for many j making inverting numerically unstable. fortunately we can use the svd of x given by x udvt as follows w vvt v d y where diag j e diag caveat since the lasso objective is convex this method should always find the global optimum. unfortunately this sometimes does not happen for numerical reasons. in particular suppose that in j further suppose that we set wj in an m step. in the following e the true solution w step we infer that j so then we set wj again thus we can never undo our mistake. fortunately in practice this situation seems to be rare. see and li for further discussion. regularization extensions in this section we discuss various extensions of vanilla regularization. group lasso in standard regularization we assume that there is a correspondence between parameters and variables so that if wj we interpret this to mean that variable j is excluded. but in more complex models there may be many parameters associated with a given variable. in particular we may have a vector of weights for each input wj. here are some examples multinomial logistic regression each feature is associated with c different weights one per class. linear regression with categorical inputs each scalar input is one-hot encoded into a vector of length c. multi-task learning in multi-task learning we have multiple related prediction problems. for example we might have c separate regression or binary classification problems. thus each feature is associated with c different weights. we may want to use a feature for all of the tasks or none of the tasks and thus select weights at the group level et al. if we use an regularizer of the form c we may end up with with some elements of wj being zero and some not. to prevent this kind of situation we partition the parameter vector into g groups. we now minimize the following objective j jw nllw where j g j chapter sparse linear models is the of the group weight vector. group lasso and lin if the nll is least squares this method is called we often use a larger penalty for larger groups by setting g number of elements in group g. for example objective becomes dg where dg is the if we have groups and the jw nllw note that if we had used the square of the the model would become equivalent to ridge regression since j g j g by using the square root we are penalizing the radius of a ball containing the group s weight vector the only way for the radius to be small is if all elements are small. thus the square root results in group sparsity. a variant of this technique replaces the with the infinity-norm et al. zhao et al. max j g it is clear that this will also result in group sparsity. an illustration of the difference is shown in figures and in both cases we have a true signal w of size d divided into groups each of size we randomly choose groups of w and assign them non-zero values. in the first example the values are drawn from a n in the second example the values are all set to we then pick a random design matrix x of size n d where n finally we generate y xw where n given this data we estimate the support of w using or group and then estimate the non-zero values using least squares. we see that group lasso does a much better job than vanilla lasso since it respects the known group we also see that the norm has a tendency to make all the elements within a block to have similar magnitude. this is appropriate in the second example but not the first. value of was the same in all examples and was chosen by hand. gsm interpretation of group lasso group lasso is equivalent to map estimation using the following prior pw exp the slight non-zero noise in the group lasso results is presumably due to numerical errors. regularization extensions original number groups active groups standard tau mse block tau mse block linf tau mse figure illustration of group lasso where the original signal is piecewise gaussian. top left original signal. bottom left vanilla lasso estimate. top right group lasso estimate using a norm on the blocks. bottom right group lasso estimate using an norm on the blocks. based on figures of et al. figure generated by grouplassodemo based on code by mario figueiredo. now one can show that this prior can be written as a gsm as follows wg g n g ga dg g idg where dg is the size of group g. so we see that there is one variance term per group each of which comes from a gamma prior whose shape parameter depends on the group size and whose rate parameter is controlled by figure gives an example where we have groups one of size and one of size this picture also makes it clearer why there should be a grouping effect. suppose is will be estimated to be small which will force to be small. converseley will be estimated to be large which will allow to be become small then suppose is large then large as well. chapter sparse linear models original number groups active groups standard tau mse block tau mse block linf tau mse figure same as figure except the original signal is piecewise constant. yi xi figure graphical model for group lasso with groups the first has size the second has size regularization extensions algorithms for group lasso there are a variety of algorithms for group lasso. here we briefly mention two. the first approach is based on proximal gradient descent discussed in section since the regularizer g the proximal operator decomposes into g separate operators is separable rw of the form where b kg tkgkg. if p one can show and wajs that this can be implemented as follows proxrb argmin z r dg where c is the ball. using equation if we have proxrb b proj cb proxrb b b otherwise we have proxrb b b b we can combine these into a vectorial soft-threshold function as follows et al. proxrb b if p we use c which is the ball. we can project onto this in odg time using an algorithm described in et al. another approach is to modify the em algorithm. the method is almost the same as for vanilla lasso. if we define gj where gj is the group to which dimension j belongs we can use the same full conditionals for and w as before. the only changes are as follows j we must modify the full conditional for the weight precisions which are estimated based on w y x inversegaussian j g jg. for the e step we can use a shared set of weights g e where g we must modify the full conditional for the tuning parameter which is now only estimated based on g values of g p gaa b g g chapter sparse linear models h g c index figure example of the fused lasso. the vertical axis represents array cgh genome source figure of hybridization intensity and the horizontal axis represents location along a genome. source figure of used with kind permission of holger hoefling. fused lasso estimate using lattice prior. noisy image. fused lasso in some problem settings functional data analysis we want neighboring coefficients to be similar to each other in addition to being sparse. an example is given in figure where we want to fit a signal that is mostly off but in addition has the property that neighboring locations are typically similar in value. we can model this by using a prior of the form d pw exp wj this is known as the fused lasso penalty. in the context of functional data analysis we often use x i so there is one coefficient for each location in the signal section in this case the overall objective has the form jw wi n s v this is a sparse version of equation it is possible to generalize this idea beyond chains and to consider other graph structures using a penalty of the form jw wt s v e this is called graph-guided fused lasso e.g. et al. the graph might come from some prior knowledge e.g. from a database of known biological pathways. another example is shown in figure where the graph structure is a lattice. regularization extensions gsm interpretation of fused lasso one can show et al. that the fused lasso model is equivalent to the following hierarchical model w n j expon j expon j j where and is a tridiagonal precision matrix with main diagonal j off diagonal j j j where we have defined d this is very similar to the model in section where we used a chain-structured gaussian markov random field as the prior with fixed variance. here we just let the variance be random. in the case of graph-guided lasso the structure of the graph is reflected in the zero pattern of the gaussian precision matrix section algorithms for fused lasso it is possible to generalize the em algorithm to fit the fused lasso model by exploiting the markov structure of the gaussian prior for efficiency. direct solvers don t use the latent variable trick can also be derived e.g. however this model is undeniably more expensive to fit than the other variants we have considered. elastic net and lasso combined although lasso has proved to be effective as a variable selection technique problems and hastie such as the following it has several if there is a group of variables that are highly correlated genes that are in the same pathway then the lasso tends to select only one of them chosen rather arbitrarily. is evident from the lars algorithm once one member of the group has been chosen the remaining members of the group will not be very correlated with the new residual and hence will not be chosen. it is usually better to select all the relevant variables in a group. if we know the grouping structure we can use group lasso but often we don t know the grouping structure. in the d n case lasso can select at most n variables before it saturates. if n d but the variables are correlated prediction performance of ridge is better than that of lasso. it has been empirically observed that the chapter sparse linear models zou and hastie and hastie proposed an approach called the elastic net which is a hybrid between lasso and ridge regression which solves all of these problems. it is apparently called the elastic net because it is like a stretchable fishing net that retains all the big fish and hastie vanilla version the vanilla version of the model defines the following objective function jw notice that this penalty function is strictly convex so there is a unique global minimum even if x is not full rank. it can be shown and hastie that any strictly convex penalty on w will exhibit a grouping effect which means that the regression coefficients of highly correlated variables tend to be equal to a change of sign if they are negatively correlated. for example if two features are equal so xj xk one can show that their estimates are also equal wj wk. by contrast with lasso we may have that wj and wk or vice versa. algorithms for vanilla elastic net it is simple to show that the elastic net problem can be reduced to a lasso problem on modified data. in particular define x c x y y where c then we solve w arg min w y x c and set w c w. we can use lars to solve this subproblem this is known as the lars-en algorithm. if we stop the algorithm after m variables have been included the cost is note that we can use m d if we wish since x has rank d. this is in contrast to lasso which cannot select more than n variables jumping to the ols solution if n d. when using lars-en other solvers one typically uses cross-validation to select and improved version unfortunately it turns out that the vanilla elastic net does not produce functions that predict very accurately unless it is very close to either pure ridge or pure lasso. intuitively the reason is that it performs shrinkage twice once due to the penalty and again due to the penalty. the solution is simple undo the shrinkage by scaling up the estimates from the vanilla version. in other words if w is the solution of equation then a better estimate is w w non-convex regularizers we will call this a corrected estimate. one can show that the corrected estimates are given by xt x w xw w arg min w wt now xt x i where so the the elastic net is like lasso but where we use a version of that is shrunk towards i. section for more discussion of regularized estimates of covariance matrices. gsm interpretation of elastic net the implicit prior being used by the elastic net obviously has the form pw exp j which is just a product of gaussian and laplace distributions. this can be written as a hierarchical prior as follows et al. chen et al. wj j n j j expon clearly if this reduces to the regular lasso. it is possible to perform map estimation in this model using em or bayesian inference using mcmc et al. or variational bayes et al. non-convex regularizers although the laplace prior results in a convex optimization problem from a statistical point of view this prior is not ideal. there are two main problems with it. first it does not put enough probability mass near so it does not sufficiently suppress noise. second it does not put enough probability mass on large values so it causes shrinkage of relevant coefficients corresponding to signal can be seen in figure we see that estimates of large coefficients are significantly smaller than their ml estimates a phenomenon known as bias. both problems can be solved by going to more flexible kinds of priors which have a larger spike at and heavier tails. even though we cannot find the global optimum anymore these non-convex methods often outperform regularization both in terms of predictive accuracy and in detecting relevant variables and li schniter et al. we give some examples below. chapter sparse linear models bridge regression a natural generalization of regularization known as bridge regression and friedman has the form w nllw j for b this corresponds to map estimation using a exponential power distribution given by exppowerw a b b exp b a if b we get the gaussian distribution a corresonding to ridge regression if we set b we get the laplace distribution corresponding to lasso if we set b we get regression which is equivalent to best subset selection. unfortunately the objective is not convex for b and is not sparsity promoting for b so the norm is the tightest convex approximation to the norm. the effect of changing b is illustrated in figure where we plot the prior for b b and b we assume pw we also plot the posterior after seeing a single observation y which imposes a single linear constraint of the form y wt x with a certain tolerance controlled by the observation noise to figure we see see that the mode of the laplace is on the vertical axis corresponding to by contrast there are two modes when using b corresponding to two different sparse solutions. when using the gaussian the map estimate is not sparse mode does not lie on either of the coordinate axes. hierarchical adaptive lasso recall that one of the principal problems with lasso is that it results in biased estimates. this is because it needs to use a large value of to squash the irrelevant parameters but this then over-penalizes the relevant parameters. it would be better if we could associate a different penalty parameter with each parameter. of course it is completely infeasible to tune d parameters by cross validation but this poses no problem to the bayesian we simply make each j have its own private tuning parameter j which are now treated as random variables coming from the conjugate prior j iga b. the full model is as follows j iga b j j j wj j n j see figure this has been called the hierarchical adaptive lasso et al. also et al. cevher armagan et al. we can integrate out j which induces a j distribution on wj as before. the result is that pwj is now a it turns out that we can fit this model compute a local scaled mixture of laplacians. posterior mode using em as we explain below. the resulting estimate whal often works non-convex regularizers figure top plot of log prior for three different distributions with unit variance gaussian laplace and exponential power. bottom plot of log posterior after observing a single observation corresponding to a single linear constraint. the precision of this observation is shown by the diagonal lines in the top figure. in the case of the laplace prior the posterior is unimodal and asymmetric in the case of the exponential prior the posterior is bimodal. based on figure of figure generated by sparsepostplot written by florian steinke. in the case of the gaussian prior the posterior is unimodal and symmetric. much better than the estimate returned by lasso in the sense that it is more likely to contain zeros in the right places selection consistency and more likely to result in good predictions consistency et al. we give an explanation for this behavior in section em for hal since the inverse gamma is conjugate to the laplace we find that the e step for j is given by p jwj iga b the e step for is the same as for vanilla lasso. the prior for w has the following form pw exp j j j hence the m step must optimize argmax w log n j j chapter sparse linear models a b hal j j wj d yi xi n a b figure dgm for hierarchical adaptive lasso. contours of hierarchical adpative laplace. based on figure of et al. figure generated by normalgammapenaltyplotdemo. the expectation is given by e j a b j st j thus the m step becomes a weighted lasso problem argmin w j st j j this is easily solved using standard methods lars. note that if the coefficient was estimated to be large in the previous iteration wt j will be small so large coefficients are not penalized heavily. conversely small coefficients do get penalized heavily. this is the way that the algorithm adapts the penalization strength of each coefficient. the result is an estimate that is often much sparser than returned by lasso but also less biased. is large then the scaling factor st note that if we seta b and we only perform iteration of em we get a method that is closely related to the adaptive lasso of zou and li this em algorithm is also closely related to some iteratively reweighted methods proposed in the signal processing community and yin candes et al. understanding the behavior of hal we can get a better understanding of hal by integrating out j to get the following marginal distribution pwja b a b non-convex regularizers lasso hal p a m w p a m w wmle b b b wmle figure thresholding behavior of hierarchical adaptive laplace. normalgammathresholdplotdemo. two penalty functions log priors. based on figure of et al. laplace. figure generated by this is an instance of the generalized t distribution and newey armagan et al. this is called the double pareto distribution defined as gtw a c q q a acq where c is the scale parameter controls the degree of sparsity and a is related to the degrees of freedom. when q and c we recover the standard t distribution when a we recover the exponential power distribution and when q and a we in the context of the current model we see that pwja b get the laplace distribution. a ba the resulting penalty term has the form log pwj b const where b are the tuning parameters. we plot this penalty in we plot in figure for various values of b. compared to the diamond-shaped laplace penalty shown in figure we see that the hal penalty looks more like a star fish it puts much more density along the spines thus enforcing sparsity more aggressively. note that this penalty is clearly not convex. we can gain further understanding into the behavior of this penalty function by considering applying it to the problem of linear regression with an orthogonal design matrix. in this case p j ga j ga ig c pwj p j fixed iga b a ba gaa b fixed fixed c b nega b ng njwj t horseshoeb chapter sparse linear models ref and mallows west et al. cevher armagan et al. and brown chen et al. and brown and mallows west et al. table some scale mixtures of gaussians. abbreviations c half-rectified cauchy ga gamma and rate parameterization gt generalized t ig inverse gamma neg normal-exponentialgamma ng normal-gamma nj normal-jeffreys. the horseshoe distribution is the name we give to the distribution induced on wj by the prior described in et al. this has no simple analytic form. the definitions of the neg and ng densities are a bit complicated but can be found in the references. the other distributions are defined in the text. one can show that the objective becomes jw j wmle where wmle xt y is the mle and y x wmle. thus we can compute the map estimate one dimension at a time by solving the following optimization problem wj argmin wj j wmle in figure we plot the lasso estimate vs the ml estimate wmle. we see that the estimator has the usual soft-thresholding behavior seen earlier in figure however this behavior is undesirable since the large magnitude coefficients are also shrunk towards whereas we would like them to be equal to their unshrunken ml estimates. in figure we plot the hal estimate whal vs the ml estimate wmle. we see that this approximates the more desirable hard thresholding behavior seen earlier in figure much more closely. other hierarchical priors many other hierarchical sparsity-promoting priors have been proposed see table for a brief in some cases we can analytically derive the form of the marginal prior for wj. summary. generally speaking this prior is not concave. a particularly interesting prior is the improper normal-jeffreys prior which has been used j in this puts a non-informative jeffreys prior on the variance ga automatic relevance determination bayesian learning j the resulting marginal has the form pwj wj this gives rise to a thresholding rule that looks very similar to hal in figure which in turn is very similar to hard thresholding. however this prior has no free parameters which is both a good thing to tune and a bad thing ability to adapt the level of sparsity. automatic relevance determination bayesian learning all the methods we have considered so far for the spike-and-slab methods in section have used a factorial prior of the form pw j pwj. we have seen how these priors can be represented in terms of gaussian scale mixtures of the form wj n j where j has one of the priors listed in table using these latent variances we can represent the j wj y x. we can then use em to perform map estimation model in the form j and in the m step we estimate w from y x and where in the e step we inferp this m step either involves a closed-form weighted optimization the case of gaussian scale mixtures or a weighted optimization the case of laplacian scale mixtures. we also discussed how to perform bayesian inference in such models rather than just computing map estimates. in this section we discuss an alternative approach based on type ii ml estimation bayes whereby we integrate out w and maximize the marginal likelihood wrt this eb procedure can be implemented via em or via a reweighted scheme as we will explain below. having estimated the variances we plug them in to compute the posterior mean of the weights e rather surprisingly view of the gaussian prior the result is an sparse estimate for reasons we explain below. in the context of neural networks this this method is called called automatic relevance determination or ard neal see section in the context of the linear models we are considering in this chapter this method is called sparse bayesian learning or sbl combining ardsbl with basis function expansion in a linear model gives rise to a technique called the relevance vector machine which we will discuss in section ard for linear regression we will explain the procedure in the context of linear regression ard for glms requires the use of the laplace some other approximation. case can be it is conventional when discussing ard sbl to denote the weight precisions by j j and the measurement precision by not confuse this with the use of in statistics to represent the regression coefficients!. in particular we will assume the following model pyx w x pw a chapter sparse linear models where a diag the marginal likelihood can be computed analytically as follows pyx n in adw n in xa exp yt c y where c xa compare this to the marginal likelihood in equation in the spike and slab model modulo the factor missing from the second term the equations are the same except we have replaced the binary j with continuous j r in log form the objective becomes log pyx log yt c y to regularize the problem we may put a conjugate prior on each precision j gaa b and gac d. the modified objective becomes log pyx log yt c y log ga ja b log ga d log j b j log d j j this is useful when performing bayesian inference for and and tipping however when performing ii point estimation we will use the improper prior a b c d which results in maximal sparsity. below we describe how to optimize wrt the precision terms and this is a proxy for finding the most probable model setting of in the spike and slab model which in turn is closely related to regularization. in particular it can be shown et al. that the objective in equation has many fewer local optima than the objective and hence is much easier to optimize. once we have estimated and we can compute the posterior over the parameters using pwd n xt x a xt y the fact that we compute a posterior over w while simultaneously encouraging sparsity is why the method is called sparse bayesian learning nevertheless since there are many ways to be sparse and bayesian we will use the ard term instead even in the linear model context. addition sbl is only being bayesian about the values of the coefficients rather than reflecting uncertainty about the set of relevant variables which is typically of more interest. an alternative approach to optimizing is to put a gamma prior on and to integrate it out to get a student posterior for w and weigend however it turns out that this results in a less accurate estimate for in addition working with gaussians is easier than working with the student distribution and the gaussian case generalizes more easily to other cases such as logistic regression. automatic relevance determination bayesian learning x y c y figure illustration of why ard results in sparsity. the vector of inputs x does not point towards the vector of outputs y so the feature should be removed. for finite the probability density is spread in directions away from y. when the probability density at y is maximized. based on figure of whence sparsity? j if j we find wj wmle since the gaussian prior shrinking wj towards has zero precision. however if we find that j then the prior is very confident that wj and hence that feature j is irrelevant hence the posterior mean will have wj thus irrelevant features automatically have their weights turned off or pruned out we now give an intuitive argument based on about why ml-ii should encourage j for irrelevant features. consider a linear regression with training examples so x x and y we can plot x and y as vectors in the plane as shown in figure suppose the feature is irrelevant for predicting the response so x points in a nearly orthogonal direction to y. let us see what happens to the marginal likelihood as we change the marginal likelihood is given by pyx n c where c i xxt if is finite the posterior will be elongated along the direction of x as in figure however if we find c if is held constant the latter assigns higher probability density to the observed response vector y so this is the preferred solution. in other words the marginal likelihood punishes solutions where j is small but xj is irrelevant since these waste probability mass. it is more parsimonious the point of view of bayesian occam s razor to eliminate redundant dimensions. i so c is spherical as in figure connection to map estimation ard seems quite different from the map estimation methods we have been considering earlier in this chapter. in particular in ard we are not integrating out and optimizing w but vice chapter sparse linear models versa. because the parameters wj become correlated in the posterior to explaining away when we estimate j we are borrowing information from all the features not just feature j. consequently the effective prior pw is non-factorial and furthermore it depends on the data d however in and nagarajan it was shown that ard can be viewed as the following map estimation problem gardw j log ward arg min w gardw min j the proof which is based on convex analysis is a little complicated and hence is omitted. furthermore and nagarajan wipf et al. prove that map estimation with non-factorial priors is strictly better than map estimation with any possible factorial prior in the following sense the non-factorial objective always has fewer local minima than factorial objectives while still satisfying the property that the global optimum of the non-factorial objective corresponds to the global optimum of the objective a property that regularization which has no local minima does not enjoy. algorithms for ard in this section we review several different algorithms for implementing ard. em algorithm the easiest way to implement sblard is to use em. the expected complete data log likelihood is given by log n log n a n log e log j trawwt const q x trxt x j n log log j j tra t const where and are computed in the e step using equation suppose we put a gaa b prior on j and a gac d prior on the penalized objective becomes q log j b j log d j setting d j j e we get the following m step j j jj automatic relevance determination bayesian learning if j and a b the update becomes d e w d t tr the update for is given by x j jj new n this is exercise fixed-point algorithm a faster and more direct approach is to directly optimize the objective in equation one can show that the equations d lead to the following fixed d j point updates and j j j n x j j j j jj the quantity j is a measure of how well-determined wj is by the data hence j j is the effective degrees of freedom of the model. see section for further discussion. since and both depend on and can be computed using equation or the laplace approximation we need to re-estimate these equations until convergence. properties of this algorithm have been studied in and nagarajan at convergence the results are formally identical to those obtained by em but since the objective is non-convex the results can depend on the initial values. iteratively reweighted algorithm another approach to solving the ard problem is based on the view that it is a map estimation problem. although the log prior gw is rather complex in form it can be shown to be a non-decreasing concave function of this means that it can be solved by an iteratively reweighted problem of the form arg min w nllw j j in and nagarajan the following procedure for setting the penalty terms is suggested on a convex bound to the penalty function. we initialize with j and then at chapter sparse linear models iteration t compute j by iterating the following equation a few j xj j xt d n we see that the new penalty j depends on all the old weights. this is quite different from the adaptive lasso method of section to understand this difference consider the noiseless case where and assume d n solutions which perfectly reconstruct the data xw y and which in this case there are have sparsity n these are called basic feasible solutions or bfs. what we want are solutions that satsify xw y but which are much sparser than this. suppose the method has found a bfs. we do not want to increase the penalty on a weight just because it is small in adaptive lasso since that will just reinforce our current local optimum. instead we want to increase the penalty on a weight if it is small and if we have n the covariance term has this effect if w is a bfs this matrix will be full rank so the penalty will not increase much but if w is sparser than n the matrix will not be full rank so the penalties associated with zero-valued coefficients will increase thus reinforcing this solution and nagarajan j ard for logistic regression now consider binary logistic regression pyx w berysigmwt x using the same gaussian prior pw a we can no longer use em to estimate since the gaussian prior is not conjugate to the logistic likelihood so the e step cannot be done exactly. one approach is to use a variational approximation to the e step as discussed in section a simpler approach is to use a laplace approximation section in the e step. we can then use this approximation inside the same em procedure as before except we no longer need to update note however that this is not guaranteed to converge. an alternative is to use the techniques from section in this case we can use exact methods to compute the inner weighted regularized logistic regression problem and no approximations are required. sparse coding so far we have been concentrating on sparse priors for supervised learning. in this section we discuss how to use them for unsupervised learning. in section we discussed ica which is like pca except it uses a non-gaussian prior for the latent factors zi. if we make the non-gaussian prior be sparsity promoting such as a laplace distribution we will be approximating each observed vector xi as a sparse combination of basis vectors of w note that the sparsity pattern by zi changes from data case to data case. if we relax the constraint that w is orthogonal we get a method called the algorithm in and nagarajan is equivalent to a single iteration of equation however since the equation is cheap to compute on time it is worth iterating a few times before solving the more expensive problem. sparse coding method pca fa ica sparse coding sparse pca sparse mf pzi gauss gauss non-gauss laplace gauss laplace pw w orthogonal laplace maybe laplace yes no yes no no table summary of various latent factor models. a dash in the pw column means we are performing ml parameter estimation rather than map parameter estimation. summary of abbreviations pca principal components analysis fa factor analysis ica independent components analysis mf matrix factorization. sparse coding. in this context we call the factor loading matrix w a dictionary each column is referred to as an in view of the sparse representation it is common for l d in which case we call the representation overcomplete. in sparse coding the dictionary can be fixed or learned. if it is fixed it is common to use a wavelet or dct basis since many natural signals can be well approximated by a small number of such basis functions. however it is also possible to learn the dictionary by maximizing the likelihood log pdw n log zi we discuss ways to optimize this below and then we present several interesting applications. do not confuse sparse coding with sparse pca e.g. et al. journee et al. this puts a sparsity promoting prior on the regression weights w whereas in sparse coding we put a sparsity promoting prior on the latent factors zi. of course the two techniques can be combined we call the result sparse matrix factorization although this term is nonstandard. see table for a summary of our terminology. learning a sparse coding dictionary since equation is a hard objective to maximize approximation it is common to make the following log pdw max zi log n log pzi if pzi is laplace we can rewrite the nll as nllw z it is common to denote the dictionary by d and to denote the latent factors by i. however we will stick with the w and zi notation. chapter sparse linear models to prevent w from becoming arbitrarily large it is common to constrain the norm of its columns to be less than or equal to let us denote this constraint set by c r d l s.t. wt j wj then we want to solve minw cz rn l nllw z. for a fixed zi the optimization over w is a simple least squares problem. and for a fixed dictionary w the optimization problem over z is identical to the lasso problem for which many fast algorithms exist. this suggests an obvious iterative optimization scheme in which we alternate between optimizing w and z. called this kind of approach an analysis-synthesis loop where estimating the basis w is the analysis phase and estimating the coefficients z is the synthesis phase. in cases where this is too slow more sophisticated algorithms can be used see e.g. et al. a variety of other models result in an optimization problem that is similar to equation for example non-negative matrix factorization or nmf and tapper lee and seung requires solving an objective of the form s.t. w zi min w cz rl n that this has no hyper-parameters to tune. the intuition behind this constraint is that the learned dictionary may be more interpretable if it is a positive sum of positive parts rather than a sparse sum of atoms that may be positive or negative. of course we can combine nmf with a sparsity promoting prior on the latent factors. this is called non-negative sparse coding alternatively we can drop the positivity constraint but impose a sparsity constraint on both the factors zi and the dictionary w. we call this sparse matrix factorization. to ensure strict convexity we can use an elastic net type penalty on the weights et al. resulting in s.t. min wz there are several related objectives one can write down. for example we can replace the lasso nll with group lasso or fused lasso et al. we can also use other sparsity-promoting priors besides the laplace. for example et al. propose a model in which the latent factors zi are made sparse using the binary mask model of section each bit of the mask can be generated from a bernoulli distribution with parameter which can be drawn from a beta distribution. alternatively we can use a non-parametric prior such as the beta process. this allows the model to use dictionaries of unbounded size rather than having to specify l in advance. one can perform bayesian inference in this model using e.g. gibbs sampling or variational bayes. one finds that the effective size of the dictionary goes down as the noise level goes up due to the bayesian occam s razor. this can prevent overfitting. see et al. for details. results of dictionary learning from image patches one reason that sparse coding has generated so much interest recently is because it explains an interesting phenomenon in neuroscience. in particular the dictionary that is learned by applying sparse coding figure illustration of the filters learned by various methods when applied to natural image patches. ica. figure generated by icabasisdemo patch is first centered and normalized to unit norm. kindly provided by aapo hyvarinen. sparse pca with low sparsity on weight matrix. sparse pca with high sparsity on weight matrix. figure generated by sparsedictdemo written by julien mairal. pca. non-negative matrix factorization. sparse coding. chapter sparse linear models sparse coding to patches of natural images consists of basis vectors that look like the filters that are found in simple cells in the primary visual cortex of the mammalian brain and field in particular the filters look like bar and edge detectors as shown in figure this example the parameter was chosen so that the number of active basis functions components of zi is about interestingly using ica gives visually similar results as shown in figure by contrast applying pca to the same data results in sinusoidal gratings as shown in figure these do not look like cortical cell response it has therefore been conjectured that parts of the cortex may be performing sparse coding of the sensory input the resulting latent representation is then further processed by higher levels of the brain. figure shows the result of using nmf and figure show the results of sparse pca as we increase the sparsity of the basis vectors. compressed sensing imagine that instead of observing the data x r although it is interesting to look at the dictionaries learned by sparse coding it is not necessarily very useful. however there are some practical applications of sparse coding which we discuss below. d we observe a low-dimensional projection m r is a m d matrix m d and is a noise term of it y rx where y r gaussian. we assume r is a known sensing matrix corresponding to different linear projections of x. for example consider an mri scanner each beam direction corresponds to a vector encoded as a row in r. figure illustrates the modeling assumptions. our goal is to infer pxy r. how can we hope to recover all of x if we do not measure all of x? the answer is we can use bayesian inference with an appropriate prior that exploits the fact that natural signals can be expressed as a weighted combination of a small number of suitably chosen basis functions. that is we assume x wz where z has a sparse prior and w is suitable dictionary. this is called compressed sensing or compressive sensing et al. baruniak candes and wakin bruckstein et al. for cs to work it is important to represent the signal in the right basis otherwise it will not be sparse. in traditional cs applications the dictionary is fixed to be a standard form such as wavelets. however one can get much better performance by learning a domain-specific dictionary using sparse coding et al. as for the sensing matrix r it is often chosen to be a random matrix for reasons explained in and wakin however one can get better performance by adapting the projection matrix to the dictionary and nickish chang et al. image inpainting and denoising suppose we have an image which is corrupted in some way e.g. by having text or scratches sparsely superimposed on top of it as in figure we might want to estimate the underlying the reason pca discovers sinusoidal grating patterns is because it is trying to model the covariance of the data which in the case of image patches is translation invariant. this means cov y for some function f where ix y is the image intensity at location y. one can show et al. that the eigenvectors of a matrix of this kind are always sinusoids of different phases i.e. pca discovers a fourier basis. f sparse coding z x y w r figure schematic dgm for compressed sensing. we observe a low dimensional measurement y generated by passing x through a measurement matrix r and possibly subject to observation noise with variance we assume that x has a sparse decomposition in terms of the dictionary w and the latent variables z. the parameter controlls the sparsity level. figure an example of image inpainting using sparse coding. left original image. right reconstruction. source figure of et al. used with kind permission of julien mairal. clean image. this is called image inpainting. one can use similar techniques for image denoising. we can model this as a special kind of compressed sensing problem. the basic idea is as follows. we partition the image into overlapping patches yi and concatenate them to form y. we define r so that the i th row selects out patch i. now define v to be the visible components of y and h to be the hidden components. to perform image inpainting we just compute pyhyv where are the model parameters which specify the dictionary w and the sparsity level of z. we can either learn a dictionary offline from a database of images or we can learn a dictionary just for this image based on the non-corrupted patches. from undamaged color patches in the mega-pixel image. figure shows this technique in action. the dictionary size atoms was learned an alternative approach is to use a graphical model the fields of experts model chapter sparse linear models and black which directly encodes correlations between neighboring image patches rather than using a latent variable model. unfortunately such models tend to be computationally more expensive. exercises exercise partial derivative of the rss define rssw a. show that wk rssw ak kwk ck ik xikyi wt kxi k ck where w k w without component k xi k is xi without component k and rk y wt kx k is the residual due to using all the features except feature k. hint partition the weights into those involving k and those not involving k. rssw then b. show that if wk wk xt hence when we sequentially add features the optimal weight for feature k is computed by computing orthogonally projecting xk onto the current residual. exercise derivation of m step for eb for linear regression derive equations and hint the following identity should be useful xt x xt x a a x a a xt x a x a a exercise derivation of fixed point updates for eb for linear regression derive equations and hint the easiest way to derive this result is to rewrite log pd as in equation this is exactly equivalent since in the case of a gaussian prior and likelihood the posterior is also gaussian so the laplace approximation is exact. in this case we get log pd n log j log j mt am log d the rest is straightforward algebra. sparse coding exercise marginal likelihood for linear regression suppose we use a g-prior of the form gxt x show that equation simplifies to pd g s t y g d s x yt x y exercise reducing elastic net to lasso define and y x c where c and y x y x c show arg min carg min i.e. and hence that one can solve an elastic net problem using a lasso solver on modified data. exercise shrinkage in linear regression jaakkola. consider performing linear regression with an orthonormal design matrix so for each column k and xt figure plots wk vs ck xk the correlation of feature k with the response for different esimation methods ordinary least squares ridge regression with parameter and lasso with parameter so we can estimate each parameter wk separately. a. unfortunately we forgot to label the plots. which method does the solid dotted and dashed line correspond to? hint see section b. what is the value of c. what is the value of exercise prior for the bernoulli rate parameter in the spike and slab model consider the model in section suppose we put a prior on the sparsity rates j beta derive an expression for p after integrating out the j s. discuss some advantages and disadvantages of this approach compared to assuming j for fixed chapter sparse linear models k w c k figure plot of wk vs amount of correlation ck for three different estimators. exercise deriving e step for gsm prior show that e j where log pwj and pwj intn j j j hint j n exp j j j j exp j j j dwjn d j hint d dwj pwj pwj d dwj log pwj exercise em for sparse probit regression with laplace prior derive an em algorithm for fitting a binary probit classifier using a laplace prior on the weights. you get stuck see ding and harrison exercise gsm representation of group lasso j ga ignoring the grouping issue for now. the marginal distribution consider the prior induced on the weights by a gamma mixing distribution is called the normal gamma distribution and is sparse coding given by ngwj z j n k j j where k is the modified bessel function of the second kind besselk function in matlab. now suppose we have the following prior on the variances j g p p p ga j g the corresponding marginal for each group of weights has the form pwg g dg k g dg ug where ug j g gj now suppose g so g that the resulting map estimate is equivalent to group lasso. conveniently we have k exp z. show exercise projected gradient descent for regularized least squares consider the bpdn problem argmin rss by using the split variable trick introducted in section by defining rewrite this as a quadratic program with a simple bound constraint. then sketch how to use projected gradient descent to solve this problem. you get stuck consult et al. exercise subderivative of the hinge loss function let f x be the hinge loss function where z. what are f f and f exercise lower bounds to convex functions let f be a convex function. explain how to find a global affine lower bound to f at an arbitrary point x domf kernels introduction so far in this book we have been assuming that each object that we wish to classify or cluster or process in anyway can be represented as a fixed-size feature vector typically of the form xi r d. however for certain kinds of objects it is not clear how to best represent them as fixed-sized feature vectors. for example how do we represent a text document or protein sequence which can be of variable length? or a molecular structure which has complex geometry? or an evolutionary tree which has variable size and shape? one approach to such problems is to define a generative model for the data and use the inferred latent representation andor the parameters of the model as features and then to plug these features in to standard methods. for example in chapter we discuss deep learning which is essentially an unsupervised way to learn good feature representations. another approach is to assume that we have some way of measuring the similarity between objects that doesn t require preprocessing them into feature vector format. for example when be some comparing strings we can compute the edit distance between them. let measure of similarity between objects x x where x is some abstract space we will call a kernel function. note that the word kernel has several meanings we will discuss a different interpretation in section in this chapter we will discuss several kinds of kernel functions. we then describe some algorithms that can be written purely in terms of kernel function computations. such methods can be used when we don t have access to choose not to look at the inside of the objects x that we are processing. kernel functions we define a kernel function to be a real-valued function of two arguments x x typically the function is symmetric give several examples below. r for x and non-negative so it can be interpreted as a measure of similarity but this is not required. we rbf kernels chapter kernels the squared exponential kernel kernel or gaussian kernel is defined by exp if is diagonal this can be written as j exp we can interpret the j as defining the characteristic length scale of dimension j. if j if is the corresponding dimension is ignored hence this is known as the ard kernel. spherical we get the isotropic kernel exp here is known as the bandwidth. equation is an example of a a radial basis function or rbf kernel since it is only a function of kernels for comparing documents when performing document classification or retrieval it is useful to have a way of comparing two documents xi and if we use a bag of words representation where xij is the number of times words j occurs in document i we can use the cosine similarity which is defined by xt i this quantity measures the cosine of the angle between xi and when interpreted as vectors. since xi is a count vector hence non-negative the cosine similarity is between and where means the vectors are orthogonal and therefore have no words in common. unfortunately this simple method does not work very well for two main reasons. first if xi has any word in common with it is deemed similar even though some popular words such as the or and occur in many documents and are therefore not discriminative. are known as stop words. second if a discriminative word occurs many times in a document the similarity is artificially boosted even though word usage tends to be bursty meaning that once a word is used in a document it is very likely to be used again section fortunately we can significantly improve performance using some simple preprocessing. the idea is to replace the word count vector with a new feature vector called the tf-idf representation which stands for term frequency inverse document frequency we define this as follows. first the term frequency is defined as a log-transform of the count this reduces the impact of words that occur many times within one document. second the inverse document frequency is defined as tfxij xij idfj log n ixij kernel functions where n is the total number of documents and the denominator counts how many documents contain term j. finally we define tf-idfxi idfjv are several other ways to define the tf and idf terms see et al. for details. we then use this inside the cosine similarity measure. that is our new kernel has the form where tf-idfx. this gives good results for information retrieval et al. a probabilistic interpretation of the tf-idf kernel is given in mercer definite kernels some methods that we will study require that the kernel function satisfy the requirement that the gram matrix defined by k xn xn be positive definite for any set of inputs we call such a kernel a mercer kernel or positive definite kernel. it can be shown and smola that the gaussian kernel is a mercer kernel as is the cosine similarity kernel and heilman the importance of mercer kernels is the following result known as mercer s theorem. if the gram matrix is positive definite we can compute an eigenvector decomposition of it as follows k ut u where is a diagonal matrix of eigenvalues i now consider an element of k kij uit let us define uj ui. then we can write kij thus we see that the entries in the kernel matrix can be computed by performing an inner product of some feature vectors that are implicitly defined by the eigenvectors u. in general if the kernel is mercer then there exists a function mapping x x to r d such that where depends on the eigen functions of d is a potentially infinite dimensional space. rm where r one can show that the corresponding feature vector will contain all terms up to degree m for example if m r and x r for example consider the polynomial kernel xt we have xt chapter kernels this can be written as where so using this kernel is equivalent to working in a dimensional feature space. in the case of a gaussian kernel the feature map lives in an infinite dimensional space. in such a case it is clearly infeasible to explicitly represent the feature vectors. an example of a kernel that is not a mercer kernel is the so-called sigmoid kernel defined by tanh xt r that this uses the tanh function even though it is called a sigmoid kernel. this kernel was inspired by the multi-layer perceptron section but there is no real reason to use it. a true neural net kernel which is positive definite see section in general establishing that a kernel is a mercer kernel is difficult and requires techniques from functional analysis. however one can show that it is possible to build up new mercer kernels from simpler ones using a set of standard rules. for example if and are both mercer so is see e.g. and smola for details. linear kernels deriving the feature vector implied by a kernel is in general quite difficult and only possible if the kernel is mercer. however deriving a kernel from a feature vector is easy we just use if x we get thelinear kernel defined by xt this is useful if the original data is already high dimensional and if the original features are individually informative e.g. a bag of words representation where the vocabulary size is large or the expression level of many genes. in such a case the decision boundary is likely to be representable as a linear combination of the original features so it is not necessary to work in some other feature space. of course not all high dimensional problems are linearly separable. for example images are high dimensional but individual pixels are not very informative so image classification typically requires non-linear kernels e.g. section matern kernels the matern kernel which is commonly used in gaussian process regression section has the following form r k r kernel functions where r and k is a modified bessel function. as this approaches the se kernel. if the kernel simplifies to exp if d and we use this kernel to define a gaussian process chapter we get the ornstein-uhlenbeck process which describes the velocity of a particle undergoing brownian motion corresponding function is continuous but not differentiable and hence is very jagged string kernels consider two strings x and the real power of kernels arises when the inputs are structured objects. as an example we now describe one way of comparing two variable length strings using a string kernel. we follow the presentation of and williams and et al. of lengths d d each defined over the alphabet a. for example consider two amino acid sequences defined over the letter alphabet a r n d c e q g h i l k m f p s t w y v let x be the following sequence of length iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv erlfknlslikkyidgqkkkcgeerrrvnqfldylqeflgvmntewi and let be the following sequence of length phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaerlqenlqayrtfhvlla rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk lwglkvlqelsqwtvrsihdlrfisshqtgip these strings have the substring lqe in common. we can define the similarity of two strings to be the number of substrings they have in common. more formally and more generally let us say that s is a substring of x if we can write x usv for some empty strings u s and v. now let sx denote the number of times that substring s appears in string x. we define the kernel between two strings x and as s a ws sx where ws and a is the set of all strings any length from the alphabet a is known as the kleene star operator. this is a mercer kernel and be computed in ox time certain settings of the weights using suffix trees et al. vishwanathan and smola shawe-taylor and cristianini there are various cases of interest. if we set ws for we get a bag-of-characters kernel. this defines to be the number of times each character in a occurs in x. if we require s to be bordered by white-space we get a bag-of-words kernel where counts how many times each possible word occurs. note that this is a very sparse vector since most words chapter kernels optimal partial matching matching figure of kristen grauman. illustration of a pyramid match kernel computed from two images. used with kind permission if we only consider strings of a fixed length k we get the k-spectrum will not be present. kernel. this has been used to classify proteins into scop superfamilies et al. for example if k we have lqex and for the two strings above. various extensions are possible. for example we can allow character mismatches et al. and we can generalize string kernels to compare trees as described in and duffy this is useful for classifying ranking parse trees evolutionary trees etc. pyramid match kernels in computer vision it is common to create a bag-of-words representation of an image by computing a feature vector using sift from a variety of points in the image commonly chosen by an interest point detector. the feature vectors at the chosen places are then vector-quantized to create a bag of discrete symbols. one way to compare two variable-sized bags of this kind is to use a pyramid match kernel and darrell the basic idea is illustrated in figure each feature set is mapped to a multi-resolution histogram. these are then compared using weighted histogram intersection. it turns out that this provides a good approximation to the similarity measure one would obtain by performing an optimal bipartite match at the finest spatial resolution and then summing up pairwise similarities between matched points. however the histogram method is faster and is more robust to missing and unequal numbers of points. this is a mercer kernel. kernel functions kernels derived from probabilistic generative models suppose we have a probabilistic generative model of feature vectors px then there are several ways we can use this model to define kernel functions and thereby make the model suitable for discriminative tasks. we sketch two approaches below. probability product kernels one approach is to define a kernel as follows pxxi pxxj dx xj where and pxxi is often approximated by px where is a parameter estimate computed using a single data vector. this is called a probability product kernel et al. although it seems strange to fit a model to a single data point it is important to bear in mind that the fitted model is only being used to see how similar two objects are. in particular if we fit the model to xi and then the model thinks xj is likely this means that xi and xj are similar. for example suppose px where is fixed. if and we use xi and xj we find et al. that xj exp which is to a constant factor the rbf kernel. it turns out that one can compute equation for a variety of generative models including ones with latent variables such as hmms. this provides one way to define kernels on variable length sequences. furthermore this technique works even if the sequences are of real-valued vectors unlike the string kernel in section see et al. for further details. fisher kernels a more efficient way to use generative models to define kernels is to use a fisher kernel and haussler which is defined as follows gxt f gx log px f log px where g is the gradient of the log likelihood or score vector evaluated at the mle and f is the fisher information matrix which is essentially the hessian note that is a function of all the data so the similarity of x and of all the data as well. also note that we only have to fit one model. is computed in the context the intuition behind the fisher kernel is the following let gx be the direction parameter space in which x would like the parameters to move so as to maximize its own chapter kernels rbf prototypes figure xor truth table. fitting a linear logistic regression classifier using degree polynomial expansion. same model but using an rbf kernel with centroids specified by the black crosses. figure generated by logregxordemo. likelihood call this the directional gradient. then we say that two vectors x and are similar if their directional gradients are similar wrt the the geometry encoded by the curvature of the likelihood function section interestingly it was shown in et al. that the string kernel of section is equivalent to the fisher kernel derived from an l th order markov chain section also it was shown in that a kernel defined by the inner product of tf-idf vectors is approximately equal to the fisher kernel for a certain generative model of text based on the compound dirichlet multinomial model using kernels inside glms in this section we discuss one simple way to use kernels for classification and regression. we will see other approaches later. kernel machines we define a kernel machine to be a glm where the input feature vector has the form k where k x are a set of k centroids. if is an rbf kernel this is called an rbf network. we discuss ways to choose the k parameters below. we will call equation a kernelised feature vector. note that in this approach the kernel need not be a mercer kernel. we can use the kernelized feature vector for logistic regression by defining pyx berwt this provides a simple way to define a non-linear decision boundary. as an example consider the data coming from the exclusive or or xor function. this is a binaryvalued function of two binary inputs. its truth table is shown in figure in figure we have show some data labeled by the xor function but we have jittered the points to make the picture we see we cannot separate the data even using a degree polynomial. jittering is a common visualization trick in statistics wherein points in a plotdisplay that would otherwise land on top of each other are dispersed with uniform additive noise. using kernels inside glms x figure rbf basis in left column fitted function. middle column basis functions evaluated on a grid. right column design matrix. top to bottom we show different bandwidths figure generated by linregrbfdemo. however using an rbf kernel and just prototypes easily solves the problem as shown in figure we can also use the kernelized feature vector inside a linear regression model by defining pyx n for example figure shows a data set fit with k uniformly spaced rbf prototypes but with the bandwidth ranging from small to large. small values lead to very wiggly functions since the predicted function value will only be non-zero for points x that are close to one of the prototypes k. if the bandwidth is very large the design matrix reduces to a constant matrix of s since each point is equally close to every prototype hence the corresponding function is just a straight line. rvms and other sparse vector machines the main issue with kernel machines is how do we choose the centroids k? if the input is low-dimensional euclidean space we can uniformly tile the space occupied by the data with prototypes as we did in figure however this approach breaks down in higher numbers if k r d we can try to perform of dimensions because of the curse of dimensionality. numerical optimization of these parameters e.g. or we can use mcmc inference e.g. et al. kohn et al. but the resulting objective function posterior is highly multimodal. furthermore these techniques is hard to extend to structured input spaces where kernels are most useful. another approach is to find clusters in the data and then to assign one prototype per cluster chapter kernels center clustering algorithms just need a similarity metric as input. however the regions of space that have high density are not necessarily the ones where the prototypes are most useful for representing the output that is clustering is an unsupervised task that may not yield a representation that is useful for prediction. furthermore there is the need to pick the number of clusters. a simpler approach is to make each example xi be a prototype so we get xn now we see d n so we have as many parameters as data points. however we can use any of the sparsity-promoting priors for w discussed in chapter to efficiently select a subset of the training exemplars. we call this a sparse vector machine. the most natural choice is to use regularization et al. that in the multi-class case it is necessary to use group lasso since each exemplar is associated with c weights one per class. we call this which stands for vector machine by analogy we define the use of an regularizer to be a or vector machine this of course will not be sparse. we can get even greater sparsity by using ardsbl resulting in a method called the relevance vector machine or rvm one can fit this model using generic ardsbl algorithms although in practice the most common method is the greedy algorithm in and faul is the algorithm implemented in mike tipping s code which is bundled with pmtk. another very popular approach to creating a sparse kernel machine is to use a support vector machine or svm. this will be discussed in detail in section rather than using a sparsity-promoting prior it essentially modifies the likelihood term which is rather unnatural from a bayesian point of view. nevertheless the effect is similar as we will see. in figure we compare rvm and an svm using the same rbf kernel on a binary classification problem in for simplicity was chosen by hand for and for rvms the parameters are estimated using empirical bayes and for the svm we use cv to pick c since svm performance is very sensitive to this parameter section we see that all the methods give similar performance. however rvm is the sparsest hence fastest at test time then and then svm. rvm is also the fastest to train since cv for an svm is slow. is despite the fact that the rvm code is in matlab and the svm code is in c. this result is fairly typical. in figure we compare rvm and an svm using an rbf kernel on a regression problem. again we see that predictions are quite similar but rvm is the sparsest then then svm. this is further illustrated in figure the kernel trick rather than defining our feature vector in terms of kernels xn we can instead work with the original feature vectors x but modify the algorithm so that it replaces all inner products of the form with a call to the kernel function this is called the kernel trick. it turns out that many algorithms can be kernelized in this way. we give some examples below. note that we require that the kernel be a mercer kernel for this trick to work. the kernel trick rvm svm figure example of non-linear binary classification using an rbf kernel with bandwidth with rvm. svm with c chosen by cross validation. black circles denote the support vectors. figure generated by kernelbinaryclassifdemo. with kernelized nearest neighbor classification recall that in a classifier we just need to compute the euclidean distance of a test vector to all the training points find the closest one and look up its label. this can be kernelized by observing that xi this allows us to apply the nearest neighbor classifier to structured data objects. kernelized k-medoids clustering k-means clustering uses euclidean distance to measure dissimilarity which is not always appropriate for structured objects. we now describe how to develop a kernelized chapter kernels svm rvm figure example of kernel based regression on the noisy sinc function using an rbf kernel with bandwidth rvm. svm regression with c chosen by cross validation and default for svmlight. red circles denote the retained training exemplars. figure generated by kernelregrdemo. with with version of the algorithm. the first step is to replace the k-means algorithm with the k-medoids algorothm. this is similar to k-means but instead of representing each cluster s centroid by the mean of all data vectors assigned to this cluster we make each centroid be one of the data vectors themselves. thus we always deal with integer indexes rather than data objects. we assign objects to their closest centroids as before. when we update the centroids we look at each object that belongs to the cluster and measure the sum of its distances to all the others in the same cluster we then pick the one which has the smallest such sum mk argmin izik di the kernel trick weights for weights for weights for rvm weights for svm figure coefficient vectors of length n for the models in figure figure generated by kernelregrdemo. where di this takes k work per cluster whereas k-means takes onkd to update each cluster. the pseudo-code is given in algorithm this method can be modified to derive a classifier by computing the nearest medoid for each class. this is known as nearest medoid classification et al. di this algorithm can be kernelized by using equation to replace the distance computation chapter kernels algorithm k-medoids algorithm initialize as a random subset of size k from n repeat zi argmink di mk for i mk argminizik di for k k until converged kernelized ridge regression applying the kernel trick to distance-based methods was straightforward. it is not so obvious how to apply it to parametric models such as ridge regression. however it can be done as we now explain. this will serve as a good warm up for studying svms. the primal problem let x r want to minimize d be some feature vector and x be the corresponding n d design matrix. we jw xwt xw the optimal solution is given by w x id y i xixt i id y the dual problem equation is not yet in the form of inner products. however using the matrix inversion lemma we rewrite the ridge estimate as follows w xt in which takes on n time to compute. this can be advantageous if d is large. furthermore we see that we can partially kernelize this by replacing xxt with the gram matrix k. but what about the leading xt term? let us define the following dual variables in then we can rewrite the primal variables as follows w xt ixi this tells us that the solution vector is just a linear sum of the n training vectors. when we plug this in at test time to compute the predictive mean we get f wt x ixt i x i xi the kernel trick figure visualization of the first kernel principal component basis functions derived from some data. we use an rbf kernel with figure generated by kpcascholkopf written by bernhard scholkopf. so we have succesfully kernelized ridge regression by changing from primal to dual variables. this technique can be applied to many other linear models such as logistic regression. computational cost the cost of computing the dual variables is on whereas the cost of computing the primal variables w is hence the kernel method can be useful in high dimensional settings even if we only use a linear kernel the svd trick in equation however prediction using the dual variables takes on d time while prediction using the primal variables only takes od time. we can speedup prediction by making sparse as we discuss in section kernel pca in section we saw how we could compute a low-dimensional linear embedding of some data using pca. this required finding the eigenvectors of the sample covariance matrix s chapter kernels xixt i x. however we can also compute pca by finding the eigenvectors n of the inner product matrix xxt as we show below. this will allow us to produce a nonlinear embedding using the kernel trick a method known as kernel pca et al. first let u be an orthogonal matrix containing the eigenvectors of xxt with corresponding eigenvalues in by definition we have u pre-multiplying by xt gives xxt u u from which we see that the eigenvectors of xt x hence of s are v xt u with eigenvalues given by as before. however these eigenvectors are not normalized since j uj j. so the normalized eigenvectors are given by vpca xt u ut j xxt uj jut this is a useful trick for regular pca if d n since xt x has size d d whereas xxt has size n n it will also allow us to use the kernel trick as we now show. now let k xxt be the gram matrix. recall from mercer s theorem that the use of a kernel implies some underlying feature space so we are implicitly replacing xi with i. let be the corresponding design matrix and s i be the corresponding n covariance matrix in feature space. the eigenvectors are given by vkpca t u where u and contain the eigenvectors and eigenvalues of k. of course we can t actually compute vkpca since i is potentially infinite dimensional. however we can compute the projection of a test vector x onto the feature space as follows i i t t vkpca t u kt u where k xn there is one final detail to worry about. so far we have assumed the projected data has zero mean which is not the case in general. we cannot simply subtract off the mean in feature space. however there is a trick we can use. define the centered feature vector as i the gram matrix of the centered feature vectors is given by n kij t i j i j n t i k t n t j k n xl t k l xj n xk n xk n this can be expressed in matrix notation as follows n n k hkh where h i is the centering matrix. we can convert all this algebra into the pseudocode shown in algorithm whereas linear pca is limited to using l d components in kpca we can use up to n components since the rank of is n d is the infinite dimensionality of embedded feature vectors. figure gives an example of the method applied to some d dimensional data using an rbf kernel. we project points in the unit grid onto the first where d the kernel trick n algorithm kernel pca input k of size n n k of size n n num. latent dimensions l o k k ok ko oko k for i do i vi ui o n k k o k k o o k o z k v pca kpca figure visualization of some data. generated by based on code by l.j.p. van der maaten. pca projection. kernel pca projection. figure components and visualize the corresponding surfaces using a contour plot. we see that the first two component separate the three clusters and following components split the clusters. although the features learned by kpca can be useful for classification et al. they are not necessarily so useful for data visualization. for example figure shows the projection of the data from figure onto the first principal bases computed using pca and kpca. obviously pca perfectly represents the data. kpca represents each cluster by a different line. of course there is no need to project data back into so let us consider a different data set. we will use a dimensional data set representing the three known phases of flow in an oil pipeline. data which is widely used to compare data visualization methods is synthetic and comes from and james we project this into using pca and kpca an rbf kernel. the results are shown in figure if we perform nearest neighbor classification in the low-dimensional space kpca makes errors and pca makes chapter kernels figure representation of dimensional oil flow data. the different colorssymbols represent the phases of oil flow. pca. kernel pca with gaussian kernel. compare to figure from figure of used with kind permission of neil lawrence. nevertheless the kpca projection is rather unnatural. how to make kernelized versions of probabilistic pca. in section we will discuss note that there is a close connection between kernel pca and a technique known as multidimensional scaling or mds. this methods finds a low-dimensional embedding such that euclidean distance in the embedding space approximates the original dissimilarity matrix. see e.g. for details. support vector machines in section we saw one way to derive a sparse kernel machine namely by using a glm with kernel basis functions plus a sparsity-promoting prior such as or ard. an alternative approach is to change the objective function from negative log likelihood to some other loss in particular consider the regularized empirical function as we discussed in section risk function jw lyi yi where yi wt xi moment. defined in equation this is equivalent to logistic regression. far this is in the original feature space we introduce kernels in a if l is quadratic loss this is equivalent to ridge regression and if l is the log-loss in the ridge regression case we know that the solution to this has the form w x y and plug-in predictions take the form wt x. as we saw in section i we can rewrite these equations in a way that only involves inner products of the form xt which we can replace by calls to a kernel function this is kernelized but not sparse. however if we replace the quadratic log-loss with some other loss function to be explained below we can ensure that the solution is sparse so that predictions only depend on a subset of the training data known as support vectors. this combination of the kernel trick plus a modified loss function is known as a support vector machine or svm. this technique was support vector machines insensitive huber yx y y y x figure illustration of huber and loss functions where figure generated illustration of the used in svm regression. points above the tube have by huberlossdemo. i and i points inside the tube have i i points below the tube have i and i based on figure of originally designed for binary classification but can be extended to regression and multi-class classification as we explain below. note that svms are very unnatural from a probabilistic point of view. first they encode sparsity in the loss function rather than the prior. second they encode kernels by using an algorithmic trick rather than being an explicit part of the model. finally svms do not result in probabilistic outputs which causes various difficulties especially in the multi-class classification setting section for details. it is possible to obtain sparse probabilistic multi-class kernel-based classifiers which work as well or better than svms using techniques such as the or rvm discussed in section however we include a discussion of svms despite their non-probabilistic nature for two main reasons. first they are very popular and widely used so all students of machine learning should know about them. second they have some computational advantages over probabilistic methods in the structured output case see section svms for regression the problem with kernelized ridge regression is that the solution vector w depends on all the training inputs. we now seek a method to produce a sparse estimate. vapnik et al. proposed a variant of the huber loss function called the epsilon insensitive loss function defined by ly y y if y otherwise this means that any point lying inside an around the prediction is not penalized as in figure the corresponding objective function is usually written in the following form j c lyi yi chapter kernels where yi f t xi and c is a regularization constant. this objective is convex and unconstrained but not differentiable because of the absolute value function in the loss term. as in section where we discussed the lasso problem there are several possible algorithms we could use. one popular approach is to formulate the problem as a constrained in particular we introduce slack variables to represent the degree to optimization problem. which each point lies outside the tube yi f yi f i i given this we can rewrite the objective as follows j c i i this is a quadratic function of w and must be minimized subject to the linear constraints i this is a in equations as well as the positivity constraints standard quadratic program in d variables. i and one can show e.g. and smola that the optimal solution has the form w ixi i where i furthermore it turns out that the vector is sparse because we don t care about errors which are smaller than the xi for which i are called the support vectors thse are points for which the errors lie on or outside the tube. once the model is trained we can then make predictions using yx wt x plugging in the definition of w we get yx ixt i x finally we can replace xt i x with x to get a kernelized solution i yx i x i svms for classification we now discuss how to apply svms to classification. we first focus on the binary case and then discuss the multi-class case in section hinge loss in section we showed that the negative log likelihood of a logistic regression model l nlly log pyx w e y support vector machines was a convex upper bound on the risk of a binary classifier where f t x is the log odds ratio and we have assumed the labels are y rather than in this section we replace the nll loss with the hinge loss defined as lhingey y y here f is our confidence in choosing label y however it need not have any probabilistic semantics. see figure for a plot. we see that the function looks like a door hinge hence its name. the overall objective has the form min c yif once again this is non-differentiable because of the max term. however by introducing slack variables i one can show that this is equivalent to solving min c i s.t. i yixt i w i i this is a quadratic program in n d variables subjet to on constraints. we can eliminate the primal variables w and i and just solve the n dual variables which correspond to the lagrange multipliers for the constraints. standard solvers take on time. however specialized algorithms which avoid the use of generic qp solvers have been developed for this problem such as the sequential minimal optimization or smo algorithm in practice this can take on however even this can be too slow if n is large. in such settings it is common to use linear svms which take on time to train bottou et al. one can show that the solution has the form w ixi i where i iyi and where is sparse of the hinge loss. the xi for which i are called support vectors these are points which are either incorrectly classified or are classified correctly but are on or inside the margin disuss margins below. see figure for an illustration. at test time prediction is done using yx sgnf sgn wt x yx sgn i x using equation and the kernel trick we have this takes osd time to compute where s n is the number of support vectors. this depends on the sparsity level and hence on the regularizer c. chapter kernels figure right a separating hyper-plane with small margin. illustration of the large margin principle. left a separating hyper-plane with large margin. y y y w x r f x y y y figure illustration of the geometry of a linear decision boundary in a point x is classified as belonging in decision region if f otherwise it belongs in decision region here f is known as a discriminant function. the decision boundary is the set of points such that f w is a vector which is perpendicular to the decision boundary. the term controls the distance of the decision boundary from the origin. the signed distance of x from its orthogonal projection onto the decision boundary x is given by f based on figure of illustration of the soft margin principle. points with circles around them are support vectors. we also indicate the value of the corresponding slack variables. based on figure of support vector machines the large margin principle in this section we derive equation form a completely different perspective. recall that our goal is to derive a discriminant function f which will be linear in the feature space implied by the choice of kernel. consider a point x in this induced space. referring to figure we see that x x r w where r is the distance of x from the decision boundary whose normal vector is w and x is the orthogonal projection of x onto this boundary. hence f t x x wt w now f so t x hence f r wt w we would like to make this distance r f as large as possible for reasons illustrated in figure in particular there might be many lines that perfectly separate the training data if we work in a high dimensional feature space but intuitively the best one to pick is the one that maximizes the margin i.e. the perpendicular distance to the closest point. in addition we want to ensure each point is on the correct side of the boundary hence we want f so our objective becomes wt w and r f yiwt xi n min max note that by rescaling the parameters using w kw and we do not change the distance of any point to the boundary since the k factor cancels out when we divide by therefore let us define the scale factor such that yifi for the point that is closest to the decision boundary. we therefore want to optimize min s.t. yiwt xi i fact of is added for convenience and doesn t affect the optimal parameters. the constraint says that we want all points to be on the correct side of the decision boundary with a margin of at least for this reason we say that an svm is an example of a large margin classifier. if the data is not linearly separable after using the kernel trick there will be no feasible solution in which yifi for all i. we therefore introduce slack variables i such that i if the point is on or inside the correct margin boundary and i fi otherwise. if i the point lies inside the margin but on the correct side of the decision boundary. if i the point lies on the wrong side of the decision boundary. see figure we replace the hard constraints that yifi with the soft margin constraints that yifi i. the new objective becomes i s.t. i yixt i w i min c chapter kernels correct log odds rvm yx svm yx figure log-odds vs x for different methods. based on figure of used with kind permission of mike tipping. which is the same as equation since i means point i is misclassified we can interpret i i as an upper bound on the number of misclassified points. the parameter c is a regularization parameter that controls the number of errors we are it is common to define this using c n where willing to tolerate on the training set. controls the fraction of misclassified points that we allow during the training phase. this is called a classifier. this is usually set using cross-validation section probabilistic output an svm classifier produces a hard-labeling yx signf however we often want a measure of confidence in our prediction. one heuristic approach is to interpret f as the log-odds ratio log we can then convert the output of an svm to a probability using py where a b can be estimated by maximum likelihood on a separate validation set. the training set to estimate a and b leads to severe overfitting. this technique was first proposed in however the resulting probabilities are not particularly well calibrated since there is nothing in the svm training procedure that justifies interpreting f as a log-odds ratio. to illustrate this consider an example from suppose we have data where pxy and pxy since the class-conditional distributions overlap in the middle the log-odds of class over class should be zero in and infinite outside this region. we sampled points from the model and then fit an rvm and an svm with a gaussian kenel of width both models can perfectly capture the decision boundary and achieve a generalizaton error of which is bayes optimal in this problem. the probabilistic output from the rvm is a good approximation to the true log-odds but this is not the case for the svm as shown in figure support vector machines not not the one-versus-rest approach. the green region is predicted to be both class and class the one-versus-one approach. the label of the green region is ambiguous. based on figure of figure svms for multi-class classification in section we saw how we could upgrade a binary logistic regression model to the multiclass case by replacing the sigmoid function with the softmax and the bernoulli distribution with the multinomial. upgrading an svm to the multi-class case is not so easy since the outputs are not on a calibrated scale and hence are hard to compare to each other. the obvious approach is to use a one-versus-the-rest approach called one-vs-all in which we train c binary classifiers fcx where the data from class c is treated as positive and the data from all the other classes is treated as negative. however this can result in regions of input space which are ambiguously labeled as shown in figure a common alternative is to pick yx arg maxc fcx. however this technique may not work either since there is no guarantee that the different fc functions have comparable magnitudes. in addition each binary subproblem is likely to suffer from the class imbalance problem. to see this suppose we have equally represented classes. when training we will have positive examples and negative examples which can hurt performance. it is possible to devise ways to train all c classifiers simultaneously and watkins but the resulting method takes oc time instead of the usual ocn time. another approach is to use the one-versus-one or ovo approach also called all pairs in which we train cc classifiers to discriminate all pairs we then classify a point into the class which has the highest number of votes. however this can also result in ambiguities as shown in figure also it takes oc time to train and oc to test each data point where nsv is the number of support see also et al. for an approach based on error-correcting output codes. it is worth remembering that all of these difficulties and the plethora of heuristics that have been proposed to fix them fundamentally arise because svms do not model uncertainty using probabilities so their output scores are not comparable across classes. we can reduce the test time by structuring the classes into a dag acyclic graph and performing oc pairwise comparisons et al. however the factor in the training time is unavoidable. r o r r e v c chapter kernels r o r r e v c c c figure a cross validation estimate of the error for an svm classifier with rbf kernel with different precisions and different regularizer applied to a synthetic data set drawn from a mixture of gaussians. a slice through this surface for the red dotted line is the bayes optimal error computed using bayes rule applied to the model used to generate the data. based on figure of et al. figure generated by svmcgammademo. choosing c svms for both classification and regression require that you specify the kernel function and the parameter c. typically c is chosen by cross-validation. note however that c interacts quite strongly with the kernel parameters. for example suppose we are using an rbf kernel with precision if corresponding to narrow kernels we need heavy regularization and hence small c is big. if a larger value of c should be used. so we see that and c are tightly coupled. this is illustrated in figure which shows the cv estimate of the risk as a function of c and the authors of libsvm recommend et al. using cv over a grid with values c in addition it is important to standardize the data first for a spherical gaussian kernel to make sense. to choose c efficiently one can develop a path following algorithm in the spirit of lars the basic idea is to start with large so that the margin is wide and hence all points are inside of it and have i by slowly decreasing a small set of points will move from inside the margin to outside and their i values will change from to as they cease to be support vectors. when is maximal the function is completely smoothed and no support vectors remain. see et al. for the details. and summary of key points summarizing the above discussion we recognize that svm classifiers involve three key ingredients the kernel trick sparsity and the large margin principle. the kernel trick is necessary to prevent underfitting i.e. to ensure that the feature vector is sufficiently rich that a linear classifier can separate the data. from section that any mercer kernel can be viewed as implicitly defining a potentially high dimensional feature vector. if the original features are already high dimensional in many gene expression and text classification problems it suffices to use a linear kernel which is equivalent to working with the original features. t comparison of discriminative kernel methods method opt. w convex convex not convex rvm convex svm gp na opt. kernel eb cv eb cv eb sparse no yes yes yes no prob. multiclass non-mercer yes yes yes no yes yes yes yes indirectly yes yes yes yes no no section table comparison of various kernel based classifiers. eb empirical bayes cv cross validation. see text for details. the sparsity and large margin principles are necessary to prevent overfitting i.e. to ensure that we do not use all the basis functions. these two ideas are closely related to each other and both arise this case from the use of the hinge loss function. however there are other methods of achieving sparsity as and also other methods of maximizing the margin as boosting. a deeper discussion of this point takes us outside of the scope of this book. see e.g. et al. for more information. a probabilistic interpretation of svms in section we saw how to use kernels inside glms to derive probabilistic classifiers such as the and rvm. and in section we will discuss gaussian process classifiers which also use kernels. however all of these approaches use a logistic or probit likelihood as opposed to the hinge loss used by svms. it is natural to wonder if one can interpret the svm more directly as a probabilistic model. to do so we must interpret cgm as a negative log likelihood where gm m where m yf is the margin. hence py exp cgf and py exp cg f by summing over both values of y we require that exp cgf exp cg f be a constant independent of f. but it turns out this is not possible for any c however if we are willing to relax the sum-to-one condition and work with a pseudolikelihood we can derive a probabilistic interpretation of the hinge loss and scott in particular one can show that i exp i yixt i i exp yixt i w d i thus the exponential of the negative hinge loss can be represented as a gaussian scale mixture. this allows one to fit an svm using em or gibbs sampling where i are the latent variables. this in turn opens the door to bayesian methods for setting the hyper-parameters for the prior on w. see and scott for details. also et al. for a different probabilistic interpretation of svms. comparison of discriminative kernel methods we have mentioned several different methods for classification and regression based on kernels which we summarize in table stands for gaussian process which we discuss in chapter the columns have the following meaning chapter kernels optimize w a key question is whether the objective jw log pdw log pw is convex or not. and svms have convex objectives. rvms do not. gps are bayesian methods that do not perform parameter estimation. optimize kernel all the methods require that one tune the kernel parameters such as the bandwidth of the rbf kernel as well as the level of regularization. for methods based on gaussians including rvms and gps we can use efficient gradient based optimizers to maximize the marginal likelihood. for svms and we must use cross validation which is slower section sparse rvms and svms are sparse kernel methods in that they only use a subset of the training examples. gps and are not sparse they use all the training examples. the principle advantage of sparsity is that prediction at test time is usually faster. in addition one can sometimes get improved accuracy. probabilistic all the methods except for svms produce probabilistic output of the form pyx. svms produce a confidence value that can be converted to a probability but such probabilities are usually very poorly calibrated section multiclass all the methods except for svms naturally work in the multiclass setting by using a multinoulli output instead of bernoulli. the svm can be made into a multiclass classifier but there are various difficulties with this approach as discussed in section mercer kernel svms and gps require that the kernel is positive definite the other techniques do not. apart from these differences there is the natural question which method works best? in a small we found that all of these methods had similar accuracy when averaged over a range of problems provided they have the same kernel and provided the regularization constants are chosen appropriately. given that the statistical performance is roughly the same what about the computational performance? gps and are generally the slowest taking on time since they don t exploit sparsity various speedups are possible see section svms also take on time to train we use a linear kernel in which case we only need on time however the need to use cross validation can make svms slower than rvms. should be faster than an rvm since an rvm requires multiple rounds of minimization section however in practice it is common to use a greedy method to train rvms which is faster than minimization. this is reflected in our empirical results. the conclusion of all this is as follows if speed matters use an rvm but if well-calibrated probabilistic output matters for active learning or control problems use a gp. the only circumstances under which using an svm seems sensible is the structured output case where likelihood-based methods can be slow. attribute the enormous popularity of svms not to their superiority but to ignorance of the alternatives and also to the lack of high quality software implementing the alternatives. section gives a more extensive experimental comparison of supervised learning methods including svms and various non kernel methods. see kernels for building generative models boxcar epanechnikov tricube gaussian figure a comparison of some popular smoothing kernels. the boxcar kernel has compact support but is not smooth. the epanechnikov kernel has compact support but is not differentiable at its boundary. the tri-cube has compact support and two continuous derivatives at the boundary of its support. the gaussian is differentiable but does not have compact support. based on figure of et al. figure generated by smoothingkernelplot. kernels for building generative models there is a different kind of kernel known as a smoothing kernel which can be used to create non-parametric density estimates. this can be used for unsupervised density estimation px as well as for creating generative models for classification and regression by making models of the form py x. smoothing kernels a smoothing kernel is a function of one argument which satisfies the following properties x a simple example is the gaussian kernel e we can control the width of the kernel by introducing a bandwidth parameter h hx h x h we can generalize to vector valued inputs by defining an rbf kernel hx hx in the case of the gaussian kernel this becomes hx exp j chapter kernels although gaussian kernels are popular they have unbounded support. an alternative kernel with compact support is the epanechnikov kernel defined by this is plotted in figure compact support can be useful for efficiency reasons since one can use fast nearest neighbor methods to evaluate the density. unfortunately the epanechnikov kernel is not differentiable at the boundary of its support. an alterative is the tri-cube kernel defined as follows ix this has compact support and has two continuous derivatives at the boundary of its support. see figure the boxcar kernel is simply the uniform distribution ix we will use this kernel below. kernel density estimation recall the gaussian mixture model from section this is a parametric density estimator for d. however it requires specifying the number k and locations k of the clusters. an data in r alternative to estimating the k is to allocate one cluster center per data point so i xi. in this case the model becomes pxd n n we can generalize the approach by writing px n h xi this is called a parzen window density estimator or kernel density estimator and is a simple non-parametric density model. the advantage over a parametric model is that no model fitting is required for tuning the bandwidth usually done by cross-validation. and there is no need to pick k. the disadvantage is that the model takes a lot of memory to store and a lot of time to evaluate. it is also of no use for clustering tasks. figure illustrates kde in for two kinds of kernel. on the top we use a boxcar kernel z the result is equivalent to a histogram estimate of the density since we just count how many data points land within an interval of size h around xi. on the bottom we use a gaussian kernel which results in a smoother fit. the usual way to pick h is to minimize an estimate as cross validation of the frequentist risk e.g. and azzalini in section we discuss a bayesian approach to non-parametric density estimation based on dirichlet process mixture models which allows us kernels for building generative models unif unif gauss gauss figure a nonparametric density estimator in estimated from data points denoted by x. top row uniform kernel. bottom row gaussian kernel. rows represent increasingly large bandwidth parameters. based on httpen.wikipedia.orgwikikernel_density_estimation. figure generated by to infer h. dp mixtures can also be more efficient than kde since they do not need to store all the data. see also section where we discuss an empirical bayes approach to estimating kernel parameters in a gaussian process model for classification regression. from kde to knn we can use kde to define the class conditional densities in a generative classifier. this turns out to provide an alternative derivation of the nearest neighbors classifier which we introduced in section to show this we follow the presentation of in kde with a boxcar kernel we fixed the bandwidth and count how many data points fall within the hyper-cube centered on a datapoint. suppose that instead of fixing the bandwidth h we instead chapter kernels gaussian kernel regression true data estimate figure an example of kernel regression in using a gaussian kernel. kernelregressiondemo based on code by yi cao. figure generated by allow the bandwidth or volume to be different for each data point. specifically we will grow a volume around x until we encounter k data points regardless of their class label. let the resulting volume have size v was previously hd and let there be ncx examples from class c in this volume. then we can estimate the class conditional density as follows pxy cd ncx ncv where nc is the total number of examples in class c in the whole data set. the class prior can be estimated by py cd nc n hence the class posterior is given by py cxd ncx ncv nc n v ncx ncx k n where we used the fact that class around every point. this is equivalent to equation since ncx c. c ncx k since we choose a total of k points of i nk iyi kernel regression in section we discussed the use of kernel density estimation or kde for unsupervised learning. we can also use kde for regression. the goal is to compute the conditional expectation f e y pyxdy y px ydy px ydy kernels for building generative models we can use kde to approximate the joint density px y as follows px y n hx xi hy yi n n hx xi hx xi hx xiyi hx xi y hy yidy hy yidy hence f to derive this result we used two properties of smoothing kernels. first that they integrate to y hy yidy yi. this follows by one i.e. defining x y yi and using the zero mean property of smoothing kernels hy yidy and second the fact that yi hxdx x hxdx yi hxdx yi yi we can rewrite the above result as follows f wix wixyi hx xi hx we see that the prediction is just a weighted sum of the outputs at the training points where the weights depend on how similar x is to the stored training points. this method is called kernel regression kernel smoothing or the nadaraya-watson model. see figure for an example where we use a gaussian kernel. note that this method only has one free parameter namely h. one can show and azzalini that for data if the true density is gaussian and we are using gaussian kernels the optimal bandwidth h is given by we can compute a robust approximation to the standard deviation by first computing the mean absolute deviation mad medianx medianx and then using mad mad the code used to produce figure estimated hx and hy separately and then set h hxhy. h chapter kernels although these heuristics seem to work well their derivation rests on some rather dubious assumptions as gaussianity of the true density. furthermore these heuristics are limited to tuning just a single parameter. in section we discuss an empirical bayes approach to estimating multiple kernel parameters in a gaussian process model for classification regression which can handle many tuning parameters and which is based on much more transparent principles the marginal likelihood. locally weighted regression if we define hx xi xi we can rewrite the prediction made by kernel regression as follows f yi xi note that xi need not be a smoothing kernel. normalization term so we can just write if it is not we no longer need the f yi xi this model is essentially fitting a constant function locally. we can improve on this by fitting a linear regression model for each point x by solving xiyi min where x. this is called locally weighted regression. an example of such a method is loess akalowess which stands for locally-weighted scatterplot smoothing and devlin see also et al. for a bayesian version of this model. we can compute the paramters for each test case by solving the following weighted least squares problem t dx where is an n design matrix and d diag xi. the corresponding prediction has the form t dx f t dx t dx wix the term wix which combines the local smoothing kernel with the effect of linear regression is called the equivalent kernel. see also section exercises exercise fitting an svm classifier by hand jaakkola. consider a dataset with points in and consider mapping each point to using the feature vector is equivalent to kernels for building generative models using a second order polynomial kernel. the max margin classifier has the form s.t. a. write down a vector that is parallel to the optimal vector w. hint recall from figure version that w is perpendicular to the decision boundary between the two points in the feature space. b. what is the value of the margin that is achieved by this w? hint recall that the margin is the distance from each support vector to the decision boundary. hint think about the geometry of points in space with a line separating one from the other. c. solve for w using the fact the margin is equal to d. solve for using your value for w and equations to hint decision boundary so the inequalities will be tight. the points will be on the e. write down the form of the discriminant function f wt as an explicit function of x. exercise linear separability koller.. consider fitting an svm with c to a dataset that is linearly separable. is the resulting decision boundary guaranteed to separate the classes? gaussian processes introduction in supervised learning we observe some inputs xi and some outputs yi. we assume that yi f for some unknown function f possibly corrupted by noise. the optimal approach is to infer a distribution over functions given the data pfx y and then to use this to make predictions given new inputs i.e. to compute py x y py x ydf up until now we have focussed on parametric representations for the function f so that in this chapter we discuss a way to perform instead of inferring pfd we infer p bayesian inference over functions themselves. our approach will be based on gaussian processes or gps. a gp defines a prior over functions which can be converted into a posterior over functions once we have seen some data. although it might seem difficult to represent a distribution over a function it turns out that we only need to be able to define a distribution over the function s values at a finite but arbitrary set of points say xn a gp assumes that pf f is jointly gaussian with some mean and covariance given by ij xj where is a positive definite kernel function section information on kernels. the key idea is that if xi and xj are deemed by the kernel to be similar then we expect the output of the function at those points to be similar too. see figure for an illustration. it turns out that in the regression setting all these computations can be done in closed form in on time. discuss faster approximations in section in the classification setting we must use approximations such as the gaussian approximation since the posterior is no longer exactly gaussian. gps can be thought of as a bayesian alternative to the kernel methods we discussed in chapter including rvm and svm. although those methods are sparser and therefore faster they do not give well-calibrated probabilistic outputs section for further discussion. having properly tuned probabilistic output is important in certain applications such as online tracking for vision and robotics and fox reinforcement learning and optimal control et al. deisenroth et al. global optimization of non-convex functions et al. lizotte brochu et al. experiment design et al. etc. chapter gaussian processes f f f figure a gaussian process for training points and testing point represented as a mixed directed and undirected graphical model representing py fx kx i pyifi. the hidden nodes fi f represent the value of the function at each of the data points. these hidden nodes are fully interconnected by undirected edges forming a gaussian graphical model the edge strengths represent the covariance terms ij xj. if the test point x is similar to the training points and then the predicted output y will be similar to and our presentation is closely based on and williams which should be consulted for futher details. see also and ribeiro which discusses the related approach known as kriging which is widely used in the spatial statistics literature. gps for regression in this section we discuss gps for regression. let the prior on the regression function be a gp denoted by f gp where mx is the mean function and is the kernel or covariance function i.e. mx mxf we obviously require that be a positive definite kernel. for any finite set of points this process defines a joint gaussian pfx n k where kij xj and mxn note that it is common to use a mean function of mx since the gp is flexible enough to model the mean arbitrarily well as we will see below. however in section we will consider parametric models for the mean function so the gp just has to model the residual errors. this semi-parametric approach combines the interpretability of parametric models with the accuracy of non-parametric models. gps for regression figure left some functions sampled from a gp prior with se kernel. right some samples from a gp posterior after conditioning on noise-free observations. the shaded area represents e based on figure of and williams figure generated by gprdemonoisefree. predictions using noise-free observations suppose we observe a training set d fi i n where fi f is the noise-free observation of the function evaluated at xi. given a test set x of size n d we want to predict the function outputs f if we ask the gp to predict f for a value of x that it has already seen we want the gp to return the answer f with no uncertainty. in other words it should act as an interpolator of the training data. this will only happen if we assume the observations are noiseless. we will consider the case of noisy observations below. now we return to the prediction problem. by definition of the gp the joint distribution has the following form f f n k k kt k where k x is n n k x is n n and k x is n n by the standard rules for conditioning gaussians the posterior has the following form pf x f t k k kt k this process is illustrated in figure on the left we show sample samples from the prior in pfx where we use a squared exponential kernel aka gaussian kernel or rbf kernel. this is given by f exp here controls the horizontal length scale over which the function varies and f controls the vertical variation. discuss how to estimate such kernel parameters below. on the right we chapter gaussian processes show samples from the posterior pf x f we see that the model perfectly interpolates the training data and that the predictive uncertainty increases as we move further away from the observed data. one application of noise-free gp regression is as a computationally cheap proxy for the behavior of a complex simulator such as a weather forecasting program. the simulator is stochastic we can define f to be its mean output note that there is still no observation noise. one can then estimate the effect of changing simulator parameters by examining their effect on the gp s predictions rather than having to run the simulator many times which may be prohibitively slow. this strategy is known as dace which stands for design and analysis of computer experiments et al. predictions using noisy observations now let us consider the case where what we observe is a noisy version of the underlying function y f where n y. in this case the model is not required to interpolate the data but it must come close to the observed data. the covariance of the observed noisy responses is cov yq xq y pq where pq ip q. in other words cov k yin ky the second matrix is diagonal because we assumed the noise terms were independently added to each observation. the joint density of the observed data and the latent noise-free function on the test points is given by n y f ky k kt k pf x y kt k y y k kt k y k where we are assuming the mean is zero for notational simplicity. hence the posterior predictive density is in the case of a single test input this simplifies as follows y y k kt k pf x y k y k where k xn and k x another way to write the posterior mean is as follows f kt k y y i x where k y y. we will revisit this expression later. gps for regression figure some gps with se kernels but different hyper-parameters fit to noisy observations. the kernel has the form in equation the hyper-parameters f y are as follows based on figure of and williams figure generated by gprdemochangehparams written by carl rasmussen. effect of the kernel parameters the predictive performance of gps depends exclusively on the suitability of the chosen kernel. suppose we choose the following squared-exponential kernel for the noisy observations yxp xq y pq f exp here is the horizontal scale over which the function changes f controls the vertical scale of the function and y is the noise variance. figure illustrates the effects of changing these parameters. we sampled noisy data points from the se kernel using f y and then made predictions various parameters conditional on the data. in figure we use f y and the result is a good fit. in figure we reduce the length scale to other parameters were optimized by maximum likelihood a technique we discuss below now the function looks more wiggly also the uncertainty goes up faster since the effective distance from the training points increases more rapidly. in figure we increase the length scale to now the function looks smoother. chapter gaussian processes y t u p t u o y t u p t u o input input input input y t u p u o t input input figure kernel has the form in equation where m i m written by carl rasmussen. some functions sampled from a gp with an se kernel but different hyper-parameters. the m based on figure of and williams figure generated by gprdemoard we can extend the se kernel to multiple dimensions as follows yxp xq f exp xqt mxp xq y pq we can define the matrix m in several ways. the simplest is to use an isotropic matrix see figure for an example. we can also endow each dimension with its own characteristic length scale if any of these length scales become large the corresponding feature dimension is deemed irrelevant just as in ard in figure we use m with so the function changes faster along the direction than the direction. where is a d k matrix where k d. and williams calls this the factor analysis distance function by analogy to the fact that factor analysis approximates a covariance matrix as a low rank matrix plus a diagonal matrix. the columns of correspond to relevant directions in input space. in figure we use and so the function changes mostly rapidly in the direction which is perpendicular to we can also create a matrix of the form t gps for regression estimating the kernel parameters to estimate the kernel parameters we could use exhaustive search over a discrete grid of values with validation loss as an objective but this can be quite slow. is the approach used to tune kernels used by svms. here we consider an empirical bayes approach which will allow us to use continuous optimization methods which are much faster. in particular we will maximize the marginal pyx pyf xpfxdf since pfx n k and pyf log pyx log n ky i n y y yk log n y the marginal likelihood is given by the first term is a data fit term the second term is a model complexity term and the third term is just a constant. to understand the tradeoff between the first two terms consider a se kernel y fixed. let log pyx for short in as we vary the length scale and hold length scales the fit will be good so yt k y y will be small. however the model complexity will be high k will be almost diagonal in figure top right since most points will not be considered near any others so the log will be large. for long length scales the fit will be poor but the model complexity will be low k will be almost all s in figure bottom right so log will be small. we now discuss how to maximize the marginal likelhiood. let the kernel parameters called hyper-parameters be denoted by one can show that trk log pyx yt k ky j j y ky j y y y k ky j t k y tr it takes on time to compute k where k y y. parameter to compute the gradient. y and then on time per hyper the form of ky j depends on the form of the kernel and which parameter we are taking y derivatives with respect to. often we have constraints on the hyper-parameters such as in this case we can define log y and then use the chain rule. given an expression for the log marginal likelihood and its derivative we can estimate the kernel parameters using any standard gradient-based optimizer. however since the objective is not convex local minima can be a problem as we illustrate below. example consider figure we use the se kernel in equation with x and y are the data points shown in panels b and c as we vary and f and plot log pyx y y. the two the reason it is called the marginal likelihood rather than just likelihood is because we have marginalized out the latent gaussian vector f this moves us up one level of the bayesian hierarchy and reduces the chances of overfitting number of kernel parameters is usually fairly small compared to a standard parametric model. n o i t i a v e d d r a d n a t s e s o n i chapter gaussian processes y t u p t u o input x input x characteristic lengthscale y t u p t u o illustration of local minima in the marginal likelihood surface. y and for fixed figure likelihood vs corresponding to the lower left local minimum noise. smooth and has high noise. the data was generated using and williams figure generated by gprdemomarglik written by carl rasmussen. we plot the log marginal f using the data points shown in panels b and c. the function n this is quite wiggly and has low n this is quite source figure of the function corresponding to the top right local minimum n local optima are indicated by the bottom left optimum corresponds to a low-noise shortlength scale solution in panel b. the top right optimum corresponds to a high-noise long-length scale solution in panel c. with only data points there is not enough evidence to confidently decide which is more reasonable although the more complex model b has a marginal likelihood that is about higher than the simpler model c. with more data the map estimate should come to dominate. y of panel a corresponds to the case where the noise is very high in this regime the marginal likelihood is insensitive to the length scale by the horizontal contours since all the data is explained as noise. the region where hand side of panel a corresponds to the case where the length scale is very short in this regime the marginal likelihood is insensitive to the noise level since the data is perfectly interpolated. neither of these regions would be chosen by a good optimizer. figure illustrates some other interesting typical features. the region where gps for regression e d u t i n g a m g o l z z e d u t i n g a m g o l loglength scale loglength scale e d u t i n g a m g o l loglength scale figure three different approximations to the posterior over hyper-parameters grid-based monte carlo and central composite design. source figure of used with kind permission of jarno vanhatalo. bayesian inference for the hyper-parameters an alternative to computing a point estimate of the hyper-parameters is to compute their posterior. let represent all the kernel parameters as well as y. if the dimensionality of is small we can compute a discrete grid of possible values centered on the map estimate as above. we can then approximate the posterior over the latent variables using pfd pfd sp sd s where s denotes the weight for grid point s. in higher dimensions a regular grid suffers from the curse of dimensionality. an obvious alternative is monte carlo but this can be slow. another approach is to use a form of quasimonte carlo whereby we place grid points at the mode and at a distance from the mode along each dimension for a total of points. this is called a central composite design et al. is also used in the unscented kalman filter see section to make this gaussian-like approximation more reasonable we often log-transform the hyper-parameters. see figure for an illustration. chapter gaussian processes multiple kernel learning a quite different approach to optimizing kernel parameters known as multiple kernel learning. the idea is to define the kernel as a weighted sum of base kernels j wj jx and then to optimize the weights wj instead of the kernel parameters themselves. this is particularly useful if we have different kinds of data which we wish to fuse together. see e.g. et al. for an approach based on risk-minimization and convex optimization and and rogers for an approach based on variational bayes. computational and numerical issues the predictive mean is given by f kt k y y. for reasons of numerical stability it is unwise to directly invert ky. a more robust alternative is to compute a cholesky decomposition ky llt we can then compute the predictive mean and variance and the log marginal likelihood as shown in the pseudo-code in algorithm on and williams it takes on time to compute the cholesky decomposition and on time to solve for k y y l t l we can then compute the mean using kt in on time and the variance using k kt l t l in on time for each test case. an alternative to cholesky decomposition is to solve the linear system ky y using conjugate gradients if we terminate this algorithm after k iterations it takes okn time. it gives the exact solution in on time. another approach is to if we run for k n approximate the matrix-vector multiplies needed by cg using the fast gauss transform. et al. however this doesn t scale to high-dimensional inputs. see also section for a discussion of other speedup techniques. yi algorithm gp regression l choleskyk lt y e kt v l k yt var x vt v log pyx i log lii n semi-parametric gps sometimes it is useful to use a linear model for the mean of the process as follows f t where rx parametric model and is known as a semi-parametric model. models the residuals. this combines a parametric and a nonif we assume n b we can integrate these parameters out to get a new gp hagan f gp b xt b gps meet glms log pyifi log sigmyifi log log pyifi fi ti i yi f i log pyifi i yifi i likelihood gradient and hessian for binary logistic probit gp regression. we assume yi table and define ti and i sigmfi for logistic regression and i for probit regression. also and are the pdf and cdf of n from and williams integrating out the corresponding predictive distribution for test inputs x has the following form and williams pf x y cov f t kt k t k y y b k y y b y k rt k y t cov kt k y r k the predictive mean is the output of the linear model plus a correction term due to the gp and the predictive covariance is the usual gp covariance plus an extra term due to the uncertainty in gps meet glms in this section we extend gps to the glm setting focussing on the classification case. as with bayesian logistic regression the main difficulty is that the gaussian prior is not conjugate to the bernoulli multinoulli likelihood. there are several approximations one can adopt gaussian approximation expectation propagation and rasmussen nickisch and rasmussen variational and rogers opper and archambeau mcmc christensen et al. etc. here we focus on the gaussian approximation since it is the simplest and fastest. binary classification in the binary case we define the model as pyixi where following and williams we assume yi and we let sigmz regression or regression. as for gp regression we assume f computing the posterior define the log of the unnormalized posterior as follows log pyf log pfx log pyf f t k log n log chapter gaussian processes let jf be the function we want to minimize. the gradient and hessian of this are given by g log pyf h log pyf w k note that w log pyf is a diagonal matrix because the data are iid on f expressions for the gradient and hessian of the log likelihood for the logit and probit case are given in sections and and summarized in table we can use irls to find the map estimate. the update has the form f new f h f w log pyf k w log pyf at convergence the gaussian approximation of the posterior takes the following form pfx y n f w computing the posterior predictive we now compute the posterior predictive. first we predict the latent function at the test case x for the mean we have e x y e x x y pfx ydf kt k pfx ydf kt k e y kt k f where we used equation to get the mean of f given noise-free f to compute the predictive variance we use the rule of iterated variance var var where all probabilities are conditioned on x x y. from equation we have e e from equation we have var var combining these we get k kt k k k k kt k kt k k var kt k k from equation we have cov w get var k kt k kt k w k kt w using the matrix inversion lemma we gps meet glms so in summary we have pf x y n var to convert this in to a predictive distribution for binary responses we use py x y x ydf this can be approximated using any of the methods discussed in section where we discussed bayesian logistic regression. for example using the probit approximation of section we have sigm wherev var and computing the marginal likelihood we need the marginal likelihood in order to optimize the kernel parameters. using the laplace approximation in equation we have log pyx f log const hence log pyx log py f computing the derivatives log pyx f t k f log log w is more complex than in the regression case since f and w as well as k depend on details can be found in and williams j numerically stable computation to implement the above equations in a numerically stable way it is best to avoid inverting k or w. and williams suggest defining b in w kw which has eigenvalues bounded below by of the i and above by n wii and hence can be safely inverted. maxij kij one can use the matrix inversion lemma to show w k kw hence the irls update becomes b k f new w log pyf b ki w k w kb b lt kb a chapter gaussian processes where b llt is a cholesky decomposition of b. the fitting algorithm takes in ot n time and on space where t is the number of newton iterations. at convergence we have a k f so we can evaluate the log marginal likelihood tion using log pyx log py f at f i log lii where we exploited the fact that w w we now compute the predictive distribution. rather than using e kt k f we exploit the fact that at the mode so f k log py f hence we can rewrite the predictive mean as kw e kt log py f to compute the predictive variance we exploit the fact that w w w w w w b to get var k kt w k k vt v where v l k we can then compute the whole algorithm is summarized in algorithm based on and williams fitting takes on time and prediction takes on time where n is the number of test cases. example in figure we show a synthetic binary classification problem in we use an se kernel. on the left we show predictions using hyper-parameters set by hand we use a short length scale hence the very sharp turns in the decision boundary. on the right we show the predictions using the learned hyper-parameters the model favors a more parsimonious explanation of the data. multi-class classification in this section we consider a model of the form pyixi catyisfi where fi fic and we assume f.c c. thus we have one latent function per class which are a priori independent and which may use different kernels. as before we will use a gaussian approximation to the posterior. similar model but using the multinomial probit function instead of the multinomial logit is described in and rogers we see that training points that are well-predicted by the model for which i log pyifi do not contribute strongly to the prediction at test points this is similar to the behavior of support vectors in an svm section gps meet glms algorithm gp binary classification using gaussian approximation first compute map estimate using irls f repeat w log pyf b in w kw l choleskyb b wf log pyf a b w f ka lt kb at f until converged log pyx log pyf now perform prediction e kt log pyf v l k var k vt v sigmzn var py i log lii se kernel se kernel figure contours of the posterior predictive probability for the red circle class generated by a gp with an se kernel. thick black line is the decision boundary if we threshold at a probability of manual parameters short length scale. learned parameters long length scale. figure generated by based on code by carl rasmussen. chapter gaussian processes computing the posterior the unnormalized log posterior is given by f t k yt f log where exp fic log cn log f fn fn fn ct and y is a dummy encoding of the yi s which has the same layout as f also k is a block diagonal matrix containing kc where kc cxi xj models the correlation of the c th latent function. the gradient and hessian are given by k y k w where w diag t where is a cn n matrix obtained by stacking diag vertically. these expressions to standard logistic regression in section we can use irls to compute the mode. the newton step has the form f new w y naively implementing this would take oc time. however we can reduce this to ocn as shown in and williams computing the posterior predictive we can compute the posterior predictive in a manner analogous to section for the mean of the latent response we have e c kcx k c fc kcx c we can put this in vector form by writing e q t where q kcx using a similar argument to equation we can show that the covariance of the latent response is given by cov qt k w diagkx x qt w gps meet glms where is a c c diagonal matrix with cc cx x kt kx x cx x to compute the posterior predictive for the visible response we need to use pyx x y catysf cov c c kcx and we can use any of deterministic approximations to the softmax function discussed in section to compute this. alternatively we can just use monte carlo. computing the marginal likelihood using arguments similar to the binary case we can show that f t k f yt f log exp fic log pyx log w kw this can be optimized numerically in the usual way. numerical and computational issues one can implement model fitting in ot cn time and ocn space where t is the number of newton iterations using the techniques described in and williams prediction takes ocn cn time where n is the number of test cases. gps for poisson regression in this section we illustrate gps for poisson regression. an interesting application of this is to spatial disease mapping. for example et al. discuss the problem of modeling the relative risk of heart attack in different regions in finland. the data consists of the heart attacks in finland from aggregated into x lattice cells. the model has the following form yi poieiri where ei is the known expected number of deaths to the population of cell i and the overall death rate and ri is the relative risk of cell i which we want to infer. since the data counts are small we regularize the problem by sharing information with spatial neighbors. hence we assume f logr where we use a matern kernel with and a length scale and magnitude that are estimated from data. figure gives an example of the kind of output one can obtain from this method based on data from locations. on the left we plot the posterior mean relative risk and on the right the posterior variance. we see that the rr is higher in eastern finland which is consistent with other studies. we also see that the variance in the north is higher since there are fewer people living there. chapter gaussian processes posterior mean of the relative risk fic posterior variance of the relative risk fic figure we show the relative risk of heart disease in finland using a poisson gp. left posterior mean. right posterior variance. figure generated by gpspatialdemolaplace written by jarno vanhatalo. connection with other methods there are variety of other methods in statistics and machine learning that are closely related to gp regression classification. we give a brief review of some of these below. linear models compared to gps consider bayesian linear regression for d-dimensional features where the prior on the weights is pw n the posterior predictive distribution is given by the following pf x y n xt a y y xt a y xt x one can show that we can rewrite the above distribution as where a follows xt xt xt x xt xt yi x xt where we have defined k x xt which is of size n n since the features only ever appear in the form x xt xt xt or xt x we can kernelize the above expression by defining thus we see that bayesian linear regression is equivalent to a gp with covariance function note however that this is a degenerate covariance function since it has at most d non-zero eigenvalues. intuitively this reflects the fact that the model can only represent a limited number of functions. this can result in underfitting since the model is not flexible enough to capture the data. what is perhaps worse it can result in overconfidence since the xt connection with other methods model s prior is so impoverished that its posterior will become too concentrated. so not only is the model wrong it think it s right! linear smoothers compared to gps a linear smoother is a regression function which is a linear function of the training outputs f wix yi i where wix is called the weight function not confuse this with a linear model where the output is a linear function of the input vector. there are a variety of linear smoothers such as kernel regression locally weighted regression smoothing splines and gp regression. to see that gp regession is a linear smoother note that the mean of the posterior predictive distribution of a gp is given by f kt yin yiwix where wix yin in kernel regression we derive the weight function from a smoothing kernel rather than a mercer kernel so it is clear that the weight function will then have local support. in the case of a gp things are not as clear since the weight function depends on the inverse of k. for certain gp kernel functions we can analytically derive the form of wix this is known as the wix although we may equivalent kernel one can show that have wix so we are computing a linear combination but not a convex combination of the yi s. more interestingly wix is a local function even if the original kernel used by the gp is not local. futhermore the effective bandwidth of the equivalent kernel of a gp automatically decreases as the sample size n increases whereas in kernel smoothing the bandwidth h needs to be set by hand to adapt to n see e.g. and williams sec for details. degrees of freedom of linear smoothers it is clear why this method is called linear but why is it called a smoother this is best explained in terms of gps. consider the prediction on the training set f kk y iuiut i since k is real and symmetric now let k have the eigendecomposition k positive definite the eigenvalues i are real and non-negative and the eigenvectors ui are orthonormal. now let y i y. then we can rewrite the above equation as follows iui where i ut i i i y ui f chapter gaussian processes this is the same as equation except we are working with the eigenvectors of the gram matrix k instead of the data matrix x. in any case the interpretation is similar if then the corresponding basis function ui will not have much influence. consequently the highfrequency components in y are smoothed out. the effective degrees of freedom of the linear smoother is defined as dof trkk i y i i yi i y this specifies how wiggly the curve is. svms compared to gps we saw in section that the svm objective for binary classification is given by equation jw yifi c ij i jxt we also know from equation that the optimal solution has the form w i ixi i xj. kernelizing we get k from equation and so absorbing the term into one of the kernels we have f k so f t k hence the svm objective can be rewritten as yifi f t f c jf compare this to map estimation for gp classifier f t f jf log pyifi it is tempting to think that we can convert an svm into a gp by figuring out what likelihood would be equivalent to the hinge loss. however it turns out there is no such likelihood although there is a pseudo-likelihood that matches the svm section from figure we saw that the hinge loss and the logistic loss well as the probit loss are quite similar to each other. the main difference is that the hinge loss is strictly for errors larger than this gives rise to a sparse solution. in section we discussed other ways to derive sparse kernel machines. we discuss the connection between these methods and gps below. and rvms compared to gps sparse kernel machines are just linear models with basis function expansion of the form xn from section we know that this is equivalent to a gp with the following kernel jx j connection with other methods where pw n diag j this kernel function has two interesting properties. first it is degenerate meaning it has at most n non-zero eigenvalues so the joint distribution pf f will be highly constrained. second the kernel depends on the training data. this can cause the model to be overconfident when extrapolating beyond the training data. to see this consider a point x far outside the convex hull of the data. all the basis functions will have values close to so the prediction will back off to the mean of the gp. more worryingly the variance will back off to the noise variance. by contrast when using a non-degenerate kernel function the predictive variance increases as we move away from the training data as desired. see and qui onero-candela for further discussion. neural networks compared to gps in section we will discuss neural networks which are a nonlinear generalization of glms. in the binary classification case a neural network is defined by a logistic regression model applied to a logistic regression model pyx ber ysigm wt sigmvx it turns out there is an interesting connection between neural networks and gaussian processes as first pointed out by to explain the connection we follow the presentation of and williams consider a neural network for regression with one hidden layer. this has the form pyx n where f b vjgx uj where b is the offset of bias term vj is the output weight from hidden unit j to the response y uj are the inputs weights to unit j from the input x and g is the hidden unit activation function. this is typically the sigmoid or tanh function but can be any smooth function. j n w u let us use the following priors on the weights where b n j puj for some unspecified puj. denoting all the weights by we have b v e e b j b h v ev v eu u uj v scale as where the last equality follows since the h hidden units are iid. more hidden units will increase the input to the final node so we should scale down the magnitude of the weights then the last term becomes u. this is a sum over h iid random variables. assuming that g is bounded we can apply the central limit theorem. the result is that as h we get a gaussian process. if we let eu chapter gaussian processes x t u p n i input x x f t u p t u o input x figure covariance function n n this kernel using various values of figure generated by gpnndemo written by chris williams. for samples from from a gp with z ujxj where erfz e dt and we choose u n then showed that the covariance if we use as activation transfer function gx u kernel has the form n n sin xt xt where x xd. this is a true neural network kernel unlike the sigmoid kernel tanha bxt which is not positive definite. figure illustrates this kernel when d and diag figure shows some functions sampled from the corresponding gp. these are equivalent to functions which are superpositions of ux where and u are random. as increases the variance of u increases so the function varies more quickly. unlike the rbf kernel functions sampled from this kernel do not tend to away from the data but rather they tend to remain at the same value they had at the edge of the data. of the form gx u exp coresponding kernel is equivalent to the rbf or se kernel. now suppose we use an rbf network which is equivalent to a hidden unit activation function ui one can show that the if u n g. smoothing splines compared to gps smoothing splines are a widely used non-parametric method for smoothly interpolating data and silverman they are are a special case of gps as we will see. they are usually used when the input is or dimensional. univariate splines the basic idea is to fit a function f by minimizing the discrepancy to the data plus a smoothing if we penalize the m th derivative of the term that penalizes functions that are too wiggly connection with other methods function the objective becomes jf dm dxm f one can show and silverman that the solution is a piecewise polynomial where the polynomials have order in the interior bins xi i and order m in the two outermost intervals and f jxj ix i ix ix i ix xim m for example if m we get the cubic spline f ix i ix ix i ix xi which is a series of truncated cubic polynomials whose left hand sides are located at each of the n training points. fact that the model is linear on the edges prevents it from extrapolating too wildly beyond the range of the data if we drop this requirement we get an unrestricted spline. t y where the for i and xi for i or i n columns of are xi and however we can also derive an on time method and silverman sec we can clearly fit this model using ridge regression w t in regression splines in general we can place the polynomials at a fixed set of k locations known as knots denoted k. the result is called a regression spline. this is a parametric model which uses basis function expansion of the following form we drop the interior exterior distinction for simplicity f jx choosing the number and locations of the knots is just like choosing the number and values of the support vectors in section if we impose an regularizer on the regression coefficients j the method is known as penalized splines. see section for a practical example of penalized splines. the connection with gps one can show and williams that the cubic spline is the map estimate of the following function f rx chapter gaussian processes where p j that we don t penalize the zero th and first derivatives of f and rx f spx spx where udu note that the kernel in equation is rather unnatural and indeed posterior samples from the resulting gp are rather unsmooth. however the posterior modemean is smooth. this shows that regularizers don t always make good priors. input splines one can generalize cubic splines to input by defining a regularizer of the following form one can show that the solution has the form i ix f t x where ix xi and log this is known as a thin plate spline. this is equivalent to map estimation with a gp whose kernel is defined in and fitzgibbon higher-dimensional inputs it is hard to analytically solve for the form of the optimal solution when using higher-order inputs. however in the parametric regression spline setting where we forego the regularizer on f we have more freedom in defining our basis functions. one way to handle multiple inputs is to use a tensor product basis defined as the cross product of basis functions. for example for input we can define f m m m it is clear that for high-dimensional data we cannot allow higher-order interactions because there will be too many parameters to fit. one approach to this problem is to use a search procedure to look for useful interaction terms. this is known as mars which stands for multivariate adaptive regression splines see section for details. rkhs methods compared to gps we can generalize the idea of penalizing derivatives of functions as used in smoothing splines to fit functions with a more general notion of smoothness. recall from section that connection with other methods mercer s theorem says that any positive definite kernel function can be represented in terms of eigenfunctions i ix the i form an orthormal basis for a function space hk f i i f fi ix now define the inner product between two functions f gi ix in this space as follows figi i fi ix and gx in exercise we show that this definition implies that this is called the reproducing property and the space of functions hk is called a reproducing kernel hilbert space or rkhs. now consider an optimization problem of the form jf y f where is the norm of a function h f i i the intuition is that functions that are complex wrt the kernel will have large norms because they will need many eigenfunctions to represent them. we want to pick a simple function that provides a good fit to the data. one can show e.g. and smola that the solution must have the form f i xi this is known as the representer theorem and holds for other convex loss functions besides squared error. i xi and using the reproducing we can solve for the by substituting in f property to get j k y t k minimizing wrt we find yi and hence f i xi kt yi chapter gaussian processes i this is identical to equation the posterior mean of a gp predictive distribution. indeed since the mean and mode of a gaussian are the same we can see that linear regresson with an rkhs regularizer is equivalent to map estimation with a gp. an analogous statement holds for the gp logistic regression case which also uses a convex likelihood loss function. gp latent variable model in section we discussed kernel pca which applies the kernel trick to regular pca. in this section we discuss a different way to combine kernels with probabilistic pca. the resulting method is known as the gp-lvm which stands for gaussian process latent variable model to explain the method we start with ppca. recall from section that the ppca model is as follows pzi i pyizi we can fit this model by maximum likelihood by integrating out the zi and maximizing w the objective is given by pyw exp trc y where c wwt as we showed in theorem the mle for this can be computed in terms of the eigenvectors of yt y. now we consider the dual problem whereby we maximize z and integrate out w. we will j n i. the corresponding likelihood becomes use a prior of the form pw pyz n zzt exp trk z yyt where kz zzt based on our discussion of the connection between the eigenvalues of yyt and of yt y in section it should come as no surprise that we can also solve the dual problem using eigenvalue methods for the details. if we use a linear kernel we recover pca. but we can also use a more general kernel kz k where k is the gram matrix for z. the mle for z will no longer be available gp latent variable model figure representation of dimensional oil flow data. the different colorssymbols represent the phases of oil flow. gp-lvm with gaussian kernel. the shading represents the precision of the posterior where lighter pixels have higher precision. from figure of used with kind permission of neil lawrence. kernel pca with gaussian kernel. via eigenvalue methods instead we must use gradient-based optimization. the objective is given by d log trk z yyt and the gradient is given by zij kz kz zij where kz k z yyt k z dk z the form of kz where kz zzt we have kz optimizer such as conjugate gradient descent. zij will of course depend on the kernel used. example with a linear kernel z z. we can then pass this gradient to any standard let us now compare gp-lvm to kernel pca. in kpca we learn a kernelized mapping from the observed space to the latent space whereas in gp-lvm we learn a kernelized mapping from the latent space to the observed space. figure illustrates the results of applying kpca and gp-lvm to visualize the dimensional oil flow data shown in in figure we see that the embedding produced by gp-lvm is far better. if we perform nearest neighbor classification in the latent space gp-lvm makes errors while kernel pca the same kernel but separately optimized hyper-parameters makes errors and regular pca makes errors. gp-lvm inherits the usual advantages of probabilistic generative models such as the ability to handle missing data and data of different types the ability to use gradient-based methods of grid search to tune the kernel parameters the ability to handle prior information chapter gaussian processes etc. for a discussion of some other probabilistic methods for dimensionality reduction see approximation methods for large datasets the principal drawback of gps is that they take on time to use. this is because of the need to invert compute the cholesky decomposition of the n n kernel matrix k. a variety of approximation methods have been devised which take om time where m is a user-specifiable parameter. for details see et al. exercises exercise reproducing property prove equation adaptive basis function models introduction in chapters and we discussed kernel methods which provide a powerful way to create nonlinear models for regression and classification. the prediction takes the form f t where we define n and where k are either all the training data or some subset. models of this form essentially perform a form of template matching whereby they compare the input x to the stored prototypes k. although this can work well it relies on having a good kernel function to measure the similarity between data vectors. often coming up with a good kernel function is quite difficult. for example how do we define the similarity between two images? pixel-wise comparison of intensities is what a gaussian kernel corresponds to does not work well. although it is possible indeed common to hand-engineer kernels for specific tasks e.g. the pyramid match kernel in section it would be more interesting if we could learn the kernel. in section we discussed a way to learn the parameters of a kernel function by maxi mizing the marginal likelihood. for example if we use the ard kernel exp jxj we can can estimate the j and thus perform a form of nonlinear feature selection. however such methods can be computationally expensive. another approach known as multiple kernel et al. uses a convex combination of base kernels learning e.g. and then estimates the mixing weights wj. but this relies on having good base kernels is also computationally expensive. j wj jx an alternative approach is to dispense with kernels altogether and try to learn useful features directly from the input data. that is we will create what we call an adaptive basisfunction model which is a model of the form f wm mx chapter adaptive basis function models where mx is the m th basis function which is learned from data. this framework covers all of the models we will discuss in this chapter. typically the basis functions are parametric so we can write mx vm where vm are the parameters of the basis function itself. we will use to denote the entire parameter set. the resulting model is not linear-in-the-parameters anymore so we will only be able to compute a locally optimal mle or map estimate of nevertheless such models often significantly outperform linear models as we will see. classification and regression trees classification and regression trees or cart models also called decision trees to be confused with the decision trees used in decision theory are defined by recursively partitioning the input space and defining a local model in each resulting region of input space. this can be represented by a tree with one leaf per region as we explain below. basics to explain the cart approach consider the tree in figure the first node asks if is less than some threshold if yes we then ask if is less than some other threshold if yes we are in the bottom left quadrant of space if no we ask if is less than and so on. the result of these axis parallel splits is to partition space into regions as shown in figure we can now associate a mean response with each of these regions resulting in the piecewise constant surface shown in figure we can write the model in the following form f e wmix rm wm vm where rm is the m th region wm is the mean response in this region and vm encodes the choice of variable to split on and the threshold value on the path from the root to the m th leaf. this makes it clear that a cart model is just a an adaptive basis-function model where the basis functions define the regions and the weights specify the response value in each region. we discuss how to find these basis functions below. we can generalize this to the classification setting by storing the distribution over class labels in each leaf instead of the mean response. this is illustrated in figure this model can be used to classify the data in figure for example we first check the color of the object. if it is blue we follow the left branch and end up in a leaf labeled which means we have positive examples and negative examples which match this criterion. hence we predict py if x is blue. if it is an ellipse we end up in a leaf labeled so we predict py if it is red but not an ellipse we predict py if it is some other colour we check the size if less than we predict py otherwise py these probabilities are just the empirical fraction of positive examples that satisfy each conjunction of feature values which defines a path from the root to a leaf. if it is red we then check the shape classification and regression trees figure a simple regression tree on two inputs. based on figure of et al. figure generated by regtreesurfacedemo. blue color red other shape ellipse other size yes no figure a simple decision tree for the data in figure a leaf labeled as means that there are positive examples that match this path and negative examples. in this tree most of the leaves are pure meaning they only have examples of one class or the other the only exception is leaf representing red ellipses which has a label distribution of we could distinguish positive from negative red ellipses by adding a further test based on size. however it is not always desirable to construct trees that perfectly model the training data due to overfitting. growing a tree finding the optimal partitioning of the data is np-complete and rivest so it is common to use the greedy procedure shown in algorithm to compute a locally optimal mle. this method is used by cart et al. and dtfit for a simple matlab which are three popular implementations of the method. implementation. the split function chooses the best feature and the best value for that feature as follows t costxi yi xij t yi xij t arg min j min t tj chapter adaptive basis function models algorithm recursive procedure to grow a classification regression tree function fittreenode d depth node.prediction meanyi i d or class label distribution t splitd if not worthsplittingdepth cost dl dr then return node else node.test x.xj t node.left fittreenode dl node.right fittreenode dr return node anonymous function where the cost function for a given dataset will be defined below. for notational simplicity we have assumed all inputs are real-valued or ordinal so it makes sense to compare a feature xij to a numeric value t. the set of possible thresholds tj for feature j can be obtained by sorting the unique values of xij. for example if feature has the values then we set in the case of categorical inputs the most common approach is to consider splits of the form xij ck and xij ck for each possible class label ck. although we could allow for multi-way splits in non-binary trees this would result in data fragmentation meaning too little data might fall into each subtree resulting in overfitting. the function that checks if a node is worth splitting can use several stopping heuristics such as the following is the reduction in cost too small? typically we define the gain of using a feature to be a normalized measure of the reduction in cost costdr costdl costd has the tree exceeded the maximum desired depth? is the distribution of the response in either dl or dr sufficiently homogeneous all labels are the same so the distribution is pure? is the number of examples in either dl or dr too small? all that remains is to specify the cost measure used to evaluate the quality of a proposed split. this depends on whether our goal is regression or classification. we discuss both cases below. regression cost i d in the regression setting we define the cost as follows costd classification and regression trees where y i d yi is the mean of the response variable in the specified set of data. alternatively we can fit a linear regression model for each leaf using as inputs the features that were chosen on the path from the root and then measure the residual error. classification cost i d c in the classification setting there are several ways to measure the quality of a split. first we fit a multinoulli model to the data in the leaf satisfying the test xj t by estimating the class-conditional probabilities as follows iyi c where d is the data in the leaf. given this there are several common error measures for evaluating a proposed partition misclassification rate. we define the most probable class label as yc argmaxc c. the corresponding error rate is then iyi y y i d entropy ordeviance h c log c note that minimizing the entropy is equivalent to maximizing the information gain between test xj t and the class label y defined by infogainxj t y h h t c c py c log py c py cxj t log pcxj t since c is an mle for the distribution pcxj if xj is categorical and we use tests of the form xj k then taking expectations over values of xj gives k pxj kinfogainxj k y the mutual h h i xj information between xj and y e y chapter adaptive basis function models error rate gini entropy figure node impurity measures for binary classification. the horizontal axis corresponds to p the probability of class the entropy measure has been rescaled to pass through based on figure of et al. figure generated by ginidemo. gini index c c c c c c c this is the expected error rate. to see this note that c is the probability a random entry in the leaf belongs to class c and c is the probability it would be misclassified. in the two-class case where p the misclassification rate is maxp p the entropy is and the gini index is p. these are plotted in figure we see that the cross-entropy and gini measures are very similar and are more sensitive to changes in class probability than is the misclassification rate. for example consider a two-class problem with cases in each class. suppose one split created the nodes and while the other created the nodes and both splits produce a misclassification rate of however the latter seems preferable since one of the nodes is pure i.e. it only contains one class. the cross-entropy and gini measures will favor this latter choice. example as an example consider two of the four features from the iris dataset shown in figure the resulting tree is shown in figure and the decision boundaries are shown in figure we see that the tree is quite complex as are the resulting decision boundaries. in figure we show that the cv estimate of the error is much higher than the training set error indicating overfitting. below we discuss how to perform a tree-pruning stage to simplify the tree. t h d w i l a p e s setosa versicolor virginica y classification and regression trees unpruned decision tree versicolor setosa virginica sepal length x figure petal length and petal width. decision boundaries induced by the decision tree in figure iris data. we only show the first two features sepal length and sepal width and ignore sl sl w sw sl sl versicolorsetosa sw sw sl sl sl sl setosa sw sw virginica sw sw versicolor versicolor sw sw versicolor sl sl sw sw versicolor versicolorvirginica sl sl virginica sw sw sl sl sl sl virginicaversicolor sw sw sw sw versicolor sw virginica sw virginicaversicolor versicolorvirginica cross validation training set min std. err. best choice r o r r e n o i t a c i f i s s a c s m i l t s o c number of terminal nodes figure tree. figure generated by dtreedemoiris. unpruned decision tree for iris data. plot of misclassification error rate vs depth of pruning a tree to prevent overfitting we can stop growing the tree if the decrease in the error is not sufficient to justify the extra complexity of adding an extra subtree. however this tends to be too myopic. for example on the xor data in figure it would might never make any splits since each feature on its own has little predictive power. the standard approach is therefore to grow a full tree and then to perform pruning. this can be done using a scheme that prunes the branches giving the least increase in the error. see et al. for details. to determine how far to prune back we can evaluate the cross-validated error on each such subtree and then pick the tree whose cv error is within standard error of the minimum. this is illustrated in figure the point with the minimum cv error corresponds to the simple tree in figure chapter adaptive basis function models sl sl sw sw sl sl versicolor setosa sw sw virginica y pruned decision tree versicolor setosa virginica versicolor setosa x figure pruned decision tree for iris data. figure generated by dtreedemoiris. pros and cons of trees cart models are popular for several reasons they are easy to they can easily handle mixed discrete and continuous inputs they are insensitive to monotone transformations of the inputs the split points are based on ranking the data points they perform automatic variable selection they are relatively robust to outliers they scale well to large data sets and they can be modified to handle missing however cart models also have some disadvantages. the primary one is that they do not predict very accurately compared to other kinds of model. this is in part due to the greedy nature of the tree construction algorithm. a related problem is that trees are unstable small changes to the input data can have large effects on the structure of the tree due to the hierarchical nature of the tree-growing process causing errors at the top to affect the rest of the tree. in frequentist terminology we say that trees are high variance estimators. we discuss a solution to this below. random forests one way to reduce the variance of an estimate is to average together many estimates. for example we can train m different trees on different subsets of the data chosen randomly with we can postprocess the tree to derive a series of logical rules such as if then the standard heuristic for handling missing inputs in decision trees is to look for a series of backup variables which can induce a similar partition to the chosen variable at any given split these can be used in case the chosen variable is unobserved at test time. these are called surrogate splits. this method finds highly correlated features and can be thought of as learning a local joint model of the input. this has the advantage over a generative model of not modeling the entire joint distribution of inputs but it has the disadvantage of being entirely ad hoc. a simpler approach applicable to categorical variables is to code missing as a new value and then to treat the data as fully observed. classification and regression trees replacement and then compute the ensemble f m fmx where fm is the m th tree. this technique is called bagging which stands for bootstrap aggregating unfortunately simply re-running the same learning algorithm on different subsets of the data can result in highly correlated predictors which limits the amount of variance reduction that is possible. the technique known as random forests tries to decorrelate the base learners by learning trees based on a randomly chosen subset of input variables as well as a randomly chosen subset of data cases. such models often have very good predictive accuracy and niculescu-mizil and have been widely used in many applications for body pose recognition using microsoft s popular kinect sensor et al. bagging is a frequentist concept. it is also possible to adopt a bayesian approach to learning trees. in particular et al. denison et al. wu et al. perform approximate inference over the space of trees and parameters using mcmc. this reduces the variance of the predictions. we can also perform bayesian inference over the space of ensembles of trees which tends to work much better. this is known as bayesian adaptive regression trees or bart et al. note that the cost of these sampling-based bayesian methods is comparable to the sampling-based random forest method. that is both approaches are farily slow to train but produce high quality classifiers. unfortunately methods that use multiple trees derived from a bayesian or frequentist standpoint lose their nice interpretability properties. fortunately various post-processing measures can be applied as discussed in section cart compared to hierarchical mixture of experts an interesting alternative to a decision tree is known as the hierarchical mixture of experts. figure gives an illustration where we have two levels of experts. this can be thought of as a probabilistic decision tree of depth since we recursively partition the space and apply a different expert to each partition. hastie et al. et al. write that the hme approach is a promising competitor to cart trees some of the advantages include the following the model can partition the input space using any set of nested linear decision boundaries. by contrast standard decision trees are constrained to use axis-parallel splits. the model makes predictions by averaging over all experts. by contrast in a standard decision tree predictions are made only based on the model in the corresponding leaf. since leaves often contain few training examples this can result in overfitting. fitting an hme involves solving a smooth continuous optimization problem using em which is likely to be less prone to local optima than the standard greedy discrete optimization methods used to fit decision trees. for similar reasons it is computationally easier to be bayesian about the parameters of an hme e.g. et al. bishop chapter adaptive basis function models and svens n than about the structure and parameters of a decision tree e.g. et al. generalized additive models a simple way to create a nonlinear model with multiple inputs is to use a generalized additive model and tibshirani which is a model of the form f fdxd here each fj can be modeled by some scatterplot smoother and f can be mapped to pyx using a link function as in a glm the term generalized additive model. if we use regression splines some other fixed basis function expansion approach for the fj then each fjxj can be written as t j jxj so the whole model can be written as f t where dxd. however it is more common to use smoothing splines for the fj. in this case the objective the regression setting becomes yi j j j fd fjxij where j is the strength of the regularizer for fj. backfitting we now discuss how to fit the model using mle. the constant is not uniquely identifiable since we can always add or subtract constants to any of the fj functions. the convention is to assume fjxij for all j. in this case the mle for is just n to fit the rest of the model we can center the responses subtracting and then iteratively update each fj in turn using as a target vector the residuals obtained by omitting term fj yi. fkxikn fj smootheryi fj fj n fjxij we should then ensure the output is zero mean using this is called the backfitting algorithm and tibshirani if x has full column rank then the above objective is convex each smoothing spline is a linear operator as shown in section so this procedure is guaranteed to converge to the global optimum. in the glm case we need to modify the method somewhat. the basic idea is to replace the weighted least squares step of irls section with a weighted backfitting algorithm. in the logistic regression case each response has weight si i associated with it where i sigm fjxij. generalized additive models computational efficiency each call to the smoother takes on time so the total cost is on dt where t is the number of iterations. if we have high-dimensional inputs fitting a gam is expensive. one approach is to combine it with a sparsity penalty see e.g. the spam additive model approach of et al. alternatively we can use a greedy approach such as boosting section multivariate adaptive regression splines in general we can create an anova we can extend gams by allowing for interaction effects. decomposition f fjxj fjkxj xk fjklxj xk xl jk jkl of course we cannot allow for too many higher-order interactions because there will be too many parameters to fit. it is common to use greedy search to decide which variables to add. the multivariate adaptive regression splines or mars algorithm is one example of this et al. it fits models of the form in equation where it uses a tensor product basis of regression splines to represent the multidimensional regression functions. for example for input we might use f m m m to create such a function we start with a set of candidate basis functions of the form c t xj t xn j j d these are linear splines where the knots are at all the observed values for that variable. we consider splines sloping up in both directions this is called a reflecting pair. see figure let m represent the current set of basis functions. we initialize by using m we consider creating a new basis function pair by multplying an hm m with one of the reflecting pairs in c. for example we might initially get f obtained by multiplying with a reflecting pair involving with knot t this pair is added to m. see figure at the next step we might create a model such as f obtained by multiplying from m by the new reflecting pair and this new function is shown in figure chapter adaptive basis function models linear spline function with a knot at solid blue dotted red x. a figure mars model in given by equation a simple mars model in given by equation figure generated by marsdemo. we proceed in this way until the model becomes very large. may impose an upper bound on the order of interactions. then we prune backwards at each step eliminating the basis function that causes the smallest increase in the residual error until the cv error stops improving. the whole procedure is closely related to cart. to see this suppose we replace the piecewise linear basis functions by step functions ixj t and ixj t. multiplying by a pair of reflected step functions is equivalent to splitting a node. now suppose we impose the constraint that once a variable is involved in a multiplication by a candidate term that variable gets replaced by the interaction so the original variable is no longer available. this ensures that a variable can not be split more than once thus guaranteeing that the resulting model can be represented as a tree. in this case the mars growing strategy is the same as the cart growing strategy. boosting boosting and freund is a greedy algorithm for fitting adaptive basis-function models of the form in equation where the m are generated by an algorithm called a weak learner or a base learner. the algorithm works by applying the weak learner sequentially to weighted versions of the data where more weight is given to examples that were misclassified by earlier rounds. this weak learner can be any classification or regression algorithm but it is common to use a cart model. in the late leo breiman called boosting where the weak learner is a shallow decision tree the best off-the-shelf classifier in the world et al. this is supported by an extensive empirical comparison of different classifiers in and niculescu-mizil who showed that boosted decision trees were the best both in terms of misclassification error and in terms of producing well-calibrated probabilities as judged by roc curves. second best method was random forests invented by breiman see section by contrast single decision trees performed very poorly. boosting was originally derived in the computational learning theory literature freund and schapire where the focus is binary classification. in these papers it was proved that one could boost the performance the training set of any weak learner arbitrarily boosting train test figure performance of adaboost using a decision stump as a weak learner on the data in figure training blue and test red error vs number of iterations. figure generated by boostingdemo written by richard stapenhurst. high provided the weak learner could always perform slightly better than chance. for example in figure we plot the training and test error for boosted decision stumps on a dataset shown in figure we see that the training set error rapidly goes to near zero. what is more surprising is that the test set error continues to decline even after the training set error has reached zero the test set error will eventually go up. thus boosting is very resistant to overfitting. decision stumps form the basis of a very successful face detector and jones which was used to generate the results in figure and which is used in many digital cameras. in view of its stunning empirical success statisticians started to become interested in this method. breiman showed that boosting can be interpreted as a form of gradient descent in function space. this view was then extended in et al. who showed how boosting could be extended to handle a variety of loss functions including for regression robust regression poisson regression etc. in this section we shall present this statistical interpretation of boosting drawing on the reviews in and hothorn and et al. which should be consulted for further details. forward stagewise additive modeling the goal of boosting is to solve the following optimization problem lyi f min f and ly y is some loss function and f is assumed to be an abm model as in equation common choices for the loss function are listed in table if we use squared error loss the optimal estimate is given by f f e argmin eyx f chapter adaptive basis function models name squared error absolute error exponential loss logloss loss f f exp yif yi exp yif e yifi derivative yi f sgnyi f yi i algorithm f e medianyxi gradient boosting log i i log i i adaboost logitboost some commonly used loss functions their gradients their population minimizers f table and some algorithms to minimize the loss. for binary classification problems we assume yi yi and i for regression problems we assume yi r. adapted from et al. and and hothorn logloss exp s s o l y f figure illustration of various loss functions for binary classification. the horizontal axis is the margin y the vertical axis is the loss. the log loss uses log base figure generated by hingelossplot. as we showed in section of course this cannot be computed in practice since it requires knowing the true conditional distribution pyx. hence this is sometimes called the population minimizer where the expectation is interpreted in a frequentist sense. below we will see that boosting will try to approximate this conditional expectation. for binary classification the obvious loss is loss but this is not differentiable. instead it is common to use logloss which is a convex upper bound on loss as we showed in section in this case one can show that the optimal estimate is given by log where y one can generalize this framework to the multiclass case but we will not discuss that here. an alternative convex upper bound is exponential loss defined by l y f exp yf see figure for a plot. this will have some computational advantages over the logloss to be discussed below. it turns out that the optimal estimate for this loss is also f f p y p y boosting p p y to see this we can just set the derivative of the expected loss each x to log zero f e e yf y f p y f p y f p y p y p y so in both cases we can see that boosting should try to approximate the log-odds ratio. since finding the optimal f is hard we shall tackle it sequentially. we initialise by defining arg min lyi f for example if we use squared error we can set y and if we use log-loss or exponential loss we can set iyi we could also use a more powerful model for our baseline such as a glm. where log n then at iteration m we compute lyi fm m m argmin and then we set fmx fm m m the key point is that we do not go back and adjust earlier parameters. this is why the method is called forward stagewise additive modeling. we continue this for a fixed number of iterations m in fact m is the main tuning parameter of the method. often we pick it by monitoring the performance on a separate validation set and then stopping once performance starts to decrease this is called early stopping. alternatively we can use model selection criteria such as aic or bic e.g. and hothorn for details. in practice better set performance can be obtained by performing partial updates of the form fmx fm m m here is a step-size parameter. in practice it is common to use a small value such as this is called shrinkage. below we discuss how to solve the suproblem in equation this will depend on the form of loss function. however it is independent of the form of weak learner. suppose we used squared error loss. then at step m the loss has the form lyi fm chapter adaptive basis function models figure example of adaboost using a decision stump as a weak learner. the degree of blackness represents the confidence in the red class. the degree of whiteness represents the confidence in the blue class. the size of the datapoints represents their weight. decision boundary is in yellow. after after rounds. figure generated by boostingdemo written by richard round. stapenhurst. after rounds. where rim yi fm is the current residual and we have set without loss of generality. hence we can find the new basis function by using the weak learner to predict rm. this is called or least squares boosting and yu in section we will see that this method with a suitable choice of weak learner can be made to give the same results as lars which can be used to perform variable selection section adaboost consider a binary classification problem with exponential loss. at step m we have to minimize lm where wim exp yifm is a weight applied to datacase i and yi we can rewrite this objective as follows wim e exp yifm wim exp yi lm e m log errm errm yi wim e wimi yi e consequently the optimal function to add is m argmin wimi yi wim this can be found by applying the weak learner to a weighted version of the dataset with weights wim. subsituting m into lm and solving for we find boosting where errm wii yi mxi wim the overall update becomes fmx fm m mx with this the weights at the next iteration become wime m yi mxi wime mxi m i mxie m where we exploited the fact that yi mxi if yi mxi and yi mxi otherwise. since e m will cancel out in the normalization step we can drop it. the result is the algorithm shown in algorithm known an example of this algorithm in action using decision stumps as the weak learner is given in figure we see that after many iterations we can carve out a complex decision boundary. what is rather surprising is that adaboost is very slow to overfit as is apparent in figure see section for a discussion of this point. algorithm for binary classification with exponential loss wi for m do fit a classifier mx to the training set using weights w wim i mxi compute errm compute m errmerrm set wi wi exp mi yi mxi m mx wim return f sgn logitboost the trouble with exponential loss is that it puts a lot of weight on misclassified examples as is apparent from the exponential blowup on the left hand side of figure this makes the method very sensitive to outliers examples. in addition e yf is not the logarithm of any pmf for binary variables y consequently we cannot recover probability estimates from f in et al. this is called discrete adaboost since it assumes that the base classifier m returns a binary class label. if m returns a probability instead a modified algorithm known as real adaboost can be used. see et al. for details. chapter adaptive basis function models a natural alternative is to use logloss instead. this only punishes mistakes linearly as is clear from figure furthermore it means that we will be able to extract probabilities from the final learned function using py e ef e f ef the goal is to minimze the expected log-loss given by lm log exp yifm by performing a newton upate on this objective to irls one can derive the algorithm shown in algorithm this is known as logitboost et al. it can be generalized to the multi-class setting as explained in et al. algorithm logitboost for binary classification with log-loss wi i for m do i i y compute the working response zi i compute the weights wi i wizi m argmin update f f compute i exp mx return f sgn mx boosting as functional gradient descent rather than deriving new versions of boosting for every different loss function it is possible to derive a generic version known as gradient boosting mason et al. to explain this imagine minimizing f argmin f lf where f f are the parameters we will solve this stagewise using gradient descent. at step m letg m be the gradient of lf evaluated at f fm gim lyi f f f gradients of some common loss functions are given in table we then make the update fm fm mgm boosting where m is the step length chosen by m argmin lfm gm this is called functional gradient descent. in its current form this is not much use since it only optimizes f at a fixed set of n points so we do not learn a function that can generalize. however we can modify the algorithm by fitting a weak learner to approximate the negative gradient signal. that is we use this update gim m argmin the overall algorithm is summarized in algorithm which is not strictly necessary as argued in and hothorn have omitted the line search step algorithm gradient boosting initialize argmin for m do lyi compute the gradient residual using rim f use the weak learner to compute m which minimizes update fmx fm m lyif f return f fm if we apply this algorithm using squared loss we recover if we apply this algorithm to log-loss we get an algorithm known as binomialboost and hothorn the advantage of this over logitboost is that it does not need to be able to do weighted fitting it just applies any black-box regression model to the gradient vector. also it is relatively easy to extend to the multi-class case et al. we can also apply this algorithm to other loss functions such as the huber loss which is more robust to outliers than squared error loss. sparse boosting suppose we use as our weak learner the following algorithm search over all possible variables j and pick the one jm that best predicts the residual vector jm argmin j xijrim ij jm mx jmm xjm chapter adaptive basis function models this method which is known as sparse boosting and yu is identical to the matching pursuit algorithm discussed in section it is clear that this will result in a sparse estimate at least if m is small. to see this let us rewrite the update as follows m m jmm where the non-zero entry occurs in location jm. this is known as forward stagewise linear regression et al. which becomes equivalent to the lar algorithm discussed in section as increasing the number of steps m in boosting is analogous to decreasing the regularization penalty if we modify boosting to allow some variable deletion steps and yu we can make it equivalent to the lars algorithm which computes the full regularization path for the lasso problem. the same algorithm can be used for sparse logistic regression by simply modifying the residual to be the appropriate negative gradient. now consider a weak learner that is similar to the above except it uses a smoothing spline instead of linear regression when mapping from xj to the residual. the result is a sparse generalized additive model section it can obviously be extended to pick pairs of variables at a time. the resulting method often works much better than mars and yu multivariate adaptive regression trees it is quite common to use cart models as weak learners. it is usually advisable to use a shallow tree so that the variance is low. even though the bias will be high a shallow tree is likely to be far from the truth this will compensated for in subsequent rounds of boosting. the height of the tree is an additional tuning parameter addition to m the number of rounds of boosting and the shrinkage factor. suppose we restrict to trees with j leaves. if j we get a stump which can only split on a single variable. if j we allow for in general it is recommended in et al. two-variable interactions etc. and and niculescu-mizil to use j if we combine the gradient boosting algorithm with regression trees we get a model known as mart which stands for multivariate adaptive regression trees this actually includes a slight refinement to the basic gradient boosting algorithm after fitting a regression tree to the residual gradient we re-estimate the parameters at the leaves of the tree to minimize the loss jm argmin xi rjm lyi fm where rjm is the region for leaf j in the m th tree and jm is the corresponding parameter mean response of y for regression problems or the most probable class label for classification problems. why does boosting work so well? we have seen that boosting works very well especially for classifiers. there are two main reasons for this. first it can be seen as a form of regularization which is known to help feedforward neural networks perceptrons prevent overfitting by eliminating irrelevant features. to see this imagine pre-computing all possible weak-learners and defining a feature vector of the form kx. we could use regularization to select a subset of these. alternatively we can use boosting where at each step the weak learner creates a new k on the fly. it is possible to combine boosting and regularization to get an algorithm known as and singer essentially this method greedily adds the best features learners using boosting and then prunes off irrelevant ones using regularization. another explanation has to do with the concept of margin which we introduced in section et al. ratsch et al. proved that adaboost maximizes the margin on the training data. et al. generalized this to other loss functions such as log-loss. a bayesian view so far our presentation of boosting has been very frequentist since it has focussed on greedily minimizing loss functions. a likelihood interpretation of the algorithm was given in and mackay meek et al. the idea is to consider a mixture of experts model of the form pyx mpyx m where each expert pyx m is like a weak learner. we usually fit all m experts at once using em but we can imagine a sequential scheme whereby we only update the parameters in the e step the posterior responsibilities will reflect how well the for one expert at a time. existing experts explain a given data point if this is a poor fit these data points will have more influence on the next expert that is fitted. view naturally suggest a way to use a boosting-like algorithm for unsupervised learning we simply sequentially fit mixture models instead of mixtures of experts. notice that this is a rather broken mle procedure since it never goes back to update the parameters of an old expert. similarly if boosting ever wants to change the weight assigned to a weak learner the only way to do this is to add the weak learner again with a new weight. this can result in unnecessarily large models. by contrast the bart model et al. uses a bayesian version of backfitting to fit a small sum of weak learners trees. feedforward neural networks perceptrons a feedforward neural network aka multi-layer perceptron is a series of logistic regression models stacked on top of each other with the final layer being either another logistic regression or a linear regression model depending on whether we are solving a classification or regression problem. for example if we have two layers and we are solving a regression problem the model has the form pyx zx zx where g is a non-linear activation or transfer function the logistic function zx x v is called the hidden layer deterministic function of the input h is the x gvt h x chapter adaptive basis function models xnd xni vij znh znj wjk ync ynk figure a neural network with one hidden layer. number of hidden units v is the weight matrix from the inputs to the hidden nodes and w is the weight vector from the hidden nodes to the output. it is important that g be nonlinear otherwise the whole model collapses into a large linear regression model of the form y wt one can show that an mlp is a universal approximator meaning it can model any suitably smooth function given enough hidden units to any desired level of accuracy to handle binary classification we pass the output through a sigmoid as in a glm pyx berysigmwt zx we can easily extend the mlp to predict multiple outputs. for example in the regression case we have pyx n v see figure for an illustration. if we add mutual inhibition arcs between the output units ensuring that only one of them turns on we can enforce a sum-to-one constraint which can be used for multi-class classification. the resulting model has the form pyx catyswzx convolutional neural networks the purpose of the hidden units is to learn non-linear combinations of the original inputs this is called feature extraction or feature construction. these hidden features are then passed as input to the final glm. this approach is particularly useful for problems where the original input features are not very individually informative. for example each pixel in an image is not very informative it is the combination of pixels that tells us what objects are present. conversely for a task such as document classification using a bag of words representation each feature count is informative on its own so extracting higher order features is less important. not suprisingly then much of the work in neural networks has been motivated by visual pattern feedforward neural networks perceptrons source httpwww.codep figure the convolutional neural network from et al. roject.comkblibraryneuralnetrecognition.aspx used with kind permission of mike o neill. recognition et al. although they have also been applied to other types of data including text and weston a form of mlp which is particularly well suited to signals like speech or text or signals like images is the convolutional neural network. this is an mlp in which the hidden units have local receptive fields in the primary visual cortex and in which the weights are tied or shared across the image in order to reduce the number of parameters. intuitively the effect of such spatial parameter tying is that any useful features that are discovered in some portion of the image can be re-used everywhere else without having to be independently learned. the resulting network then exhibits translation invariance meaning it can classify patterns no matter where they occur inside the input image. figure gives an example of a convolutional network designed by simard and colleagues et al. with layers layers of adjustable parameters designed to classify gray-scale images of handwritten digits from the mnist dataset section in layer we have feature maps each of which has size each hidden node in one of these feature maps is computed by convolving the image with a weight matrix called a kernel adding a bias and then passing the result through some form of nonlinearity. there are therefore neurons in layer and weights. is for the bias. if we did not share these parameters there would be weights at the first layer. in layer we have feature maps each of which is obtained by convolving each feature map in layer with a weight matrix adding them up adding a bias and passing through a nonlinearity. there are therefore neurons in layer adjustable weights kernel for each pair of feature chapter adaptive basis function models maps in layers and and connections. layer is fully connected to layer and has neurons and weights. finally layer is also fully connected and has neurons and weights. adding the above numbers there are a total of neurons adjustable weights and connections. this model is usually trained using stochastic gradient descent section for details. a single pass over the data set is called an epoch. when mike o neill did these experiments in he found that a single epoch took about minutes that there are training examples in mnist. since it took about epochs for the error rate to converge the total training time was about using this technique he obtained a misclassification rate on the test cases of about to further reduce the error rate a standard trick is to expand the training set by including distorted versions of the original data to encourage the network to be invariant to small changes that don t affect the identity of the digit. these can be created by applying a random flow field to shift pixels around. see figure for some examples. we use online training such as stochastic gradient descent we can create these distortions on the fly rather than having to store them. using this technique mike o neill obtained a misclassification rate on the test cases of about which is close to the current state of the yann le cun and colleagues et al. obtained similar performance using a slightly more complicated architecture shown in figure this model is known as and historically it came before the model in figure there are two main differences. first has a subsampling layer between each convolutional layer which either averages or computes the max over each small window in the previous layer in order to reduce the size and to obtain a small amount of shift invariance. the convolution and sub-sampling combination was inspired by hubel and wiesel s model of simple and complex cells in the visual cortex and wiesel and it continues to be popular in neurally-inspired models of visual object recognition and poggio a similar idea first appeared in fukushima s neocognitron though no globally supervised training algorithm was available at that time. the second difference between and the simard architecture is that the final layer is actually an rbf network rather than a more standard sigmoidal or softmax layer. this model gets a test error rate of about when trained with no distortions and when trained with distortions. figure shows all errors made by the system. some are genuinely ambiguous but several are errors that a person would never make. a web-based demo of the can be found at httpyann.lecun.comexdblenetindex.html. of course classifying isolated digits is of limited applicability in the real world people usually write strings of digits or other letters. this requires both segmentation and classification. le cun and colleagues devised a way to combine convolutional neural networks with a model similar to a conditional random field in section to solve this problem. the system was eventually deployed by the us postal service. et al. for a more detailed account of the system which remains one of the best performing systems for this task. implementation details mike used c code and a variety of speedup tricks. he was using standard era hardware intel pentium hyperthreaded processor running at see httpwww.codeproject.comkb libraryneuralnetrecognition.aspx for details. a list of various methods along with their misclassification rates on the mnist test set is available from http error rates within of each other are not statistically significantly different. feedforward neural networks perceptrons figure several synthetic warpings of a handwritten digit. based on figure of figure generated by elasticdistortionsdemo written by kevin swersky. input feature maps f. maps f. maps f. maps layer layer output convolutions subsampling convolutions subsampling full connection full connection gaussian connections figure et al. used with kind permission of yann lecun. a convolutional neural net for classifying handwritten digits. source figure from chapter adaptive basis function models figure these are the errors made by on the test cases of mnist. below each image is a label of the form correct-label estimated-label. source figure of et al. used with kind permission of yann lecun. to figure which shows the results of a deep generative model. other kinds of neural networks other network topologies are possible besides the ones discussed above. for example we can have skip arcs that go directly from the input to the output skipping the hidden layer we can have sparse connections between the layers etc. however the mlp always requires that the weights form a directed acyclic graph. if we allow feedback connections the model is known as a recurrent neural network this defines a nonlinear dynamical system but does not have a simple probabilistic interpretation. such rnn models are currently the best approach for language modeling performing word prediction in natural language et al. significantly outperforming the standard n-gram-based methods discussed in section if we allow symmetric connections between the hidden units the model is known as a hopfield network or associative memory its probabilistic counterpart is known as a boltzmann machine section and can be used for unsupervised learning. a brief history of the field neural networks have been the subject of great interest for many decades due to the desire to understand the brain and to build learning machines. it is not possible to review the entire history here. instead we just give a few edited highlights the field is generally viewed as starting with mcculloch and pitts and pitts who devised a simple mathematical model of the neuron in in which they approximated the feedforward neural networks perceptrons output as a weighted sum of inputs passed through a threshold function y i i wixi for some threshold this is similar to a sigmoidal activation function. frank rosenblatt invented the perceptron learning algorithm in which is a way to estimate the parameters of a mcculloch-pitts neuron section for details. a very similar model called the adaline adaptive linear element was invented in by widrow and hoff. in minsky and papert and papert published a famous book called perceptrons in which they showed that such linear models with no hidden layers were very limited in their power since they cannot classify data that is not linearly separable. this considerably reduced interest in the field. in rumelhart hinton and williams et al. discovered the backpropathe gation algorithm section which allows one to fit models with hidden layers. backpropagation algorithm was originally discovered in and ho and independently in however it was et al. that brought the algorithm to people s attention. this spawned a decade of intense interest in these models. in sejnowski and rosenberg and rosenberg created the famous nettalk system that learned a mapping from english words to phonetic symbols which could be fed into a speech synthesizer. an audio demo of the system as it learns over time can be found at the systems starts by babbling and then gradually learns to pronounce english words. nettalk learned a distributed representation its hidden layer of various sounds and its success spawned a big debate in psychology between connectionism based on neural networks and computationalism based on syntactic rules. this debate lives on to some extent in the machine learning community where there are still arguments about whether learning is best performed using low-level neurallike representations or using more structured models. in yann le cun and others et al. created the famous lenet system described in section in the support vector machine section was invented et al. svms provide similar prediction accuracy to neural networks while being considerably easier to train they use a convex objective function. this spawned a decade of interest in kernel methods in note however that svms do not use adaptive basis functions so they require a fair amount of human expertise to design the right kernel function. in geoff hinton invented the contrastive divergence training procedure which provided a way for the first time to learn deep networks by training one layer at a time in an unsupervised fashion section for details. this in turn has spawned renewed interest in neural networks over the last few years chapter the backpropagation algorithm unlike a glm the nll of an mlp is a non-convex function of its parameters. nevertheless we can find a locally optimal ml or map estimate using standard gradient-based optimization methods. since mlps have lots of parameters they are often trained on very large data sets. it became part of the folklore during the that to get published in the top machine learning conference known as nips which stands for neural information processing systems it was important to ensure your paper did not contain the word neural network chapter adaptive basis function models tanh sigmoid figure two possible activation functions. tanh maps r to and is the preferred nonlinearity for the hidden nodes. sigm maps r to and is the preferred nonlinearity for binary nodes at the output layer. figure generated by tanhplot. consequently it is common to use first-order online methods such as stochastic gradient descent whereas glms are usually fit with irls which is a second-order offline method. we now discuss how to compute the gradient vector of the nll by applying the chain rule of calculus. the resulting algorithm is known as backpropagation for reasons that will become apparent. for notational simplicity we shall assume a model with just one hidden layer. it is helpful to distinguish the pre- and post-synaptic values of a neuron that is before and after we apply the nonlinearity. let xn be the n th input an vxn be the pre-synaptic hidden layer and zn gan be the post-synaptic hidden layer where g is some transfer function. we typically use ga sigma but we may also use ga tanha see figure for a comparison. the input to sigm or tanh is a vector we assume it is applied component-wise. we now convert this hidden layer to the output layer as follows. let bn wzn be the pre-synaptic output layer and yn hbn be the post-synaptic output layer where h is another nonlinearity corresponding to the canonical link for the glm. reserve the notation yn without the hat for the output corresponding to the n th training case. for a regression model we use hb b for binary classifcation we use hb sigmbc for multi-class classification we use hb sb. we can write the overall model as follows v an g zn w bn h yn xn the parameters of the model are w the first and second layer weight matrices. offset or bias terms can be accomodated by clamping an element of xn and zn to in the regression setting we can easily estimate the variance of the output noise using the empirical variance of the y after training is complete. there will be one value of for each output node residual errors n if we are performing multi-target regression as we usually assume. feedforward neural networks perceptrons n k in the regression case with k outputs the nll is given by the squared error j ynk in the classification case with k classes the nll is given by the cross entropy j ynk log ynk our task is to compute j. we will derive this for each n separately the overall gradient is obtained by summing over n although often we just use a mini-batch section n k let us start by considering the output layer weights. we have wk wk bnk jn zn jn bnk jn bnk since bnk wt equation tells us that k zn. assuming h is the canonical link function for the output glm then jn bnk w nk ynk ynk which is the error signal. so the overall gradient is wk jn w nkzn which is the pre-synaptic input to the output layer namely zn times the error signal namely w nk. for the input layer weights we have vj jn vj anj v njxn jn anj j xn. all that remains is to compute the first level where we exploited the fact that anj vt error signal v nj. we have v nj jn anj jn bnk bnk anj j now bnk so wkjganj bnk anj da ga. for tanh units where for sigmoid units d d v nj w w nk bnk anj da tanha and d da hence chapter adaptive basis function models thus the layer errors can be computed by passing the layer errors back through the w matrix hence the term backpropagation the key property is that we can compute the gradients locally each node only needs to know about its immediate neighbors. this is supposed to make the algorithm neurally plausible although this interpretation is somewhat controversial. putting it all together we can compute all the gradients as follows we first perform a forwards pass to compute an zn bn and yn. we then compute the error for the output layer n yn yn which we pass backwards through w using equation to compute the error for the hidden layer n we then compute the overall gradient as follows j ij jk v nxn w n zn n identifiability it is easy to see that the parameters of a neural network are not identifiable. for example we can change the sign of the weights going into one of the hidden units so long as we change the sign of all the weights going out of it these effects cancel since tanh is an odd function so tanh a tanha. there will be h such sign flip symmetries leading to equivalent settings of the parameters. similarly we can change the identity of the hidden units without affecting the likelihood. there are h! such permutations. the total number of equivalent parameter settings the same likelihood is therefore in addition there may be local minima due to the non-convexity of the nll. this can be a more serious problem although with enough data these local optima are often quite shallow and simple stochastic optimization methods can avoid them. in addition it is common to perform multiple restarts and to pick the best solution or to average over the resulting predictions. does not make sense to average the parameters themselves since they are not identifiable. regularization as usual the mle can overfit especially if the number of nodes is large. a simple way to prevent this is called early stopping which means stopping the training procedure when the error on the validation set first starts to increase. this method works because we usually initialize from small random weights so the model is initially simple the tanh and sigm functions are nearly linear near the origin. as training progresses the weights become larger and the model becomes nonlinear. eventually it will overfit. another way to prevent overfitting that is more in keeping with the approaches used elsewhere in this book is to impose a prior on the parameters and then use map estimation. it is standard to use a n prior to regularization where is the precision of the prior. in the neural networks literature this is called weight decay since it encourages small weights and hence simpler models. the penalized nll objective becomes j log pynxn ij jk feedforward neural networks perceptrons j that we don t penalize the bias terms. the gradient of the modified objective becomes v nxn v w n zn w n n as in section if the regularization is sufficiently strong it does not matter if we have too many hidden units from wasted computation. hence it is advisable to set h to be as large as you can afford and then to choose an appropriate regularizer. we can set the parameter by cross validation or empirical bayes section as with ridge regression it is good practice to standardize the inputs to zero mean and unit variance so that the spherical gaussian prior makes sense. consistent gaussian priors one can show that using the same regularization parameter for both the first and second layer weights results in the lack of a certain desirable invariance property. in particular suppose we linearly scale and shift the inputs andor outputs to a neural network regression model. then we would like the model to learn to predict the same function by suitably scaling its internal weights and bias terms. however the amount of scaling needed by the first and second layer weights to compensate for a change in the inputs andor outputs is not the same. therefore we need to use a different regularization strength for the first and second layer. fortunately this is easy to do we just use the following prior in c p n w in v in b i where b and c are the bias to get a feeling for the effect of these hyper-parameters we can sample mlp parameters from this prior and plot the resulting random functions. figure shows some examples. decreasing v allows the first layer weights to get bigger making the sigmoid-like shape of the functions steeper. decreasing b allows the first layer biases to get bigger which allows the center of the sigmoid to shift left and right more. decreasing w allows the second layer weights to get bigger making the functions more wiggly sensitivity to change in the input and hence larger dynamic range. and decreasing c allows the second layer biases to get bigger allowing the mean level of the function to move up and down more. chapter we will see an easier way to define priors over functions. weight pruning since there are many weights in a neural network it is often helpful to encourage sparsity. various ad-hoc methods for doing this with names such as optimal brain damage were devised in the see e.g. for details. since we are regularizing the output bias terms it is helpful in the case of regression to normalize the target responses in the training set to zero mean to be consistent with the fact that the prior on the output bias has zero mean. chapter adaptive basis function models figure the effects of changing the hyper-parameters on an mlp. v b w c factor of mlppriorsdemo. default parameter values decreasing b by decreasing c by factor of figure generated by decreasing w by factor of decreasing v by factor of feedforward neural networks perceptrons neural network y data deep neural net figure a deep but sparse neural network. the connections are pruned using regularization. at each level nodes numbered are clamped to so their outgoing weights correspond to the offsetbias predictions made by the model on the training set. figure generated by sparsennetdemo terms. written by mark schmidt. however we can also use the more principled sparsity-promoting techniques we discussed in chapter one approach is to use an regularizer. see figure for an example. another approach is to use ard this is discussed in more detail in section soft weight sharing another way to regularize the parameters is to encourage similar weights to share statistical strength. but how do we know which parameters to group together? we can learn this by using a mixture model. that is we model p as a mixture of gaussians. parameters that are assigned to the same cluster will share the same mean and variance and thus will have similar values the variance for that cluster is low. this is called soft weight sharing and hinton in practice this technique is not widely used. see e.g. if you want to know the details. semi-supervised embedding an interesting way to regularize deep feedforward neural networks is to encourage the hidden layers to assign similar objects to similar representations. this is useful because it is often easy to obtain side information consisting of sets of pairs of similar and dissimilar objects. for example in a video classification task neighboring frames can be deemed similar but frames that are distant in time can be deemed dis-similar et al. note that this can be done without collecting any labels. let sij if examples i and j are similar and sij otherwise. let f be some embedding of item xi e.g. f zxi where z is the hidden layer of a neural network. now define a loss function lf f sij that depends on the embedding of two objects chapter adaptive basis function models and the observed similarity measure. for example we might want to force similar objects to have similar embeddings and to force the embeddings of dissimilar objects to be a minimal distance apart lfi fj sij m if sij if sij ij u where m is some minimal margin. we can now define an augmented loss function for training the neural network nllf yi lf f sij i l where l is the labeled training set u is the unlabeled training set and is some tradeoff parameter. this is called semi-supervised embedding et al. such an objective can be easily optimized by stochastic gradient descent. at each iteration pick a random labeled training example yn and take a gradient step to optimize nllf yi. then pick a random pair of similar unlabeled examples xi xj can sometimes be generated on the fly rather than stored in advance and make a gradient step to optimize lf f finally pick a random unlabeled example xk which with high probability is dissimilar to xi and make a gradient step to optimize lf f note that this technique is effective because it can leverage massive amounts of data. in a related approach and weston trained a neural network to distinguish valid english sentences from invalid ones. this was done by taking all million words from english wikipedia and then creating windows of length containing neighboring words. this constitutes the positive examples. to create negative examples the middle word of each window was replaced by a random english word is likely to be an invalid sentence either grammatically andor semantically with high probability. this neural network was then trained over the course of week and its latent representation was then used as the input to a supervised semantic role labeling task for which very little labeled training data is available. also and zhang for related work. bayesian inference although map estimation is a succesful way to reduce overfitting there are still some good reasons to want to adopt a fully bayesian approach to fitting neural networks integrating out the parameters instead of optimizing them is a much stronger form of regularization than map estimation. we can use bayesian model selection to determine things like the hyper-parameter settings and the number of hidden units. this is likely to be much faster than cross validation especially if we have many hyper-parameters as in ard. modelling uncertainty in the parameters will induce uncertainty in our predictive distributions which is important for certain problems such as active learning and risk-averse decision making. feedforward neural networks perceptrons we can use online inference methods such as the extended kalman filter to do online learning one can adopt a variety of approximate bayesian inference techniques in this context. in this section we discuss the laplace approximation first suggested in one can also use hybrid monte carlo or variational bayes and camp barber and bishop parameter posterior for regression we start by considering regression following the presentation of sec which summarizes the work of we will use a prior of the form pw n where w represents all the weights combined. we will denote the precision of the noise by the posterior can be approximated as follows pwd exp ew ew edw e w f edw ew wt w where ed is the data error ew is the prior error and e is the overall error log prior plus log likelihood. now let us make a second-order taylor series approximation of ew around its minimum map estimate ew ewm p wm p aw wm p where a is the hessian of e a ewm p h i where h edwm p is the hessian of the data error. this can be computed exactly in time where d is the number of parameters using a variant of backpropagation if we use a quasi-newton method to find sec for details. alternatively the mode we can use its internally computed approximation to h. that diagonal approximations of h are usually very inaccurate. in either case using this quadratic approximation the posterior becomes gaussian pw n p a chapter adaptive basis function models parameter posterior for classification the classification case is the same as the regression case except and ed is a crossentropy error of the form edw ln f w yn ln f w predictive posterior for regression the posterior predictive density is given by pyxd n w p a this is not analytically tractable because of the nonlinearity of f w. let us therefore construct a first-order taylor series approximation around the mode f w f wm p t wm p where g wf wwwm p we now have a linear-gaussian model with a gaussian prior on the weights. from equation we have pyxd n wm p where the predictive variance depends on the input x as follows gt a the error bars will be larger in regions of input space where we have little training data. see figure for an example. predictive posterior for classification in this section we discuss how to approximate pyxd in the case of binary classification. the situation is similar to the case of logistic regression discussed in section except in addition the posterior predictive mean is a non-linear function of w. specifically we have e w sigmax w where ax w is the pre-synaptic output of the final layer. let us make a linear approximation to this ax w am p t wm p where am p x wm p and g xax wm p can be found by a modified version of backpropagation. clearly paxd n wm p gxt a feedforward neural networks perceptrons t e g r a t data function network error bars input data function prediction samples figure the posterior predictive density for an mlp with hidden nodes trained on data points. the dashed green line is the true function. result of using a laplace approximation after performing empirical bayes to optimize the hyperparameters. the solid red line is the posterior mean prediction and the dotted blue lines are standard deviation above and below the mean. figure generated by mlpregevidencedemo. result of using hybrid monte carlo using the same trained hyperparameters as in the solid red line is the posterior mean prediction and the dotted blue lines are samples from the posterior predictive. figure generated by mlpreghmcdemo written by ian nabney. hence the posterior predictive for the output is py sigmapaxdda sigm abt wm p where is defined by equation which we repeat here for convenience of course a simpler potentially more accurate alternative to this is to draw a few samples from the gaussian posterior and to approximate the posterior predictive using monte carlo. in either case the effect of taking uncertainty of the parameters into account as in section is to moderate the confidence of the output the decision boundary itself is unaffected however. ard for neural networks once we have made the laplace approximation to the posterior we can optimize the marginal likelihood wrt the hyper-parameters using the same fixed-point equations as in section typically we use one hyper-parameter for the weight vector leaving each node to achieve an effect similar to group lasso that is the prior has the form n vi i n wj i if we find vi then input feature i is irrelevant and its weight vector vi is pruned out. similarly if we find wj then hidden feature j is irrelevant. this is known as automatic p chapter adaptive basis function models relevancy determination or ard which was discussed in detail in section applying this to neural networks gives us an efficient means of variable selection in non-linear models. the software package netlab contains a simple example of ard applied to a neural network called demard. this demo creates some data according to a nonlinear regression function f where is a noisy copy of we see that and are irrelevant for predicting the target. however is correlated with which is relevant. using ard the final hyper-parameters are as follows this clearly indicates that feature is irrelevant feature is only weakly relevant and feature is very relevant. ensemble learning m m ensemble learning refers to learning a weighted combination of base models of the form f wmfmyx where the wm are tunable parameters. ensemble learning is sometimes called a committee method since each base model fm gets a weighted vote clearly ensemble learning is closely related to learning adaptive-basis function models. in fact one can argue that a neural net is an ensemble method where fm represents the m th hidden unit and wm are the output layer weights. also we can think of boosting as kind of ensemble learning where the weights on the base models are determined sequentially. below we describe some other forms of ensemble learning. stacking an obvious way to estimate the weights in equation is to use w argmin w lyi wmfmx however this will result in overfitting with wm being large for the most complex model. a simple solution to this is to use cross-validation. in particular we can use the loocv estimate w argmin w lyi wm f i m where f i m is the predictor obtained by training on data excluding yi. this is known as stacking which stands for stacked generalization this technique is more robust to the case where the true model is not in the model class than standard bma this approach was used by the netflix team known as the ensemble which tied the submission of the winning team s pragmatic chaos in terms of accuracy et al. stacking has also been used for problems such as image segmentation and labeling. ensemble learning class table part of a error-correcting output code for a problem. each row defines a two-class problem. based on table of et al. error-correcting output codes an interesting form of ensemble learning is known as error-correcting output codes or ecoc and bakiri which can be used in the context of multi-class classification. the idea is that we are trying to decode a symbol the class label which has c possible states. we could use a bit vector of length b to encode the class label and train b separate binary classifiers to predict each bit. however by using more bits and by designing the codewords to have maximal hamming distance from each other we get a method that is more resistant to individual bit-flipping errors for example in table we use b bits to encode a c class problem. the minimum hamming distance between any pair of rows is the decoding rule is pbx cx min c where ccb is the b th bit of the codeword for class c. and hastie showed that a random code worked just as well as the optimal code both methods work by averaging the results of multiple classifiers thereby reducing variance. ensemble learning is not equivalent to bayes model averaging in section we discussed bayesian model selection. an alternative to picking the best model and then using this to make predictions is to make a weighted average of the predictions made by each model i.e. we compute pyxd pyx md m m this is called bayes model averaging and can sometimes give better performance than using any single model et al. of course averaging over all models is typically computationally infeasible integration is obviously not possible in a discrete space although one can sometimes use dynamic programming to perform the computation exactly e.g. and jaakkola a simple approximation is to sample a few models from the posterior. an even simpler approximation the one most widely used in practice is to just use the map model. it is important to note that bma is not equivalent to ensemble learning this latter technique corresponds to enlarging the model space by defining a single new model chapter adaptive basis function models model bst-dt rf bag-dt svm ann knn bst-stmp dt logreg nb table fraction of time each method achieved a specified rank when sorting by mean performance across datasets and metrics. based on table of and niculescu-mizil used with kind permission of alexandru niculescu-mizil. which is a convex combination of base models as follows pyx mpyx m m m in principle we can now perform bayesian inference to compute p we then make predictions using pyxd pyx however it is much more common to use point estimation methods for as we saw above. experimental comparison we have described many different methods for classification and regression. which one should you use? that depends on which inductive bias you think is most appropriate for your domain. usually this is hard to assess so it is common to just try several different methods and see how they perform empirically. below we summarize two such comparisons that were carefully conducted the data sets that were used are relatively small. see the website mlcomp.org for a distributed way to perform large scale comparisons of this kind. of course we must always remember the no free lunch theorem which tells us that there is no universally best learning method. low-dimensional features in rich caruana and alex niculescu-mizil and niculescu-mizil conducted a very extensive experimental comparison of different binary classification methods on different data sets. the data sets all had training cases and had test sets containing examples on average. the number of features ranged from to so this is much lower dimensional than the nips feature selection challenge. cross validation was used to assess average test error. is separate from any internal cv a method may need to use for model selection. experimental comparison the methods they compared are as follows in roughly decreasing order of performance as assessed by table bst-dt boosted decision trees rf random forest bag-dt bagged decision trees svm support vector machine ann artificial neural network knn k-nearest neighbors bst-stmp boosted stumps dt decision tree logreg logistic regression nb naive bayes they used different performance measures which can be divided into three groups. threshold metrics just require a point estimate as output. these include accuracy f-score etc. ordering ranking metrics measure how well positive cases are ordered before the negative cases. these include area under the roc curve average precision and the precisionrecall break even point. finally the probability metrics included cross-entropy and squared error methods such as svms that do not produce calibrated probabilities were post-processed using platt s logistic regression trick or using isotonic regression. performance measures were standardized to a scale so they could be compared. obviously the results vary by dataset and by metric. therefore just averaging the performance does not necessarily give reliable conclusions. however one can perform a bootstrap analysis which shows how robust the conclusions are to such changes. the results are shown in table we see that most of the time boosted decision trees are the best method followed by random forests bagged decision trees svms and neural networks. however the following methods all did relatively poorly knn stumps single decision trees logistic regression and naive bayes. these results are generally consistent with conventional wisdom of practioners in the field. of course the conclusions may change if there the features are high dimensional and or there are lots of irrelevant features in section or if there is lots of noise etc. high-dimensional features in the nips conference ran a competition where the goal was to solve binary classification problems with large numbers of irrelevant features given small training sets. was called a feature selection challenge but performance was measured in terms of predictive accuracy not in terms of the ability to select features. the five datasets that were used are summarized in table the term probe refers to artifical variables that were added to the problem to make it harder. these have no predictive power but are correlated with the original features. results of the competition are discussed in et al. the overall winner was an in a follow-up study approach based on bayesian neural networks and zhang chapter adaptive basis function models type dataset aracene dexter dorothea drug discovery gisette madelon domain mass spectrometry dense text classification sparse sparse dense dense digit recognition artificial d probes ntrain nval ntest table datasets the features are binary. for the others the features are real-valued. summary of the data used in the nips feature selection challenge. for the dorothea method hmc mlp boosted mlp bagged mlp boosted trees random forests screened features ard avg rank avg time avg rank avg time performance of different methods on the nips feature selection challenge. table stands for hybrid monte carlo see section we report the average rank is better across the datasets. we also report the average training time in minutes error in brackets. the mcmc and bagged mlps use two hidden layers of and units. the boosted mlps use one hidden layer with or hidden units. the boosted trees used depths between and and shrinkage between and each tree was trained on of the data chosen at random at each step stochastic gradient boosting. from table of et al. bayesian neural nets with hidden layers were compared to several other methods based on bagging and boosting. note that all of these methods are quite similar in each case the prediction has the form f wme m the bayesian mlp was fit by mcmc monte carlo so we set wm and set m in bagging we set wm and m is estimated by fitting to a draw from the posterior. in boosting we set wm and the m are the model to a bootstrap sample from the data. estimated sequentially. to improve computational and statistical performance some feature selection was performed. two methods were considered simple uni-variate screening using t-tests and a method based on mlpard. results of this follow-up study are shown in table we see that bayesian mlps are again the winner. in second place are either random forests or boosted mlps depending on the preprocessing. however it is not clear how statistically significant these differences are since the test sets are relatively small. in terms of training time we see that mcmc is much slower than the other methods. it would be interesting to see how well deterministic bayesian inference laplace approximation would perform. it will be much faster but the question is how much would one lose interpreting black-box models e c n e d n e p e d l a i t r a p e c n e d n e p e d l a i t r a p figure partial dependence plots for the predictors in friedman s synthetic regression problem. source figure of et al. used with kind permission of hugh chipman. in statistical performance? interpreting black-box models linear models are popular in part because they are easy to interpet. however they often are poor predictors which makes them a poor proxy for nature s mechanism thus any conclusions about the importance of particular variables should only be based on models that have good predictive accuracy many standard statistical tests of goodness of fit do not test the predictive accuracy of a model. in this chapter we studied black-box models which do have good predictive accuracy. unfortunately they are hard to interpret directly. fortunately there are various heuristics we can use to probe such models in order to assess which input variables are the most important. as a simple example consider the following non-linear function first proposed to illustrate the power of mars f sin where n we see that the output is a complex function of the inputs. by augmenting the x vector with additional irrelevant random variables all drawn uniform on we can create a challenging feature selection problem. in the experiments below we add extra dummy variables. chapter adaptive basis function models e g a s u figure average usage of each variable in a bart model fit to data where only the first features are relevant. the different coloured lines correspond to different numbers of trees in the ensemble. source figure of et al. used with kind permission of hugh chipman. one useful way to measure the effect of a set s of variables on the output is to compute a partial dependence plot this is a plot of f vs xs where f is defined as the response to xs with the other predictors averaged out f n f xi s figure shows an example where we use sets corresponding to each single variable. the data was generated from equation with irrelevant variables added. we then fit a bart model and computed the partial dependence plots. we see that the predicted response is invariant for s indicating that these variables are irrelevant. the response is roughly linear in and and roughly quadratic in error bars are obtained by computing empirical quantiles of f based on posterior samples of alternatively we can use bootstrap. another very useful summary computes the relative importance of predictor variables. this can be thought of as a nonlinear or even model free way of performing variable selection although the technique is restricted to ensembles of trees. the basic idea originally proposed in et al. is to count how often variable j is used as a node in any of the trees. ij tm be the proportion of all splitting rules that use xj in particular let vj where tm is the m th tree. if we can sample the posterior of trees we can easily m compute the posterior for vj. alternatively we can use bootstrap. figure gives an example using bart. we see that the five relevant variables are chosen much more than the five irrelevant variables. as we increase the number m of trees all the variables are more likely to be chosen reducing the sensitivity of this method but for small m the method is farily diagnostic. interpreting black-box models exercises exercise nonlinear regression for inverse dynamics in this question we fit a model which can predict what torques a robot needs to apply in order to make its arm reach a desired point in space. the data was collected from a sarcos robot arm with degrees of freedom. the input vector x r encodes the desired position velocity and accelaration of the joints. the output vector y r encodes the torques that should be applied to the joints to reach that point. the mapping from x to y is highly nonlinear. we have n training points and ntest testing points. for simplicity we following standard practice and focus on just predicting a scalar output namely the torque for the first joint. download the data from httpwww.gaussianprocess.orggpml. standardize the inputs so they have zero mean and unit variance on the training set and center the outputs so they have zero mean on the training set. apply the corresponding transformations to the test data. below we will describe various models which you should fit to this transformed data. then make predictions and compute the standardized mean squared error on the test set as follows sm se ntest train where ntrain is the variance of the output computed on the training set. a. the first method you should try is standard linear regression. turn in your numbers and code. to and williams you should be able to achieve a smse of using this method. b. now try running k-means clustering cross validation to pick k. then fit an rbf network to the data using the k estimated by k-means. use cv to estimate the rbf bandwidth. what smse do you get? turn in your numbers and code. to and williams gaussian process regression can get an smse of so the goal is to get close to that. c. now try fitting a feedforward neural network. use cv to pick the number of hidden units and the strength of the regularizer. what smse do you get? turn in your numbers and code. markov and hidden markov models introduction in this chapter we discuss probabilistic models for sequences of observations xt of arbitrary length t such models have applications in computational biology natural language processing time series forecasting etc. we focus on the case where we the observations occur at discrete time steps although time may also refer to locations within a sequence. markov models recall from section that the basic idea behind a markov chain is to assume that xt captures all the relevant information for predicting the future we assume it is a sufficient statistic. if we assume discrete time steps we can write the joint distribution as follows pxtxt this is called a markov chain or markov model. if we assume the transition function pxtxt is independent of time then the chain is called homogeneous stationary or time-invariant. this is an example of parameter tying since the same parameter is shared by multiple variables. this assumption allows us to model an arbitrary number of variables using a fixed number of parameters such models are called stochastic processes. if we assume that the observed variables are discrete so xt k this is called a discrete-state or finite-state markov chain. we will make this assumption throughout the rest of this section. transition matrix when xt is discrete so xt k the conditional distribution pxtxt can be written as a k k matrix known as the transition matrix a where aij pxt jxt i is the probability of going from state i to state j. each row of the matrix sums to one j aij so this is called a stochastic matrix. chapter markov and hidden markov models figure left-to-right chain. state transition diagrams for some simple markov chains. left a chain. right a a stationary finite-state markov chain is equivalent to a stochastic automaton. it is common to visualize such automata by drawing a directed graph where nodes represent states and arrows represent legal transitions i.e. non-zero elements of a. this is known as a state transition diagram. the weights associated with the arcs are the probabilities. for example the following chain a a is illustrated in figure the following chain is illustrated in figure this is called a left-to-right transition matrix and is commonly used in speech recognition the aij element of the transition matrix specifies the probability of getting from i to j in one step. the n-step transition matrix an is defined as aijn pxtn jxt i which is the probability of getting from i to j in exactly n steps. obviously a. the chapman-kolmogorov equations state that aijm n aikmakjn in words the probability of getting from i to j in m n steps is just the probability of getting from i to k in m steps and then from k to j in n steps summed up over all k. we can write the above as a matrix multiplication am n aman hence an a an a a an an thus we can simulate multiple steps of a markov chain by powering up the transition matrix. markov models says it s not in the cards legendary reconnaissance by rollie democracies unsustainable could strike redlining visits to profit booking wait here at madison square garden county courthouse where he had been done in three already in any way in which a teacher table example output from an word model trained using backoff smoothing on the broadcast news corpus. the first words are specified by hand the model generates the word and then the results are fed back into the model. source m.txt application language modeling one important application of markov models is to make statistical language models which are probability distributions over sequences of words. we define the state space to be all the words in english some other language. the marginal probabilities pxt k are called unigram statistics. if we use a first-order markov model then pxt kxt j is called a bigram if we use a second-order markov model then pxt kxt j xt i is model. called a trigram model. and so on. in general these are called n-gram models. for example figure shows and counts for the letters z represents space estimated from darwin s on the origin of species. language models can be used for several things such as the following sentence completion a language model can predict the next word given the previous words in a sentence. this can be used to reduce the amount of typing required which is particularly important for disabled users e.g. david mackay s dasher or uses of mobile devices. data compression any density model can be used to define an encoding scheme by assigning short codewords to more probable strings. the more accurate the predictive model the fewer the number of bits it requires to store the data. text classification any density model can be used as a class-conditional density and hence turned into a classifier. note that using a class-conditional density only unigram statistics would be equivalent to a naive bayes classifier section automatic essay writing one can sample from to generate artificial text. this is in table we give an example of text one way of assessing the quality of the model. generated from a model trained on a corpus with million words. et al. describes a much better language model based on recurrent neural networks which generates much more semantically plausible text. httpwww.inference.phy.cam.ac.ukdasher chapter markov and hidden markov models unigrams bigrams a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z figure unigram and bigram counts from darwin s on the origin of species. the picture on the right is a hinton diagram of the joint distribution. the size of the white squares is proportional to the value of the entry in the corresponding vector matrix. based on figure generated by ngramplot. mle for markov language models we now discuss a simple way to estimate the transition matrix from training data. the probability of any particular sequence of length t is given by axt xt hence the log-likelihood of a set of sequences d xn where xi xiti is a sequence of length ti is given by log pd log pxi ti j where we define the following counts j n j njk n j log j njk log ajk j k ixit j k markov models hence we can write the mle as the normalized counts j ajk n j n j k njk these results can be extended in a straightforward way to higher order markov models. however the problem of zero-counts becomes very acute whenever the number of states k andor the order of the chain n is large. an n-gram models has ok n parameters. if we have k words in our vocabulary then a bi-gram model will have about billion free parameters corresponding to all possible word pairs. it is very unlikely we will see all of these in our training data. however we do not want to predict that a particular word string is totally impossible just because we happen not to have seen it in our training text that would be a severe form of a simple solution to this is to use add-one smoothing where we simply add one to all the empirical counts before normalizing. the bayesian justification for this is given in section however add-one smoothing assumes all n-grams are equally likely which is not very realistic. a more sophisticated bayesian approach is discussed in section an alternative to using smart priors is to gather lots and lots of data. for example google has fit n-gram models n based on one trillion words extracted from the web. their data which is over when uncompressed is publically an example of their data for a set of is shown below. serve as the incoming serve as the incubator serve as the independent serve as the index serve as the indication serve as the indicator serve as the indicators serve as the indispensable serve as the indispensible serve as the individual although such an approach based on brute force and ignorance can be successful it is rather unsatisfying since it is clear that this is not how humans learn e.g. and xu a more refined bayesian approach that needs much less data is described in section empirical bayes version of deleted interpolation a common heuristic used to fix the sparse data problem is called deleted interpolation and goodman this defines the transition matrix as a convex combination of the bigram a famous example of an improbable but syntactically valid english word string due to noam chomsky is colourless green ideas sleep furiously we would not want our model to predict that this string is impossible. even ungrammatical constructs should be allowed by our model with a certain probability since people frequently violate grammatical rules especially in spoken language. see for details. chapter markov and hidden markov models frequencies fjk njknj and the unigram frequencies fk nkn ajk fk the term is usually set by cross validation. there is also a closely related technique called backoff smoothing the idea is that if fjk is too small we back off to a more reliable estimate namely fk. we will now show that the deleted interpolation heuristic is an approximation to the predictions made by a simple hierarchical bayesian model. our presentation follows and peto first let us use an independent dirichlet prior on each row of the transition matrix aj dir dir dir where aj is row j of the transition matrix m is the prior mean is the prior strength. we will use the same prior for each row see figure k mk and the posterior is given by aj dir nj where nj njk is the vector that records the number of times we have transitioned out of state j to each of the other states. from equation the posterior predictive density is kxt jd ajk where ajk e and jfjk nj njk mk fjknj mk nj j nj this is very similar to equation but not identical. the main difference is that the bayesian model uses a context-dependent weight j to combine mk with the empirical frequency fjk rather than a fixed weight this is like adaptive deleted interpolation. furthermore rather than backing off to the empirical marginal frequencies fk we back off to the model parameter mk. the only remaining question is what values should we use for and m? let s use empirical bayes. since we assume each row of the transition matrix is a priori independent given the marginal likelihood for our markov model is found by applying equation to each row pd bnj b j where nj njk are the counts for leaving state j and b is the generalized beta function. we can fit this using the methods discussed in however we can also use the following approximation and peto mk njk this says that the prior probability of word k is given by the number of different contexts in which it occurs rather than the number of times it occurs. to justify the reasonableness of this result mackay and peto and peto give the following example. markov models figure a markov chain in which we put a different dirichlet prior on every row of the transition matrix a but the hyperparameters of the dirichlet are shared. imagine you see that the language you see has you see a frequently occuring couplet you see you see in which the second word of the couplet see follows the first word you with very high probability you see. then the marginal statistics you see are going to become hugely dominated you see by the words you and see with equal frequency you see. if we use the standard smoothing formula equation then pyounovel and pseenovel for some novel context word not seen before would turn out to be the same since the marginal frequencies of you and see are the same times each. however this seems unreasonable. you appears in many contexts so pyounovel should be high but see only follows you so pseenovel should be low. if we use the bayesian formula equation we will get this effect for free since we back off to mk not fk and mk will be large for you and small for see by equation unfortunately although elegant this bayesian model does not beat the state-of-the-art language model known as interpolated kneser-ney and ney chen and goodman however in it was shown how one can build a non-parametric bayesian model which outperforms interpolated kneser-ney by using variable-length contexts. in et al. this method was extended to create the sequence memoizer which is currently the best-performing language handling out-of-vocabulary words while the above smoothing methods handle the case where the counts are small or even zero none of them deal with the case where the test set may contain a completely novel word. in particular they all assume that the words in the vocabulary the state space of xt is fixed and known it is the set of unique words in the training data or in some dictionary. interestingly these non-parametric methods are based on posterior inference using mcmc andor particle filtering rather than optimization methods such as eb. despite this they are quite efficient. chapter markov and hidden markov models figure some markov chains. a aperiodic chain. a reducible chain. even if all ajk s are non-zero none of these models will predict a novel word outside of this set and hence will assign zero probability to a test sentence with an unfamiliar word. words are bound to occur because the set of words is an open class. for example the set of proper nouns of people and places is unbounded. a standard heuristic to solve this problem is to replace all novel words with the special symbol unk which stands for unknown a certain amount of probability mass is held aside for this event. a more principled solution would be to use a dirichlet process which can generate a countably infinite state space as the amount of data increases section if all novel words are accepted as genuine words then the system has no predictive power since any misspelling will be considered a new word. so the novel word has to be seen frequently enough to warrant being added to the vocabulary. see e.g. and singer griffiths and tenenbaum for details. stationary distribution of a markov chain we have been focussing on markov models as a way of defining joint probability distributions over sequences. however we can also interpret them as stochastic dynamical systems where we hop from one state to another at each time step. in this case we are often interested in the long term distribution over states which is known as the stationary distribution of the chain. in this section we discuss some of the relevant theory. later we will consider two important applications google s pagerank algorithm for ranking web pages and the mcmc algorithm for generating samples from hard-to-normalize probability distributions what is a stationary distribution? let aij pxt jxt i be the one-step transition matrix and let tj xt j be the probability of being in state j at time t. it is conventional in this context to assume that is a row vector. if we have an initial distribution over states of then at time we have i or in matrix notation markov models we can imagine iterating these equations. if we ever reach a stage where a then we say we have reached the stationary distribution called the invariant distribution or equilibrium distribution. once we enter the stationary distribution we will never leave. for example consider the chain in figure to find its stationary distribution we write so or in general we have i aij jaji in other words the probability of being in state i times the net flow out of state i must equal the probability of being in each other state j times the net flow from that state into i. these are called the global balance equations. we can then solve these equations subject to the constraint that j j computing the stationary distribution to find the stationary distribution we can just solve the eigenvector equation at v v and then to set vt where v is an eigenvector with eigenvalue can be sure such an eigenvector exists since a is a row-stochastic matrix so also recall that the eigenvalues of a and at are the same. of course since eigenvectors are unique only up to constants of proportionality we must normalize v at the end to ensure it sums to one. note however that the eigenvectors are only guaranteed to be real-valued if the matrix is positive aij hence aij due to the sum-to-one constraint. a more general approach which can handle chains where some transition probabilities are or as figure is as follows we have k constraints from a and constraint from since we only have k unknowns this is overconstrained. so let us replace any column the last of i a with to get a new matrix call it m. next we define r where the in the last position corresponds to the column of all in m. we then solve m r. for example for a state chain we have to solve this linear system chapter markov and hidden markov models for the chain in figure we find we can easily verify this is correct since a. see mcstatdist for some matlab code. unfortunately not all chains have a stationary distribution. as we explain below. when does a stationary distribution exist? consider the chain in figure if we start in state we will stay there forever since is anabsorbing state. thus is one possible stationary distribution. however if we start in or we will oscillate between those two states for ever. so is another possible stationary distribution. if we start in state we could end up in either of the above stationary distributions. we see from this example that a necessary condition to have a unique stationary distribution is that the state transition diagram be a singly connected component i.e. we can get from any state to any other state. such chains are called irreducible. now consider the chain in figure this is irreducible provided suppose it is clear by symmetry that this chain will spend of its time in each state. thus but now suppose in this case the chain will oscillate between the two states but the long-term distribution on states depends on where you start from. if we start in state then on every odd time step we will be in state but if we start in state then on every odd time step we will be in state this example motivates the following definition. let us say that a chain has a limiting if this holds then distribution if j limn an ij exists and is independent of i for all j. the long-run distribution over states will be independent of the starting state p j p iaijt j as t i let us now characterize when a limiting distribution exists. define the period of state i to be di aiit where gcd stands for greatest common divisor i.e. the largest integer that divides all the members of the set. for example in figure we have and we say a state i is aperiodic if di sufficient condition to ensure this is if state i has a self-loop but this is not a necessary condition. we say a chain is aperiodic if all its states are aperiodic. one can show the following important result theorem every irreducible connected aperiodic finite state markov chain has a limiting distribution which is equal to its unique stationary distribution. a special case of this result says that every regular finite state chain has a unique stationary distribution where a regular chain is one whose transition matrix satisfies an ij for some integer n and all i j i.e. it is possible to get from any state to any other state in n steps. consequently after n steps the chain could be in any state no matter where it started. one can show that sufficient conditions to ensure regularity are that the chain be irreducible connected and that every state have a self-transition. to handle the case of markov chains whose state-space is not finite the countable set of all integers or all the uncountable set of all reals we need to generalize some of the earlier markov models definitions. since the details are rather technical we just briefly state the main results without proof. see e.g. and stirzaker for details. for a stationary distribution to exist we require irreducibility connected and aperiodicity as before. but we also require that each state is recurrent. chain in which all states are recurrent is called a recurrent chain. recurrent means that you will return to that state with probability as a simple example of a non-recurrent state a transient state consider figure states is transient because one immediately leaves it and either spins around state forever or oscillates between states and forever. there is no way to return to state it is clear that any finite-state irreducible chain is recurrent since you can always get back to where you started from. but now consider an example with an infinite state space. suppose we perform a random walk on the integers x let p be the probability of moving right and aii p be the probability of moving left. suppose we start at if p we will shoot off to we are not guaranteed to return. similarly if p we will shoot off to so in both cases the chain is not recurrent even though it is irreducible. it should be intuitively obvious that we require all states to be recurrent for a stationary distribution to exist. however this is not sufficient. to see this consider the random walk on the integers again and suppose p in this case we can return to the origin an infinite number of times so the chain is recurrent. however it takes infinitely long to do so. this prohibits it from having a stationary distribution. the intuitive reason is that the distribution keeps spreading out over a larger and larger set of the integers and never converges to a stationary distribution. more formally we define a state to be non-null recurrent if the expected time to return to this state is finite. a chain in which all states are non-null is called a non-null chain. for brevity we we say that a state is ergodic if it is aperiodic recurrent and non-null and we say a chain is ergodic if all its states are ergodic. we can now state our main theorem theorem every irreducible connected ergodic markov chain has a limiting distribution which is equal to its unique stationary distribution. this generalizes theorem since for irreducible finite-state chains all states are recurrent and non-null. detailed balance establishing ergodicity can be difficult. we now give an alternative condition that is easier to verify. we say that a markov chain a is time reversible if there exists a distribution such that iaij jaji these are called the detailed balance equations. this says that the flow from i to j must equal the flow from j to i weighted by the appropriate source probabilities. we have the following important result. theorem balance wrt distribution then is a stationary distribution of the chain. if a markov chain with transition matrix a is regular and satisfies detailed chapter markov and hidden markov models figure a very small world wide web. figure generated by pagerankdemo written by tim davis. proof. to see this note that iaij jaji j i i and hence a i aji j note that this condition is sufficient but not necessary figure for an example of a chain with a stationary distribution which does not satisfy detailed balance. in section we will discuss markov chain monte carlo or mcmc methods. these take as input a desired distribution and construct a transition matrix in general a transition kernel a which satisfies detailed balance wrt thus by sampling states from such a chain we will eventually enter the stationary distribution and will visit states with probabilities given by application google s pagerank algorithm for web page ranking the results in section form the theoretical underpinnings to google s pagerank algorithm which is used for information retrieval on the world-wide web. we sketch the basic idea below see and leise for a more detailed explanation. we will treat the web as a giant directed graph where nodes represent web pages and edges represent we then perform a process called web crawling. we start at a few designated root nodes such as dmoz.org the home of the open directory project and then follows the links storing all the pages that we encounter until we run out of time. next all of the words in each web page are entered into a data structure called an inverted index. that is for each word we store a list of the documents where this word occurs. practice we store a list of hash codes representing the urls. at test time when a user enters in google said it had indexed trillion unique urls. if we assume there are about urls per page average this means there were about billion unique web pages. estimates for are about billion unique web pages. source markov models a query we can just look up all the documents containing each word and intersect these lists queries are defined by a conjunction of search terms. we can get a refined search by storing the location of each word in each document. we can then test if the words in a document occur in the same order as in the query. let us give an example from httpen.wikipedia.orgwikiinverted_index. we have documents it is what it is what is it and it is a banana then we can create the following inverted index where each pair represents a document and word location for example we see that the word what occurs at location from in document and location in document suppose we search for what is it if we ignore word order we retrieve the following documents if we require that the word order matches only document would be returned. more generally we can allow out-of-order matches but can give bonus points to documents whose word order matches the query s word order or to other features such as if the words occur in the title of a document. we can then return the matching documents in decreasing order of their score relevance. this is called document ranking. so far we have described the standard process of information retrieval. but the link structure of the web provides an additional source of information. the basic idea is that some web pages are more authoritative than others so these should be ranked higher they match the query. a web page is an authority if it is linked to by many other pages. but to protect against the effect of so-called link farms which are dummy pages which just link to a given site to boost its apparent relevance we will weight each incoming link by the source s authority. thus we get the following recursive definition for the authoritativeness of page j also called its pagerank j aij i i where aij is the probability of following a link from i to j. we recognize equation as the stationary distribution of a markov chain. in the simplest setting we define ai. as a uniform distribution over all states that i is connected to. however to ensure the distribution is unique we need to make the chain into a regular chain. this can be done by allowing each state i to jump to any other state itself with some small probability. this effectively makes the transition matrix aperiodic and fully connected the adjacency matrix gij of the web itself is highly sparse. we discuss efficient methods for computing the leading eigenvector of this giant matrix below. but first let us give an example of the pagerank algorithm. consider the small web in figure chapter markov and hidden markov models nz figure web graph of sites rooted at www.harvard.edu. corresponding page rank vector. figure generated by pagerankdemopmtk based on code by cleve moler we find that the stationary distribution is so a random surfer will visit site about of the time. we see that node has a higher pagerank than nodes or even though they all have the same number of in-links. this is because being linked to from an influential nodehelps increase your pagerank score more than being linked to by a less influential node. as a slightly larger example figure shows a web graph derived from the root of harvard.edu. figure shows the corresponding pagerank vector. efficiently computing the pagerank vector let gij iff there is a link from j to i. now imagine performing a random walk on this graph where at every time step with probability p you follow one of the outlinks uniformly at random and with probability p you jump to a random node again chosen uniformly at random. if there are no outlinks you just jump to a random page. random jumps including self-transitions ensure the chain is irreducible connected and regular. hence we can solve for its unique stationary distribution using eigenvector methods. this defines the following transition matrix if cj if cj where n is the number of nodes pn is the probability of jumping from one page i gij represents the out-degree of page j. to another without following a link and cj n and p then here m is a stochastic matrix in which columns sum to one. note that m at in our earlier notation. mij pgijcj we can represent the transition matrix compactly as follows. define the diagonal matrix d with entries djj if cj if cj hidden markov models define the vector z with components zj if cj if cj then we can rewrite equation as follows m pgd the matrix m is not sparse but it is a rank one modification of a sparse matrix. most of the elements of m are equal to the small constant obviously these do not need to be stored explicitly. our goal is to solve v mv where v t one efficient method to find the leading eigenvector of a large matrix is known as the power method. this simply consists of repeated matrix-vector multiplication followed by normalization v mv pgdv v it is possible to implement the power method without using any matrix multiplications by simply sampling from the transition matrix and counting how often you visit each state. this is essentially a monte carlo approximation to the sum implied by v mv. applying this to the data in figure yields the stationary distribution in figure this took iterations to also the function pagerankdemo by tim converge starting from a uniform distribution. davis for an animation of the algorithm in action applied to the small web example. to handle changing web structure we can re-run this algorithm every day or every week starting v off at the old distribution and meyer for details on how to perform this monte carlo power method in a parallel distributed computing environment see e.g. and ullman web spam pagerank is not foolproof. for example consider the strategy adopted by jc penney a department store in the usa. during the christmas season of it planted many links to its home page on of irrelevant web pages thus increasing its ranking on google s search engine even though each of these source pages has low pagerank there were so many of them that their effect added up. businesses call this search engine optimization google calls it web spam. when google was notified of this scam the new york times it manually downweighted jc penney since such behavior violates google s code of conduct. the result was that jc penney dropped from rank to rank essentially making it disappear from view. automatically detecting such scams relies on various techniques which are beyond the scope of this chapter. hidden markov models as we mentioned in section a hidden markov model or hmm consists of a discrete-time discrete-state markov chain with hidden states zt k plus an observation model chapter markov and hidden markov models figure hidden state sequence. based on figure of figure generated by hmmlillypaddemo. some data sampled from a state hmm. each state emits from a gaussian. the pxtzt. the corresponding joint distribution has the form pztzt pxtzt the observations in an hmm can be discrete or continuous. if they are discrete it is common for the observation model to be an observation matrix pxt lzt k bk l if the observations are continuous it is common for the observation model to be a conditional gaussian pxtzt k n k k figure shows an example where we have states each of which emits a different gaussian. the resulting model is similar to a gaussian mixture model except the cluster membership hmms are sometimes called markov switching models has markovian dynamics. we see that we tend to get multiple observations in the same location and then a sudden jump to a new cluster. applications of hmms hmms can be used as black-box density models on sequences. they have the advantage over markov models in that they can represent long-range dependencies between observations mediated via the latent variables. in particular note that they do not assume the markov property holds for the observations themselves. such black-box models are useful for timeseries prediction they can also be used to define class-conditional densities inside a generative classifier. however it is more common to imbue the hidden states with some desired meaning and to then try to estimate the hidden states from the observations i.e. to compute if we are hidden markov models bat rat cat gnat goat d i m m i begin x x x a g c a a g c a g a a a a a c c a g d i m m d i m end figure some dna sequences. state transition diagram for a profile hmm. source figure of et al. used with kind permission of richard durbin. in an online scenario or if we are in an offline scenario section for further discussion of the differences between these two approaches. below we give some examples of applications which use hmms in this way automatic speech recognition. here xt represents features extracted from the speech signal and zt represents the word that is being spoken. the transition model pztzt represents the language model and the observation model pxtzt represents the acoustic model. see e.g. jurafsky and martin for details. activity recognition. here xt represents features extracted from a video frame and zt is the class of activity the person is engaged in running walking sitting etc. see e.g. for details. part of speech tagging. here xt represents a word and zt represents its part of speech verb adjective etc. see section for more information on pos tagging and chapter markov and hidden markov models related tasks. gene finding. here xt represents the dna nucleotides and zt represents whether we are inside a gene-coding region or not. see e.g. et al. for details. protein sequence alignment. here xt represents an amino acid and zt represents whether this matches the latent consensus sequence at this location. this model is called a profile hmm and is illustrated in figure the hmm has states called match insert and delete. if zt is a match state then xt is equal to the t th value of the consensus. if zt is an insert state then xt is generated from a uniform distribution that is unrelated to the consensus sequence. if zt is a delete state then xt in this way we can generate noisy copies of the consensus sequence of different lengths. in figure the consensus is agc and we see various versions of this below. a path through the state transition diagram shown in figure specifies how to align a sequence to the consensus e.g. for the gnat the most probable path is d d i i i m this means we delete the a and g parts of the consensus sequence we insert a s and then we match the final c. we can estimate the model parameters by counting the number of such transitions and the number of emissions from each kind of state as shown in figure see section for more information on training an hmm and et al. for details on profile hmms. note that for some of these tasks conditional random fields which are essentially discrimi native versions of hmms may be more suitable see chapter for details. inference in hmms we now discuss how to infer the hidden state sequence of an hmm assuming the parameters are known. exactly the same algorithms apply to other chain-structured graphical models such as chain crfs section in chapter we generalize these methods to arbitrary graphs. and in section we show how we can use the output of inference in the context of parameter estimation. types of inference problems for temporal models there are several different kinds of inferential tasks for an hmm ssm in general. to illustrate the differences we will consider an example called the occasionally dishonest casino in this model xt represents which dice face shows from et al. up and zt represents the identity of the dice that is being used. most of the time the casino uses a fair dice z but occasionally it switches to a loaded dice z for a short period. if z the observation distribution is a uniform multinoulli over the symbols if z the observation distribution is skewed towards face figure if we sample from this model we may observe data such as the following rolls die listing example output of casinodemo llllllllllllllffffffllllllllllllllffffffffffffffffffllllllll here rolls refers to the observed symbol and die refers to the hidden state is loaded and f is fair. thus we see that the model generates a sequence of symbols but the statistics of the inference in hmms figure an hmm for the occasionally dishonest casino. the blue arrows visualize the state transition diagram a. based on et al. filtered smoothed viterbi d e d a o l p d e d a o l p roll number roll number d e d a o l f r i a e t a t s p a m roll number figure inference in the dishonest casino. vertical gray bars denote the samples that we generated using a loaded die. map trajectory. figure generated by casinodemo. filtered estimate of probability of using a loaded dice. smoothed estimates. distribution changes abruptly every now and then. in a typical application we just see the rolls and want to infer which dice is being used. but there are different kinds of inference which we summarize below. filtering means to compute the belief state online or recursively as the data streams in. this is called filtering because it reduces the noise more than simply estimating the hidden state using just the current estimate pztxt. we will see below that we can perform filtering by simply applying bayes rule in a sequential fashion. see figure for an example. smoothing means to compute offline given all the evidence. see figure for an example. by conditioning on past and future data our uncertainty will be significantly reduced. to understand this intuitively consider a detective trying to figure out who committed a crime. as he moves through the crime scene his uncertainty is high until he finds the key clue then he has an aha moment his uncertainty is reduced and all the previously confusing observations are in hindsight easy to explain. chapter markov and hidden markov models figure the main kinds of inference for state-space models. the shaded region is the interval for which we have data. the arrow represents the time step at which we want to perform inference. t is the current time t is the sequence length is the lag and h is the prediction horizon. see text for details. fixed lag smoothing is an interesting compromise between online and offline estimation it involves computing pzt where is called the lag. this gives better performance than filtering but incurs a slight delay. by changing the size of the lag one can trade off accuracy vs delay. prediction instead of predicting the past given the future as in fixed lag smoothing we might want to predict the future given the past i.e. to compute where h is called the prediction horizon. for example suppose h then we have zt it is straightforward to perform this computation we just power up the transition matrix and apply it to the current belief state. the quantity is a prediction about future hidden states it can be converted into a prediction about future observations using zth this is the posterior predictive density and can be used for time-series forecasting for details. see figure for a sketch of the relationship between filtering smoothing and prediction. which is a most probin the context of hmms this is known as viterbi decoding map estimation this means computing arg able state sequence. inference in hmms section figure illustrates the difference between filtering smoothing and map decoding for the occasionally dishonest casino hmm. we see that the smoothed estimate is indeed smoother than the filtered estimate. if we threshold the estimates at and compare to the true sequence we find that the filtered method makes errors out of and the smoothed method makes the map path makes errors. it is not surprising that smoothing makes fewer errors than viterbi since the optimal way to minimize bit-error rate is to threshold the posterior marginals section nevertheless for some applications we may prefer the viterbi decoding as we discuss in section posterior samples if there is more than one plausible interpretation of the data it can be useful to sample from the posterior these sample paths contain much more information than the sequence of marginals computed by smoothing. probability of the evidence we can compute the probability of the evidence this can be used to by summing up over all hidden paths classify sequences if the hmm is used as a class conditional density for model-based clustering for anomaly detection etc. the forwards algorithm we now describe how to recursively compute the filtered marginals in an hmm. the algorithm has two steps. first comes the prediction step in which we compute the one-step-ahead predictive density this acts as the new prior for time t pzt pzt jzt ipzt i next comes the update step in which we absorb the observed data from time t using bayes rule tj pzt pzt jxt pxtzt zt where the normalization constant is given by zt pzt j j this process is known as the predict-update cycle. the distribution is called the belief state at time t and is a vector of k numbers often denoted by t. in matrixvector notation we can write the update in the following simple form t t t t where tj xtzt j is the local evidence at time t j zt jzt i is the transition matrix and u v is the hadamard product representing elementwise vector multiplication. see algorithm for the pseudo-code and hmmfilter for some matlab code. chapter markov and hidden markov models in addition to computing the hidden states we can use this algorithm to compute the log probability of the evidence log log log zt need to work in the log domain to avoid numerical underflow. algorithm forwards algorithm input transition matrices j pzt jzt i local evidence vectors tj pxtzt j initial state distribution j normalize for t do return and log t zt normalize t t t t log zt subroutine z normalizeu z j uj vj ujz the forwards-backwards algorithm in section we explained how to compute the filtered marginals pzt using online inference. we now discuss how to compute the smoothed marginals pzt using offline inference. basic idea the key decomposition relies on the fact that we can break the chain into two parts the past and the future by conditioning on zt pzt pzt j pzt let tj pzt be the filtered belief state as before. also define tj j likelihood of future evidence given that the hidden state at time t is j. as the conditional that this is not a probability distribution over states since it does not need to satisfy j tj finally define tj pzt as the desired smoothed posterior marginal. from equation we have tj tj tj inference in hmms we have already described how to recursively compute the s in a left-to-right fashion in section we now describe how to recursively compute the s in a right-to-left fashion. if we have already computed t we can compute t as follows t xttzt i pzt j xt i j jpxtzt j zt ixtpzt j xtzt i zt ipzt jzt i j j j tj tj j j we can write the resulting equation in matrix-vector form as t t t the base case is t pxt i i which is the probability of a non-event. having computed the forwards and backwards messages we can combine them to compute tj tj tj. the overall algorithm is known as the forwards-backwards algorithm. the pseudo code is very similar to the forwards case see hmmfwdback for an implementation. we can think of this algorithm as passing messages from left to right and then from right to left and then combining them at each node. we will generalize this intuition in section when we discuss belief propagation. two-slice smoothed marginals t when we estimate the parameters of the transition matrix using em section we will need to compute the expected number of transitions from state i to state j pzt i the term pzt i is called a two-slice marginal and can be computed as follows e i t nij j pzt i ti j chapter markov and hidden markov models in matrix-vector form we have t for another interpretation of these equations see section time and space complexity it is clear that a straightforward implementation of fb takes ok time since we must perform a k k matrix multiplication at each step. for some applications such as speech recognition k is very large so the ok term becomes prohibitive. fortunately if the in a left-to-right transition matrix is sparse we can reduce this substantially. for example transition matrix the algorithm takes ot k time. in some cases we can exploit special properties of the state space even if the transition matrix is not sparse. in particular suppose the states represent a discretization of an underlying continuous state-space and the transition matrix has the form j exp zj where zi is the continuous vector represented by state i. then one can implement the forwardsbackwards algorithm in ot k log k time. this is very useful for models with large state spaces. see section for details. in some cases the bottleneck is memory not time. the expected sufficient statistics needed t t j this takes constant space of t however to compute by em are them we need okt working space since we must store t for t t until we do the backwards pass. it is possible to devise a simple divide-and-conquer algorithm that reduces the space complexity from okt to ok log t at the cost of increasing the running time from to ok log t see et al. zweig and padmanabhan for details. the viterbi algorithm the viterbi algorithm can be used to compute the most probable sequence of states in a chain-structured graphical model i.e. it can compute z arg max this is equivalent to computing a shortest path through the trellis diagram in figure where the nodes are possible states at each time step and the node and edge weights are log probabilities. that is the weight of a path zt is given by log log zt log tzt map vs mpe before discussing how the algorithm works let us make one important remark the most probable sequence of states is not necessarily the same as the sequence of most probable states. the former is given by equation and is what viterbi computes whereas the latter is given by the maximizer of the posterior marginals or mpm z max arg max zt inference in hmms figure the trellis of states vs time for a markov chain. based on as a simple example of the difference consider a chain with two time steps defining the following joint the joint map estimate is whereas the sequence of marginal mpms is the advantage of the joint map estimate is that is is always globally consistent. for example suppose we are performing speech recognition and someones says recognize speech this could be mis-heard as wreck a nice beach locally it may appear that beach is the most probable interpretation of that particular window of sound but when we add the requirement that the data be explained by a single linguistically plausible path this interpretation becomes less likely. on the other hand the mpm estimates can be more robust et al. to see why note that in viterbi when we estimate zt we max out the other variables z t arg max zt zt max whereas we when we use forwards-backwards we sum out the other variables zt this makes the mpm in equation more robust since we estimate each node averaging over its neighbors rather than conditioning on a specific value of its in general we may want to mix max and sum. for example consider a joint distribution where we observe chapter markov and hidden markov models details of the algorithm it is tempting to think that we can implement viterbi by just replacing the sum-operator in forwards-backwards with a max-operator. the former is called the sum-product and the latter the max-product algorithm. if there is a unique mode running max-product and then computing using equation will give the same result as using equation and freeman but in general it can lead to incorrect results if there are multiple equally probably joint assignments. the reasons is that each node breaks ties independently and hence may do so in a manner that is inconsistent with its neighbors. the viterbi algorithm is therefore not quite as simple as replacing sum with max. in particular the forwards pass does use maxproduct but the backwards pass uses a traceback procedure to recover the most probable path through the trellis of states. essentially once zt picks its most probable state the previous nodes condition on this event and therefore they will break ties consistently. in more detail define tj max zt this is the probability of ending up in state j at time t given that we take the most probable path. the key insight is that the most probable path to state j at time t must consist of the most probable path to some other state i at time t followed by a transition from i to j. hence tj max i t j tj we also keep track of the most likely previous state for each possible state that we end up in atj argmax i t j tj that is atj tells us the most likely previous state on the most probable path to zt j. we initialize by setting j and we terminate by computing the most probable final state z t z t arg max t i we can then compute the most probable sequence of states using traceback z t as usual we have to worry about numerical underflow. we are free to normalize the t terms at each step this will not affect the maximum. however unlike the forwards-backwards case xn let n be the remaining nuisance variables. we define the map estimate as x v and we want to query q q pxq xnxv where we max over xq and sum over xn. by contrast we define the mpe or arg maxxq n arg maxxq pxq xnxv where we max over both xq and xn. this most probable explanation as terminology is due to although it is not widely used outside the bayes net literatire. obviously mapmpe if n however if n then summing out the nuisance variables can give different results than maxing them out. summing out nuisance variables is more sensible but computationally harder because of the need to combine max and sum operations and parr q x inference in hmms figure illustration of viterbi decoding in a simple hmm for speech recognition. a hmm for a single phone. we are visualizing the state transition diagram. we assume the observations have been vector quantized into possible symbols each state has a different distribution over these symbols. based on figure of and norvig illustration of the viterbi algorithm applied to this model with data sequence the columns represent time and the rows represent states. an arrow from state i at t to state j at t is annotated with two numbers the first is the probability of the i j transition and the second is the probability of generating observation xt from state j. the bold lines circles represent the most probable sequence of states. based on figure of and norvig we can also easily work in the log domain. the key difference is that log max max log whereas log log. hence we can use log tj max max i log zt log t log j log tj in the case of gaussian observation models this can result in a significant factor speedup since computing log pxtzt can be much faster than computing pxtzt for a highdimensional gaussian. this is one reason why the viterbi algorithm is widely used in the e step of em when training large speech recognition systems based on hmms. example figure gives a worked example of the viterbi algorithm based on et al. suppose we observe the discrete sequence of observations representing codebook entries in a vector-quantized version of a speech signal. the model starts in state the probability of generating in is so we have and for all other states. next we can self-transition to with probability or transition to with proabability if we end up in the probability of generating is if we end up in chapter markov and hidden markov models the probability of generating is hence we have thus state is more probable at t see the second column of figure in time step we see that there are two paths into from and from the bold arrow indicates that the latter is more probable. hence this is the only one we have to remember. the algorithm continues in this way until we have reached the end of the sequence. one we have reached the end we can follow the black arrows back to recover the map path is time and space complexity is clearly ok in general and the space complexity the time complexity of viterbi is okt both the same as forwards-backwards. if the transition matrix has the form j exp where zi is the continuous vector represented by state i we can implement viterbi in ot k time instead of ot k log k needed by forwards-backwards. see section for details. n-best list the viterbi algorithm returns one of the most probable paths. it can be extended to return the top n paths and chow nilsson and goldberger this is called the n-best list. once can then use a discriminative method to rerank the paths based on global features derived from the fully observed state sequence well as the visible features. this technique is widely used in speech recognition. for example consider the sentence recognize speech it is possible that the most probable interpretation by the system of this acoustic signal is wreck a nice speech or maybe wreck a nice beach maybe the correct interpretation is much lower down on the list. however by using a re-ranking system we may be able to improve the score of the correct interpretation based on a more global context. one problem with the n list is that often the top n paths are very similar to each other rather than representing qualitatively different interpretations of the data. instead we might want to generate a more diverse set of paths to more accurately represent posterior uncertainty. one way to do this is to sample paths from the posterior as we discuss below. for some other ways to generate diverse map estimates see e.g. et al. kulesza and taskar forwards filtering backwards sampling it is often useful to sample paths from the posterior zs we can do this is as follow run forwards backwards to compute the two-slice smoothed posteriors pzt next compute the conditionals pztzt by normalizing sample from the initial pair of states z t note that the above solution requires a forwards-backwards pass and then an additional forwards sampling pass. an alternative is to do the forwards pass and then perform sampling finally recursively sample z t pztz learning for hmms in the backwards pass. the key insight into how to do this is that we can write the joint from right to left using tt we can then sample zt given future sampled states using t pztzs zs the sampling distribution is given by pzt j j ti the base case is t pzt t zs this algorithm forms the basis of blocked-gibbs sampling methods for parameter inference as we will see below. learning for hmms we now discuss how to estimate the parameters a b where i is the initial state distribution ai j zt jzt i is the transition matrix and b are the parameters of the class-conditional densities pxtzt j. we first consider the case where is observed in the training set and then the harder case where is hidden. training with fully observed data if we observe the hidden state sequences we can compute the mles for a and exactly as in section if we use a conjugate prior we can also easily compute the posterior. the details on how to estimate b depend on the form of the observation model. the situation is identical to fitting a generative classifier. for example if each state has a multinoulli distribution associated with it with parameters bjl pxt lzt j where l l represents the observed symbol the mle is given by izit j xit l bjl n x jl nj n x jl chapter markov and hidden markov models this result is quite intuitive we simply add up the number of times we are in state j and we see a symbol l and divide by the number of times we are in state j. similarly if each state has a gaussian distribution associated with it we have sec tion the following mles k xk nk k k nk k t k nk where the sufficient statistics are given by xk k izit kxit izit kxitxt it analogous results can be derived for other kinds of distributions. one can also easily extend all of these results to compute map estimates or even full posteriors over the parameters. em for hmms baum-welch algorithm if the zt variables are not observed we are in a situation analogous to fitting a mixture model. the most common approach is to use the em algorithm to find the mle or map parameters although of course one could use other gradient-based methods e.g. and chauvin in this section we derive the em algorithm. when applied to hmms this is also known as the baum-welch algorithm et al. e step it is straightforward to show that the expected complete data log likelihood is given by q old e n k log k e log ajk pzt kxi old log pxit k e n k where the expected counts are given by kxi old e e pzit jxi old pzit j zit kxi old learning for hmms these expected sufficient statistics can be computed by running the forwards-backwards algorithm on each sequence. in particular this algorithm computes the following smoothed node and edge marginals itj pzt itj k pzt j zt m step based on section we have that the m step for a and is to just normalize the expected counts ajk e e e k n k n this result is quite intuitive we simply add up the expected number of transitions from j to k and divide by the expected number of times we transition from j to anything else. for a multinoulli observation model the expected sufficient statistics are e itjixit l txitl the m step has the form bjl e e itj this result is quite intuitive we simply add up the expected number of times we are in state j and we see a symbol l and divide by the expected number of times we are in state j. for a gaussian observation model the expected sufficient statistics are given by e itkxit e k itkxitxt it the m step becomes k e e e k e k t k e k this can should be regularized in the same way we regularize gmms. initialization as usual with em we must take care to ensure that we initialize the parameters carefully to minimize the chance of getting stuck in poor local optima. there are several ways to do this such as chapter markov and hidden markov models use some fully labeled data to initialize the parameters. initially ignore the markov dependencies and estimate the observation parameters using the standard mixture model estimation methods such as k-means or em. randomly initialize the parameters use multiple restarts and pick the best solution. techniques such as deterministic annealing and nakano rao and rose can help mitigate the effect of local minima. also just as k-means is often used to initialize em for gmms so it is common to initialize em for hmms using viterbi training which means approximating the posterior over paths with the single most probable path. is not necessarily a good idea since initially the parameters are often poorly estimated so the viterbi path will be fairly arbitrary. a safer option is to start training using forwards-backwards and to switch to viterbi near convergence. bayesian methods for fitting hmms em returns a map estimate of the parameters. in this section we briefly discuss some methods for bayesian parameter estimation in hmms. methods rely on material that we will cover later in the book. one approach is to use variational bayes em which we discuss in general terms in section the details for the hmm case can be found in beal but the basic idea is this the e step uses forwards-backwards but where speaking we plug in the posterior mean parameters instead of the map estimates. the m step updates the parameters of the conjugate posteriors instead of updating the parameters themselves. an alternative to vbem is to use mcmc. a particularly appealing algorithm is block gibbs sampling which we discuss in general terms in section the details for the hmm case can be found in but the basic idea is this we sample given the data and parameters using forwards-filtering backwards-sampling and we then sample the parameters from their posteriors conditional on the sampled latent paths. this is simple to implement but one does need to take care of unidentifiability switching just as with mixture models section discriminative training sometimes hmms are used as the class conditional density inside a generative classifier. in this case pxy c can be computed using the forwards algorithm. we can easily maximize the pxi yi by using em some other method to fit the hmm for each joint likelihood class-conditional density separately. however we might like to find the parameters that maximize the conditional likelihood pyixi i pyi c pyi c this is more expensive than maximizing the joint likelihood since the denominator couples all c class-conditional hmms together. furthermore em can no longer be used and one must resort generalizations of hmms to generic gradient based methods. nevertheless discriminative training can result in improved accuracies. the standard practice in speech recognition is to initially train the generative models separately using em and then to fine tune them discriminatively model selection in hmms the two main model selection issues are how many states and what topology to use for the state transition diagram. we discuss both of these issues below. choosing the number of hidden states choosing the number of hidden states k in an hmm is analogous to the problem of choosing the number of mixture components. here are some possible solutions use grid-search over a range of k s using as an objective function cross-validated likelihood the bic score or a variational lower bound to the log-marginal likelihood. use reversible jump mcmc. see for details. note that this is very slow and is not widely used. use variational bayes to extinguish unwanted components by analogy to the gmm case discussed in section see beal for details. use an infinite hmm which is based on the hierarchical dirichlet process. see e.g. et al. teh et al. for details. structure learning the term structure learning in the context of hmms refers to learning a sparse transition matrix. that is we want to learn the structure of the state transition diagram not the structure of the graphical model is fixed. a large number of heuristic methods have been proposed. most alternate between parameter estimation and some kind of heuristic split merge method e.g. and omohundro alternatively one can pose the problem as map estimation using a minimum entropy prior of the form pai exp h this prior prefers states whose outgoing distribution is nearly deterministic and hence has low entropy the corresponding m step cannot be solved in closed form but numerical methods can be used. the trouble with this is that we might prune out all incoming transitions to a state creating isolated islands in state-space. the infinite hmm presents an interesting alternative to these methods. see e.g. et al. teh et al. for details. generalizations of hmms many variants of the basic hmm model have been proposed. we briefly discuss some of them below. chapter markov and hidden markov models dt dt qt qt qt xt xt figure encoding a hidden semi-markov model as a dgm. dt are deterministic duration counters. variable duration hmms in a standard hmm the probability we remain in state i for exactly d steps is pti d aiiad ii expd log aii where aii is the self-loop probability. this is called the geometric distribution. however this kind of exponentially decaying function of d is sometimes unrealistic. to allow for more general durations one can use a semi-markov model. it is called semimarkov because to predict the next state it is not sufficient to condition on the past state we also need to know how long we ve been in that state. when the state space is not observed directly the result is called a hidden semi-markov model a variable duration hmm or an explicit duration hmm. hsmms are widely used in many gene finding programs since the length distribution of exons and introns is not geometric e.g. et al. and in some chip-seq data analysis programs e.g. et al. hsmms are useful not only because they can model the waiting time of each state more accurately but also because they can model the distribution of a whole batch of observations at once instead of assuming all observations are conditionally iid. that is they can use likelihood models of the form pxttlzt k dt l which generate l correlated observations if the duration in state k is for l time steps. this is useful for modeling data that is piecewise linear or shows other local trends et al. hsmm as augmented hmms one way to represent a hsmm is to use the graphical model shown in figure this figure we have assumed the observations are iid within each state but this is not required as mentioned above. the dt d node is a state duration counter where d is the maximum duration of any state. when we first enter state j we sample dt from the duration distribution for that state dt pj thereafer dt deterministically counts down generalizations of hmms p p p p p p p figure over sequence lengths for p and various n. figure generated by hmmselfloopdist. a markov chain with n repeated states and self loops. the resulting distribution until dt while dt the state zt is not allowed to change. when dt we make a stochastic transition to a new state. more precisely we define the cpds as follows ajk pdt d zt j pzt kzt j dt d d and d if d if otherwise if d and j k if d otherwise note that pjd could be represented as a table non-parametric approach or as some kind of parametric distribution such as a gamma distribution. if pjd is a geometric distribution this emulates a standard hmm. one can perform inference in this model by defining a mega-variable yt zt. however this is rather inefficient since dt is deterministic. it is possible to marginalize dt out and derive special purpose inference procedures. see yu and kobayashi for details. unfortunately all these methods take ot k time where t is the sequence length k is the number of states and d is the maximum duration of any state. approximations to semi-markov models a more efficient but less flexible way to model non-geometric waiting times is to replace each state with n new states each with the same emission probabilities as the original state. for example consider the model in figure obviously the smallest sequence this can generate is of length n any path of length d through the model has probability pd pn multiplying by the number of possible paths we find that the total probability of a path of length d is pd pd pn d n chapter markov and hidden markov models words on need phones aa n end n iy d end the dh n ax iy end subphones end end figure an example of an hhmm for an asr system which can recognize words. the top level represents bigram word probabilities. the middle level represents the phonetic spelling of each word. the bottom level represents the subphones of each phone. is traditional to represent a phone as a state hmm representing the beginning middle and end. based on figure of and martin this is equivalent to the negative binomial distribution. by adjusting n and the self-loop probabilities p of each state we can model a wide range of waiting times see figure let e be the number of expansions of each state needed to approximate pjd. forwardsbackwards on this model takes ot time where fin is the average number of predecessor states compared to ot kfin for the hsmm. for typical speech recognition applications fin d k t figures apply to problems such as gene finding which also often uses hsmms. since fin d efin the expanded state method is much faster than an hsmm. see for details. hierarchical hmms a hierarchical hmm et al. is an extension of the hmm that is designed to model domains with hierarchical structure. figure gives an example of an hhmm used in automatic speech recognition. the phone and subphone models can be called from different higher level contexts. we can always flatten an hhmm to a regular hmm but a factored representation is often easier to interpret and allows for more efficient inference and model fitting. hhmms have been used in many application domains e.g. speech recognition gene finding et al. plan recognition et al. monitoring transportation patterns et al. indoor robot localization et al. etc. hhmms are less expressive than stochastic context free grammars since they only allow hierarchies of bounded depth but they support more efficient inference. in particular inference in scfgs the inside outside algorithm and martin takes ot whereas inference in an hhmm takes ot time and paskin we can represent an hhmm as a directed graphical model as shown in figure t represents the state at time t and level a state transition at level is only allowed if the generalizations of hmms f f f f f f f f f figure an hhmm represented as a dgm. level has finished its exit state otherwise f nodes are hidden. we may optionally clamp f to ensure all models have finished by the end of the sequence. t is the state at time t level f t if the hmm at t shaded nodes are observed the remaining t where t is the length of the observation sequence source figure of and paskin chain at the level below has finished as determined by the f chain below finishes when it chooses to enter its end state. this mechanism ensures that higher level chains evolve more slowly than lower level chains i.e. lower levels are nested within higher levels. node. t a variable duration hmm can be thought of as a special case of an hhmm where the top level is a deterministic counter and the bottom level is a regular hmm which can only change states once the counter has timed out see and paskin for further details. input-output hmms it is straightforward to extend an hmm to handle inputs as shown in figure this defines a conditional density model for sequences of the form where ut is the input at time t this is sometimes called a control signal. outputs are continuous a typical parameterization would be pztxt zt i catztswiut pytxt zt j j if the inputs and thus the transition matrix is a logistic regression model whose parameters depend on the previous state. the observation model is a gaussian whose parameters depend on the current chapter markov and hidden markov models ut ut zt zt yt yt zt xt figure input-output hmm. first-order auto-regressive hmm. a second-order buried markov model. depending on the value of the hidden variables the effective graph structure between the components of the observed variables the non-zero elements of the regression matrix and the precision matrix can change although this is not shown. state. the whole model can be thought of as a hidden version of a maximum entropy markov model conditional on the inputs and the parameters one can apply the standard forwardsit is also straightforward to derive an em backwards algorithm to estimate the hidden states. algorithm to estimate the parameters and frasconi for details. auto-regressive and buried hmms the standard hmm assumes the observations are conditionally independent given the hidden state. in practice this is often not the case. however it is straightforward to have direct arcs from xt to xt as well as from zt to xt as in figure this is known as an auto-regressive hmm or aregime switching markov model. for continuous data the observation model becomes pxtxt zt j n j j this is a linear regression model where the parameters are chosen according to the current hidden state. we can also consider higher-order extensions where we condition on the last l observations pxtxt lt zt j n j j such models are widely used in econometrics similar models can be defined for discrete observations. the ar-hmm essentially combines two markov chains one on the hidden variables to capture long range dependencies and one on the observed variables to capture short range dependencies since the x nodes are observed the connections between them only generalizations of hmms figure a factorial hmm with chains. a coupled hmm with chains. change the computation of the local evidence inference can still be performed using the standard forwards-backwards algorithm. parameter estimation using em is also straightforward the e step is unchanged as is the m step for the transition matrix. if we assume scalar observations for notational simplicty the m step involves minimizing t lt log e t yt tj j t focussing on the w terms we see that this requires solving k weighted least squares problems yt t lt where tj zt is the smoothed posterior marginal. this is a weighted linear regression problem where the design matrix has a toeplitz form. this subproblem can be solved efficiently using the levinson-durbin method and koopman buried markov models generalize ar-hmms by allowing the dependency structure between the observable nodes to change based on the hidden state as in figure such a model is called a dynamic bayesian multi net since it is a mixture of different networks. in the linear-gaussian setting we can change the structure of the of xt xt arcs by using sparse regression matrices wj and we can change the structure of the connections within the components of xt by using sparse gaussian graphical models either directed or undirected. see for details. factorial hmm an hmm represents the hidden state using a single discrete random variable zt k. to represent bits of information would require k states. by contrast consider a distributed representation of the hidden state where each zct represents the c th chapter markov and hidden markov models bit of the t th hidden state. now we can represent bits using just binary variables as illustrated in figure this model is called a factorial hmm and jordan the hope is that this kind of model could capture different aspects of a signal e.g. one chain would represent speaking style another the words that are being spoken. unfortunately conditioned on xt all the hidden variables are correlated to explaining away the common observed child xt. this make exact state estimation intractable. however we can derive efficient approximate inference algorithms as we discuss in section coupled hmm and the influence model if we have multiple related data streams we can use a coupled hmm as illustrated in figure this is a series of hmms where the state transitions depend on the states of neighboring chains. that is we represent the joint conditional distribution as pzctzt zctzct zc c pztzt pzctzt this has been used for various tasks such as audio-visual speech recognition et al. and modeling freeway traffic flows and murphy the trouble with the above model is that it requires ock parameters to specify if there are c chains with k states per chain because each state depends on its own past plus the past of its two neighbors. there is a closely related model known as the influence model which uses fewer parameters. it models the joint conditional distribution as pzctzt for each c. that is we use a convex combination of pairwise transition where matrices. the parameter specifies how much influence chain c has on chain this model only takes oc ck parameters to specify. furthermore it allows each chain to be influenced by all the other chains not just its nearest neighbors. the corresponding graphical model is similar to figure except that each node has incoming edges from all the previous nodes. this has been used for various tasks such as modeling conversational interactions between people et al. unfortunately inference in both of these models takes ot time since all the chains become fully correlated even if the interaction graph is sparse. various approximate inference methods can be applied as we discuss later. dynamic bayesian networks a dynamic bayesian network is just a way to represent a stochastic process using a directed graphical note that the network is not dynamic structure and parameters are fixed the acronym dbn can stand for either dynamic bayesian network or deep belief network depending on the context. geoff hinton invented the term deep belief network has suggested the acronyms dybn and deebn to avoid this ambiguity. generalizations of hmms slice t slice evidence figure the batnet dbn. the transient nodes are only shown for the second slice to minimize clutter. the dotted lines can be ignored. used with kind permission of daphne koller. rather it is a network representation of a dynamical system. all of the hmm variants we have seen above could be considered to be dbns. however we prefer to reserve the term dbn for graph structures that are more irregular and problem-specific. an example is shown in figure which is a dbn designed to monitor the state of a simulated autonomous car known as the bayesian automated taxi or batmobile et al. defining dbns is straightforward you just need to specify the structure of the first time-slice the structure between two time-slices and the form of the cpds. learning is also easy. the main problem is that exact inference can be computationally expensive because all the hidden variables become correlated over time is known as entanglement see e.g. and for details. thus a sparse graph does not necessarily result in friedman sec. tractable exact inference. however later we will see algorithms that can exploit the graph structure for efficient approximate inference. exercises exercise derivation of q function for hmm derive equation exercise two filter approach to smoothing in hmms assuming that ti i for all i and t derive a recursive algorithm for updating rti pst hint it should be very similar to the standard forwards algorithm but using a timereversed transition matrix. then show how to compute the posterior marginals ti chapter markov and hidden markov models from the backwards filtered messages rti the forwards filtered messages ti and the stationary distribution ti. exercise em for for hmms with mixture of gaussian observations consider an hmm where the observation model has the form wjkn jk jk k pxtzt j draw the dgm. derive the e step. derive the m step. exercise em for for hmms with tied mixtures in many applications it is common that the observations are high-dimensional vectors in speech recognition xt is often a vector of cepstral coefficients and their derivatives so xt r so estimating a full covariance matrix for km values m is the number of mixture components per hidden state as in exercise requires a lot of data. an alternative is to use just m gaussians rather than m k gaussians and to let the state influence the mixing weights but not the means and covariances. this is called a semi-continuous hmm or tied-mixture hmm. draw the corresponding graphical model. derive the e step. derive the m step. state space models introduction a state space model or ssm is just like an hmm except the hidden states are continuous. the model can be written in the following generic form zt gut zt yt hzt ut t where zt is the hidden state ut is an optional input or control signal yt is the observation g is the transition model h is the observation model is the system noise at time t and t is the observation noise at time t. we assume that all parameters of the model are known if not they can be included into the hidden state as we discuss below. one of the primary goals in using ssms is to recursively estimate the belief state we will often drop the conditioning on u and for brevity. we will discuss algorithms for this later in this chapter. we will also discuss how to convert our beliefs about the hidden state into predictions about future observables by computing the posterior predictive in other an important special case of an ssm is where all the cpds are linear-gaussian. words we assume the transition model is a linear function zt atzt btut the observation model is a linear function yt ctzt dtut t the system noise is gaussian n qt the observation noise is gaussian t n rt this model is called a linear-gaussian ssm or alinear dynamical system if the parameters t bt ct dt qt rt are independent of time the model is called stationary. observed truth chapter state space models observed smoothed observed filtered illustration of kalman filtering and smoothing. figure observations cirles are generated filtered estimated is shown by an object moving to the right location denoted by black squares. by dotted red line. red cross is the posterior mean blue circles are confidence ellipses derived from the posterior covariance. for clarity we only plot the ellipses every other time step. same as but using offline kalman smoothing. figure generated by kalmantrackingdemo. the lg-ssm is important because it supports exact inference as we will see. in particular if the initial belief state is gaussian then all subsequent belief states will also be gaussian we will denote them by n tt tt. notation t denotes e and similarly for tt thus denotes the prior for before we have seen any data. for brevity we will denote the posterior belief states using tt t and tt t. we can compute these quantities efficiently using the celebrated kalman filter as we show in section but before discussing algorithms we discuss some important applications. applications of ssms ssms have many applications some of which we discuss in the sections below. we mostly focus on lg-ssms for simplicity although non-linear andor non-gaussian ssms are even more widely used. ssms for object tracking one of the earliest applications of kalman filtering was for tracking objects such as airplanes and missiles from noisy measurements such as radar. here we give a simplified example to illustrate the key ideas. consider an object moving in a plane. let and be the horizontal and vertical locations of the object and and be the corresponding velocity. we can represent this as a state vector zt r as follows zt t applications of ssms let us assume that the object is moving at constant velocity but is perturbed by random gaussian noise due to the wind. thus we can model the system dynamics as follows zt atzt where n q is the system noise and is the sampling period. this says that the new location zjt is the old location zjt plus times the old velocity zjt plus random noise for j also the new velocity zjt is the old velocity zjt plus random noise for j this is called a random accelerations model since the object moves according to newton s laws but is subject to random changes in velocity. now suppose that we can observe the location of the object but not its velocity. let yt r represent our observation which we assume is subject to gaussian noise. we can model this as follows yt ctzt t where t n r is the measurement noise. finally we need to specify our initial beliefs about the state of the object we will assume this is a gaussian n we can represent prior ignorance by making suitably broad e.g. i. we have now fully specified the model and can perform sequential bayesian updating to compute using an algorithm known as the kalman filter to be described in section figure gives an example. the object moves to the right and generates an observation at each time step of blips on a radar screen. we observe these blips and filter out the noise by using the kalman filter. at every step we have from which we can compute by marginalizing out the dimensions corresponding to the velocities. is easy to do since the posterior is gaussian. our best guess about the location of the object is the posterior mean denoted as a red cross in figure our uncertainty associated with this is represented as an ellipse which contains of the probability mass. we see that our uncertainty goes down over time as the effects of the initial uncertainty get washed out we also see that the estimated trajectory has filtered out some of the noise. to obtain the much smoother plot in figure we need to use the kalman smoother which computes this depends on future as well as past data as discussed in section robotic slam consider a robot moving around an unknown world. it needs to learn a map and keep track of its location within that map. this problem is known as simultaneous localization and chapter state space models xt yt figure illustration of graphical model underlying slam. li is the fixed location of landmark i xt is the location of the robot and yt is the observation. in this trace the robot sees landmarks and at time step then just landmark then just landmark etc. based on figure of and friedman robot pose figure illustration of the slam problem. a robot starts at the top left and moves clockwise in a circle back to where it started. we see how the posterior uncertainty about the robot s location increases and then decreases as it returns to a familar location closing the loop. if we performed smoothing this new information would propagate backwards in time to disambiguate the entire trajectory. we show the precision matrix representing sparse correlations between the landmarks and between the landmarks and the robot s position this sparse precision matrix can be visualized as a gaussian graphical model as shown. source figure of and friedman used with kind permission of daphne koller. applications of ssms mapping or slam for short and is widely used in mobile robotics as well as other applications such as indoor navigation using cellphones gps does not work inside buildings. let us assume we can represent the map as the locations of a fixed set of k landmarks denote them by lk is a vector in r for simplicity we will assume these are uniquely identifiable. let xt represent the unknown location of the robot at time t. we define the state space to be zt we assume the landmarks are static so their motion if yt measures the distance from xt to model is a constant and they have no system noise. the set of closest landmarks then the robot can update its estimate of the landmark locations based on what it sees. figure shows the corresponding graphical model for the case where k and where on the first step it sees landmarks and then just landmark then just landmark etc. if we assume the observation model pytzt l is linear-gaussian and we use a gaussian motion model for pxtxt ut we can use a kalman filter to maintain our belief state about the location of the robot and the location of the landmarks and cheeseman choset and nagatani over time the uncertainty in the robot s location will increase due to wheel slippage etc. but when the robot returns to a familiar location its uncertainty will decrease again. this is called closing the loop and is illustrated in figure where we see the uncertainty ellipses representing cov grow and then shrink. that in this section we assume that a human is joysticking the robot through the environment so is given as input i.e. we do not address the decision-theoretic issue of choosing where to explore. since the belief state is gaussian we can visualize the posterior covariance matrix t. actually it is more interesting to visualize the posterior precision matrix t since that is fairly sparse as shown in figure the reason for this is that zeros in the precision matrix correspond to absent edges in the corresponding undirected gaussian graphical model section initially all the landmarks are uncorrelated we have a diagonal prior on l so the ggm is a disconnected graph and t is diagonal. however as the robot moves about it will induce correlation between nearby landmarks. intuitively this is because the robot is estimating its position based on distance to the landmarks but the landmarks locations are being estimated based on the robot s position so they all become inter-dependent. this can be seen more clearly from the graphical model in figure it is clear that and are not d-separated by because there is a path between them via the unknown sequence of nodes. as a consequence of the precision matrix becoming denser exact inference takes ok time. is an example of the entanglement problem for inference in dbns. this prevents the method from being applied to large maps. t there are two main solutions to this problem. the first is to notice that the correlation pattern moves along with the location of the robot figure the remaining correlations become weaker over time. consequently we can dynamically prune out weak edges from the ggm using a technique called the thin junction tree filter trees are explained in section a second approach is to notice that conditional on knowing the robot s path the landmark locations are independent. that is this forms the basis of a method known as fastslam which combines kalman filtering and particle filtering as discussed in section et al. provides a more detailed account of slam and mobile robotics. chapter state space models t t yt yt xt xt s t h g e w i online linear regression batch batch time figure a dynamic generalization of linear regression. illustration of the recursive least squares algorithm applied to the model pyx we plot the marginal posterior of var after seeing all and vs number of data points. the data we converge to the offline ml squares solution represented by the horizontal lines. figure generated by linregonlinedemokalman. bars represent e online parameter learning using recursive least squares we can perform online bayesian inference for the parameters of various statistical models using ssms. in section we discuss logistic regression. in this section we focus on linear regression the basic idea is to let the hidden state represent the regression parameters and to let the observation model represent the current data vector. in more detail define the prior to be p we want to do online ml estimation we can just set i. let the hidden state be zt if we assume the regression parameters do not change we can set at i and qt so p t t n t t t t we do let the parameters change over time we get a so-called dynamic linear model west and harrison petris et al. let ct xt t and rt so the observation model has the form n rt n t t applying the kalman filter to this model provides a way to update our posterior beliefs about the parameters as the data streams in. this is known as the recursive least squares or rls algorithm. we can derive an explicit form for the updates as follows. in section we show that the kalman update for the posterior mean has the form t at t ktyt ctat t applications of ssms where kt is known as the kalman gain matrix. based on equation one can show that t r in this context we have kt txt hence the update for the kt tct parameters becomes t t t ttyt xt t t if we approximate tt with ti we recover the least mean squares or lms algorithm discussed in section in lms we need to specify how to adapt the update parameter t to ensure convergence to the mle. furthermore the algorithm may take multiple passes through the data. by contrast the rls algorithm automatically performs step-size adaptation and converges to the optimal posterior in one pass over the data. see figure for an example. ssm for time series forecasting ssms are very well suited for time-series forecasting as we explain below. we focus on the case of scalar dimensional time series for simplicity. our presentation is based on see also harvey west and harrison durbin and koopman petris et al. prado and west for good books on this topic. at first sight it might not be apparent why ssms are useful since the goal in forecasting is to predict future visible variables not to estimate hidden states of some system. indeed most classical methods for time series forecasting are just functions of the form f where hidden variables play no role section the idea in the state-space approach to time series is to create a generative model of the data in terms of latent processes which capture different aspects of the signal. we can then integrate out the hidden variables to compute the posterior predictive of the visibles. since the model is linear-gaussian we can just add these processes together to explain the observed data. this is called a structural time series model. below we explain some of the basic building blocks. local level model the simplest latent process is known as the local level model which has the form t n r yt at t t n q at at t where the hidden state is just zt at. this model asserts that the observed data yt r is equal to some unknown level term at r plus observation noise with variance r. in addition the level at evolves over time subject to system noise with variance q. see figure for some examples. at at yt chapter state space models local level local level model. sample output for black solid line q r figure system noisy observations. red dotted line q r system deterministic observation. blue dot-dash line q r system and observations. figure generated by ssmtimeseriessimple. at bt at bt yt local trend figure local trend. sample output for color code as in figure figure generated by ssmtimeseriessimple. local linear trend many time series exhibit linear trends upwards or downwards at least locally. we can model this by letting the level at change by an amount bt at each step as follows see figure we can write this in standard form by defining zt bt and when qb we have bt which is some constant defining the slope of the line. if in addition we have qa we havea t at unrolling this we have at and yt at t at at bt t bt bt t t n r t n qa a c t n qb q qa qb applications of ssms at bt t t t at bt t t t yt seasonal model figure color code as in figure figure generated by ssmtimeseriessimple. seasonal model. sample output for with a period of hence e this is thus a generalization of the classic constant linear trend model an example of which is shown in the black line of figure seasonality many time series fluctuate periodically as illustrated in figure this can be modeled by adding a latent process consisting of a series offset terms ct which sum to zero average over a complete cycle of s steps ct s t t n qc see figure for the graphical model for the case s only need seasonal variable because of the sum-to-zero constraint. writing this in standard lg-ssm form is left to exercise arma models the classical approach to time-series forecasting is based on arma models. arma stands for auto-regressive moving-average and refers to a model of the form ct s xt ixt i jwt j vt where vt wt n are independent gaussian noise terms. if q we have a pure ar model where xt xixt p for i t p. for example if p we have the model chapter state space models figure an model. an model represented as a bi-directed graph. an model. source figure of used with kind permission of myung choi. vt nodes are implicit in the gaussian cpd for xt. this is just a shown in figure if p we have a pure ma model where xt xi for i t q. first-order markov chain. for example if q we have the model shown in figure here the wt nodes are hidden common causes which induces dependencies between adjacent time steps. this models short-range correlation. if p q we get the model shown in figure which captures correlation at short and long time scales. it turns out that arma models can be represented as ssms as explained in harvey west and harrison durbin and koopman petris et al. prado and west however the structural approach to time series is often easier to understand than the arma approach. in addition it allows the parameters to evolve over time which makes the models more adaptive to non-stationarity. inference in lg-ssm in this section we discuss exact inference in lg-ssm models. we first consider the online case which is analogous to the forwards algorithm for hmms. we then consider the offline case which is analogous to the forwards-backwards algorithm for hmms. the kalman filtering algorithm the kalman filter is an algorithm for exact bayesian filtering for linear-gaussian state space models. we will represent the marginal posterior at time t by n t t since everything is gaussian we can perform the prediction and update steps in closed form as we explain below. the resulting algorithm is the gaussian analog of the hmm filter in section inference in lg-ssm prediction step the prediction step is straightforward to derive n btut qtn t t n tt tt tt at t btut tt at t t qt measurement step the measurement step can be computed using bayes rule as follows pztyt pytzt in section we show that this is given by ut t t t tt ktrt t ktct tt where rt is the residual or innovation given by the difference between our predicted observation and the actual observation rt yt yt yt e ct tt dtut and kt is the kalman gain matrix given by kt tt t s t where st cov e ct tt t ytctzt t t rt where t n rt is an observation noise term which is independent of all other noise sources. note that by using the matrix inversion lemma the kalman gain matrix can also be written as kt tt tt r tt ct rc r we now have all the quantities we need to implement the algorithm see kalmanfilter for some matlab code. let us try to make sense of these equations. in particular consider the equation for the mean update t tt ktrt. this says that the new mean is the old mean plus a chapter state space models correction factor which is kt times the error signal rt. the amount of weight placed on the error signal depends on the kalman gain matrix. which is the ratio between the covariance of the prior the dynamic model and the covariance if we have a strong prior andor very noisy sensors will be of the measurement error. small and we will place little weight on the correction term. conversely if we have a weak prior andor high precision sensors then will be large and we will place a lot of weight on the correction term. if ct i then kt tt t marginal likelihood as a byproduct of the algorithm we can also compute the log-likelihood of the sequence using log log t where n tt st posterior predictive the one-step-ahead posterior predictive density for the observations can be computed as follows n rn tt tt n tt c tt r this is useful for time series forecasting. computational issues there are two dominant costs in the kalman filter the matrix inversion to compute the kalman gain matrix kt which takes time and the matrix-matrix multiply to compute t which takes time. in some applications robotic mapping we have so the latter cost dominates. however in such cases we can sometimes use sparse approximations et al. in cases where we can precompute kt since suprisingly it does not depend on the actual observations unusual property that is specific to linear gaussian systems. the iterative equations for updating t are called the ricatti equations and for time invariant systems where t they converge to a fixed point. this steady state solution can then be used instead of using a time-specific gain matrix. in practice more sophisticated implementations of the kalman filter should be used for reasons of numerical stability. one approach is the information filter which recursively updates the canonical parameters of the gaussian t and t t t instead of the moment parameters. another approach is the square root filter which works with the cholesky decomposition or the utdtut decomposition of t. this is much more numerically stable than directly updating t. further details can be found at httpwww.cs.unc.eduwelchkal man and in various books such as t inference in lg-ssm derivation we now derive the kalman filter equations. for notational simplicity we will ignore the input terms from bayes rule for gaussians we have that the posterior precision is given by t tt ct t r t ct from the matrix inversion lemma we can rewrite this as t tt tt ktct tt t ct tt t tt from bayes rule for gaussians the posterior mean is given by t tctr t yt t tt tt we will now massage this into the form stated earlier. applying the second matrix inversion lemma to the first term of equation we have tctr t yt tt ct t ct t r t ct tt t t yt ktyt tt now applying the matrix inversion lemma to the second term of equation we have t putting the two together we get t tt ktyt ct tt the kalman smoothing algorithm in section we described the kalman filter which sequentially computes for each t. this is useful for online inference problems such as tracking. however in an offline setting we can wait until all the data has arrived and then compute by conditioning on past and future data our uncertainty will be significantly reduced. this is illustrated in figure where we see that the posterior covariance ellipsoids are smaller for the smoothed trajectory than for the filtered trajectory. ellipsoids are larger at the beginning and end of the trajectory since states near the boundary do not have as many useful neighbors from which to borrow information. t ct t r tt tt tt ktct tt ktct tt ct tt tt ct t tt t tt tt tt tt tt t tt t tt tt tt chapter state space models we now explain how to compute the smoothed estimates using an algorithm called the rts smoother named after its inventors rauch tung and striebel et al. it is also known as the kalman smoothing algorithm. the algorithm is analogous to the forwardsbackwards algorithm for hmms although there are some small differences which we discuss below. algorithm kalman filtering can be regarded as message passing on a graph from left to right. when the messages have reached the end of the graph we have successfully computed now we work backwards from right to left sending information from the future back to the past and them combining the two information sources. the question is how do we compute these backwards equations? we first give the equations then the derivation. we have tt tt tt tt jt tt tt jt t jt ttat where jt is the backwards kalman gain matrix. the algorithm can be initialized from tt and tt from the kalman filter. note that this backwards pass does not need access to the data that is it does not need this allows us to throw away potentially high dimensional observation vectors and just keep the filtered belief states which usually requires less memory. derivation we now derive the kalman smoother following the presentation of sec the key idea is to leverage the markov property which says that zt is independent of future data as long as is known. of course is not known but we have a distribution over it. so we condition on and then integrate it out as follows. by induction assume we have already computed the smoothed distribution for t the question is how do we perform the integration? n first we compute the filtered two-slice distribution pzt as follows pzt n tt tt ttat zt tt inference in lg-ssm now we use gaussian conditioning to compute as follows tt tt jt t we can compute the smoothed distribution for t using the rules of iterated expectation and tt e e iterated covariance. first the mean e e tt e tt jt e e cov tt cov now the covariance tt cov jtcov t tt jt jt jt tt jt t cov cov tt jt t e t tt jt jt t the algorithm can be initialized from tt and tt from the last step of the filtering algorithm. t comparison to the forwards-backwards algorithm for hmms note that in both the forwards and backwards passes for lds we always worked with normalized distributions either conditioned on the past data or conditioned on all the data. furthermore the backwards pass depends on the results of the forwards pass. this is different from the usual presentation of forwards-backwards for hmms where the backwards pass can be computed independently of the forwards pass section it turns out that we can rewrite the kalman smoother in a modified form which makes it more similar to forwards-backwards for hmms. in particular we have pzt now so chapter state space models which is the conditional likelihood of the future data. this backwards message can be computed independently of the forwards message. however this approach has several disadvantages it needs access to the original observation sequence the backwards message is a likelihood not a posterior so it need not to integrate to over zt in fact it may not always be possible to represent as a gaussian with positive definite covariance problem does not arise in discrete state-spaces as used in hmms when exact inference is not possible it makes more sense to try to approximate the smoothed distribution rather than the backwards likelihood term section there is yet another variant known as two-filter smoothing whereby we compute in the forwards pass as usual and the filtered posterior in the backwards pass. these can then be easily combined to compute see briers et al. for details. learning for lg-ssm in this section we briefly discuss how to estimate the parameters of an lg-ssm. in the control theory community this is known as systems identification when using ssms for time series forecasting and also in some physical state estimation problems the observation matrix c and the transition matrix a are both known and fixed by definition of the model. in such cases all that needs to be learned are the noise covariances q and r. initial state estimate is often less important since it will get washed away by the data after a few time steps. this can be encouraged by setting the initial state covariance to be large representing a weak prior. although we can estimate q and r offline using the methods described below it is also possible to derive a recursive procedure to exactly compute the posterior pzt r which has the form of a normal-inverse-wishart see and harrison prado and west for details. identifiability and numerical stability in the more general setting where the hidden states have no pre-specified meaning we need to learn a and c. however in this case we can set q i without loss of generality since an arbitrary noise covariance can be modeled by appropriately modifying a. also by analogy with factor analysis we can require r to be diagonal without loss of generality. doing this reduces the number of free parameters and improves numerical stability. another constraint that is useful to impose is on the eigenvalues of the dynamics matrix a. in this case the hidden to see why this is important consider the case of no system noise. state at time t is given by zt u tu where u is the matrix of eigenvectors for a and diag i contains the eigenvalues. if any i then for large t zt will blow up in magnitude. consequently to ensure stability it is useful to require that all the eigenvalues are less than et al. of course if all the eigenvalues are less than then e for large t so the state will return to the origin. fortunately when we add noise the state become non-zero so the model does not degenerate. approximate online inference for non-linear non-gaussian ssms below we discuss how to estimate the parameters. however for simplicity of presentation we do not impose any of the constraints mentioned above. training with fully observed data if we observe the hidden state sequences we can fit the model by computing the mles even the full posteriors for the parameters by solving a multivariate linear regression problem for zt zt and for zt yt. that is we can estimate a by solving the least squares problem azt and similarly for c. we can estimate the system noise covariance ja q from the residuals in predicting zt from zt and estimate the observation noise covariance r from the residuals in predicting yt from zt. em for lg-ssm if we only observe the output sequence we can compute ml or map estimates of the parameters using em. the method is conceptually quite similar to the baum-welch algorithm for hmms except we use kalman smoothing instead of forwards-backwards in the e step and use different calculations in the m step. we leave the details to exercise subspace methods em does not always give satisfactory results because it is sensitive to the initial parameter estimates. one way to avoid this is to use a different approach known as a subspace method and moor katayama to understand this approach let us initially assume there is no observation noise and no in this case we have zt azt and yt czt and hence yt cat system noise. consequently all the observations must be generated from a dimzt-dimensional linear manifold or subspace. we can identify this subspace using pca the above references for details. once we have an estimate of the zt s we can fit the model as if it were fully observed. we can either use these estimates in their own right or use them to initialize em. bayesian methods for fitting lg-ssms there are various offline bayesian alternatives to the em algorithm including variational bayes em barber and chiappa and blocked gibbs sampling and kohn cappe et al. fruhwirth-schnatter the bayesian approach can also be used to perform online learning as we discussed in section unfortunately once we add the ssm parameters to the state space the model is generally no longer linear gaussian. consequently we must use some of the approximate online inference methods to be discussed below. approximate online inference for non-linear non-gaussian ssms in section we discussed how to perform exact online inference for lg-ssms. however many models are non linear. for example most moving objects do not move in straight lines. and even if they did if we assume the parameters of the model are unknown and add them chapter state space models to the state space the model becomes nonlinear. furthermore non-gaussian noise is also very common e.g. due to outliers or when inferring parameters for glms instead of just linear regression. for these more general models we need to use approximate inference. the approximate inference algorithms we discuss below approximate the posterior by a gausin general if y f where x has a gaussian distribution and f is a non-linear sian. function there are two main ways to approximate py by a gaussian. the first is to use a first-order approximation of f. the second is to use the exact f but to project f onto the space of gaussians by moment matching. we discuss each of these methods in turn. also section where we discuss particle filtering which is a stochastic algorithm for approximate online inference which uses a non-parametric approximation to the posterior which is often more accurate but slower to compute. extended kalman filter in this section we focus on non-linear models but we assume the noise is gaussian. that is we consider models of the form zt gut zt qt yt hzt rt where the transition model g and the observation model h are nonlinear but differentiable functions. furthermore we focus on the case where we approximate the posterior by a single gaussian. simplest way to handle more general posteriors multi-modal discrete etc. is to use particle filtering which we discuss in section the extended kalman filter or ekf can be applied to nonlinear gaussian dynamical systems of this form. the basic idea is to linearize g and h about the previous state estimate using a first order taylor series expansion and then to apply the standard kalman filter equations. noise variance in the equations and r is not changed i.e. the additional error due to linearization is not modeled. thus we approximate the stationary non-linear dynamical system with a non-stationary linear dynamical system. the intuition behind the approach is shown in figure which shows what happens when we pass a gaussian distribution px shown on the bottom right through a nonlinear function y gx shown on the top right. the resulting distribution by monte carlo is shown in the shaded gray area in the top left corner. the best gaussian approximation to this computed from e and var by monte carlo is shown by the solid black line. the ekf approximates this gaussian as follows it linearizes the g function at the current mode and then passes the gaussian distribution px through this linearized function. in this example the result is quite a good approximation to the first and second moments of py for much less cost than an mc approximation. in more detail the method works as follows. we approximate the measurement model using pytzt n tt tyt tt rt where ht is the jacobian matrix of h evaluated at the prior mode hij hiz zj ht hz tt approximate online inference for non-linear non-gaussian ssms py gaussian of py mean of py ekf gaussian mean of ekf y py x g y x p function gx taylor approx. mean g px mean x x figure nonlinear transformation of a gaussian random variable. the prior px is shown on the bottom right. the function y gx is shown on the top right. the transformed distribution py is shown in the top left. a linear function induces a gaussian distribution but a non-linear function induces a complex distribution. the solid line is the best gaussian approximation to this the dotted line is the ekf approximation to this. source figure of et al. used with kind permission of sebastian thrun. similarly we approximate the system model using pztzt ut n t tzt t qt where giju giu z zj gt gutz t so g is the jacobian matrix of g evaluated at the prior mode. given this we can then apply the kalman filter to compute the posterior as follows tt gut t vtt gtvt t qt t kt vtt t tt ktyt h tt vt kthtvtt t rt chapter state space models actual sigma-point linearized covariance sigma points mean f x f i i y true mean s-p mean f x y a p p at x true covariance s-p covariance transformed sigma points a xp at f x figure an example of the unscented transform in two dimensions. source and der merwe used with kind permission of eric wan. we see that the only difference from the regular kalman filter is that when we compute the state prediction we use gut t instead of at t btut and when we compute the measurement update we use h tt instead of ct tt it is possible to improve performance by repeatedly re-linearizing the equations around t instead of tt this is called the iterated ekf and yields better results although it is of course slower. there are two cases when the ekf works poorly. the first is when the prior covariance is large. in this case the prior distribution is broad so we end up sending a lot of probability mass through different parts of the function that are far from the mean where the function has been linearized. the other setting where the ekf works poorly is when the function is highly nonlinear near the current mean. in section we will discuss an algorithm called the ukf which works better than the ekf in both of these settings. unscented kalman filter the unscented kalman filter is a better version of the ekf and uhlmann it is so-called because it doesn t stink the key intuition is this it is easier to approximate a gaussian than to approximate a function. so instead of performing a linear approximation to the function and passing a gaussian through it instead pass a deterministically chosen set of points known as sigma points through the function and fit a gaussian to the resulting transformed points. this is known as the unscented transform and is sketched in figure explain this figure in detail below. approximate online inference for non-linear non-gaussian ssms the ukf basically uses the unscented transform twice once to approximate passing through the system model g and once to approximate passing through the measurement model h. we give the details below. note that the ukf and ekf both perform operations per time step where d is the size of the latent state-space. however the ukf is accurate to at least second order whereas the ekf is only a first order approximation both the ekf and ukf can be extended to capture higher order terms. furthermore the unscented transform does not require the analytic evaluation of any derivatives or jacobians so-called derivative free filter making it simpler to implement and more widely applicable. the unscented transform before explaining the ukf we first explain the unscented transform. assume px n and consider estimating py where y f for some nonlinear function f the unscented transform does this as follows. first we create a set of sigma points xi given by x where d is a scaling parameter to be specified below and the notation mi means the i th column of matrix m. these sigma points are propagated through the nonlinear function to yield yi f and the mean and covariance for y is computed as follows y y wi myi cyi yyi yt wi where the w s are weighting terms given by wi m wi c d d wi m wi c see figure for an illustration. in general the optimal values of and are problem dependent but when d they are thus in the case so the sigma points are and the ukf algorithm the ukf algorithm is simply two applications of the unscented tranform one to compute and the other to compute we give the details below. chapter state space models the first step is to approximate the predictive density n t t by passing the old belief state n t t through the system model g as follows t t t t z i t gut t mz i wi t t t t cz i wi t tz i t t t t d t y i t t my i wi yt t t t zy t t ty i t ytt cz i wi t s t kt zy t t ktyt yt t t ktstkt t the second step is to approximate the likelihood pytzt n yt st by passing the where prior n t t through the observation model h tid t tid cy i wi t yty i t ytt rt st finally we use bayes rule for gaussians to get the posterior n t t assumed density filtering in this section we discuss inference where we perform an exact update step but then approximate the posterior by a distribution of a certain convenient form such as a gaussian. more precisely let the unknowns that we want to infer be denoted by t. suppose that q is a set of tractable distributions e.g. gaussians with a diagonal covariance matrix or a product of discrete distributions. suppose that we have an approximate prior qt t p t where qt q. we can update this with the new measurement to get the approximate posterior p t zt pyt tqtt t approximate online inference for non-linear non-gaussian ssms u pdate qtt predict qt u pdate predict p r o j e c t pt p r o j e c t qt t t st st yt yt xt xt figure ical logistic regression model. compare to figure illustration of the predict-update-project cycle of assumed density filtering. a dynam where zt pyt tqtt td t is the normalization constant and qtt t p t t t t is the one step ahead predictive distribution. if the prior is from a suitably restricted family this one-step update process is usually tractable. however we often find that the resulting posterior is no longer in our tractable family p t q. so after updating we seek the best tractable approximation by computing q t argmin q q kl p tq t this minimizes the the kullback-leibler divergence from the approximation q t to the exact posterior p t and can be thought of as projecting p onto the space of tractable distributions. the whole algorithm consists of predict-update-project cycles. this is known as assumed density filtering or adf see figure for a sketch. if q is in the exponential family one can show that this kl minimization can be done by moment matching. we give some examples of this below. boyen-koller algorithm for online inference in dbns if we are performing inference in a discrete-state dynamic bayes net where tj is the j th hidden variable at time t then the exact posterior p t becomes intractable to compute because of the entanglement problem. suppose we use a fully factored approximation cat tj tj where tjk q tj k is the probability variable of the form q t j is in state k and d is the number of variables. in this case the moment matching operation becomes tjk p tj k chapter state space models this can be computed by performing a predict-update step using the factored prior and then computing the posterior marginals. this is known as the boyen-koller algorithm named after the authors of and koller who demonstrated that the error incurred by this series of repeated approximations remains bounded certain assumptions about the stochasticity of the system. gaussian approximation for online inference in glms n tj tj tj where tj is the variance. then the optimal now suppose q t parameters of the tractable approximation to the posterior are tj e p tj tj var p tj this method can be used to do online inference for the parameters of many statistical models. for example thetrueskill system used in microsoft s xbox to rank players over time uses this form of approximation et al. we can also apply this method to simpler models such as glm which have the advantage that the posterior is log-concave. below we explain how to do this for binary logistic regression following the presentation of the model has the form pytxt t berytsigmxt p t t t t t t where is some process noise which allows the parameters to change slowly over time. can be set to as in the recursive least squares method if desired. we will j n t t t is the tractable prior. we can compute the assume qt t one-step-ahead predictive density qtt t using the standard linear-gaussian update. so now we concentrate on the measurement update step. define the deterministic quantity st t if qtt t j n tj tt tt then we can compute the predictive distribution for st as follows qtt vtt t xt as shown in figure j j mtt vtt xtj tt tj tt the posterior for st is given by qtst vt mt vt zt st zt zt pytstqtt pytstqtt t t pytstqtt hybrid discretecontinuous ssms where pytst berytst. these integrals are one dimensional and so can be computed using gaussian quadrature for details. this is the same as one step of the ukf algorithm. having inferred qst we need to compute q this can be done as follows. define m as the change in the mean of st and v as the change in the variance mt mtt m vt vtt v then one can show that the new factored posterior over the model parameters is given by q tj tj tj tj tj tt aj m tj tt j v xtj tt aj thus we see that the parameters which correspond to inputs with larger magnitude or larger uncertainty tt get updated most which makes intuitive sense. tt in a version of this algorithm is derived using a probit likelihood section in this case the measurement update can be done in closed form without the need for numerical in either case the algorithm only takes od operations per time step so it can integration. be applied to models with large numbers of parameters. and since it is an online algorithm it can also handle massive datasets. for example et al. use a version of this algorithm to fit a multi-class classifier online to very large datasets. they beat alternative bayesian online learning algorithms and sometimes even outperform state of the art batch learning methods such as svms in section hybrid discretecontinuous ssms many systems contain both discrete and continuous hidden variables these are known as hybrid systems. for example the discrete variables may indicate whether a measurement sensor is faulty or not or which regime the system is in. we will see some other examples below. a special case of a hybrid system is when we combine an hmm and an lg-ssm. this is called a switching linear dynamical system a jump markov linear system or a switching state space model more precisely we have a discrete latent variable qt k a continuous latent variable zt r l an continuous observed response yt r u we then assume that the continuous variables have linear gaussian cpds conditional on the discrete states d and an optional continuous observed input or control ut r pqt kqt j ij pztzt qt k ut bkut qk pytzt qt k ut dkut rk see figure for the dgm representation. chapter state space models ut qt ut qt zt zt yt yt figure a switching linear dynamical system. squares represent discrete nodes circles represent continuous nodes. illustration of how the number of modes in the belief state grows exponentially over time. we assume there are two binary states. inference unfortunately inference state estimation in hybrid models including the switching lgis intractable. to see why suppose qt is binary but that only the dynamics ssm model a depend on qt not the observation matrix. our initial belief state will be a mixture of gaussians corresponding to and the one-step-ahead predictive density will be a mixture of gaussians and obtained by passing each of the prior modes through the possible transition models. the belief state at step will also be a mixture of gaussians obtained by updating each of the above distributions with at step the belief state will be a mixture of gaussians. and so on. so we see there is an exponential explosion in the number of modes figure various approximate inference methods have been proposed for this model such as the following prune off low probability trajectories in the discrete tree this is the basis of multiple hypothesis tracking and fortmann bar-shalom and li use monte carlo. essentially we just sample discrete trajectories and apply an analytical filter to the continuous variables conditional on a trajectory. see section for details. use adf where we approximate the exponentially large mixture of gaussians with a smaller mixture of gaussians. see section for details. a gaussian sum filter for switching ssms a gaussian sum filter and alspach approximates the belief state at each step by a mixture of k gaussians. this can be implemented by running k kalman filters in hybrid discretecontinuous ssms filter t merge t b b bbn merge t t t b b filter t b filter t filter t t t merge t filter t t filter t figure adf for a switching linear dynamical system. for details. method. imm method. see text parallel. this is particularly well suited to switching ssms. we now describe one version of this algorithm known as the second order generalized pseudo bayes filter and fortmann we assume that the prior belief state bt is a mixture of k gaussians one per discrete state t pzt qt t t t bi we then pass this through the k different linear models to get t pzt qt i qt tijn tij tij bij where tij t jqt i. finally for each value of j we collapse the k gaussian mixtures down to a single mixture to give t pzt qt tjn tj tj bj chapter state space models see figure for a sketch. q arg minq kl where pz be solved by moment matching that is the optimal way to approximate a mixture of gaussians with a single gaussian is given by k kn k k and qz n this can e k k k cov k k k k k in the graphical model literature this is called weak marginalization since it preserves the first two moments. applying these equations to our model we can go from bij to t bj t as follows we drop the t subscript for brevity ij i i ji ij j ji j j ji ij ij j ij jt i this algorithm requires running k filters at each step. a cheaper alternative is to represent the belief state by a single gaussian marginalizing over the discrete switch at each step. this is a straightforward application of adf. an offline extension to this method called expectation correction is described in mesot and barber another heuristic approach known as interactive multiple models or imm and fortmann can be obtained by first collapsing the prior to a single gaussian moment matching and then updating it using k different kalman filters one per value of qt. see figure for a sketch. application data association and multi-target tracking suppose we are tracking k objects such as airplanes and at time t we observe detection events e.g. blips on a radar screen. we can have k due to occlusion or missed detections. we can have k due to clutter or false alarms. or we can have k. in any case we need to figure out the correspondence between the detections ytk and the k objects ztj. this is called the problem of data association and it arises in many application domains. figure gives an example in which we are tracking k objects. at each time step qt is the unknown mapping which specifies which objects caused which observations. it specifies the wiring diagram for time slice t. the standard way to solve this problem is to compute a weight which measures the compatibility between object j and measurement k typically based on how close k is to where the model thinks j should be so-called nearest neighbor data association heuristic. this gives us a k weight matrix. we can make this into a hybrid discretecontinuous ssms zt zt yt yt yt qt qt figure a model for tracking two objects in the presence of data-assocation ambiguity. we observe and detections in the first three time steps. square matrix of size n n wheren maxk by adding dummy background objects which can explain all the false alarms and adding dummy observations which can explain all the missed detections. we can then compute the maximal weight bipartite matching using the hungarian algorithm which takes on time e.g. et al. conditional on this we can perform a kalman filter update where objects that are assigned to dummy observations do not perform a measurement update. an extension of this method to handle a variable andor unknown number of objects is known as multi-target tracking. this requires dealing with a variable-sized state space. there are many ways to do this but perhaps the simplest and most robust methods are based on sequential monte carlo et al. or mcmc et al. oh et al. application fault diagnosis consider the model in figure this represents an industrial plant consisting of various tanks of liquid interconnected by pipes. in this example we just have two tanks for simplicity. we want to estimate the pressure inside each tank based on a noisy measurement of the flow into and out of each tank. however the measurement devices can sometimes fail. furthermore pipes can burst or get blocked we call this a resistance failure this model is widely used as a benchmark in the fault diagnosis community and biswas we can create a probabilistic model of the system as shown in figure the square nodes represent discrete variables such as measurement failures and resistance failures. the remaining variables are continuous. a variety of approximate inference algorithms can be applied to this model. see and lerner for one approach based on rao-blackwellized particle filtering is explained in section chapter state space models rf rf m f m f f m f m p p m f m f f m f m rf p rf rf p f m f m m f m f rf figure the two-tank system. the goal is to infer when pipes are blocked or have burst or sensors have broken from observations of the flow out of tank f out of tank f or between tanks and f is a hidden variable representing the resistance of the pipe out of tank p is a hidden variable representing the pressure in tank etc. source figure of and lerner used with kind permission of daphne koller. dynamic bayes net representation of the two-tank system. discrete nodes are squares continuous nodes are circles. abbreviations r resistance p pressure f flow m measurement rf resistance failure mf measurement failure. based on figure of and lerner application econometric forecasting the switching lg-ssm model is widely used in econometric forecasting where it is called a regime switching model. for example we can combine two linear trend models section one in which bt reflects a growing economy and one in which bt reflects a shrinking economy. see and harrison for further details. exercises exercise derivation of em for lg-ssm derive the e and m steps for computing a optimal mle for an lg-ssm model. hint the results are in and hinton your task is to derive these results. exercise seasonal lg-ssm model in standard form write the seasonal model in figure as an lg-ssm. define the matrices a c q and r. undirected graphical models random fields introduction in chapter we discussed directed graphical models commonly known as bayes nets. however for some domains being forced to choose a direction for the edges as required by a dgm is rather awkward. for example consider modeling an image. we might suppose that the intensity values of neighboring pixels are correlated. we can create a dag model with a lattice topology as shown in figure this is known as a causal mrf or a markov mesh et al. however its conditional independence properties are rather unnatural. in particular the markov blanket in section of the node in the middle is the other colored nodes and rather than just its nearest neighbors as one might expect. an alternative is to use an undirected graphical model also called a markov random field or markov network. these do not require us to specify edge orientations and are much more natural for some problems such as image analysis and spatial statistics. for example an undirected lattice is shown in figure now the markov blanket of each node is just its nearest neighbors as we show in section roughly speaking the main advantages of ugms over dgms are they are symmetric and therefore more natural for certain domains such as spatial or relational data and discriminativel ugms conditional random fields or crfs which define conditional densities of the form pyx work better than discriminative dgms for reasons we explain in section the main disadvantages of ugms compared to dgms are the parameters are less interpretable and less modular for reasons we explain in section and parameter estimation is computationally more expensive for reasons we explain in section see et al. for an empirical comparison of the two approaches for an image processing task. conditional independence properties of ugms key properties ugms define ci relationships via simple graph separation as follows for sets of nodes a b and c we say xa g xbxc iff c separates a from b in the graph g. this means that when we remove all the nodes in c if there are no paths connecting any node in a to any node in b then the ci property holds. this is called the global markov property for ugms. for example in figure we have that chapter undirected graphical models random fields a lattice represented as a dag. the dotted red node is independent of all other figure nodes given its markov blanket which include its parents children and co-parents the same model represented as a ugm. the red node is independent of the other black nodes given its neighbors nodes. figure a dgm. its moralized version represented as a ugm. the set of nodes that renders a node t conditionally independent of all the other nodes in the graph is called t s markov blanket we will denote this by mbt. formally the markov blanket satisfies the following property t v cltmbt where clt mbt is the closure of node t. one can show that in a ugm a node s markov blanket is its set of immediate neighbors. this is called the undirected local markov property. for example in figure we have from the local markov property we can easily see that two nodes are conditionally independent given the rest if there is no direct edge between them. this is called the pairwise markov property. in symbols this is written as s tv t g st using the three markov properties we have discussed we can derive the following ci properties others from the ugm in figure pairwise local conditional independence properties of ugms g l p px figure relationship between markov properties of ugms. the ancestral graph induced by the dag in figure wrt u figure moralized version of the global it is obvious that global markov implies local markov which implies pairwise markov. what is less obvious but nevertheless true px for all x i.e. that p is a positive density is that pairwise implies global and hence that all these markov properties are the same as illustrated in figure e.g. and friedman for a the importance of this result is that it is usually easier to empirically assess pairwise conditional independence such pairwise ci statements can be used to construct a graph from which global ci statements can be extracted. an undirected alternative to d-separation we have seen that determinining ci relationships in ugms is much easier than in dgms because we do not have to worry about the directionality of the edges. in this section we show how to determine ci relationships for a dgm using a ugm. it is tempting to simply convert the dgm to a ugm by dropping the orientation of the edges but this is clearly incorrect since a v-structure a b c has quite different ci properties than the corresponding undirected chain a b c. the latter graph incorrectly states that a cb. to avoid such incorrect ci statements we can add edges between the unmarried parents a and c and then drop the arrows from the edges forming this case a fully connected undirected graph. this process is called moralization. figure gives a larger the restriction to positive densities arises because deterministic constraints can result in independencies present in the distribution that are not explicitly represented in the graph. see e.g. and friedman for some examples. distributions with non-graphical ci properties are said to be unfaithful to the graph so ip ig. chapter undirected graphical models random fields figure dgms and ugms can perfectly represent different sets of distributions. some distributions can be perfectly represented by either dgms or ugms the corresponding graph must be chordal. example of moralization we interconnect and since they have a common child and we interconnect and since they have a common child unfortunately moralization loses some ci information and therefore we cannot use the moralized ugm to determine ci properties of the dgm. for example in figure using d-separation we see that adding a moralization arc would lose this fact figure however notice that the moralization edge due to the common child is not needed if we do not observe or any of its descendants. this suggests the following approach to determining if a bc. first we form the ancestral graph of dag g with respect to u a b c. this means we remove all nodes from g that are not in u or are not ancestors of u we then moralize this ancestral graph and apply the simple graph separation rules for ugms. for example in figure we show the ancestral graph for figure using u in figure we show the moralized version of this graph. it is clear that we now correctly conclude that comparing directed and undirected graphical models which model has more expressive power a dgm or a ugm? to formalize this question recall that we say that g is an i-map of a distribution p if ig ip. now define g to be perfect map of p if ig in other words the graph can represent all only the ci properties of the distribution. it turns out that dgms and ugms are perfect maps for different sets of distributions figure in this sense neither is more powerful than the other as a representation language. as an example of some ci relationships that can be perfectly modeled by a dgm but not a ugm consider a v-structure a c b. this asserts that a b and a bc. if we drop the arrows we get a c b which asserts a bc and a b which is incorrect. in fact there is no ugm that can precisely represent all and only the two ci statements encoded by a vstructure. in general ci properties in ugms are monotonic in the following sense if a bc then a bc d. but in dgms ci properties can be non-monotonic since conditioning parameterization of mrfs a a d b d b c c b a d c figure a ugm and two failed attempts to represent it as a dgm. source figure of and friedman used with kind permission of daphne koller. on extra variables can eliminate conditional independencies due to explaining away. as an example of some ci relationships that can be perfectly modeled by a ugm but not a dgm consider the shown in figure one attempt to model this with a dgm is shown in figure this correctly asserts that a cb d. however it incorrectly asserts that b da. figure is another incorrect dgm it correctly encodes a cb d but incorrectly encodes b d. in fact there is no dgm that can precisely represent all and only the ci statements encoded by this ugm. some distributions can be perfectly modeled by either a dgm or a ugm the resulting graphs are called decomposable or chordal. roughly speaking this means the following if we collapse together all the variables in each maximal clique to make mega-variables the resulting graph will be a tree. of course if the graph is already a tree includes chains as a special case it will be chordal. see section for further details. parameterization of mrfs although the ci properties of ugm are simpler and more natural than for dgms representing the joint distribution for a ugm is less natural than for a dgm as we see below. the hammersley-clifford theorem since there is no topological ordering associated with an undirected graph we can t use the chain rule to represent py. so instead of associating cpds with each node we associate potential functions orfactors with each maximal clique in the graph. we will denote the potential function for clique c by cyc c. a potential function can be any non-negative function of its arguments. the joint distribution is then defined to be proportional to the product of clique potentials. rather surprisingly one can show that any positive distribution whose ci properties can be represented by a ugm can be represented in this way. we state this result more formally below. chapter undirected graphical models random fields theorem a positive distribution py satisfies the ci properties of an undirected graph g iff p can be represented as a product of factors one per maximal clique i.e. cyc c where c is the set of all the cliques of g and z is the partition function given by cyc c c c py z z c c x note that the partition function is what ensures the overall distribution sums to the proof was never published but can be found in e.g. and friedman for example consider the mrf in figure if p satisfies the ci properties of this graph then we can write p as follows py z y where z in particular there is a there is a deep connection between ugms and statistical physics. model known as the gibbs distribution which can be written as follows py exp z eyc c c where eyc is the energy associated with the variables in clique c. we can convert this to a ugm by defining cyc c exp eyc c we see that high probability states correspond to low energy configurations. models of this form are known as energy based models and are commonly used in physics and biochemistry as well as some branches of machine learning et al. note that we are free to restrict the parameterization to the edges of the graph rather than the maximal cliques. this is called a pairwise mrf. in figure we get py s t stys yt this form is widely used due to its simplicity although it is not as general. the partition function is denoted by z because of the german word zustandssumme which means sum over states this reflects the fact that a lot of pioneering working in statistical physics was done by germans. parameterization of mrfs representing potential functions if the variables are discrete we can represent the potential or energy functions as tables of numbers just as we did with cpts. however the potentials are not probabilities. rather they represent the relative compatibility between the different assignments to the potential. we will see some examples of this below. a more general approach is to define the log potentials as a linear function of the parameters log cyc cyct c where cxc is a feature vector derived from the values of the variables yc. the resulting log probability has the form log py cyct c z c this is also known as a maximum entropy or a log-linear model. for example consider a pairwise mrf where for each edge we associate a feature vector of length k as follows stys yt j yt k if we have a weight for each feature we can convert this into a k k potential function as follows stys j yt k exp t st stjk exp stj k so we see that we can easily represent tabular potentials using a log-linear form. but the log-linear form is more general. to see why this is useful suppose we are interested in making a probabilistic model of english spelling. since certain letter combinations occur together quite frequently ing we will need higher order factors to capture this. suppose we limit ourselves to letter trigrams. a tabular potential still has parameters in it. however most of these triples will never occur. an alternative approach is to define indicator functions that look for certain special triples such as ing qu- etc. then we can define the potential on each trigram as follows yt exp k kyt yt k py exp where k indexes the different features corresponding to ing qu- etc. and k is the corresponding binary feature function. by tying the parameters across locations we can define the probability of a word of any length using k kyt yt t k this raises the question of where these feature functions come from. in many applications they are created by hand to reflect domain knowledge will see examples later but it is also possible to learn them from data as we discuss in section chapter undirected graphical models random fields examples of mrfs in this section we show how several popular probability models can be conveniently expressed as ugms. ising model the ising model is an example of an mrf that arose from statistical it was originally used for modeling the behavior of magnets. in particular let ys represent the spin in some magnets called ferro-magnets of an atom which can either be spin down or up. neighboring spins tend to line up in the same direction whereas in other kinds of magnets called anti-ferromagnets the spins want to be different from their neighbors. we can model this as an mrf as follows. we create a graph in the form of a or lattice and connect neighboring variables as in figure we then define the following pairwise clique potential stys yt ewst e wst e wst ewst here wst is the coupling strength between nodes s and t. if two nodes are not connected in the graph we set wst we assume that the weight matrix w is symmetric so wst wts. often we assume all edges have the same strength so wst j wst if all the weights are positive j then neighboring spins are likely to be in the same state this can be used to model ferromagnets and is an example of an associative markov network. if the weights are sufficiently strong the corresponding probability distribution will have two modes corresponding to the all s state and the all s state. these are called the ground states of the system. if all of the weights are negative j then the spins want to be different from their neighbors this can be used to model an anti-ferromagnet and results in a frustrated system in which not all the constraints can be satisfied at the same time. the corresponding probability distribution will have multiple modes. interestingly computing the partition function zj can be done in polynomial time for associative markov networks but is np-hard in general there is an interesting analogy between ising models and gaussian graphical models. first assuming yt we can write the unnormalized log probability of an ising model as follows log py yswstyt yt wy s t arises because we sum each edge twice. if wst j we get a low energy factor of hence high probability if neighboring states agree. sometimes there is an external field which is an energy term which is added to each spin. this can be modelled using a local energy term of the form bt y where b is sometimes called ernst ising was a german-american physicist examples of mrfs a bias term. the modified distribution is given by log py wstysyt bsys yt wy bt y s t s where b. if we define that looks similar to a gaussian w and c t we can rewrite this in a form py exp one very important difference is that in the case of gaussians the normalization constant z requires the computation of a matrix determinant which can be computed in time whereas in the case of the ising model the normalization constant requires summing over all bit vectors this is equivalent to computing the matrix permanent which is np-hard in general et al. hopfield networks a hopfield network is a fully connected ising model with a symmetric weight matrix w wt these weights plus the bias terms b can be learned from training data using maximum likelihood as described in section the main application of hopfield networks is as an associative memory or content addressable memory. the idea is this suppose we train on a set of fully observed bit vectors corresponding to patterns we want to memorize. then at test time we present a partial pattern to the network. we would like to estimate the missing variables this is called pattern completion. see figure for an example. this can be thought of as retrieving an example from memory based on a piece of the example itself hence the term associative memory since exact inference is intractable in this model it is standard to use a coordinate descent algorithm known as iterative conditional modes which just sets each node to its most likely energy state given all its neighbors. the full conditional can be shown to be pys s sigmwt sy s bs picking the most probable state amounts to using the rule y y s otherwise. better inference algorithms will be discussed later in this book. since inference is deterministic it is also possible to interpret this model as a recurrent neural network. is quite different from the feedforward neural nets studied in section they are univariate conditional density models of the form pyx which can only be used for supervised learning. see hertz et al. for further details on hopfield networks. t wstyt bs and using s if a boltzmann machine generalizes the hopfield ising model by including some hidden inference in such models nodes which makes the model representationally more powerful. often uses gibbs sampling which is a stochastic version of icm section for details. ml estimation works much better than the outer product rule proposed in in because it not only lowers the energy of the observed patterns but it also raises the energy of the non-observed patterns in order to make the distribution sum to one et al. chapter undirected graphical models random fields figure examples of how an associative memory can reconstruct images. these are binary images of size pixels. top training images. row partially visible test images. row estimate after iterations. bottom final state estimate. based on figure of hertz et al. figure generated by hopfielddemo. figure visualizing a sample from a potts model of size for different association j j j the regions are labeled according to size blue is strengths largest red is smallest. used with kind permission of erik sudderth. see gibbsdemoising for matlab code to produce a similar plot for the ising model. however we could equally well apply gibbs to a hopfield net and icm to a boltzmann machine the inference algorithm is not part of the model definition. see section for further details on boltzmann machines. examples of mrfs ys xs yt xt figure a grid-structured mrf with local evidence nodes. potts model it is easy to generalize the ising model to multiple discrete states yt k. common to use a potential function of the following form it is ej ej ej stys yt this is called the potts if j then neighboring nodes are encouraged to have the same label. some samples from this model are shown in figure we see that for j large clusters occur for j many small clusters occur and at the critical value of k there is a mix of small and large clusters. this rapid change in behavior as we vary a parameter of the system is called a phase transition and has been widely studied in the physics community. an analogous phenomenon occurs in the ising model see ch for details. the potts model can be used as a prior for image segmentation since it says that neighboring pixels are likely to have the same discrete label and hence belong to the same segment. we can combine this prior with a likelihood term as follows py x pyj pxtyt t where pxtyt k is the probability of observing pixel xt given that the corresponding segment belongs to class k. this observation model can be modeled using a gaussian or a non-parametric density. that we label the hidden nodes yt and the observed nodes xt to be compatible with section zj yt j t pxtyt the corresponding graphical model is a mix of undirected and directed edges as shown in figure the undirected lattice represents the prior py in addition there are directed edge from each yt to its corresponding xt representing the local evidence. technically speaking this combination of an undirected and directed graph is called a chain graph. however renfrey potts was an australian mathematician s t chapter undirected graphical models random fields since the xt nodes are observed they can be absorbed into the model thus leaving behind an undirected backbone this model is a analog of an hmm and could be called a partially observed mrf. as in an hmm the goal is to perform posterior inference i.e. to compute function of pyx unfortunately the case is provably much harder than the case and we must resort to approximate methods as we discuss in later chapters. although the potts prior is adequate for regularizing supervised learning problems it is not sufficiently accurate to perform image segmentation in an unsupervised way since the segments produced by this model do not accurately represent the kinds of segments one sees in natural images et al. for the unsupervised case one needs to use more sophisticated priors such as the truncated gaussian process prior of and jordan gaussian mrfs an undirected ggm also called a gaussian mrf e.g. and held is a pairwise mrf of the following form s t py stys yt tyt stys yt exp tyt exp t ys styt t tyt that we could easily absorb the node potentials t into the edge potentials but we have kept them separate for clarity. the joint distribution can be written as follows py exp t y yt y we recognize this as a multivariate gaussian written in information form where and if st then there is no pairwise term connecting s and t so by the factorization theorem we conclude that ys yty st the zero entries in are called structural zeros since they represent the absent edges in the graph. thus undirected ggms correspond to sparse precision matrices a fact which we exploit in section to efficiently learn the structure of the graph. comparing gaussian dgms and ugms in section we saw that directed ggms correspond to sparse regression matrices and hence sparse cholesky factorizations of covariance matrices whereas undirected ggms correspond to an influential paper and geman which introduced the idea of a gibbs sampler proposed using the potts model as a prior for image segmentation but the results in their paper are misleading because they did not run their gibbs sampler for long enough. see figure for a vivid illustration of this point. examples of mrfs figure a process represented as a dynamic chain graph. source used with kind permission of rainer dahlhaus and oxford university press. and eichler sparse precision matrices. the advantage of the dag formulation is that we can make the regression weights w and hence be conditional on covariate information without worrying about positive definite constraints. the disadavantage of the dag formulation is its dependence on the order although in certain domains such as time series there is already a natural ordering of the variables. it is actually possible to combine both representations resulting in a gaussian chain graph. for example consider a a discrete-time second-order markov chain in which the states are continuous yt r d. the transition function can be represented as a lineargaussian cpd pytyt yt n this is called vector auto-regressive or var process of order such models are widely used in econometrics for time-series forecasting. the time series aspect is most naturally modeled using a dgm. however if is sparse then the correlation amongst the components within a time slice is most naturally modeled using a ugm. for example suppose we have and chapter undirected graphical models random fields figure based on figures of used with kind permission of myung choi. a bi-directed graph. the equivalent dag. here the w nodes are latent confounders. the resulting graphical model is illustrated in figure zeros in the transition matrices and correspond to absent directed arcs from yt and yt into yt. zeros in the precision matrix correspond to absent undirected arcs between nodes in yt. sometimes we have a sparse covariance matrix rather than a sparse precision matrix. this can be represented using a bi-directed graph where each edge has arrows in both directions as in figure here nodes that are not connected are unconditionally independent. for example in figure we see that in the gaussian case this means graph representing a sparse covariance matrix is called a covariance graph. by contrast if this were an undirected model we would have that and where a bidirected graph can be converted to a dag with latent variables where each bidirected edge is replaced with a hidden variable representing a hidden common cause or confounder as illustrated in figure the relevant ci properties can then be determined using dseparation. we can combine bidirected and directed edges to get a directed mixed graphical model. this is useful for representing a variety of models such as arma models structural equation models etc. markov logic networks in section we saw how we could unroll markov models and hmms for an arbitrary number of time steps in order to model variable-length sequences. similarly in section we saw how we could expand a lattice ugm to model images of any size. what about more complex domains where we have a variable number of objects and relationships between them? creating models for such scenarios is often done using first-order logic e.g. and norvig for example consider the sentences smoking causes cancer and if two people are friends and one smokes then so does the other we can write these sentences in first-order examples of mrfs friendsab friendsaa smokesa smokesb friendsbb cancera cancerb friendsba figure an example of a ground markov logic network represented as a pairwise mrf for people. based on figure from and lowd used with kind permission of pedro domingos. logic as follows x.smx cax x. y.f rx y smx smy where sm and ca are predicates and f r is a of course such rules are not always true. indeed this brittleness is the main reason why logical approaches to ai are no longer widely used at least not in their pure form. there have been a variety of attempts to combine first order logic with probability theory an area known as statistical relational ai or probabilistic relational modeling et al. one simple approach is to take logical rules and attach weights as certainty factors to them and then to interpret them as conditional probability distributions. for example we might say pcax unfortunately the rule does not say what to predict if smx furthermore combining cpds in this way is not guaranteed to define a consistent joint distribution because the resulting graph may not be a dag. an alternative approach is to treat these rules as a way of defining potential functions in an unrolled ugm. the result is known as a markov logic network and lowd to specify the network we first rewrite all the rules in conjunctive normal form also known as clausal form. in this case we get smx cax f rx y smx smy the first clause can be read as either x does not smoke or he has cancer which is logically equivalent to equation that in a clause any unbound variable such as x is assumed to be universally quantified. a predicate is just a function of one argument known as an object that evaluates to true or false depending on whether the property holds or not of that object. a relation is just a function of two or more arguments that evaluates to true or false depending on whether the relationship holds between that set of objects or not. chapter undirected graphical models random fields inference in first-order logic is only semi-decidable so it is common to use a restricted subset. a common approach used in prolog is to restrict the language to horn clauses which are clauses that contain at most one positive literal. essentially this means the model is a series of if-then rules where the right hand side of the rules then part or consequence has only a single term. once we have encoded our knowledge base as a set of clauses we can attach weights to each one these weights are the parameter of the model and they define the clique potentials as follows cxc expwc cxc where cxc is a logical expression which evaluates clause c applied to the variables xc and wc is the weight we attach to this clause. roughly speaking the weight of a clause specifies the probability of a world in which this clause is satsified relative to a world in which it is not satisfied. now suppose there are two objects in the world anna and bob which we will denote by constant symbols a and b. we can make a ground network from the above clauses by creating binary random variables sx cx and fxy for x y b and then wiring these up according to the clauses above. the result is the ugm in figure with binary nodes. note that we have not encoded the fact that f r is a symmetric relation so f ra b and f rb a might have different values. similarly we have the degenerate nodes f ra a and f rb b since we did not enforce x y in equation we add such constraints then the model compiler which generates the ground network could avoid creating redundant nodes. in summary we can think of mlns as a convenient way of specifying a ugm template that can get unrolled to handle data of arbitrary size. there are several other ways to define relational probabilistic models see e.g. and friedman kersting et al. for details. in some cases there is uncertainty about the number or existence of objects or relations so-called open universe problem. section gives a concrete example in the context of multi-object tracking. see e.g. and norvig kersting et al. and references therein for further details. learning in this section we discuss how to perform ml and map parameter estimation for mrfs. we will see that this is quite computationally expensive. for this reason it is rare to perform bayesian inference for the parameters of mrfs see et al. training maxent models using gradient methods consider an mrf in log-linear form py z exp t c cy c learning where c indexes the cliques. the scaled log-likelihood is given by c cyi log z t log pyi n n i i c since mrfs are in the exponential family we know that this function is convex in section so it has a unique global maximum which we can find using gradient-based optimizers. in particular the derivative for the weights of a particular clique c is given by c n cyi c log z i exercise asks you to show that the derivative of the log partition function wrt c is the expectation of the c th feature under the model i.e. cypy log z c e cy y hence the gradient of the log likelihood is e cy cyi c n i in the first term we fix y to its observed values this is sometimes called the clamped term. in the second term y is free this is sometimes called the unclamped term or contrastive term. note that computing the unclamped term requires inference in the model and this must be done once per gradient step. this makes ugm training much slower than dgm training. the gradient of the log likelihood can be rewritten as the expected feature vector according to the empirical distribution minus the model s expectation of the feature vector c epemp cy ep cy at the optimum the gradient will be zero so the empirical distribution of the features will match the model s predictions epemp cy ep cy this is called moment matching. this observation motivates a different optimization algorithm which we discuss in section training partially observed maxent models suppose we have missing data andor hidden variables in our model. represent such models as follows in general we can t c ch y py h z exp c chapter undirected graphical models random fields the log likelihood has the form n where hi i n log z hi pyi hi pyi hi log i py h exp t c ch y hi pyi hi is the same as the partition function is the unnormalized distribution. the term for the whole model except that y is fixed at yi. hence the gradient is just the expected features where we clamp yi but average over h c hi c log pyi hi so the overall gradient is given by e ch yi c n i ch yi e ch y the first set of expectations are computed by clamping the visible nodes to their observed values and the second set are computed by letting the visible nodes be free. in both cases we marginalize over hi. an alternative approach is to use generalized em where we use gradient methods in the m step. see and friedman for details. approximate methods for computing the mles of mrfs when fitting a ugm there is general no closed form solution for the ml or the map estimate of the parameters so we need to use gradient-based optimizers. this gradient requires inference. in models where inference is intractable learning also becomes intractable. this has motivated various computationally faster alternatives to mlmap estimation which we list in table we dicsuss some of these alternatives below and defer others to later sections. pseudo likelihood one alternative to mle is to maximize the pseudo likelihood defined as follows l pempy log pydy d n log pyidyi d that is we optimize the product of the full conditionals also known as the composite likelihood compare this to the objective for maximum likelihood l pempy log py log pyi yx y learning method closed form ipf gradient-based optimization max-margin training pseudo-likelihood stochastic ml contrastive divergence minimum probability flow restriction only chordal mrf only tabular gaussian mrf low tree width only crfs no hidden variables can integrate out the hiddens exact mle? exact exact exact na approximate exact to mc error approximate approximate section section section section section section section section sohl-dickstein et al. table some methods that can be used to compute approximate ml map parameter estimates for mrfs crfs. low tree-width means that in order for the method to be efficient the graph must tree-like see section for details. figure a small lattice. observed neighbors. based on figure of the representation used by pseudo likelihood. solid nodes are in the case of gaussian mrfs pl is equivalent to ml but this is not true in general and jordan the pl approach is illustrated in figure for a grid. we learn to predict each node given all of its neighbors. this objective is generally fast to compute since each full conditional pyidyi d only requires summing over the states of a single node yid in order to compute the local normalization constant. the pl approach is similar to fitting each full conditional separately is the method used to train dependency networks discussed in section except that the parameters are tied between adjacent nodes. one problem with pl is that it is hard to apply to models with hidden variables and welling another more subtle problem is that each node assumes that its neighbors have if node t nbrs is a perfect predictor for node s then s will learn to rely known values. completely on node t even at the expense of ignoring other potentially useful information such as its local evidence. however experiments in and welling hoefling and tibshirani suggest that pl works as well as exact ml for fully observed ising models and of course pl is much faster. stochastic maximum likelihood recall that the gradient of the log-likelihood for a fully observed mrf is given by i n e chapter undirected graphical models random fields the gradient for a partially observed mrf is similar. in both cases we can approximate the model expectations using monte carlo sampling. we can combine this with stochastic gradient descent which takes samples from the empirical distribution. pseudocode for the resulting method is shown in algorithm algorithm stochastic maximum likelihood for fitting an mrf initialize weights randomly k for each epoch do for each minibatch of size b do for each sample s s do sample ysk py k gik e e s for each training case i in minibatch do gk i b gik k gk b k k decrease step size typically we use mcmc to generate the samples. of course running mcmc to convergence at each step of the inner loop would be extremely slow. fortunately it was shown in that we can start the mcmc chain at its previous value and just take a few steps. in otherwords we sample ysk by initializing the mcmc chain at ysk and then run for a few iterations. this is valid since py k is likely to be close to py k since we only changed the parameters a small amount. we call this algorithm stochastic maximum likelihood or sml. is a closely related algorithm called persistent contrastive divergence which we discuss in section feature induction for maxent models mrfs require a good set of features. one unsupervised way to learn such features known as feature induction is to start with a base set of features and then to continually create new feature combinations out of old ones greedily adding the best ones to the model. this approach was first proposed in et al. zhu et al. and was later extended to the crf case in to illustrate the basic idea we present an example from et al. which described how to build unconditional probabilistic models to represent english spelling. initially the model has no features which represents the uniform distribution. the algorithm starts by choosing to add the feature iyt z t learning which checks if any letter is lower case or not. after the feature is added the parameters are by maximum likelihood. for this feature it turns out that which means that a word with a lowercase letter in any position is about times more likely than the same word without a lowercase letter in that position. some samples from this model generated using gibbs sampling are shown m r xevo ijjiir b to jz gsr wq vf x ga msmgh pcp d ozivlal hzagh yzop io advzmxnv ijv_bolft x emx kayerf mlj rawzyb jp ag ctdnnnbg wgdw t kguv cy spxcq uzflbbf dxtkkn cxwx jpd ztzh lv zhpkvnu l r qee nynrx ik se w lrh hp yrqyka h zcngotcnx igcump zjcjs lqpwiqu cefmfhc o lb fdcy tzby yopxmvk by fz t govyccm ijyiduwfzo duh ejv pk pjw l fl w the second feature added by the algorithm checks if two adjacent characters are lower case iys z yt z s t now the model has the form py z exp continuing in this way the algorithm adds features for the strings s and ing where represents the end of word and for various regular expressions such as etc. some samples from the model with features generated using gibbs sampling are shown below. was reaser in there to will was by homes thing be reloverated ther which conists at fores anditing with mr. proveral the on t prolling prothere mento at yaou chestraing for have to intrally of qut best compers cluseliment uster of is deveral this thise of offect inatever thifer constranded stater vill in thase in youse menttering and of in verate of to this approach of feature learning can be thought of as a form of graphical model structure learning except it is more fine-grained we add features that are useful regardless of the resulting graph structure. however the resulting graphs can become densely connected which makes inference hence parameter estimation intractable. iterative proportional fitting consider a pairwise mrf where the potentials are represented as tables with one parameter per variable setting. we can represent this in log-linear form using stys yt exp t stiys yt iys k yt k and similarly for tyt. thus the feature vectors are just indicator functions. we thank john lafferty for sharing this example. chapter undirected graphical models random fields from equation we have that at the maximum of the likelihood the empirical expectation of the features equals the model s expectation epemp j yt k ep j yt k pempys j yt k where pemp is the empirical probability ys j yt k pempys j yt k nstjk n iyns j ynt k n for a general graph the condition that must hold at the optimum is pempyc pyc for a special family of graphs known as decomposable graphs in section one can show that pyc cyc. however even if the graph is not decomposable we can imagine trying to enforce this condition. this suggests an iterative coordinate ascent scheme where at each step we compute c t cyc pempyc pyc t where the multiplication is elementwise. this is known as iterative proportional fitting or ipf bishop et al. see algorithm for the pseudocode. algorithm iterative proportional fitting algorithm for tabular mrfs initialize c for c repeat for c do pc pyc pc pempyc c c pc pc until converged example let us consider a simple example from httpen.wikipedia.orgwikiiterative_propo rtional_fitting. we have two binary variables and where if man n is left handed and otherwise similarly if woman n is left handed and otherwise. we can summarize the data using the following contingency table male female total right-handed left-handed total learning suppose we want to fit a disconnected graphical model containing nodes and but with c no edge between them. that is we want to find vectors and such that m t where m are the model s expected counts and c are the empirical counts. by moment matching we find that the row and column sums of the model must exactly match the row and column sums of the data. one possible solution is to use and below we show the model s predictions m t total right-handed left-handed male female total it is easy to see that this matches the required constraints. see for some matlab code that computes these numbers. this method is easily to generalized to arbitrary graphs. speed of ipf ipf is a fixed point algorithm for enforcing the moment matching constraints and is guaranteed to converge to the global optimum et al. the number of iterations depends on the form of the model. if the graph is decomposable then ipf converges in a single iteration but in general ipf may require many iterations. it is clear that the dominant cost of ipf is computing the required marginals under the model. efficient methods such as the junction tree algorithm can be used resulting in something called efficient ipf and preucil nevertheless coordinate descent can be slow. an alternative method is to update all the parameters at once by simply following the gradient of the likelihood. this gradient approach has the further significant advantage that it works for models in which the clique potentials may not be fully parameterized i.e. the features may not consist of all possible indicators for each clique but instead can be arbitrary. although it is possible to adapt ipf to this setting of general features resulting in a method known as iterative scaling in practice the gradient method is much faster minka generalizations of ipf we can use ipf to fit gaussian graphical models instead of working with empirical counts we work with empirical means and covariances and kiiveri it is also possible to create a bayesian ipf algorithm for sampling from the posterior of the model s parameters e.g. and massam ipf for decomposable graphical models there is a special family of undirected graphical models known as decomposable graphical models. this is formally defined in section but the basic idea is that it contains graphs which are tree-like such graphs can be represented by ugms or dgms without any loss of information. in the case of decomposable graphical models ipf converges in one iteration. in fact the chapter undirected graphical models random fields mle has a closed form solution in particular for tabular potentials we have cyc k iyic k n and for gaussian potentials we have c yic n c iyic cxic ct n by using conjugate priors we can also easily compute the full posterior over the model parameters in the decomposable case just as we did in the dgm case. see for details. conditional random fields a conditional random field or crf et al. sometimes a discriminative random field and hebert is just a version of an mrf where all the clique potentials are conditioned on input features pyx w cycx w zx w c a crf can be thought of as a structured output extension of logistic regression. we will usually assume a log-linear representation of the potentials cycx w expwt c yc where yc is a feature vector derived from the global inputs x and the local set of labels yc. we will give some examples below which will make this notation clearer. the advantage of a crf over an mrf is analogous to the advantage of a discriminative classifier over a generative classifier section namely we don t need to waste resources modeling things that we always observe. instead we can focus our attention on modeling what we care about namely the distribution of labels given the data. another important advantage of crfs is that we can make the potentials factors of the model be data-dependent. for example in image processing applications we may turn off the label smoothing between two neighboring nodes s and t if there is an observed discontinuity in the image intensity between pixels s and t. similarly in natural language processing problems we can make the latent labels depend on global properties of the sentence such as which language it is written in. it is hard to incorporate global features into generative models. the disadvantage of crfs over mrfs is that they require labeled training data and they are slower to train as we explain in section this is analogous to the strengths and weaknesses of logistic regression vs naive bayes discussed in section chain-structured crfs memms and the label-bias problem the most widely used kind of crf uses a chain-structured graph to model correlation amongst neighboring labels. such models are useful for a variety of sequence labeling tasks section conditional random fields yt yt yt xt xt xt xg yt xt xg yt xt yt xt figure various models for sequential data. directed memm. a discriminative undirected crf. a generative directed hmm. a discriminative traditionally hmms in detail in chapter have been used for such tasks. these are joint density models of the form px yw pytyt wpxtyt w where we have dropped the initial term for simplicity. see figure if we observe both xt and yt for all t it is very easy to train such models using techniques described in section an hmm requires specifying a generative observation model pxtyt w which can be difficult. furthemore each xt is required to be local since it is hard to define a generative model for the whole stream of observations x an obvious way to make a discriminative version of an hmm is to reverse the arrows from yt to xt as in figure this defines a directed discriminative model of the form pyx w pytyt x w t where x xg xg are global features and xt are features specific to node t. partition into local and global is not necessary but helps when comparing to hmms. this is called a maximum entropy markov model or memm et al. kakade et al. an memm is simply a markov chain in which the state transition probabilities are conditioned is therefore a special case of an input-output hmm discussed in on the input features. section this seems like the natural generalization of logistic regression to the structuredoutput setting but it suffers from a subtle problem known obscurely as the label bias problem et al. the problem is that local features at time t do not influence states prior to time t. this follows by examining the dag which shows that xt is d-separated from yt all earlier time points by the v-structure at yt which is a hidden child thus blocking the information flow. to understand what this means in practice consider the part of speech tagging task. suppose we see the word banks this could be a verb in he banks at boa or a noun in the river banks were overflowing locally the pos tag for the word is ambiguous. however chapter undirected graphical models random fields figure example of handwritten letter recognition. in the word brace the r and the c look very similar but can be disambiguated using context. source et al. used with kind permission of ben taskar. suppose that later in the sentence we see the word fishing this gives us enough context to infer that the sense of banks is river banks however in an memm in an hmm and crf the fishing evidence will not flow backwards so we will not be able to disambiguate banks now consider a chain-structured crf. this model has the form pyx w w w t zx w from the graph in figure we see that the label bias problem no longer exists since yt does not block the information from xt from reaching other nodes. the label bias problem in memms occurs because directed models are locally normalized meaning each cpd sums to by contrast mrfs and crfs are globally normalized which means that local factors do not need to sum to since the partition function z which sums over all joint configurations will ensure the model defines a valid distribution. however this solution comes at a price we do not get a valid probability distribution over y until we have seen the whole sentence since only then can we normalize over all configurations. consequently crfs are not as useful as dgms discriminative or generative for online or real-time inference. furthermore the fact that z depends on all the nodes and hence all their parameters makes crfs much slower to train than dgms as we will see in section applications of crfs crfs have been applied to many interesting problems we give a representative sample below. these applications illustrate several useful modeling tricks and will also provide motivation for some of the inference techniques we will discuss in chapter handwriting recognition a natural application of crfs is to classify hand-written digit strings as illustrated in figure the key observation is that locally a letter may be ambiguous but by depending on the labels of one s neighbors it is possible to use context to reduce the error rate. note that the node potential tytxt is often taken to be a probabilistic discriminative classifier conditional random fields b adj i n o v o in o v b prp i n o b in dt i n i np n pos british airways rose after announcing its withdrawal from the ual deal key b i o n adj begin noun phrase within noun phrase not a noun phrase noun adjective v in prp dt verb preposition possesive pronoun determiner a an the figure a crf for joint pos tagging and np segmentation. friedman used with kind permission of daphne koller. source figure of and such as a neural network or rvm that is trained on isolated letters and the edge potentials stys yt are often taken to be a language bigram model. later we will discuss how to train all the potentials jointly. noun phrase chunking one common nlp task is noun phrase chunking which refers to the task of segmenting a sentence into its distinct noun phrases this is a simple example of a technique known as shallow parsing. in more detail we tag each word in the sentence with b beginning of a new np i inside a np or o outside an np. this is called bio notation. for example in the following sentence the nps are marked with brackets b i o o o b i o b i i airways rose after announcing withdrawl from uai deal need the b symbol so that we can distinguish i i meaning two words within a single np from b b meaning two separate nps. a standard approach to this problem would first convert the string of words into a string of pos tags and then convert the pos tags to a string of bios. however such a pipeline method can propagate errors. a more robust approach is to build a joint probabilistic model of the form one way to do this is to use the crf in figure the connections between adjacent labels encode the probability of transitioning between the b i and o states and can enforce constraints such as the fact that b must preceed i. the features are usually hand engineered and include things like does this word begin with a capital letter is this word followed by a full stop is this word a noun etc. typically there are features per node. the number of features has minimal impact on the inference time since the features are increase in the cost of observed and do not need to be summed over. is a small chapter undirected graphical models random fields b-per i-per oth oth oth b-loc i-loc b-per oth oth oth oth mrs. green spoke today in new york green chairs the finance committee key b-per i-per b-loc begin person name within person name begin location name i-loc oth within location name not an entitiy figure a skip-chain crf for named entity recognition. source figure of and friedman used with kind permission of daphne koller. evaluating potential functions with many features but this is usually negligible if not one can use regularization to prune out irrelevant features. however the graph structure can have a dramatic effect on inference time. the model in figure is tractable since it is essentially a fat chain so we can use the forwards-backwards algorithm for exact inference in time where is the number of pos tags and is the number of np tags. however the seemingly similar graph in figure to be explained below is computationally intractable. named entity recognition a task that is related to np chunking is named entity extraction. instead of just segmenting out noun phrases we can segment out phrases to do with people and locations. similar techniques are used to automatically populate your calendar from your email messages this is called information extraction. a simple approach to this is to use a chain-structured crf but to expand the state space from bio to b-per i-per b-loc i-loc and other. however sometimes it is ambiguous whether a word is a person location or something else. nouns are particularly difficult to deal with because they belong to an open class that is there is an unbounded number of possible names unlike the set of nouns and verbs which is large but essentially fixed. we can get better performance by considering long-range correlations between words. for example we might add a link between all occurrences of the same word and force the word to have the same tag in each occurence. same technique can also be helpful for resolving the identity of pronouns. this is known as a skip-chain crf. see figure for an illustration. we see that the graph structure itself changes depending on the input which is an additional advantage of crfs over generative models. unfortunately inference in this model is generally more expensive than in a simple chain with local connections for reasons explained in section conditional random fields figure illustration of a simple parse tree based on a context free grammar in chomsky normal form. the feature vector y y counts the number of times each production rule was used. source figure of et al. used with kind permission of yasemin altun. natural language parsing a generalization of chain-structured models for language is to use probabilistic grammars. in particular a probabilistic context free grammar or pcfg is a set of re-write or production rules of the form or x where are non-terminals to parts of speech and x x are terminals i.e. words. see figure for an example. each such rule has an associated probability. the resulting model defines a probability distribution over sequences of words. we can compute the probability of observing a particular sequence x xt by summing over all trees that generate it. this can be done in ot time using the inside-outside algorithm see e.g. and martin manning and schuetze for details. pcfgs are generative models. it is possible to make discriminative versions which encode the probability of a labeled tree y given a sequence of words x by using a crf of the form pyx expwt y. for example we might define y to count the number of times each production rule was used is analogous to the number of state transitions in a chain-structured model. see e.g. et al. for details. hierarchical classification suppose we are performing multi-class classification where we have a label taxonomy which groups the classes into a hierarchy. we can encode the position of y within this hierarchy by defining a binary vector where we turn on the bit for component y and for all its children. this can be combined with input features using a tensor product y see figure for an example. this method is widely used for text classification where manually constructed taxnomies as the open directory project at www.dmoz.org are quite common. the benefit is that information can be shared between the parameters for nearby categories enabling generalization across classes. chapter undirected graphical models random fields figure illustration of a simple label taxonomy and how it can be used to compute a distributed representation for the label for class in this figure x y is denoted by and wt y is denoted by source figure of et al. used with kind permission of yasemin altun. protein side-chain prediction an interesting analog to the skip-chain model arises in the problem of predicting the structure of protein side chains. each residue in the side chain has dihedral angles which are usually discretized into values called rotamers. the goal is to predict this discrete sequence of angles y from the discrete sequence of amino acids x. we can define an energy function ex y where we include various pairwise interaction terms between nearby residues of the y vector. this energy is usually defined as a weighted sum of individual energy terms ex yw jejx y where the ej are energy contribution due to various electrostatic charges hydrogen bonding potentials etc and w are the parameters of the model. see et al. for details. argmin ex yw. in general this problem is np-hard depending on the nature of the graph induced by the ej terms due to long-range connections between the variables. nevertheless some special cases can be efficiently handled using methods discussed in section given the model we can compute the most probable side chain configuration using y stereo vision low-level vision problems are problems where the input is an image set of images and the output is a processed version of the image. in such cases it is common to use latticestructured models the models are similar to figure except that the features can be global and are not generated by the model. we will assume a pairwise crf. a classic low-level vision problem is dense stereo reconstruction where the goal is to estimate the depth of every pixel given two images taken from slightly different angles. in this section on and freeman we give a sketch of how a simple crf can be used to solve this task. see e.g. et al. for a more sophisticated model. by using some standard preprocessing techniques one can convert depth estimation into a conditional random fields problem of estimating the disparity ys between the pixel at location js in the left image and the corresponding pixel at location ys js in the right image. we typically assume that corresponding pixels have similar intensity so we define a local node potential of the form sysx exp js xris ys js where xl is the left image and xr is the right image. this equation can be generalized to model the intensity of small windows around each location. in highly textured regions it is usually possible to find the corresponding patch using cross correlation but in regions of low texture there will be considerable ambiguity about the correct value of ys. we can easily add a gaussian prior on the edges of the mrf that encodes the assumption that neighboring disparities ys yt should be similar as follows stys yt exp the resulting model is a gaussian crf. however using gaussian edge-potentials will oversmooth the estimate since this prior fails to account for the occasional large changes in disparity that occur between neighboring pixels which are on different sides of an occlusion boundary. one gets much better results using a truncated gaussian potential of the form stys yt exp min where encodes the expected smoothness and encodes the maximum penalty that will be imposed if disparities are significantly different. this is called a discontinuity preserving potential note that such penalties are not convex. the local evidence potential can be made robust in a similar way in order to handle outliers due to specularities occlusions etc. figure illustrates the difference between these two forms of prior. on the top left is an image from the standard middlebury stereo benchmark dataset and szeliski on the bottom left is the corresponding true disparity values. the remaining columns represent the estimated disparity after and an infinite number of rounds of loopy belief propagation section where by infinite we mean the results at convergence. the top row shows the results using a gaussian edge potential and the bottom row shows the results using the truncated potential. the latter is clearly better. unfortunately performing inference with real-valued variables is computationally difficult unless the model is jointly gaussian. consequently it is common to discretize the variables. example figure used states. the edge potentials still have the form given in equation the resulting model is called a metric crf since the potentials form a metric. inference in metric crfs is more efficient than in crfs where the discrete labels have no natural ordering as we explain in section see section for a comparison of various approximate inference methods applied to low-level crfs and see et al. prince for more details on probabilistic models for computer vision. a function f is said to be a metric if it satisfies the following three properties reflexivity f b iff a b symmetry f b a and triangle inequality f b f c f c. if f satisfies only the first two properties it is called a semi-metric. chapter undirected graphical models random fields figure illustration of belief propagation for stereo depth estimation. left column image and true disparities. remaining columns initial estimate estimate after iteration and estimate at convergence. top row gaussian edge potentials. bottom row robust edge potentials. source figure of and freeman used with kind permission of erik sudderth. crf training we can modify the gradient based optimization of mrfs described in section to the crf case in a straightforward way. in particular the scaled log-likelihood becomes and the gradient becomes i n wc n n i i c n log pyixi w i cyi xi wc cyi xi e cy xi c cyi xi log zw xi wt log zw xi note that we now have to perform inference for every single training case inside each gradient step which is on times slower than the mrf case. this is because the partition function depends on the inputs xi. in most applications of crfs some applications of mrfs the size of the graph structure can vary. hence we need to use parameter tying to ensure we can define a distribution of arbitrary size. in the pairwise case we can write the model as follows pyx w zw x exp wt x structural svms where w we are the node and edge parameters and x tyt x stys yt x s t are the summed node and edge features are the sufficient statistics. the gradient expression is easily modified to handle this case. in practice it is important to use a prior regularization to prevent overfitting. if we use a t i i gaussian prior the new objective becomes n log pyixi w it is simple to modify the gradient expression. alternatively we can use regularization. for example we could use for the edge weights we to learn a sparse graph structure and for the node weights wn as in et al. in other words the objective becomes n log pyixi w unfortunately the optimization algorithms are more complicated when we use section although the problem is still convex. to handle large datasets we can use stochastic gradient descent as described in section it is possible useful to define crfs with hidden variables for example to allow for an unknown alignment between the visible features and the hidden labels e.g. et al. in this case the objective function is no longer convex. nevertheless we can find a locally optimal ml or map parameter estimate using em and or gradient methods. structural svms we have seen that training a crf requires inference in order to compute the expected sufficient statistics needed to evaluate the gradient. for certain models computing a joint map estimate of the states is provably simpler than computing marginals as we discuss in section in this section we discuss a way to train structured output classifiers that that leverages the existence of fast map solvers. avoid confusion with map estimation of parameters we will often refer to map estimation of states as decoding. these methods are known as structural support vector machines or ssvms et al. is also a very similar class of methods known as max margin markov networks or et al. see section for a discussion of the differences. ssvms a probabilistic view in this book we have mostly concentrated on fitting models using map parameter estimation i.e. by minimizing functions of the form log pyixi w rm ap log pw chapter undirected graphical models random fields however at test time we pick the label so as to minimize the posterior expected loss in section yxw argmin l y ypyx w y y where ly y is the loss we incur when we estimate y but the truth is y it therefore seems reasonable to take the loss function into account when performing parameter so following and he let us instead minimized the posterior expected loss on the training set relw log pw log lyi ypyxi w in the special case of loss lyi y yyi this reduces to rm ap y we will assume that we can write our model in the following form pyx w expwt y zx w exp ew z pw where zx w this we can rewrite our objective as follows y expwt y. also let us define lyi y exp lyi y. with relw log pw ew log i y exp lyi y expwt y zx w lvyi y t y i log zxi w log exp y y y we will now consider various bounds in order to simplify this objective. first note that for any function f we have y y f log max expf log exp max y f log max y f for example suppose y and f y. then we have we can ignore the log term which is independent of y and treat maxy y f as both a lower and upper bound. hence we see that relw ew lyi y t y wt y max y max y note that this violates the fundamental bayesian distinction between inference and decision making. however performing these tasks separately will only result in an optimal decision if we can compute the exact posterior. in most cases this is intractable so we need to perform loss-calibrated inference et al. in this section we just perform loss-calibrated map parameter estimation which is computationally simpler. also et al. structural svms where x y means x y for some constants unfortunately this objective is not convex in w. however we can devise a convex upper bound by exploiting the following looser lower bound on the log-sum-exp function expf f log y for any y. applying this equation to our earlier example for f y and and applying this bound to rel we get we get lyi y t y wt yi relw ew max y if we set ew to a spherical gaussian prior we get rssv m c lyi y t y wt yi max y this is the same objective as used in the ssvm approach of et al. in the special case that y ly y yy and y criterion reduces to the following considering the two cases that y yi and y yi yx this rsv m c yiwt which is the standard binary svm objective equation so we see that the ssvm criterion can be seen as optimizing an upper bound on the bayesian objective a result first shown in and he this bound will be tight hence the approximation will be a good one when is large since in that case pyx w will concentrate its mass on argmaxy pyx w. unfortunately a large corresponds to a model that is likely to overfit so it is unlikely that we will be working in this regime we will tune the strength of the regularizer to avoid this situation. an alternative justification for the svm criterion is that it focusses effort on fitting parameters that affect the decision boundary. this is a better use of computational resources than fitting the full distribution especially when the model is wrong. ssvms a non-probabilistic view we now present ssvms in a more traditional way following et al. the resulting objective will be the same as the one above. however this derivation will set the stage for the algorithms we discuss below. let f w argmaxy y wt y be the prediction function. we can obtain zero loss on the training set using this predictor if wt y wt yi i. max y yyi chapter undirected graphical models random fields each one of these nonlinear inequalities can be equivalently replaced by linear inequalities resulting in a total of ny n linear constraints of the following form i. y y yi.wt yi wt y for brevity we introduce the notation iy yi y so we can rewrite these constraints as wt iy if we can achieve zero loss there will typically be multiple solution vectors w. we pick the one that maximizes the margin defined as f f yi w max yy min i w since the margin can be made arbitrarily large by rescaling w we fix its norm to be resulting in the optimization problem max s.t. i. y y yi. wt iy equivalently we can write min w s.t. i. y y yi. wt iy to allow for the case where zero loss cannot be achieved to the data being inseparable in the case of binary classification we relax the constraints by introducing slack terms i one per data case. this yields c min w s.t. i. y y yi. wt iy i i i in the case of structured outputs we don t want to treat all constraint violations equally. for example in a segmentation problem getting one position wrong should be punished less than getting many positions wrong. one way to achieve this is to divide the slack variable by the size of the loss is called slack re-scaling. this yields c min w s.t. i. y y yi. wt iy i i lyi y i alternatively we can define the margin to be proportional to the loss is called margin re-rescaling. this yields c min w i s.t. i. y y yi. wt iy lyi y i i fact we can write y y instead of y y yi since if y yi then wt iy and i by using the simpler notation which doesn t exclude yi we add an extra but redundant constraint. this latter approach is used in structural svms for future reference note that we can solve for the i max y wt i max y y lyi y t y c min w max y i i terms as follows y wt i wt yi substituting in and dropping the constraints we get the following equivalent problem empirical risk minimization let us pause and consider whether the above objective is reasonable. recall that in the frequentist approach to machine learning the goal is to minimize the regularized empirical risk defined by rw c n lyi f w where rw is the regularizer and f w argmaxy wt y yi is the prediction. since this objective is hard to optimize because the loss is not differentiable we will construct a convex upper bound instead. i we can show that rw c n max y y wt i is such a convex upper bound. to see this note that lyi f w lyi f w wt yi t yi max using this bound and rw lyi y wt yi t y yields equation y computational issues although the above objectives are simple quadratic programs they have ony constraints. this is intractable since y is usually exponentially large. in the case of the margin rescaling formulation it is possible to reduce the exponential number of constraints to a polynomial number provided the loss function and the feature vector decompose according to a graphical model. this is the approach used in et al. an alternative approach is to work directly with the exponentially sized qp. this allows for the use of more general loss functions. there are several possible methods to make this feasible. one is to use cutting plane methods. another is to use stochastic subgradient methods. we discuss both of these below. chapter undirected graphical models random fields fi l l c i illustration of the cutting plane algorithm in we start with the estimate w figure we add the first constraint the shaded region is the new feasible set. the new minimum norm solution is we add a third constraint. source figure of et al. used with kind permission of yasemin altun. we add another constraint the dark shaded region is the new feasible set. f h h s l h l cutting plane methods for fitting ssvms in this section we discuss an efficient algorithm for fitting ssvms due to et al. this method can handle general loss functions and is implemented in the popular svmstruct the method is based on the cutting plane method from convex optimization the basic idea is as follows. we start with an initial guess w and no constraints. at each iteration we then do the following for each example i we find the most violated constraint involving xi and yi. if the loss-augmented margin violation exceeds the current value of i by more than we add yi to the working set of constraints for this training case wi and then solve the resulting new qp to find the new w see figure for a sketch and algorithm for the pseudo code. at each step we only add one new constraint we can warm-start the qp solver. we can can easily modify the algorithm to optimize the slack rescaling version by replacing the expression lyi y wt i yi with lyi wt i yi. the key to the efficiency of this method is that only polynomially many constraints need to be added and as soon as they are the exponential number of other constraints are guaranteed to also be satisfied to within a tolerance of et al. for the proof. loss-augmented decoding the other key to efficiency is the ability to find the most violated constraint in line of the algorithm i.e. to compute argmax y y lyi y wt iy argmax y y lyi y wt y httpsvmlight.joachims.orgsvm_struct.html structural svms algorithm cutting plane algorithm for ssvms rescaling n version input d yn c wi i for i n repeat for i n do yi argmax yi y lyi y wt i yi if lyi y wt i yi i then wi wi yi argminw c i s.t. i wi wt i yi lyi i until no wi has changed return we call this process loss-augmented decoding. et al. this procedure is called the separation oracle. if the loss function has an additive decomposition of the same form as the features then we can fold the loss into the weight vector i.e. we can find a new set of parameters iy wt iy. we can then use a standard decoding algorithm such as viterbi on the model pyx with a value of of wt i y or it will be the second best solution i.e. in the special case of loss the optimum will either be the best solution argmaxy wt y such that wt y y argmax y which achieves an overall value of wt i y. for chain structured crfs we can use the viterbi algorithm to do decoding the second best path will differ from the best path in a single position which can be obtained by changing the variable whose max marginal is closest to its decision boundary to its second best value. we can generalize this a bit more work to find the n list and chow nilsson and goldberger t yt and for the score in section we can devise a dynamic programming algorithm to compute equation see et al. for details. other models and loss function combinations will require different methods. for hamming loss ly y t iy a linear time algorithm although the above algorithm takes polynomial time we can do better and devise an algorithm that runs in linear time assuming we use a linear kernel we work with the original features y and do not apply the kernel trick. the basic idea as explained in et al. is to have a single slack variable instead of n but to use constraints instead of chapter undirected graphical models random fields just ny. specifically we optimize the following the margin rescaling formulation min w c s.t. yn y n n wt iyi n lyi yi compare this to the original version which was min w c n s.t. i y y wt iy lyi yi i one can show that any solution w vice versa with n i of equation is also a solution of equation and argminw algorithm cutting plane algorithm for ssvms rescaling version input d yn c w repeat c s.t. yn w for i n do w w yn i yi lyi yi n wt iyi lyi yi yi argmax yi y lyi yi t yi n wt n until return n we can optimize equation using the cutting plane algorithm in algorithm is what is implemented in svmstruct. the inner qp in line can be solved in on time using the method of in line we make n calls to the loss-augmented decoder. finally it can be shown that the number of iterations is a constant independent on n thus the overall running time is linear. online algorithms for fitting ssvms although the cutting plane algorithm can be made to run in time linear in the number of data points that can still be slow if we have a large dataset. in such cases it is preferable to use online learning. we briefly mention a few possible algorithms below. the structured perceptron algorithm a very simple algorithm for fitting ssvms is the structured perceptron algorithm this method is an extension of the regular perceptron algorithm of section at each structural svms step we compute y argmax pyx using the viterbi algorithm for the current training sample x. if y y we do nothing otherwise we update the weight vector using wk x y x to get good performance it is necessary to average the parameters over the last few updates section for details rather than using the most recent value. stochastic subgradient descent the disadvantage of the structured perceptron algorithm is that it implicitly assumes loss and it does not enforce any kind of margin. an alternative approach is to perform stochastic subgradient descent. a specific instance of this the pegasos algorithm et al. which stands for primal estimated sub-gradient solver for svm pegasos was designed for binary svms but can be extended to ssvms. let us start by considering the objective function lyi yi t yi wt yi f max yi letting yi be the argmax of this max. then the subgradient of this objective function is gw yi yi w in stochastic subgradient descent we approximate this gradient with a single term i and then perform an update wk kgiwk wk k yi yi w where k is the step size parameter which should satisfy the robbins-monro conditions that the perceptron algorithm is just a special case where and tion k to ensure that w has unit norm we can project it onto the ball after each update. latent structural svms in many applications of interest we have latent or hidden variables h. for example in object detections problems we may be told that the image contains an object so y but we may not know where it is. the location of the object or its pose can be considered a hidden variable. or in machine translation we may know the source text x english and the target text y french but we typically do not know the alignment between the words. we will extend our model as follows to get a latent crf py hx w expwt y h zx w zx w expwt y h yh chapter undirected graphical models random fields in addition we introduce the loss function ly y h this measures the loss when the action that we take is to predict y using latent variables h. we could just use ly y as before since h is usually a nuisance variable and not of direct interest. however h can sometimes play a useful role in defining a loss given the loss function we define our objective as relw log pw exp lyi y h expwt y h zx w using the same loose lower bound as before we get relw ew lyi y h t y h i yh log max yh wt yi h max h if we set ew and joachims we get the same objective as is optimized in latent svms unfortunately this objective is no longer convex. however it is a difference of convex functions and hence can be solved efficiently using the cccp or concave-convex procedure and rangarajan this is a method for minimizing functions of the form f gw where f and g are convex. the method alternates between finding a linear upper bound u on g and then minimizing the convex function f w see algorithm for the pseudocode. cccp is guaranteed to decrease the objective at every iteration and to converge to a local minimum or a saddle point. algorithm concave-convex procedure set t and initialize repeat find hyperplane vt such that gw gwt wtt vt for all w solve argminw f t vt set t t until converged when applied to latent ssvms cccp is very similar to em. in the e step we compute for example consider the problem of learning to classify a set of documents as relevant or not to a query. that is given n documents n for a single query q we want to produce a labeling yj representing whether document j is relevant to q or not. suppose our goal is to maximize the precision at k which is a metric widely used in ranking section we will introduce a latent variable for each document hj representing its degree of relevance. this corresponds to a latent total ordering that has to be consistent with the observed partial ordering y. given this we can define the following loss function ly y h ny iyhj where ny is the total number of relevant documents. this loss is essentially just minus the precisionk except we replace with nyk so that the loss will have a minimum of zero. see and joachims for details. k k structural svms the linear upper bound by setting vt c hi argmax h wt t yi h yi h i where in the m step we estimate w using techniques for solving fully visible ssvms. specifically we minimize lyi y h t y h wt yi h i max yh c exercises c exercise derivative of the log partition function derive equation exercise ci properties of gaussian graphical models jordan. in this question we study the relationship between sparse matrices and sparse graphs for gaussian graphical models. consider a multivariate gaussian n in dimensions. suppose throughout. recall that for jointly gaussian random variables we know that xi and xj are independent iff they are uncorrelated ie. ij is not true in general or even if xi and xj are gaussian but not jointly gaussian. also xi is conditionally independent of xj given all the other variables iff ij a. suppose are there any marginal independencies amongst and what about conditional indepen which pairwise terms xixj are missing? draw dencies? hint compute an undirected graphical model that captures as many of these independence statements and conditional as possible but does not make any false independence assertions. and expand out xt b. suppose are there any marginal independencies amongst and are there any conditional independencies amongst and draw an undirected graphical model that captures as many of these independence statements and conditional as possible but does not make any false independence assertions. c. now suppose the distribution on x can be represented by the following dag let the cpds be as follows p n p n p n multiply these cpds together and complete the square to find the corresponding joint distribution n may find it easier to solve for rather than chapter undirected graphical models random fields d. for the dag model in the previous question are there any marginal independencies amongst and what about conditional independencies? draw an undirected graphical model that captures as many of these independence statements as possible but does not make any false independence assertions marginal or conditional. exercise independencies in gaussian graphical models mackay. a. consider the dag assume that all the cpds are linear-gaussian. which of the following matrices could be the covariance matrix? d b c a b. which of the above matrices could be inverse covariance matrix? c. consider the dag assume that all the cpds are linear-gaussian. which of the above matrices could be the covariance matrix? d. which of the above matrices could be the inverse covariance matrix? e. let three variables have covariance matrix and precision matrix as follows now focus on and which of the following statements about their covariance matrix and precision matrix are true? a b exercise cost of training mrfs and crfs koller. consider the process of gradient-ascent training for a log-linear model with k features given a data set with n training instances. assume for simplicity that the cost of computing a single feature over a single instance in our data set is constant as is the cost of computing the expected value of each feature once we compute a marginal over the variables in its scope. assume that it takes c time to compute all the marginals for each data case. also assume that we need r iterations for the gradient process to converge. using this notation what is the time required to train an mrf in big-o notation? using this notation what is the time required to train a crf in big-o notation? exercise full conditional in an ising model consider an ising model xn expjijxixj z exphixi where ij denotes all unique pairs all edges jij r is the coupling strength on edge i j hi r is the local evidence term and h are all the parameters. structural svms if xi derive an expression for the full conditional pxi i pxi where x i are all nodes except i and nbi are the neighbors of i in the graph. hint you answer should use the sigmoid logistic function e z. now suppose xi derive a related expression for pxix i in this case. result can be used when applying gibbs sampling to the model. exact inference for graphical models introduction in section we discussed the forwards-backwards algorithm which can exactly compute the posterior marginals pxtv in any chain-structured graphical model where x are the hidden variables discrete and v are the visible variables. this algorithm can be modified to compute the posterior mode and posterior samples. a similar algorithm for linear-gaussian chains known as the kalman smoother was discussed in section our goal in this chapter is to generalize these exact inference algorithms to arbitrary graphs. the resulting methods apply to both directed and undirected graphical models. we will describe a variety of algorithms but we omit their derivations for brevity. see e.g. koller and friedman for a detailed exposition of exact inference techniques for discrete directed graphical models. belief propagation for trees in this section we generalize the forwards-backwards algorithm from chains to trees. the resulting algorithm is known as belief propagation or the sum-product algorithm. serial protocol zv s v e we initially assume notational simplicity that the model is a pairwise mrf crf i.e. pxv sxs stxs xt where s is the local evidence for node s and st is the potential for edge s t. we will consider the case of models with higher order cliques as directed trees later on. one way to implement bp for undirected trees is as follows. pick an arbitrary node and call it the root r. now orient all edges away from r we can imagine picking up the graph at node r and letting all the edges dangle down. this gives us a well-defined notion of parent and child. now we send messages up from the leaves to the root collect evidence phase and then back down from the root distribute evidence phase in a manner analogous to forwards-backwards on chains. chapter exact inference for graphical models root v st t v st t root s u s u figure message passing on a tree. collect-to-root phase. distribute-from-root phase. to explain the process in more detail consider the example in figure suppose we want to compute the belief state at node t. we will initially condition the belief only on evidence that is at or below t in the graph i.e. we want to compute bel t we will call this a bottom-up belief state suppose by induction that we have computed messages from t s two children summarizing what they think t should know about the evidence in their subtrees i.e. we have computed m st is all the evidence on the downstream side of the s t edge figure and similarly we have computed mu txt. then we can compute the bottom-up belief state at t as follows s txt xtv st where v t pxtv t pxtv t bel zt txt c cht m c txt where txt pxtvt is the local evidence for node t and zt is the local normalization in words we multiply all the incoming messages from our children as well as the constant. incoming message from our local evidence and then normalize. how do we compute the messages themselves? consider computing m of t s children. assume by recursion that we have computed bel can compute the message as follows s we have explained how to compute the bottom-up belief states from the bottom-up messages. s txt where s is one s then we s pxsv stxs xtbel s txt m xs essentially we convert beliefs about xs into beliefs about xt by using the edge potential st. we continue in this way up the tree until we reach the root. once at the root we have seen all the evidence in the tree so we can compute our local belief state at the root using belrxr pxrv pxtv r rxr m c rxr c chr this completes the end of the upwards pass which is analogous to the forwards pass in an hmm. as a side effect we can compute the probability of the evidence by collecting the belief propagation for trees normalization constants pv zt t we can now pass messages down from the root. for example consider node s with parent t as shown in figure to compute the belief state for s we need to combine the bottom-up belief for s together with a top-down message from t which summarizes all the information in the rest of the graph m st is all the evidence on the upstream side of the s t edge as shown in figure we then have st where v t sxs pxtv belsxs pxsv bel s m t sxt t pas how do we compute these downward messages? for example consider the message from t to s. suppose t s parent is r and t s children are s and u as shown in figure we want to include in m t s all the information that t has received except for the information that s sent it xt xt t sxs pxsv m st stxs xt beltxt m s txt rather than dividing out the message sent up to t we can plug in the equation of belt to get m t sxs stxs xt txt m p txt m c txt c p pat in other words we multiply together all the messages coming into t from all nodes except for the recipient s combine together and then pass through the edge potential st. in the case of a chain t only has one child s and one parent p so the above simplifies to m t sxs stxs xt txtm p txt xt the version of bp in which we use division is called belief updating and the version in which we multiply all-but-one of the messages is called sum-product. the belief updating version is analogous to how we formulated the kalman smoother in section the topdown messages depend on the bottom-up messages. this means they can be interpreted as conditional posterior probabilities. the sum-product version is analogous to how we formulated the backwards algorithm in section the top-down messages are completely independent of the bottom-up messages which means they can only be interpreted as conditional likelihoods. see section for a more detailed discussion of this subtle difference. parallel protocol so far we have presented a serial version of the algorithm in which we send messages up to the root and back. this is the optimal approach for a tree and is a natural extension of forwards-backwards on chains. however as a prelude to handling general graphs with loops we now consider a parallel version of bp. this gives equivalent results to the serial version but is less efficient when implemented on a serial machine. chapter exact inference for graphical models the basic idea is that all nodes receive messages from their neighbors in parallel they then updates their belief states and finally they send new messages back out to their neighbors. this process repeats until convergence. this kind of computing architecture is called a systolic array due to its resemblance to a beating heart. more precisely we initialize all messages to the all s vector. then in parallel each node absorbs messages from all its neighbors using t nbrs mt sxs sxs stxs xt belsxs sxs ms txt xs then in parallel each node sends messages to each of its neighbors mu sxs u nbrst the ms t message is computed by multiplying together all incoming messages except the one sent by the recipient and then passing through the st potential. at iteration t of the algorithm belsxs represents the posterior belief of xs conditioned on the evidence that is t steps away in the graph. after dg steps where dg is the diameter of the graph largest distance between any two pairs of nodes every node has obtained information from all the other nodes. its local belief state is then the correct posterior marginal. since the diameter of a tree is at most the algorithm converges in a linear number of steps. we can actually derive the up-down version of the algorithm by imposing the condition that a node can only send a message once it has received messages from all its other neighbors. this means we must start with the leaf nodes which only have one neighbor. the messages then propagate up to the root and back. we can also update the nodes in a random order. the only requirement is that each node get updated dg times. this is just enough time for information to spread throughout the whole tree. similar parallel distributed algorithms for solving linear systems of equations are discussed in in particular the gauss-seidel algorithm is analogous to the serial up-down version of bp and the jacobi algorithm is analogous to the parallel version of bp. gaussian bp now consider the case where pxv is jointly gaussian so it can be represented as a gaussian pairwise mrf as in section we now present the belief propagation algorithm for this class of models follow the presentation of also et al. we will assume the following node and edge potentials txt exp stxs xt exp t btxt xsastxt so the overall model has the form pxv exp xt ax bt x belief propagation for trees this is the information form of the mvn exercise where a is the precision matrix. note that by completing the square the local evidence can be rewritten as a gaussian tt n t txt n a below we describe how to use bp to compute the posterior node marginals pxtv n t t if the graph is a tree the method is exact. if the graph is loopy the posterior means may still be exact but the posterior variances are often too small and freeman although the precision matrix a is often sparse computing the posterior mean requires inverting it since a bp provides a way to exploit graph structure to perform this computation in od time instead of this is related to various methods from linear algebra as discussed in since the model is jointly gaussian all marginals and all messages will be gaussian. the key operations we need are to multiply together two gaussian factors and to marginalize out a variable from a joint gaussian factor. for multiplication we can use the fact that the product of two gaussians is gaussian n n n which follows from the normalization constant of a gaussian we now have all the pieces we need. in particular let the message ms txt be a gaussian with mean st and precision st. from equation the belief at node s is given by the product of incoming messages times the local evidence and hence belsxs sxs mtsxs n s s t nbrs t nbrs s ts s s ts ts t nbrs where c exp see exercise for the proof. for marginalization we have the following result exp bxdx chapter exact inference for graphical models to compute the messages themselves we use equation which is given by stxs xt sxs stxs xtfstxsdxs xs xs mu sxs u nbrst dxs ms txt where fstxs is the product of the local evidence and all incoming messages excluding the message from t fstxs sxs mu sxs u nbrst n st st st u nbrst us us us u nbrst returning to equation we have st st xs ms txt xs exp n st st st st ast st st st st stxsxt exp xsastxt exp st st astxtxs exp st st st fstxs dxs dxs const one can generalize these equations to the case where each node is a vector and the messages become small mvns instead of scalar gaussians and agogino if we apply the resulting algorithm to a linear dynamical system we recover the kalman smoothing algorithm of section to perform message passing in models with non-gaussian potentials one can use sampling methods to approximate the relevant integrals. this is called non-parametric bp et al. isard sudderth et al. other bp variants in this section we briefly discuss several variants of the main algorithm. belief propagation for trees ft t xt vt figure illustration of how to compute the two-slice distribution for an hmm. the t and terms are the local evidence messages from the visible nodes vt to the hidde nodes xt respectively ft is the forwards message from xt and is the backwards message from max-product algorithm it is possible to devise a max-product version of the bp algorithm by replacing the operator with the max operator. we can then compute the local map marginal of each node. however if there are ties this might not be globally consistent as discussed in section fortunately we can generalize the viterbi algorithm to trees where we use max and argmax in the collectto-root phase and perform traceback in the distribute-from-root phase. see for details. sampling from a tree it is possible to draw samples from a tree structured model by generalizing the forwards filtering backwards sampling algorithm discussed in section see for details. computing posteriors on sets of variables in section we explained how to compute the two-slice distribution j pxt i jv in an hmm namely by using j ti j since ti tifti where ft is the forwards message we can think of this as sending messages ft and t into xt and into and then combining them with the matrix as shown in figure this is like treating xt and as a single mega node and then multiplying all the incoming messages as well as all the local factors chapter exact inference for graphical models coherence coherence difficulty intelligence difficulty intelligence grade sat grade sat letter job letter job happy happy figure left the student dgm. right the equivalent ugm. we add moralization arcs d-i g-j and l-s. based on figure of and friedman the variable elimination algorithm we have seen how to use bp to compute exact marginals on chains and trees. in this section we discuss an algorithm to compute pxqxv for any kind of graph. we will explain the algorithm by example. consider the dgm in figure this model from and friedman is a hypothetical model relating various variables pertaining to a typical student. the corresponding joint has the following form p d i g s l j h p dp sp j note that the forms of the cpds do not matter since all our calculations will be symbolic. however for illustration purposes we will assume all variables are binary. before proceeding we convert our model to undirected form. this is not required but it makes for a more unified presentation since the resulting method can then be applied to both dgms and ugms as we will see in section to a variety of other problems that have nothing to do with graphical models. since the computational complexity of inference in dgms and ugms is generally speaking the same nothing is lost in this transformation from a computational point of to convert the dgm to a ugm we simply define a potential or factor for every cpd yielding pc d i g s l j h cc dd c i gg i d ss i ll g j l s h g there are a few tricks one can exploit in the directed case that cannot easily be exploited in the undirected case. one important example is barren node removal. to explain this consider a naive bayes classifier as in figure suppose we want to infer y and we observe and but not and it is clear that we can safely remove and since in general once we have removed hidden leaves we can apply this process recursively. since potential functions do not necessary sum to one we cannot use this trick in the undirected case. see and friedman for a variety of other speedup tricks. and similarly for the variable elimination algorithm since all the potentials are locally normalized since they are cpds there is no need for a global normalization constant so z the corresponding undirected graph is shown in figure note that it has more edges than the dag. in particular any unmarried nodes that share a child must get married by adding an edge between them this process is known as moralization. only then can the arrows be dropped. in this example we added d-i g-j and l-s moralization arcs. the reason this operation is required is to ensure that the ci properties of the ugm match those of the dgm as explained in section it also ensures there is a clique that can store the cpds of each family. now suppose we want to compute pj the marginal probability that a person will get a job. since we have binary variables we could simply enumerate over all possible assignments to all the variables for j adding up the probability of each joint instantiation pj pc d i g s l j h l s g h i d c however this would take time. we can be smarter by pushing sums inside products. this is the key idea behind the variable elimination algorithm and poole also called bucket elimination or in the context of genetic pedigree trees the peeling algorithm et al. in our example we get we now evaluate this expression working right to left as shown in table first we multiply together all the terms in the scope of the c operator to create the temporary factor then we marginalize out c to get the new factor d cc dd c c d next we multiply together all the terms in the scope of the out to create i d gg i d i i d d d operator and then marginalize pj pc d i g s l j h lsghidc cc dd c i gg i d ss i ll g lsghidc j l s h g j j l s ls g d c gg i d cc dd c ll g h g j ss i i h i chapter exact inference for graphical models l s j l s g ll g h h g j s i i d gg i d i h g j i s i i l s j l s g ll g l s j l s ll g h h g j h g l s j l s g ll g l s j l s c d c dd c gg i d s i i i h g j s i h ll g j s j l s l s l l g l s table eliminating variables from figure in the order c d i h g s l to compute p next we multiply together all the terms in the scope of the out to create i s ss i i i s i s i i operator and then marginalize and so on. the above technique can be used to compute any marginal of interest such as pj or pj h. to compute a conditional we can take a ratio of two marginals where the visible variables have been clamped to their known values hence don t need to be summed over. for example pj ji h in general we can write pxqxv pxq xv pxv pj j i h pj i h pxh xq xv pxh q xv xh xh q the variable elimination algorithm the normalization constant in the denominator pxv is called the probability of the evidence. see variableelimination for a simple matlab implementation of this algorithm which works for arbitrary graphs and arbitrary discrete factors. but before you go too crazy please read section which points out that ve can be exponentially slow in the worst case. the generalized distributive law pxqxv abstractly ve can be thought of as computing the following expression cxc x c it is understood that the visible variables xv are clamped and not summed over. ve uses non-serial dynamic programming and brioschi caching intermediate results to avoid redundant computation. however there are other tasks we might like to solve for any given graphical model. for example we might want the map estimate x argmax x c cxc fortunately essentially the same algorithm can also be used to solve this task we just replace sum with max. also need a traceback step which actually recovers the argmax as opposed to just the value of max these details are explained in section two binary operations called and which satisfy the following three axioms the operation is associative and commutative and there is an additive identity element in general ve can be applied to any commutative semi-ring. this is a set k together with called such that k k for all k k. the operation is associative and commutative and there is a multiplicative identity element called such that k for all k k. the distributive law holds i.e. b c a c for all triples b c from k. this framework covers an extremely wide range of important applications including constraint satisfaction problems et al. dechter the fast fourier transform and mceliece etc. see table for some examples. computational complexity of ve the running time of ve is clearly exponential in the size of the largest factor since we have sum over all of the corresponding variables. some of the factors come from the original model are thus unavoidable but new factors are created in the process of summing out. for example chapter exact inference for graphical models domain f f name sum-product max-product min-sum t boolean satisfiability dd c table some commutative semirings. j l s i s i d c h l s i dd c d c h l s j l s d c dd c h l g i gg i d ll h g j i s i d l j h j l s l s j h s dd c d c h d c dd c l l j h j h h dd c j j d d c table eliminating variables from figure in the order g i s l h c d. in equation we created a factor involving g i and s but these nodes were not originally present together in any factor. the order in which we perform the summation is known as the elimination order. this can have a large impact on the size of the intermediate factors that are created. for example consider the ordering in table the largest created factor the original ones in the model has size corresponding to l s. now consider the ordering in table now the largest factors are d l j h and l s j h which are much bigger. we can determine the size of the largest factor graphically without worrying about the actual numerical values of the factors. when we eliminate a variable xt we connect it to all variables the variable elimination algorithm coherence coherence coherence difficulty intelligence difficulty intelligence difficulty intelligence grade sat grade sat letter job letter job happy happy grade sat letter job happy figure example of the elimination process in the order c d i etc. when we eliminate i c we add a fill-in edge between g and s since they are not connected. based on figure of and friedman that share a factor with xt reflect the new temporary factor t. the edges created by this process are called fill-in edges. for example figure shows the fill-in edges introduced when we eliminate in the order c d i the first two steps do not introduce any fill-ins but when we eliminate i we connect g and s since they co-occur in equation let g be the graph induced by applying variable elimination to g using elimination ordering the temporary factors generated by ve correspond to maximal cliques in the graph g for example with ordering d i h g s l the maximal cliques are as follows dd i gg l s jg j hg i s it is clear that the time complexity of ve is kc c cg where c are the cliques that are created is the size of the clique c and we assume for notational simplicity that all the variables have k states each. let us define the induced width of a graph given elimination ordering denoted w as the size of the largest factor the largest clique in the induced graph minus then it is easy to see that the complexity of ve with ordering is okw obviously we would like to minimize the running time and hence the induced width. let us define the treewidth of a graph as the minimal induced width. w min max c g then clearly the best possible running time for ve is odk unfortunately one can show that for arbitrary graphs finding an elimination ordering that minimizes w is np-hard et al. in practice greedy search techniques are used to find reasonable orderings although people have tried other heuristic methods for discrete optimization chapter exact inference for graphical models such as genetic algorithms et al. algorithms with provable performance guarantees it is also possible to derive approximate in some cases the optimal elimination ordering is clear. for example for chains we should work forwards or backwards in time. for trees we should work from the leaves to the root. these orderings do not introduce any fill-in edges so w consequently inference in chains and trees takes ov k time. this is one reason why markov chains and markov trees are so widely used. unfortunately for other graphs the treewidth is large. for example for an m n lattice the treewidth is ominm n and tarjan so ve on a ising model would take time. of course just because ve is slow doesn t mean that there isn t some smarter algorithm out there. we discuss this issue in section a weakness of ve the main disadvantage of the variable elimination algorithm from its exponential dependence on treewidth is that it is inefficient if we want to compute multiple queries conditioned on the same evidence. for example consider computing all the marginals in a chain-structured graphical model such as an hmm. we can easily compute the final marginal pxtv by eliminating all the nodes to xt in order. this is equivalent to the forwards algorithm and takes ok time. but now suppose we want to compute pxt we have to run ve again at a cost of ok time. so the total cost to compute all the marginals is ok however we know that we can solve this problem in ok using forwards-backwards. the difference is that fb caches the messages computed on the forwards pass so it can reuse them later. the same argument holds for bp on trees. for example consider the tree in figure we can compute by eliminating this is equivalent to sending messages up to messages correspond to the factors created by ve. similarly we can compute and then we see that some of the messages used to compute the marginal on one node can be re-used to compute the marginals on the other nodes. by storing the messages for later re-use we can compute all the marginals in odk time. this is what the up-down algorithm on trees does. the question is how can we combine the efficiency of bp on trees with the generality of ve? the answer is given in section the junction tree algorithm the junction tree algorithm or jta generalizes bp from trees to arbitrary graphs. we sketch the basic idea below for details see e.g. and friedman creating a junction tree the basic idea behind the jta is this. we first run the ve algorithm symbolically adding fill-in edges as we go according to a given elimination ordering. the resulting graph will be a chordal graph which means that every undirected cycle x k of length k has a the junction tree algorithm figure of the messages needed to compute all singleton marginals. based on figure of sending multiple messages along a tree. is root. is root. is root. all figure left this graph is not triangulated despite appearances since it contains a chordless right one possible triangulation by adding the and fill-in edges. based on chapter exact inference for graphical models chord i.e. an edge connects xi xj for all non-adjacent nodes ij in the having created a chordal graph we can extract its maximal cliques. in general finding max cliques is computationally hard but it turns out that it can be done efficiently from this special kind of graph. figure gives an example where the max cliques are as follows dg i dg s ig j s lh g j note that if the original graphical model was already chordal the elimination process would not add any extra fill-in edges the optimal elimination ordering was used. we call such models decomposable since they break into little pieces defined by the cliques. it turns out that the cliques of a chordal graph can be arranged into a special kind of tree known as a junction tree. this enjoys the running intersection property which means that any subset of nodes containing a given variable forms a connected component. figure gives an example of such a tree. we see that the node i occurs in two adjacent tree nodes so they can share information about this variable. a similar situation holds for all the other variables. one can show that if a tree that satisfies the running intersection property then applying bp to this tree we explain below will return the exact values of pxcv for each node c in the tree clique in the induced graph. from this we can easily extract the node and edge marginals pxtv and pxs xtv from the original model by marginalizing the clique message passing on a junction tree having constructed a junction tree we can use it for inference. the process is very similar to belief propagation on a tree. as in section there are two versions the sum-product form also known as the shafer-shenoy algorithm named after and shenoy and the belief updating form involves division also known as the hugin after a company or the lauritzen-spiegelhalter algorithm after and spiegelhalter see and shenoy for a detailed comparison of these methods. below we sketch how the hugin algorithm works. we assume the original model has the following form cxc px c cg where cg are the cliques of the original graph. on the other hand the tree defines a distribution of the following form z px c ct cxc s st sxs the largest loop in a chordal graph is length consequently chordal graphs are sometimes called triangulated. however it is not enough for the graph to look like it is made of little triangles. for example figure is not chordal even though it is made of little triangles since it contains the chordless if we want the joint distribution of some variables that are not in the same clique a so-called out-of-clique query we can adapt the technique described in section as follows create a mega node containing the query variables and any other nuisance variables that lie on the path between them multiply in messages onto the boundary of the mega node and then marginalize out the internal nuisance variables. this internal marginalization may require the use of bp or ve. see and friedman for details. the junction tree algorithm coherence coherence difficulty intelligence difficulty intelligence grade sat grade sat letter letter job job happy happy cd gid gsi gjsl hgj d gi gs gj the student graph with fill-in edges added. figure the junction tree. an edge between nodes s and t is labeled by the intersection of the sets on nodes s and t this is called the separating set. from figure of and friedman used with kind permission of daphne koller. the maximal cliques. where ct are the nodes of the junction tree are the cliques of the chordal graph and st are the separators of the tree. to make these equal we initialize by defining s for all separators and c for all cliques. then for each clique in the original model c cg we find a clique in the tree ct which contains it c. we then multiply c onto by computing c. after doing this for all the cliques in the original graph we have cxc cxc c ct c cg as in section we now send messages from the leaves to the root and back as sketched in figure in the upwards pass also known as the collect-to-root phase node i sends to its parent j the following message mi jsij ici cisij j chi that is we marginalize out the variables that node i knows about which are irrelevant to j and then we send what is left over. once a node has received messages from all its children it updates its belief state using ici ici mj isij chapter exact inference for graphical models which is the posterior over the nodes in clique at the root rcr represents pxcr cr conditioned on all the evidence. its normalization constant is where is the normalization constant for the unconditional prior px. have if the original model was a dgm. in the downwards pass also known as the distribute-from-root phase node i sends to its children j the following message ici mi jsij cisij mj isij we divide out by what j sent to i to avoid double counting the evidence. this requires that we store the messages from the upwards pass. once a node has received a top-down message from its parent it can compute its final belief state using jcj jcjmi jsij an equivalent way to present this algorithm is based on storing the messages inside the separator potentials. so on the way up sending from i to j we compute the separator potential and then update the recipient potential ijsij ici cisij j jcj ij cisij ijsij ijsij i and then update the recipient potential j j ij ijsij that we initialize ijsij this is sometimes called passing a flow from i to j. on the way down from i to j we compute the separator potential this process is known as junction tree calibration. see figure for an illustration. its correctness follows from the fact that each edge partitions the evidence into two distinct groups plus the fact that the tree satisfies rip which ensures that no information is lost by only performing local computations. example jtree algorithm on a chain it is interesting to see what happens if we apply this process to a chain structured graph such as an hmm. a detailed discussion can be found in et al. but the basic idea is this. the cliques are the edges and the separators are the nodes as shown in figure we initialize the potentials as follows we set s for all the separators we set cxt xt pxtxt for clique c xt and we set cxt yt pytxt for clique c yt. the junction tree algorithm figure the junction tree derived from an hmmssm of length t next we send messages from left to right. consider clique xt with potential pxtxt it receives a message from clique xt via separator xt of the form pxt xt xt when combined with the clique potential xt this becomes the two-slice predictive density pxtxt pxt the clique xt also receives a message from yt via separator xt of the form pytxt which corresponds to its local evidence. when combined with the updated clique potential this becomes the two-slice filtered posterior pxt pxt thus the messages in the forwards pass are the filtered belief states t and the clique potentials are the two-slice distributions. in the backwards pass the messages are the update factors t t where tk xt and tk xt by multiplying by this message we swap out the old t message and swap in the new t message. we see that the backwards pass involves working with posterior beliefs not conditional likelihoods. see section for further discussion of this difference. computational complexity of jta if all nodes are discrete with k states each it is clear that the jta takes ock time and space where is the number of cliques and w is the treewidth of the graph i.e. the size of the largest clique minus unfortunately choosing a triangulation so as to minimize the treewidth is np-hard as explained in section the jta can be modified to handle the case of gaussian graphical models. the graph-theoretic steps remain unchanged. only the message computation differs. we just need to define how to multiply divide and marginalize gaussian potential functions. this is most easily done in information form. see e.g. murphy cemgil for the details. the algorithm takes time and space. when applied to a chain structured graph the algorithm is equivalent to the kalman smoother in section chapter exact inference for graphical models qn cm am cm x figure encoding a problem on n variables and m clauses as a dgm. the qs variables are binary random variables. the ct variables are deterministic functions of the qs s and compute the truth value of each clause. the at nodes are a chain of and gates to ensure that the cpt for the final x node has bounded size. the double rings denote nodes with deterministic cpds. source figure of and friedman used with kind permission of daphne koller. jta generalizations we have seen how to use the jta algorithm to compute posterior marginals in a graphical model. there are several possible generalizations of this algorithm some of which we mention below. all of these exploit graph decomposition in some form or other. they only differ in terms of how they define compute messages and beliefs the key requirement is that the operators which compute messages form a commutative semiring section computing the map estimate. we just replace the sum-product with max-product in the collect phase and use traceback in the distribute phase as in the viterbi algorithm see for details. computing the n-most probable configurations computing posterior samples. the collect pass is the same as usual but in the distribute pass we sample variables given the values higher up in the tree thus generalizing forwardsfiltering backwards-sampling for hmms described in section see for details. solving constraint satisfaction problems solving logical reasoning problems and mcilraith computational intractability of exact inference in the worst case as we saw in sections and ve and jta take time that is exponential in the treewidth of a graph. since the treewidth can be onumber of nodes in the worst case this means these algorithms can be exponential in the problem size. of course just because ve and jta are slow doesn t mean that there isn t some smarter algorithm out there. unfortunately this seems unlikely since it is easy to show that exact inference is np-hard and luby the proof is a simple reduction from the satisfiability prob computational intractability of exact inference in the worst case method forwards-backwards belief propagation variable elimination junction tree algorithm loopy belief propagation convex belief propagation mean field gibbs sampling restriction chains d or lg trees d or lg low treewidth d or lg single query low treewidth d or lg approximate d or lg approximate d or lg approximate c-e approximate section section section section section section section section section table summary of some methods that can be used for inference in graphical models. d means that all the hidden variables must be discrete. l-g means that all the factors must be linear-gaussian. the term single query refers to the restriction that ve only computes one marginal pxqxv at a time. see section for a discussion of this point. c-e stands for conjugate exponential this means that variational mean field only applies to models where the likelihood is in the exponential family and the prior is conjugate. this includes the d and lg case but many others as well as we will see in section in particular note that we can encode any as a dgm with deterministic lem. links as shown in figure we clamp the final node x to be on and we arrange the cpts so that px iff there a satisfying assignment. computing any posterior marginal requires evaluating the normalization constant px which represents the probability of the evidence so inference in this model implicitly solves the sat problem. in fact exact inference is which is even harder than np-hard. e.g. and barak for definitions of these terms. the intuitive reason for this is that to compute the normalizing constant z we have tocount how many satisfying assignments there are. by contrast map estimation is provably easier for some model classes et al. since intuitively speaking it only requires finding one satisfying assignment not counting all of them. approximate inference many popular probabilistic models support efficient exact inference since they are based on chains trees or low treewidth graphs. but there are many other models for which exact in fact even simple two node models of the form x may not inference is intractable. support exact inference if the prior on is not conjugate to the likelihood px therefore we will need to turn to approximate inference methods. see table for a summary of coming attractions. for the most part these methods do not come with any guarantee as to their accuracy or running time. theoretical computer scientists would therefore describe them as heuristics rather than approximation algorithms. in fact one can prove that a problem is a logical expression of the form where the qi are binary variables and each clause consists of the conjunction of three variables their negation. the goal is to find a satisfying assignment which is a set of values for the qi variables such that the expression evaluates to true. for discrete random variables conjugacy is not a concern since discrete distributions are always closed under conditioning and marginalization. consequently graph-theoretic considerations are of more importance when discussing inference in models with discrete hidden states. chapter exact inference for graphical models it is not possible to construct polynomial time approximation schemes for inference in general discrete gms and luby roth fortunately we will see that for many of these heuristic methods often perform well in practice. exercises exercise variable elimination consider the mrf in figure a. suppose we want to compute the partition function using the elimination ordering if we use the variable elimination algorithm we will create new intermediate factors. what is the largest intermediate factor? b. add an edge to the original mrf between every pair of variables that end up in the same factor. are called fill in edges. draw the resulting mrf. what is the size of the largest maximal clique in this graph? c. now consider elimination ordering i.e. if we use the variable elimination algorithm we will create new intermediate factors. what is the largest intermediate factor? d. add an edge to the original mrf between every pair of variables that end up in the same factor. are called fill in edges. draw the resulting mrf. what is the size of the largest maximal clique in this graph? exercise gaussian times gaussian is gaussian prove equation hint use completing the square. exercise message passing on a tree consider the dgm in figure which represents the following fictitious biological model. each gi represents the genotype of a person gi if they have a healthy gene and gi if they have an unhealthy gene. and may inherit the unhealthy gene from their parent xi r is a continuous measure of blood pressure which is low if you are healthy and high if you are unhealthy. we define the cpds as follows pxigi n pxigi n the meaning of the matrix for is that etc. computational intractability of exact inference in the worst case figure a simple dag representing inherited diseases. a. suppose you observe and is unobserved. what is the posterior belief on i.e. b. now suppose you observe amd what is explain your answer c. now suppose what is explain your answer intuitively. d. now suppose what is explain your answer intuitively. intuitively. exercise inference in lattice mrfs consider an mrf with a m n lattice graph structure so each hidden node xij is connected to its nearest neighbors as in an ising model. in addition each hidden node has its own local evidence yij. assume all hidden nodes have k states. in general exact inference in such models is intractable because the maximum cliques of the corresponding triangulated graph have size omaxm n. suppose m n i.e. the lattice is short and fat. a. how can one efficiently perform exact inference a deterministic algorithm in such models? exact inference i mean computing marginal probabilities p exactly where is all the evidence. give a brief description of your method. b. what is the asymptotic complexity time of your algorithm? c. now suppose the lattice is large and square so m n but all hidden states are binary k in this case how can one efficiently exactly compute a deterministic algorithm the map estimate arg maxx p wherex is the joint assignment to all hidden nodes? variational inference introduction we have now seen several algorithms for computing of a posterior distribution. for discrete graphical models we can use the junction tree algorithm to perform exact inference as explained in section however this takes time exponential in the treewidth of the graph rendering exact inference often impractical. for the case of gaussian graphical models exact inference is cubic in the treewidth. however even this can be too slow if we have many variables. in addition the jta does not work for continuous random variables outside of the gaussian case nor for mixed discrete-continuous variables outside of the conditionally gaussian case. for some simple two node graphical models of the form x d we can compute the exact posterior pxd in closed form provided the prior px is conjugate to the likelihood pdx means the likelihood must be in the exponential family. see chapter for some that in this chapter x represent the unknown variables whereas in examples of this. chapter we used to represent the unknowns. in more general settings we must use approximate inference methods. in section we discussed the gaussian approximation which is useful for inference in two node models of the form x d where the prior is not conjugate. example section applied the method to logistic regression. the gaussian approximation is simple. however some posteriors are not naturally modelled using gaussians. for example when inferring multinomial parameters a dirichlet distribution is a better choice and when inferring states in a discrete graphical model a categorical distribution is a better choice. in this chapter we will study a more general class of deterministic approximate inference algorithms based on variational inference et al. jaakkola and jordan jaakkola wainwright and jordan the basic idea is to pick an approximation qx to the distribution from some tractable family and then to try to make this approximation as close pxd. this reduces inference to an optimization as possible to the true posterior p problem. by relaxing the constraints andor approximating the objective we can trade accuracy for speed. the bottom line is that variational inference often gives us the speed benefits of map estimation but the statistical benefits of the bayesian approach. chapter variational inference variational inference suppose p is our true but intractable distribution and qx is some approximation chosen from some tractable family such as a multivariate gaussian or a factored distribution. we assume q has some free parameters which we want to optimize so as to make q similar to p an obvious cost function to try to minimize is the kl divergence kl log p p qx however this is hard to compute since taking expectations wrt p a natural alternative is the reverse kl divergence x kl qx log qx p x is assumed to be intractable. the main advantage of this objective is that computing expectations wrt q is tractable choosing a suitable form for q. we discuss the statistical differences between these two objectives in section unfortunately equation is still not tractable as written since even evaluating p pxd pointwise is hard since it requires evaluating the intractable normalization constant z pd. however usually the unnormalized distribution px pxd is tractable to compute. we therefore define our new objective function as follows jq kl p where we are slightly abusing notation since p is not a normalized distribution. plugging in the definition of kl we get jq x x qx log qx px qx log qx zp qx p log z qx log kl x log z since z is a constant by minimizing jq we will force q to become close to p log likelihood since kl divergence is always non-negative we see that jq is an upper bound on the nll jq kl log z log z log pd alternatively we can try to maximize the following quantity and friedman this is referred to as the energy functional which is a lower bound on the log likelihood of the data lq jq kl log z log z log pd we see that variational inference is closely related to em since this bound is tight when q p section variational inference alternative interpretations of the variational objective there are several equivalent ways of writing this objective that provide different insights. one formulation is as follows jq eq qx eq log px h q which is the expected energy ex log px minus the entropy of the system. statistical physics jq is called the variational free energy or the helmholtz free in another formulation of the objective is as follows jq q qx log pxpdx eq qx log px log pdx eq log pdx kl this is the expected nll plus a penalty term that measures how far the approximate posterior is from the exact prior. we can also interpret the variational objective from the point of view of information theory so-called bits-back argument. see and camp honkela and valpola for details. forward or reverse kl? since the kl divergence is not symmetric in its arguments minimizing kl wrt q will give different behavior than minimizing kl below we discuss these two different methods. first consider the reverse kl kl also known as an i-projection or information projection. by definition we have kl qx ln qx px x x this is infinite if px and qx thus if px we must ensure qx we say that the reverse kl is zero forcing for q. hence q will typically under-estimate the support of p. now consider the forwards kl also known as an m-projection or moment projection kl px ln px qx this is infinite if qx and px so if px we must ensure qx we say that the forwards kl is zero avoiding for q. hence q will typically over-estimate the support of p. the difference between these methods is illustrated in figure we see that when the true distribution is multimodal using the forwards kl is a bad idea q is constrained to be unimodal since the resulting posterior modemean will be in a region of low density right between the two peaks. in such contexts the reverse kl is not only more tractable to compute but also more sensible statistically. chapter variational inference figure illustrating forwards vs reverse kl on a bimodal distribution. the blue curves are the contours of the true distribution p. the red curves are the contours of the unimodal approximation q. minimizing forwards kl q tends to cover p. minimizing reverse kl q locks on to one of the two modes. based on figure of figure generated by klfwdreversemixgauss. figure illustrating forwards vs reverse kl on a symmetric gaussian. the blue curves are the contours of the true distribution p. the red curves are the contours of a factorized approximation q. minimizing kl minimizing kl based on figure of figure generated by klpqgauss. another example of the difference is shown in figure where the target distribution is an elongated gaussian and the approximating distribution is a product of two gaussians. that is px n where in figure we show the result of minimizing kl in this simple example one can show that the solution has the form qx it is called free because the variables x are free to vary rather than being fixed. the variational free energy is a function of the distribution q whereas the regular energy is a function of the state vector x. the mean field method its variance is controlled by the direction of smallest variance of p. figure shows that we have correctly captured the mean but the approximation is too compact in fact it is often the case not always et al. that minimizing kl where q is factorized results in an approximation that is overconfident. in figure we show the result of minimizing kl as we show in exercise the optimal solution when minimizing the forward kl wrt a factored approximation is to set q to be the product of marginals. thus the solution has the form qx n figure shows that this is too broad since it is an over-estimate of the support of p. for the rest of this chapter and for most of the next we will focus on minimizing kl in section when we discuss expectation proagation we will discuss ways to locally optimize kl one can create a family of divergence measures indexed by a parameter r by defining the alpha divergence as follows d this measure satisfies d iff p q but is obviously not symmetric and hence is not a metric. kl corresponds to the limit whereas kl corresponds to the limit when we get a symmetric divergence measure that is linearly related to the hellinger distance defined by qx px dx dh is a valid distance metric that is note that satisfies the triangle inequality. see for details. it is symmetric non-negative and dh the mean field method one of the most popular forms of variational inference is called the mean field approximation and saad in this approach we assume the posterior is a fully factorized approximation of the form qx qixi i our goal is to solve this optimization problem kl min where we optimize over the parameters of each marginal distribution qi. derive a coordinate descent method where at each step we make the following update in section we log qjxj e qj px const chapter variational inference model ising model factorial hmm univariate gaussian linear regression logistic regression mixtures of gaussians latent dirichlet allocation section section section section section section section section table algorithm. some models in this book for which we provide detailed derivations of the mean field inference where px xd is the unnormalized posterior and the notation e qj means to take the expectation over f with respect to all the variables except for xj. for example if we have three variables then e where sums get replaced by integrals where necessary. when updating qj we only need to reason about the variables which share a factor with xj i.e. the terms in j s markov blanket section the other terms get absorbed into the constant term. since we are replacing the neighboring values by their mean value the method is known as mean field. this is very similar to gibbs sampling except instead of sending sampled values between neighboring nodes we send mean values between nodes. this tends to be more efficient since the mean can be used as a proxy for a large number of samples. the other hand mean field messages are dense whereas samples are sparse this can make sampling more scalable to very large models. of course updating one distribution at a time can be slow since it is a form of coordinate descent. several methods have been proposed to speed up this basic approach including using pattern search et al. and techniques based on parameter expansion and jaakkola however we will not consider these methods in this chapter. it is important to note that the mean field method can be used to infer discrete or continuous latent quantities using a variety of parametric forms for qi as we will see below. this is in contrast to some of the other variational methods we will encounter later which are more restricted in their applicability. table lists some of the examples of mean field that we cover in this book. derivation of the mean field update equations recall that the goal of variational inference is to minimize the upper bound jq log pd. equivalently we can try to maximize the lower bound lq jq qx log px qx log pd x we will do this one term at a time. the mean field method if we write the objective singling out the terms that involve qj and regarding all the other terms as constants we get lqj i x xj xj xj x j log qkxk k k qixi qjxj log px qixi x j log px qjxj qixi log px log qkxk qjxj qixi log qkxk jxj qjxj log qjxj const xj where log fjxj x j qjxj log fjxj x j lqj kl qjxj zj exp e qj px xj qixi log px e qj px so we average out all the hidden variables except for xj. thus we can rewrite lqj as follows we can maximize l by minimizing this kl which we can do by setting qj fj as follows we can usually ignore the local normalization constant zj since we know qj must be a normalized distribution. hence we usually work with the form log qjxj e qj px const the functional form of the qj distributions will be determined by the type of variables xj as if xj is a well as the form of the model. discrete random variable then qj will be a discrete distribution if xj is a continuous random variable then qj will be some kind of pdf. we will see examples of this below. is sometimes called free-form optimization. example mean field for the ising model consider the image denoising example from section where xi are the hidden pixel values of the clean image. we have a joint model of the form px y xpyx chapter variational inference where the prior has the form px exp wijxixj j nbri and the likelihood has the form pyx pyixi exp lixi i i therefore the posterior has the form pxy ex z exp ex lixi i i qx qxi i we will now approximate this by a fully factored approximation where i is the mean value of node i. to derive the update for the variational parameter i we first write out log px ex dropping terms that do not involve xi log px xi wijxj lixi const this only depends on the states of the neighboring nodes. now we take expectations of this wrt j nbri xi qjxj to get qixi exp mi j nbri wij j j nbri wij j lixi thus we replace the states of the neighbors by their average values. let be the mean field influence on node i. also approximate marginal posterior is given by let l i and l i li the qixi emil ai mi i emil i e mil i l i i e i l i structured mean field sample meanfieldh sample meanfieldh mean after sweeps of meanfieldh figure example of image denoising using mean field parallel updates and a damping factor of we use an ising prior with wij and a gaussian noise model with we show the results after and iterations across the image. compare to figure figure generated by isingimagedenoisedemo. similarly we have qixi sigm from this we can compute the new mean for site i i eqi qixi qixi hence the update equation becomes i tanh wij j eai eai e ai i l i e ai e ai eai tanhai e j nbri j nbri see also exercise for an alternative derivation of these equations. we can turn the above equations in to a fixed point algorithm by writing t i tanh wij t j i l i it is usually better to use damped updates of the form i t t i tanh wij t j i l i j nbri for we can update all the nodes in parallel or update them asychronously. figure shows the method in action applied to a ising model with homogeneous attractive potentials wij we use parallel updates with a damping factor of we don t use damping we tend to get checkerboard artefacts. structured mean field assuming that all the variables are independent in the posterior is a very strong assumption that can lead to poor results. sometimes we can exploit tractable substructure in our problem so chapter variational inference figure chains approximation. based on figure of and jordan a factorial hmm with chains. a fully factorized approximation. a product-of that we can efficiently handle some kinds of dependencies. this is called the structured mean field approach and jordan the approach is the same as before except we group sets of variables together and we update them simultaneously. follows by simply treating all the variables in the i th group as a single mega-variable and then repeating the derivation in section as long as we can perform efficient inference in each qi the method is tractable overall. we give an example below. see and jordan for some more recent work in this area. example factorial hmm consider the factorial hmm model and jordan introduced in section suppose there are m chains each of length t and suppose each hidden node has k states. the model is defined as follows pxtmxt px y where pxtm kxt j mjk is an entry in the transition matrix for chain m k mk is the initial state distribution for chain m and m t yt pytxt n wmxtm is the observation model where xtm is a encoding of xtm and wm is a d k matrix yt r d. figure illustrates the model for the case where m even though each chain is a priori independent they become coupled in the posterior due to having an observed common child yt. the junction tree algorithm applied to this graph takes ot m km time. below we will derive a structured mean field algorithm that takes ot m k time where i is the number of mean field iterations i suffices for good performance. structured mean field we can write the exact posterior in the following form pxy z exp ex y m yt xt m wmxtm yt xt tm amxt ex y wmxtm m m m where am log am and m log m interpreted elementwise. we can approximate the posterior as a product of marginals as in figure but a better approximation is to use a product of chains as in figure each chain can be tractably updated individually using the forwards-backwards algorithm. more precisely we assume qxtmxt tm qxy qxtmxt tm zq tmk xtmk we see that the tmk parameters play the role of an approximate local evidence averaging out the effects of the other chains. this is contrast to the exact local evidence which couples all the chains together. we can rewrite the approximate posterior as qx zq tm tm xt m xt eqx exp eqx where xt tm amxt where tm log tm. we see that this has the same temporal factors as the exact posterior but the local evidence term is different. the objective function is given by where the expectations are taken wrt q. one can show that the update has the form kl e log zq log z ytm yt wt tm exp m diagwt m ytm m m chapter variational inference the tm parameter plays the role of the local evidence averaging over the neighboring chains. having computed this for each chain we can perform forwards-backwards in parallel using these approximate local evidence terms to compute for each m and t. the update cost is ot m k for a full sweep over all the variational parameters since we have to run forwards-backwards m times for each chain independently. this is the same cost as a fully factorized approximation but is much more accurate. variational bayes make a fully factorized mean field approximation p so far we have been concentrating on inferring latent variables zi assuming the parameters of the model are known. now suppose we want to infer the parameters themselves. if we k q k we get a method known as variational bayes or vb and camp mackay attias beal and ghahramani smidl and quinn we give some examples of vb below assuming that there are no latent variables. if we want to infer both latent variables and parameters and we make an approximation of the form p q i qizi we get a method known as variational bayes em which we described in section example vb for a univariate gaussian following let us consider how to apply vb to infer the posterior over the parameters for a gaussian p where is the precision. for convenience we will use a conjugate prior of the form p n however we will use an approximate factored posterior of the form q q we do not need to specify the forms for the distributions q and q the optimal forms will fall out automatically during the derivation conveniently they turn out to be gaussian and gamma respectively. you might wonder why we would want to do this since we know how to compute the exact posterior for this model there are two reasons. first it is a useful pedagogical exercise since we can compare the quality of our approximation to the exact posterior. second it is simple to modify the method to handle a semi-conjugate prior of the form p n for which exact inference is no longer possible. this method was originally called ensemble learning since we are using an ensemble of parameters distribution instead of a point estimate. however the term ensemble learning is also used to describe methods such as boosting so we prefer the term vb. variational bayes target distribution the unnormalized log posterior has the form log p log p log pd log p log p n log log log const updating q the optimal form for q is obtained by averaging over log q q pd log p const eq const by completing the square one can show that q n n n where n n x n n n at this stage we don t know what q is and hence we cannot compute e but we will derive this below. updating q the optimal form for q is given by log q q pd log p log p const log eq log log n const we recognize this as the log of a gamma distribution hence q ga bn where an bn n eq chapter variational inference computing the expectations n we have to implement the updates we have to specify how to compute the various expectations. since q n n eq n n n eq since q ga bn we have eq an bn we can now give explicit forms for the update equations. for q we have n x n n n n an bn and for q we have an n bn xi i e we see that n and an are in fact fixed constants and only n and bn need to be updated fact one can solve for the fixed points of n and bn analytically but we don t iteratively. do this here in order to illustrate the iterative updating scheme. illustration figure gives an example of this method in action. the green contours represent the exact posterior which is gaussian-gamma. the dotted red contours represent the variational approximation over several iterations. we see that the final approximation is reasonably close to the exact solution. however it is more compact than the true distribution. it is often the case that mean field inference underestimates the posterior uncertainty see section for more discussion of this point. lower bound in vb we are maximizing lq which is a lower bound on the log marginal likelihood lq log pd log pd d it is very useful to compute the lower bound itself for three reasons. first it can be used to assess convergence of the algorithm. second it can be used to assess the correctness of one s variational bayes exact vb exact vb exact vb exact vb factored variational approximation to the gaussian-gamma distribution figure initial guess. after updating q after updating q at convergence iterations. based on of figure generated by unigaussvbdemo. code as with em if the bound does not increase monotonically there must be a bug. third the bound can be used as an approximation to the marginal likelihood which can be used for bayesian model selection. unfortunately computing this lower bound involves a fair amount of tedious algebra. we work out the details for this example but for other models we will just state the results without proof or even omit discussion of the bound altogether for brevity. for this model lq can be computed as follows lq q log pd q d d e pd e p e p e q e q where all expectations are wrt q we recognize the last two terms as the entropy of a gaussian and the entropy of a gamma distribution which are given by n n h log n h bn log logbn n chapter variational inference where is the digamma function. to compute the other terms we need the following facts e xx gaa b logb e gaa b xx n n a b e e for the expected log likelihood one can show that eq pd n n n n eq e log bn n an n x n n eq where x and are the empirical mean and variance. for the expected log prior of we have eq p log log log bn an bn log log eq for the expected log prior of one can show that eq p log log an bn n e q log bn n putting it altogether one can show that lq log n log an log bn const this quantity monotonically increases after each vb update. example vb for linear regression in section we discussed an empirical bayes approach to setting the hyper-parameters for ridge regression known as the evidence procedure. in particular we assumed a likelihood of the form pyx and a prior of the form pw we then variational bayes computed a type ii estimate of and the same approach was extended in section to handle a prior of the form n diag which allows one hyper-parameter per feature a technique known as automatic relevancy determination. in this section we derive a vb algorithm for this model. we follow the presentation of initially we will use the following prior pw b b we choose to use the following factorized approximation to the posterior qw qw given these assumptions one can show that the optimal form for the posterior is qw n n b n b n where v n a xx wn vn xt y n a n a b n b a n a n b b a a n b n a n b n i wt d n awn wt n wn trvn this method can be extended to the ard case in a straightforward way by using the following priors pw diag ga ja p b the posterior for w and is computed as before except we use a diaga n nj instead of note that drugowitsch uses as the hyper-parameters for p and as the hyper-parameters for p whereas sec uses as the hyper-parameters for p and treats as fixed. to avoid confusion i use a as the hyper-parameters for p and a as the hyper-parameters for p b b chapter variational inference a n n i. the posterior for has the form q n b nj a n a b nj b nj j ga ja a n b n the algorithm alternates between updating qw and q once w and have been inferred the posterior predictive is a student distribution as shown in equation specifically for a single data case we have xt vn x n x n pyxd b n a n the exact marginal likelihood which can be used for model selection is given by pd pyx w d we can compute a lower bound on log pd as follows lq n wt n xt i vn xi a n b n log d log log log b log b a n b n b log log n a n a n log b n n log b n a n in the ard case the last line becomes log log b log n a n log b nj figure compare vb and eb on a model selection problem for polynomial regression. we see that vb gives similar results to eb but the precise behavior depends on the sample size. when n vb s estimate of the posterior over models is more diffuse than eb s since vb models uncertainty in the hyper-parameters. when n the posterior estimate of the hyperindeed if we compute e when we have an parameters becomes more well-determined. uninformative prior a we get b a n b n a n b n wt n wn trvn variational bayes em methodvb methodeb d m p d m p d m p m methodvb d m p m m methodeb m figure we plot the posterior over models of degree and assuming a uniform prior pm we approximate the marginal likelihood using vb and eb. in we use n data points in figure in we use n data points in figure figure generated by linregebmodelselvsn. compare this to equation for eb d e w d wt n wn trvn n and b modulo the a in hindsight this is perhaps not that surprising since eb is trying to maximize log pd and vb is trying to maximize a lower bound on log pd. n terms these are the same. variational bayes em now consider latent variable models of the form zi xi this includes mixtures models pca hmms etc. there are now two kinds of unknowns parameters and latent variables zi. as we saw in section it is common to fit such models using em where in the e step we infer the posterior over the latent variables pzixi and in the m step we compute a point estimate of the parameters the justification for this is two-fold. first it results in simple algorithms. second the posterior uncertainty in is usually less than in zi since the are informed by all n data cases whereas zi is only informed by xi this makes a map estimate of chapter variational inference more reasonable than a map estimate of zi. however vb provides a way to be more bayesian by modeling uncertainty in the parameters as well in the latent variables zi at a computational cost that is essentially the same as em. this method is known as variational bayes em or vbem. the basic idea is to use mean field where the approximate posterior has the form p q q qzi i the first factorization between and z is a crucial assumption to make the algorithm tractable. the second factorization follows from the model since the latent variables are iid conditional on in vbem we alternate between updating qzid variational e step and updating q variational m step. we can recover standard em from vbem by approximating the parameter posterior using a delta function q the variational e step is similar to a standard e step except instead of plugging in a map estimate of the parameters and computing pzid we need to average over the parameters. roughly speaking this can be computed by plugging in the posterior mean of the parameters instead of the map estimate and then computing pzid using standard algorithms such as forwards-backwards. unfortunately things are not quite this simple but this is the basic idea. the details depend on the form of the model we give some examples below. the variational m step is similar to a standard m step except instead of computing a point estimate of the parameters we update the hyper-parameters using the expected sufficient statistics. this process is usually very similar to map estimation in regular em. again the details on how to do this depend on the form of the model. the principle advantage of vbem over regular em is that by marginalizing out the parameters we can compute a lower bound on the marginal likelihood which can be used for model selection. we will see an example of this in section vbem is also egalitarian since it treats parameters as first class citizens just like any other unknown quantity whereas em makes an artificial distinction between parameters and latent variables. example vbem for mixtures of gaussians let us consider how to fit a mixture of gaussians using vbem. use scare quotes since we are not estimating the model parameters but inferring a posterior over them. we will follow the presentation of sec unfortunately the details are rather complicated. fortunately as with em one gets used to it after a bit of practice. usual with math simply reading the equations won t help much you should really try deriving these results yourself try some of the exercises if you want to learn this stuff in depth. the variational posterior the likelihood function is the usual one for gaussian mixture models pz x k n k zik k i k where zik if data point i belongs to cluster k and zik otherwise. variational bayes em we will assume the following factored conjugate prior p dir n k k where k is the precision matrix for cluster k. the subscript means these are parameters of the prior we assume all the prior parameters are the same for all clusters. for the mixing weights we usually use a symmetric prior the exact posterior pz is a mixture of k n distributions corresponding to all possible labelings z. we will try to approximate the volume around one of these modes. we will use the standard vb approximation to the posterior p q qzi i at this stage we have not specified the forms of the q functions these will be determined by the form of the likelihood and prior. below we will show that the optimal form is as follows qz catziri i dir n kmk k k klk k k lack of subscript means these are parameters of the posterior not the prior. below we will derive the update equations for these variational parameters. derivation of qz e step the form for qz can be obtained by looking at the complete data log joint ignoring terms that do not involve z and taking expectations of what s left over wrt all the hidden variables except for z. we have log qz q px z const zik log ik const i i where we define log ik eq k eq k d kt kxi k eq using the fact that q dir we have log k e k k chapter variational inference where is the digamma function. exercise for the detailed derivation. next we use the fact that q k k n kmk k k to get log k e k klk k d log log k k j finally for the expected value of the quadratic form we get kt kxi k d k kxi mkt kxi mk e putting it altogether we get that the posterior responsibility of cluster k for datapoint i is rik k k exp k mkt kxi mk compare this to the expression used in regular em rem ik k k exp kt kxi k d k k i k the significance of this difference is discussed further in section derivation of q m step using the mean field recipe we have log q log p log p k k eqz pzi eqz log n k k const i we see this factorizes into the form q q k k k for the term we have log q k k i log k rik log k const exponentiating we recognize this as a dirichlet distribution q dir k nk nk rik i variational bayes em variational bayes objective for gmm on old faithful data d o o h i l e k i i l l a n g r a m g o l n o d n u o b r e w o l iter figure lower bound vs iterations for the vb algorithm in figure the steep parts of the curve correspond to places where the algorithm figures out that it can increase the bound by killing off unnecessary mixture components as described in section the plateaus correspond to slowly moving the clusters around. figure generated by mixgaussvbdemofaithful. for the k and k terms we have q k k kmk k k klk k nk k nk mk nkxk k l nksk k k nk l nk i nk i xk sk rikxi rikxi xkxi xkt this is very similar to the m step for map estimation discussed in section except here we are computing the parameters of the posterior over rather than map estimates of lower bound on the marginal likelihood the algorithm is trying to maximize the following lower bound z l qz log px z qz d log pd this quantity should increase monotonically with each iteration as shown in figure unfortunately deriving the bound is a bit messy because we need to compute expectations of the unnormalized log posterior as well as entropies of the q distribution. we leave the details are similar to section to exercise chapter variational inference posterior predictive distribution k we showed that the approximate posterior has the form q dir n kmk k k klk k consequently the posterior predictive density can be approximated as follows using the results from section pxz kn k k d k z k k lk k k t mk k d pxd mk this is just a weighted sum of student distributions. if instead we used a plug-in approximation we would get a weighted sum of gaussian distributions. model selection using vbem the simplest way to select k when using vb is to fit several models and then to use the variational lower bound to the log marginal likelihood lk log pdk to approximate pkd elk pkd however the lower bound needs to be modified somewhat to take into account the lack of identifiability of the parameters in particular although vb will approximate the volume occupied by the parameter posterior it will only do so around one of the local modes. with k components there are k! equivalent modes which differ merely by permuting the labels. therefore we should use log pdk lk logk!. automatic sparsity inducing effects of vbem although vb provides a reasonable approximation to the marginal likelihood than bic and ghahramani this method still requires fitting multiple models one for each value of k being considered. a faster alternative is to fit a single model where k is set large but where is set very small from figure we see that the resulting prior for the mixing weights has spikes near the corners of the simplex encouraging a sparse mixing weight vector. in regular em the map estimate of the mixing weights will have the form k k where k nk. unforuntately this can be negative if and nk variational bayes em iter iter figure we visualize the posterior mean parameters at various stages of the vbem algorithm applied to a mixture of gaussians model on the old faithful data. shading intensity is proportional to the mixing weight. we initialize with k-means and use as the dirichlet hyper-parameter. based on figure of figure generated by mixgaussvbdemofaithful based on code by emtiyaz khan. iter iter figure we visualize the posterior values of k for the model in figure we see that unnecessary components get killed off figure generated by mixgaussvbdemofaithful. and jain however in vbem we use exp k exp k now exp x for x so if k when we compute k it s like we substract from the posterior counts. this will hurt small clusters more than large clusters a regressive the effect is that clusters which have very few members become more and more empty over successive iterations whereas the popular clusters get more and more members. this is called the rich get richer phenomenon we will encounter it again in section when we discuss dirichlet process mixture models. this automatic pruning method is demonstrated in figure we fit a mixture of gaussians to the old faithful dataset but the data only really needs clusters so the rest get killed off for more details see et al. chapter variational inference in this example we used if we use a larger we do not get a sparsity effect. in figure we plot q at various iterations we see that the unwanted components get extinguished. this provides an efficient alternative to performing a discrete search over the number of clusters. variational message passing and vibes we have seen that mean field methods at least of the fully-factorized variety are all very similar just compute each node s full conditional and average out the neighbors. this is very similar to gibbs sampling except the derivation of the equations is usually a bit more work. fortunately it is possible to derive a general purpose set of update equations that work for any dgm for which all cpds are in the exponential family and for which all parent nodes have conjugate distributions and beal et al. for a recent extension to handle non-conjugate priors. one can then sweep over the graph updating nodes one at a time in a manner similar to gibbs sampling. this is known as variational message passing or vmp and bishop and has been implemented in the open-source program this is a vb analog to bugs which is a popular generic program for gibbs sampling discussed in section vmp mean field is best-suited to inference where one or more of the hidden nodes are continuous when performing bayesian learning for models where all the hidden nodes are discrete more accurate approximate inference algorithms can be used as we discuss in chapter local variational bounds so far we have been focusing on mean field inference which is a form of variational inference based on minimizing kl p where q is the approximate posterior assumed to be factorized and p is the exact unnormalized posterior. however there is another kind of variational inference where we replace a specific term in the joint distribution with a simpler function to simplify computation of the posterior. such an approach is sometimes called a local variational approximation since we are only modifying one piece of the model unlike mean field which is a global approximation. in this section we study several examples of this method. motivating applications before we explain how to derive local variational bounds we give some examples of where this is useful. variational logistic regression consider the problem of how to approximate the parameter posterior for multiclass logistic regression model under a gaussian prior. one approach is to use a gaussian approximation as discussed in section however a variational approach can produce a more available at httpvibes.sourceforge.net. local variational bounds accurate approximation to the posterior since it has tunable parameters. another advantage is that the variational approach monotonically optimizes a lower bound on the likelihood of the data as we will see. to see why we need a bound note that the likelihood can be written as follows pyx w i i lse i yt exp where i xiwi identifiability and where we define the log-sum-exp or lse function as follows i xt i wm where m c we set wc for lse i log e im the main problem is that this likelihood is not conjugate to the gaussian prior. below we discuss how to compute gaussian-like lower bounds to this likelihood which give rise to approximate gaussian posteriors. multi-task learning one important application of bayesian inference for logistic regression is where we have multiple related classifiers we want to fit. in this case we want to share information between the parameters for each classifier this requires that we maintain a posterior distibution over the parameters so we have a measure of confidence as well as an estimate of the value. we can embed the above variational method inside of a larger hierarchical model in order to perform such multi-task learning as described in e.g. and mcauliffe discrete factor analysis another situation where variational bounds are useful arises when we fit a factor analysis model to discrete data. this model is just like multinomial logistic regression except the input variables are hidden factors. we need to perform inference on the hidden variables as well as the regression weights. for simplicity we might perform point estimation of the weights and just integrate out the hidden variables. we can do this using variational em where we use the variational bound in the e step. see section for details. correlated topic model a topic model is a latent variable model for text documents and other forms of discrete data see section for details. often we assume the distribution over topics has a dirichlet prior but a more powerful model known as the correlated topic model uses a gaussian prior which can model correlations more easily section for details. unfortunately this also involves the lse function. however we can use our variational bounds in the context of a variational em algorithm as we will see later. chapter variational inference bohning s quadratic bound to the log-sum-exp function all of the above examples require dealing with multiplying a gaussian prior by a multinomial likelihood this is difficult because of the log-sum-exp term. in this section we derive a way to derive a gaussian-like lower bound on this likelihood. consider a taylor series expansion of the lse function around i r lse i lse i i it g i g i exp i lse i s i h i diagg i g ig it i it h i i i m where g and h are the gradient and hessian of lse and i r m is chosen such that equality holds. an upper bound to lse can be found by replacing the hessian matrix h i with a matrix ai such that ai h i. showed that this can be achieved if we use the matrix ai that m c is the number of classes. note that ai is independent of i however we still write it as ai than dropping the i subscript since other bounds that we consider below will have a data-dependent curvature term. the upper bound on lse therefore becomes im m m i i ci m lse i i ai i bt t im ai m bi ai i g i ci where i r i ai i g it i lse i t m is a vector of variational parameters. we can use the above result to get the following lower bound on the softmax likelihood log pyi cxi w wt xiaixiw bt i xiw ci i xiw yt c to simplify notation define the pseudo-measurement yi a i yi then we can get a gaussianized version of the observation model pyixi w f i n yixiw a i where f i is some function that does not depend on w. given this it is easy to compute the posterior qw n vn using bayes rule for gaussians. below we will explain how to update the variational parameters i. local variational bounds applying bohning s bound to multinomial logistic regression let us see how to apply this bound to multinomial logistic regression. from equation we can define the goal of variational inference as maximizing log pyixi w lq kl eq kl eq i eq i kl vn v kl i i lse i yt yt eq i where qw vn is the approximate posterior. the first term is just the kl divergence between two gaussians which is given by trvn v log v dm where dm is the dimensionality of the gaussian and we assume a prior of the form pw n where typically and is block diagonal. the second term is simply yt i eq i yt i mi where mi ximn the final term can be lower bounded by taking expectations of our quadratic upper bound on lse as follows eq i trai vi miai mi bt i mi ci where vi xivn xt lqj i putting it altogether we have log v trvn v dm i mi yt trai vi v i mi ci miai mi bt this lower bound combines jensen s inequality in mean field inference plus the quadratic lower bound due to the lse term so we write it as lqj we will use coordinate ascent to optimize this lower bound. that is we update the variational posterior parameters vn and mn and then the variational likelihood parameters i. we leave chapter variational inference xt i aixi the detailed derivation as an exercise and just state the results. we have vn mn vn v xt i bi i mi ximn we can exploit the fact that ai is a constant matrix plus the fact that xi has block structure to simplify the first two terms as follows vn a v xixt i mn vn where denotes the kronecker product. see algorithm for some pseudocode and http for some matlab code. bi xi inference for multi-class logistic regression using bohning s d i n prior algorithm variational bound input yi c xi r define m c dummy encode yi define xi blockdiagxt i m define y yn x xn and a vn initialize mn repeat im xt i axi v m xmn reshapem m n g exp lse b a g b v mn vn compute the lower bound lqj using equation xt b until converged return mn and vn bounds for the sigmoid function in many models we just have binary data. i wt xi where w r d is a weight vector matrix. in this case we have yi m and in this case the bohning bound local variational bounds bohning bound jj bound figure quadratic lower bounds on the sigmoid function. in solid red we plot sigmx vs x. in dotted blue we plot the lower bound lx vs x for bohning bound. this is tight at jj bound. this is tight at figure generated by sigmoidlowerbounds. becomes a b c e a b a e c a e e it is possible to derive an alternative quadratic bound for this case as shown in and jordan this has the following form e tanh sigm we shall refer to this as the jj bound after its inventors and jordan to facilitate comparison with bohning s bound let us rewrite the jj bound as a quadratic form as follows e a b c a b c e the jj bound has an adaptive curvature term since a depends on in addition it is tight at two points as is evident from figure by contrast the bohning bound is a constant curvature bound and is only tight at one point as is evident from figure chapter variational inference if we wish to use the jj bound for binary logistic regression we can make some small modifications to algorithm first we use the new definitions for ai bi and ci. the fact that ai is not constant when using the jj bound unlike when using the bohning bound means we cannot compute vn outside of the main loop making the method a constant factor slower. next we note that xi xt i so the updates for the posterior become n v v mn vn v ixixt i finally to compute the update for i we isolate the terms in lqj that depend on i to get i eq wwt xi i const l optimizing this wrt i gives the equation ln sigm i ixt ixt i eq wwt i is monotonic for i and we do not need to consider negative values of i by now symmetry of the bound around i figure hence the only way to make the above expression is if we have i hence the update becomes xi wwt i e xi i new i xt i mn mt n although the jj bound is tighter than the bohning bound sometimes it is not tight enough in order to estimate the posterior covariance accurately. a more accurate approach which uses a piecewise quadratic upper bound to lse is described in et al. by increasing the number of pieces the bound can be made arbitrarily tight. other bounds and approximations to the log-sum-exp function there are several other bounds and approximations to the multiclass lse function which we can use which we briefly summarize below. note however that all of these require numerical optimization methods to compute mn and vn making them more complicated to implement. product of sigmoids the approach in exploits the fact that log e k e k it then applies the jj bound to the term on the right. local variational bounds jensen s inequality the approach in and lafferty uses jensen s inequality as follows eq i eq log expxt i wc log log eq expxt i wc expxt i mnc xt i vnccxi where the last term follows from the mean of a log-normal distribution which is e multivariate delta method the approach in and xing braun and mcauliffe uses the multivariate delta method which is a way to approximate moments of a function using a taylor series expansion. in more detail let f be the function of interest. using a second-order approximation around m we have f f mt gw m where g and h are the gradient and hessian evaluated at m. if qw n v we have mt hw m eq f trhv if we use f lsexiw we get eq lsexim trxihxt i v where g and h for the lse function are defined in equations and variational inference based on upper bounds so far we have been concentrating on lower bounds. however sometimes we need to use an upper bound. for example et al. derives a mean field algorithm for sigmoid belief nets which are dgms in which each cpd is a logistic regression function unlike the case of ising models the resulting mrf is not pairwise but contains higher order interactions. this makes the standard mean field updates intractable. in particular they turn out to involve computing an expression which requires evaluating e j pai wij xj e e log sigmwt i xpai the minus sign in front. et al. show how to derive an upper bound on the sigmoid function so as to make this update tractable resulting in a monotonically convergent inference procedure. chapter variational inference exercises exercise laplace approximation to p log for a univariate gaussian compute a laplace approximation of p log for a gaussian using an uninformative prior p log exercise laplace approximation to normal-gamma consider estimating and log for a gaussian using an uniformative normal-gamma prior. the log posterior is log p n log ny a. show that the first derivatives are ny log p log p n ny b. show that the hessian matrix is given by log p log p n log p log p y ny y h c. use this to derive a laplace approximation to the posterior p exercise variational lower bound for vb for univariate gaussian fill in the details of the derivation in section exercise variational lower bound for vb for gmms consider vbem for gmms as in section show that the lower bound has the following form l e pxz e pz e p e p e qz e q e q local variational bounds where e pxz k nk ln k d k ktrsklk kxk mkt lkxk mk d e pz e p ln cdir rik ln k k i k ln k e p e qz e q e q k d ln cw d ln ln k d k kmk lkmk k ln k ln cdir rik ln rik k k i ln k d ln k d k ln k h k ktrl lk where the normalization constant for the dirichlet and wishart is given by cdir k k k k cw il d d dd where d is the multivariate gamma function. finally the entropy of the wishart is given by h ln cw il d e where e is given in equation exercise derivation of e k under a dirichlet distribution show that d exp k exp expe k where dir exercise alternative derivation of the mean field updates for the ising model derive equation by directly optimizing the variational free energy one term at a time. chapter variational inference exercise forwards vs reverse kl divergence exercise of consider a factored approximation qx y qxqy to a joint distribution px y. show that to minimize the forwards kl kl we should set qx px and qy py i.e. the optimal approximation is a product of marginals now consider the following joint distribution where the rows represent y and the columns x. x show that the reverse kl kl for this p has three distinct minima. evaluate kl at each of them. what is the value of kl if we set qx y pxpy? exercise derivation of the structured mean field updates for fhmm derive the updates in section identify those minima and exercise variational em for binary fa with sigmoid link consider the binary fa model pxizi berxijsigmwt j zi j i w zi zi w pzi i berxijsigm ij derive an em algorithm to fit this model using the jaakkola-jordan bound. hint the answer is in but the exercise asks you to derive these equations. exercise vb for binary fa with probit link in section we showed how to use em to fit probit regression using a model of the form pyi where zi n xi is latent. now consider the case where the inputs xi are also unknown as in binary factor analysis. show how to fit this model using variational bayes making an qwl. hint qxi and qxiqzi approximation to the posterior of the form qx z w qwi will be gaussian and qzi will be a truncated univariate gaussian. more variational inference introduction in chapter we discussed mean field inference which approximates the posterior by a product of marginal distributions. this allows us to use different parametric forms for each variable which is particularly useful when performing bayesian inference for the parameters of statistical models as the mean and variance of a gaussian or gmm or the regression weights in a glm as we saw when we discussed variational bayes and vb-em. in this chapter we discuss a slightly different kind of variational inference. the basic idea is to minimize jq kl p where p is the exact but unnormalized posterior as before but where we no longer require q to be factorized. in fact we do not even require q to be a globally instead we only require that q is locally consistent meaning that the valid joint distribution. joint distribution of two adjacent nodes agrees with the corresponding marginals will define this more precisely below. in addition to this new kind of inference we will discuss approximate methods for map state estimation in discrete graphical models. it turns out that algorithms for solving the map problem are very similar to some approximate methods for computing marginals as we will see. loopy belief propagation algorithmic issues there is a very simple approximate inference algorithm for discrete gaussian graphical models known as loopy belief propagation or lbp. the basic idea is extremely simple we apply the belief propagation algorithm of section to the graph even if it has loops even if it is not a tree. this method is simple and efficient and often works well in practice outperforming mean field in this section we discuss the algorithm in more detail. in the next section we analyse this algorithm in terms of variational inference. a brief history when applied to loopy graphs bp is not guaranteed to give correct results and may not even converge. indeed judea pearl who invented belief propagation for trees wrote the following about loopy bp in when loops are present the network is no longer singly connected and local propagation chapter more variational inference schemes will invariably run into trouble if we ignore the existence of loops and permit the nodes to continue communicating with each other as if the network were singly connected messages may circulate indefinitely around the loops and the process may not converge to a stable equilibrium such oscillations do not normally occur in probabilistic networks which tend to bring all messages to some stable equilibrium as time goes on. however this asymptotic equilibrium is not coherent in the sense that it does not represent the posterior probabilities of all nodes of the network despite these reservations pearl advocated the use of belief propagation in loopy networks as an approximation scheme pearl personal communication and exercise in investigates the quality of the approximation when it is applied to a particular loopy belief network. however the main impetus behind the interest in bp arose when mceliece et al. showed that a popular algorithm for error correcting codes known as turbo codes et al. could be viewed as an instance of bp applied to a certain kind of graph. this was an important observation since turbo codes have gotten very close to the theoretical lower bound on coding efficiency proved by shannon. approach known as low density parity check or ldpc codes has achieved comparable performance it also uses lbp for decoding see figure for an example. in et al. lbp was experimentally shown to also work well for inference in other kinds of graphical models beyond the error-correcting code context and since then the method has been widely used in many different applications. lbp on pairwise models we now discuss how to apply lbp to an undirected graphical model with pairwise factors discuss the directed case which can involve higher order factors in the next section. the method is simple see algorithm for the pseudocode and beliefpropagation for some matlab code. we will discuss issues such as convergence and accuracy of this method shortly. just continually apply equations and until convergence. algorithm loopy belief propagation for a pairwise mrf input node potentials sxs edge potentials stxs xt initialize messages ms txt for all edges s t initialize beliefs belsxs for all nodes s repeat send message on each edge ms txt update belief of each node belsxs sxs sxs stxs xt xs until beliefs don t change significantly return marginal beliefs belsxs u nbrst mu sxs t nbrs mt sxs loopy belief propagation algorithmic issues figure a simple factor graph representation of a low-density parity check code graphs are defined in section each message bit round circle is connected to two parity factors black squares and each parity factor is connected to three bits. each parity factor has the form stuxs xt xu xt xu where is the xor operator. the local evidence factors for each hidden node are not shown. a larger example of a random ldpc code. we see that this graph is locally tree-like meaning there are no short cycles rather each cycle has length log m where m is the number of nodes. this gives us a hint as to why loopy bp works so well on such graphs. however that some error correcting code graphs have short loops so this is not the full explanation. source figure from and jordan used with kind permission of martin wainwright. lbp on a factor graph to handle models with higher-order clique potentials includes directed models where some nodes have more than one parent it is useful to use a representation known as a factor graph. we explain this representation below and then describe how to apply lbp to such models. factor graphs a factor graph et al. frey is a graphical representation that unifies directed and undirected models and which simplifies certain message passing algorithms. more precisely a factor graph is an undirected bipartite graph with two kinds of nodes. round nodes represent variables square nodes represent factors and there is an edge from each variable to every factor that mentions it. for example consider the mrf in figure if we assume one potential per maximal clique we get the factor graph in figure which represents the function f if we assume one potential per edge. we get the factor graph in figure which represents the function f chapter more variational inference figure a simple ugm. a factor graph representation assuming one potential per maximal clique. a factor graph representation assuming one potential per edge. figure a simple dgm. its corresponding factor graph. based on figure of et al. we can also convert a dgm to a factor graph just create one factor per cpd and connect that factor to all the variables that use that cpd. for example figure represents the following factorization f where we define etc. if each node has at most one parent hence the graph is a chain or simple tree then there will be one factor per edge nodes can have their prior cpds absorvbed into their children s factors. such models are equivalent to pairwise mrfs. loopy belief propagation algorithmic issues figure message passing on a bipartite factor graph. square nodes represent factors and circles represent variables. source figure of et al. used with kind permission of brendan frey. bp on a factor graph we now derive a version of bp that sends messages on a factor graph as proposed in et al. specifically we now have two kinds of messages variables to factors mx f h nbrxf and factors to variables mf xx f y mh xx y nbrf my f here nbrx are all the factors that are connected to variable x and nbrf are all the variables that are connected to factor f. these messages are illustrated in figure at convergence we can compute the final beliefs as a product of incoming messages y belx f nbrx mf xx in the following sections we will focus on lbp for pairwise models rather than for factor graphs but this is just for notational simplicity. convergence lbp does not always converge and even when it does it may converge to the wrong answers. this raises several questions how can we predict when convergence will occur? what can we do to increase the probability of convergence? what can we do to increase the rate of convergence? we briefly discuss these issues below. we then discuss the issue of accuracy of the results at convergence. chapter more variational inference d e g r e v n o c s e g a s s e m f o x p x p x p time time x p x p time time time time synchronous asynchronous no smoothing true illustration of the behavior of loopy belief propagation on an ising grid with figure random potentials wij unif c c where c for larger c inference becomes harder. percentage of messasges that have converged vs time for different update schedules dotted damped sychronous nodes converge dashed undamped asychnronous the nodes converge solid marginal beliefs of certain nodes vs time. solid straight damped asychnronous nodes converge. line truth dashed sychronous solid damped asychronous. source figure of and friedman used with kind permission of daphne koller. when will lbp converge? the details of the analysis of when lbp will converge are beyond the scope of this chapter but we briefly sketch the basic idea. the key analysis tool is the computation tree which visualizes the messages that are passed as the algorithm proceeds. figure gives a simple example. in the first iteration node receives messages from nodes and in the second iteration it receives one message from node node one from node node and two messages from node nodes and and so on. the key insight is that t iterations of lbp is equivalent to exact computation in a computation tree of height t if the strengths of the connections on the edges is sufficiently weak then the influence of the leaves on the root will diminish over time and convergence will occur. see and jordan and references therein for more information. loopy belief propagation algorithmic issues figure a simple loopy graph. the computation tree rooted at node after rounds of message passing. nodes and occur more often in the tree because they have higher degree than nodes and source figure of and jordan used with kind permission of martin wainwright. making lbp converge although the theoretical convergence analysis is very interesting in practice when faced with a model where lbp is not converging what should we do? one simple way to reduce the chance of oscillation is to use damping. that is instead of sending the message m k ts we send a damped message of the form tsxs mtsxs m k m k ts where is the damping factor clearly if this reduces to the standard scheme but for this partial updating scheme can help improve convergence. using a value such as is standard practice. the benefits of this approach are shown in figure where we see that damped updating results in convergence much more often than undamped updating. it is possible to devise methods known as double loop algorithms which are guaranteed to converge to a local minimum of the same objective that lbp is minimizing welling and teh unfortunately these methods are rather slow and complicated and the accuracy of the resulting marginals is usually not much greater than with standard lbp. oscillating marginals is sometimes a sign that the lbp approximation itself is a poor one. consequently these techniques are not very widely used. in section we will see a different convergent version of bp that is widely used. increasing the convergence rate message scheduling even if lbp converges it may take a long time. the standard approach when implementing lbp is to perform synchronous updates where all nodes absorb messages in parallel and then send out messages in parallel. that is the new messages at iteration k are computed in parallel using femk where e is the number of edges and fstm is the function that computes the message for edge s t given all the old messages. this is analogous to the jacobi method for solving linear chapter more variational inference systems of equations. it is well known that the gauss-seidel method which performs asynchronous updates in a fixed round-robin fashion converges faster when solving linear systems of equations. we can apply the same idea to lbp using updates of the form j j j i fi j imk where the message for edge i is computed using new messages k from edges earlier in the ordering and using old messages k from edges later in the ordering. this raises the question of what order to update the messages in. one simple idea is to use a fixed or random order. the benefits of this approach are shown in figure where we see that asynchronous updating results in convergence much more often than synchronous updating. a smarter approach is to pick a set of spanning trees and then to perform an up-down sweep on one tree at a time keeping all the other messages fixed. this is known as tree reparameterization et al. which should not be confused with the more sophisticated tree-reweighted bp abbreviated to trw to be discussed in section however we can do even better by using an adaptive ordering. the intuition is that we should focus our computational efforts on those variables that are most uncertain. et al. proposed a technique known as residual belief propagation in which messages are scheduled to be sent according to the norm of the difference from their previous value. that is we define the residual of new message mst at iteration k to be log rs t k log mst log mk st max msti mk sti i we can store messages in a priority queue and always send the one with highest residual. when a message is sent from s to t all of the other messages that depend on mst messages of the form mtu where u nbrt s need to be recomputed their residual is recomputed and they are added back to the queue. in et al. they showed that this method converges more often and much faster than using sychronous updating asynchronous updating with a fixed order and the trp approach. a refinement of residual bp was presented in and mccallum in this paper they use an upper bound on the residual of a message instead of the actual residual. this means that messages are only computed if they are going to be sent they are not just computed for the purposes of evaluating the residual. this was observed to be about five times faster than residual bp although the quality of the final results is similar. accuracy of lbp for a graph with a single loop one can show that the max-product version of lbp will find the correct map estimate if it converges for more general graphs one can bound the error in the approximate marginals computed by lbp as shown in et al. vinyals et al. much stronger results are available in the case of gaussian models and freeman johnson et al. bickson in particular in the gaussian case if the method converges the means are exact although the variances are not the beliefs are over confident. loopy belief propagation algorithmic issues other speedup tricks for lbp there are several tricks one can use to make bp run faster. we discuss some of them below. fast message computation for large state spaces the cost of computing each message in bp in a tree or a loopy graph is ok f where k is the number of states and f is the size of the largest factor for pairwise in many vision problems image denoising k is quite large because ugms. it represents the discretization of some underlying continuous space so ok per message is too expensive. fortunately for certain kinds of pairwise potential functions of the form stxs xt xt one can compute the sum-product messages in ok log k time using the fast fourier transform or fft as explained in and huttenlocher the key insight is that message computation is just convolution xs m k stxt xthxs v nbrst m k vs where hxs sxs if the potential function is a gaussian-like potential we can compute the convolution in ok time by sequentially convolving with a small number of box filters and huttenlocher for the max-product case a technique called the distance transform can be used to compute messages in ok time. however this only works if exp ez and where ez has one the following forms quadratic ez truncated linear ez or potts model ez c iz see and huttenlocher for details. multi-scale methods a method which is specific to lattice structures which commonly arise in computer vision is based on multi-grid techniques. such methods are widely used in numerical linear algebra where one of the core problems is the fast solution of linear systems of equations this is equivalent to map estimation in a gaussian mrf. in the computer vision context and huttenlocher suggested using the following heuristic to significantly speedup bp construct a coarse-to-fine grid compute messages at the coarse level and use this to initialize messages at the level below when we reach the bottom level just a few iterations of standard bp are required since long-range communication has already been achieved via the initialization process. the beliefs at the coarse level are computed over a small number of large blocks. the local evidence is computed from the average log-probability each possible block label assigns to all the pixels in the block. the pairwise potential is based on the discrepancy between labels of neighboring blocks taking into account their size. we can then run lbp at the coarse level and then use this to initialize the messages one level down. note that the model is still a flat grid however the initialization process exploits the multi-scale nature of the problem. see and huttenlocher for details. chapter more variational inference cascades another trick for handling high-dimensional state-spaces that can also be used with exact inference for chain-structured crfs is to prune out improbable states based on a comin fact one can create a hierarchy of models which tradeoff putationally cheap filtering step. speed and accuracy. this is called a computational cascade. in the case of chains one can guarantee that the cascade will never filter out the true map solution et al. loopy belief propagation theoretical issues we now attempt to understand the lbp algorithm from a variational point of view. our presentation is closely based on an excellent review article and jordan this paper is sometimes called the monster its own authors! in view of its length and technical difficulty. this section just sketches some of the main results. to simplify the presentation we focus on the special case of pairwise ugms with discrete variables and tabular potentials. many of the results generalize to ugms with higher-order clique potentials includes dgms but this makes the notation more complex and friedman for details of the general case. ugms represented in exponential family form we assume the distribution has the following form s v e px g z exp sxs stxs xt px ex t z where graph g has nodes v and edges e. we will drop the explicit conditioning on and g for brevity since we assume both are known and fixed. we can rewrite this in exponential family form as follows exp ex where sj stjk are all the node and edge parameters canonical parameters and jixs j xt k are all the node and edge indicator functions sufficient statistics. note we use s t v to index nodes and j k x to index states. the mean of the sufficient statistics are known as the mean parameters of the model and are given by e jspxs j xt sjs this is a vector of length d containing the node and edge marginals. it completely characterizes the distribution px so we sometimes treat as a distribution itself. equation is called the standard overcomplete representation. it is called overcomplete because it ignores the sum-to-one constraints. in some cases it is convenient to remove loopy belief propagation theoretical issues this redundancy. for example consider an ising model where xs the model can be written as s v e px z exp sxs stxsxt hence we can use the following minimal parameterization s v xsxt t e r d where d the corresponding mean parameters are s pxs and st pxs xt the marginal polytope x the space of allowable vectors is called the marginal polytope and is denoted mg where g is the structure of the graph defining the ugm. this is defined to be the set of all mean parameters for the given model that can be generated from a valid probability distribution d p s.t. mg r for example consider an ising model. px for some px if we have just two nodes connected as one can show that we have the following minimal set of constraints and we can write these in matrix-vector form as x these four constraints define a series of half-planes whose intersection defines a polytope as shown in figure since mg is obtained by taking a convex combination of the vectors it can also be written as the convex hull of the feature set mg conv dx for example for a node mrf with binary states we have mg these are the four black dots in figure we see that the convex hull defines the same volume as the intersection of half-spaces. the marginal polytope will play a crucial role in the approximate inference algorithms we discuss in the rest of this chapter. chapter more variational inference illustration of the marginal polytope for an ising model with two variables. figure cartoon illustration of the set mf which is a nonconvex inner bound on the marginal polytope mg. mf is used by mean field. cartoon illustration of the relationship between mg and lg which is used by loopy bp. the set lg is always an outer bound on mg and the inclusion mg lg is strict whenever g has loops. both sets are polytopes which can be defined as an intersection of half-planes by facets or as the convex hull of the vertices. lg actually has fewer facets than mg despite the picture. in fact lg has oxv facets where is the number of states per variable is the number of variables and is the number of edges. by contrast mg has oxv facets. on the other hand lg has more vertices than mg despite the picture since lg contains all the binary vector extreme points mg plus additional fractional extreme points. source figures and of and jordan used with kind permission of martin wainwright. exact inference as a variational optimization problem recall from section that the goal of variational inference is to find the distribution q that maximizes the energy functional lq kl log z eq px h log z where px is the unnormalized posterior. let q p then the exact energy functional becomes max mg t h if we write log px t and we where ep is a joint distribution over all state configurations x it is valid to write h since the kl divergence is zero when p q we know that max mg t h log z this is a way to cast exact inference as a variational optimization problem. equation seems easy to optimize the objective is concave since it is the sum of a linear function and a concave function figure to see why entropy is concave furthermore we are maximizing this over a convex set. however the marginal polytope mg has exponentially many facets. in some cases there is structure to this polytope that can be exploited by dynamic programming we saw in chapter but in general exact inference takes exponential time. most of the existing deterministic approximate inference schemes that have been proposed in the literature can be seen as different approximations to the marginal polytope as we explain below. loopy belief propagation theoretical issues mean field as a variational optimization problem we discussed mean field at length in chapter let us re-interpret mean field inference in our new more abstract framework. this will help us compare it to other approximate methods which we discuss below. first let f be an edge subgraph of the original graph g and let if i be the subset of sufficient statistics associated with the cliques of f let be the set of canonical parameters for the full model and define the canonical parameter space for the submodel as follows i if in other words we require that the natural parameters associated with the sufficient statistics outside of our chosen class to be zero. in the case of a fully factorized approximation we remove all edges from the graph giving for example st t e in the case of structured mean field we set st for edges which are not in our tractable subgraph. next we define the mean parameter space of the restricted model as follows mf r this is called an inner approximation to the marginal polytope since mf mg. see figure for a sketch. note that mf is a non-convex polytope which results in multiple local optima. by contrast some of the approximations we will consider later will be convex. for some d e we define the entropy of our approximation h as the entropy of the distribution defined on submodel f then we define the mean field energy functional optimization problem as follows max mf t h log z sxs sxs max p d s v xs e xsxt in the case of the fully factorized mean field approximation for pairwise ugms we can write this objective as follows stxs xt sxs txt h s s v where s p and p is the probability simplex over x mean field involves a concave objective being maximized over a non-convex set. it is typically optimized using coordinate ascent since it is easy to optimize a scalar concave function over p for each s. for example for a pairwise ugm we get sxs exp sxs exp txt stxs xt t nbrs xt lbp as a variational optimization problem in this section we explain how lbp can be viewed as a variational inference problem. chapter more variational inference figure illustration of pairwise ugm on binary nodes together with a set of pseudo marginals that are not globally consistent. a slice of the marginal polytope illustrating the set of feasible edge marginals assuming the node marginals are clamped at source figure of and jordan used with kind permission of martin wainwright. an outer approximation to the marginal polytope if we want to consider all possible probability distributions which are markov wrt our model we need to consider all vectors mg. since the set mg is exponentially large it is usually infeasible to optimize over. a standard strategy in combinatorial optimization is to relax the constraints. in this case instead of requiring probability vector to live in mg we consider a vector that only satisfies the following local consistency constraints xt sxs xs stxs xt sxs the first constraint is called the normalization constraint and the second is called the marginalization constraint. we then define the set lg holds s v and holds t e the set lg is also a polytope but it only has ov constraints. it is a convex outer approximation on mg as shown in figure we call the terms s st lg pseudo marginals since they may not correspond to marginals of any valid probability distribution. as an example of this consider figure the picture shows a set of pseudo node and edge marginals which satisfy the local consistency requirements. however they are not globally consistent. to see why note that implies implies but implies which is not possible and jordan for a formal proof. indeed figure shows that lg contains points that are not in mg. we claim that mg lg with equality iff g is a tree. to see this first consider loopy belief propagation theoretical issues an element mg. any such vector must satisfy the normalization and marginalization constraints hence mg lg. now consider the converse. suppose t is a tree and let lt by definition this satisfies the normalization and marginalization constraints. however any tree can be represented in the form p sxs stxs xt sxs txt s v e hence satsifying normalization and local consistency is enough to define a valid distribution for any tree. hence mt as well. in contrast if the graph has loops we have that mg lg. see figure for an example of this fact. the entropy approximation from equation we can write the exact entropy of any tree structured distribution mt as follows e ist st sxs log sxs hs s xs xs s v xs xt h hs s ist st stxs xt log stxs xt sxs txt note that we can rewrite the mutual information term in the form ist st hs sht t hst st and hence we get the following alternative but equivalent expression h s hst st e where ds is the degree of neighbors for node s. the approximation to the entropy is simply the use of equation even when we s v s v don t have a tree hbethe hs s we define the bethe free energy as fbethe t hbethe e ist st we define the bethe energy functional as the negative of the bethe free energy. hans bethe was a german-american physicist chapter more variational inference the lbp objective combining the outer approximation lg with the bethe approximation to the entropy we get the following bethe variational problem min lg fbethe max lg t hbethe the space we are optimizing over is a convex set but the objective itself is not concave hbethe is not concave. thus there can be multiple local optima of the bvp. the value obtained by the bvp is an approximation to log z in the case of trees the approximation is exact and in the case of models with attractive potentials the approximation turns out to be an upper bound et al. message passing and lagrange multipliers xs sxs and the marginalization constraint as ctsxs sxs in this subsection we will show that any fixed point of the lbp algorithm defines a stationary point of the above constrained objective. let us define the normalization constraint at css stxs xt for each edge t s. we can now write the lagrangian as sscss l t hbethe xt s constraint that is not explicitly enforced but one can show that it will hold at the optimum since some simple algebra then shows that l yields tsxsctsxs stxtcstxt st xs xt xt. using the fact that the marginalization con t nbrs log sxs ss sxs tsxs log stxs xt sxs txt stxs xt tsxs stxt where we have defined sxs xt straint implies sxs sxs we get log stxs xt ss tt stxs xt sxs txt utxt usxs u nbrst u nbrts sxs exp sxs stxs xt exp stxs xt sxs txt t nbrs mtsxs musxs mutxt u nbrst u nbrts to make the connection to message passing define mtsxs exp tsxs. with this notation we can rewrite the above equations taking exponents of both sides as follows extensions of belief propagation where the terms are absorbed into the constant of proportionality. we see that this is equivalent to the usual expression for the node and edge marginals in lbp. to derive an equation for the messages in terms of other messages than in terms of stxs xt sxs. then one can show ts we enforce the marginalization condition that exp stxs xt txt xt mtsxs xt mutxt u nbrts we see that this is equivalent to the usual expression for the messages in lbp. loopy bp vs mean field it is interesting to compare the naive mean field and lbp approximations. there are several obvious differences. first lbp is exact for trees whereas mf is not suggesting lbp will in general be more accurate et al. for an analysis. second lbp optimizes over node and edge marginals whereas mf only optimizes over node marginals again suggesting lbp will be more accurate. third in the case that the true edge marginals factorize so st s t the free energy approximations will be the same in both cases. what is less obvious but which nevertheless seems to be true is that the mf objective has many more local optima than the lbp objective so optimizing the mf objective seems to be harder. in particular shows empirically that optimizing mf starting from uniform or random initial conditions often leads to poor results whereas optimizing bp from uniform initial messages often leads to good results. furthermore initializing mf with the bp marginals also leads to good results mf tends to be more overconfident than bp indicating that the problem is caused not by the inaccuracy of the mf approximation but rather by the severe non-convexity of the mf objective and by the weakness of the standard coordinate descent optimization method used by however the advantage of mf is that it gives a lower bound on the partition function unlike bp which is useful when using it as a subroutine inside a learning algorithm. also mf is easier to extend to other distributions besides discrete and gaussian as we saw in chapter intuitively this is because mf only works with marginal distributions which have a single type rather than needing to define pairwise distributions which may need to have two different types. extensions of belief propagation in this section we discuss various extensions of lbp. generalized belief propagation we can improve the accuracy of loopy bp by clustering together nodes that form a tight loop. this is known as the cluster variational method. the result is a hyper-graph which is a graph et al. discusses the use of the pattern search algorithm to speedup mean field inference in the case of continuous random variables. it is possible that similar ideas could be adapted to the discrete case although there may be no reason to do this given that lbp already works well in the discrete case. chapter more variational inference figure kikuchi clusters superimposed on a lattice graph. source figure of and jordan used with kind permission of martin wainwright. corresponding hyper-graph. where there are hyper-edges between sets of vertices instead of between single vertices. note that a junction tree is a kind of hyper-graph. we can represent hyper-graph using a poset ordered set diagram where each node represents a hyper-edge and there is an arrow if see figure for an example. let t be the size of the largest hyper-edge in the hyper-graph. if we allow t to be as large as the treewidth of the graph then we can represent the hyper-graph as a tree and the method will be exact just as lbp is exact on regular trees treewidth in this way we can define a continuum of approximations from lbp all the way to exact inference. define ltg to be the set of all pseudo-marginals such that normalization and marginalization constraints hold on a hyper-graph whose largest hyper-edge is of size t for example in figure we impose constraints of the form furthermore we approximate the entropy as follows hkikuchi cghg g where hg g is the entropy of the joint distribution on the vertices in set g and cg is called the overcounting number of set g. these are related to mobious numbers in set theory. rather than giving a precise definition we just give a simple example. for the graph in figure we have g e putting these two approximations together we can define the kikuchi free as follows hkikuchi fkikuchi t hkikuchi ryoichi kikuchi is a japanese physicist. extensions of belief propagation our variational problem becomes fkikuchi max ltg min t hkikuchi ltg just as with the bethe free energy this is not a concave objective. there are several possible algorithms for finding a local optimum of this objective including a message passing algorithm known as generalized belief propagation. however the details are beyond the scope of this chapter. see e.g. and jordan sec or and friedman sec for more information. suffice it to say that the method gives more accurate results than lbp but at increased computational cost of the need to handle clusters of nodes. this cost plus the complexity of the approach have precluded it from widespread use. convex belief propagation the mean field energy functional is concave but it is maximized over a non-convex inner approximation to the marginal polytope. the bethe and kikuchi energy functionals are not concave but they are maximized over a convex outer approximation to the marginal polytope. consequently for both mf and lbp the optimization problem has multiple optima so the methods are sensitive to the initial conditions. given that the exact formulation a concave objective maximized over a convex set it is natural to try to come up with an appproximation which involves a concave objective being maximized over a convex set. we now describe one method known as convex belief propagation. this involves working with a set of tractable submodels f such as trees or planar graphs. for each model f g the entropy is higher h h since f has fewer constraints. consequently any convex combination of such subgraphs will have higher entropy too h h f f where and define the convex free energy as fconvex t h f furthermore h is a concave function of we now we define the concave energy functional as the negative of the convex free energy. we discuss how to optimize below. having defined an upper bound on the entropy we now consider a convex outerbound on the marginal polytope of mean parameters. we want to ensure we can evaluate the entropy of any vector in this set so we restrict it so that the projection of onto the subgraph g lives in the projection of m onto f lgf r d mf f f this is a convex set since each mf is a projection of a convex set. hence we define our problem as min lgf fconvex max lgf t h this is a concave objective being maximized over a convex set and hence has a unique maximum. we give a specific example below. chapter more variational inference f e b f e b f e b f e b figure a graph. some of its spanning trees. source figure of and jordan used with kind permission of martin wainwright. tree-reweighted belief propagation consider the specific case where f is all spanning trees of a graph. for any given tree the entropy is given by equation to compute the upper bound obtained by averaging over f for single nodes will just be hs since node s all trees note that the terms f but the mutual information term ist receives weight appears in every tree and st e t et known as the edge appearance probability. hence we have the following upper bound on the entropy h hs s stist st e s v the edge appearance probabilities live in a space called the spanning tree polytope. this is because they are constrained to arise from a distribution over trees. figure gives an example of a graph and three of its spanning trees. suppose each tree has equal weight under the edge f occurs in of the trees so f the edge e occurs in of the trees so e the edge b appears in all of the trees so b and so on. ideally we can find a distribution or equivalently edge probabilities in the spanning tree polytope that make the above bound as tight as possible. an algorithm to do this is described in et al. simpler approach is to generate spanning trees of g at random until all edges are covered or use all single edges with weight e what about the set we are optimizing over? we require mt for each tree t which means enforcing normalization and local consistency. since we have to do this for every tree we are enforcing normalization and local consistency on every edge. hence lgf lg. so our final optimization problem is as follows t s v max lg hs s stist st eg which is the same as the lbp objective except for the crucial st weights. so long as st for all edges t this problem is strictly concave with a unique maximum. how can we find this global optimum? as for lbp there are several algorithms but perhaps the simplest is a modification of belief propagation known as tree reweighted belief propagation expectation propagation also called trw or trbp for short. the message from t to s is now a function of all messages sent from other neighbors v to t as before but now it is also a function of the message sent from s to t. specifically exp xt mtsxs st stxs xt txt v nbrtsmvtxt vt ts at convergence the node and edge pseudo marginals are given by sxs exp sxs stxs xt stxs xt vs v nbrs v nbrstmvsxs vs st v nbrtsmvtxt vt ts stxs xt exp stxs xt sxs txt st this algorithm can be derived using a method similar to that described in section if st for all edges t e the algorithm reduces to the standard lbp algorithm. however the condition st implies every edge is present in every spanning tree with probability which is only possible if the original graph is a tree. hence the method is only equivalent to standard lbp on trees when the method is of course exact. in general this message passing scheme is not guaranteed to converge to the unique global optimum. one can devise double-loop methods that are guaranteed to converge and shashua but in practice using damped updates as in equation is often sufficient to ensure convergence. it is also possible to produce a convex version of the kikuchi free energy which one can optimize with a modified version of generalized belief propagation. see and jordan sec for details. from equation and using the fact that the trbp entropy approximation is an upper bound on the true entropy wee see that the trbp objective is an upper bound on log z. using the fact that ist hs ht hst we can rewrite the upper bound as follows cshs s log z log z t where cs sthst st st t st. s expectation propagation expectation propagation is a form of belief propagation where the messages are approximated. it is a generalization of the assumed density filtering algorithm discussed in section in that method we approximated the posterior at each step using an assumed functional form such as a gaussian. this posterior can be computed using moment matching which locally optimizes kl for a single term. from this we derived the message to send to the next time step. chapter more variational inference adf works well for sequential bayesian updating but the answer it gives depends on the order in which the data is seen. ep essentially corrects this flaw by making multiple passes over the data ep is an offline or batch inference algorithm. ep as a variational inference problem we now explain how to view ep in terms of variational inference. we follow the presentation of and jordan sec which should be consulted for further details. suppose the joint distribution can be written in exponential family form as follows px exp t t i ix exp where we have partitioned the parameters and the sufficient statistics into a tractable term of size dt and di intractable terms i each of size b. for example consider the problem of inferring an unknown vector x when the observation model is a mixture of two gaussians one centered at x and one centered at can be used to represent outliers for example. minka invented ep calls this the clutter problem. more formally we assume an observation model of the form pyx wn i n ai where w is the known mixing weight of outliers and a is the variance of the background distribution. assuming a fixed prior of the form px we can write our model in the required form as follows n pyix exp xt exp log pyix xt this matches our canonical form where exp t corresponds to exp using xxt and we set ix log pyix i and di n the exact inference problem corresponds to max t t h m where m is the set of mean parameters realizable by any probability distribution as seen through the eyes of the sufficient statistics dt r m r as it stands it is intractable to perform inference in this distribution. for example in our clutter example the posterior contains modes. but suppose we incorporate just one of the intractable terms say the i th one we will call this the i-augmented distribution di b e di px i exp t exp t i ix expectation propagation in our clutter example this becomes px i exp xt ai wn i this is tractable to compute since it is just a mixture of gaussians. fashion. first we approximate the convex set m with another larger convex set the key idea behind ep is to work with these the i-augmented distributions in an iterative l m i m i dt e and m i i r where m r b i ix. next we approximate the entropy by the following term-by-term approximation dt r hep h i h then the ep problem becomes max l t t hep optimizing the ep objective using moment matching we now discuss how to maximize the ep objective in equation let us duplicate di times to yield i the augmented set of parameters we need to optimize is now i idi r dt dt r bdi subject to the constraints that i and i i m i. let us associate a vector of lagrange multipliers i r dt with the first set of constraints. then the partial lagrangian becomes l t h i i h i i h i t t i i by solving l we can show that the corresponding distribution in m has the form qx exp it the t i terms represents an approximation to the i th intractable term using the sufficient statistics from the base distribution as we will see below. similarly by solving i il we find that the corresponding distribution in m i has the form qix i exp jt t i ix chapter more variational inference this corresponds to removing the approximation to the i th term i from the base distribution and adding in the correct i th term i. finally l just enforces the constraints that eq and i eqi are equal. in other words we get the following moment matching constraints qx qix i thus the overall algorithm is as follows. first we initialize the i. then we iterate the following to convergence pick a term i compute qi to removing the old approximation to i and adding in the new one then update the i term in q by solving the moment matching equation eqi eq that this particular optimization scheme is not guaranteed to converge to a fixed point. an equivalent way of stating the algorithm is as follows. let us assume the true distribution is given by pxd z i fix i qx z fix we approximate each fi by fi and set now we repeat the following until convergence choose a factor fi to refine. remove fi from the posterior by dividing it out q ix qx fix zi this can be implemented by substracting off the natural parameters of fi from q. compute the new posterior qnewx by solving fixq ixqnewx kl min qnewx this can be done by equating the moments of qnewx with those of qix q ixfix. the corresponding normalization constant has the form zi q ixfixdx compute the new factor that was implicitly used it can be later removed fix zi qnewx q ix expectation propagation after convergence we can approximate the marginal likelihood using pd fixdx i we will give some examples of this below which will make things clearer. ep for the clutter problem let us return to considering the clutter problem. our presentation is based on for simplicity we will assume that the prior is a spherical gaussian px n bi. also we choose to approximate the posterior by a spherical gaussian qx n vi. we set to be the prior this can be held fixed. the factor approximations will be gaussian like terms of the form fix sin vii note however that in the ep updates the variances may be negative! thus these terms should the variance is be interpreted as functions but not necessarily probability distributions. negative it means the that fi curves upwards instead of downwards. first we remove fix from qx by division which yields q ix n i v ii where v i v v m i m v iv i mi the normalization constant is given by i zi wn i i n ai next we compute qnewx by computing the mean and variance of q ixfix as follows m m i i v v i i i w zi m i i iyi dv i v i n ai v i i v i where d is the dimensionality of x and i can be interpreted as the probability that yi is not clutter. i v v i finally we compute the new factor fi whose parameters are given by v mi m i v iv i m i i v ii si zi for a handy crib sheet containing many of the standard equations needed for deriving gaussian ep algorithms see httpresearch.microsoft.comen-usumpeopleminkapapersepminka-ep-quickref.pdf. chapter more variational inference at convergence we can approximate the marginal likelihood as follows pd c mt m v mt i mi vi vi in it is shown that at least on this example ep gives better accuracy per unit of cpu time than vb and mcmc. lbp is a special case of ep we now show that loopy belief propagation is a special case of ep where the base distribution contains the node marginals and the intractable terms correspond to the edge potentials. we if there are m nodes the assume the model has the pairwise form shown in equation base distribution takes the form px m exp sxs the entropy of this distribution is simply h h s s if we add in the u v edge the uv augmented distribution has the form px uv exp sxs exp uvxu xv s v s v since this graph is a tree the exact entropy of this distribution is given by h s i uv h uv where i uv u v h uv is the mutual information. thus the ep approximation to the entropy of the full distribution is given by s hep s s h s h s uv h s h s i uv e i uv e h s s e which is precisely the bethe approximation to the entropy. expectation propagation we now show that the convex set that ep is optimizing over l given by equation is the same as the one that lbp is optimizing over lg given in equation first let us consider the set m this consists of all marginal distributions s s v realizable by a factored distribution. this is therefore equivalent to the set of all distributions which satisfy non-negativity sxs and the local normalization constraint now consider the set m uv for a single u v edge. this is equivalent to the marginal polytope mguv where guv is the graph with the single u v edge added. since this graph corresponds to a tree this set also satisfies the marginalization conditions xs uvxu xv uxu uvxu xv vxv xv xu since l is the union of such sets as we sweep over all edges in the graph we recover the same set as lg. we have shown that the bethe approximation is equivalent to the ep approximation. we now show how the ep algorithm reduces to lbp. associated with each intractable term i v will be a pair of lagrange multipliers uvxv vuxu. recalling that t sxss the base distribution in equation has the form qx exp sxs exp uvxv vuxu s sxs e exp s tsxs t n similarly the augmented distribution in equation has the form quvx qx exp uvxu xv uvxv vuxu we now need to update uxu and vxv to enforce the moment matching constraints eq equv it can be shown that this can be done by performing the usual sum-product message passing step along the u v edge both directions where the messages are given by muvxv exp uvxv and mvuxu exp vuxu. once we have updated q we can derive the corresponding messages uv and vu. the above analysis suggests a natural extension where we make the base distribution be a tree structure instead of a fully factored distribution. we then add in one edge at a time absorb its effect and approximate the resulting distribution by a new tree. this is known as tree ep and qi and is more accurate than lbp and sometimes faster. by considering other kinds of structured base distributions we can derive algorothms that outperform generalization belief propagation et al. ranking players using trueskill we now present an interesting application of ep to the problem of ranking players who compete in games. microsoft uses this method known as trueskill et al. to rank chapter more variational inference figure a dgm representing the trueskill model for players and teams where team is player team is players and and team is player we assume there are two games team vs team and team vs team nodes with double circles are deterministic. a factor graph representation of the model where we assume there are players no teams. there are games player vs player and player vs player the numbers inside circles represent steps in the message passing algorithm. expectation propagation ist i players who use the xbox live online gaming system this system process over games per day making this one of the largest application of bayesian statistics to the same method can also be applied to other games such as tennis or the basic idea is shown in figure we assume each player i has a latent or true underlying skill level si r. these skill levels can evolve over time according to a simple in any given game we define the performance dynamical model pst of player i to be pi which has the conditional distribution ppisi we then define the performance of a team to be the sum of the performance of its constituent players. for example in figure we assume team is composed of players and so we define finally we assume that the outcome of a game depends on the difference in performance levels of the two teams. for example in figure we assume where and where means team won and means team won. thus the prior probability that team wins is n ist i where n and n to simplify the presentation of the algorithm we will ignore the dynamical model and assume a common static factored gaussian prior n on the skills. also we will assume that each team consists of player so ti pi and that there can be no ties. finally we will integrate out the performance variables pi and assume leading to a final model of the form ps i n sjg signdg pdgs pygdg where ig is the first player of game g and jg is the second player. this is represented in factor graph form in in figure we have kinds of factors the prior factor fisi n the game factor hgsig sjg dg sjg and the outcome factor kgdg yg iyg signdg. since the likelihood term is not conjugate to the gaussian priors we will have to perform approximate inference. thus even when the graph is a tree we will need to iterate. there were an additional game say between player and player then the graph would no longer be a tree. we will represent all messages and marginal beliefs by gaussians. we will use the notation and v for the mean and variance moment parameters and and for the precision and precision-adjusted mean natural parameters. naive bayes classifiers which are widely used in spam filters are often described as the most common application of bayesian methods. however the parameters of such models are usually fit using non-bayesian methods such as penalized maximum likelihood. our presentation of this algorithm is based in part on lecture notes by carl rasmussen joaquin quinonero-candela available at note that this is very similar to probit regression discussed in section except the inputs are differences of latent dimensional factors. if we assume a logistic noise model instead of a gaussian noise model we recover the bradley terry model of ranking. chapter more variational inference we initialize by assuming that at iteration the initial upward messages from factors hg to variables si are uniform i.e. hg sig hg sig hg sig and similarly as illustrated in figure we give the details of these steps below. the messages passing algorithm consists of steps per game hg sjg compute the posterior over the skills variables qtsi t i hg si ncsi t mt t hg si i t i t i t hg si g g g compute the message from the skills variables down to the game factor hg mt sig hg qtsig hg sig mt mt sjg hg qtsjg hg sjg mt where the division is implemented by subtracting the natural parameters as follows t sig hg t sig t hg sig t sig hg t sig t hg sig and similarly for sjg compute the message from the game factor hg down to the difference variable dg mt hg dg sig hg sjg hg sig hg vt sig hg sjg hgdg sig n sjg t sjg hg dsjg hg dg sjg hg vt sig hg vt sjg hg hg dg vt n t n t vt hg dg vt t hg dg t sig hg t sjg hg dsjg compute the posterior over the difference variables qtdg mt hg dg dg n t n t hg dg g vt g vt hg dg signdg expectation propagation function function figure function. function. based on figure of et al. figure generated by trueskillplot. that the upward message from the kg factor is constant. we can find these parameters by moment matching as follows yg t hg dg t hg dg yg t hg dg t hg dg g yg t t hg dg t hg dg vt g vt hg dg n derivation of these equations is left as a modification to exercise these functions are plotted in figure let us try to understand these equations. suppose t hg dg is a large positive number. that means we expect based on the current estimate of the skills that dg will be large and positive. consequently if we observe yg we will not be surprised that ig is the winner which is reflected in the fact that the update factor for the hg dg similarly the update factor for the variance is small mean is small t hg dg however if we observe yg then the update factor for the mean t and variance becomes quite large. compute the upward message from the difference variable to the game factor hg mt dg hg qtdg mt dg hg g t t dg hh t hg dg t dg hh t g t hg dg compute the upward messages from the game factor to the skill variables. let us assume chapter more variational inference figure a dag representing a partial ordering of players. posterior mean plusminus standard deviation for the latent skills of each player based on games. figure generated by trueskilldemo. that ig is the winner and jg is the loser. then we have mt hg sig sjg dg hg sjg hg vt hg sig t hg sig and similarly hgdg sig t hg sig dg hg vt vt hg sig sjg hg dg hg t sjg hg n vt t mt hg sjg sjg dg hg sig hg n vt t dg hg hgdg sig t hg sjg dg hg vt t sig hg vt hg sjg sig hg vt hg sjg t hg sjg when we compute at the next iteration by combining mt with the prior factor we will see that the posterior mean of sig goes up. similarly the posterior mean of sjg goes down. hg sig it is straightforward to combine ep with adf to perform online inference which is necessary for most practical applications. let us consider a simple example of this method. we create a partial ordering of players as shown in figure we then sample some game outcomes from this graph where a map state estimation parent always beats a child. we pass this data into iterations of the ep algorithm and infer the posterior mean and variance for each player s skill level. the results are shown in figure we see that the method has correctly inferred the rank ordering of the players. other applications of ep the trueskill model was developed by researchers at microsoft. they and others have extended the model to a variety of other interesting applications including personalized ad recommendation et al. predicting click-through-rate on ads in the bing search engine et al. etc. they have also developed a general purpose bayesian inference toolbox based on ep called infer.net et al. ep has also been used for a variety of other models such as gaussian process classification and rasmussen see httpresearch.microsoft.comen-usumpeople minkapaperseproadmap.html for a list of other ep applications. map state estimation in this section we consider the problem of finding the most probable configuration of variables in a discrete-state graphical model i.e. our goal is to find a map assignment of the following form x ixi px arg max x x m f arg max x x m t arg max x x m i v f f where i are the singleton node potentials and f are the factor potentials. this section we follow the notation of et al. which considers the case of general potentials not just pairwise ones. note that the partition function z plays no role in map estimation. if the treewidth is low we can solve this problem with the junction tree algorithm but in general this problem is intractable. in this section we discuss various approximations building on the material from section linear programming relaxation we can rewrite the objective in terms of the variational parameters as follows t arg max x x m t arg max mg where jixf k and is a probability vector in the marginal polytope. to see why this equation is true note that we can just set to be a degenerate distribution with ixs x s is the optimal assigment of node s. so instead of optimizing over discrete assignments we now optimize over probability distributions s where x it seems like we have an easy problem to solve since the objective in equation is linear in and the constraint set mg is convex. the trouble is mg in general has a number of facets that is exponential in the number of nodes. a standard strategy in combinatorial optimization is to relax the constraints. in this case instead of requiring probability vector to live in the marginal polytope mg we allow it to chapter more variational inference live inside a convex outer bound lg. having defined this relaxed constraint set we have max x x m t max mg t max lg t if the solution is integral it is exact if it is fractional it is an approximation. this is called a order linear programming relaxtion. the reason it is called first-order is that the constraints that are enforced are those that correspond to consistency on a tree which is a graph of treewidth it is possible to enforce higher-order consistency using graphs with larger treewidth and jordan sec for details. how should we actually perform the optimization? we can use a generic linear programming package but this is often very slow. fortunately in the case of graphical models it is possible to devise specialised distributed message passing algorithms for solving this optimization problem as we explain below. max-product belief propagation the map objective in equation max mg t is almost identical to the inference objective in equation max mg t h apart from the entropy term. one heuristic way to proceed would be to consider the zero temperature limit of the probability distribution where the probability distribution has all its mass centered on its mode section in such a setting the entropy term becomes zero. we can then modify the message passing methods used to solve the inference problem so that they solve the map estimation problem instead. in the zero temperature limit the sum operator becomes the max operator which results in a method called max-product belief propagation. in particular in more detail let a max mg t h max mg now consider an inverse temperature going to infinity. we have lim a lim h max mg t lim h max mg t it is the concavity of the objective function that allows us to interchange the lim and max operators and jordan for details. now consider the bethe approximation which has the form max lg t hbethe we showed that loopy bp finds a local optimum of this objective. in the zero temperature limit this objective is equivalent to the lp relaxation of the map problem. unfortunately max-product loopy bp does not solve this lp relaxation unless the graph is a tree and jordan the reason is that bethe energy functional is not concave on trees so we are not licensed to swap the limit and max operators in the above zero-temperature derivation. however if we use tree-reweighted bp or trbp trw we have a concave objective. in this case map state estimation one can show and wainwright that the max-product version of trbp does solve the above lp relaxation. a certain scheduling of this algorithm known as sequential trbp trbp-s ortrw-s can be shown to always converge and furthermore it typically does so faster than the standard parallel updates. the idea is to pick an arbitrary node ordering xn we then consider a set of trees which is a subsequence of this ordering. at each iteration we perform max-product bp from towards xn and back along one of these trees. it can be shown that this monotonically minimizes a lower bound on the energy and thus is guaranteed to converge to the global optimum of the lp relaxation. graphcuts in this section we show how to find map state estimates or equivalently minimum energy configurations by using the max flowmin cut algorithm for this class of methods is known as graphcuts and is very widely used especially in computer vision applications. we will start by considering the case of mrfs with binary nodes and a restricted class of potentials in this case graphcuts will find the exact global optimum. we then consider the case of multiple states per node which are assumed to have some underlying ordering we can approximately solve this case by solving a series of binary subproblems as we will see. graphcuts for the generalized ising model st if xu xv if xu xv let us start by considering a binary mrf where the edge energies have the following form euvxu xv where st is the edge cost. this encourages neighboring nodes to have the same value we are trying to minimize energy. since we are free to add any constant we like to the overall energy without affecting the map state estimate let us rescale the local energy terms such that either or now let us construct a graph which has the same set of nodes as the mrf plus two distinguished nodes the source s and the sinkt. if we add the edge xu t with cost ensures that if u is not in partition xt meaning u is assigned to state we will pay a cost of in the cut. similarly if we add the edge xu s with cost finally for every pair of variables that are connected in the mrf we add edges xu xv and xv xu both with cost uv figure illustrates this construction for an mrf with nodes and with the following non-zero energy values having constructed the graph we compute a minimal s t cut. this is a partition of the nodes into two sets xs which are nodes connected to s and xt which are nodes connected to t. we there are a variety of ways to implement this algorithm see e.g. oev log v or ov time where e is the number of edges and v is the number of nodes. and wayne the best take chapter more variational inference t s figure illustration of graphcuts applied to an mrf with nodes. dashed lines are ones which contribute to the cost of the cut bidirected edges we only count one of the costs. here the min cut has cost source figure from and friedman used with kind permission of daphne koller. pick the partition which minimizes the sum of the cost of the edges between nodes on different sides of the partition costxsxt xu xsxv xt costxu sv in figure we see that the min-cut has cost minimizing the cost in this graph is equivalent to minimizing the energy in the mrf. hence nodes that are assigned to s have an optimal state of and the nodes that are assigned to t have an optimal state of in figure we see that the optimal map estimate is graphcuts for binary mrfs with submodular potentials we now discuss how to extend the graphcuts construction to binary mrfs with more general kinds of potential functions. in particular suppose each pairwise energy satisfies the following condition in other words the sum of the diagonal energies is less than the sum of the off-diagonal energies. in this case we say the energies are submodular and zabin an example of a submodular energy is an ising model where uv this is also known as an attractive mrf or associative mrf since the model wants neighboring states to be the same. submodularity is the discrete analog of convexity. intuitively it corresponds to the law of diminishing returns that is the extra value of adding one more element to a set is reduced if the set is already large. more formally we say that f r is submodular if for any a b s and x s we havef f f f if f is submodular then f is supermodular. map state estimation to apply graphcuts to a binary mrf with submodular potentials we construct the pairwise edge weights as follows this is guaranteed to be non-negative by virtue of the submodularity assumption. in addition we construct new local edge weights as follows first we initialize eu and then for each edge pair v we update these values as follows we now construct a graph in a similar way to before. specifically if add the edge u s with cost xu xv with cost finally for every mrf edge for which don t need to add the edge in both directions. we otherwise we add the edge u t with cost we add a graphcuts edge one can show that the min cut in this graph is the same as the minimum energy configuration. thus we can use max flowmin cut to find the globally optimal map estimate et al. graphcuts for nonbinary metric mrfs we now discuss how to use graphcuts for approximate map estimation in mrfs where each node can have multiple states et al. however we require that the pairwise energies form a metric. we call such a model a metric mrf. for example suppose the states have a natural ordering as commonly arises if they are a discretization of an underlying continuous in this case we can define a metric of the form exs xt min xt or a space. semi-metric of the form exs xt min for some constant this energy encourages neighbors to have similar labels but never punishes them by more than this term prevents over-smoothing which we illustrate in figure one version of graphcuts is the alpha expansion. at each step it picks one of the available labels or states and calls it then it solves a binary subproblem where each variable can choose to remain in its current state or to become state figure for an illustration. more precisely we define a new mrf on binary nodes and we define the energies of this new model relative to the current assignment x as follows euvxu euxu eu euv xv euvxu xv euv to optimize using graph cuts thus figure out the optimal alpha expansion move we require that the energies be submodular. plugging in the definition we get the following constraint euvxu xv uv euvxu uv xv for any distance function euv and the remaining inequality follows from the triangle inequality. thus we can apply the alpha expansion move to any metric mrf. chapter more variational inference initial labeling standard move an image with labels. figure just flips the label of one pixel. be relabeled as if this decreases the energy. labeled as to be relabeled as if this decreases the energy. used with kind permission of ramin zabih. a standard local move by iterative conditional modes an swap allows all nodes that are currently labeled as to an expansion allows all nodes that are not currently source figure of et al. at each step of alpha expansion we find the optimal move from amongst an exponentially large set thus we reach a strong local optimum of much lower energy than the local optima found by standard greedy label flipping methods such as iterative conditional modes. in fact one can show that once the algorithm has converged the energy of the resulting solution is at most times the optimal energy where c max e max euv min euv see exercise for the proof. approximation. in the case of the potts model c so we have a another version of graphcuts is the alpha-beta swap. at each step two labels are chosen call them and all the nodes currently labeled can change to vice versa if this reduces the energy figure for an illustration. the resulting binary subproblem can be solved exactly even if the energies are only semi-metric is the triangle inequality need not hold see exercise although the swap version can be applied to a broader class of models than the version it is theoretically not as powerful. indeed in various low-level vision problems et al. show empirically that the expansion version is usually better than the swap version section experimental comparison of graphcuts and bp in section we described lattice-structured crfs for various low-level vision problems. et al. performed an extensive comparison of different approximate optimization techniques for this class of problems. some of the results for the problem of stereo depth estimation are shown in figure we see that the graphcut and tree-reweighted maxproduct bp give the best results with regular max-product bp being much worse. in terms of speed graphcuts is the fastest with trw a close second. other algorithms such as icm simulated annealing or a standard domain-specific heuristic known as normalize correlation are map state estimation y g r e n e max-product bp a-expansion a-b swap trw y g r e n e max-product bp a-expansion a-b swap trw running time running time figure energy minimization on a crf for stereo depth estimation. top row two input images along with the ground truth depth values. bottom row energy vs time for different optimization algorithms. bottom left results are for the teddy image in top row. bottom right results are for the tsukuba image in figure source figure of and friedman used with kind permission of daphne koller. even worse as shown qualitatively in figure since trw is optimizing the dual of the relaxed lp problem we can use its value at convergence to evaluate the optimal energy. it turns out that for many of the images in the stereo benchmark dataset the ground truth has higher energy probability than the globally optimal estimate et al. this indicates that we are optimizing the wrong model. this is not surprising since the pairwise crf ignores known long-range constraints. unfortunately if we add these constraints to the model the graph either becomes too dense bp slow andor the potentials become non-submodular graphcuts inapplicable. one way around this is to generate a diverse set of local modes using repeated applications of graph cuts as described in et al. we can then apply a more sophisticated model which uses global features to rerank the solutions. chapter more variational inference left image labels ground truth swap algorithm expansion algorithm normalized correlation simulated annealing figure an example of stereo depth estimation using an mrf. left image of size pixels from the university of tsukuba. map estimates using different methods ground truth depth map quantized to levels. swap expansion normalized cross correlation simulated annealing. source figure of et al. used with kind permission of ramin zabih. corresponding right image is similar but not shown. dual decomposition we are interested in computing f f p max x x m i v ixi f where f represents a set of factors. we will assume that we can tractably optimize each local factor but the combination of all of these factors makes the problem intractable. one way to proceed is to optimize each term independently but then to introduce constraints that force all the local estimates of the variables values to agree with each other. we explain this in more detail below following the presentation of et al. map state estimation fxf xf xf xg xf xh hxh xh gxg xg xg xh xk xk xk kxk figure a pairwise mrf with different edge factors. we have separate variables plus a copy of each variable for each factor it participates in. source figure of et al. used with kind permission of david sontag. basic idea let us duplicate the variables xi once for each factor and then force them to be equal. i f be the set of variables used by factor f. this construction is specifically let xf illustrated in figure we can reformulate the objective as follows i xi f i f f s.t. xf ixi f f p max xxf i v f f let us now introduce lagrange multipliers or dual variables f ik to enforce these constraints. the lagrangian becomes i v ixi f f f f i f xi l x xf f f ixi xi ixf f i xi i xi this is equivalent to our original problem in the following sense for any value of we have p max xxf l x xf s.t. xf i xi f i f since if the constraints hold the last term is zero. we can get an upper bound by dropping the consistency constraints and just optimizing the following upper bound l max xxf max xi i l x xf ixi f f f ixi f max xf f i f f ixi see figure for an illustration. chapter more variational inference f f f f figure illustration of dual decomposition. kind permission of david sontag. source figure of et al. used with this objective is tractable to optimize since each xf term is decoupled. furthermore we see since by relaxing the consistency constraints we are optimizing over a larger that l p space. furthermore we have the property that l p min so the upper bound is tight at the optimal value of which enforces the original constraints. minimizing this upper bound is known as dual decomposition or lagrangian relaxation et al. sontag et al. rush and collins furthemore it can be shown that l is the dual to the same lp relaxation we saw before. we will discuss several possible optimization algorithms below. the main advantage of dual decomposition from a practical point of view is that it allows one to mix and match different kinds of optimization algorithms in a convenient way. for example we can combine a grid structured graph with local submodular factors to perform image segmentation together with a tree structured model to perform pose estimation exercise analogous methods can be used in natural language processing where we often have a mix of local and global constraints e.g. et al. rush and collins theoretical guarantees what can we say about the quality of the solutions obtained in this way? to understand this let us first introduce some more notation f ixi f ixi f f i ixi f f i f this represents a reparameterization of the original problem in the sense that map state estimation ixi i and hence l f max xi i f i f f max xf f i f i and an assignment x f in this case we have now suppose there is a set of dual variables f argmaxxf ix such that the maximizing assignments to the singleton terms agrees with the assignments to the factor terms i.e. so that x l now ix i argmaxxi i i and x f f p l f f f f i i i i f i f i f we conclude that l p sox is the map assignment. so if we can find a solution where all the subproblems agree we can be assured that it is the global optimum. this happens surprisingly often in practical problems. subgradient descent l is a convex and continuous objective but it is non-differentiable at points where or the elements of at the same time as follows i f have multiple optima. one approach is to use subgradient descent. this updates all f i t f ixi tgt f ixi one can show that the gradient is given by the following sparse vector. first let xs where gt the subgradient of l at t. tion this method is guaranteed to converge to a global optimum of the dual. et al. for details. if the step sizes t are set appropriately secsee i t f next let gf ixi for all elements. finally argmaxxi i xs if xf i and gf ixf i bringing them closer to agreement. similarly the subgradient update will decrease the value of t f to compute the gradient we need to be able to solve subproblems of the following form t i and xf i factor f disagrees with the local term on how to set variable i we set gf ixs i and increasing t i this has the effect of decreasing t f argmaxxf i xfi. i f i i xfi and increasing the value of t f t f argmax xf argmax xf t f ixi i f chapter more variational inference et al. these subproblems are called slaves whereas l is called the master. obviously if the scope of factor f is small this is simple. for example if each factor is pairwise and each variable has k states the cost is just k however there are some kinds of global factors that also support exact and efficient maximization including the following graphical models with low tree width. factors that correspond to bipartite graph matchings e.g. et al. this is useful for data association problems where we must match up a sensor reading with an unknown source. we can find the maximal matching using the so-called hungarian algorithm in time e.g. and steiglitz supermodular functions. we discuss this case in more detail in section cardinality constraints. for example we might have a factor over a large set of binary variables that enforces that a certain number of bits are turned on this can be useful in problems such as image segmentation. in particular suppose f if i f xi l and f otherwise. we can find the maximizing assignment in of log time as follows first define ei f f now sort the ei finally set xi for the first l values and xi for the rest et al. factors which are constant for all but a small set s of distinguished values of xf then we can optimize over the factor in os time et al. coordinate descent an alternative to updating the entire vector at once sparsely is to update it using block coordinate descent. by choosing the size of the blocks we can trade off convergence speed with ease of the local optimization problem. one approach which optimizes f ixi for all i f and all xi at the same time a fixed factor f is known as max product linear programming and jaakkola algorithmically this is similar to belief propagation on a factor graph. in particular we define f i as messages sent from factor f to variable i and we define i f as messages sent from variable i to factor f. these messages can be computed as follows and jaakkola for the i f ixi g ixi f ixi i f max xfi f j f j f we then set the dual variables f ixi to be the messages f ixi. for example consider a grid mrf with the following pairwise factors f and the outgoing message from factor f to variable is a note that we denote their f by i f i map state estimation function of all messages coming into f and f s local factor f f max f f f similarly the outgoing message from variable to factor f is a function of all the messages sent into variable from other connected factors this example just factor h and the local potential f the key computational bottleneck is computing the max marginals of each factor where we max out all the variables from xf except for xi i.e. we need to be able to compute the following max marginals efficiently j f hxfi xi hxfi xi f max xfi jf the difference from equation is that we are maxing over all but one of the variables. we can solve this efficiently for low treewidth graphical models using message passing we can also solve this efficiently for factors corresponding to bipartite matchings et al. or to cardinality constraints et al. however there are cases where maximizing over all the variables in a factor s scope is computationally easier than maximizing over all-but-one et al. sec for an example in such cases we may prefer to use a subgradient method. coordinate descent is a simple algorithm that is often much faster at minimizing the dual than gradient descent especially in the early iterations. it also reduces the objective monotonically and does not need any step size parameters. unfortunately it is not guaranteed to converge to the global optimum since l is convex but not strictly convex implies there may be more than one globally optimizing value. one way to ensure convergence is to replace the max function in the definition of l with the soft-max function which makes the objective strictly convex e.g. and shashua for details. in general computing x from recovering the map assignment so far we have been focussing on finding the optimal value of but what we really want is the optimal value of x is np-hard even if the lp relaxation is tight and the map assignment is unique et al. theorem troublesome cases arise when there are fractional assignments with the same optimal value as the map estimate. however suppose that each is locally decodable to x one can show than in this case the lp relaxation is unique and its solution is indeed x if many but not all of the nodes are uniquely decodable we can clamp the uniquely decodable ones to their map value and then use exact inference algorithms to figure out the optimal assignment to the remaining variables. using this method et al. was able to optimally solve various stereo vision crf estimation problems and et al. was able to optimally solve various protein side-chain structure predicition problems. another approach is to use the upper bound provided by the dual in a branch and bound i in this case we say that i has a unique maximum x search procedure exercises chapter more variational inference exercise graphcuts for map estimation in binary submodular mrfs ex. of and friedman show that using the graph construction described in section the cost of the cut is equal to the energy of the corresponding assignment up to an irrelevant constant. this exercise involves a lot of algebraic book-keeping. ex const. i ti if i and is submodular if e is a semimetric. i xi is unchanged f xi and xi exercise graphcuts for alpha-beta swap ex. of and friedman show how the optimal alpha-beta swap can be found by running min-cut on an appropriately constructed graph. more precisely a. define a set of binary variables tn such that ti means b. define an energy function over the new variables such that c. show that exercise constant factor optimality for alpha-expansion daphne koller.. let x be a pairwise metric markov random field over a graph g e. suppose that the variables are nonbinary and that the node potentials are nonnegative. let a denote the set of labels for each x x though it is not possible to find the globally optimal assignment in general the algorithm provides a method for finding assignments x that are locally optimal with respect to a large set of transformations i.e. the possible moves. despite the fact that only produces a locally optimal map assignment it is possible to prove that the energy of this assignment is within a known factor of the energy of the globally optimal solution in fact this is a special case of a more general principle that applies to a wide variety of algorithms including max-product belief propagation and more general move-making algorithms if one can prove that the solutions obtained by the algorithm are strong local minima i.e. local minima with respect to a large set of potential moves then it is possible to derive bounds on the suboptimality of these solutions and the quality of the bounds will depend on the nature of the moves considered. is a precise definition of large set of moves consider the following approach to proving the suboptimality bound for a. let x be a local minimum with respect to expansion moves. for each a let v v s i.e. the set of nodes labelled in the global minimum. let be an assignment that is equal to on v and equal to x elsewhere this is an of x. verify that e x b. building on the previous part show that e x where c maxst e and e denotes the energy of an assignment. hint. think about where agrees with x and where it agrees with max st min st exercise dual decomposition for pose segmentation daphne koller.. two important problems in computer vision are that of parsing articulated objects the human body called pose estimation and segmenting the foreground and the background called segmentation. intuitively these two problems are linked in that solving either one would be easier if the solution to the other were available. we consider solving these problems simultaneously using a joint model over human poses and foregroundbackground labels and then using dual decomposition for map inference in this model. we construct a two-level model where the high level handles pose estimation and the low level handles pixel-level background segmentation. let g be an undirected grid over the pixels. each node i v represents a pixel. suppose we have one binary variable xi for each pixel where xi means that pixel i is in the foreground. denote the full set of these variables by x map state estimation in addition suppose we have an undirected tree structure t on the parts. for each body part we have a discrete set of candidate poses that the part can be in where each pose is characterized by parameters specifying its position and orientation. candidates are generated by a procedure external to the algorithm described here. define yjk to be a binary variable indicating whether body part is in configuration k. then the full set of part variables is given by y with j j and k k where j is the total number of body parts and k is the number of candidate poses for each part. note that in order to describe a valid configuration y must satisfy the constraint that yjk for each j. suppose we have the following energy function on pixels xj ij. i e i v p assume that the ij arises from a metric based on differences in pixel intensities so this can be viewed as the energy for a pairwise metric mrf with respect to g. we then have the following energy function for parts pyp pqyp yq. since each part candidate yjk is assumed to come with a position and orientation we can compute a jki v where binary mask in the image plane. the mask assigns a value to each pixel denoted by wi jk if pixel i lies on the skeleton and decreases as we move away. we can use this to define an energy function relating the parts and the pixels y yjk wi jk. i v j in other words this energy term only penalizes the case where a part candidate is active but the pixel underneath is labeled as background. formulate the minimization of as an integer program and show how you can use dual decomposition to solve the dual of this integer program. your solution should describe the decomposition into slaves the method for solving each one and the update rules for the overall algorithm. briefly justify your design choices particularly your choice of inference algorithms for the slaves. monte carlo inference introduction so far we discussed various deterministic algorithms for posterior inference. these methods enjoy many of the benefits of the bayesian approach while still being about as fast as optimization-based point-estimation methods. the trouble with these methods is that they can be rather complicated to derive and they are somewhat limited in their domain of applicability they usually assume conjugate priors and exponential family likelihoods although see et al. for some recent extensions of mean field to more complex distributions. furthermore although they are fast their accuracy is often limited by the form of the approximation which we choose. s in this chapter we discuss an alternative class of algorithms based on the idea of monte carlo approximation which we first introduced in section the idea is very simple generate some samples from the posterior xs pxd and then use these to compute any quantity of interest such as a posterior marginal or the posterior of the difference of two quantities or the posterior predictive pyd etc. all of these quantities can be approximated by e f for some suitable function f. by generating enough samples we can achieve any desired level of accuracy we like. the main issue is how do we efficiently generate samples from a probability distribution particularly in high dimensions? in this chapter we discuss non-iterative methods for generating independent samples. in the next chapter we discuss an iterative method known as markov chain monte carlo or mcmc for short which produces dependent samples but which works well in high dimensions. note that sampling is a large topic. the reader should consult other books such as robert and casella for more information. sampling from standard distributions we briefly discuss some ways to sample from or dimensional distributions of standard form. these methods are often used as subroutines by more complex methods. using the cdf the simplest method for sampling from a univariate distribution is based on the inverse probability transform. let f be a cdf of some distribution we want to sample from and let f chapter monte carlo inference f u x figure sampling using an inverse cdf. figure generated by samplecdf. be its inverse. then we have the following result. theorem if u u is a uniform rv then f f proof. prf x pru f f to both sides f pru y y where the first line follows since f is a monotonic function and the second line follows since u is uniform on the unit interval. hence we can sample from any univariate distribution for which we can evaluate its inverse cdf as follows generate a random number u u using a pseudo random number generator e.g. et al. for details. let u represent the height up the y axis. then slide along the x axis until you intersect the f curve and then drop down and return the corresponding x value. this corresponds to computing x f see figure for an illustration. for example consider the exponential distribution exponx e x ix the cdf is f e x ix whose inverse is the quantile function f p by the above theorem if u we know that f expon furthermore since u as well we can sample from the exponential distribution by first sampling from the uniform and then transforming the results using lnu rejection sampling sampling from a gaussian method we now describe a method to sample from a gaussian. the idea is we sample uniformly from a unit radius circle and then use the change of variables formula to derive samples from a spherical gaussian. this can be thought of as two samples from a gaussian. in more detail sample uniformly and then discard pairs that do not satisfy the result will be points uniformly distributed inside the unit circle so pz iz inside circle. now define lnr xi zi for i wherer using the multivariate change of variables formula we have exp exp hence and are two independent samples from a univariate gaussian. this is known as the box-muller method. to sample from a multivariate gaussian we first compute the cholesky decomposition of its covariance matrix llt where l is lower triangular. next we sample x n i using the box-muller method. finally we set y lx this is valid since cov lcov lt l i lt rejection sampling when the inverse cdf method cannot be used one simple alternative is to use rejection sampling which we now explain. basic idea in rejection sampling we create a proposal distribution qx which satisifes m qx px for some constant m where px is an unnormalized version of px px pxzp for some possibly unknown constant zp. the function m qx provides an upper envelope for p. we then sample x qx which corresponds to picking a random x location and then we sample u u which corresponds to picking a random height location under the envelope. if u px m qx we reject the sample otherwise we accept it. see figure where the acceptance region is shown shaded and the rejection region is the white region between the shaded zone and the upper envelope. we now prove that this procedure is correct. let s u pxm qx u u pxm qx chapter monte carlo inference mqx accept region umqx px reject region target px comparison function mqx x qx x figure schematic illustration of rejection sampling. source figure of et al. rejection sampling from a ga used with kind permission of nando de freitas. distribution blue using a proposal of the form m gak red where k the curves touch at k figure generated by rejectionsamplingdemo. then the cdf of the accepted points is given by p accepted p x accepted p accepted ix u ix u sqxdudx pxdx pxdx which is the cdf of px as desired. how efficient is this method? since we generate with probability qx and accept with probability px m qx the probability of acceptance is paccept px m qx qxdx m hence we want to choose m as small as possible while still satisfying m qx px. pxdx example for example suppose we want to sample from a gamma gax x exp x iid expon and y xk then y gak for one can show that if xi non-integer shape parameters we cannot use this trick. however we can use rejection sampling this section is based on notes by ioana a. cosma available at rejection sampling fx half gaussian samples from fx ars figure idea behind adaptive rejection sampling. we place piecewise linear upper lower bounds on the log-concave density. based on figure of and wild figure generated by arsenvelope. using ars to sample from a half-gaussian. figure generated by arsdemo written by daniel eaton. using a gak distribution as a proposal where k the ratio has the form px qx gax gaxk x k exp x x exp x xk exp this ratio attains its maximum when x k. hence m ga k ga kk see figure for a plot. based on the cauchy distribution. asks you to devise a better proposal distribution application to bayesian statistics suppose we want to draw samples from the posterior p pd we can use rejection sampling with p pd as the target distribution q p as our proposal and m pd where arg max pd is the mle this was first suggested in and gelfand we accept points with probability p m q pd pd thus samples from the prior that have high likelihood are more likely to be retained in the posterior. of course if there is a big mismatch between prior and posterior will be the case if the prior is vague and the likelihood is informative this procedure is very inefficient. we discuss better algorithms later. adaptive rejection sampling we now describe a method that can automatically come up with a tight upper envelope qx to any log concave density px. the idea is to upper bound the log density with a piecewise chapter monte carlo inference linear function as illustrated in figure we choose the initial locations for the pieces based on a fixed grid over the support of the distribution. we then evaluate the gradient of the log density at these locations and make the lines be tangent at these points. since the log of the envelope is piecewise linear the envelope itself is piecewise exponential qx i i exp ix xi xi x xi where xi are the grid points. it is relatively straightforward to sample from this distribution. if the sample x is rejected we create a new grid point at x and thereby refine the envelope. as the number of grid points is increased the tightness of the envelope improves and the rejection rate goes down. this is known as adaptive rejection sampling and wild figure gives an example of the method in action. as with standard rejection sampling it can be applied to unnormalized distributions. rejection sampling in high dimensions it is clear that we want to make our proposal qx as close as possible to the target distribution px while still being an upper bound. but this is quite hard to achieve especially in high dimensions. to see this consider sampling from px pi using as a proposal qx n in d dimensions the optimum value is given by m q pd. the acceptance rate is both p and q are normalized which decreases exponentially fast with dimension. for example if q exceeds p by just then in dimensions the acceptance ratio will be about this is a fundamental weakness of rejection sampling. q i. obviously we must have q p in order to be an upper bound. in chapter we will describe mcmc sampling which is a more efficient way to sample from high dimensional distributions. sometimes this uses rejection sampling as a subroutine which is known as adaptive rejection metropolis sampling et al. importance sampling we now describe a monte carlo method known as importance sampling for approximating integrals of the form i e f basic idea the idea is to draw samples x in regions which have high probability px but also where is large. the result can be super efficient meaning it needs less samples than if we were to sample from the exact distribution px. the reason is that the samples are focussed on the important parts of space. for example suppose we want to estimate the probability of a rare event. define f e for some set e. then it is better to sample from a proposal of the form qx f than to sample from px itself. importance sampling samples from any proposal qx. it then uses these samples to estimate importance sampling the integral as follows e f px qx qxdx s wsf i qxs are the importance weights. note that unlike rejection sampling we use all how should we choose the proposal? a natural criterion is to minimize the variance of the where ws pxs the samples. estimate i s wsf now varqx eqx eqx f f i since the last term is independent of q we can ignore it. by jensen s inequality we have the following lower bound the lower bound is obtained when we use the optimal importance distribution q when we don t have a particular target function f in mind we often just try to make qx as close as possible to px. in general this is difficult especially in high dimensions but it is possible to adapt the proposal distribution to improve the approximation. this is known as adaptive importance sampling and berger handling unnormalized distributions it is frequently the case that we can evaluate the unnormalized target distribution px but not its normalization constant zp. we may also want to use an unnormalized proposal qx with possibly unknown normlization constant zq. we can do this as follows. first we evaluate e zq zp f px qx qxdx zq zp s wsf qxs is the unnormalized importance weight. we can use the same set of samples where ws pxs to evaluate the ratio zpzq as follows pxdx px qx qxdx s zp zq hence s i zq s s wsf s ws wsf ws where ws chapter monte carlo inference are the normalized importance weights. the resulting estimate is a ratio of two estimates and hence is biased. however as s we have that i i under weak assumptions e.g. and casella for details. importance sampling for a dgm likelihood weighting we now describe a way to use importance sampling to generate samples from a distribution which can be represented as a directed graphical model if we have no evidence we can sample from the unconditional joint distribution of a dgm px as follows first sample the root nodes then sample their children then sample their children etc. this is known as ancestral sampling. it works because in a dag we can always topologically order the nodes so that parents preceed children. that there is no equivalent easy method for sampling from an unconditional undirected graphical model. now suppose we have some evidence so some nodes are clamped to observed values and we want to sample from the posterior pxd. if all the variables are discrete we can use the following simple procedure perform ancestral sampling but as soon as we sample a value that is inconsistent with an observed value reject the whole sample and start again. this is known as logic sampling needless to say logic sampling is very inefficient and it cannot be applied when we have real-valued evidence. however it can be modified as follows. sample unobserved variables as before conditional on their parents. but don t sample observed variables instead we just use their observed values. this is equivalent to using a proposal of the form qx pxtxpat x t t e e where e is the set of observed nodes and x therefore give the overall sample an importance weight as follows t is the observed value for node t. we should wx px qx pxtxpat pxtxpat pxtxpat t e e t e pxtxpat this technique is known as likelihood weighting and chang shachter and peot sampling importance resampling we can draw unweighted samples from px by first using importance sampling proposal q to generate a distribution of the form ws xs s px particle filtering where ws are the normalized importance weights. we then sample with replacement from equation where the probability that we pick xs is ws. let this procedure induce a distribution denoted by p. to see that this is valid note that s ixs pxsqxs s pxsqxs s px qx qxdx ixs px ix qx qxdx ix pxdx pxdx ix px px px this is known as sampling importance resampling the result is an unweighted approximation of the form xs note that we typically take s. this algorithm can be used to perform bayesian inference in low-dimensional settings and gelfand that is suppose we want to draw samples from the posterior p pd we can use importance sampling with p pd as the unnormalized posterior and q p as our proposal. the normalized weights have the form ws pd s pd we can then use sir to sample from p p sq s p of course if there is a big discrepancy between our proposal prior and the target posterior we will need a huge number of importance samples for this technique to work reliably since otherwise the variance of the importance weights will be very large implying that most samples carry no useful information. issue will come up again in section when we discuss particle filtering. particle filtering particle filtering is a monte carlo or simulation based algorithm for recursive bayesian inference. that is it approximates the predict-update cycle described in section it is very widely used in many areas including tracking time-series forecasting online parameter learning etc. we explain the basic algorithm below. for a book-length treatment see et al. for a good tutorial see et al. or just read on. chapter monte carlo inference sequential importance sampling the basic idea is to appproximate the belief state the entire state trajectory using a weighted set of particles ws t zs where ws t is the normalized weight of sample s at time t. from this representation we can easily compute the marginal distribution over the most recent state by simply ignoring the previous parts of the trajectory fact that pf samples in the space of entire trajectories has various implications which we will discuss later. if the proposal has the form qzs we update this belief state using importance sampling. then the importance weights are given by t pzs ws qzs ws which can be normalized as follows t ws t we can rewrite the numerator recursively as follows pytztpztzt where we have made the usual markov assumptions. we will restrict attention to proposal densities of the following form so that we can grow the trajectory by adding the new state zt to the end. importance weights simplify to tzs ws t pytzs t t tzs pytzs tzs t t tzs qzs ws qzs t if we further assume that yt then we only need to keep the most recent part of the trajectory and observation sequence rather than the whole history in order to compute the new sample. in this case the weight becomes in this case the t ws ws t pytzs qzs tzs t yt t tzs t particle filtering hence we can approximate the posterior filtered density using ws t zs t as s one can show that this approaches the true posterior et al. the basic algorithm is now very simple for each old sample s propose an extension using t qztzs zs t using equation unfortunately this basic algorithm does not work very well as we discuss below. t yt and give this new particle weight ws the degeneracy problem the basic sequential importance sampling algorithm fails after a few steps because most of the particles will have negligible weight. this is called the degeneracy problem and occurs because we are sampling in a high-dimensional space fact the space is growing in size over time using a myopic proposal distribution. we can quantify the degree of degeneracy using the effective sample size defined by where w s t yt is the true weight of particle s. this quantity cannot be computed exactly since we don t know the true posterior but we can approximate it using t pzs tzs s eff s var s t t s eff if the variance of the weights is large then we are wasting our resources updating particles with low weight which do not contribute much to our posterior estimate. there are two main solutions to the degeneracy problem adding a resampling step and using a good proposal distribution. we discuss both of these in turn. the resampling step the main improvement to the basic sis algorithm is to monitor the effective sampling size and whenever it drops below a threshold to eliminate particles with low weight and then pf is sometimes called survival of the to create replicates of the surviving particles. fittest et al. in particular we generate a new set by sampling with replacement s times from the weighted distribution t ws t zs t where the probability of choosing particle j for replication is wj is sometimes called t rejuvenation. the result is an iid unweighted sample from the discrete density equation so we set the new weights to ws t this scheme is illustrated in figure chapter monte carlo inference figure illustration of particle filtering. there are a variety of algorithms for peforming the resampling step. the simplest is multi nomial resampling which computes ks mus we then make ks copies of zs t various improvements exist such as systematic resampling residual resampling and stratified sampling which can reduce the variance of the weights. all these methods take os time. see et al. for details. t ws t the overall particle filtering algorithm is summarized in algorithm that if an estimate of the state is required it should be computed before the resampling step since this will result in lower variance. algorithm one step of a generic particle filter for s s do t qztzs draw zs compute weight ws pytzs qzs tzs t t tzs t t yt t ws t ws t t t normalize weights ws compute seff if s smin then eff resample s indices wt t z z t ws t although the resampling step helps with the degeneracy problem it introduces problems of its own. in particular since the particles with high weight will be selected many times there is a loss of diversity amongst the population. this is known as sample impoverishment. in the particle filtering extreme case of no process noise if we have static but unknown parameters as part of the state space then all the particles will collapse to a single point within a few iterations. to mitigate this problem several solutions have been proposed. only resample when original bootstrap filter resampled at necessary not at every time step. every step but this is suboptimal. after replicating old particles sample new values using an mcmc step which leaves the posterior distribution invariant e.g. the resample-move algorithm in and berzuini create a kernel density estimate on top of the particles t zs ws t where is some smoothing kernel. we then sample from this smoothed distribution. this is known as a regularized particle filter et al. when performing inference on static parameters add some artificial process noise. this is undesirable other algorithms must be used for online parameter estimation e.g. et al. the proposal distribution the simplest and most widely used proposal distribution is to sample from the prior qztzs t yt pztzs t in this case the weight update simplifies to t ws ws t t this can be thought of a generate and test approach we sample values from the dynamic model and then evaluate how good they are after we see the data figure this is the approach used in the condensation algorithm stands for conditional density propagation used for visual tracking and blake however if the likelihood is narrower than the dynamical prior the sensor is more informative than the motion model which is often the case this is a very inefficient approach since most particles will be assigned very low weight. it is much better to actually look at the data yt when generating a proposal. qztzs t yt pztzs optimal proposal distribution has the following form pytztpztzs t pytzs if we use this proposal the new weight is given by tzs t t ws ws t ws t t yt t t t in fact the this proposal is optimal since for any given zs regardless of the value drawn for zs true weights var s t is zero. t the new weight ws t hence conditional on the old values z. t takes the same value t the variance of chapter monte carlo inference in general it is intractable to sample from pztzs t yt and to evaluate the integral needed to compute the predictive density pytzs t however there are two cases when the optimal proposal distribution can be used. the first setting is when zt is discrete so the integral becomes a sum. of course if the entire state space is discrete we can use an hmm filter instead but in some cases some parts of the state are discrete and some continuous. the second setting is when pztzs t yt is gaussian. this occurs when the dynamics are nonlinear but the observations are linear. see exercise for the details. in cases where the model is not linear-gaussian we may still compute a gaussian approximation to pztzs t yt using the unscented transform and use this as a proposal. this is known as the unscented particle filter der merwe et al. in more general settings we can use other kinds of data-driven proposals perhaps based on discriminative models. unlike mcmc we do not need to worry about the proposals being reversible. application robot localization consider a mobile robot wandering around an office environment. we will assume that it already has a map of the world represented in the form of an occupancy grid which just specifies whether each grid cell is empty space or occupied by an something solid like a wall. the goal is for the robot to estimate its location. this can be solved optimally using an hmm filter since we are assuming the state space is discrete. however since the number of states k is often very large the ok time complexity per update is prohibitive. we can use a particle filter as a sparse approximation to the belief state. this is known as monte carlo localization and is described in detail in et al. figure gives an example of the method in action. the robot uses a sonar range finder so it can only sense distance to obstacles. it starts out with a uniform prior reflecting the fact that the owner of the robot may have turned it on in an arbitrary location. out where you are starting from a uniform prior is called global localization. after the first scan which indicates two walls on either side the belief state is shown in the posterior is still fairly broad since the robot could be in any location where the walls are fairly close by such as a corridor or any of the narrow rooms. after moving to location the robot is pretty sure it must be in the corridor as shown in after moving to location the sensor is able to detect the end of the corridor. however due to symmetry it is not sure if it is in location i true location or location ii. is an example of perceptual aliasing which refers to the fact that different things may look the same. after moving to locations and it is finally able to figure out precisely where it is. the whole process is analogous to someone getting lost in an office building and wandering the corridors until they see a sign they recognize. in section we discuss how to estimate location and the map at the same time. application visual object tracking our next example is concerned with tracking an object this case a remote-controlled helicopter in a video sequence. the method uses a simple linear motion model for the centroid of the object and a color histogram for the likelihood model using bhattacharya distance to compare histograms. the proposal distribution is obtained by sampling from the likelihood. see et al. for further details. particle filtering path and reference poses belief at reference pose room a start room b room c belief at reference pose belief at reference pose ii i belief at reference pose belief at reference pose i ii figure with kind permission of sebastian thrun. illustration of monte carlo localization. source figure of et al. used figure shows some example frames. the system uses s particles with an effective sample size of s eff shows the belief state at frame the system has had to resample times to keep the effective sample size above the threshold of shows the belief state at frame the red lines show the estimated location of the center of the object over the last frames. shows that the system can handle visual clutter as long as it does not have the same color as the target object. shows that the system is confused between the grey of the helicopter and the grey of the building. the posterior is bimodal. the green ellipse representing the posterior mean and covariance is in between the two modes. shows that the probability mass has shifted to the wrong mode the system has lost track. shows the particles spread out over the gray building recovery of the object is very unlikely from this state using this chapter monte carlo inference figure example of particle filtering applied to visual object tracking based on color histograms. succesful tracking green ellipse is on top of the helicopter. tracker gets distracted by gray clutter in the background. see text for details. figure generated by pfcolortrackerdemo written by sebastien paris. proposal. we see that the method is able to keep track for a fairly long time despite the presence of clutter. however eventually it loses track of the object. note that since the algorithm is stochastic simply re-running the demo may fix the problem. but in the real world this is not an option. the simplest way to improve performance is to use more particles. an alternative is to perform tracking by detection by running an object detector over the image every few frames. see and ponce szeliski prince for details. rao-blackwellised particle filtering application time series forecasting in section we discussed how to use the kalman filter to perform time series forecasting. this assumes that the model is a linear-gaussian state-space model. there are many models which are either non-linear andor non-gaussian. for example stochastic volatility models which are widely used in finance assume that the variance of the system andor observation noise changes over time. particle filtering is widely used in such settings. see e.g. et al. and references therein for details. rao-blackwellised particle filtering in some models we can partition the hidden variables into two kinds qt and zt such that we can analytically integrate out zt provided we know the values of this means we only have sample and can represent parametrically. thus each particle s represents a value for qs these hybrid particles are are sometimes called distributional particles or collapsed particles and friedman sec and a distribution of the form qs the advantage of this approach is that we reduce the dimensionality of the space in which we are sampling which reduces the variance of our estimate. hence this technique is known as rao-blackwellised particle filtering or rbpf for short named after theorem the method is best explained using a specific example. rbpf for switching lg-ssms a canonical example for which rbpf can be applied is the switching linear dynamical system model discussed in section and liu doucet et al. we can represent qs using a mean and covariance matrix for each particle s where qt k. t k qs if we propose from the prior qqt kqs t ws ws t the weight update becomes ws t tk where pytqt k tk is the predictive density for the new observation yt conditioned on qt k and in the case of slds models this can be computed using the normalization k the quantity ls the history qs constant of the kalman filter equation ls tk we give some pseudo-code in algorithm step marked kfupdate refers to the kalman filter update equations in section this is known as a mixture of kalman filters. if k is small we can compute the optimal proposal distribution which is pqt qs t kyt ps t k ps ps ps t tkpqt kqs ls t pqt ls t t k chapter monte carlo inference algorithm one step of rbpf for slds using prior as proposal for s s do k pqtqs qs t k t s s ws t ws tk kfupdate s t ls t ts t yt k t s t normalize weights ws compute s if s smin then eff t ws t t eff resample s indices wt t t q q t ws t t t t where we use the following shorthand t p qs ps we then sample from pqtqs t ws ws t and give the resulting particle weight ws t tkpqt kqs ls t k since the weights of the particles in equation are independent of the new value that is actually sampled for qt we can compute these weights first and use them to decide which particles to propagate. that is we choose the fittest particles at time t using information from time t. this is called look-ahead rbpf freitas et al. in more detail the idea is this. we pass each sample in the prior through all k models to get k posteriors one per sample. the normalization constants of this process allow us to compute the optimal weights in equation we then resample s indices. finally for each old particle s that is chosen we sample one new state qs t k and use the corresponding posterior from the k possible alternative that we have already computed. the pseudo-code is shown in algorithm this method needs oks storage but has the advantage that each particle is chosen using the latest information yt. a further improvement can be obtained by exploiting the fact that the state space is discrete. hence we can use the resampling method of which avoids duplicating particles. application tracking a maneuvering target one application of slds is to track moving objects that have piecewise linear dynamics. for example suppose we want to track an airplane or missile qt can specify if the object is flying normally or is taking evasive action. this is called maneuvering target tracking. figure gives an example of an object moving in the setup is essentially the same as in section except that we add a three-state discrete markov chain which controls the rao-blackwellised particle filtering algorithm one step of look-ahead rbpf for slds using optimal proposal for s s do t s t yt k tkpqtkqs ls ls t tkpqtkqs t ws tk s t s t ws for k k do tk lk k lk normalize weights ws resample s indices wt for s do ts kfupdate s tspqt kqs t ws t t compute optimal proposal pkqs sample k pkqs t k s qs t s ws t t s tk tk s method misclassification rate mse pf rbpf time table comparison of pf an rbpf on the maneuvering target problem in figure input to the system. we define ut and set so the system will turn in different directions depending on the discrete state. figure shows the true state of the system from a sample run starting at the colored symbols denote the discrete state and the location of the symbol denotes the y location. the small dots represent noisy observations. figure shows the estimate of the state computed using particle filtering with particles where the proposal is to sample from the prior. the colored symbols denote the map estimate of the state and the location of the symbol denotes the mmse mean square error estimate of the location which is given by the posterior mean. figure shows the estimate computing using rbpf with particles using the optimal proposal distribution. a more quantitative comparison is shown in table we see that rbpf has slightly better performance although it is also slightly slower. figure visualizes the belief state of the system. in we show the distribution over the discrete states. we see that the particle filter estimate of the belief state column is not as accurate as the rbpf estimate column in the beginning although after the first few observations performance is similar for both methods. in we plot the posterior over the x locations. for simplicity we use the pf estimate which is a set of weighted samples but we could also have used the rbpf estimate which is a set of weighted gaussians. chapter monte carlo inference data pf mse rbpf mse figure particle filter estimate. nando de freitas. a maneuvering target. the colored symbols represent the hidden discrete state. rbpf estimate. figure generated by rbpfmaneuverdemo based on code by application fast slam in section we introduced the problem of simultaneous localization and mapping or slam for mobile robotics. the main problem with the kalman filter implementation is that it is cubic in the number of landmarks. however by looking at the dgm in figure we see that conditional on knowing the robot s path where qt r the landmark locations z r assume the landmarks don t move so we drop the t subscript. that is are independent. consequently we can use rbpf where we sample the robot s trajectory and we run l independent kalman filters inside each particle. this takes ol time per particle. fortunately the number of particles needed for good performance is quite small partly depends on the control exploration policy so the algorithm is essentially linear in the number of particles. this technique has the additional advantage that rao-blackwellised particle filtering truth pf error rate rbpf error rate pf t y t x p t figure belief states corresponding to figure discrete state. the system starts in state x in figure then moves to state in figure returns briefly to state then switches to state circle in figure etc. horizontal location estimate. figure generated by rbpfmaneuverdemo based on code by nando de freitas. it is easy to use sampling to handle the data association ambiguity and that it allows for other representations of the map such as occupancy grids. this idea was first suggested in and was subsequently extended and made practical in et al. who christened the technique fastslam. see rbpfslamdemo for a simple demo in a discrete grid world. exercises exercise sampling from a cauchy show how to use inverse probability transform to sample from a standard cauchy t exercise rejection sampling from a gamma using a cauchy proposal show how to use a cauchy proposal to perform rejection sampling from a gamma distribution. derive the optimal constant m and plot the density and its upper envelope. exercise optimal proposal for particle filtering with linear-gaussian measurement model consider a state-space model of the following form zt ftzt n qt yt htzt n rt derive expressions for pztzt yt and pytzt which are needed to compute the optimal variance proposal distribution. hint use bayes rule for gaussians. markov chain monte carlo inference introduction in chapter we introduced some simple monte carlo methods including rejection sampling and importance sampling. the trouble with these methods is that they do not work well in high dimensional spaces. the most popular method for sampling from high-dimensional distributions is markov chain monte carlo or mcmc. in a survey bysiam news mcmc was placed in the top most important algorithms of the century. the basic idea behind mcmc is to construct a markov chain on the state space x whose stationary distribution is the target density p of interest may be a prior or a posterior. that is we perform a random walk on the state space in such a way that the fraction of time we spend in each state x is proportional to p by drawing samples from the chain we can perform monte carlo integration wrt p we give the details below. the mcmc algorithm has an interesting history. it was discovered by physicists working on the atomic bomb at los alamos during world war ii and was first published in the open literature in et al. in a chemistry journal. an extension was published in the statistics literature in but was largely unnoticed. a special case sampling section was independently invented in in the context of ising models and was published in and geman but it was not until and smith that the algorithm became well-known to the wider statistical community. since then it has become wildly popular in bayesian statistics and is becoming increasingly popular in machine learning. it is worth briefly comparing mcmc to variational inference the advantages of variational it is deterministic is it easy to determine when to stop it often provides a lower bound on the log likelihood. the advantages of sampling are it is often easier to implement it is applicable to a broader range of models such as models whose size or structure changes depending on the values of certain variables as happens in matching problems or models without nice conjugate priors sampling can be faster than variational methods when applied to really huge models or inference are for small to medium problems it is usually faster source the reason is that sampling passes specific values of variables sets of variables whereas in variational inference we pass around distributions. thus sampling passes sparse messages whereas variational inference passes dense messages for comparisons of the two approaches see e.g. and west and articles in et al. chapter markov chain monte carlo inference gibbs sampling in this section we present one of the most popular mcmc algorithms known as gibbs physics this method is known as glauber dynamics or the heat bath method. this is the mcmc analog of coordinate descent. basic idea the idea behind gibbs sampling is that we sample each variable in turn conditioned on the values of all the other variables in the distribution. that is given a joint sample xs of all the variables we generate a new sample by sampling each component in turn based on the most recent values of the other variables. for example if we have d variables we use xs xs this readily generalizes to d variables. if xi is a visible variable we do not sample it since its value is already known. the expression pxix i is called the full conditional for variable i. in general xi may only depend on some of the other variables. if we represent px as a graphical model we can infer the dependencies by looking at i s markov blanket which are its neighbors in the graph. thus to sample xi we only need to know the values of i s neighbors. in this sense gibbs sampling is a distributed algorithm. however it is not a parallel algorithm since the samples must be generated sequentially. for reasons that we will explain in section it is necessary to discard some of the initial samples until the markov chain has burned in or entered its stationary distribution. we discuss how to estimate when burnin has occured in section in the examples below we just discard the initial of the samples for simplicity. example gibbs sampling for the ising model in section we applied mean field to an ising model. here we apply gibbs sampling. gibbs sampling in pairwise mrfcrf takes the form pxtx t stxs xt s nbrt in the case of an ising model with edge potentials xt expjxsxt where xt josiah willard gibbs was an american physicist. gibbs sampling sample gibbs sample gibbs mean after sweeps of gibbs figure example of image denoising. we use an ising prior with wij j and a gaussian noise model with we use gibbs sampling to perform approximate inference. sample from the posterior after one sweep over the image. sample after sweeps. posterior mean computed by averaging over sweeps. compare to figure which shows the results of using mean field inference. figure generated by isingimagedenoisedemo. the full conditional becomes pxt t s nbrt stxt xs s nbrt xs s nbrt xs s nbrt xs expj expj s nbrt xs s nbrt xs exp j expj t expj t exp j t t s nbrt xt and sigmu e u is the sigmoid where j is the coupling strength t function. it is easy to see that t xtat dt where at is the number of neighbors that agree with the same sign as t and dt is the number of neighbors who disagree. if this number is equal the forces on xt cancel out so the full conditional is uniform. observation model we have txt n the full conditional becomes we can combine an ising prior with a local evidence term t. for example with a gaussian pxt t y expj t expj t exp j t t t log t sigm now the probability of xt entering each state is determined both by compatibility with its neighbors ising prior and compatibility with the data local likelihood term. see figure for an example of this algorithm applied to a simple image denoising problem. the results are similar to mean field except that the final estimate on averaging the samples is somewhat blurrier due to the fact that mean field tends to be over-confident. chapter markov chain monte carlo inference example gibbs sampling for inferring the parameters of a gmm it is straightforward to derive a gibbs sampling algorithm to fit a mixture model especially if we use conjugate priors. we will focus on the case of mixture of gaussians although the results are easily extended to other kinds of mixture models. derivation which follows from the results of section is much easier than the corresponding variational bayes algorithm in section suppose we use a semi-conjugate prior. then the full joint distribution is given by px z xz p kp k kn k kizik dir n we use the same prior for each mixture component. the full conditionals are as follows. for the discrete indicators we have pzi kxi kn k k for the mixing weights we have results from section p dir k izi kk for the means we have results from section k p k k z x kmk vk nk v v mk vk nk izi k xk izi kxi k k nkxk v nk for the covariances we have results from section p k k z x iw ksk k izi kxi kxi kt sk k nk see gaussmissingfitgibbs for some matlab code. values for x if necessary. code can also sample missing gibbs sampling label switching although it is simple to implement gibbs sampling for mixture models has a fundamental weakness. the problem is that the parameters of the model and the indicator functions z are unidentifiable since we can arbitrarily permute the hidden labels without affecting the likelihood section consequently we cannot just take a monte carlo average of the samples to compute posterior means since what one sample considers the parameters for cluster may be what another sample considers the parameters for cluster indeed if we could average over all modes we would find e kd is the same for all k a symmetric prior. this is called the label switching problem. this problem does not arise in em or vbem which just lock on to a single mode. however it arises in any method that visits multiple modes. in problems one can try to prevent this problem by introducing constraints on the parameters to ensure identifiability e.g. and green however this does not always work since the likelihood might overwhelm the prior and cause label switching anyway. furthermore this technique does not scale to higher dimensions. another approach is to post-process the samples by searching for a global label permutation to apply to each sample that minimizes some loss function however this can be slow. perhaps the best solution is simply to not ask questions that cannot be uniquely identified. for example instead of asking for the probability that data point i belongs to cluster k ask for the probability that data points i and j belong to the same cluster. the latter question is invariant to the labeling. furthermore it only refers to observable quantities i and j grouped together or not rather than referring to unobservable quantities such as latent clusters. this approach has the further advantage that it extends to infinite mixture models discussed in section where k is unbounded in such models the notion of a hidden cluster is not well defined but the notion of a partitioning of the data is well defined collapsed gibbs sampling in some cases we can analytically integrate out some of the unknown quantities and just sample the rest. this is called a collapsed gibbs sampler and it tends to be much more efficient since it is sampling in a lower dimensional space. more precisely suppose we sample z and integrate out thus the parameters do not participate in the markov chain consequently we can draw conditionally independent samples s p which will have much lower variance than samples drawn from the joint state space et al. this process is called rao-blackwellisation named after the following theorem theorem let z and be dependent random variables and f be some scalar function. then varz varz this theorem guarantees that the variance of the estimate created by analytically integrating out will always be lower rather will never be higher than the variance of a direct mc in collapsed gibbs we sample z with integrated out the above rao-blackwell estimate. theorem still applies in this case et al. chapter markov chain monte carlo inference zn xn zi xi k zn xn zi xi figure a mixture model. after integrating out the parameters. we will encounter rao-blackwellisation again in section although it can reduce statistical variance it is only worth doing if the integrating out can be done quickly otherwise we will not be able to produce as many samples per second as the naive method. we give an example of this below. example collapsed gibbs for fitting a gmm consider a gmm with a fully conjugate prior. in this case we can analytically integrate out the model parameters k k and and just sample the indicators z. once we integrate out all the zi nodes become inter-dependent. similarly once we integrate out k all the xi nodes become inter-dependent as shown in figure nevertheless we can easily compute the full conditionals as follows pzi kz i x pzi kz i k z i pzi kz i i zi k z i px izi k z i pzi kz i i zi k z i where are the hyper-parameters for the class-conditional densities. the first term can be obtained by integrating out suppose we use a symmetric prior of the form dir where k from equation we have zn gibbs sampling hence pzi kz i where nk i x pz i i i izn k k and where we exploited the fact that i nk i n to obtain the second term in equation which is the posterior predictive distribution for xi given all the other data and all the assignments we use the fact that pxix i z i zi k pxid ik where d ik zj k j i is all the data assigned to cluster k except for xi. if we use a conjugate prior for k we can compute pxid ik in closed form. furthermore we can efficiently update these predictive likelihoods by caching the sufficient statistics for each cluster. to compute the above expression we remove xi s statistics from its current cluster zi and then evaluate xi under each cluster s posterior predictive. once we have picked a new cluster we add xi s statistics to this new cluster. some pseudo-code for one step of the algorithm is shown in algorithm based on update the nodes in random order to improve the mixing time as suggested in and sahu we can initialize the sample by sequentially sampling from fmgibbs for some matlab code by yee-whye teh. in the case of gmms both the naive sampler and collapsed sampler take on kd time per step. algorithm collapsed gibbs sampler for a mixture model for each i in random order do remove xi s sufficient statistics from old cluster zi for each k do compute pkxi pxixj zj k j i compute pzi kz id i sample zi pzi add xi s sufficient statistics to new cluster zi a comparison of this method with the standard gibbs sampler is shown in figure the vertical axis is the data log probability at each iteration computed using log pdz log zi pxi zi to compute this quantity using the collapsed sampler we have to sample given the data and the current assignment z. in figure we see that the collapsed sampler does indeed generally work better than the vanilla sampler. occasionally however both methods can get stuck in poor local modes. chapter markov chain monte carlo inference x p g o l standard gibbs sampler rao blackwellized sampler iteration x p g o l standard gibbs sampler rao blackwellized sampler iteration figure comparison of collapsed and vanilla gibbs sampling for a mixture of k twodimensional gaussians applied to n data points in figure we plot log probability of the data vs iteration. different random initializations. logprob averaged over different random initializations. solid line is the median thick dashed in the and quantiles and thin dashed are the and quintiles. source figure of used with kind permission of erik sudderth. e r o c s h t a m e p o s l ses sample size e r o c s h t a m ses figure least squares regression lines for math scores vs socio-economic status for schools. plot of slope vs nj size for the population mean estimate is in bold. schools. the extreme slopes tend to correspond to schools with smaller sample sizes. predictions from the hierarchical model. population mean is in bold. based on figure of figure generated by multilevellinregdemo written by emtiyaz khan. that the error bars in figure are averaged over starting values whereas the theorem refers to mc samples in a single run. gibbs sampling for hierarchical glms often we have data from multiple related sources. if some sources are more reliable andor data-rich than others it makes sense to model all the data simultaneously so as to enable the borrowing of statistical strength. one of the most natural way to solve such problems is to use hierarchical bayesian modeling also called multi-level modeling. in section we discussed a way to perform approximate inference in such models using variational methods. here we discuss how to use gibbs sampling. to explain the method consider the following example. suppose we have data on students gibbs sampling w w wj yij xij nj j figure multi-level model for linear regression. in different schools. such data is naturally modeled in a two-level hierarchy we let yij be the response variable we want to predict for student i in school j. this prediction can be based on school and student specific covariates xij. since the quality of schools varies we want to use a separate parameter for each school. so our model becomes yij xt ijwj we will illustrate this model below using a dataset from where xij is the socio-economic status of student i in school y and yij is their math score. we could fit each wj separately but this can give poor results if the sample size of a given school is small. this is illustrated in figure which plots the least squares regression line estimated separately for each of the j schools. we see that most of the slopes are positive but there are a few errant cases where the slope is negative. it turns out that the lines with extreme slopes tend to be in schools with small sample size as shown in figure thus we may not necessarily trust these fits. we can get better results if we construct a hierarchical bayesian model in which the wj are assumed to come from a common prior wj n w w. this is illustrated in figure in this model the schools with small sample size borrow statistical strength from the schools with larger sample size because the wj s are correlated via the latent common parents w w. is crucial that these hyper-parameters be inferrred from data if they were fixed constants the wj would be conditionally independent and there would be no information sharing between them. to complete the model specification we must specify priors for the shared parameters. fol lowing we will use the following semi-conjugate forms for convenience w n w iw s ig given this it is simple to show that the full conditionals needed for gibbs sampling have the chapter markov chain monte carlo inference following forms. for the group-specific weights pwjdj j j j xt j xj j j xt j yj for the overall mean p w n n j n v n n j wj. for the overall covariance j where w j p w w s j wwj wt s for the noise variance j p ig n wt j applying gibbs sampling to our hierarchical model we get the results shown in figure the light gray lines plot the mean of the posterior predictive distribution for each school e xt ij wj where wj e s ws j the dark gray line in the middle plots the prediction using the overall mean parameters xt ij w. we see that the method has regularized the fits quite nicely without enforcing too much amount of shrinkage is controlled by w which in turns depends on the uniformity. hyper-parameters in this example we used vague values. bugs and jags one reason gibbs sampling is so popular is that it is possible to design general purpose software that will work for almost any model. this software just needs a model specification usually in the form a directed graphical model in a file or created with a graphical user interface and a library of methods for sampling from different kinds of full conditionals. can often be done using adaptive rejection sampling described in section an example gibbs sampling of such a package is bugs et al. which stands for bayesian updating using gibbs sampling bugs is very widely used in biostatistics and social science. another more recent but very similar package is jags which stands for just another gibbs sampler this uses a similar model specification language to bugs. for example we can describe the model in figure as follows model for in for in yij dnormy.hatij tau.y y.hatij inprodwj xi j tau.y powsigma.y sigma.y for in wj dmnormmu sigmainv sigmainv mu we can then just pass this model to bugs or jags which will generate samples for us. see the webpages for details. although this approach is appealing unfortunately it can be much slower than using handwritten code especially for complex models. there has been some work on automatically deriving model-specific optimized inference code and schumann but fast code still typically requires human expertise. the imputation posterior algorithm the imputation posterior or ip algorithm and wong is a special case of gibbs sampling in which we group the variables into two classes hidden variables z and parameters this should sound familiar it is basically an mcmc version of em where the e step gets replaced by the i step and the m step gets replaced the p step. this is an example of a more general strategy called data augmentation whereby we introduce auxiliary variables in order to simplify the posterior computations the computation of p see van dyk and meng for more information. blocking gibbs sampling gibbs sampling can be quite slow since it only updates one variable at a time single site updating. if the variables are highly correlated it will take a long time to move away from the current state. this is illustrated in figure where we illustrate sampling from a gaussian exercise for the details. if the variables are highly correlated the algorithm chapter markov chain monte carlo inference figure illustration of potentially slow sampling when using gibbs sampling for a skewed gaussian. based on figure of figure generated by gibbsgaussdemo. will move very slowly through the state space. in particular the size of the moves is controlled by the variance of the conditional distributions. if this is in the direction and the support of the distribution is l along this dimension then we need steps to obtain an independent sample. in some cases we can efficiently sample groups of variables at a time. this is called blocking gibbs sampling or blocked gibbs sampling et al. wilkinson and yeung and can make much bigger moves through the state space. metropolis hastings algorithm although gibbs sampling is simple it is somewhat restricted in the set of models to which it can be applied. for example it is not much help in computing pwd for a logistic regression model since the corresponding graphical model has no useful markov structure. in addition gibbs sampling can be quite slow as we mentioned above. fortunately there is a more general algorithm that can be used known as the metropolis hastings or mh algorithm which we describe below. basic idea the basic idea in mh is that at each step we propose to move from the current state x to a with probability where q is called the proposal distribution called new state the kernel. the user is free to use any kind of proposal they want subject to some conditions which we explain below. this makes mh quite a flexible method. a commonly used proposal is a symmetric gaussian distribution centered on the current state this is called a random walk metropolis algorithm. we discuss how to choose in section if we use a proposal of the form where the new state is independent of the old state we get a method known as the independence sampler which is similar to importance sampling having proposed a move to we then decide whether to accept this proposal or not according to some formula which ensures that the fraction of time spent in each state is proportional to p otherwise the new state if the proposal is accepted the new state is metropolis hastings algorithm is the same as the current state x we repeat the sample. if the proposal is symmetric so following formula p p r the acceptance probability is given by the is more probable than x we definitely move there p we see that if p but if is less probable we may still move there anyway depending on the relative probabilities. so instead of greedily moving to only more probable states we occasionally allow downhill moves to less probable states. in section we prove that this procedure ensures that the fraction of time we spend in each state x is proportional to p if the proposal is asymmetric so we need the hastings correction given by the following r p p p p this correction is needed to compensate for the fact that the proposal distribution itself than just the target distribution might favor certain states. know the target density up to a normalization constant. in particular suppose p where px is an unnormalized distribution and z is the normalization constant. then an important reason why mh is a useful algorithm is that when evaluating we only need to z px pxz so the z s cancel. hence we can sample from p have to do is evaluate p pointwise where px p even if z is unknown. the overall algorithm is summarized in algorithm in particular all we gibbs sampling is a special case of mh it turns out that gibbs sampling which we discussed in section is a special case of mh. in particular it is equivalent to using mh with a sequence of proposals of the form ix i x i that is we move to a new state where xi is sampled from its full conditional but x i is left unchanged. we now prove that the acceptance rate of each such proposal is so the overall algorithm also has an acceptance rate of we have pxix ipx i ix i ix ipx ipxix i ix i pxix ipx chapter markov chain monte carlo inference algorithm metropolis hastings algorithm initialize for s do define x xs sample compute acceptance probability compute r sample u u set new sample to xs if u r if u r where we exploited the fact that i x i and that ix i. the fact that the acceptance rate is does not necessarily mean that gibbs will converge rapidly since it only updates one coordinate at a time section fortunately there are many other kinds of proposals we can use as we discuss below. proposal distributions for a given target distribution p a proposal distribution q is valid or admissible if it gives a non-zero probability of moving to the states that have non-zero probability in the target. formally we can write this as xsuppq suppp for example a gaussian random walk proposal has non-zero probability density on the entire state space and hence is a valid proposal for any continuous state space. of course in practice it is important that the proposal spread its probability mass in just the right way. figure shows an example where we use mh to sample from a mixture of two gaussians using a random walk proposal v. this is a somewhat tricky target distribution since it consists of two well separated modes. it is very important to set the variance of the proposal v correctly if the variance is too low the chain will only explore one of the modes as shown in figure but if the variance is too large most of the moves will be rejected and the chain will be very sticky i.e. it will stay in the same state for a long time. this is evident from the long stretches of repeated values in figure if we set the proposal s variance just right we get the trace in figure where the samples clearly explore the support of the target distribution. we discuss how to tune the proposal below. one big advantage of gibbs sampling is that one does not need to choose the proposal metropolis hastings algorithm mh with proposal mh with proposal iterations samples iterations samples mh with proposal iterations samples figure an example of the metropolis hastings algorithm for sampling from a mixture of two gaussians using a gaussian proposal with variances of v when v the chain gets trapped near the starting state and fails to sample from the mode at when v the chain is very sticky so its effective sample size is low reflected by the rough histogram approximation at the end. using a variance of v is just right and leads to a good approximation of the true distribution in red. figure generated by mcmcgmmdemo. based on code by christophe andrieu and nando de freitas. distribution and furthermore the acceptance rate is of course a acceptance can trivially be achieved by using a proposal with variance we start at a mode but this is obviously not exploring the posterior. so having a high acceptance is not the ultimate goal. we can increase the amount of exploration by increasing the variance of the gaussian kernel. often one experiments with different parameters until the acceptance rate is between and which theory suggests is optimal at least for gaussian target distributions. these short initial runs used to tune the proposal are called pilot runs. chapter markov chain monte carlo inference w intercept slope figure joint posterior of the parameters for logistic regression when applied to some sat data. marginal for the offset marginal for the slope we see that the marginals do not capture the fact that the parameters are highly correlated. figure generated by logregsatmhdemo. gaussian proposals if we have a continuous state space the hessian h at a local mode w can be used to define the covariance of a gaussian proposal distribution. this approach has the advantage that the hessian models the local curvature and length scales of each dimension this approach therefore avoids some of the slow mixing behavior of gibbs sampling shown in figure there are two obvious approaches an independence proposal n w h or a random walk proposal n where is a scale factor chosen to facilitate rapid mixing. and rosenthal prove that if the posterior is gaussian the asymptotically optimal value is to use where d is the dimensionality of w this results in an acceptance rate of for example consider mh for binary logistic regression. from equation we have that the hessian of the log-likelihood is hl xt dx where d diag i and i sigm wt xi. if we assume a gaussian prior pw n we have h v hl so the asymptotically optimal gaussian proposal has the form n w d v xt dx see rossi et al. fruhwirth-schnatter and fruhwirth for further details. the approach is illustrated in figure where we sample parameters from a logistic regression model fit to some sat data. we initialize the chain at the mode computed using irls and then use the above random walk metropolis sampler. if you cannot afford to compute the mode or its hessian xdx an alternative approach suggested in is to approximate the above proposal as follows n w v xt x metropolis hastings algorithm mixture proposals if one doesn t know what kind of proposal to use one can try a mixture proposal which is a convex combination of base proposals where wk are the mixing weights. as long as each qk is individually valid the overall proposal will also be valid. data-driven mcmc the most efficient proposals depend not just on the previous hidden state but also the visible data i.e. they have the form this is called data-driven mcmc e.g. and zhu to create such proposals one can sample pairs from the forwards model and then train a discriminative classifier to predict pxf where f are some features extracted from the visible data. typically x is a high-dimensional vector position and orientation of all the limbs of a person in a visual object detector so it is hard to predict the entire state vector pxf instead we might train a discriminative detector to predict parts of the state-space pxkfkd such as the location of just the face of a person. we can then use a proposal of the form kfkd k where is a standard data-independent proposal random walk and qk updates the k th component of the state space. for added efficiency the discriminative proposals should suggest joint changes to multiple variables but this is often hard to do. the overall procedure is a form of generate and test the discriminative proposals generate new hypotheses which are then tested by computing the posterior ratio pxd to see if the new hypothesis is better or worse. by adding an annealing step one can modify the algorithm to find posterior modes this is called simulated annealing and is described in section one advantage of using the mode-seeking version of the algorithm is that we do not need to ensure the proposal distribution is reversible. adaptive mcmc one can change the parameters of the proposal as the algorithm is running to increase efficiency. this is called adaptive mcmc. this allows one to start with a broad covariance allowing large moves through the space until a mode is found followed by a narrowing of the covariance to ensure careful exploration of the region around the mode. however one must be careful not to violate the markov property thus the parameters of the proposal should not depend on the entire history of the chain. it turns out that a sufficient condition to ensure this is that the adaption is faded out gradually over time. see e.g. and thoms for details. chapter markov chain monte carlo inference initialization and mode hopping it is necessary to start mcmc in an initial state that has non-zero probability. if the model has deterministic constraints finding such a legal configuration may be a hard problem in itself. it is therefore common to initialize mcmc methods at a local mode found using an optimizer. in some domains with discrete state spaces it is a more effective use of computation time to perform multiple restarts of an optimizer and to average over these modes rather than exploring similar points around a local mode. however in continuous state spaces the mode contains negligible volume so it is necessary to locally explore around each mode in order to visit enough posterior probability mass. why mh works to prove that the mh procedure generates samples from p chain theory so be sure to read section first. we have to use a bit of markov the mh algorithm defines a markov chain with the following transition matrix qxx if x otherwise this follows from a case analysis if you move to from x you must have proposed it probability and it must have been accepted probability otherwise you stay in state x either because that is what you proposed probability qxx or because you proposed something else probability but it was rejected probability let us analyse this markov chain. recall from section that a chain satisfies detailed balance if we also showed that if a chain satisfies detailed balance then p is its stationary distribution. our goal is to show that the mh algorithm defines a transition function that satisfies detailed balance and hence that p is its stationary distribution. equation holds we say that p is an invariant distribution wrt the markov transition kernel q. theorem if the transition matrix defined by the mh algorithm by equation is ergodic and irreducible then p proof. consider two states x and is its unique limiting distribution. p either p or p p we will ignore ties occur with probability zero for continuous distributions. without loss p of generality assume that p p p hence metropolis hastings algorithm hence we have and now to move fromx to we must first propose p p hence p p and then accept it. hence p p the backwards probability is since inserting this into equation we get p p so detailed balance holds wrt p is a stationary distribution. furthermore from theorem this distribution is unique since the chain is ergodic and irreducible. hence from theorem p reversible jump mcmc suppose we have a set of models with different numbers of parameters e.g. mixture models in which the number of mixture components is unknown. let the model be denoted by m and let its unknowns parameters be denoted by xm xm xm r nm where nm is the dimensionality of model m. sampling in spaces of differing dimensionality is called transdimensional mcmc we could sample the model indicator m m and xm but this is very inefficient. it is sample all the parameters from the product space xm where we only worry more parsimonious to sample in the union space x m about parameters for the currently active model. the difficulty with this approach arises when we move between models of different dimensionality. the trouble is that when we compute the mh acceptance ratio we are comparing densities defined in different dimensionality spaces which is meaningless. it is like trying to compare a sphere with a circle. the solution proposed by and known as reversible jump mcmc or rjmcmc is to augment the low dimensional space with extra random variables so that the two spaces have a common measure. unfortunately we do not have space to go into details here. suffice it to say that the method can be made to work in theory although it is a bit tricky in practice. if however the continuous parameters can be integrated out in a method called collapsed rjmcmc much of the difficulty goes away since we are just left with a discrete state space where there is no need to worry about change of measure. for example et al. includes many examples of applications of collapsed rjmcmc applied to bayesian inference fro adaptive basis-function models. they sample basis functions from a fixed set of candidates centered on the data points and integrate out the other parameters analytically. this provides a bayesian alternative to using rvms or svms. chapter markov chain monte carlo inference initial condition x initial condition x figure illustration of convergence to the uniform distribution over using a symmetric random walk starting from state and state based on figures and of figure generated by speed and accuracy of mcmc in this section we discuss a number of important theoretical and practical issues to do with mcmc. the burn-in phase we start mcmc from an arbitrary initial state. as we explained in section only when the chain has forgotten where it started from will the samples be coming from the chain s stationary distribution. samples collected before the chain has reached its stationary distribution do not come from p and are usually thrown away. the initial period whose samples will be ignored is called the burn-in phase. for example consider a uniform distribution on the integers suppose we sample from this using a symmetric random walk. in figure we show two runs of the algorithm. on the left we start in state on the right we start in state even in this small problem it takes over steps until the chain has forgotten where it started from. it is difficult to diagnose when the chain has burned in an issue we discuss in more detail below. is one of the fundamental weaknesses of mcmc. as an interesting example of what can happen if you start collecting samples too early consider the potts model. figure shows a sample after iterations of gibbs sampling. this suggests that the model likes speed and accuracy of mcmc figure illustration of problems caused by poor mixing. one sample from a potts model on a grid with nearest neighbor connectivity and j in and geman after iterations. one sample from the same model after iterations. used with kind permission of erik sudderth. medium-sized regions where the label is the same implying the model would make a good prior for image segmentation. indeed this was suggested in the original gibbs sampling paper and geman however it turns out that if you run the chain long enough you get isolated speckles as in figure the results depend on the coupling strength but in general it is very hard to find a setting which produces nice medium-sized blobs most parameters result in a few super-clusters or lots of small fragments. in fact there is a rapid phase transition between these two regimes. this led to a paper called the isingpotts model is not well suited to segmentation tasks et al. it is possible to create priors more suited to image segmentation and jordan but the main point here is that sampling before reaching convergence can lead to erroneous conclusions. mixing rates of markov chains the amount of time it takes for a markov chain to converge to the stationary distribution and forget its initial state is called the mixing time. more formally we say that the mixing time from state is the minimal time such that for any constant we have that mint t p where is a distribution with all its mass in state t is the transition matrix of the chain depends on the target p and the proposal q and t is the distribution after t steps. the mixing time of the chain is defined as max the mixing time is determined by the eigengap which is the difference of the chapter markov chain monte carlo inference figure a markov chain with low conductance. the dotted arcs represent transitions with very low probability. source figure of and friedman used with kind permission of daphne koller. first and second eigenvalues of the transition matrix. in particular one can show that o log n where n is the number of states. since computing the transition matrix can be hard to do especially for high dimensional andor continuous state spaces it is useful to find other ways to estimate the mixing time. an alternative approach is to examine the geometry of the state space. for example consider the chain in figure we see that the state space consists of two islands each of which is connected via a narrow bottleneck they were completely disconnected the chain would not be ergodic and there would no longer be a unique stationary distribution. we define the conductance of a chain as the minimum probability over all subsets of states of transitioning from that set to its complement x sc t p min p one can show that o log n hence chains with low conductance have high mixing time. for example distributions with well-separated modes usually have high mixing time. simple mcmc methods often do not work well in such cases and more advanced algorithms such as parallel tempering are necessary e.g. practical convergence diagnostics computing the mixing time of a chain is in general quite difficult since the transition matrix is usually very hard to compute. in practice various heuristics have been proposed to diagnose speed and accuracy of mcmc convergence see cowles and carlin brooks and roberts for a review. strictly speaking these methods do not diagnose convergence but rather non-convergence. that is the method may claim the chain has converged when in fact it has not. this is a flaw common to all convergence diagnostics since diagnosing convergence is computationally intractable in general et al. one of the simplest approaches to assessing when the method has converged is to run multiple chains from very different overdispersed starting points and to plot the samples of some variables of interest. this is called a trace plot. if the chain has mixed it should have forgotten where it started from so the trace plots should converge to the same distribution and thus overlap with each other. figure gives an example. we show the traceplot for x which was sampled from a mixture of two gaussians using four different methods mh with a symmetric gaussian proposal of variance and gibbs sampling. we see that has not mixed which is also evident from figure which shows that a single chain never leaves the area where it started. the results for the other methods indicate that the chains rapidly converge to sticky nature of the the stationary distribution no matter where they started. proposal is very evident. this reduces the computational efficiency as we discuss below but not the statistical validity. estimated potential scale reduction we can assess convergence more quantitatively as follows. the basic idea is to compare the variance of a quantity within each chain to its variance across chains. more precisely suppose we collect s samples burn-in from each of c chains of d variables xisc i d s c let ysc be a scalar quantity of interest derived from ysc xisc for some chosen i. define the within-sequence mean and overall mean as y c s y c ysc y c define the between-sequence and within-sequence variance as b s c c y w c s y we can now construct two estimates of the variance of y. the first estimate is w this should underestimate var if the chains have not ranged over the full posterior. the second estimate is w s b s s v this is an estimate of var that is unbiased under stationarity but is an overestimate if the starting points were overdispersed and rubin from this we can define the following convergence diagnostic statistic known as the estimated potential scale reduction or epsr r v w chapter markov chain monte carlo inference mh rhat mh rhat mh rhat gibbs rhat figure traceplots for mcmc samplers. each color represents the samples from a different starting point. gibbs sampling. figure generated by mcmcgmmdemo. mh with proposal n for corresponding to figure this quantity which was first proposed in and rubin measures the degree to which the posterior variance would decrease if we were to continue sampling in the s limit. if r for any given quantity then that estimate is reliable at least is not unreliable. the r values for the four samplers in figure are and so this diagnostic has correctly identified that the sampler using the first proposal is untrustworthy. accuracy of mcmc the samples produced by mcmc are auto-correlated and this reduces their information content relative to independent or perfect samples. we can quantify this as suppose we want this section is based on sec speed and accuracy of mcmc mh mh mh gibbs figure autocorrelation functions corresponding to figure figure generated by mcmcgmmdemo. to estimate the mean of f for some function f wherex p. denote the true mean by f e f s a monte carlo estimate is given by fs chapter markov chain monte carlo inference where fs f and xs px. an mcmc estimate of the variance of this estimate is given by e f f f varm cm cf e f s e f f e f varm cf where the first term is the monte carlo estimate of the variance if the samples weren t correlated and the second term depends on the correlation of the samples. we can measure this as follows. define the sample-based auto-correlation at lag t of a set of samples fs as follows t f f f s s t t this is called the autocorrelation function this is plotted in figure for our four samplers for the gaussian mixture model. we see that the acf of the gibbs sampler right dies off to much more rapidly than the mh samplers indicating that each gibbs sample is worth more than each mh sample. a simple method to reduce the autocorrelation is to use thinning in which we keep every n th sample. this does not increase the efficiency of the underlying sampler but it does save space since it avoids storing highly correlated samples. we can estimate the information content of a set of samples by computing the effective sample size s eff defined by s eff varm cf varm cm cf from figure it is clear that the effective sample size of the gibbs sampler is higher than that of the other samplers this example. how many chains? a natural question to ask is how many chains should we run? we could either run one long chain to ensure convergence and then collect samples spaced far apart or we could run many short chains but that wastes the burnin time. in practice it is common to run a medium number of chains of medium length steps and to take samples from each after discarding the first half of the samples. if we initialize at a local mode we may be able to use all the samples and not wait for burn-in. auxiliary variable mcmc em ep gibbs gibbs with ars model goal method probit map gradient probit map post probit post probit post probit probit post mh using irls proposal map gradient logit post logit logit post gibbs with student gibbs with ks reference section section and rasmussen exercise and smith section and fruhwirth and held table summary of some possible algorithms for estimation and inference for binary classification problems using gaussian priors. abbreviations aux. auxiliary variable sampling ars adaptive rejection sampling ep expectation propagation gibbs gibbs sampling with auxiliary variables irls iterative reweighted least squares ks kolmogorov smirnov map maximum a posteriori mh metropolis hastings post posterior. auxiliary variable mcmc sometimes we can dramatically improve the efficiency of sampling by introducing dummy auxiliary variables in order to reduce correlation between the original variables. if the original variables are denoted by x and the auxiliary variables by z we require that z px z px and that px z is easier to sample from than just px. if we meet these two conditions we can sample in the enlarged model and then throw away the sampled z values thereby recovering samples from px. we give some examples below. auxiliary variable sampling for logistic regression in section we discussed the latent variable interpretation of probit regression. recall that this had the form zi wt xi n yi izi we exploited this representation in section where we used em to find an ml estimate. it is straightforward to convert this into an auxiliary variable gibbs sampler since pwd is gaussian and pzixi yi w is truncated gaussian both of which are easy to sample from. now let us discuss how to derive an auxiliary variable gibbs sampler for logistic regression. let follow a logistic distribution with pdf plogistic e e with mean e and variance var the cdf has the form f sigm which pyi w wt xi f f f xi sigmwt xi as required. we can derive an auxiliary variable gibbs sampler by sampling from pzwd and pwzd. unfortunately sampling directly from pwzd is not possible. one approach is to define n i where i and i ks the kolmogorov smirnov distribution and then to sample w z and and held a simpler approach is to approximate the logistic distribution by the student distribution and chib specifically we will make the approximation t where we can now use the scale mixture of gaussians representation of the student to simplify inference. in particular we write i ga n zi wt xi yi izi i all of the full conditionals now have a simple form see exercise for the details. note that if we set then zi n xi which is equivalent to probit regression section rather than choosing between probit or logit regression we can simply estimate the parameter. there is no convenient conjugate prior but we can consider a finite range of possible values and evaluate the posterior as follows p p i e chapter markov chain monte carlo inference is the logistic function. since yi iff wt xi we have by symmetry that wt xi furthermore if we define we can sample as well. for example suppose we use a ig prior for the posterior is given by ig j this can be interleaved with the other gibbs sampling steps and provides an appealing bayesian alternative to cross validation for setting the strength of the regularizer. d see table for a summary of various algorithms for fitting probit and logit models. many of these methods can also be extended to the multinomial logistic regression case. for details see fruhwirth-schnatter and fruhwirth slice sampling consider sampling from a univariate but multimodal distribution px. we can sometimes improve the ability to make large moves by adding an auxiliary variable u. we define the joint distribution as follows if u px otherwise px u auxiliary variable mcmc fx u x x x illustration of the principle behind slice sampling. given a previous sample xi we figure sample uniformly on f where f is the target density. we then sample along the slice where f source figure of et al. used with kind permission of nando de freitas. slice sampling in action. figure generated by x y t i s n e d r o i r e t s o p slope intercept figure binomial regression for data. approximation. figure generated by grid approximation to posterior. slice sampling where zp px pxdx. the marginal distribution over x is given by px udu zp du px zp px so we can sample from px by sampling from px u and then ignoring u. the full conditionals have the form pux pxu pxu uax where a px u is the set of points on or above the chosen height u. this corresponds to a slice through the distribution hence the term slice sampling see figure in practice it can be difficult to identify the set a. so we can use the following approach construct an interval xmin x xmax around the current point xs of some width. we then chapter markov chain monte carlo inference test to see if each end point lies within the slice. if it does we keep extending in that direction until it lies outside the slice. this is called stepping out. a candidate value is then chosen if it lies within the slice it is kept so otherwise we uniformly from this region. shrink the region such that forms one end and such that the region still contains xs. then another sample is drawn. we continue in this way until a sample is accepted. to apply the method to multivariate distributions we can sample one extra auxiliary variable for each dimension. the advantage of slice sampling over gibbs is that it does not need a specification of the full-conditionals just the unnormalized joint. the advantage of slice sampling over mh is that it does not need a user-specified proposal distribution it does require a specification of the width of the stepping out interval. figure illustrates the algorithm in action on a synthetic problem. figure illustrates its behavior on a slightly harder problem namely binomial logistic regression. the model has the form yi binni logit we use a vague gaussian prior for the j s. figure shows a grid-based approximation to the posterior and figure shows a sample-based approximation. in this example the grid is faster to compute but for any problem with more than dimensions the grid approach is infeasible. swendsen wang consider an ising model of the following form z e fexe px where xe xj for edge e j xi and the edge factor fe is defined by ej where j is the edge strength. gibbs sampling in such models can be slow when e j j is large in absolute value because neighboring states can be highly correlated. the swendsen wang algorithm and wang is a auxiliary variable mcmc sampler which mixes much faster at least for the case of attractive or ferromagnetic models with j e j ej suppose we introduce auxiliary binary variables one per edge. these are called bond variables and will be denoted by z. we then define an extended model px z of the form px z gexe ze e where ze and we define the new factor as follows gexe ze ej e j ej e j e j e j e j e j and gexe ze it is clear that gexe ze fexe our presentation of the method is based on some notes by david mackay available from httpwww.inference auxiliary variable mcmc figure illustration of the swendsen wang algorithm on a grid. used with kind permission of kevin tang. z px z px. so if we can sample from this extended model we can just and hence that throw away the z samples and get valid x samples from the original distribution. fortunately it is easy to apply gibbs sampling to this extended model. the full conditional pzx factorizes over the edges since the bond variables are conditionally independent given the node variables. furthermore the full conditional pzexe is simple to compute if the nodes on either end of the edge are in the same state xj we set the bond ze to with probability p e otherwise we set it to in figure right the bonds that could be turned on their corresponding nodes are in the same state are represented in figure right the bonds that are randomly turned on are by dotted edges. represented by solid edges. to sample pxz we proceed as follows. find the connected components defined by the graph induced by the bonds that are turned on. that a connected component may consist of a singleton node. pick one of these components uniformly at random. all the nodes in each such component must have the same state since the off-diagonal terms in the gexe ze factor are pick a state uniformly at random and force all the variables in this component to adopt this new state. this is illustrated in figure left where the green square chapter markov chain monte carlo inference denotes the selected connected component and we choose to force all nodes within in to enter the white state. the validity of this algorithm is left as an exercise as is the extension to handle local evidence and non-stationary potentials. it should be intuitively clear that swendsen wang makes much larger moves through the state space than gibbs sampling. in fact sw mixes much faster than gibbs sampling on lattice ising models for a variety of values of the coupling parameter provided j more precisely let the edge strength be parameterized by jt where t is a computational temperature. for large t the nodes are roughly independent so both methods work equally well. however as t approaches a critical temperature tc the typical states of the system have very long correlation lengths and gibbs sampling takes a very long time to generate independent samples. as the temperature continues to drop the typical states are either all on or all off. the frequency with which gibbs sampling moves between these two modes is exponentiall small. by contrast sw mixes rapidly at all temperatures. unfortunately if any of the edge weights are negative j the system is frustrated and there are exponentially many modes even at low temperature. sw does not work very well in this setting since it tries to force many neighboring variables to have the same state. in fact computation in this regime is provably hard for any algorithm and sinclair hybridhamiltonian mcmc in this section we briefly mention a way to perform mcmc sampling for continuous state spaces for which we can compute the gradient of the log-posterior. this is the case in neural network models for example. the basic idea is to think of the parameters as a particle in space and to create auxiliary variables which represent the momentum of this particle. we then update this parameter momentum pair according to certain rules e.g. et al. neal mackay neal for details. the resulting method is called hybrid mcmc or hamiltonian mcmc. the two main parameters that the user must specify are how many leapfrog steps to take when updating the position momentum and how big to make these steps. performance can be quite sensitive to these parameters see and gelman for a recent way to set them automatically. this method can be combined with stochastic gradient descent in order to handle large datasets as explained in et al. recently a more powerful extension of this method has been developed that exploits second order gradient information. see et al. for details. annealing methods many distributions are multimodal and hence hard to sample from. however by analogy to the way metals are heated up and then cooled down in order to make the molecules align we can imagine using a computational temperature parameter to smooth out a distribution gradually cooling it to recover the original bumpy distribution. we first explain this idea in more detail in the context of an algorithm for map estimation. we then discuss extensions to the sampling case. annealing methods temp temp x y x y x figure an energy surface at different temperatures. note the different vertical scales. t figure generated by sademopeaks. t simulated annealing simulated annealing et al. is a stochastic algorithm that attempts to find the global optimum of a black-box function f it is closely related to the metropolishastings algorithm for generating samples from a probability distribution which we discussed in section sa can be used for both discrete and continuous optimization. the method is inspired by statistical physics. the key quantity is the boltzmann distribution which specifies that the probability of being in any particular state x is given by px exp f where f is the energy of the system and t is the computational temperature. as the temperature approaches the system is cooled the system spends more and more time in its minimum energy probable state. figure gives an example of a function at different temperatures. at high temperatures t the surface is approximately flat and hence it is easy to move around to avoid local optima. as the temperature cools the largest peaks become larger and the smallest peaks disappear. by cooling slowly enough it is possible to track the largest peak and thus find the global optimum. this is an example of a continuation method. we can generate an algorithm from this as follows. at each step sample a new state according to some proposal distribution q for real-valued parameters this is often simply a xk where n for discrete optimization other random walk proposal kinds of local moves must be defined. having proposed a new state we compute exp f we then accept the new state set with probability otherwise we stay in the current state set xk. this means that if the new state has lower energy more probable we will definitely accept it but it it has higher energy less probable we might still accept depending on the current temperature. thus the algorithm allows down-hill moves in probability space in energy space but less frequently as the temperature drops. chapter markov chain monte carlo inference temperature vs iteration energy vs iteration figure a run of simulated annealing on the energy surface in figure iteration. energy vs iteration. figure generated by sademopeaks. temperature vs iter temp iter temp y x y x figure histogram of samples from the annealed posterior at different time points produced by simulated annealing on the energy surface shown in figure note that at cold temperatures most of the samples are concentrated near the peak at figure generated by sademopeaks. the rate at which the temperature changes over time is called the cooling schedule. it has been shown et al. that if one cools sufficiently slowly the algorithm will provably find the global optimum. however it is not clear what sufficient slowly means. in practice it is common to use an exponential cooling schedule of the following form tk k where is the initial temperature and c is the cooling rate c see figure for a plot of this cooling schedule. cooling too quickly means one can get stuck in a local maximum but cooling too slowly just wastes time. the best cooling schedule is difficult to determine this is one of the main drawbacks of simulated annealing. figure shows an example of simulated annealing applied to the function in figure using a random walk proposal. we see that the method stochastically reduces the energy over time. figures illustrate histogram of samples drawn from the cooled probability distribution over time. we see that most of the samples are concentrated near the global maximum. when the algorithm has converged we just return the largest value found. annealing methods annealed importance sampling we now describe a method known as annealed importance sampling that combines ideas from simulated annealing and importance sampling in order to draw independent samples from difficult multimodal distributions. suppose we want to sample from but we cannot do so easily for example this might represent a multimodal posterior. suppose however that there is an easier distribution which we can sample from call it pnx fnx for example this might be the prior. we can now construct a sequence of intermediate distributions than move slowly from pn to as follows fjx j j where n where j is an inverse temperature. this to the scheme used by simulated annealing which has the form fjx j this makes it hard to sample from pn. furthermore suppose we have a series of markov chains tjx x to which leave each pj invariant. given this we can sample x from by first sampling a sequence z as follows sample zn pn sample zn tn sample finally we set x and give it weight w fn fnzn fn fn this can be shown to be correct by viewing the algorithm as a form of importance sampling in an extended state space z zn consider the following distribution on this state space pz f tn zn where tj is the reversal of tj tjz f so we can safely just use the part of these it is clear that sequences to recover the original ditribution. now consider the proposal distribution defined by the algorithm qz gz fnzn zn t one can show that the importance weights w f are given by equation parallel tempering another way to combine mcmc and annealing is to run multiple chains in parallel at different temperatures and allow one chain to sample from another chain at a neighboring temperature. in this way the high temperature chain can make long distance moves through the state space and have this influence lower temperature chains. this is known as parallel tempering. see e.g. and deem for details. chapter markov chain monte carlo inference approximating the marginal likelihood the marginal likelihood pdm is a key quantity for bayesian model selection and is given by pdm pd m unfortunately this integral is often intractable to compute for example if we have non conjugate in this section we briefly discuss some ways to priors andor we have hidden variables. approximate this expression using monte carlo. see and meng for a more extensive review. the candidate method there is a simple method for approximating the marginal likelihood known as the candidate method this exploits the following identity pdm pd m p m this holds for any value of once we have picked some value we can evaluate pd m and p quite easily. if we have some estimate of the posterior near we can then evaluate the denominator as well. this posterior is often approximated using mcmc. the flaw with this method is that it relies on the assumption that p m has marginalized over all the modes of the posterior which in practice is rarely possible. consequently the method can give very inaccurate results in practice harmonic mean estimate newton and raftery proposed a simple method for approximating pd using the output of mcmc as follows s where s p this expression is the harmonic mean of the likelihood of the data under each sample. the theoretical correctness of this expression follows from the following identity pd s pd p pd pd pd d pd p pd unfortunately in practice this method works very poorly. indeed radford neal called this the worst monte carlo method ever the reason it is so bad is that it depends only on samples drawn from the posterior. but the posterior is often very insensitive to the prior whereas the marginal likelihood is not. we only mention this method in order to warn against its use. we present a better method below. source te-carlo-method-ever. approximating the marginal likelihood annealed importance sampling f zn f gzdz gz gzdz gzdz eq we can use annealed importance sampling to evaluate a ratio of partition functions. notice that gzdz. hence f and zn fnxdx f gz s ws if fn is a prior and is the posterior we can estimate zn pd using the above equation provided the prior has a known normalization constant this is generally considered the method of choice for evaluating difficult partition functions. exercises exercise gibbs sampling from a gaussian suppose x n where and derive the full conditionals and implement the algorithm and plot the marginals and as histograms. superimpose a plot of the exact marginals. exercise gibbs sampling for a gaussian mixture model consider applying gibbs sampling to a univariate mixture of gaussians as in section derive the expressions for the full conditionals. hint if we know zn j then j gets connected to xn but all other values of i for all i j are irrelevant. is an example of context-specific independence where the structure of the graph simplifies once we have assigned values to some of the nodes. hence given all the zn values the posteriors of the s should be independent so the conditional of j should be independent of j. for j. exercise gibbs sampling from the potts model modify the code in gibbsdemoising to draw samples from a potts prior at different temperatures as in figure exercise full conditionals for hierarchical model of gaussian means let us reconsider the gaussian-gaussian model parameters j. that we use the following conjugate priors on the hyper-parameters in section for modelling multiple related mean in this exercise we derive a gibbs sampler instead of using eb. suppose following n ig ig chapter markov chain monte carlo inference we can set to uninformative values. given this model specification show that the full conditionals for and the j are as follows p d d p j j njxj nj p ig d p ig nj j j exercise gibbs sampling for robust linear regression with a student t likelihood modify the em algorithm in exercise to perform gibbs sampling for pw zd exercise gibbs sampling for probit regression modify the em algorithm in section to perform gibbs sampling for pw zd. hint we can sample from a truncated gaussian n z b in two steps first sample u u then set z exercise gibbs sampling for logistic regression with the student approximation derive the full conditionals for the joint model defined by equations to clustering introduction clustering is the process of grouping similar objects together. there are two kinds of inputs we might use. in similarity-based clustering the input to the algorithm is an n n dissimilarity matrix or distance matrix d. in feature-based clustering the input to the algorithm is an n d feature matrix or design matrix x. similarity-based clustering has the advantage that it allows for easy inclusion of domain-specific similarity or kernel functions featurebased clustering has the advantage that it is applicable to raw potentially noisy data. we will see examples of both below. in addition to the two types of input there are two possible types of output flat clustering also called partitional clustering where we partition the objects into disjoint sets and hierarchical clustering where we create a nested tree of partitions. we will discuss both of these below. not surprisingly flat clusterings are usually faster to create d for flat vs on log n for hierarchical but hierarchical clusterings are often more useful. furthermore most hierarchical clustering algorithms are deterministic and do not require the specification of k the number of clusters whereas most flat clustering algorithms are sensitive to the initial conditions and require some model selection method for k. will discuss how to choose k in more detail below. the final distinction we will make in this chapter is whether the method is based on a probabilistic model or not. one might wonder why we even bother discussing non-probabilistic methods for clustering. the reason is two-fold first they are widely used so readers should know about them second they often contain good ideas which can be used to speed up inference in a probabilistic models. measuring a dissimilarity matrix d is a matrix where dii and dij is a measure of distance between objects i and j. subjectively judged dissimilarities are seldom distances in the strict sense since the triangle inequality dij dik djk often does not hold. some algorithms require d to be a true distance matrix but many do not. if we have a similarity matrix s we can convert it to a dissimilarity matrix by applying any monotonically decreasing function e.g. d maxs s. the most common way to define dissimilarity between objects is in terms of the dissimilarity chapter clustering of their attributes jxij some common attribute dissimilarity functions are as follows squared distance jxij of course this only makes sense if attribute j is real-valued. squared distance strongly emphasizes large differences differences are squared. a more robust alternative is to use an distance jxij this is also called city block distance since in the distance can be computed by counting how many rows and columns we have to move horizontally and vertically to get from xi to if xi is a vector a time-series of real-valued data it is common to use the correlation j coefficient section jxij corr so clustering based on correlation and hence is equivalent to clustering based on squared distance if the data is standardized then corr for ordinal variables such as medium high it is standard to encode the values as real-valued numbers say if there are possible values. one can then apply any dissimilarity function for quantitative variables such as squared distance. for categorical variables such as green blue we usually assign a distance of if the features are different and a distance of otherwise. summing up over all the categorical features gives xi ixij this is called the hamming distance. evaluating the output of clustering methods the validation of clustering structures is the most difficult and frustrating part of cluster analysis. without a strong effort in this direction cluster analysis will remain a black art accessible only to those true believers who have experience and great courage. jain and dubes and dubes introduction figure three clusters with labeled objects inside. based on figure of et al. clustering is an unupervised learning technique so it is hard to evaluate the quality of the output of any given method. if we use probabilistic models we can always evaluate the likelihood of a test set but this has two drawbacks first it does not directly assess any clustering that is discovered by the model and second it does not apply to non-probabilistic methods. so now we discuss some performance measures not based on likelihood. intuitively the goal of clustering is to assign points that are similar to the same cluster and to ensure that points that are dissimilar are in different clusters. there are several ways of measuring these quantities e.g. see and dubes kaufman and rousseeuw however these internal criteria may be of limited use. an alternative is to rely on some external form of data with which to validate the method. for example suppose we have labels for each object as in figure we can have a reference clustering given a clustering we can induce a set of labels and vice versa. then we can compare the clustering with the labels using various metrics which we describe below. we will use some of these metrics later when we compare clustering methods. purity i let nij be the number of objects in cluster i that belong to class j and let ni nij be the total number of objects in cluster i. define pij nijni this is the empirical distribution over class labels for cluster i. we define the purity of a cluster as pi maxj pij and the overall purity of a clustering as purity ni n pi for example in figure we have that the purity is the purity ranges between and however we can trivially achieve a purity of by putting each object into its own cluster so this measure does not penalize for the number of clusters. rand index let u ur and v vc be two different partitions of the n data points i.e. two different clusterings. for example u might be the estimated clustering and v is reference clustering derived from the class labels. now define a contingency table chapter clustering containing the following numbers t p is the number of pairs that are in the same cluster in both u and v positives t n is the number of pairs that are in the different clusters in both u and v negatives f n is the number of pairs that are in the different clusters in u but the same cluster in v negatives and f p is the number of pairs that are in the same cluster in u but different clusters in v positives. a common summary statistic is the rand index r t p t n t p f p f n t n this can be interpreted as the fraction of clustering decisions that are correct. clearly r for example consider figure the three clusters contain and points so the number of positives pairs of objects put in the same cluster regardless of label is t p f p of these the number of true positives is given by t p where the last two terms come from cluster there are pairs labeled a. so f p similarly one can show f n and t n so the rand index is pairs labeled c and one can define an adjusted rand index and arabie as follows the rand index only achieves its lower bound of if t p t n which is a rare event. ar index expected index max index expected index here the model of randomness is based on using the generalized hyper-geometric distribution i.e. the two partitions are picked at random subject to having the original number of classes and objects in each and then the expected value of t p t n is computed. this model can be used to compute the statistical significance of the rand index. the rand index weights false positives and false negatives equally. various other summary statistics for binary decision problems such as the f-score can also be used. one can compute their frequentist sampling distribution and hence their statistical significance using methods such as bootstrap. mutual information another way to measure cluster quality is to compute the mutual information between u and v and dom to do this let pu v j be the probability that a randomly chosen object belongs to cluster ui in u and vj in v also let pu be the be the probability that a randomly chosen object belongs to cluster ui in u define vj n dirichlet process mixture models pv similarly. then we have pu v j pu iu v pu v j log this lies between and minh h unfortunately the maximum value can be achieved by using lots of small clusters which have low entropy. to compensate for this we can use the normalized mutual information n m iu v iu v this lies between and a version of this that is adjusted for chance a particular random data model is described in et al. another variant called variation of information is described in dirichlet process mixture models the simplest approach to clustering is to use a finite mixture model as we discussed in section this is sometimes called model-based clustering since we define a probabilistic model of the data and optimize a well-defined objective likelihood or posterior as opposed to just using some heuristic algorithm. the principle problem with finite mixture models is how to choose the number of components k. we discussed several techniques in section however in many cases there is no welldefined number of clusters. even in the simple height-weight data it is not clear if the correct value of k should be or it would be much better if we did not have to choose k at all. in this section we discuss infinite mixture models in which we do not impose any a priori bound on k. to do this we will use a non-parametric prior based on the dirichlet process this allows the number of clusters to grow as the amount of data increases. it will also prove useful later when we discuss hiearchical clustering. the topic of non-parametric bayes is currently very active and we do not have space to go into details et al. for a recent book on the topic. instead we just give a brief review of the dp and its application to mixture modeling based on the presentation in sec from finite to infinite mixture models consider a finite mixture model as shown in figure the usual representation is as follows pxizi k xi k pzi k k p dir the form of p k is chosen to be conjugate to pxi k. we can write pxi k as xi f zi where f is the observation distribution. similarly we can write k h where h is the prior. chapter clustering figure two different representations of a finite mixture model. left traditional representation. right representation where parameters are samples from g a discrete measure. the picture on the right illustrates the case where k and we sample gaussian means k from a gaussian prior h. the height of the spikes reflects the mixing weights k. this weighted sum of delta functions is g. we then generate two parameters and from g one per data point. finally we generate two data points and from n and n source figure of used with kind permission of erik sudderth. an equivalent representation for this model is the parameter used to generate observation xi these parameters are sampled from distribution g which has the form is shown in figure here i g k k k and k h. thus we see that g is a finite mixture of delta functions where dir centered on the cluster parameters k. the probability that i is equal to k is exactly k the prior probability for that cluster. if we sample from this model we will always probability one get exactly k clusters with data points scattered around the cluster centers. we would like a more flexible model that can generate a variable number of clusters. furthermore the more data we generate the more likely we should be to see a new cluster. the way to do this is to replace the discrete distribution g with a random probability measure. below we will show that the dirichlet process denoted g dp h is one way to do this. before we go into the details we show some samples from this non-parametric model in figure we see that it has the desired properties of generating a variable number of clusters with more clusters as the amount of data increases. the resulting samples look much more like real data than samples from a finite mixture model. of course working with an infinite model sounds scary. fortunately as we show below even though this model is potentially infinite we can perform inference using an amount of computation that is not only tractable but is often much less than that required to fit a set dirichlet process mixture models figure some samples from a dirichlet process mixture model of gaussians with concentration parameter from left to right we show n n and n samples. each row is a different run. we also show the model parameters as ellipses which are sampled from a vague niw base distribution. based on figure of figure generated by dpmsampledemo written by yee-whye teh. of finite mixture models for different k. the intuitive reason is that we can get evidence that certain values of k are appropriate high posterior support long before we have been able to estimate the parameters so we can focus our computational efforts on models of appropriate complexity. thus going to the infinite limit can sometimes be faster. this is especially true when we have multiple model selection problems to solve. chapter clustering a base measure h on a space figure where the shading of cell tk is proportional to e htk. regions. source figure of used with kind permission of erik sudderth. one possible partition into k regions a refined partition into k the dirichlet process recall from chapter that a gaussian process is a distribution over functions of the form f x r. it is defined implicitly by the requirement that pf f be jointly gaussian for any set of points xi x the parameters of this gaussian can be computed using a mean function and covariance function k. we write f gp k. furthermore the gp is consistently defined so that pf can be derived from pf f etc. require g and gtk has a joint dirichlet distribution where we g the dp is defined implicitly by the requirement that a dirichlet process is a distribution over probability measures g r dir htk for any finite partition tk of if this is the case we write g dp h where is called the concentration parameter and h is called the base an example of a dp is shown in figure where the base measure is a gaussian. the distribution over all the cells gtk is dirichlet so the marginals in each cell are beta distributed beta hti htj the dp is consistently defined in the sense that if and form a partition of then and g both follow the same beta distribution. recall that if dir and z cat then we can integrate out to get the predictive distribution for the dirichlet-multinoulli model z cat k unlike a gp knowing something about gtk does not tell us anything about beyond the sum-to-one constraint we say that the dp is a neutral process. other stochastic processes can be defined that do not have this property but they are not so computationally convenient. dirichlet process mixture models illustration of the stick breaking construction. figure we have a unit length stick which we break at a random point the length of the piece we keep is called we then recursively break off pieces of the remaining stick to generate source figure of used with samples of k from this process for row and kind permission of erik sudderth. row. figure generated by stickbreakingdemo written by yee-whye teh. where given one observation is given by k k. in other words pz k k also the updated posterior for dir iz k iz k the dp generalizes this to arbitrary partitions. if g dp h then p ti hti and the posterior is gtk h dir htk this holds for any set of partitions. hence if we observe multiple samples i g the new posterior is given by g n h dp n n h i thus we see that the dp effectively defines a conjugate prior for arbitrary measurable spaces. the concentration parameter is like the effective sample size of the base measure h. stick breaking construction of the dp our discussion so far has been very abstract. we now give a constructive definition for the dp known as the stick-breaking construction. be an infinite sequence of mixture weights derived from the following process let k k k k k l k l chapter clustering this is often denoted by gem where gem stands for griffiths engen and mccloskey term is due to some samples from this process are shown in figure one can show that this process process will terminate with probability although the number of elements it generates increases with furthermore the size of the k components decreases on average. now define g k k where gem and k h. then one can show that g dp h. as a consequence of this construction we see that samples from a dp are discrete with probability one. in other words if you keep sampling it you will get more and more repetitions of previously generated values. so if we sample i g we will see repeated values let us number the unique values etc. data sampled from i will therefore cluster around the k. this is evident in figure where most data comes from the gaussians with large k values represented by ellipses with thick borders. this is our first indication that the dp might be useful for clustering. the chinese restaurant process working with infinite dimensional sticks is problematic. however we can exploit the clustering property to draw samples form a gp as we now show. if i g are n observations from g dp h taking on k the key result is this distinct values k then the predictive distribution of the next observation is given by p n h n h nk k where nk is the number of previous observations equal to k. this is called the polya urn or blackwell-macqueen sampling scheme. this provides a constructive way to sample from a dp. it is much more convenient to work with discrete variables zi which specify which value of k to use. that is we define i zi. based on the above expression we have pzn n iz k nkiz k where k represents a new cluster index that has not yet been used. this is called the chinese restaurant process or crp based on the seemingly infinite supply of tables at certain chinese restaurants. the analogy is as follows the tables are like clusters and the customers are like observations. when a person enters the restaurant he may choose to join an existing table with probability proportional to the number of people already sitting at this table nk otherwise with a probability that diminishes as more people enter the room to the n term dirichlet process mixture models figure two views of a dp mixture model. left gem right g is drawn from a dp. compare to figure used with kind permission of erik sudderth. infinite number of clusters parameters k and source figure of he may choose to sit at a new table k integers which is like a distribution of customers to tables. the result is a distribution over partitions of the the fact that currently occupied tables are more likely to get new customers is sometimes called the rich get richer phenomenon. indeed one can derive an expression for the distribution of cluster sizes induced by this prior process it is basically a power law. the number of occupied tables k almost surely approaches logn as n showing that the model complexity will indeed grow logarithmically with dataset size. more flexible priors over cluster sizes can also be defined such as the two-parameter pitman-yor process. applying dirichlet processes to mixture modeling the dp is not particularly useful as a model for data directly since data vectors rarely repeat exactly. however it is useful as a prior for the parameters of a stochastic data generating mechanism such as a mixture model. to create such a model we follow exactly the same setup as section but we define g dp h. equivalently we can write the model as follows gem zi k h xi f zi this is illustrated in figure we see that g is now a random draw of an unbounded number of parameters k from the base distribution h each with weight k. each data point xi is generated by sampling its own private parameter i from g. as we get more and more data it becomes increasingly likely that i will be equal to one of the k s we have seen before and thus xi will be generated close to an existing datapoint. chapter clustering fitting a dp mixture model the simplest way to fit a dpmm is to modify the collapsed gibbs sampler of section from equation we have pzi kz i x pzi kz i i zi k z i n by exchangeability we can assume that zi is the last customer to enter the restaurant. hence the first term is given by pziz i izi k nk iizi k where k is the number of clusters used by z i and k this is as follows is a new cluster. another way to write nk i pzi kz i if k has been seen before if k is a new cluster nk i interestingly this is equivalent to equation which has the form pzi kz i to compute the second term pxix i zi k z i let us partition the data x i into clusters based on z i. let x ic zj c j i be the data assigned to cluster c. if zi k then xi is conditionally independent of all the data points except those assigned to cluster k. hence we have in thek limit neal pxix i z i zi k xix ik where pxi x ik pxi k pxi x ik px ik h k k pxj k is the marginal likelihood of all the data assigned to cluster k including i and px ik is an analogous expression excluding i. thus we see that the term pxix i z i zi k is the posterior preditive distribution for cluster k evaluated at xi. corresponding to a new cluster we have if zi k pxix i z i zi k pxi pxi which is just the prior predictive distribution for a new cluster evaluated at xi. see algorithm for the pseudocode. is called algorithm in this is very similar to collapsed gibbs for finite mixtures except that we have to consider the case zi k an example of this procedure in action is shown in figure the sample clusterings and the induced posterior over k seems reasonable. the method tends to rapidly discover a good clustering. by contrast gibbs sampling em for a finite mixture model often gets stuck in affinity propagation algorithm collapsed gibbs sampler for dp mixtures for each i in random order do remove xi s sufficient statistics from old cluster zi for each k do compute pkxi pxix ik set nk i dimx ik compute pzi kz id nk i compute p pxi compute pzi id normalize pzi sample zi pzi add xi s sufficient statistics to new cluster zi if any cluster is empty remove it and decrease k poor local optima shown. this is because the dpmm is able to create extra redundant clusters early on and to use them to escape local optima. figure shows that most of the time the dpmm converges more rapidly than a finite mixture model. a variety of other fitting methods have been proposed. shows how one can use a star search and beam search to quickly find an approximate map estimate. et al. discusses how to fit a dpmm online using particle filtering which is a like a stochastic version of beam search. this can be more efficient than gibbs sampling particularly for large datasets. et al. develops a variational approximation that is even faster also extensions to the case of non-conjugate priors are discussed in another important issue is how to set the hyper-parameters. for the dp the value of does not have much impact on predictive accuracy but it does affect the number of clusters. one approach is to put a gaa b prior for and then to from its posterior p n a b using auxiliary variable methods and west alternatively one can use empirical bayes et al. similarly for the base distribution we can either sample the hyper-parameters or use empirical bayes et al. affinity propagation mixture models whether finite or infinite require access to the raw n d data matrix and need to specify a generative model of the data. an alternative approach takes as input an n n similarity matrix and then tries to identify examplars which will act as cluster centers. the k-medoids or k-centers algorithm is one approach but it can suffer from local minima. here we describe an alternative approach called affinity propagation and dueck that works substantially better in practice. the idea is that each data point must choose another data point as its exemplar or centroid some data points will choose themselves as centroids and this will automatically determine the number of clusters. more precisely let ci n represent the centroid for datapoint i. iter iter chapter clustering iter figure data points in are clustered using a dp mixture fit with collapsed gibbs sampling. we show samples from the posterior after samples. we also show the posterior over k based on samples discarding the first as burnin. figure generated by written by yee whye teh. the goal is to maximize the following function sc si ci kc the first term measures the similarity of each point to its centroid. the second term is a penalty term that is if some data point i has chosen k as its exemplar ci k but k has not chosen itself as an exemplar we do not have ck k. more formally if ck k but i ci k kc otherwise the objective function can be represented as a factor graph. we can either use n nodes affinity propagation x p g o l dirichlet process mixture finite mixture iteration x p g o l dirichlet process mixture finite mixture iteration figure comparison of collapsed gibbs samplers for a dp mixture blue and a finite mixture red with k applied to n data points in figure left logprob vs iteration for different starting values. right median line and quantiles lines over different starting values. source figure of used with kind permission of erik sudderth. a k n ci cn si sn figure factor graphs for affinity propagation. circles are variables squares are factors. each ci node has n possible states. from figure of and dueck used with kind permission of brendan frey. each with n possible values as shown in figure or we can use n binary nodes and frey for the details. we will assume the former representation. we can find a strong local maximum of the objective by using max-product loopy belief propagation referring to the model in figure each variable nodes ci sends a message to each factor node k. it turns out that this vector of n numbers can be reduced to a scalar message denote ri k known as the responsibility. this is a measure of how much i thinks k would make a good exemplar compared to all the other exemplars i has looked at. in addition each factor node k sends a message to each variable node ci. again this can be reduced to a scalar message ai k known as the availability. this is a measure of how strongly k believes it should an exemplar for i based on all the other data points k has looked at. as usual with loopy bp the method might oscillate and convergence is not guaranteed. chapter clustering figure example of affinity propagation. each point is colored coded by how much it wants to be an exemplar is the most green is the least. this can be computed by summing up all the incoming availability messages and the self-similarity term. the darkness of the i k arrow reflects how much point i wants to belong to exemplar k. from figure of and dueck used with kind permission of brendan frey. however by using damping the method is very reliable in practice. if the graph is densely connected message passing takes on time but with sparse similarity matrices it only takes oe time where e is the number of edges or non-zero entries in s. the number of clusters can be controlled by scaling the diagonal terms si i which reflect how much each data point wants to be an exemplar. figure gives a simple example of some data where the negative euclidean distance was used to measured similarity. the si i values were set to be the median of all the pairwise similarities. the result is clusters. many other results are reported in and dueck who show that the method significantly outperforms k-medoids. spectral clustering an alternative view of clustering is in terms of graph cuts. the idea is we create a weighted undirected graph w from the similarity matrix s typically by using the nearest neighbors of each point this ensures the graph is sparse which speeds computation. if we want to find a partition into k clusters say ak one natural criterion is to minimize ak w ak spectral clustering where ak v ak is the complement of ak and w b i aj b wij. for k this problem is easy to solve. unfortunately the optimal solution often just partitions off a single data point from the rest. to ensure the sets are reasonably large we can define the normalized cut to be ak cutak ak volak i a di and di where vola wij is the weighted degree of node i. this splits the graph into k clusters such that nodes within each cluster are similar to each other but are different to nodes in other clusters. we can formulate the ncut problem in terms of searching for binary vectors ci where cik if point i belongs to cluster k that minimize the objective. unfortunately this is np-hard and wagner affinity propagation is one way to solve the problem. another is to relax the constraints that ci be binary and allow them to be real-valued. the result turns into an eigenvector problem known as spectral clustering e.g. and malik in general the technique of performing eigenalysis of graphs is called spectral graph theory going into the details would take us too far afield but below we give a very brief summary based on luxburg since we will encounter some of these ideas later on. graph laplacian let w be a symmetric weight matrix for a graph where wij wji let d diagdi be a diaogonal matrix containing the weighted degree of each node. we define the graph laplacian as follows l d w this matrix has various important properties. because each row sums to zero we have that is an eigenvector with eigenvalue furthermore the matrix is symmetric and positive semi-definite. to see this note that f t lf f t df f t wf fifjwij i dif ij i wijfi i dif fifjwij djf j i ij j ij hence f t lf for all f r eigenvalues n n consequently we see that l has n non-negative real-valued to get some intuition as to why l might be useful for graph-based clustering we note the following result. theorem the set of eigenvectors of l with eigenvalue is spanned by the indicator vectors where ak are the k connected components of the graph. chapter clustering proof. let us start with the case k if f is an eigenvector with eigenvalue then ij wijfi if two nodes are connected so wij we must have that fi fj. hence f is constant for all vertices which are connected by a path in the graph. now suppose k in this case l will be block diagonal. a similar argument to the above shows that we will have k indicator functions which select out the connected components. this suggests the following algorithm. compute the first k eigenvectors uk of l. let u uk be an n k matrix with the eigenvectors in its columns. let yi r k be the i th row of u. since these yi will be piecewise constant we can apply k-means clustering to them to recover the connected components. now assign point i to cluster k iff row i of y was assigned to cluster k. in reality we do not expect a graph derived from a real similarity matrix to have isolated connected components that would be too easy. but it is reasonable to suppose the graph is a small perturbation from such an ideal. in this case one can use results from perturbation theory to show that the eigenvectors of the perturbed laplacian will be close to these ideal indicator functions et al. note that this approach is related to kernel pca in particular kpca uses the largest eigenvectors of w these are equivalent to the smallest eigenvectors of i w. this is similar to the above method which computes the smallest eigenvectors of l d w. see in practice spectral clustering gives much better results than et al. for details. kpca. normalized graph laplacian in practice it is important to normalize the graph laplacian to account for the fact that some nodes are more highly connected than others. there are two comon ways to do this. one method used in e.g. and malik meila creates a stochastic matrix where each row sums to one lrw d i d the eigenvalues and eigenvectors of l and lrw are closely related to each other luxburg for details. furthemore one can show that for lrw the eigenspace of is again spanned by the indicator vectors this suggests the following algorithm find the smallest k eigenvectors of lrw create u cluster the rows of u using k-means then infer the partitioning of the original points and malik that the eigenvectors values of lrw are equivalent to the generalized eigenvectors values of l which solve lu du. ld k wd i d another method used in e.g. et al. creates a symmetric matrix lsym d this time the eigenspace of is spanned by d this suggest the following algorithm find the smallest k eigenvectors of lsym create u normalize each row to unit norm by creating tij uij ik cluster the rows of t using k-means then infer the partitioning of the original points et al. there is an interesting connection between ncuts and random walks on a graph first note that p d i lrw is a stochastic matrix where pij wijdi hierarchical clustering y k means clustering x y spectral clustering x figure clustering data consisting of spirals. k-means. spectral clustering. figure generated by spectralclusteringdemo written by wei-lwun lu. can be interpreted as the probability of going from i to j. if the graph is connected and non-bipartite it possesses a unique stationary distribution n where i divolv furthermore one can show that ncuta a paa aa this means that we are looking for a cut such that a random walk rarely makes transitions from a to a or vice versa. example figure illustrates the method in action. in figure we see that k-means does a poor job of clustering since it implicitly assumes each cluster corresponds to a spherical gaussian. next we try spectral clustering. we define a similarity matrix using the gaussian kernel. we compute the first two eigenvectors of the laplacian. from this we can infer the clustering in figure since the method is based on finding the smallest k eigenvectors of a sparse matrix it takes on time. however a variety of methods can be used to scale it up for large datasets e.g. et al. hierarchical clustering mixture models whether finite or infinite produce a flat clustering. often we want to learn a hierarchical clustering where clusters can be nested inside each other. there are two main approaches to hierarchical clustering bottom-up or agglomerative clustering and top-down or divisive clustering. both methods take as input a dissimilarity matrix between the objects. in the bottom-up approach the most similar groups are merged at each chapter clustering figure an example of single link clustering using city block distance. pairs and are both distance apart so get merged first. the resulting dendrogram. based on figure of figure generated by agglomdemo. hierarchical clustering of profiles figure hierarchical clustering applied to the yeast gene expression data. the rows are permuted according to a hierarchical clustering scheme link agglomerative clustering in order to bring similar rows close together. clusters induced by cutting the average linkage tree at a certain height. figure generated by hclustyeastdemo. in the top-down approach groups are split using various different criteria. we give the step. details below. note that agglomerative and divisive clustering are both just heuristics which do not optimize any well-defined objective function. thus it is hard to assess the quality of the clustering they produce in any formal sense. furthermore they will always produce a clustering of the input data even if the data has no structure at all it is random noise. later in this section we will discuss a probabilistic version of hierarchical clustering that solves both these problems. hierarchical clustering algorithm agglomerative clustering initialize clusters as singletons for i to n do ci initialize set of clusters available for merging s n repeat pick most similar clusters to merge k arg minjk s djk create new cluster cj ck mark j and k as unavailable s s k if n then foreach i s do mark as available s s update dissimilarity matrix di until no more clusters are available for merging figure illustration of single linkage. complete linkage. average linkage. agglomerative clustering agglomerative clustering starts with n groups each initially containing one object and then at each step it merges the two most similar groups until there is a single group containing all the data. see algorithm for the pseudocode. since picking the two most similar clusters to merge takes on time and there are on steps in the algorithm the total running time is on however by using a priority queue this can be reduced to on log n e.g. et al. ch. for details. for large n a common heuristic is to first run k-means which takes okn d time and then apply hierarchical clustering to the estimated cluster centers. the merging process can be represented by a binary tree called a dendrogram as shown in figure the initial groups are at the leaves the bottom of the figure and every time two groups are merged we join them in the tree. the height of the branches represents the dissimilarity between the groups that are being joined. the root of the tree is at the top represents a group containing all the data. if we cut the tree at any given height we induce a clustering of a given size. for example if we cut the tree in figure at height we get the clustering we discuss the issue of how to choose the height number of clusters below. a more complex example is shown in figure where we show some gene expression if we cut the tree in figure at a certain height we get the clusters shown in data. figure there are actually three variants of agglomerative clustering depending on how we define the dissimilarity between groups of objects. these can give quite different results as shown in chapter clustering single link complete link average link figure hierarchical clustering of yeast gene expression data. single linkage. complete linkage. average linkage. figure generated by hclustyeastdemo. hierarchical clustering figure we give the details below. single link in single link clustering also called nearest neighbor clustering the distance between two groups g and h is defined as the distance between the two closest members of each group dslg h min i h see figure the tree built using single link clustering is a minimum spanning tree of the data which is a tree that connects all the objects in a way that minimizes the sum of the edge weights to see this note that when we merge two clusters we connect together the two closest members of the clusters this adds an edge between the corresponding nodes and this is guaranteed to be the lightest weight edge joining these two clusters. and once two clusters have been merged they will never be considered again so we cannot create cycles. as a consequence of this we can actually implement single link clustering in on time whereas the other variants take on time. complete link in complete link clustering also called furthest neighbor clustering the distance between two groups is defined as the distance between the two most distant pairs dclg h max i h see figure single linkage only requires that a single pair of objects be close for the two groups to be considered close together regardless of the similarity of the other members of the group. thus clusters can be formed that violate the compactness property which says that all the observations within a group should be similar to each other. in particular if we define the diameter of a group as the largest dissimilarity of its members dg maxi g then we can see that single linkage can produce clusters with large diameters. complete linkage represents the opposite extreme two groups are considered close only if all of the observations in their union are relatively similar. this will tend to produce clusterings with small diameter i.e. compact clusters. average link in practice the preferred method is average link clustering which measures the average distance between all pairs ngnh i g h davgg h where ng and nh are the number of elements in groups g and h. see figure average link clustering represents a compromise between single and complete link clustering. it tends to produce relatively compact clusters that are relatively far apart. however since it chapter clustering involves averaging of the s any change to the measurement scale can change the result. in contrast single linkage and complete linkage are invariant to monotonic transformations of since they leave the relative ordering the same. divisive clustering divisive clustering starts with all the data in a single cluster and then recursively divides each cluster into two daughter clusters in a top-down fashion. since there are ways to split a group of n items into groups it is hard to compute the optimal split so various heuristics are used. one approach is pick the cluster with the largest diameter and split it in two using the k-means or k-medoids algorithm with k this is called the bisecting k-means algorithm et al. we can repeat this until we have any desired number of clusters. this can be used as an alternative to regular k-means but it also induces a hierarchical clustering. another method is to build a minimum spanning tree from the dissimilarity graph and then to make new clusters by breaking the link corresponding to the largest dissimilarity. actually gives the same results as single link agglomerative clustering. is as follows. we start with a single cluster containing all the data g n. we then measure the average dissimilarity of i g to all the other g yet another method called dissimilarity analysis et al. g dg i ng we remove the most dissimilar object and put it in its own cluster h i arg max i g i g g h dg we now continue to move objects from g to h until some stopping criterion is met. specifically to move that maximizes the average dissimilarity to each g but minimizes we pick a point i the average dissimilarity to each h dh i nh h i i dh dg i arg max i g i dh i we continue to do this until dg is negative. the final result is that we have split g into two daughter clusters g and h. we can then recursively call the algorithm on g andor h or on any other node in the tree. for example we might choose to split the node g whose average dissimilarity is highest or whose maximum dissimilarity diameter is highest. we continue the process until the average dissimilarity within each cluster is below some threshold andor all clusters are singletons. divisive clustering is less popular than agglomerative clustering but it has two advantages. first it can be faster since if we only split for a constant number of levels it takes just on time. second the splitting decisions are made in the context of seeing all the data whereas bottom-up methods make myopic merge decisions. hierarchical clustering choosing the number of clusters it is difficult to choose the right number of clusters since a hierarchical clustering algorithm will always create a hierarchy even if the data is completely random. but as with choosing k for k-means there is the hope that there will be a visible gap in the lengths of the links in the dendrogram the dissimilarity between merged groups between natural clusters and unnatural clusters. of course on real data this gap might be hard to detect. in section we will present a bayesian approach to hierarchical clustering that nicely solves this problem. bayesian hierarchical clustering there are several ways to make probabilistic models which produce results similar to hierarchical clustering e.g. neal castro et al. lau and green here we present one particular approach called bayesian hierarchical clustering and ghahramani algorithmically it is very similar to standard bottom-up agglomerative clustering and takes comparable time whereas several of the other techniques referenced above are much slower. however it uses bayesian hypothesis tests to decide which clusters to merge any rather than computing the similarity between groups of points in some ad-hoc way. these hypothesis tests are closely related to the calculations required to do inference in a dirichlet process mixture model as we will see. furthermore the input to the model is a data matrix not a dissimilarity matrix. the algorithm let d xn represent all the data and let di be the set of datapoints at the leaves of the substree ti. at each step we compare two trees ti and tj to see if they should be merged into a new tree. define dij as their merged data and let mij if they should be merged and mij otherwise. the probability of a merge is given by pdijtij rij pdijmij pdijtij dijmij pdijmij here pmij is the prior probability of a merge which can be computed using a bottom-up algorithm described below. we now turn to the likelihood terms. if mij the data in dij is assumed to come from the same model and hence p pdijmij pxn if mij the data in dij is assumed to have been generated by each tree independently so pdijmij pditipdjtj xn dij these two terms will have already been computed by the bottom-up process. consequently we have all the quantities we need to decide which trees to merge. see algorithm for the pseudocode assuming pmij is uniform. when finished we can cut the tree at points where rij chapter clustering algorithm bayesian hierarchical clustering initialize di i compute pditi i repeat for each pair of clusters i j do compute pdijtij find the pair di and dj with highest merge probability rij merge dk di dj delete di dj until all clusters merged the connection with dirichlet process mixture models in this section we will establish the connection between bhc and dpmms. this will in turn give us an algorithm to compute the prior probabilities pmij note that the marginal likelihood of a dpmm summing over all partitions is given by pdk pvpdv l v v mv pdv l pv pdv l where v is the set of all possible partitions of dk pv is the probability of partition v mv is is the number of points in cluster l of partition v dv the number of clusters in partition v nv are the points in cluster l of partition v and nk are the number of points in dk. l one can show and ghahramani that pdktk computed by the bhc algorithm is similar to pdk given above except for the fact that it only sums over partitions which are consistent with tree tk. number of tree-consistent partitions is exponential in the number of data points for balanced binary trees but this is obviously a subset of all possible partitions. in this way we can use the bhc algorithm to compute a lower bound on the marginal likelihood of the data from a dpmm. furthermore we can interpret the algorithm as greedily searching through the exponentially large space of tree-consistent partitions to find the best ones of a given size at each step. we are now in a position to compute k pmk for each node k with children i and j. this is equal to the probability of cluster dk coming from the dpmm relative to all other partitions of dk consistent with the current tree. this can be computed as follows initialize di and i for each leaf i then as we build the tree for each internal node k compute dk idj and k wherei and j are k s left and right children. dk clustering datapoints and features data set synthetic newsgroups spambase digits fglass single linkage complete linkage average linkage bhc table purity scores for various hierarchical clustering schemes applied to various data sets. the synthetic data has n d c and real features. newsgroups is extracted from the newsgroups dataset n c binary features. spambase has n c d binary features. digits is the cedar buffalo digits c d binarized features. fglass is forensic glass dataset c d real features. source table of and ghahramani used with kind permission of katherine heller. learning the hyper-parameters the model has two free-parameters and where are the hyper-parameters for the prior on the parameters in and ghahramani they show how one can back-propagate gradients of the form pdktk through the tree and thus perform an empirical bayes estimate of the hyper-parameters. experimental results and ghahramani compared bhc with traditional agglomerative clustering algorithms on various data sets in terms of purity scores. the results are shown in table we see that bhc did much better than the other methods on all datasets except the forensic glass one. figure visualizes the tree structure estimated by bhc and agglomerative hierarchical clustering on the newsgroup data a beta-bernoulli model. the bhc tree is clearly superior at the colors at the leaves which represent class labels. figure is a zoom-in on the top few nodes of these two trees. bhc splits off clusters concerning sports from clusters concerning cars and space. ahc keeps sports and cars merged together. although sports and cars both fall under the same rec newsgroup heading opposed to space that comes under the sci newsgroup heading the bhc clustering still seems more reasonable and this is borne out by the quantitative purity scores. bhc has also been applied to gene expression data with good results et al. clustering datapoints and features so far we have been concentrating on clustering datapoints. but each datapoint is often described by multiple features and we might be interested in clustering them as well. below we describe some methods for doing this. chapter clustering newsgroups average linkage clustering newsgroups bayesian hierarchical clustering figure hierarchical clustering applied to documents from newsgroups is rec.autos blue is rec.sport.baseball green is rec.sport.hockey and magenta is sci.space. top average linkage hierarchical clustering. bottom bayesian hierarchical clustering. each of the leaves is labeled with a color according to which newsgroup that document came from. we see that the bayesian method results in a clustering that is more consistent with these labels were not used during model fitting. source figure of and ghahramani used with kind permission of katherine heller. clustering datapoints and features all data game team play car space nasa baseball pitch hit nhl hockey round car dealer drive space nasa orbit all data quebec jet boston car baseball engine pitcher boston ball car player space team game hockey vehicle dealer driver figure zoom-in on the top nodes in the trees of figure average linkage. we show the most probable words per cluster. the number of documents at each cluster is also given. source figure of and ghahramani used with kind permission of katherine heller. bayesian method. biclustering clustering the rows and columns is known as biclustering or coclustering. this is widely used in bioinformatics where the rows often represent genes and the columns represent conditions. it can also be used for collaborative filtering where the rows represent users and the columns represent movies. a variety of ad hoc methods for biclustering have been proposed see and oliveira for a review. here we present a simple probabilistic generative model based on et al. also et al. for a related approach. the idea is to associate each row and each column with a latent indicator ri k r cj k c. we then assume the data are iid across samples and across features within each block pxr c pxijri cj pxij ricj i j where ab are the parameters for row cluster a and column cluster b. rather than using a finite number of clusters for the rows and columns we can use a dirchlet process as in the infinite relational model which we discuss in section we can fit this model using e.g. gibbs sampling. the behavior of this model is illustrated in figure the data has the form xi j iff animal i has feature j where i and j the animals represent whales bears horses etc. the features represent properties of the habitat tree coastal or anatomical properties teeth quadrapedal or behavioral properties eats meat etc. the model using a bernoulli likelihood was fit to the data. it discovered animal clusters and feature clusters. for example it discovered a bicluster that represents the fact that mammals tend to have aquatic features. multi-view clustering the problem with biclustering is that each object can only belong to one cluster. intuitively an object can have multiple roles and can be assigned to different clusters depending on which chapter clustering killer whale blue whale humpback seal walrus dolphin antelope horse giraffe zebra deer monkey gorilla chimp hippo elephant rhino grizzly bear polar bear flippers strain teeth swims arctic coastal ocean water hooves long neck horns hands bipedal jungle tree bulbous body shape slow inactive meat teeth eats meat hunter fierce walks quadrapedal ground figure illustration of biclustering we show of the animal clusters and of the feature clusters. the original data matrix is shown partitioned according to the discovered clusters. from figure of et al. used with kind permission of charles kemp. cj jk k j d riv v xij i n figure illustration of multi-view clustering. here we have views partitions. in the first view we have clusters partitions. in the third view we have clusters. the number of views and partitions are inferred from data. rows within each colored block are assumed to generated iid however each column can have a different distributional form which is useful for modeling discrete and continuous data. from figure of et al. used with kind permission of jennifer dy. corresponding dgm. in the second view we have clusters. subset of features you use. for example in the animal dataset we may want to group the animals on the basis of anatomical features mammals are warm blooded reptiles are not or on the basis of behavioral features predators vs prey. we now present a model that can capture this phenomenon. this model was independently proposed in et al. mansinghka et al. who call it crosscat crosscategorization and in et al. cui et al. who call it multi-clust. also and ghosh for a very similar model. the idea is that we partition the columns into v groups or views so cj v where j d indexes clustering datapoints and features features. we will use a dirichlet process prior for pc which allows v to grow automatically. then for each partition of the columns each view call it v we partition the rows again using a dp as illustrated in figure let riv kv be the cluster to which the i th row belongs in view v. finally having partitioned the rows and columns we generate the data we assume all the rows and columns within a block are iid. we can define the model more precisely as follows pc rd cprcpdr c pc c prc v v pdr c dprv pxij jkp jkd jk jcj irivk see figure for the if the data is binary and we use a beta prior for jk the likelihood reduces to v jcj pdr c betanjkv njkv beta where njkv irivk ixij counts the number of features which are on in the j th column for view v and for row cluster k. similarly njkv counts how many features are off. the model is easily extended to other kinds of data by replacing the beta-bernoulli with say the gaussian-gamma-gaussian model as discussed in et al. mansinghka et al. approximate map estimation can be done using stochastic search et al. and approximate inference can be done using variational bayes et al. or gibbs sampling et al. the hyper-parameter for the likelihood can usually be set in a noninformative way but results are more sensitive to the other two parameters since controls the number of column partitions and controls the number of row partitions. hence a more robust technique is to infer the hyper-parameters using mh. this also speeds up convergence et al. figure illustrates the model applied to some binary data containing animals and features. the figures shows the map partition. the first partition of the columns contains taxonomic features such as has bones is warm-blooded lays eggs etc. this divides the animals into birds reptiles amphibians mammals and invertebrates. the second partition of the columns contains features that are treated as noise with no apparent structure for the single row labeled frog the third partition of the columns contains ecological features like dangerous carnivorous lives in water etc. this divides the animals into prey land predators sea predators and air predators. thus each animal can belong to a different the dependence between r and c is not shown since it is not a dependence between the values of riv and cj but between the cardinality of v and cj. in other words the number of row partitions we need to specify number of views indexed by v depends on the number of column partitions that we have. a leopard sheep seal dolphin monkey bat alligator iguana frog python finch ostrich seagull owl penguin eagle grasshopper ant bee jellyfish octopus dragonfly b c frog chapter clustering leopard alligator python seal dolphin frog jellyfish octopus penguin finch seagull owl eagle dragonfly bat grasshopper ant bee sheep monkey iguana ostrich d r a z i l d r o c l n e e r g s i a s i s k w a u q s k a e b a s a h e u g n o t a s a h s r e p p i l f s a h l i a t a s a h i s w a p s a h n a r b e g r a y r r u f s i n w o r b s i e c i m s t a e s t n e d o r s t a e t u o n s a s a h e a n n e t n a s a h i a n p s a s a h l a s a h s g g e s y a s e n o b s a h l l a m m a m a s i l d e d o o b m r a w s i t e e f s a h h t e e t s a h t r a m s s i s p u o r g n i s l e v a r t s e s i o n d u o l s e k a m l l a t s i h s i f a s i y m i l s s i s r a o r s n i f s a h e n i l e f a s i s n r o h s a h s e v o o h s a h t n e d o r a s i s e k a l n i s e v i l i i n a b h p m a n a s i h t o o m s e e r t n e g r a l s i s t u n s t a e s s i i s e v i l t e e f d e b b e w s a h s e t a m l i l c d o c n i s e v i l i s u o i c o r e f s i s u o r e g n a d s i e r o v n r a c a s i r o t a d e r p a s i r e t a w n i s e v i l s e i l f g n o l s i s s a r g n h s i f s t a e s e t a m s e v a e l s t a e i s l a m n a s t a e i s e v i l i l c t o h n i s e v i l figure map estimate produced by the crosscat system when applied to a binary data matrix of animals by features see text for details. source figure of et al. used with kind permission of vikash mansingkha. cluster depending on what set of features are considered. uncertainty about the partitions can be handled by sampling. it is interesting to compare this model to a standard infinite mixture model. while the standard model can represent any density on fixed-sized vectors as n it cannot cope with d since it has no way to handle irrelevant noisy or redundant features. by contrast the crosscatmulti-clust system is robust to irrelevant features it can just partition them off and cluster the rows only using the relevant features. note however that it does not need a separate background model since everything is modelled using the same mechanism. this is useful since one s person s noise is another person s signal. this symmetry may explain why multi-clust outperformed the sparse mixture model approach of et al. in the experiments reported in et al. graphical model structure learning introduction we have seen how graphical models can be used to express conditional independence assumptions between variables. in this chapter we discuss how to learn the structure of the graphical model itself. that is we want to compute pgd whereg is the graph structure represented as an v v adjacency matrix. as we discussed in section there are two main applications of structure learning knowledge discovery and density estimation. the former just requires a graph topology whereas the latter requires a fully specified model. the main obstacle in structure learning is that the number of possible graphs is exponential in the number of nodes a simple upper bound is thus the full posterior pgd is prohibitively large even if we could afford to compute it we could not even store it. so we will seek appropriate summaries of the posterior. these summary statistics depend on our task. is knowledge discovery we may want to compute posterior edge marginals pgst we can then plot the corresponding graph where the thickness of each edge represents our confidence in its presence. by setting a threshold we can generate a sparse graph which can be useful for visualization purposes figure if our goal is density estimation we may want to compute the map graph g argmaxg pgd. if our goal in most cases finding the globally optimal graph will take exponential time so we will use discrete optimization methods such as heuristic search. however in the case of trees we can find the globally optimal graph structure quite efficiently using exact methods as we discuss in section if density estimation is our only goal it is worth considering whether it would be more appropriate to learn a latent variable model which can capture correlation between the visible variables via a set of latent common causes chapters and such models are often easier to learn and perhaps more importantly they can be applied prediction purposes much more efficiently since they do not require performing inference in a learned graph with potentially high treewidth. the downside with such models is that the latent factors are often unidentifiable and hence hard to interpret. of course we can combine graphical model structure learning and latent variable learning as we will show later in this chapter. in some cases we don t just want to model the observed correlation between variables instead we want to model the causal structure behind the data so we can predict the effects of manipulating variables. this is a much more challenging task which we briefly discuss in chapter graphical model structure learning bible case course evidence children mission launch christian fact israel government earth gun nasa lunar god human jews war president law orbit shuttle moon jesus computer religion world rights solar space science university state figure part of a relevance network constructed from the data shown in figure we show edges whose mutual information is greater than or equal to of the maximum pairwise mi. for clarity the graph has been cropped so we only show a subset of the nodes and edges. figure generated by relevancenetworknewsgroupdemo. section structure learning for knowledge discovery since computing the map graph or the exact posterior edge marginals is in general computationally intractable in this section we discuss some quick and dirty methods for learning graph structures which can be used to visualize one s data. the resulting models do not constitute consistent joint probability distributions so they cannot be used for prediction and they cannot even be formally evaluated in terms of goodness of fit. nevertheless these methods are a useful ad hoc tool to have in one s data visualization toolbox in view of their speed and simplicity. relevance networks a relevance network is a way of visualizing the pairwise mutual information between multiple random variables we simply choose a threshold and draw an edge from node i to node j if i xj is above this threshold. in the gaussian case i xj ij where ij is the correlation coefficient exercise so we are essentially visualizing this is known as the covariance graph this method is quite popular in systems biology et al. where it is used to visualize the interaction between genes. the trouble with biological examples is that they are hard for non-biologists to understand. so let us instead illustrate the idea using natural language text. figure gives an example where we visualize the mi between words in the newsgroup dataset from figure the results seem intuitively reasonable. however relevance networks suffer from a major problem the graphs are usually very dense since most variables are dependent on most other variables even after thresholding the mis. for example suppose directly influences which directly influences these form components of a signalling cascade then has non-zero mi with vice versa so there will be a edge in the relevance network. indeed most pairs will be structure learning for knowledge discovery children case course fact question earth bible christian food baseball mission god disk mac car aids doctor fans nasa jesus pc dos drive bmw israel government health games hockey hit launch memory scsi jews engine dealer state war computer president medicine season puck nhl shuttle religion data card honda power oil world insurance science studies team software solar graphics driver gun research university water human cancer win league lunar system display video windows law disease won players moon server files rights problem evidence space program format patients msg help mars orbit technology ftp image number vitamin email satellite version phone figure a dependency network constructed from the data. we show all edges with regression weight above in the markov blankets estimated by penalized logistic regression. undirected edges represent cases where a directed edge was found in both directions. from figure of used with kind permission of mark schmidt. connected. a better approach is to use graphical models which represent conditional independence in the above example is conditionally independent of given rather than dependence. so there will not be a edge. consequently graphical models are usually much sparser than relevance networks and hence are a more useful way of visualizing interactions between multiple variables. dependency networks a simple and efficient way to learn a graphical model structure is to independently fit d sparse full-conditional distributions pxtx t this is called a dependency network et al. the chosen variables constitute the inputs to the node i.e. its markov blanket. we can then visualize the resulting sparse graph. the advantage over relevance networks is that redundant variables will not be selected as inputs. we can use any kind of sparse regression or classification method to fit each cpd. and buhlmann use al. uses classification regression trees regularized linear regression et al. use logistic regression depnetfit for some code uses bayesian variable selection etc. chapter graphical model structure learning and buhlmann discuss theoretical conditions under which linear regression can recover the true graph structure assuming the data was generated from a sparse gaussian graphical model. figure shows a dependency network that was learned from the data using regularized logistic regression where the penalty parameter was chosen by bic. many of the words present in these estimated markov blankets represent fairly natural associations baseballfans biblegod bmwcar cancerpatients etc.. however some of the estimated statistical dependencies seem less intuitive such as baseballwindows and bmwchristian. we can gain more insight if we look not only at the sparsity pattern but also the values of the regression weights. for example here are the incoming weights for the first words aids children disease fact health president research baseball christian drive games god government hit memory players season software windows bible car card christian fact god jesus orbit program religion version bmw car christian engine god government help windows cancer disease medicine patients research studies words in italic red have negative weights which represents a dissociative relationship. for example the model reflects that baseballwindows is an unlikely combination. it turns out that most of the weights are negative negative positive zero in this model. in addition to visualizing the data a dependency network can be used for inference. however the only algorithm we can use is gibbs sampling where we repeatedly sample the nodes with missing values from their full conditionals. unfortunately a product of full conditionals does not in general constitute a representation of any valid joint distribution et al. so the output of the gibbs sampler may not be meaningful. nevertheless the method can sometimes give reasonable results if there is not much missing data and it is a useful method for data imputation and raghunathan in addition the method can be used as an initialization technique for more complex structure learning methods that we discuss below. learning tree structures for the rest of this chapter we focus on learning fully specified joint probability models which can be used for density estimation prediction and knowledge discovery. since the problem of structure learning for general graphs is np-hard we start by considering the special case of trees. trees are special because we can learn their structure efficiently as we disuscs below and because once we have learned the tree we can use them for efficient exact inference as discussed in section learning tree structures figure an undirected tree and two equivalent directed trees. directed or undirected tree? before continuing we need to discuss the issue of whether we should use directed or undirected trees. a directed tree with a single root node r defines a joint distribution as follows pxt pxtxpat where we define par for example in figure we have we see that the choice of root does not matter both of these models are equivalent. to make the model more symmetric it is preferable to use an undirected tree. this can be represented as follows pxt pxt e t v t v pxs xt pxspxt where pxs xt is an edge marginal and pxt is a node marginal. for example in figure we have to see the equivalence with the directed representation let us cancel terms to get where pxtxs pxs xtpxs. thus a tree can be represented as either an undirected or directed graph the number of parameters is the same and hence the complexity of learning is the same. and of course inference is the same in both representations too. the undirected representation which is symmetric is useful for structure learning but the directed representation is more convenient for parameter learning. chapter graphical model structure learning chow-liu algorithm for finding the ml tree structure using equation we can write the log-likelihood for a tree as follows log pd t ntk log pxt k k t st jk nstjk log pxs j xt k pxs j k where nstjk is the number of times node s is in state j and node t is in state k and ntk is the number of times node t is in state k. we can rewrite these counts in terms of the empirical distribution nstjk n pempxs j xt k and ntk n pempxt k. setting to the mles this becomes log pd t pempxt k log pempxt k n t v ixs xt st k et where ixs xt st is the mutual information between xs and xt given the empirical distribution ixs xt st pempxs j xt k log pempxs j xt k pempxs jpempxt k j k since the first term in equation is independent of the topology t we can ignore it when learning structure. thus the tree topology that maximizes the likelihood can be found by computing the maximum weight spanning tree where the edge weights are the pairwise mutual informations iys yt st. this is called the chow-liu algorithm and liu there are several algorithms for finding a max spanning tree the two best known are prim s algorithm and kruskal s algorithm. both can be implemented to run in oe log v time where e v is the number of edges and v is the number of nodes. see e.g. and wayne for details. thus the overall running time is on v v log v where the first term is the cost of computing the sufficient statistics. figure gives an example of the method in action applied to the binary newsgroups data shown in figure the tree has been arbitrarily rooted at the node representing email the connections that are learned seem intuitively reasonable. finding the map forest since all trees have the same number of parameters we can safely used the maximum likelihood score as a model selection criterion without worrying about overfitting. however sometimes we may want to fit a forest rather than a single tree since inference in a forest is much faster than in a tree can run belief propagation in each tree in the forest in parallel. the mle criterion will never choose to omit an edge. however if we use the marginal likelihood or a penalized likelihood as bic the optimal solution may be a forest. below we give the details for the marginal likelihood case. learning tree structures email ftp phone files number disk format windows drive memory system image card dos driver pc program version car scsi data problem display graphics video software space win team won bmw dealer engine honda mac help server launch moon nasa orbit shuttle technology fans games hockey league players puck season oil lunar mars earth satellite solar mission nhl baseball god hit bible christian jesus religion jews government israel children power president rights state war health human law university world aids food insurance medicine fact gun research science msg water patients studies case course evidence question computer cancer disease doctor vitamin figure the mle tree on the data. from figure of used with kind permission of mark schmidt. topologically equivalent tree can be produced using chowliutreedemo. in section we explain how to compute the marginal likelihood of any dag using a dirichlet prior for the cpts. the resulting expression can be written as follows log pdt pxitxipat tp td t scorentpat log t v t where ntpat are the counts statistics for node t and its parents and score is defined in equation now suppose we only allow dags with at most one parent. following et al. let us associate a weight with each s t edge wst scorets where is the score when t has no parents. note that the weights might be negative the mle case where edge weights are aways non-negative because they correspond to mutual information. then we can rewrite the objective as follows log pdt scoretpat wpatt t t t the last term is the same for all trees t so we can ignore it. thus finding the most probable tree amounts to finding a maximal branching in the corresponding weighted directed graph. this can be found using the algorithm in et al. chapter graphical model structure learning if the scoring function is prior and likelihood equivalent terms are explained in sec tion we have scorest scorets and hence the weight matrix is symmetric. in this case the maximal branching is the same as the maximal weight forest. we can apply a slightly modified version of the mst algorithm to find this et al. to see this let g e be a graph with both positive and negative edge weights. now let be a graph obtained by omitting all the negative edges from g. this cannot reduce the total weight so we can find the maximum weight forest of g by finding the mst for each connected component of we can do this by running kruskal s algorithm directly on there is no need to find the connected components explicitly. mixtures of trees a single tree is rather limited in its expressive power. later in this chapter we discuss ways to learn more general graphs. however the resulting graphs can be expensive to do inference in. an interesting alternative is to learn a mixture of trees and jordan where each mixture component may have a different tree topology. this is like an unsupervised version of the tan classifier discussed in section we can fit a mixture of trees by using em in the e step we compute the responsibilities of each cluster for each data point and in the m step we use a weighted version of the chow-liu algorithm. see and jordan for details. in fact it is possible to create an infinite mixture of trees by integrating out over all possible trees. remarkably this can be done in v time using the matrix tree theorem. this allows us to perform exact bayesian inference of posterior edge marginals etc. however it is not tractable to use this infinite mixture for inference of hidden nodes. see and jaakkola for details. learning dag structures in this section we discuss how to compute of pgd where g is constrained to be a dag. this is often called bayesian network structure learning. in this section we assume there is no missing data and that there are no hidden variables. this is called the complete data assumption. for simplicity we will focus on the case where all the variables are categorical and all the cpds are tables although the results generalize to real-valued data and other kinds of cpds such as linear-gaussian cpds. our presentation is based in part on et al. although we will follow the notation of section in particular let xit kt be the value of node t in case i where kt is the number of states for node t. let tck pxt kxpat c for k t and c t where ct is the number of parent combinations conditioning cases. for notational simplicity we will often assume kt k so all nodes have the same number of states. we will also let dt dimpat be the degree or fan-in of node t so that ct k dt. markov equivalence in this section we discuss some fundamental limits to our ability to learn dag structures from data. learning dag structures figure three dags. and are markov equivalent is not. consider the following dgms x y z x y z and x y z. these all represent the same set of ci statements namely x zy x z we say these graphs are markov equivalent since they encode the same set of ci assumptions. that is they all belong to the same markov equivalence class. however the v-structure x y z encodes x z and x zy which represents the opposite set of ci assumptions. one can prove the following theorem. theorem and pearl and pearl two structures are markov equivalent iff they have the same undirected skeleton and the same set of v-structures. for example referring to figure we see that since reversing the arc creates a new v-structure. however since reversing the arc does not create a new v-structure. we can represent a markov equivalence class using a single partially directed acyclic graph also called an essential graph or pattern in which some edges are directed and some undirected. the undirected edges represent reversible edges any combination is possible so long as no new v-structures are created. the directed edges are called compelled edges since changing their orientation would change the v-structures and hence change the equivalence class. for example the pdag x y z represents y z x y z x y z which encodes x z and x zy see figure the significance of the above theorem is that when we learn the dag structure from data we will not be able to uniquely identify all of the edge directions even given an infinite amount of data. we say that we can learn dag structure up to markov equivalence this also cautions us not to read too much into the meaning of particular edge orientations since we can often change them without changing the model in any observable way. chapter graphical model structure learning x y z x y z x y z x y z x y z x y z figure pdag representation of markov equivalent dags. exact structural inference in this section we discuss how to compute the exact posterior over graphs pgd ignoring for now the issue of computational tractability. deriving the likelihood assuming there is no missing data and that all cpds are tabular the likelihood can be written as follows pdg catxitxipat t catxit tcixipatc ixitkxipatc tck ntck tck where ntck is the number of times node t is in state k and its parents are in state c. these counts depend on the graph structure g but we drop this from the notation. deriving the marginal likelihood of course choosing the graph with the maximum likelihood will always pick a fully connected graph to the acyclicity constraint since this maximizes the number of parameters. to avoid such overfitting we will choose the graph with the maximum marginal likelihood pdg the magic of the bayesian occam s razor will then penalize overly complex graphs. to compute the marginal likelihood we need to specify priors on the parameters. we will make two standard assumptions. first we assume global prior parameter independence which means p p t learning dag structures second we assume local prior parameter independence which means p t p tc for each t. must be a dirichlet and heckerman that is it turns out that these assumtions imply that the prior for each row of each cpt p tc dir tc tc given these assumptions and using the results of section we can write down the marginal likelihood of any dag as follows dir tcd tc catxit tc ixipatc bntc tc b tc g tck tc g ijk pdg scorentpat where ntc node t and its parents and score is a local scoring function defined by k tck ntpat is the vector of counts statistics for k ntck tc scorentpat bntc tc b tc we say that the marginal likelihood decomposes or factorizes according to the graph structure. setting the prior how should we set the hyper-parameters tck? it is tempting to use a jeffreys prior of the form tck however it turns out that this violates a property called likelihood equivalence which is sometimes considered desirable. this property says that if and are markov equivalent they should have the same marginal likelihood since they are essentially equivalent models. geiger and heckerman proved that for complete graphs the only prior that satisfies likelihood equivalence and parameter independence is the dirichlet prior where the pseudo counts have the form tck k xpat c where is called the equivalent sample size and is some prior joint probability distribution. this is called the bde prior which stands for bayesian dirichlet likelihood equivalent. chapter graphical model structure learning to derive the hyper-parameters for other graph structures geiger and heckerman invoked an additional assumption called parameter modularity which says that if node xt has the same parents in and then p with this assumption we can always derive t for a node t in any other graph by marginalizing the pseudo counts in equation typically the prior distribution is assumed to be uniform over all possible joint configura tions. in this case we have tck ktct ktct thus if we sum the pseudo counts over all ct kt since k xpat c entries in the cpt we get a total equivalent sample size of this is called the bdeu prior where the u stands for uniform. this is the most widely used prior for learning bayes net structures. for advice on setting the global tuning parameter see et al. simple worked example we now give a very simple worked example from suppose we have just binary nodes and the following data cases suppose we are interested in two possible graphs is and is the disconnected graph. the empirical counts for node in are and for node are the bdeu prior for is and for the prior for is the same and for it is if we set and use the bdeu prior we find and hence the posterior probabilites under a uniform graph prior are and and example analysis of the college plans dataset we now consider a more interesting example from et al. consider the data set collected in by sewell and shah which measured variables that might influence the decision of high school students about whether to attend college. specifically the variables are as follows learning dag structures figure the two most probable dags learned from the sewell-shah data. source et al. used with kind permission of david heckerman sex male or female ses socio economic status low lower middle upper middle or high. pe parental encouragment low or high cp college plans yes or no. iq intelligence quotient discretized into low lower middle upper middle or high. these variables were measured for wisconsin high school seniors. there are possible joint configurations. heckerman et al. computed the exact posterior over all possible node dags except for ones in which sex andor ses have parents andor cp have children. prior probability of these graphs was set to based on domain knowledge. they used the bdeu score with although they said that the results were robust to any in the range to the top two graphs are shown in figure we see that the most probable one has approximately all of the probability mass so the posterior is extremely peaked. it is tempting to interpret this graph in terms of causality section in particular it seems that socio-economic status iq and parental encouragment all causally influence the decision about whether to go to college which makes sense. also sex influences college plans only indirectly through parental encouragement which also makes sense. however the direct link from socio economic status to iq seems surprising this may be due to a hidden common cause. in section we will re-examine this dataset allowing for the presence of hidden variables. the algorithm suppose we know a total ordering of the nodes. then we can compute the distribution over parents for each node independently without the risk of introducing any directed cycles we chapter graphical model structure learning simply enumerate over all possible subsets of ancestors and compute their marginal if we just return the best set of parents for each node we get the the algorithm and herskovits handling non-tabular cpds if all cpds are linear gaussian we can replace the dirichlet-multinomial model with the normalgamma model and thus derive a different exact expression for the marginal likelihood. see and heckerman for the details. in fact we can easily combine discrete nodes and gaussian nodes as long as the discrete nodes always have discrete parents this is called a conditional gaussian dag. again we can compute the marginal likelihood in closed form. see and dethlefsen for the details. in the general case everything except gaussians and cpts we need to approximate the marginal likelihood. the simplest approach is to use the bic approximation which has the form log pdt t ktct log n t scaling up to larger graphs the main challenge in computing the posterior over dags is that there are so many possible graphs. more precisely showed that the number of dags on d nodes satisfies the following recurrence if i f d i for d the base case is f solving this recurrence yields the following sequence in view of the enormous size of the hypothesis space we are generally forced to use approximate methods some of which we review below. approximating the mode of the posterior we can use dynamic programming to find the globally optimal map dag to markov equivalence and sood silander and myllmaki unfortunately this method takes v time and space making it intractable beyond about nodes. indeed the general problem of finding the globally optimal map dag is provably np-complete consequently we must settle for finding a locally optimal map dag. the most common method is greedy hill climbing at each step the algorithm proposes small changes to the current graph such as adding deleting or reversing a single edge it then moves to the neighboring graph which most increases the posterior. the method stops when it reaches a local maximum. it is important that the method only proposes local changes to the graph we can make this method more efficient by using to select the parents et al. in this case we need to approximate the marginal likelhood as we discuss below. a longer list of values can be found at interestingly the number of dags is equal to the number of matrices all of whose eigenvalues are positive real numbers et al. learning dag structures evidence case course question msg fact god gun christian nasa shuttle drive scsi disk government religion jesus car disease mission space law jews engine patients orbit games program rights power bible honda computer bmw medicine earth solar season launch technology dos dealer science moon system team satellite files problem studies mars lunar players version human israel war president hockey hit windows university nhl puck baseball won email memory ftp state research league fans win phone format video mac children world oil cancer number image data driver software water health food aids insurance doctor help vitamin pc card server graphics display figure a locally optimal dag learned from the data. from figure of used with kind permission of mark schmidt. since this enables the change in marginal likelihood hence the posterior to be computed in constant time we cache the sufficient statistics. this is because all but one or two of the terms in equation will cancel out when computing the log bayes factor log log pgd. we can initialize the search from the best tree which can be found using exact methods discussed in section for speed we can restrict the search so it only adds edges which are part of the markov blankets estimated from a dependency network figure gives an example of a dag learned in this way from the data. we can use techniques such as multiple random restarts to increase the chance of finding a good local maximum. we can also use more sophisticated local search methods such as genetic algorithms or simulated annealing for structure learning. approximating other functions of the posterior if our goal is knowledge discovery the map dag can be misleading for reasons we discussed in section a better approach is to compute the probability that each edge is present pgst of the probability there is a path from s to t. we can do this exactly using dynamic programming parviainen and koivisto unfortunately these methods take v time in the general case making them intractable for graphs with more than about chapter graphical model structure learning nodes. an approximate method is to sample dags from the posterior and then to compute the fraction of times there is an s t edge or path for each t pair. the standard way to draw samples is to use the metropolis hastings algorithm where we use the same local proposal as we did in greedy search and raftery a faster-mixing method is to use a collapsed mh sampler as suggested in and koller this exploits the fact that if a total ordering of the nodes is known we can select the parents for each node independently without worrying about cycles as discussed in section by summing over all possible choice of parents we can marginalize out this part of the problem and just sample total orders. and wong also use order-space mcmc but this time with a parallel tempering mcmc algorithm. learning dag structure with latent variables sometimes the complete data assumption does not hold either because we have missing data and or because we have hidden variables. in this case the marginal likelihood is given by pdg pd h gp pd h gp h h where h represents the hidden or missing data. in general this is intractable to compute. for example consider a mixture model where in this case there are kn possible completions of the we don t observe the cluster label. data we have k clusters we can evaluate the inner integral for each one of these assignments to h but we cannot afford to evaluate all of the integrals. course most of these integrals will correspond to hypotheses with little posterior support such as assigning single data points to isolated clusters but we don t know ahead of time the relative weight of these assignments. in this section we discuss some ways for learning dag structure when we have latent variables andor missing data. approximating the marginal likelihood when we have missing data the simplest approach is to use standard structure learning methods for fully visible dags but to approximate the marginal likelihood. in section we discussed some monte carlo methods for approximating the marginal likelihood. however these are usually too slow to use inside of a search over models. below we mention some faster deterministic approximations. bic approximation a simple approximation is to use the bic score which is given by bicg log pd g log n dimg where dimg is the number of degrees of freedom in the model and is the map or ml estimate. however the bic score often severely underestimates the true marginal likelihood and heckerman resulting in it selecting overly simple models. learning dag structure with latent variables cheeseman-stutz approximation we now present a better method known as the cheeseman-stutz approximation and stutz we first compute a map estimate of the parameters using em. denote the expected sufficient statistics of the data by d d in the case of discrete variables we just fill in the hidden variables with their expectation. we then use the exact marginal likelihood equation on this filled-in data pd gp pdg pdg however comparing this to equation we can see that the value will be exponentially smaller since it does not sum over all values of h. to correct for this we first write log pdg log pdg log pdg log pdg and then we apply a bic approximation to the last two terms log pdg log pdg dim log pd g n log pd g n dim log pd g log pd g putting it altogether we get log pdg log pdg log pd g log pd g the first term pdg can be computed by plugging in the filled-in data into the exact marginal likelihood. the second term pd g which involves an exponential sum matching the dimensionality of the left hand side can be computed using an inference algorithm. the final term pd g can be computed by plugging in the filled-in data into the regular likelihood. variational bayes em an even more accurate approach is to use the variational bayes em algorithm. recall from section that the key idea is to make the following factorization assumption p q q qzi i where zi are the hidden variables in case i. in the e step we update the qzi and in the m step we update q the corresponding variational free energy provides a lower bound on the log marginal likelihood. in and ghahramani it is shown that this bound is a much better approximation to the true log marginal likelihood estimated by a slow annealed importance sampling procedure than either bic or cs. in fact one can prove that the variational bound will always be more accurate than cs in turn is always more accurate than bic. chapter graphical model structure learning pe low low high high h piqhighpeh pmale iq ses low low high high sex ppehighsessex male female male female h ses sex pe cp h pseshighh ses low low low low high high high high iq low low high high low low high high pe low high low high low high low high pcpyessesiqpe figure the most probable dag with a single binary hidden variable learned from the sewell-shah data. map estimates of the cpt entries are shown for some of the nodes. source et al. used with kind permission of david heckerman. example college plans revisited let us revisit the college plans dataset from section recall that if we ignore the possibility of hidden variables there was a direct link from socio economic status to iq in the map dag. heckerman et al. decided to see what would happen if they introduced a hidden variable h which they made a parent of both ses and iq representing a hidden common cause. they also considered a variant in which h points to ses iq and pe. for both such cases they considered dropping none one or both of the ses-pe and pe-iq edges. they varied the number of states for the hidden node from to thus they computed the approximate posterior over different models using the cs approximation. the most probable model which they found is shown in figure this is times it is also times more more likely than the best model containing no hidden variable. likely than the second most probable model with a hidden variable. so again the posterior is very peaked. these results suggests that there is indeed a hidden common cause underlying both the socio-economic status of the parents and the iq of the children. by examining the cpt entries we see that both ses and iq are more likely to be high when h takes on the value they interpret this to mean that the hidden variable represents parent quality a genetic factor. note however that the arc between h and ses can be reversed without changing the vstructures in the graph and thus without affecting the likelihood this underscores the difficulty in interpreting hidden variables. interestingly the hidden variable model has the same conditional independence assumptions amongst the visible variables as the most probable visible variable model. so it is not possible to distinguish between these hypotheses by merely looking at the empirical conditional independencies in the data is the basis of the constraint-based approach to structure learning and verma spirtes et al. instead by adopting a bayesian approach which takes parsimony into account not just conditional independence we can discover learning dag structure with latent variables the possible existence of hidden factors. this is the basis of much of scientific and everday human reasoning e.g. and tenenbaum for a discussion. structural em one way to perform structural inference in the presence of missing data is to use a standard search procedure or stochastic and to use the methods from section to estimate the marginal likelihood. however this approach is very efficient because the marginal likelihood does not decompose when we have missing data and nor do its approximations. for example if we use the cs approximation or the vbem approximation we have to perform inference in every neighboring model just to evaluate the quality of a single move! thiesson et al. presents a much more efficient approach called the structural em algorithm. the basic idea is this instead of fitting each candidate neighboring graph and then filling in its data fill in the data once and use this filled-in data to evaluate the score of all the neighbors. although this might be a bad approximation to the marginal likelihood it can be a good enough approximation of the difference in marginal likelihoods between different models which is all we need in order to pick the best neighbor. more precisely define to be the data filled in using model with map parameters now define a modified bic score as follows scorebicgd log pd g log n dimg log pg log p where we have included the log prior for the graph and parameters. one can show that if we pick a graph g which increases the bic score relative to on the expected data it will also increase the score on the actual data i.e. scorebicgd to convert this into an algorithm we proceed as follows. first we initialize with some graph and some set of parameters then we fill-in the data using the current parameters in practice this means when we ask for the expected counts for any particular family we perform inference using our current model. we know which counts we will need we can precompute all of them which is much faster. we then evaluate the bic score of all of our neighbors using the filled-in data and we pick the best neighbor. we then refit the model parameters fill-in the data again and repeat. for increased speed we may choose to only refit the model every few steps since small changes to the structure hopefully won t invalidate the parameter estimates and the filled-in data too much. one interesting application is to learn a phylogenetic tree structure. here the observed leaves are the dna or protein sequences of currently alive species and the goal is to infer the topology of the tree and the values of the missing internal nodes. there are many classical algorithms for this task e.g. et al. but one that uses sem is discussed in et al. another interesting application of this method is to learn sparse mixture models and friedman the idea is that we have one hidden variable c specifying the cluster and we have to choose whether to add edges c xt for each possible feature xt. thus some features also et al. will be dependent on the cluster id and some will be independent. chapter graphical model structure learning figure part of a hierarchical latent tree learned from the data. from figure of and williams used with kind permission of stefan harmeling. for a different way to perform this task using regular em and a set of bits one per feature that are free to change across data cases. discovering hidden variables in section we introduced a hidden variable by hand and then figured out the local topology by fitting a series of different models and computing the one with the best marginal likelihood. how can we automate this process? figure provides one useful intuition if there is a hidden variable in the true model then its children are likely to be densely connected. this suggest the following heuristic et al. perform structure learning in the visible domain and then look for structural signatures such as sets of densely connected nodes introduce a hidden variable and connect it to all nodes in this near-clique and then let structural em sort out the details. unfortunately this technique does not work too well since structure learning algorithms are biased against fitting models with densely connected cliques. another useful intuition comes from clustering. in a flat mixture model also called a latent class model the discrete latent variable provides a compressed representation of its children. thus we want to create hidden variables with high mutual information with their children. one way to do this is to create a tree-structured hierarchy of latent variables each of which calls this a hierarchical latent class only has to explain a small set of children. model. they propose a greedy local search algorithm to learn such structures based on adding or deleting hidden nodes adding or deleting edges etc. that learning the optimal latent learning dag structure with latent variables president government power children war religion earth lunar orbit satellite solar law state human rights world israel jews bible god gun christian jesus moon mars technology mission space launch shuttle nasa health case course evidence fact question program food aids insurance msg water studies medicine car dealer cancer disease doctor patients vitamin bmw engine honda oil version files ftp email format phone windows image number card driver dos puck season team win video disk memory pc software display server games baseball league players fans hockey nhl won graphics system data scsi drive computer hit problem help mac science university research figure a partially latent tree learned from the data. note that some words can have multiple meanings and get connected to different latent variables representing different topics for example the word win can refer to a sports context by or the microsoft windows context by from figure of et al. used with kind permission of jin choi. tree is np-hard recently and williams proposed a faster greedy algorithm for learning such models based on agglomerative hierarchical clustering. rather than go into details we just give an example of what this system can learn. figure shows part of a latent forest learned from the data. the algorithm imposes the constraint that each latent node has exactly two children for speed reasons. nevertheless we see interpretable clusters arising. for example figure shows separate clusters concerning medicine sports and religion. this provides an alternative to lda and other topic models with the added advantage that inference in latent trees is exact and takes time linear in the number of nodes. an alternative approach is proposed in et al. in which the observed data is not constrained to be at the leaves. this method starts with the chow-liu tree on the observed data and then adds hidden variables to capture higher-order dependencies between internal nodes. this results in much more compact models as shown in figure this model also has better predictive accuracy than other approaches such as mixture models or trees where all the observed data is forced to be at the leaves. interestingly one can show that this method can recover the exact latent tree structure providing the data is generated from a tree. see chapter graphical model structure learning figure google s rephil model. leaves represent presence or absence of words. internal nodes represent clusters of co-occuring words or concepts all nodes are binary and all cpds are noisy-or. the model contains million word nodes million latent cluster nodes and million edges. used with kind permission of brian milch. et al. for details. note however that this approach unlike harmeling and williams requires that the cardinality of all the variables hidden and observed be the same. furthermore if the observed variables are gaussian the hidden variables must be gaussian also. case study google s rephil in this section we describe a huge dgm called rephil which was automatically learned from the model is widely used inside google for various purposes including their famous adsense the model structure is shown in figure the leaves are binary nodes and represent the presence or absence of words or compounds as new york city in a text document or query. the latent variables are also binary and represent clusters of co-occuring words. all cpds are noisy-or since some leaf nodes words can have many parents. this means each edge can be augmented with a hidden variable specifying if the link was activated or not if the link is not active then the parent cannot turn the child on. very similar model was proposed independently in and hauskrecht parameter learning is based on em where the hidden activation status of each edge needs to be inferred and heckerman structure learning is based on the old neuroscience the original system called phil was developed by georges harik and noam shazeer. it has been published as us patent method and apparatus for learning a probabilistic generative model for text filed in rephil is a more probabilistically sound version of the method developed by uri lerner et al. the summary below is based on notes by brian milch also works at google. adsense is google s system for matching web pages with content-appropriate ads in an automatic way by extracting semantic keywords from web pages. these keywords play a role analogous to the words that users type in when searching this latter form of information is used by google s adwords system. the details are secret but gives an overview. learning dag structure with latent variables idea that nodes that fire together should wire together to implement this we run inference and check for cluster-word and cluster-cluster pairs that frequently turn on together. we then add an edge from parent to child if the link can significantly increase the probability of the child. links that are not activated very often are pruned out. we initialize with one cluster per document to a set of semantically related phrases. we then merge clusters a and b if a explains b s top words and vice versa. we can also discard clusters that are used too rarely. the model was trained on about billion text snippets or search queries this takes several weeks even on a parallel distributed computing architecture. the resulting model contains million word nodes and about million latent cluster nodes. there are about million links in the model including many cluster-cluster dependencies. the longest path in the graph has length so the model is quite deep. exact inference in this model is obviously infeasible. however note that most leaves will be off since most words do not occur in a given query such leaves can be analytically removed as shown in exercise we an also prune out unlikely hidden nodes by following the strongest links from the words that are on up to their parents to get a candidate set of concepts. we then perform iterative conditional modes to find a good set of local maxima. at each step of icm each node sets its value to its most probable state given the values of its neighbors in its markov blanket. this continues until it reaches a local maximum. we can repeat this process a few times from random starting configurations. at google this can be made to run in milliseconds! structural equation models a structural equation model is a special kind of directed mixed graph possibly cyclic in which all cpds are linear gaussian and in which all bidirected edges represent correlated gaussian noise. such models are also called path diagrams. sems are widely used especially in economics and social science. it is common to interpret the edge directions in terms of causality where directed cycles are interpreted is in terms of feedback loops e.g. however the model is really just a way of specifying a joint gaussian as we show below. there is nothing inherently causal about it at all. discuss causality in section we can define an sem as a series of full conditionals as follows xi i wijxj where n we can rewrite the model in matrix form as follows x wx x w hence the joint distribution is given by px n where w w t we draw an arc xi xj if if w is lower triangular then the graph is acyclic. if in addition is diagonal then the model is equivalent to a gaussian dgm as discussed in section such models are called recursive. if is not diagonal then we draw a bidirected chapter graphical model structure learning figure a cyclic directed mixed graphical model sem. note the feedback loop. arc xi xj for each non-zero off-diagonal term. such edges represent correlation possibly due to a hidden common cause. when using structural equation models it is common to partition the variables into latent variables zt and observed or manifest variables yt. for example figure illustrates the following model where the presence of a feedback loop is evident from the fact that w is not lower triangular. also the presence of confounding between and is evident in the off-diagonal terms in often we assume there are multiple observations for each latent variable. to ensure identifiability we can set the mean of the latent variables zt to and we can set the regression weights of zt yt to this essentially defines the scale of each latent variable. addition to the z s there are the extra hidden variables implied by the presence of the bidirected edges. the standard practice in the sem community as exemplified by the popular commercial software package called lisrel from httpwww.ssicentral.comlisrel is to learning causal dags build the structure by hand to estimate the parameters by maximum likelihood and then to test if any of the regression weights are significantly different from using standard frequentist methods. however one can also use bayesian inference for the parameters e.g. et al. structure learning in sems is rare but since recursive sems are equivalent to gaussian dags many of the techniques we have been discussing in this section can be applied. sems are closely related to factor analysis models the basic difference is that in an fa model the latent gaussian has a low-rank covariance matrix and the observed noise has a diagonal covariance no bidirected edges. in an sem the covariance of the latent gaussian has a sparse cholesky decomposition least if w is acyclic and the observed noise might have a full covariance matrix. note that sems can be extended in many ways. for example we can add covariates input variables noisily observed we can make some of the observations be discrete by using probit links and so on. learning causal dags causal models are models which can predict the effects of interventions to or manipulations of a system. for example an electronic circuit diagram implicitly provides a compact encoding of what will happen if one removes any given component or cuts any wire. a causal medical model might predict that if i continue to smoke i am likely to get lung cancer hence if i cease smoking i am less likely to get lung cancer. causal claims are inherently stronger yet more useful than purely associative claims such as people who smoke often have lung cancer causal models are often represented by dags although this is somewhat controversial we explain this causal interpretation of dags below. we then show how to use a dag to do causal reasoning. finally we briefly discuss how to learn the structure of causal dags. a more detailed description of this topic can be found in and and friedman causal interpretation of dags in this section we define a directed edge a b in a dag to mean that a directly causes b so if we manipulate a then b will change. this is known as the causal markov assumption. course we have not defined the word causes and we cannot do that by appealing to a dag lest we end up with a cyclic definition see for further disussion of this point. we will also assume that all relevant variables are included in the model i.e. there are no unknown confounders reflecting hidden common causes. this is called the causal sufficiency assumption. there are known to be confounders they should be added to the model although one can sometimes use mixed directed graphs as a way to avoid having to model confounders explicitly. assuming we are willing to make the causal markov and causal sufficiency assumptions we can use dags to answer causal questions. the key abstraction is that of a perfect intervention this represents the act of setting a variable to some known value say setting xi to xi. a real world example of such a perfect intervention is a gene knockout experiment in which a gene is silenced we need some notational convention to distinguish this from observing that xi chapter graphical model structure learning g gdoxx x x figure surgical intervention on x. based on er happens to have value xi. we use pearl s do calculus notation in the verb to do and write doxi xi to denote the event that we set xi to xi. a causal model can be used to make inferences of the form pxdoxi xi which is different from making inferences of the form pxxi xi. to understand the difference between conditioning on interventions and conditioning on observations the difference between doing and seeing consider a node dgm s y in which s if you smoke and s otherwise and y if you have yellow-stained fingers and y otherwise. if i observe you have yellow fingers i am licensed to infer that you are probably a smoker nicotine causes yellow stains ps p however if i intervene and paint your fingers yellow i am no longer licensed to infer this since i have disrupted the normal causal mechanism. thus ps s one way to model perfect interventions is to use graph surgery represent the joint distribution by a dgm and then cut the arcs coming into any nodes that were set by intervention. see figure for an example. this prevents any information flow from the nodes that were intervened on from being sent back up to their parents. having perform this surgery we can then perform probabilistic inference in the resulting mutilated graph in the usual way to reason about the effects of interventions. we state this formally as follows. theorem theorem spirtes et al. to compute pxidoxj for sets of nodes i j we can perform surgical intervention on the xj nodes and then use standard probabilistic inference in the mutilated graph. we can generalize the notion of a perfect intervention by adding interventions as explicit action nodes to the graph. the result is like an influence diagram except there are no utility nodes dawid this has been called the augmented dag we learning causal dags y x figure illustration of simpson s paradox. figure generated by simpsonsparadoxgraph. can then define the cpd pxidoxi to be anything we want. we can also allow an action to affect multiple nodes. this is called a fat hand intervention a reference to someone trying to change a single component of some system an electronic circuit but accidently touching multiple components and thereby causing various side effects and murphy for a way to model this using augmented dags. using causal dags to resolve simpson s paradox in this section we assume we know the causal dag. we can then do causal reasoning by applying d-separation to the mutilated graph. in this section we give an example of this and show how causal reasoning can help resolve a famous paradox known as simpon s paradox. simpson s paradox says that any statistical relationship between two variables can be reversed by including additional factors in the analysis. for example suppose some cause c taking a drug makes some effect e getting better more likely p p c and yet when we condition on the gender of the patient we find that taking the drug makes the effect less likely in both females and males f p f p c f p f p c f this seems impossible but by the rules of probability this is perfectly possible because the event space where we condition on c f or c f can be completely different to the event space when we just condition on c. the table of numbers below shows a concrete example e c c total combined e total rate e male e total rate e e female total rate chapter graphical model structure learning from this table of numbers we see that pec pe c pec f pe c f pec f pe f a visual representation of the paradox is given in in figure the line which goes up and to the right shows that the effect increases as the cause increases. however the dots represent the data for females and the crosses represent the data for males. within each subgroup we see that the effect decreases as we increase the cause. it is clear that the effect is real but it is still very counter-intuitive. the reason the paradox arises is that we are interpreting the statements causally but we are not using proper causal reasoning when performing our calculations. the statement that the drug c causes recovery e is p p c whereas the data merely tell us p p c this is not a contradiction. observing c is positive evidence for e since more males than females take the drug and the male recovery rate is higher of the drug. thus equation does not imply equation nevertheless we are left with a practical question should we use the drug or not? it seems like if we don t know the patient s gender we should use the drug but as soon as we discover if they are male or female we should stop using it. obviously this conclusion is ridiculous. to answer the question we need to make our assumptions more explicit. suppose reality can be modeled by the causal dag in figure to compute the causal effect of c on e we need to adjust for condition on the confounding variable f this is necessary because there is a backdoor path from c to e via f so we need to check the c e relationship for each value of f separately to make sure the relationship between c and e is not affected by any value of f suppose that for each value of f taking the drug is harmful that is pedoc f pedo c f pedoc f pedo c f then we can show that taking the drug is harmful overall pedoc pedo c the proof is as follows first from our assumptions in figure we see that drugs have no effect on gender pfdoc pfdo c pf now using the law of total probability pedoc pedoc f pedoc f fdoc pedoc f f f learning causal dags treatment gender treatment blood pressure c f c f e recovery e recovery figure two different models uses to illustrate simpson s paradox. f is gender and is a confounder for c and e. f is blood pressure and is caused by c. similarly pedo c pedo c f edo c f f since every term in equation is less than the corresponding term in equation we conclude that pedoc pedo c so if the model in figure is correct we should not administer the drug since it reduces the probability of the effect. now consider a different version of this example. suppose we keep the data the same but interpret f as something that is affected by c such as blood pressure. see figure in this case we can no longer assume pfdoc pfdo c pf and the above proof breaks down. so pedoc pedo c may be positive or negaitve. in the true model is figure then we should not condition on f when assessing the effect of c on e since there is no backdoor path in this case because of the v-structure at f that is conditioning on f might block one of the causal pathways. in other words by comparing patients with the same post-treatment blood pressure of f we may mask the effect of one of the two pathways by which the drug operates to bring about recovery. thus we see that different causal assumptions lead to different causal conclusions and hence different courses of action. this raises the question on whether we can learn the causal model from data. we discuss this issue below. learning causal dag structures in this section we discuss some ways to learn causal dag structures. chapter graphical model structure learning learning from observational data in section we discussed various methods for learning dag structures from observational data. it is natural to ask whether these methods can recover the true dag structure that was used to generate the data. clearly even if we have infinite data an optimal method can only identify the dag up to markov equivalence that is it can identify the pdag directed acylic graph but not the complete dag structure because all dags which are markov equivalent have the same likelihood. there are several algorithms the greedy equivalence search method of that are consistent estimators of pdag structure in the sense that they identify the true markov equivalence class as the sample size goes to infinity assuming we observe all the variables. however we also have to assume that the generating distribution p is faithful to the generating dag g. this means that all the conditional indepence properties of p are exactly captured by the graphical structure so ip ig this means there cannot be any ci properties in p that are due to particular settings of the parameters as zeros in a regression matrix that are not graphically explicit. for this reason a faithful distribution is also called a stable distribution. suppose the assumptions hold and we learn a pdag. what can we do with it? instead of recovering the full graph we can focus on the causal analog of edge marginals by computing the magnitude of the causal effect of one node on another a on b. if we know the dag we can do this using techniques described in if the dag is unknown we can compute a lower bound on the effect as follows et al. learn an equivalence class from data enumerate all the dags in the equivalence class apply pearl s do-calculus to compute the magnitude of the causal effect of a on b in each dag finally take the minimum of these effects as the lower bound. it is usually computationally infeasible to compute all dags in the equivalence class but fortunately one only needs to be able to identify the local neighborhood of a and b which can be esimated more efficiently as described in et al. this technique is called ida which is short for intervention-calculus when the dag is absent in et al. this technique was applied to some yeast gene expression data. gene knockout data was used to estimate the ground truth effect of each single-gene deletions on the remaining genes. then the algorithm was applied to unperturbed samples and was used to rank order the likely targets of each of the genes. the method had a precision of when the recall was set to while low this is substantially more than rival variable-selection methods such as lasso and elastic net which were only slightly above chance. learning from interventional data if we want to distinguish between dags within the equivalence class we need to use interventional data where certain variables have been set and the consequences have been measured. an example of this is the dataset in figure where proteins in a signalling pathway were perturbed and their phosphorylation status was measured using a technique called flow cytometry et al. it is straightforward to modify the standard bayesian scoring criteria such as the marginal likelihood or bic score to handle learning from mixed observational and experimental data we learning causal dags psitect akt inh plcy present missing int. edge pkc pma raf f akt pka erk jnk figure a design matrix consisting of data points measuring the status flow cytometry of proteins under different experimental conditions. the data has been discretized low medium and high some proteins were explicitly controlled using into states activating or inhibiting chemicals. a directed graphical model representing dependencies between various proteins circles and various experimental interventions ovals which was inferred from this data. we plot all edges for which pgst dotted edges are believed to exist in nature but were not discovered by the algorithm false negative. solid edges are true positives. the light colored edges represent the effects of intervention. source figure of and murphy this figure can be reproduced using the code at httpwww.cs.ubc.camurphyksoftwarebdaglindex.html. chapter graphical model structure learning just compute the sufficient statistics for a cpd s parameter by skipping over the cases where that node was set by intervention and yoo for example when using tabular cpds we modify the counts as follows ntck ixit k xipat c ixit not set the justification for this is that in cases where node t is set by force it is not sampled from its usual mechanism so such cases should be ignored when inferring the parameter t. the modified scoring criterion can be combined with any of the standard structure learning algorithms. and geng discusses some methods for choosing which interventions to perform so as to reduce the posterior uncertainty as quickly as possible form of active learning. the preceeding method assumes the interventions are perfect. in reality experimenters can rarely control the state of individual molecules. instead they inject various stimulant or inhibitor chemicals which are designed to target specific molecules but which may have side effects. we can model this quite simply by adding the intervention nodes to the dag and then learning a larger augmented dag structure with the constraint that there are no edges between the intervention nodes and no edges from the regular nodes back to the intervention nodes. figure shows the augmented dag that was learned from the interventional flow in particular we plot the median graph which cytometry data depicted in figure includes all edges for which pgij these were computed using the exact it turns out that in this example the median model has exactly algorithm of the same structure as the optimal map model argmaxg pgd which was computed using the algorithm of and sood silander and myllmaki learning undirected gaussian graphical models learning the structured of undirected graphical models is easier than learning dag structure because we don t need to worry about acyclicity. on the other hand it is harder than learning dag structure since the likelihood does not decompose section this precludes the kind of local search methods greedy search and mcmc sampling we used to learn dag structures because the cost of evaluating each neighboring graph is too high since we have to refit each model from scratch is no way to incrementally update the score of a model. in this section we discuss several solutions to this problem in the context of gaussian random fields or undirected gaussian graphical models we consider structure learning for discrete undirected models in section mle for a ggm before discussing structure learning we need to discuss parameter estimation. the task of computing the mle for a ggm is called covariance selection from equation the log likelihood can be written as log det trs learning undirected gaussian graphical models xxi xt is the empirical notational simplicity we assume we have already estimated x. n where is the precision matrix and s covariance matrix. one can show that the gradient of this is given by s however we have to enforce the constraints that st if gst zeros and that is positive definite. the former constraint is easy to enforce but the latter is somewhat challenging still a convex constraint. one approach is to add a penalty term to the objective if leaves the positive definite cone this is the approach used in ggmfitminfunc also et al. another approach is to use a coordinate descent method described in et al. and implemented in ggmfithtf. yet another approach is to use iterative proportional fitting described in section however ipf requires identifying the cliques of the graph which is np-hard in general. interestingly one can show that the mle must satisfy the following property st sst if gst or s t i.e. the covariance of a pair that are connected by an edge must match the in addition we have st if gst by definition of a ggm i.e. empirical covariance. the precision of a pair that are not connected must be we say that is a positive definite matrix completion of s since it retains as many of the entries in s as possible corresponding to the edges in the graph subject to the required sparsity pattern on corresponding to the absent edges the remaining entries in are filled in so as to maximize the likelihood. let us consider a worked example from et al. we will use the following adjacency matrix representing the cyclic structure and the following empirical covariance matrix s g the mle is given by ggmfitdemo for the code to reproduce these numbers. the constrained elements in and the free elements in both of which correspond to absent edges have been highlighted. graphical lasso we now discuss one way to learn a sparse grf structure which exploits the fact that there is a correspondence between zeros in the precision matrix and absent edges in the graph. this suggests that we can learn a sparse graph structure by using an objective that encourages zeros in the precision matrix. by analogy to lasso section one can define the following penalized nll j log det trs chapter graphical model structure learning figure sparse ggms learned using graphical lasso applied to the flow cytometry data. figure generated by ggmlassodemo. where glasso. jk jk is the of the matrix. this is called the graphical lasso or although the objective is convex it is non-smooth of the non-differentiable penalty and is constrained must be a positive definite matrix. several algorithms have been proposed for optimizing this objective and lin banerjee et al. duchi et al. although arguably the simplest is the one in et al. which uses a coordinate descent algorithm similar to the shooting algorithm for lasso. see ggmlassohtf for an implementation. also and hastie for a more recent version of this algorithm. as an example let us apply the method to the flow cytometry dataset from et al. a discretized version of the data is shown in figure here we use the original continuous data. however we are ignoring the fact that the data was sampled under intervention. in figure we illustrate the graph structures that are learned as we sweep from to a large value. these represent a range of plausible hypotheses about the connectivity of these proteins. it is worth comparing this with the dag that was learned in figure the dag has the advantage that it can easily model the interventional nature of the data but the disadvantage that it cannot model the feedback loops that are known to exist in this biological pathway the discussion in and murphy note that the fact that we show many ugms and only one dag is incidental we could easily use bic to pick the best ugm and conversely we learning undirected gaussian graphical models could easily display several dag structures sampled from the posterior. bayesian inference for ggm structure although the graphical lasso is reasonably fast it only gives a point estimate of the structure. furthermore it is not model-selection consistent meaning it cannot recover the true graph even as n it would be preferable to integrate out the parameters and perform posterior inference in the space of graphs i.e. to compute pgd. we can then extract summaries of the posterior such as posterior edge marginals pgij just as we did for dags. in this section we discuss how to do this. note that the situation is analogous to chapter where we discussed variable selection. in section we discussed bayesian variable selection where we integrated out the regression weights and computed p and the marginal inclusion probabilities p j then in section we discussed methods based on regularization. here we have the same dichotomy but we are presenting them in the opposite order. if the graph is decomposable and if we use conjugate priors we can compute the marginal likelihood in closed form and lauritzen furthermore we can efficiently identify the decomposable neighbors of a graph and green i.e. the set of legal edge additions and removals. this means that we can perform relatively efficient stochastic local search to approximate the posterior e.g. and green armstrong et al. scott and carvalho however the restriction to decomposable graphs is rather limiting if one s goal is knowledge discovery since the number of decomposable graphs is much less than the number of general undirected a few authors have looked at bayesian inference for ggm structure in the non-decomposable case et al. wong et al. jones et al. but such methods cannot scale to large models because they use an expensive monte carlo approximation to the marginal likelihood and massam and dobra suggested using a laplace approxmation. this requires computing the map estimate of the parameters for under a gwishart prior in and dobra they used the iterative proportional scaling algorithm and kiiveri hara and takimura to find the mode. however this is very slow since it requires knowing the maximal cliques of the graph which is np-hard in general. in et al. a much faster method is proposed. in particular they modify the gradient-based methods from section to find the map estimate these algorithms do not need to know the cliques of the graph. a further speedup is obtained by just using a diagonal laplace approximation which is more accurate than bic but has essentially the same cost. this plus the lack of restriction to decomposable graphs enables fairly fast stochastic search methods to be used to approximate pgd and its mode. this approach significantly outperfomed graphical lasso both in terms of predictive accuracy and structural recovery for a comparable computational cost. the number of decomposable graphs on v nodes for v is as follows if we divide these numbers by the number of undirected graphs which is we find the ratios are so we see that decomposable graphs form a vanishing fraction of the total hypothesis space. chapter graphical model structure learning handling non-gaussian data using copulas the graphical lasso and variants is inhertently limited to data that is jointly gaussian which is a rather severe restriction. fortunately the method can be generalized to handle non-gaussian but still continuous data in a fairly simple fashion. the basic idea is to estimate a set of d univariate monotonic transformations fj one per variable j such that the resulting transformed data is jointly gaussian. if this is possible we say the data belongs to the nonparametric normal distribution or nonparanormal distribution et al. this is equivalent to the family of gaussian copulas and wellner details on how to estimate the fj transformations from the empirical cdf s of each variable can be found in et al. after transforming the data we can compute the correlation matrix and then apply glasso in the usual way. one can show under various assumptions that this is a consistent estimator of the graph structure representing the ci assumptions of the original distributionliu et al. learning undirected discrete graphical models the problem of learning the structure for ugms with discrete variables is harder than the gaussian case because computing the partition function z which is needed for parameter estimation has complexity comparable to computing the permanent of a matrix which in general is intractable et al. by contrast in the gaussian case computing z only requires computing a matrix determinant which is at most ov since stochastic local search is not tractable for general discrete ugms below we mention some possible alternative approaches that have been tried. graphical lasso for mrfscrfs it is possible to extend the graphical lasso idea to the discrete mrf and crf case. however now there is a set of parameters associated with each edge in the graph so we have to use the graph analog of group lasso section for example consider a pairwise crf with ternary nodes and node and edge potentials given by stys yt x wt vt vt vt wt wt wt wt wt wt wt wt tyt x where we assume x begins with a constant term to account for the offset. x only contains the crf reduces to an mrf. note that we may choose to set some of the vtk and wstjk weights to to ensure identifiability although this can also be taken care of by the prior as shown in exercise to learn sparse structure we can minimize the following objective j t log tyit xi vt log styis yit xi wst learning undirected discrete graphical models case course children bible health christian insurance computer evidence disk email display card fact earth files graphics government god dos format help data image video gun human car president israel jesus drive memory number power law engine dealer jews baseball ftp mac scsi problem rights war religion games fans pc program phone nasa state question software research shuttle launch moon science orbit space university world system driver version technology windows hockey league nhl players season team win won figure an mrf estimated from the data using group regularization with isolated nodes are not plotted. from figure of used with kind permission of mark schmidt. where is the p-norm common choices are p or p as explained in section this method of crf structure learning was first suggested in et al. use of regularization for learning the structure of binary mrfs was proposed in et al. although this objective is convex it can be costly to evaluate since we need to perform inference to compute its gradient as explained in section is true also for mrfs. we should therefore use an optimizer that does not make too many calls to the objective function or its gradient such as the projected quasi-newton method in et al. in addition we can use approximate inference such as convex belief propagation to compute an approximate objective and gradient more quickly. another approach is to apply the group lasso penalty to the pseudo-likelihood discussed in section this is much faster since inference is no longer required and tibshirani figure shows the result of applying this procedure to the data where yit indicates the presence of word t in document i and xi the model is an mrf. chapter graphical model structure learning figure water sprinkler dgm with corresponding binary cpts. t and f stand for true and false. thin junction trees so far we have been concerned with learning sparse graphs but these do not necessarily have low treewidth. for example a d d grid is sparse but has treewidth od. this means that the models we learn may be intractable to use for inference purposes which defeats one of the two main reasons to learn graph structure in the first place other reason being knowledge discovery there have been various attempts to learn graphical models with bounded treewidth and jordan srebro elidan and gould shahaf et al. also known as thin junction trees but the exact problem in general is hard. an alternative approach is to learn a model with low circuit complexity et al. poon and domingos such models may have high treewidth but they exploit contextspecific independence and determinism to enable fast exact inference e.g. exercises exercise causal reasoning in the sprinkler network consider the causal network in figure let t represent true and f represent false. a. suppose i perform a perfect intervention and make the grass wet. what is the probability the sprinkler b. suppose i perform a perfect intervention and make the grass dry. what is the probability the sprinkler is on ps tdow t is on ps tdow f is the probability the sprinkler is on ps tdoc t c. suppose i perform a perfect intervention and make the clouds turn on by seeding them. what latent variable models for discrete data introduction in this chapter we are concerned with latent variable models for discrete data such as bit vectors sequences of categorical variables count vectors graph structures relational data etc. these models can be used to analyze voting records text and document collections low-intensity images movie ratings etc. however we will mostly focus on text analysis and this will be reflected in our terminology. since we will be dealing with so many different kinds of data we need some precise notation to keep things clear. when modeling variable-length sequences of categorical variables let yil v represent symbols or tokens such as words in a document we will the identity of the l th word in document i where v is the number of possible words in the vocabulary. we assume l li where li is the length of document i and i wheren is the number of documents. we will often ignore the word order resulting in a bag of words. this can be reduced to a fixed length vector of counts histogram. we will use niv li to denote the number of times word v occurs in document i for v note that the n v count matrix n is often large but sparse since we typically have many documents but most words do not occur in any given document. in some cases we might have multiple different bags of words e.g. bags of text words and bags of visual words. these correspond to different channels or types of features. we will denote these by yirl for r number of responses and l ir. if lir it means we have a single token bag of length in this case we just write yir vr if every channel is just a single token we write the fixed-size response vector as for brevity. in this case the n r design matrix y will not be sparse. for example in social science surveys yir could be the response of person i to the r th multi-choice question. out goal is to build joint probability models of pyi or pni using latent variables to capture the correlations. we will then try to interpret the latent variables which provide a compressed representation of the data. we provide an overview of some approaches in section before going into more detail in later sections. towards the end of the chapter we will consider modeling graphs and relations which can also be represented as sparse discrete matrices. for example we might want to model the graph of which papers mycite which other papers. we will denote these relations by r reserving the symbol y for any categorical data text associated with the nodes. chapter latent variable models for discrete data distributed state lvms for discrete data in this section we summarize a variety of possible approaches for constructing models of the form for bags of tokens for vectors of tokens and pni for vectors of integer counts. mixture models the simplest approach is to use a finite mixture model this associates a single discrete latent variable qi k with every document where k is the number of clusters. we will use a discrete prior qi cat for variable length documents we can define pyilqi k bkv where bkv is the probability that cluster k generates word v. the value of qi is known as a topic and the vector bk is the k th topic s word distribution. that is the likelihood has the form k catyilbk catyilbk k the induced distribution on the visible data is given by if the sum is unknown we can use a poisson class-conditional density to give pniqi k poiniv vk in this case liqi k poi v vk. k the generative story which this encodes is as follows for document i pick a topic qi from call it k and then for each word l i pick a word from bk. we will consider more sophisticated generative models later in this chapter. if we have a fixed set of categorical observations we can use a different topic matrix for each output variable k catyilbr k this is an unsupervised analog of naive bayes classification. if the sum li we can also model count vectors. multinomial pnili qi k munili bk v niv is known we can use a distributed state lvms for discrete data exponential family pca unfortunately finite mixture models are very limited in their expressive power. a more flexible model is to use a vector of real-valued continuous latent variables similar to the factor analysis in pca we use a gaussian prior of the form pzi and pca models in chapter n where zi r k and a gaussian likelihood of the form pyizi n indeed the method known this method can certainly be applied to discrete or count data. as latent semantic analysis orlatent semantic indexing et al. dumais and landauer is exactly equivalent to applying pca to a term by document count matrix. a better method for modeling categorical data is to use a multinoulli or multinomial distribu tion. we just have to change the likelihood to catyilswzi where w r of categorical responses we can use v k is a weight matrix and s is the softmax function. if we have a fixed number catyirswrzi where wr r v k is the weight matrix for the r th response variable. this model is called categorical pca and is illustrated in figure see section for further discussion. if we have counts we can use a multinomial model all of these models are examples of exponential family pca or epca et al. mohamed et al. which is an unsupervised analog of glms. the corresponding induced distribution on the visible variables has the form pyilzi w n fitting this model is tricky due to the lack of conjugacy. et al. proposed a coordinate ascent method that alternates between estimating the zi and w. this can be regarded as a degenerate version of em that computes a point estimate of zi in the e step. the problem with the degenerate approach is that it is very prone to overfitting since the number of latent variables is proportional to the number of datacases et al. a true em algorithm would marginalize out the latent variables zi. a way to do this for categorical pca using variational em is discussed in section for more general models one can use mcmc et al. pnili zi muniliswzi or a poisson model pnizi poiniv expwt vzi chapter latent variable models for discrete data w yir n w rkv zi i bkv niv li n figure two lvms for discrete data. circles are scalar nodes ellipses are vector nodes squares are matrix nodes. categorical pca. multinomial pca. lda and mpca in epca the quantity wzi represents the natural parameters of the exponential family. sometimes it is more convenient to use the dual parameters. for example for the multinomial the dual parameter is the probability vector whereas the natural parameter is the vector of log odds. if we want to use the dual parameters we need to constrain the latent variables so they live in the appropriate parameter space. in the case of categorical data we will need to ensure the latent vector lives in sk the k-dimensional probability simplex. to avoid confusion with epca we will denote such a latent vector by i. in this case the natural prior for the latent variables is the dirichlet i dir typically we set if we set we encourage i to be sparse as shown in figure when we have a count vector whose total sum is known the likelihood is given by pnili i munili b i this model is called multinomial pca or mpca buntine and jakulin see figure since we are assuming niv k bvk iv this can be seen as a form of matrix factorization for the count matrix. note that we use bvk to denote the parameter vector rather than wvk since we impose the constraints that bvk and v bvk the corresponding marginal distribution has the form pnili munili b idir i i unfortunately this integral cannot be computed analytically. if we have a variable length sequence known length we can use i catyilb i distributed state lvms for discrete data this is called latent dirichlet allocation or lda et al. and will be described in much greater detail below. lda can be thought of as a probabilistic extension of lsa where the latent quantities ik are non-negative and sum to one. by contrast in lsa zik can be negative which makes interpetation difficult. a predecessor to lda known as probabilistic latent semantic indexing or plsi uses the same model but computes a point estimate of i for each document to epca rather than integrating it out. thus in plsi there is no prior for i. we can modify lda to handle a fixed number of different categorical responses as follows i catyilbr i this has been called the user rating profile model and the simplex factor model and dunson gap model and non-negative matrix factorization now consider modeling count vectors where we do not constrain the sum to be observed. this case the latent variables just need to be non-negative so we will denote them by z can be ensured by using a prior of the form in i this this is called the gap model see figure in and jakulin it is shown that the gap model when conditioned on a fixed li reduces to the mpca model. this follows since a set of poisson random variables when conditioned on their sum becomes a multinomial distribution e.g. if we set k k in the gap model we recover a method known as non-negative matrix factorization or nmf and seung as shown in and jakulin nmf is not a probabilistic generative model since it does not specify a proper prior for z i furthermore the algorithm proposed in and seung is another degenerate em algorithm so suffers from overfitting. some procedures to fit the gap model which overcome these problems are given in and jakulin to encourage z i to be sparse we can modify the prior to be a spike-and-gamma type prior as follows pz ik kiz ik kgaz ik k k where k is the probability of the spike at this is called the conditional gamma poisson model and jakulin it is simple to modify gibbs sampling to handle this kind of prior although we will not go into detail here. pz i gaz ik k k the likelihood is given by pniz i poinivbt vz i chapter latent variable models for discrete data k k z ik niv n z b i qil yil b li n figure gaussian-poisson model. latent dirichlet allocation model. latent dirichlet allocation in this section we explain the latent dirichlet allocation or lda et al. model in detail. basics in a mixture of multinoullis every document is assigned to a single topic qi k in lda every word is assigned to its own topic qil drawn from a global distribution k drawn from a document-specific distribution i. since a document belongs to a distribution over topics rather than a single topic the model is called an admixture mixture or mixed membership model et al. this model has many other applications beyond text analysis e.g. genetics et al. health science et al. social network analysis et al. etc. adding conjugate priors to the parameters the full model is as i dir qil i cat i bk dir yilqil k b catbk this is illustrated in figure we can marginalize out the qi variables thereby creating a our notation is similar to the one we use elsewhere in this book but is different from that used by most lda papers. they typically use wnd for the identity of word n in document d znd to represent the discrete indicator d as the continuous latent vector for document d and k as the k th topic vector. latent dirichlet allocation topic observed document generated document figure geometric interpretation of lda. we have k topics and v words. each document dots and each topic dots is a point in the simplex. source figure of and griffiths used with kind permission of tom griffiths. direct arc from i to yil with the following cpd pyil v i pyil vqil kpqil k k k ikbkv as we mentioned in the introduction this is very similar to the multinomial pca model proposed in which in turn is closely related to categorical pca gap nmf etc. lda has an interesting geometric interpretation. each vector bk defines a distribution over v words each k is known as a topic. each document vector i defines a distribution over k topics. so we model each document as an admixture over topics. equivalently we can think of lda as a form of dimensionality reduction k v as is usually the case where we project a point in the v simplex normalized document count vector xi onto the k-dimensional simplex. this is illustrated in figure where we have v words and k topics. the observed documents live in the simplex are approximated as living on a simplex spanned by the topic vectors each of which lives in the simplex. one advantage of using the simplex as our latent space rather than euclidean space is that the simplex can handle ambiguity. this is importance since in natural language words can often have multiple meanings a phenomomen known as polysemy. for example play might refer to a verb to play ball or to play the coronet or to a noun shakespeare s play in lda we can have multiple topics each of which can generate the word play as shown in figure reflecting this ambiguity. given word l in document i we can compute pqil kyi and thus infer its most likely topic. by looking at the word in isolation it might be hard to know what sense of the word is meant but we can disambiguate this by looking at other words in the document. in particular given xi we can infer the topic distribution i for the document this acts as a prior for disambiguating qil. this is illustrated in figure where we show three documents from the tasa in the first document there are a variety of music related words which suggest the tasa corpus is a collection of high-school level english documents comprising over million words chapter latent variable models for discrete data topic topic topic word prob. music dance song play sing singing band played sang songs dancing piano playing rhythm albert musical word literature prob. poem poetry poet plays poems play literary writers drama wrote poets writer written stage shakespeare hit word prob. play ball game playing played baseball games bat run throw balls tennis home catch field figure three topics related to the word play. used with kind permission of tom griffiths. source figure of and griffiths document bix beiderbecke at on the of a the he was to from a riverboat. the had already his as well as his it was bix beiderbecke had already had he on the and his he might becoming a but bix was in another of he to the cornet. and he to document there is a why there are so few of really great in our whole world. too many have to come right at the very same time. the dramatists must have the right the must have the right playhouses the playhouses must have the right we must that to be not to be even when you a to yourself to it to it on a as you go along. as as a has to be then some of document has a the a for one. the the for one. the into the and the the see a for two. the two the the the for two. the like the into the and and the they see a for three. and and the they figure three documents from the tasa corpus containing different senses of the word play. grayed out words were ignored by the model because they correspond to uninteresting stop words as and the etc. or very low frequency words. source figure of and griffiths used with kind permission of tom griffiths. latent dirichlet allocation i will put most of its mass on the music topic this in turn makes the music interpretation of play the most likely as shown by the superscript. the second document interprets play in the theatrical sense and the third in the sports sense. note that is crucial that i be a latent variable so information can flow between the qil s thus enabling local disambiguation to use the full set of words. unsupervised discovery of topics one of the main purposes of lda is discover topics in a large collection or corpus of documents figure for an example. unfortunately since the model is unidentifiable the interpertation of the topics can be difficult et al. one approach known as labeled lda et al. exploits the existence of tags on documents as a way to ensure identifiability. in particular it forces the topics to correspond to the tags and then it learns a distribution over words for each tag. this can make the results easier to interpret. quantitatively evaluating lda as a language model in order to evaluate lda quantitatively we can treat it as a language model i.e. a probability distribution over sequences of words. of course it is not a very good language model since it ignores word order and just looks at single words but it is interesting to compare lda to other unigram-based models such as mixtures of multinoullis and plsi. such simple language models are sometimes useful for information retrieval purposes. the standard way to measure the quality of a language model is to use perplexity which we now define below. perplexity the perplexity of language model q given a stochastic p is defined as perplexityp q hp q lim n n where hp q is the cross-entropy of the two stochastic processes defined as log the cross entropy hence perplexity is minimized if q p in this case the model can predict as well as the true distribution. we can approximate the stochastic process by using a single long test sequence of multiple documents and multiple sentences complete with end-of-sentence markers call it y approximation becomes more and more accurate as the sequence gets longer provided the process is stationary and ergodic and thomas define the empirical distribution approximation to the stochastic process as y collated by a company formerly known as touchstone applied science associates but now known as questar assessment inc www.questarai.com. a stochastic process is one which can define a joint distribution over an arbitrary number of random variables. we can think of natural language as a stochastic process since it can generate an infinite stream of words. chapter latent variable models for discrete data in this case the cross-entropy becomes hpemp q n log qy and the perplexity becomes perplexitypemp q qy n i qy we see that this is the geometric mean of the inverse predictive probabilities which is the usual definition of perplexity and martin in the case of unigram models the cross entropy term is given by log qyil li h n where n is the number of documents and li is the number of words in document i. hence the perplexity of model q is given by perplexitypemp p exp log qyil li n intuitively perplexity mesures the weighted average branching factor of the model s predictive distribution. suppose the model predicts that each symbol word whatever is equally likely so then the perplexity is k. if some symbols are more likely than others and the model correctly reflects this its perplexity will be lower than k. of course hp p hp q so we can never reduce the perplexity below the entropy of the underlying stochastic process. perplexity of lda the key quantity is pv the predictive distribution of the model over possible words. is implicitly conditioned on the training set. for lda this can be approximated by plugging in b the posterior mean estimate and approximately integrating out q using mean field inference et al. for a more accurate way to approximate the predictive likelihood. in figure we compare lda to several other simple unigram models namely map estimation of a multinoulli map estimation of a mixture of multinoullis and plsi. performing map estimation the same dirichlet prior on b was used as in the lda model. the metric is perplexity as in equation and the data is a subset of the trec ap corpus containing newswire articles with unique terms. we see that lda significantly outperforms these other methods. latent dirichlet allocation unigram mixtures of unigrams lda fold in plsi l y t i x e p r e p number of topics figure perplexity vs number of topics on the trec ap corpus for various language models. based on figure of et al. figure generated by bleildaperplexityplot. n qnln ynln qnln ynln k figure the bk. lda unrolled for n documents. collapsed lda where we integrate out the i and fitting using gibbs sampling it is straightforward to derive a gibbs sampling algorithm for lda. the full conditionals are as follows pqil k explog ik log bkxil l p i dir k pbk dir v izil k ixil v zil k however one can get better performance by analytically integrating out the i s and the bk s i l chapter latent variable models for discrete data both of which have a dirichlet distribution and just sampling the discrete qil s. this approach was first suggested in and steyvers and is an example of collapsed gibbs sampling. figure shows that now all the qil variables are fully correlated. however we can sample them one at a time as we explain below. first we need some notation. let civk word v is assigned to topic k in document i. let cik word from document i has been assigned to topic k. let cvk word v has been assigned to topic k in any document. let niv times word v occurs in document i this is observed. let ck assigned to topic k. finally let li observed. iqil k yil v be the number of times v civk be the number of times any i civk be the number of times k civk be the number of v cvk be the number of words k cik be the number of words in document i this is we can now derive the marginal prior. by applying equation one can show that pq dir i i i catqil i ilqilk k catyilbk v pyq k by similar reasoning one can show dirbk from the above equations and using the fact that x we can derive the full conditional for pqilq il. define c ivk to be the same as civk except it is compute by summing over all locations in document i except for qil. also let yil v. then pqil kq il y c vk c k v c ik li k we see that a word in a document is assigned to a topic based both on how often that word is generated by the topic term and also on how often that topic is used in that document term. given equation we can implement the collapsed gibbs sampler as follows. we randomly assign a topic to each word qil k. we can then sample a new topic as follows for a given word in the corpus decrement the relevant counts based on the topic assigned to the current word draw a new topic from equation update the count matrices and repeat. this algorithm can be made efficient since the count matrices are very sparse. example this process is illustrated in figure on a small example with two topics and five words. the left part of the figure illustrates documents that were sampled from the lda model using latent dirichlet allocation river stream bank money loan river stream bank money loan figure illustration of gibbs sampling applied to a small lda example. there are n documents each containing a variable number of words drawn from a vocabulary of v words there are two topics. a white dot means word the word is assigned to topic a black dot means the word is assigned to topic a sample from the posterior after steps of gibbs sampling. source figure of and griffiths used with kind permission of tom griffiths. the initial random assignment of states. pmoneyk k k and priverk k pbankk for example we see that the first document contains the word bank times by the four dots in row of the bank column as well as various other financial terms. the right part of the figure shows the state of the gibbs sampler after iterations. the correct topic has been assigned to each token in most cases. for example in document we see that the word bank has been correctly assigned to the financial topic based on the presence of the words money and loan the posterior mean estimate of the parameters is given by pmoneyk ploank pbankk priverk pstreamk and pbankk which is impressively accurate given that there are only training examples. fitting using batch variational inference a faster alternative to mcmc is to use variational em. cannot use exact em since exact inference of i and qi is intractable. we give the details below. sequence version l following et al. we will use a fully factorized field approximation of the form q i qi dir i i catqil qil we will follow the usual mean field recipe. for qqil we use bayes rule but where we need to take expectations over the prior qilk byilk expe ik where e ik k i. ik chapter latent variable models for discrete data where is the digamma function. the update for q i is obtained by adding up the expected counts the m step is obtained by adding up the expected counts and normalizing ik k qilk l bvk v qilkiyil v count version i liv k space to store the qilk. note that the e step takes o it is much more space efficient to perform inference in the mpca version of the model which works with counts these only take on v k space which is a big savings if documents are long. contrast the collapsed gibbs sampler must work explicitly with the qil variables. we will focus on approximating p i cini li where we write ci as shorthand for ci... we will again use a fully factorized field approximation of the form v muciv.niv civ. q i ci dir i i the new e step becomes ik k civk bvk expe ik niv civk v the new m step becomes bvk v niv civk i vb version we now modify the algorithm to use vb instead of em so that we infer the parameters as well as the latent variables. there are two advantages to this. first by setting vb will encourage b to be sparse in section second we will be able to generalize this to the online learning setting as we discuss below. our new posterior approximation becomes q i ci b dir i i muciv.niv civ. v k the update for civk changes to the following civk exp bvk ik dirb.k b.k latent dirichlet allocation algorithm batch vb for lda input niv k k v estimate bvk using em for multinomial mixtures initialize counts niv while not converged do e step svk expected sufficient statistics for each document i do i ci estepni b svk niv civk m step for each topic k do bvk v svk function i ci estepni b initialize ik k repeat old i. i. ik k for each word v do for each topic k do civk exp k bv. k old i. civ. normalize civ. ik niv civk k ik old ik thresh until k also the m step becomes bvk v civk i no normalization is required since we are just updating the pseudcounts. the overall algorithm is summarized in algorithm fitting using online variational inference in the bathc version the e step clearly takes on kv t time where t is the number of mean field updates t this can be slow if we have many documents. this can be reduced by using stochastic gradient descent to perform online variational inference as we now explain. we can derive an online version following et al. we perform an e step in the usual way. we then compute the variational parameters for b treating the expected sufficient statistics from the single data case as if the whole data set had those statistics. finally we make chapter latent variable models for discrete data algorithm online variational bayes for lda input niv k k v initialize bvk randomly for t do set step size t t pick document i it i ci estepni b bnew vk v n niv civk bvk t bvk t bnew vk l y t i x e p r e p online online batch documents seen scale figure test perplexity vs number of training documents for batch and online vb-lda. from figure of et al. used with kind permission of david blei. a partial update for the variational parameters for b putting weight t on the new estimate and weight t on the old estimate. the step size t decays over time as in equation in practice we should use mini-batches the overall algorithm is summarized in algorithm as explained in section in et al. they used a batch of size figure plots the perplexity on a test set of size vs number of analyzed documents steps where the data is drawn from wikipedia. the figure shows that online variational inference is much faster than offline inference yet produces similar results. determining the number of topics choosing k the number of topics is a standard model selection problem. here are some approaches that have been taken use annealed importance sampling to approximate the evidence et al. cross validation using the log likelihood on a test set. extensions of lda use the variational lower bound as a proxy for log pdk. use non-parametric bayesian methods et al. extensions of lda many extensions of lda have been proposed since the first paper came out in we briefly discuss a few of these below. correlated topic model one weakness of lda is that it cannot capture correlation between topics. for example if a document has the business topic it is reasonable to expect the finance topic to co-occcur. the source of the problem is the use of a dirichlet prior for i. the problem with the dirichelt it that it is characterized by just a mean vector and a strength parameter but its covariance is fixed ij i j rather than being a free parameter. one way around this is to replace the dirichlet prior with the logistic normal distribution as in categorical pca the model becomes bk dir zi n izi szi qil i cat i yilqil k b catbk this is known as the correlated topic model and lafferty this is very similar to categorical pca but slightly different. to see the difference let us marginalize out the qil and i. then in the ctm we have yil catbszi where b is a stochastic matrix. by contrast in catpca we have yil catswzi where w is an unconstrained matrix. fitting this model is tricky since the prior for i is no longer conjugate to the multinomial likelihood for qil. however we can use any of the variational methods in section where we discussed bayesian multiclass logistic regression. in the ctm case things are even harder since the categorical response variables qi are hidden but we can handle this by using an additional mean field approximation. see and lafferty for details. having fit the model one can then convert to a sparse precision matrix by pruning low-strength edges to get a sparse gaussian graphical model. this allows you to visualize the correlation between topics. figure shows the result of applying this procedure to articles from science magazine from corpus contains documents and words of them unique after stop-word and low-frequency removal. nodes represent topics with the top words per topic listed inside. the font size reflects the overall prevalence of the topic in the corpus. edges represent significant elements of the precision matrix. chapter latent variable models for discrete data activated tyrosine phosphorylation activation phosphorylation kinase research funding support nih program science scientists says research people united states women universities students education receptor receptors ligand ligands apoptosis cells cell expression cell lines bone marrow amino acids cdna sequence isolated protein cell cycle activity cyclin regulation wild type mutant mutations mutants mutation mice antigen t cells antigens immune response virus hiv aids infection viruses patients disease treatment drugs clinical bacteria bacterial host resistance parasite gene disease mutations families mutation cells proteins researchers protein found enzyme enzymes iron active site reduction plants plant gene genes arabidopsis development embryos drosophila genes expression proteins protein binding domain domains rna dna rna polymerase cleavage site brain memory subjects left task computer problem information computers problems sequence sequences genome dna sequencing surface liquid surfaces fluid model magnetic magnetic field spin superconductivity superconducting fossil record birds fossils dinosaurs fossil species forest forests populations ecosystems genetic population populations differences variation ancient found impact million years ago africa neurons stimulus motor visual cortical materials organic polymer polymers molecules synapses ltp glutamate synaptic neurons physicists particles physics particle experiment surface tip image sample device laser optical light electrons quantum reaction reactions molecule molecules transition state stars astronomers universe galaxies galaxy pressure high pressure pressures core inner core mantle crust upper mantle meteorites ratios sun solar wind earth planets planet earthquake earthquakes fault images data carbon carbon dioxide methane water volcanic deposits magma eruption volcanism climate ocean ice changes climate change ozone atmospheric measurements stratosphere concentrations figure output of the correlated topic model k topics when applied to articles from science. nodes represent topics with the most probable phrases from each topic shown inside. font size reflects overall prevalence of the topic. see httpwww.cs.cmu.edulemurscience for an interactive version of this model with topics. source figure of and lafferty used with kind permission of david blei. dynamic topic model in lda the topics over words are assumed to be static. in some cases it makes sense to allow these distributions to evolve smoothly over time. for example an article might use the topic neuroscience but if it was written in the it is more likely to use words like nerve whereas if it was written in the it is more likely to use words like calcium receptor reflects the general trend of neuroscience towards molecular biology. one way to model this is use a dynamic logistic normal model as illustrated in figure in particular we assume the topic distributions evolve according to a gaussian random walk and then we map these gaussian vectors to probabilities via the softmax function btkbt n i dir t i cat t il t qt i il k bt catsbt k ilqt yt this is known as a dynamic topic model and lafferty extensions of lda t i qt il yt il t i qt il yt il i il il n n bt k bt k k n k figure the dynamic topic model. one can perform approximate infernece in this model using a structured mean field method that exploits the kalman smoothing algorithm to perform exact inference on the linear-gaussian chain between the btk nodes and lafferty for details. figure illustrates a typical output of the system when applied to years of articles from science. on the top we visualize the top words from a specific topic seems to be related to neuroscience after year intervals. on the bottom left we plot the probability of some specific words belonging to this topic. on the bottom right we list the titles of some articles that contained this topic. one interesting application of this model is to perform temporally-corrected document retrieval. that is suppose we look for documents about the inheritance of disease. modern articles will use words like dna but older articles the discovery of dna may use other terms such as heritable unit but both articles are likely to use the same topics. similar ideas can be used to perform cross-language information retrieval see e.g. et al. lda-hmm the lda model assumes words are exchangeable which is clearly not true. a simple way to model sequential dependence between words is to use a hidden markov model or hmm. the trouble with hmms is that they can only model short-range dependencies so they cannot capture the overall gist of a document. hence they can generate syntactically correct sentences e.g. table but not semantically plausible ones. it is possible to combine lda with hmm to create a model called lda-hmm et al. chapter latent variable models for discrete data brain movement action right eye hand left muscle nerve sound movement eye right hand brain left action muscle sound experiment brain eye movement right left hand nerve vision sound muscle movement movement brain sound nerve active muscle left eye right nervous sound muscle active nerve stimulate fiber reaction brain response stimulate muscle sound movement response nerve frequency fiber active brain record nerve stimulate response muscle electrode active brain fiber potential respons record stimulate nerve muscle active frequency electrode potential study response stimulate record condition active potential stimulus nerve subject eye respons cell potential stimul neuron active nerve eye record abstract cell neuron response active brain stimul muscle system nerve receptor cell channel neuron active brain receptor muscle respons current neuron active brain cell fig response channel receptor synapse signal nerve neuron mental science hemianopsia in migraine a defence of the phrenology the synchronal flashing of fireflies myoesthesis and imageless thought acetylcholine and the physiology of the nervous system brain waves and unit discharge in cerebral cortex errorless discrimination learning in the pigeon temporal summation of light by a vertebrate visual receptor hysteresis in the force-calcium relation in muscle gaba-activated chloride channels in secretory nerve endings figure part of the output of the dynamic topic model when applied to articles from science. we show the top words for the neuroscience topic over time. we also show the probability of three words within this topic over time and some articles that contained this topic. source figure of and lafferty used with kind permission of david blei. this model uses the hmm states to model function or syntactic words such as and or however and uses the lda to model content or semantic words which are harder to predict. there is a distinguished hmm state which specifies when the lda model should be used to generate the word the rest of the time the hmm generates the word. more formally for each document i the model defines an hmm with states zil c. in addition each document has an lda model associated with it. if zil we generate word yil from the semantic lda model with topic specified by qil otherwise we generate word yil from the syntactic hmm model. the dgm is shown in figure the cpds are as follows p i dir i pzil c hmm pqil k i ik pyil vqil k zil c if c if c bldak v bhmm v where blda is the usual topic-word matrix bhmm is the state-word hmm emission matrix and ahmm is the state-state hmm transition matrix. inference in this model can be done with collapsed gibbs sampling analytically integrating out all the continuous quantities. see et al. for the details. the results of applying this model k lda topics and c hmm states to the combined brown and tasa are shown in table we see that the hmm generally is the brown corpus consists of documents and word tokens with part-of-speech tags for each token. extensions of lda i qil yil zil qil yil zil blda n ahmm bhmm figure lda-hmm model. in contrast to this approach we study here how the overall network activity can control single cell parameters such as input resistance as well as time and space constants parameters that are crucial for excitability and spariotemporal integration. the integrated architecture in this paper combines feed forward control and error feedback adaptive control using neural networks. in other words for our proof of convergence we require the softassign algorithm to return a doubly stochastic matrix as theorem guarantees that it will instead of a matrix which is merely close to being doubly stochastic based on some reasonable metric. the aim is to construct a portfolio with a maximal expected return for a given risk level and time horizon while simultaneously obeying or required constraints. the left graph is the standard experiment the right from a training with samples. the graph g is called the graph and h is called the host graph. figure function and content words in the nips corpus as distinguished by the lda-hmm model. graylevel indicates posterior probability of assignment to lda component with black being highest. the boxed word appears as a function word in one sentence and as a content word in another sentence. asterisked words had low frequency and were treated as a single word type by the model. source figure of et al. used with kind permission of tom griffiths. chapter latent variable models for discrete data the blood of body heart and in to is blood heart pressure body lungs oxygen vessels arteries breathing the a his this their these your her my some the and of a in trees tree with on forest trees forests land soil areas park wildlife area rain in for to on with at by from as into the and of in land to the of to in and classes farmers government for farm farmers land crops farm food people farming wheat farms corn he it you they i she we there this who a state government state federal public local act states national laws department new other first same great good small little old the a of in to picture film image lens light eye lens image mirror eyes glass object objects lenses be have see make do know get go take find a the of in water is and matter are water matter molecules liquid particles gas solid substance temperature changes said made used came went found called the a of and drink alcohol to bottle in drugs drug alcohol people drinking person effects marijuana body use time way years day part number kind place the of a and in story is to as story stories poem characters poetry character author poems life poet can would will could may had must do have did the a in game ball and team to play ball game team baseball players football player field basketball table upper row topics extracted by the lda model when trained on the combined brown and tasa corpora. middle row topics extracted by lda part of lda-hmm model. bottom row topics extracted by hmm part of lda-hmm model. each column represents a single topicclass and words appear in order of probability in that topicclass. since some classes give almost all probability to only a few words a list is terminated when the words account for of the probability mass. source figure of et al. used with kind permission of tom griffiths. responsible for syntactic words and the lda for semantics words. if we did not have the hmm the lda topics would get polluted by function words top of figure which is why such words are normally removed during preprocessing. the model can also help disambiguate when the same word is being used syntactically or semantically. figure shows some examples when the model was applied to the nips we see that the roles of words are distinguished e.g. we require the algorithm to return a matrix vs the maximal expected return in principle a part of speech tagger could disambiguate these two uses but note that the lda-hmm method is fully unsupervised pos tags were used and sometimes a word can have the same pos tag but different senses e.g. the left graph synactic role vs the graph g semantic role. the topic of probabilistic models for syntax and semantics is a vast one which we do not the tasa corpus is an untagged collection of educational materials consisting of documents and word tokens. words appearing in fewer than documents were replaced with an asterisk but punctuation was included. the combined vocabulary was of size unique words. nips stands for neural information processing systems nips corpus volumes contains documents. it is one of the top machine learning conferences. the extensions of lda i qil yil qi ci n b w a i qil yil ci n b figure supervised lda. discriminative lda. have space to delve into any more. see e.g. and martin for further information. supervised lda in this section we discuss extensions of lda to handle side information of various kinds beyond just words. generative supervised lda suppose we have a variable length sequence of words yil v as usual but we also have a class label ci c. how can we predict ci from yi? there are many possible approaches but most are direct mappings from the words to the class. in some cases such as sentiment analysis we can get better performance by first performing inference to try to disambiguate the meaning of words. for example suppose the goal is to determine if a document is a favorable review of a movie or not. if we encounter the phrase brad pitt was excellent until the middle of the movie the word excellent may lead us to think the review is positive but clearly the overall sentiment is negative. one way to tackle such problems is to build a joint model of the form pci yi and mcauliffe proposes an approach called supervised lda where the class label ci is generated from the topics as follows pciqi bersigmwt qi here qi is the empirical topic distribution for document i qik li qilk see figure for an illustration. chapter latent variable models for discrete data w xi i qil yil n b xi w i i qil yil n b w xi i i b qil yil n figure discriminative variants of lda. mixture of experts aka mr-lda. the double ring denotes a node that i a deterministic function of its parents. dmr-lda. mixture of experts with random effects. we can fit this model using monte carlo em run the collapsed gibbs sampler in the e step to compute e and then use this as the input feature to a standard logistic regression package. discriminative supervised lda an alternative approach known as discriminative lda et al. is shown in figure this is a discriminative model of the form pyici the only change from regular lda is that the topic prior becomes input dependent as follows pqil i ci c catac where ac is a k k stochastic matrix. so far we have assumed the side information is a single categorical variable ci. often we have high dimensional covariates xi r d. for example consider the task of image tagging. the idea is that yil represent correlated tags or labels which we want to predict given xi. we now discuss several attempts to extend lda so that it can generate tags given the inputs. the simplest approach is to use a mixture of experts with multiple outputs. this is just like lda except we replace the dirichlet prior on i with a deterministic function of the input i swxi in et al. this is called multinomial regression lda. see figure eliminating the deterministic i we have pqilxi w catswxi we can fit this with em in the usual way. however et al. suggest an alternative. first fit an unsupervised lda model based only on yi then treat the inferred i as data and extensions of lda fit a multinomial logistic regression model mapping xi to i. although this is fast fitting lda in an unsupervised fashion does not necessarily result in a discriminative set of latent variables as discussed in and mcauliffe there is a more subtle problem with this model. since i is a deterministic function of the inputs it is effectively observed rendering the qil hence the tags yil independent. in other words pyixi pyilxi pyilqil k bpqil kxi w k this means that if we observe the value of one tag it will have no influence on any of the others. this may explain why the results in et al. only show negligible improvement over predicting each tag independently. one way to induce correlations is to make w a random variable. the resulting model is shown in figure we call this a random effects mixture of experts. we typically assume a gaussian prior on wi. if xi then pqilxi wi catswi so we recover the correlated topic model. it is possible to extend this model by adding markovian dynamics to the qil variables. this is called a conditional topic random field and xing a closely related approach known as dirichlet multinomial regression lda and mccallum is shown in figure this is identical to standard lda except we make a function of the input i expwxi where w is a k d matrix. eliminating the deterministic i we have i direxpwxi is known as labeled lda et al. unlike et al. this model allows information to flow between tags via the latent i. a variant of this model where xi corresponds to a bag of discrete labels and i dir in this case the labels xi are in xi correspondence with the latent topics which makes the resulting topics much more interpretable. an extension known as partially labeled lda et al. allows each label to have multiple latent sub-topics this model includes lda labeled lda and a multinomial mixture model as special cases. discriminative categorical pca an alternative to using lda is to expand the categorical pca model with inputs as shown in figure since the latent space is now real-valued we can use simple linear regression for the input-hidden mapping. for the hidden-output mapping we use traditional catpca pzixi v pyizi w catyilswzi l this model is essentially a probabilistic neural network with one hidden layer as shown in figure but with exchangeable output to handle variable numbers of tags. the chapter latent variable models for discrete data xi zi yil n v w xid zik vk yili n w figure vector nodes expanded out. categorical pca with inputs and exchangeable outputs. same as but with the key difference from a neural net is that information can flow between the yil s via the latent bottleneck layer zi. this should work better than a conventional neural net when the output labels are highly correlated even after conditioning on the features this problem frequently arises in multi label classification. note that we could allow a direct xi to yi arc but this would require too many parameters if the number of labels is we can fit this model with a small modification of the variational em algorithm in section if we use this model for regression rather than classification we can perform the e step exactly by modifying the em algorithm for factor analysis. et al. reports that this method converges faster than standard backpropagation. we can also extend the model so that the prior on zi is a mixture of gaussians using inputif the output is gaussian this corresponds to a mixture of discriminative dependent means. factor analysers zhou and liu if the output is categorical this would be an yet unpublished model which we could call discriminative mixtures of categorical factor analyzers lvms for graph-structured data another source of discrete data is when modeling graph or network structures. to see the connection recall that any graph on d nodes can be represented as a d d adjacency matrix g where gi j iff there is an edge from node i to node j. such matrices are binary and often very sparse. see figure for an example. graphs arise in many application areas such as modeling social networks protein-protein interaction networks or patterns of disease transmission between people or animals. there are usually two primary goals when analysing such data first try to discover some interesting a non-probabilistic version of this idea using squared loss was proposed in et al. this is similar to a linear feed-forward neural network with an additional edge from xi directly to yi. lvms for graph-structured data figure a directed graph. the same graph with the nodes partitioned into groups making the block structure more apparent. z r figure adjacency matrix for the graph in figure rows and columns are shown permuted to show the block structure. we also sketch of how the stochastic block model can generate this graph. from figure of et al. used with kind permission of charles kemp. structure in the graph such as clusters or communities second try to predict which links might occur in the future who will make friends with whom. below we summarize some models that have been proposed for these tasks some of which are related to lda. futher details on these and other approaches can be found in e.g. et al. and the references therein. stochastic block model in figure we show a directed graph on nodes. there is no apparent structure. however if we look more deeply we see it is possible to partition the nodes into three groups or blocks and such that most of the connections go from nodes in to or from to or from to this is illustrated in figure chapter latent variable models for discrete data m e t s y s l a n o i t l a e r x i r t a m d e t r o s a b c d a d b c a b c d a b d g e h c f a c b d e a b c d a b c d a b c d e f g h a edcb a b c d a b c d e f g h a b c d e figure some examples of graphs generated using the stochastic block model with different kinds of connectivity patterns between the blocks. the abstract graph blocks represent a ring a dominance hierarchy a common-cause structure and a common-effect structure. from figure of et al. used with kind permission of charles kemp. the problem is easier to understand if we plot the adjacency matrices. figure shows the matrix for the graph with the nodes in their original ordering. figure shows the matrix for the graph with the nodes in their permtuted ordering. it is clear that there is block structure. we can make a generative model of block structured graphs as follows. first for every node sample a latent block qi cat where k is the probability of choosing block k for k k. second choose the probability of connecting group a to group b for all pairs of groups let us denote this probability by ab. this can come from a beta prior. finally generate each edge rij using the following model prij rqi a qj b berr ab this is called the stochastic block model and snijders figure illustrates the model as a dgm and figure illustrates how this model can be used to cluster the nodes in our example. note that this is quite different from a conventional clustering problem. for example we see that all the nodes in block are grouped together even though there are no connections between them. what they share is the property that they like to connect to nodes in block and to receive connections from nodes in block figure illustrates the power of the model for generating many different kinds of graph structure. for example some social networks have hierarchical structure which can be modeled by clustering people into different social strata whereas others consist of a set of cliques. unlike a standard mixture model it is not possible to fit this model using exact em because all the latent qi variables become correlated. however one can use variational em et al. lvms for graph-structured data qi qj rij i j ab j qi j rij i j ab i qi j figure stochastic block model. mixed membership stochastic block model. collapsed gibbs sampling et al. etc. we omit the details are similar to the lda case. in et al. they lifted the restriction that the number of blocks k be fixed by replacing the dirichlet prior on by a dirichlet process section this is known as the infinite relational model. see section for details. if we have features associated with each node we can make a discriminative version of this model for example by defining prij rqi a qj b xi xj berrwt abf xj where f xj is some way of combining the feature vectors. for example we could use concatenation xj or elementwise product xi xj as in supervised lda. the overall model is like a relational extension of the mixture of experts model. mixed membership stochastic block model in et al. they lifted the restriction that each node only belong to one cluster. that is they replaced qi k with i sk. this is known as the mixed membership stochastic block model and is similar in spirit to fuzzy clustering or soft clustering. note that ik is not the same as pzi kd the former represents ontological uncertainty what degree does each object belong to a cluster wheras the latter represents epistemological uncertainty cluster does an object belong to. if we want to combine epistemological and ontological uncertainty we can compute p id. in more detail the generative process is as follows. first each node picks a distribution over blocks i dir second choose the probability of connecting group a to group b for all pairs of groups ab third for each edge sample two discrete variables one for each direction qi j cat i qi j cat j finally generate each edge rij using the following model prij j a qi j b ab chapter latent variable models for discrete data outcasts waverers loyal opposition young turks ambrose boniface mark winfrid elias basil simplicius berthold john bosco victor bonaventure amand louis albert peter gregory hugh figure who-likes-whom graph for sampson s monks. one of three groups. from figures of et al. used with kind permission of edo airoldi. mixed membership of each monk in see figure for the dgm. unlike the regular stochastic block model each node can play a different role depending on who it is connecting to. as an illustration of this we will consider a data set that is widely used in the social networks analysis literature. the data concerns who-likes-whom amongst of group of monks. it was collected by hand in by sampson over a period of months. days in the era of social media such as facebook a social network with only people is trivially small but the methods we are discussing can be made to scale. figure plots the raw data and figure plots e for each monk where k we see that most of the monk belong to one of the three clusters known as the young turks the outcasts and the loyal opposition however some individuals notably monk belong to two clusters sampson called these monks the waverers it is interesting to see that the model can recover the same kinds of insights as sampson derived by hand. one prevalent problem in social network analysis is missing data. for example if rij it may be due to the fact that person i and j have not had an opportunity to interact or that data is not available for that interaction as opposed to the fact that these people don t want to interact. in other words absence of evidence is not evidence of absence. we can model this by modifying the observation model so that with probability we generate a from the background model and we only force the model to explain observed with probability in other words we robustify the observation model to allow for outliers as follows prij rqi j a qi j b ab see et al. for details. relational topic model in many cases the nodes in our network have atttributes. for example if the nodes represent academic papers and the edges represent citations then the attributes include the text of the document itself. it is therefore desirable to create a model that can explain the text and the link structure concurrently. such a model can predict links given text or even vice versa. the relational topic model and blei is one way to do this. this is a lvms for relational data b i yil qil qi yjl qjl j qj w rij i j figure dgm for the relational topic model. simple extension of supervised lda where the response variable rij represents whether there is an edge between nodes i and j is modeled as follows prij qj sigmwt qj qilk. see recall that qi is the empirical topic distribution for document i qik figure li note that it is important that rij depend on the actual topics chosen qi and qj and not on the topic distributions i and j otherwise predictive performance is not as good. the if rij is a child of i and j it will be treated as just intuitive reason for this is as follows another word similar to the qil s and yil s but since there are many more words than edges the graph structure information will get washed out by making rij a child of qi and qj the graph information can influence the choice of topics more directly. one can fit this model in a manner similar to slda. see and blei for details. the method does better at predicting missing links than the simpler approach of first fitting an lda model and then using the qi s as inputs to a logistic regression problem. the reason is analogous to the superiority of partial least squares to pca linear regression namely that the rtm learns a latent space that is forced to be predictive of the graph structure and words whereas lda might learn a latent space that is not useful for predicting the graph. lvms for relational data graphs can be used to represent data which represents the relation amongst variables of a certain type e.g. friendship relationships between people. but often we have multiple types of objects and multiple types of relations. for example figure illustrates two relations one between people and people and one between people and movies. in general we define a k-ary relation r as a subset of k-tuples of the appropriate types r t k chapter latent variable models for discrete data figure example of relational data. there are two types of objects people and movies one relation friends people people and one function rates people movie r. age and sex are attributes functions of the people class. where ti are sets or types. a binary pairwise or dyadic relation is a relation defined on pairs of objects. for example the seen relation between people and movies might be represented as the set of movies that people have seen. we can either represent this explicitly as a set such as seen starwars tombraider jaws or implicitly using an indicator function for the set seenbob seenbob seenalice a relation between two entities of types t and t can be represented as a binary function r t t and hence as a binary matrix. this can also be represented as a bipartite graph in which we have nodes of two types. if t t this becomes a regular directed graph as in section however there are some situations that are not so easily modelled by graphs but which can still be modelled by relations. for example we might have a ternary relation r t t t where say ri j k iff protein i interacts with protein j when chemical k is present. this can be modelled by a binary matrix. we will give some examples of this in section making probabilistic models of relational data is called statistical relational learning and taskar one approach is to directly model the relationship between the variables using graphical models this is known as probabilistic relational modeling. another approach is to use latent variable models as we discuss below. infinite relational model it is straightforward to extend the stochastic block model to model relational data we just i kt with each entity i of each type t. we then define associate a latent variable qt the probability of the relation holding between specific entities by looking up the probability of the relation holding between entities of that type. for example if r t t t we have pri j k k c abc i a j b if we allow the number of clusters kt for each type to be unbounded by using a dirichlet process the model is called the infinite relational model et al. an essentially lvms for relational data interact with affects causes causes affects affects causes complicates causes complicates affects complicates disrupts affects complicates process of manifestation of affects process of result of manifestation of result of affects process of result of result of result of affects process of manifestation of associated with manifestation of affects process of manifestation of associated with figure illustration of an ontology learned by irm applied to the unified medical language system. the boxes represent of the concept clusters. predicates that belong to the same cluster are grouped together and associated with edges to which they pertain. all links with weight above have been included. from figure of et al. used with kind permission of charles kemp. identical model under the name infinite hidden relational model was concurrently proposed in et al. we can fit this model with variational bayes et al. or collapsed gibbs sampling et al. rather than go into algorithmic detail we just sketch some interesting applications. learning ontologies an ontology refers to an organisation of knowledge. e.g. and norvig but it is interesting to try and learn them from data. et al. they show how this can be done using the irm. in ai ontologies are often built by hand in the data comes from the unified medical language system which defines a semantic network with concepts as disease or syndrome diagnostic procedure animal and binary predicates as affects prevents we can represent this as a ternary relation r t t t where t is the set of concepts and t is the set of binary predicates. the result is a cube. we can then apply the irm to partition the cube into regions of roughly homogoneous response. the system found concept clusters and predicate clusters. some of these are shown in figure the system learns for example that biological functions affect organisms abc where a represents the biological function cluster b represents the organism cluster and c represents the affects cluster. clustering based on relations and features we can also use irm to cluster objects based on their relations and their features. for example et al. consider a political dataset consisting of countries binary a brazil netherlands uk usa burma indonesia jordan egypt india israel china cuba poland ussr b military alliance chapter latent variable models for discrete data t v o g l a n o i t u t i t s n o c l i c o b t s n u m m o c s n o i t c e e e e r f l i t s n u m m o c n o n l c o b n r e t s e w l i s n o i t c e e e e r f o n p h s r o s n e c h g h i y c a r e t i l l i l i e c n e o v c i t s e m o d i s t s n u m m o c n a i r a t i l a t o t t s i t i l e l n o i t a c u d e t v o g e n n o s r e p y r a t i l i m s d o o g e n r o b a e s s k o o b s u o g i i l e r p n g s t r o p x e t n e u q n i l e d n u l c o b l a r t u e n l s n o i t a n s s a s s a n o i t u o v e r t v o g i s n o g i i l e r m u n i s s i r c t v o g y r a t i l i i m g n n e v r e t n i d e m u s n o c y g r e n e p h s r o s n e c e m o s i s u m o r f r a f s e g r u p l l a f n a r i s t n e d u t s n g e r o f i y r t n u o c f o e g a h t g n e l d a o r l i a r s o g n w a l s e g a u g n a l m u n n e k a t d a i t n e s l i a m n g e r o f i s r e k r o w e a m e f l i t e d n i n e t o r p i s t n e m t s e v n i n e k a t d a s u y t i s n e d n p o p i a e r a d n a l s o g n s t r a y h c r a n o m h t g n e l d a o r s t n a r g m e i l e b a r a i t e d n i s e i r o a c l l d e y o p m e n u e n o h p e e t n o i t a u p o p e s n e f e d l l s c i l o h t a c s t s e t o r p s t a e r h t p n g sends tourists to exports books to exports to treaties conferences membership of igos c d joint joint membership of ngos e negative behavior negative communications accusations protests f book translations g h economic aid emigration i common bloc membership figure illustration of irm applied to some political data containing features and pairwise interactions. top row the partition of the countries into clusters and the features into clusters. every second column is labelled with the name of the corresponding feature. small squares at bottom these are of the clusters of interaction types. from figure of et al. used with kind permission of charles kemp. predicates representing interaction types between countries sends tourists to economic aid and features communist monarchy to create a binary dataset real-valued features were thresholded at their mean and categorical variables were dummy-encoded. the data has types t represents countries t represents interactions and t represents features. we have two relations t t t and t t problem therefore combines aspects of both the biclustering model and the ontology discovery model. when given multiple relations the irm treats them as conditionally independent. in this case we have the results are shown in figure the irm divides the features into clusters the first of which contains noncommunist which captures one of the most important aspects of this cold-war era dataset. it also clusters the countries into clusters reflecting natural geo-political groupings us and uk or the communist bloc and the predicates into clusters reflecting similar relationships negative behavior and accusations lvms for relational data probabilistic matrix factorization for collaborative filtering as discussed in section collaborative filtering requires predicting entries in a matrix r t t r where for example ri j is the rating that user i gave to movie j. thus we see that cf is a kind of relational learning problem one with particular commercial importance. much of the work in this area makes use of the data that netflix made available in their competition. in particular a large movie x user ratings matrix is provided. the full matrix would have entries but only of the entries are observed so the matrix is extremely sparse. in addition the data is quite imbalanced with many users rating fewer than movies and a few users rating over movies. the validation set is pairs. finally there is a separate test set with pairs for which the ranking is known but withheld from contestants. the performance measure is root mean square error rm se ui xmi n where xmi ui is the true rating of user ui on movie mi and xmi ui is the prediction. the baseline system known as cinematch had an rmse on the training set of and on the test set of to qualify for the grand prize teams needed to reduce the test rmse by i.e. get a test rmse of or less. we will discuss some of the basic methods used byt the winning team below. since the ratings are drawn from the set it is tempting to use a categorical observation model. however this does not capture the fact that the ratings are ordered. although we could use an ordinal observation model in practice people use a gaussian observation model for simplicity. one way to make the model better match the data is to pass the model s predicted mean response through a sigmoid and then to map the interval to and mnih alternatively we can make the data a better match to the gaussian model by transforming the data using rij rij and merugu we could use the irm for the cf task by associating a discrete latent variable for each user i and for each movie or video qv qu prij rqu j b n ab j and then defining i a qv this is just another example of co-clustering. we can also extend the model to generate side information such as attributes about each user andor movie. see figure for an illustration. another possibility is to replace the discrete latent variables with continuous latent variables i sku and v j skv however it has been found e.g. and banerjee that u one obtains much better results by using unconstrained real-valued latent factors for each user ui r we then use a likelihood of the form k and each movie vj r prij rui vj n i vj good results with discrete latent variables have been obtained on some datasets that are smaller than netflix such as movielens and eachmovie. however these datasets are much easier to predict because there is less imbalance between the number of reviews performed by different users netflix some users have rated more than movies whereas others have rated less than chapter latent variable models for discrete data figure visualization of a small relational dataset where we have one relation likesuser movie and features for movies genre and users occupation. from figure of et al. used with kind permission of zhao xu. r o t c e v r o t c a f v v vj rij t t ui u u the royal tenenbau m s julien donkey boy punch drunk love i heart huckabees lost in translation being john m alkovich belle de jour kill bill vol. natural born killers citizen kane scarface freddy g ot fingered half baked freddy vs. jason road trip the longest yard the fast and the furious arm ageddon catwo m an coyote ugly the wizard of oz runaway bride sister act step m o m m aid in m anhattan annie hall sophie s choice m oonstruck the w ay w e w ere the sound of m usic the w altons season factor vector figure a dgm for probabilistic matrix factorization. visualization of the first two factors in the pmf model estimated from the netflix challenge data. each movie j is plotted at the location specified vj. on the left we have low-brow humor and horror movies baked freddy vs jason and on the right we have more serious dramas s choice moonstruck. on the top we have critically acclaimed independent movies love i heart huckabees and on the bottom we have mainstream hollywood blockbusters runway bride. the wizard of oz is right in the middle of these axes. from figure of et al. used with kind permission of yehuda koren. this has been called probabilistic matrix factorization and mnih see figure for the dgm. the intuition behind this method is that each user and each movie get embedded into the same low-dimensional continuous space figure if a user is close to a movie in that space they are likely to rate it highly. all of the best entries in the netflix competition used this approach in one form or pmf is closely related to the svd. in particular if there is no missing data then computing the mle for the ui s and the vj s is equivalent to finding a rank k approximation to r. however as soon as we have missing data the problem becomes non-convex as shown in the winning entry was actually an ensemble of different methods including pmf nearest neighbor methods etc. lvms for relational data e s m r netflix baseline score svd pmf constrained pmf epochs e s m r plain wbiases wimplicit feedback wtemporal dynamics millions of parameters figure rmse on the validation set for different pmf variants vs number of passes through the data. svd is the unregularized version u v corresponds to u and v while corresponds to u and v corresponds to a version where the mean and diagonal covariance of the gaussian prior were learned from data. from figure of and mnih used with kind permission of ruslan salakhutdinov. rmse on the test set portion vs number of parameters for several different models. plain is the baseline pmf with suitably chosen u v with biases adds fi and gj offset terms. with implicit feedback with temporal dynamics allows the offset terms to change over time. the netflix baseline system achieves an rmse of and the grand prize s required accuracy is was obtained on september figure generated by netflixresultsplot. from figure of et al. used with kind permission of yehuda koren. and jaakkola and standard svd methods cannot be applied. netflix challenge only about of the matrix is observed. the most straightforward way to fit the pmf model is to minimize the overall nll i vj that in the ju v log pru v o log where oij if user i has seen movie j. since this is non-convex we can just find a locally optimal mle. since the netflix data is so large million observed entries it is common to use stochastic gradient descent for this task. the gradient for ui is given by dj dui d dui ioij ut i ij joij eijvj where eij rij ut i has watched the update takes the following simple form i vj is the error term. by stochastically sampling a single movie j that user ui ui eijvj where is the learning rate. the update for vj is similar. chapter latent variable models for discrete data of course just maximizing the likelihood results in overfitting as shown in figure we can regularize this by imposing gaussian priors n v v n u u pu v i j if we use u v u u ik and v v ik the new objective becomes ju v log pr u vo ioij ut u v i i j const i j where we have defined u v by varying the regularizers we can reduce the effect of overfitting as shown in figure we can find map estimates using stochastic gradient descent. we can also compute approximate posteriors using variational bayes and raiko u and v if we use diagonal covariances for the priors we can penalize each latent dimension by a different amount. also if we use non-zero means for the priors we can account for offset terms. optimizing the prior parameters u u v v at the same time as the model parameters v is a way to create an adaptive prior. this avoids the need to search for the optimal values of u and v and gives even better results as shown in figure it turns out that much of the variation in the data can be explained by movie-specific or user-specific effects. for example some movies are popular for all types of users. and some users give low scores for all types of movies. we can model this by allowing for user and movie specific offset or bias terms as follows prij rui vj n i vj fi gj where is the overall mean fi is the user bias gj is the movie bias and ut i vj is the interaction term. this is equivalent to applying pmf just to the residual matrix and gives much better results as shown in figure we can estimate the fi gj and terms using stochastic gradient descent just as we estimated u v and we can also allow the bias terms to evolve over time to reflect the changing preferences of users this is important since in the netflix competition the test data was more recent than the training data. figure shows that allowing for temporal dynamics can help a lot. often we also have side information of various kinds. in the netflix competition entrants knew which movies the user had rated in the test set even though they did not know the values of these ratings. that is they knew the value of the o matrix even on the test set. if a user chooses to rate a movie it is likely because they have seen it which in turns means they thought they would like it. thus the very act of rating reveals information. conversely if a user chooses not rate a movie it suggests they knew they would not like it. so the data is not missing at random e.g. and zemel exploiting this can improve performance as shown in figure in real problems information on the test set is not available. however we often know which movies the user has watched or declined to restricted boltzmann machines watch even if they did not rate them is called implicit feedback and this can be used as useful side information. another source of side information concerns the content of the movie such as the movie genre the list of the actors or a synopsis of the plot. this can be denoted by xv the features the case where we just have the id of the video we can treat xv as a the video. dimensional bit vector with just one bit turned on. we may also know features about the user which we can denote by xu. in some cases we only know if the user clicked on the video or not that is we may not have a numerical rating. we can then modify the model as follows pru vxu xv berru vuxut where u is a k matrix and v is a k matrix can incorporate an offset term by appending a to xu and xv in the usual way. a method for computing the approximate posterior pu vd in an online fashion using adf and ep was described in et al. this was implemented by microsoft and has been deployed to predict click through rates on all the ads used by bing. unfortunately fitting this model just from positive binary data can result in an over prediction of links since no negative examples are included. better performance is obtained if one has access to the set of all videos shown to the user of which at most one was picked data of this form is known as an impression log. in this case we can use a multinomial model instead of a binary model in et al. this was shown to work much better than a binary model. to understand why suppose some is presented with a choice of an action movie starring arnold schwarzenegger an action movie starring vin diesel and a comedy starring hugh grant. if the user picks arnold schwarzenegger we learn not only that they like prefer action movies to comedies but also that they prefer schwarzenegger to diesel. this is more informative than just knowing that they like schwarzenegger and action movies. restricted boltzmann machines so far all the models we have proposed in this chapter have been representable by directed graphical models. but some models are better represented using undirected graphs. for example the boltzmann machine et al. is a pairwise mrf with hidden nodes h and visible nodes v as shown in figure the main problem with the boltzmann machine is that exact inference is intractable and even approximate inference using e.g. gibbs sampling can be slow. however suppose we restrict the architecture so that the nodes are arranged in layers and so that there are no connections between nodes within the same layer figure then the model has the form ph v z rkvr hk where r is the number of visible variables k is the number of hidden variables and v plays the role of y earlier in this chapter. this model is known as a restricted boltzmann machine or a harmonium an rbm is a special case of a product of experts which is so-called because we are multiplying together a set of experts potential functions on each edge chapter latent variable models for discrete data h v figure a general boltzmann machine with an arbitrary graph structure. the shaded nodes are partitioned into input and output although the model is actually symmetric and defines a joint density on all the nodes. a restricted boltzmann machine with a bipartite structure. note the lack of intra-layer connections. and then normalizing whereas in a mixture of experts we take a convex combination of normalized distributions. the intuitive reason why poe models might work better than a mixture is that each expert can enforce a constraint the expert has a value which is or or a don t care condition the expert has value by multiplying these experts together in different ways we can create sharp distributions which predict data which satisfies the specified constraints and teh for example consider a distributed model of text. a given document might have the topics government mafia and playboy if we multiply the predictions of each topic together the model may give very high probability to the word berlusconi and hinton by contrast adding together experts can only make the distribution broader figure typically the hidden nodes in an rbm are binary so h specifies which constraints are active. it is worth comparing this with the directed models we have discussed. in a mixture model we have one hidden variable q k. we can represent this using a set of k bits with the restriction that exactly one bit is on at a time. this is called a localist encoding since only one hidden unit is used to generate the response vector. this is analogous to the hypothetical notion of grandmother cells in the brain that are able to recognize only one kind of object. by contrast an rbm uses a distributed encoding where many units are involved in generating each output. models that used vector-valued hidden variables such as sk as in mpca lda or z r k as in epca also use distributed encodings. the main difference between an rbm and directed two-layer models is that the hidden variables are conditionally independent given the visible variables so the posterior factorizes phv phkv k this makes inference much simpler than in a directed model since we can estimate each hk silvio berlusconi is the current prime minister of italy. restricted boltzmann machines visible binary gaussian categorical multiple categorical gaussian binary hidden binary binary binary binary gaussian gaussian name binary rbm gaussian rbm categorical rbm replicated softmax undirected lda undirected pca undirected binary pca reference and sutton et al. and hinton and movellan and sutton table summary of different kinds of rbm. independently and in parallel as in a feedforward neural network. the disadvantage is that training undirected models is much harder as we discuss below. varieties of rbms in this section we describe various forms of rbms by defining different pairwise potential functions. see table for a summary. all of these are special cases of the exponential family harmonium et al. binary rbms the most common form of rbm has binary hidden nodes and binary visible nodes. the joint distribution then has the following form z pv h exp ev h ev h vrhkwrk wh vt b ht c exp ev h z vrbr hkck v h where e is the energy function w is a r k weight matrix b are the visible bias terms c are the hidden bias terms and b c are all the parameters. for notational simplicity we will absorb the bias terms into the weight matrix by clamping dummy units and and setting c and b. note that naively computing z takes time but we can reduce this to time when using a binary rbm the posterior can be computed as follows berhksigmwt phv pvh phkv k by symmetry one can show that we can generate data given the hidden variables as follows pvrh bervrsigmwt rh r r chapter latent variable models for discrete data we can write this in matrix-vetor notation as follows e sigmwt v e sigmwh the weights in w are called the generative weights since they are used to generate the observations and the weights in wt are called the recognition weights since they are used to recognize the input. from equation we see that we activate hidden node k in proportion to how much the input vector v looks like the weight vector wk to scaling factors. thus each hidden node captures certain features of the input as encoded in its weight vector similar to a feedforward neural network. categorical rbm we can extend the binary rbm to categorical visible variables by using a encoding where c is the number of states for each vir. we define a new energy function as follows et al. salakhutdinov and hinton ev h the full conditionals are given by pvrh catsbc phk sigmck vc rhkw c rk hkw c k r vc rw c rk rkc r vc rbc hkck gaussian rbm r c we can generalize the model to handle real-valued data. in particular a gaussian rbm has the following energy function ev h wrkhkvr akhk the parameters of the model are ak br. have assumed the data is standardized so we fix the variance to compare this to a gaussian in information form ncv exp t v vt v where we see that we have set i and given by k hkwk. thus the mean is k hkwk. the full conditionals which are needed for inference and restricted boltzmann machines learning are given by pvrh wrkhk phk sigm ck wrkvr k r we see that each visible unit has a gaussian distribution whose mean is a function of the hidden bit vector. more powerful models which make the depend on the hidden state can also be developed and hinton rbms with gaussian hidden units if we use gaussian latent variables and gaussian visible variables we get an undirected version of factor analysis. however it turns out that it is identical to the standard directed version and movellan if we use gaussian latent variables and categorical observed variables we get an undirected version of categorical pca in et al. this was applied to the netflix collaborative filtering problem but was found to be significantly inferior to using binary latent variables which have more expressive power. learning rbms in this section we discuss some ways to compute ml parameter estimates of rbms using gradient-based optimizers. it is common to use stochastic gradient descent since rbms often have many parameters and therefore need to be trained on very large datasets. in addition it is standard to use regularization a technique that is often called weight decay in this context. this requires a very small change to the objective and gradient as discussed in section deriving the gradient using ph v to compute the gradient we can modify the equations from section which show how to fit a generic latent variable maxent model. in the context of the boltzmann machine we have one feature per edge so the gradient is given by e e we can write this in matrix-vector form as follows wrk n ep vht w epemp vht where pempv h phv and pempv distribution. hk vi is the empirical can derive a similar expression for the bias terms by setting vr or n the first term on the gradient when v is fixed to a data case is sometimes called the clamped phase and the second term when v is free is sometimes called the unclamped chapter latent variable models for discrete data phase. when the model expectations match the empirical expectations the two terms cancel out the gradient becomes zero and learning stops. this algorithm was first proposed in et al. the main problem is efficiently computing the expectations. we discuss some ways to do this below. deriving the gradient using pv we now present an alternative way to derive equation which also applies to other energy based models. first we marginalize out the hidden variables and write the rbm in the form pv z exp f wheref is the free energy ev h exp vrhkwrk h h h exp hr vrhrwrk exp vrwrk exp vrhkwrk f using the fact that z n next we write the log-likelihood in the following form f log z n v exp f we have f z log pvi n f f z z v f f exp f n n plugging in the free energy one can show that f vr e e n e e which matches equation wrk hence wrk restricted boltzmann machines figure illustration of gibbs sampling in an rbm. the visible nodes are initialized at a datavector then we sample a hidden vector then another visible vector etc. eventually infinity we will be producing samples from the joint distribution pv h approximating the expectations we can approximate the expectations needed to evaluate the gradient by performing block gibbs sampling using equations and in more detail we can sample from the joint distribution pv h as follows initialize the chain at by setting vi for some data vector and then sample from then from then from etc. see figure for an illustration. note however that we have to wait until the markov chain reaches equilibrium until it has burned in before we can interpret the samples as coming from the joint distribution of interest and this might take a long time. a faster alternative is to use mean field where we make the approximation e e e however since pv h is typically multimodal this is usually a very poor approximation since it will average over different modes section furthermore there is a more subtle reason not to use mean field since the gradient has the form e e we see that the negative sign in front means that the method will try to make the variational bound as loose as possible and hinton this explains why earlier attempts to use mean field to learn boltzmann machines and rodriguez did not work. contrastive divergence the problem with using gibbs sampling to compute the gradient is that it is slow. we now present a faster method known as contrastive divergence or cd cd was originally derived by approximating an objective function defined as the difference of two kl divergences rather than trying to maximize the likelihood itself. however from an algorithmic point of view it can be thought of as similar to stochastic gradient descent except it approximates the unclamped expectations with brief gibbs sampling where we initialize each markov chain at the data vectors. that is we approximate the gradient for one datavector as follows eq w e vhtvi vht where q corresponds to the distribution generated by k up-down gibbs sweeps started at vi as in figure this is known as cd-k. in more detail the procedure k is as follows chapter latent variable models for discrete data hi phvi i pvhi i i vht eq it we then make the approximation such samples are sometimes called fantasy data. we can think of i as the model s best attempt at reconstructing vi after being coded and then decoded by the model. this is similar to the way we train auto-encoders which are models which try to squeeze the data through a restricted parametric bottleneck section i in the final upwards pass since this reduces the variance. however it is not valid to use e instead of sampling hi phvi in the earlier upwards passes because then each hidden unit would be able to pass more than bit of information so it would not act as much of a bottleneck. in practice it is common to use e i instead of a sampled value the whole procedure is summarized in algorithm that we follow the positive gradient since we are maximizing likelihood. various tricks can be used to speed this algorithm up such as using a momentum term using mini-batches averaging the updates etc. such details can be found in swersky et al. algorithm training for an rbm with binary hidden and visible units initialize weights w r t for each epoch do r k randomly t t for each minibatch of size b do set minibatch gradient to zero g for each case vi in the minibatch do compute i e w sample hi phvi w i pvhi w sample i e compute i w compute gradient w it accumulate g g w update parameters w w tbg i it persistent cd in section we presented a technique called stochastic maximum likelihood for fitting maxent models. this avoids the need to run mcmc to convergence at each iteration restricted boltzmann machines by exploiting the fact that the parameters are changing slowly so the markov chains will not be pushed too far from equilibrium after each update in other words there are two dynamical processes running at different time scales the states change quickly and the parameters change slowly. this algorithm was independently rediscovered in who called it persistent cd. see algorithm for the pseudocode. pcd often works better than cd e.g. marlin et al. swersky et al. although cd can be faster in the early stages of learning. algorithm persistent cd for training an rbm with binary hidden and visible units initialize weights w r initialize chains hss for t do d l randomly randomly mean field updates for each case i n do i wk ik sigmvt mcmc updates for each sample s s do vi it parameter updates g n s w w tg decrease t vshst generate hs by brief gibbs sampling from old hs applications of rbms the main application of rbms is as a building block for deep generative models which we discuss in section but they can also be used as substitutes for directed two-layer models. they are particularly useful in cases where inference of the hidden states at test time must be fast. we give some examples below. language modeling and document retrieval we can use a categorical rbm to define a generative model for bag-of-words as an alternative to lda. one subtlety is that the partition function in an undirected models depends on how big the graph is and therefore on how long the document is. a solution to this was proposed in and hinton use a categorical rbm with tied weights but multiply the hidden activation bias terms ck by the document length l to compensate form the fact that the observed word-count vector v is larger in magnitude ev h k vchkw c vcbc r l hkck chapter latent variable models for discrete data data set nips reuters number of docs train test k d st. dev. avg. test perplexity per word nats r. unigram figure comparison of rbm softmax and lda on three corpora. k is the number of words in the vocabulary d is the average document length and st. dev. is the standard deviation of the document length. source and hinton replicated softmax d lda d n o i s i c e r p reuters replicated softmax d lda d i n o s c e r p i recall recall figure precision-recall curves for rbm softmax and lda on two corpora. from figure of and hinton used with kind permission of ruslan salakhutdinov. where vc iyil c. this is like having a single multinomial node we have dropped the r subscript with c states where c is the number of words in the vocabulary. this is called the replicated softmax model and hinton and is an undirected alternative to mpca lda. we can compare the modeling power of rbms vs lda by measuring the perplexity on a test set. this can be approximated using annealing importance sampling the results are shown in figure we see that the lda is significantly better than a unigram model but that an rbm is significantly better than lda. just a single matrix-vector another advantage of the lda is that inference is fast and exact multiply followed by a sigmoid nonlinearity as in equation in addition to being faster the rbm is more accurate. this is illustrated in figure which shows precision-recall curves for rbms and lda on two different corpora. these curves were generated as follows a query document from the test set is taken its similarity to all the training documents is computed where the similarity is defined as the cosine of the angle between the two topic vectors and then the top m documents are returned for varying m a retrieved document is considered relevant if it has the same class label as that of the query s is the only place where labels are used. restricted boltzmann machines rbms for collaborative filtering rbms have been applied to the netflix collaborative filtering competition et al. in fact an rbm with binary hidden nodes and categorical visible nodes can slightly outperform svd. by combining the two methods performance can be further improved. winning entry in the challenge was an ensemble of many different types of model exercises exercise partition function for an rbm show how to compute z for an rbm with k binary hidden nodes and r binary observed nodes in time assuming k r. deep learning introduction many of the models we have looked at in this book have a simple two-layer architecture of the form z y for unsupervised latent variable models or x y for supervised models. however when we look at the brain we seem many levels of processing. it is believed that each level is learning features or representations at increasing levels of abstraction. for example the standard model of the visual cortex and wiesel serre et al. ranzato et al. suggests that speaking the brain first extracts edges then patches then surfaces then objects etc. e.g. kandel et al. for more information about how the brain might perform vision. this observation has inspired a recent trend in machine learning known as deep learning deeplearning.net and the references therein which attempts to the idea can be applied to non-vision e.g. replicate this kind of architecture in a computer. problems as well such as speech and language. in this chapter we give a brief overview of this new field. however we caution the reader that the topic of deep learning is currently evolving very quickly so the material in this chapter may soon be outdated. deep generative models deep models often have millions of parameters. acquiring enough labeled data to train such models is diffcult despite crowd sourcing sites such as mechanical turk. in simple settings such as hand-written character recognition it is possible to generate lots of labeled data by making modified copies of a small manually labeled training set e.g. figure but it seems unlikely that this approach will scale to complex to overcome the problem of needing labeled training data we will focus on unsupervised learning. the most natural way to perform this is to use generative models. in this section we discuss three different kinds of deep generative models directed undirected and mixed. there have been some attempts to use computer graphics and video games to generate realistic-looking images of complex scenes and then to use this as training data for computer vision systems. however often graphics programs cut corners in order to make perceptually appealing images which are not reflective of the natural statistics of real-world images. chapter deep learning figure some deep multi-layer graphical models. observed variables are at the bottom. a directed model. an undirected model boltzmann machine. a mixed directed-undirected model belief net. deep directed networks perhaps the most natural way to build a deep generative model is to construct a deep directed graphical model as shown in figure the bottom level contains the observed pixels whatever the data is and the remaining layers are hidden. we have assumed just layers for notational simplicity. the number and size of layers is usually chosen by hand although one can also use non-parametric bayesian methods et al. or boosting et al. to infer the model structure. we shall call models of this form deep directed networks or ddns. binary and all cpds are logistic functions this is called a sigmoid belief net this case the model defines the following joint distribution if all the nodes are in i bervisigmht j v k l unfortunately inference in directed models such as these is intractable because the posterior on the hidden nodes is correlated due to explaining away. one can use fast mean field approximations and jordan saul and jordan but these may not be very accurate since they approximate the correlated posterior with a factorial posterior. one can also use mcmc inference adams et al. but this can be quite slow because the variables are highly correlated. slow inference also results in slow learning. deep boltzmann machines a natural alternative to a directed model is to construct a deep undirected model. for example we can stack a series of rbms on top of each other as shown in figure this is known as a deep boltzmann machine or dbm and hinton if we have hidden layers the model is defined as follows v z exp ij jk kl deep generative models where we are ignoring constant offset or bias terms. the main advantage over the directed model is that one can perform efficient block gibbs sampling or block mean field since all the nodes in each layer are conditionally independent of each other given the layers above and below and larochelle the main disadvantage is that training undirected models is more difficult because of the partition function. however below we will see a greedy layer-wise strategy for learning deep undirected models. deep belief networks an interesting compromise is to use a model that is partially directed and partially undirected. in particular suppose we construct a layered model which has directed arrows except at the top where there is an undirected bipartite graph as shown in figure this model is known as a deep belief network et al. or if we have hidden layers the model is defined as follows v bervisigmht j exp kl i z essentially the top two layers act as an associative memory and the remaining layers then generate the output. has the form the advantage of this peculiar architecture is that we can infer the hidden states in a fast bottom-up fashion. to see why suppose we only have two hidden layers and that wt so the second level weights are tied to the first level weights figure this defines a model of the form one can show that the distribution expvt which is equivalent to an rbm. since the dbn is equivalent to the rbm as far as is concerned we can infer the posterior in the dbn exactly as in the rbm. this posterior is exact even though it is fully factorized. now the only way to get a factored posterior is if the prior is a complementary prior. this is a prior which when multiplied by the likelihood results in a perfectly factored posterior. thus we see that the top level rbm in a dbn acts as a complementary prior for the bottom level directed sigmoidal likelihood function. if we have multiple hidden levels andor if the weights are not tied the correspondence between the dbn and the rbm does not hold exactly any more but we can still use the factored inference rule as a form of approximate bottom-up inference. below we show that this is a valid variational lower bound. this bound also suggests a layer-wise training strategy that we will explain in more detail later. note however that top-down inference in a dbn is not tractable so dbns are usually only used in a feedforward manner. unforuntately the acronym dbn also stands for dynamic bayes net geoff hinton who invented deep belief networks has suggested the acronyms deebns and dybns for these two different meanings. however this terminology is non-standard. chapter deep learning figure a dbn with two hidden layers and tied weights that is equivalent to an rbm. source figure of the corresponding dbn. source figure of used with kind permission of ruslan salakhutdinov. a stack of rbms trained greedily. greedy layer-wise learning of dbns the equivalence between dbns and rbms suggests the following strategy for learning a dbn. fit an rbm to learn using methods described in section unroll the rbm into a dbn with hidden layers as in figure now freeze the directed weights and let be untied so it is no longer forced to be equal to wt we will now learn a better prior for by fitting a second rbm. the input data to this new rbm is the activation of the hidden units e which can be computed using a factorial approximation. continue to add more hidden layers until some stopping criterion is satisified e.g. you run out of time or memory or you start to overfit the validation set. construct the dbn from these rbms as illustrated in figure one can show et al. that this procedure always increases a lower bound the observed data likelihood. of course this procedure might result in overfitting but that is a different matter. in practice we want to be able to use any number of hidden units in each level. this means we will not be able to initialize the weights so that wt this voids the theoretical guarantee. nevertheless the method works well in practice as we will see. the method can also be extended to train dbms in a greedy way and larochelle after using the greedy layer-wise training strategy it is standard to fine tune the weights using a technique called backfitting. this works as follows. perform an upwards sampling pass to the top. then perform brief gibbs sampling in the top level rbm and perform a cd update of the rbm parameters. finally perform a downwards ancestral sampling pass is an approximate sample from the posterior and update the logistic cpd parameters using a small gradient step. this is called the up-down procedure et al. unfortunately this procedure is very slow. deep neural networks deep neural networks given that dbns are often only used in a feed-forward or bottom-up mode they are effectively acting like neural networks. in view of this it is natural to dispense with the generative story and try to fit deep neural networks directly as we discuss below. the resulting training methods are often simpler to implement and can be faster. note however that performance with deep neural nets is sometimes not as good as with probabilistic models et al. one reason for this is that probabilistic models support top-down inference as well as bottom-up inference. do not support efficient top-down inference but dbms do and this has been shown to help and larochelle top-down inference is useful when there is a lot of ambiguity about the correct interpretation of the signal. it is interesting to note that in the mammalian visual cortex there are many more feedback connections than there are feedforward connections e.g. kandel et al. the role of these feedback connections is not precisely understood but they presumably provide contextual prior information coming from the previous frame or retinal glance which can be used to disambiguate the current bottom-up signals and mumford of course we can simulate the effect of top-down inference using a neural network. however the models we discuss below do not do this. deep multi-layer perceptrons many decision problems can be reduced to classification e.g. predict which object any is present in an image patch or predict which phoneme is present in a given acoustic feature vector. we can solve such problems by creating a deep feedforward neural network or multilayer perceptron as in section and then fitting the parameters using gradient descent back-propagation. unfortunately this method does not work very well. one problem is that the gradient becomes weaker the further we move away from the data this is known as the vanishing gradient problem and frasconi a related problem is that there can be large plateaus in the error surface which cause simple first-order gadient-based methods to get stuck and bengio consequently early attempts to learn deep neural networks proved unsuccesful. recently there has been some progress due to the adoption of gpus et al. and second-order optimization algorithms nevertheless such models remain difficult to train. below we discuss a way to initialize the parameters using unsupervised learning this is called generative pre-training. the advantage of performing unsupervised learning first is that the model is forced to model a high-dimensional response namely the input feature vector rather than just predicting a scalar response. this acts like a data-induced regularizer and helps backpropagation find local minima with good generalization properties et al. glorot and bengio chapter deep learning figure training a deep autoencoder. first we greedily train some rbms. then we construct the auto-encoder by replicating the weights. finally we fine-tune the weights using back-propagation. from figure of and salakhutdinov used with kind permission of ruslan salakhutdinov. deep auto-encoders an auto-encoder is a kind of unsupervised neural network that is used for dimensionality reduction and feature discovery. more precisely an auto-encoder is a feedforward neural network that is trained to predict the input itself. to prevent the system from learning the trivial identity mapping the hidden layer in the middle is usually constrained to be a narrow bottleneck. the system can minimize the reconstruction error by ensuring the hidden units capture the most relevant aspects of the data. suppose the system has one hidden layer so the model has the form v h v. further in this case one can show that the weights to the k suppose all the functions are linear. hidden units will span the same subspace as the first k principal components of the data and joutsensalo japkowicz et al. in other words linear auto-encoders are equivalent to pca. however by using nonlinear activation functions one can discover nonlinear representations of the data. more powerful representations can be learned by using deep auto-encoders. unfortunately training such models using back-propagation does not work well because the gradient signal becomes too small as it passes back through multiple layers and the learning algorithm often gets stuck in poor local minima. one solution to this problem is to greedily train a series of rbms and to use these to initialize an auto-encoder as illustrated in figure the whole system can then be fine-tuned using backprop in the usual fashion. this approach first suggested in and salakhutdinov applications of deep networks top-level units label units units this could be the top level of another sensory pathway units x pixel image figure used with kind permission of geoff hinton. test cases of mnist. above each image is the estimated label. used with kind permission of geoff hinton. compare to figure a dbn architecture for classifying mnist digits. source figure of et al. these are the errors made by the dbn on the source figure of et al. works much better than trying to fit the deep auto-encoder directly starting with random weights. stacked denoising auto-encoders a standard way to train an auto-encoder is to ensure that the hidden layer is narrower than the visible layer. this prevents the model from learning the identity function. but there are other ways to prevent this trivial solution which allow for the use of an over-complete representation. one approach is to impose sparsity constraints on the activation of the hidden units et al. another approach is to add noise to the inputs this is called a denoising autoencoder et al. for example we can corrupt some of the inputs for example by setting them to zero so the model has to learn to predict the missing entries. this can be shown to be equivalent to a certain approximate form of maximum likelihood training as score matching applied to an rbm of course we can stack these models on top of each other to learn a deep stacked denoising auto-encoder which can be discriminatively fine-tuned just like a feedforward neural network if desired. applications of deep networks in this section we mention a few applications of the models we have been discussing. handwritten digit classification using dbns figure shows a dbn et al. consisting of hidden layers. the visible layer corresponds to binary images of handwritten digits from the mnist data set. in addition the top rbm is connected to a softmax layer with units representing the class label. chapter deep learning interbank markets european community monetaryeconomic energy markets leading economic indicators disasters and accidents legaljudicial accounts earnings government borrowings figure visualization of some bag of words data from the reuters corpus. results of using lsa. results of using a deep auto-encoder. source figure of and salakhutdinov used with kind permission of ruslan salakhutdinov. the first hidden layers were trained in a greedy unsupervised fashion from mnist digits using epochs over the data and stochastic gradient descent with the cd heuristic. this process took a few hours per layer et al. then the top layer was trained using as input the activations of the lower hidden layer as well as the class labels. the corresponding generative model had a test error of about the network weights were then carefully fine-tuned on all training images using the up-down procedure. this process took about a week et al. the model can be used to classify by performing a deterministic bottom-up pass and then computing the free energy for the top-level rbm for each possible class label. the final error on the test set was about the misclassified examples are shown in figure this was the best error rate of any method on the permutation-invariant version of mnist at that time. permutation-invariant we mean a method that does not exploit the fact that the input is an image. generic methods work just as well on permuted versions of the input figure and can therefore be applied to other kinds of datasets. the only other method that comes close is an svm with a degree polynomial kernel which has achieved an error rate of and schoelkopf by way of comparison neighbor all examples achieves this is not as good although is much data visualization and feature discovery using deep auto-encoders deep autoencoders can learn informative features from raw data. such features are often used as input to standard supervised learning methods. to illustrate this consider fitting a deep auto-encoder with a hidden bottleneck to some one can get much improved performance on this task by exploiting the fact that the input is an image. one way to do this is to create distorted versions of the input adding small shifts and translations figure for some examples. applying this trick reduced the svm error rate to similar error rates can be achieved using convolutional neural networks trained on distorted images et al. got however the point of dbns is that they offer a way to learn such prior knowledge without it having to be hand-crafted. applications of deep networks figure precision-recall curves for document retrieval in the reuters corpus. source figure of used with kind permission of ruslan salakhutdinov. text data. the results are shown in figure on the left we show the embedding produced by lsa and on the right the embedding produced by the auto-encoder. it is clear that the low-dimensional representation created by the auto-encoder has captured a lot of the meaning of the documents even though class labels were not note that various other ways of learning low-dimensional continuous embeddings of words have been proposed. see e.g. et al. for details. information retrieval using deep auto-encoders hashing in view of the sucess of rbms for information retrieval discussed in section it is natural to wonder if deep models can do even better. in fact they can as is shown in figure more interestingly we can use a binary low-dimensional representation in the middle layer of the deep auto-encoder rather than a continuous representation as we used above. this enables very fast retrieval of related documents. for example if we use a code we can precompute the binary representation for all the documents and then create a hash-table mapping codewords to documents. this approach is known as semantic hashing since the binary representation of semantically similar documents will be close in hamming distance. for the test documents in reuters this results in about documents per entry in the table. at test time we compute the codeword for the query and then simply retrieve the relevant documents in constant time by looking up the contents of the relevant address in memory. to find other other related documents we can compute all the codewords within a some details. salakhutdinov and hinton used the reuters data set which consists of newswire articles manually classified into topics. they represent each document by counting how many times each of the top most frequent words occurs. they trained a deep auto-encoder with layers on half of the data. the visible units use a replicated softmax distribution the hidden units in the middle layer have a gaussian distribution and the remaining units have the usual bernoulli-logistic distribution. when fine tuning the auto-encoder a cross-entropy loss function to maximum likelihood under a multinoulli distribution was used. see and salakhutdinov for further details. chapter deep learning figure a small convolutional rbm with two groups of hidden units each associated with a filter of size are two different views of the data in the first window the first view is computed using the filter the second view using filter similarly are the views of the data in the second window computed using and respectively. and and hamming distance of say this results in retrieving about the key point is that the total time is independent of the size of the corpus. of course there are other techniques for fast document retrieval such as inverted indices. these rely on the fact that individual words are quite informative so we can simply intersect all the documents that contain each word. however when performing image retrieval it is clear that we do not want to work at the pixel level. recently and hinton showed that a deep autoencoder could learn a good semantic hashing function that outperformed previous techniques et al. weiss et al. on the million tiny images dataset. it is hard to apply inverted indexing techniques to real-valued data one could imagine vector quantizing image patches. learning audio features using convolutional dbns to apply dbns to time series of unbounded length it is necessary to use some form of parameter tying. one way to do this is to use convolutional dbns et al. desjardins and bengio which use convolutional rbms as their basic unit. these models are a generative version of convolutional neural nets discussed in section the basic idea is illustrated in figure the hidden activation vector for each group is computed by convolving the input vector with that group s filter vector or matrix. in other words each node within a hidden group is a weighted combination of a subset of the inputs. we compute the activaton of all the hidden nodes by sliding this weight vector over the input. this allows us to model translation invariance since we use the same weights no matter where in the input vector the pattern each group has its own filter corresponding to its own pattern detector. k is the number of bit vectors that are up to a hamming distance of away. note that it is often said that the goal of deep learnng is to discover invariant features e.g. a representation of an object that does not change even as nuisance variables such as the lighting do change. however sometimes these so-called nuisance variables may be the variables of interest. for example if the task is to determine if a photograph was taken in the morning or the evening then lighting is one of the more salient features and object identity may be less relevant. as always one task s signal is another task s noise so it unwise to throw away apparently irrelevant information discussion more formally for binary signals we can define the full conditionals in a convolutional rbm as follows et al. t sigmwk vt bt phk pvs sigm hks cs k where wk is the weight vector for group k bt and cs are bias terms and a b represents the convolution of vectors a and b. it is common to add a max pooling layer as well as a convolutional layer which computes a local maximum over the filtered response. this allows for a small amount of translation invariance. it also reduces the size of the higher levels which speeds up computation considerably. defining this for a neural network is simple but defining this in a way which allows for information flow backwards as well as forwards is a bit more involved. the basic idea is similar to a noisy-or cpd where we define a probabilistic relationship between the max node and the parts it is maxing over. see et al. for details. note however that the top-down generative process will be difficult since the max pooling operation throws away so much information. et al. applies convolutional dbns of depth to auditory data. when the input consists of speech signals the method recovers a representation that is similar to phonemes. when applied to music classification and speaker identification their method outperforms techniques using standard features such as mfcc. features were fed into the same discriminative classifier. in et al. a deep neural net was used in place of a gmm inside a conventional hmm. the use of dnns significantly improved performance on conversational speech recognition. in an interview the tech lead of this project said historically there have been very few individual technologies in speech recognition that have led to improvements of this magnitude learning image features using convolutional dbns we can extend a convolutional dbn from to in a straightforward way et al. as illustrated in figure the results of a layer system trained on four classes of visual objects motorbikes faces and airplanes from the caltech dataset are shown in figure we only show the results for layers and because layer learns gabor-like filters that are very similar to those learned by sparse coding shown in figure we see that layer has learned some generic visual parts that are shared amongst object classes and layer seems to have learned filters that look like grandmother cells that are specific to individual object classes and in some cases to individual objects. discussion so far we have been discussing models inspired by low-level processing in the brain. these models have produced useful features for simple classification tasks. but can this pure bottom-up too early. source chapter deep learning figure a convolutional rbm with max-pooling layers. the input signal is a stack of images color planes. each input layer is passed through a different set of filters. each hidden unit is obtained by convolving with the appropriate filter and then summing over the input planes. the final layer is obtained by computing the local maximum within a small window. source figure of et al. used with kind permission of bo chen. faces cars airplanes motorbikes figure visualization of the filters learned by a convolutional dbn in layers two and three. source figure of et al. used with kind permission of honglak lee. approach scale to more challenging problems such as scene interpretation or natural language understanding? to put the problem in perspective consider the dbn for handwritten digit classification in figure this has about free parameters although this is a lot it is tiny compared to the number of neurons in the brain. as hinton says this is about as many parameters as cubic millimetres of mouse cortex and several hundred networks of this complexity could fit within a single voxel of a high-resolution fmri scan. this suggests that much bigger networks may be required to compete with human shape recognition abilities. et al. to scale up to more challenging problems various groups are using gpus e.g. et al. andor parallel computing. but perhaps a more efficient approach is to work at a higher level of abstraction where inference is done in the space of objects or their parts rather discussion than in the space of bits and pixels. that is we want to bridge the signal-to-symbol divide where by symbol we mean something atomic that can be combined with other symbols in a compositional way. the question of how to convert low level signals into a more structured semantic representation is known as the symbol grounding problem traditionally such symbols are associated with words in natural language but it seems unlikely we can jump directly from low-level signals to high-level semantic concepts. instead what we need is an intermediate level of symbolic or atomic parts. a very simple way to create such parts from real-valued signals such as images is to apply vector quantization. this generates a set of visual words. these can then be modelled using some of the techniques from chapter for modeling bags of words. such models however are still quite shallow it is possible to define and learn deep models which use discrete latent parts. here we just mention a few recent approaches to give a flavor of the possibilites. et al. combine rbms with hierarchical latent dirichlet allocation methods trained in an unsupervised way. et al. use latent and-or graphs trained in a manner similar to a latent structural svm. a similar approach based on grammars is described in et al. what is interesting about these techniques is that they apply data-driven machine learning methods to rich structuredsymbolic ai-style models. this seems like a promising future direction for machine learning. notation introduction it is very difficult to come up with a single consistent notation to cover the wide variety of data models and algorithms that we discuss. furthermore conventions differ between machine learning and statistics and between different books and papers. nevertheless we have tried to be as consistent as possible. below we summarize most of the notation used in this book although individual sections may introduce new notation. note also that the same symbol may have different meanings depending on the context although we try to avoid this where possible. general math notation symbol x y x y a b a b a ix n! o r argmaxx f meaning floor of x i.e. round down to nearest integer ceiling of x i.e. round up to nearest integer convolution of x and y hadamard product of x and y logical and logical or logical not indicator function ix if x is true else ix infinity tends towards e.g. n proportional to so y ax can be written as y x absolute value size of a set factorial function vector of first derivatives hessian matrix of second derivatives defined as big-o roughly means order of magnitude the real numbers range convention n approximately equal to argmax the value x that maximizes f notation ba b b n k ij xy expx x k k k k beta function ba b multivariate beta function n choose k equal to n!k!n k! dirac delta function if x else kronecker delta equals if i j otherwise equals kronecker delta equals if x y otherwise equals exponential function ex gamma function digamma function d a set from which values are drawn x r d ux udu dx log linear algebra notation we use boldface lowercase to denote vectors such as a and boldface uppercase to denote matrices such as a. vectors are assumed to be column vectors unless noted otherwise. symbol a tra deta a a at at diaga diaga i or id or or aj ai aij x y meaning a is a positive definite matrix trace of a matrix determinant of matrix a determinant of matrix a inverse of a matrix pseudo-inverse of a matrix transpose of a matrix transpose of a vector diagonal matrix made from vector a diagonal vector extracted from matrix a identity matrix of size d d on diagonal zeros off vector of ones length d vector of zeros length d euclidean or norm norm j th column of matrix transpose of i th row of matrix column vector element j of matrix a tensor product of x and y j probability notation we denote random and fixed scalars by lower case random and fixed vectors by bold lower case and random and fixed matrices by bold upper case. occastionally we use non-bold upper case to denote scalar random variables. also we use p for both discrete and continuous random variables. notation symbol x y x y x y x y x p cov e eq h or h i y kl l a mode px pxy sigmx var z meaning x is independent of y x is not independent of y x is conditionally independent of y given z x is not conditionally independent of y given z x is distributed according to distribution p parameters of a beta or dirichlet distribution covariance of x expected value of x expected value of x wrt distribution q entropy of distribution px mutual information between x and y kl divergence from distribution p to q log-likelihood function loss function for taking action a when true state of nature is precision variance precision matrix most probable value of x mean of a scalar distribution mean of a multivariate distribution probability density or mass function conditional probability density of x given y cdf of standard normal pdf of standard normal multinomial parameter vector stationary distribution of markov chain correlation coefficient sigmoid function variance covariance matrix variance of x degrees of freedom parameter normalization constant of a probability distribution x machine learningstatistics notation in general we use upper case letters to denote constants such as c d k n s t etc. we use lower case letters as dummy indexes of the appropriate range such as c to index classes j to index input features k to index states or clusters s to index samples t to index time etc. to index data cases we use the notation i although the notation n n is also widely used. we use x to represent an observed data vector. in a supervised problem we use y or y to represent the desired output label. we use z to represent a hidden variable. sometimes we also use q to represent a hidden discrete variable. notation symbol c d r d dtest j k y k n nc q q old s t t t m l m ap w w xij xi x x x x y zij iyn c meaning number of classes dimensionality of data vector of features number of outputs variables training data d or d yii test data cost function number of states or dimensions of a variable latent kernel function kernel matrix strength of or regularizer number of data cases number of examples of class c nc basis function expansion of feature vector x basis function expansion of design matrix x approximate or proposal distribution auxiliary function in em number of samples length of a sequence test statistic for data transition matrix of markov chain parameter vector s th sample of parameter vector estimate mle or map of maximum likelihood estimate of map estimate of estimate posterior mean of vector of regression weights in statistics matrix of regression weights component feature j of data case i fori d training case i design matrix of size n d empirical mean x n future test case future test case vector of all training labels y yn latent component j for case i xi graphical model notation in graphical models we index nodes by s t u v and states by i j k x notation symbol meaning s t bel c chj descj g e mbt nbdt pat predt cxc s sjk v node s is connected to node t belief function cliques of a graph child of node j in a dag descendants of node j in a dag a graph edges of a graph markov blanket of node t neighborhood of node t parents of node t in a dag predecessors of node t in a dag wrt some ordering potential function for clique c separators of a graph prob. node s is in state k given its parents are in states j nodes of a graph notation list of commonly used abbreviations abbreviation meaning cdf cpd cpt crf dag dgm eb em ep glm gmm hmm iid iff kl lds lhs map mcmc mh mle mpm mrf mse nll ols pd pdf pmf rbpf rhs rjmcmc rss slds sse ugm vb wrt cumulative distribution function conditional probability distribution conditional probability table conditional random field directed acyclic graphic directed graphical model empirical bayes expectation maximization algorithm expectation propagation generalized linear model gaussian mixture model hidden markov model independent and identically distributed if and only if kullback leibler divergence linear dynamical system left hand side an equation maximum a posterior estimate markov chain monte carlo metropolis hastings maximum likelihood estimate maximum of posterior marginals markov random field mean squared error negative log likelihood ordinary least squares positive definite probability density function probability mass function rao-blackwellised particle filter right hand side an equation reversible jump mcmc residual sum of squares switching linear dynamical system sum of squared errors undirected graphical model variational bayes with respect to aji s. m. and r. j. mceliece march. the generalized distributive law. info. theory ieee trans. alag s. and a. agogino inference using message propogation and topology transformation in vector gaussian continuous networks. in uai. albers c. m. leisink and h. kappen the cluster variation method for efficient linkage analysis on extended pedigrees. bmc bioinformatics albert j. and s. chib bayesian analysis of binary and polychotomous response data. j. of the am. stat. assoc. allwein e. r. schapire and y. singer reducing multiclass to binary a unifying approach for margin classifiers. j. of machine learning research aloise d. a. deshpande p. hansen and p. popat np-hardness of euclidean sum-of-squares clustering. machine learning alpaydin e. machine learning. mit press. introduction to altun y. t. hofmann and i. tsochantaridis large margin methods for structured and interdependent output variables. in g. bakir t. hofmann b. scholkopf a. smola b. taskar and s. vishwanathan machine learning with structured outputs. mit press. amir e. approximation algorithms for treewidth. algorithmica amir e. and s. mcilraith partition-based reasoning for first-order and propositional artificial intelligence theories. logical bibliography abend k. t. j. harley and l. n. kanal classification of binary random patterns. ieee transactions on information theory ackley d. g. hinton and t. sejnowski a learning algorithm for boltzmann machines. cognitive science adams r. p. h. wallach and z. ghahramani learning the structure of deep sparse graphical models. in aistatistics. aggarwal d. and s. merugu predictive discrete latent factor models for large scale dyadic data. in proc. of the int l conf. on knowledge discovery and data mining. ahmed a. and e. xing on tight approximate inference of the logistic-normal topic admixture model. in aistatistics. ahn j.-h. and j.-h. oh a constrained em algorithm for principal component analysis. neural computation ahn s. a. korattikara and m. welling bayesian posterior sampling via stochastic gradient fisher scoring. in intl. conf. on machine learning. airoldi e. d. blei s. fienberg and e. xing mixed-membership stochastic blockmodels. j. of machine learning research ando r. and t. zhang a framework for learning predictive structures from multiple tasks and unlabeled data. j. of machine learning research aitchison j. the statistical analysis of compositional data. j. of royal stat. soc. series b andrews d. and c. mallows scale mixtures of normal distributions. j. of royal stat. soc. series b andrieu c. n. de freitas and a. doucet sequential bayesian estimation and model selection for dynamic kernel machines. technical report cambridge univ. andrieu c. n. de freitas and a. doucet robust full bayesian learning for radial basis networks. neural computation andrieu c. n. de freitas a. doucet and m. jordan an introduction to mcmc for machine learning. machine learning andrieu c. a. doucet and v. tadic online em for parameter estimation in nonlinear-non gaussian state-space models. in proc. ieee cdc. andrieu c. and j. thoms a tutorial on adaptive mcmc. statistical computing aoki m. state space modeling of time series. springer. archambeau c. and f. bach sparse probabilistic projections. in nips. argyriou a. t. evgeniou and m. pontil convex multi-task feature learning. machine learning armagan a. d. dunson and j. lee generalized double pareto shrinkage. technical report duke. armstrong h. bayesian estimation of decomposable gaussian graphical models. thesis unsw. ph.d. armstrong h. c. carter k. wong and r. kohn bayesian covariance matrix estimation using a mixture of decomposable graphical models. statistics and computing arnborg s. d. g. corneil and a. proskurowski complexity of finding embeddings in a ktree. siam j. on algebraic and discrete methods arora s. and b. barak complexity theory a modern approach. cambridge. arthur d. and s. vassilvitskii kmeans the advantages of careful seeding. in proc. acm-siam symp. on discrete algorithms pp. a bibliography arulampalam m. s. maskell n. gordon and t. clapp february. a tutorial on particle filters for online nonlinearnongaussian bayesian tracking. ieee trans. on signal processing asavathiratham c. the influence model a tractable representation for the dynamics of networked markov chains. ph.d. thesis mit dept. eecs. atay-kayis a. and h. massam a monte carlo method for computing the marginal likelihood in nondecomposable gaussian graphical models. biometrika attenberg j. k. weinberger a. smola a. dasgupta and m. zinkevich collaborative spam filtering with the hashing trick. in virus bulletin. attias h. independent factor analysis. neural computation attias h. a variational bayesian framework for graphical models. in bach f. bolasso model consistent lasso estimation through the bootstrap. in intl. conf. on machine learning. bach f. and m. jordan thin junction trees. in nips. bach f. and m. jordan a probabilistic interpretation of canonical correlation analysis. technical report u. c. berkeley. bach f. and e. moulines nonasymptotic analysis of stochastic approximation algorithms for machine learning. in nips. bahmani b. b. moseley a. vattani r. kumar and s. vassilvitskii scalable k-means. in vldb. bakker b. and t. heskes task clustering and gating for bayesian multitask learning. j. of machine learning research baldi p. and y. chauvin smooth online learning algorithms for hidden markov models. neural computation balding d. a tutorial on statistical methods for population association studies. nature reviews genetics technique occuring in the statistical analysis of probabalistic functions in markov chains. the annals of mathematical statistics beal m. variational algorithms for approximate bayesian inference. ph.d. thesis gatsby unit. beal m. and z. ghahramani variational bayesian learning of directed graphical models with hidden variables. bayesian analysis beal m. j. z. ghahramani and c. e. rasmussen the infinite hidden markov model. in beck a. and m. teboulle a fast iterative shrinkage-thresholding algorothm for linear inverse problems. siam j. on imaging sciences beinlich i. h. suermondt r. chavez and g. cooper the alarm monitoring system a case study with two probabilistic inference techniques for belief networks. in proc. of the second european conf. on ai in medicine pp. bekkerman r. m. bilenko and j. langford scaling up machine learning. cambridge. bell a. j. and t. j. sejnowski an information maximisation approach to blind separation and blind deconvolution. neural computation bengio y. learning deep architectures for ai. foundations and trends in machine learning bengio y. and s. bengio modeling high-dimensional discrete data with multi-layer neural networks. in nips. banerjee o. l. e. ghaoui and a. d aspremont model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. j. of machine learning research bar-shalom y. and t. fortmann tracking and data association. academic press. bar-shalom y. and x. li estimation and tracking principles techniques and software. artech house. barash y. and n. friedman context-specific bayesian clustering for gene expression data. j. comp. bio. barber d. expectation correction for smoothed inference in switching linear dynamical systems. j. of machine learning research barber d. and c. bishop ensemble learning in bayesian neural networks. in c. bishop neural networks and machine learning pp. springer. barber d. and s. chiappa unified inference for variational bayesian linear gaussian state space models. in nips. barbieri m. and j. berger optimal predictive model selection. annals of statistics bartlett p. m. jordan and j. mcauliffe convexity classification and risk bounds. j. of the am. stat. assoc. baruniak r. compressive sensieee signal processing maga ing. zine. barzilai j. and j. borwein two point step size gradient methods. ima j. of numerical analysis p. vincent bengio y. o. delalleau n. roux and j. paiement m. ouimet learning eigenfunctions links spectral embedding and kernel pca. neural computation bengio y. and p. frasconi diffusion of context and credit information in markovian models. j. of ai research bengio y. and p. frasconi inputoutput hmms for sequence processing. ieee trans. on neural networks human basu s. t. choudhury b. clarkson learnand a. pentland interactions with ing the influence model. technical report mit media lab. baum l. e. t. petrie g. soules and n. weiss a maximization bibliography bengio y. p. lamblin d. popovici and h. larochelle greedy layer-wise training of deep networks. in nips. berchtold a. the double chain markov model. comm. stat. theor. methods berger j. bayesian salesmanship. in p. k. goel and a. zellner bayesian inference and decision techniques with applications essays in honor of bruno definetti. north-holland. berger j. and r. wolpert the likelihood principle. the institute of mathematical statistics. edition. berkhin p. a survey of clustering datamining techniques. in j. kogan c. nicholas and m. teboulle grouping multidimensional data recent advances in clustering pp. springer. bernardo j. and a. smith bayesian theory. john wiley. berrou c. a. glavieux and p. thitimajashima near shannon limit error-correcting coding and decoding turbo codes. proc. ieee intl. comm. conf.. berry d. and y. hochberg bayesian perspectives on multiple comparisons. j. statist. planning and inference bertele u. and f. brioschi nonserial dynamic programming. academic press. bertsekas d. parallel and distribution computation numerical methods. athena scientific. bertsekas d. nonlinear proathena gramming ed.. scientific. bertsekas d. and j. tsitsiklis introduction to probability. athena scientific. edition. besag j. statistical analysis of non-lattice data. the statistician bhattacharya a. and d. b. dunson simplex factor models for multivariate unordered categorical data. j. of the am. stat. assoc.. to appear. bickel p. and e. levina some theory for fisher s linear discriminant function bayes and some alternatives when there are many more variables than observations. bernoulli bickson d. gaussian belief propagation theory and application. ph.d. thesis hebrew university of jerusalem. bilmes j. dynamic bayesian multinets. in uai. bilmes j. a. graphical models and automatic speech recognition. technical report univ. washington dept. of elec. eng. binder j. d. koller s. j. russell and k. kanazawa adaptive probabilistic networks with hidden variables. machine learning binder j. k. murphy and s. russell space-efficient inference in dynamic probabilistic networks. in intl. joint conf. on ai. birnbaum a. on the foundaj. of tions of statistical infernece. the am. stat. assoc. bishop c. bayesian pca. nips. in bishop c. pattern recognition and machine learning. springer. bishop c. pattern recognition and machine learning. springer. bishop c. and g. james analysis of multiphase flows using dualenergy densitometry and neural networks. nuclear instruments and methods in physics research bishop c. and m. svens n bayesian hierarchical mixtures of experts. in uai. bishop c. and m. tipping variational relevance vector machines. in uai. bishop c. m. neural networks for pattern recognition. clarendon press. bishop y. s. fienberg and p. holland discrete multivariate analysis theory and practice. mit press. bistarelli s. u. montanari and semiring-based f. rossi constraint satisfaction and optimization. j. of the acm blake a. p. kohli and c. rother advances in markov random fields for vision and image processing. mit press. blei d. and j. lafferty corre lated topic models. in nips. blei d. and j. lafferty dynamic topic models. in intl. conf. on machine learning pp. blei d. and j. lafferty a correlated topic model of annals of applied stat. blei d. and j. mcauliffe march. supervised topic models. technical report princeton. blei d. a. ng and m. jordan latent dirichlet allocation. j. of machine learning research blumensath t. and m. davies on the difference between orthogonal matching pursuit and orthogonal least squares. technical report u. edinburgh. bo l. c. sminchisescu a. kanaujia and d. metaxas fast algorithms for large scale conditional prediction. in cvpr. bohning d. multinomial logistic regression algorithm. annals of the inst. of statistical math. bollen k. structural equation john models with latent variables. wiley sons. july. bordes a. l. bottou and p. gallinari sgd-qn careful quasi-newton stochastic gradient descent. j. of machine learning research bhatnagar n. c. bogdanov and e. mossel the computational complexity of estimating convergence time. technical report bishop c. m. mixture density networks. technical report ncrg neural computing research group department of computer science aston university. bordes a. l. bottou p. gallinari j. chang and s. a. smith erratum sgdqn is less careful than expected. j. of machine learning research bibliography boser b. e. i. m. guyon and v. n. vapnik a training algorithm for optimal margin classifiers. in proc. of the workshop on computational learning theory. brand m. structure learning in conditional probability models via an entropic prior and parameter extinction. neural computation brown p. m. vannucci and t. fearn multivariate bayesian variable selection and prediction. j. of the royal statistical society b bottcher s. g. and c. dethlefsen deal a package for learning bayesian networks. j. of statistical software braun m. and j. mcauliffe variational inference for large-scale models of discrete choice. j. of the am. stat. assoc. bottolo l. and s. richardson search. evolutionary bayesian analysis stochastic bottou l. online algorithms and stochastic approximations. in d. saad online learning and neural networks. cambridge. bottou l. learning with large datasets tutorial. bottou l. o. chapelle d. decoste and j. weston large scale kernel machines. mit press. bouchard g. efficient bounds for the softmax and applications to approximate inference in hybrid models. in nips workshop on approximate inference in hybrid models. bouchard-cote a. and m. jordan optimization of structured mean field objectives. in uai. bowman a. and a. azzalini applied smoothing techniques for data analysis. oxford. box g. and n. draper empirical model-building and response surfaces. wiley. box g. and g. tiao bayesian in statistical analysis. inference addison-wesley. boyd s. and l. vandenberghe convex optimization. cambridge. boyen x. and d. koller tractable inference for complex stochastic processes. in uai. boykov y. o. veksler and r. zabih fast approximate energy minimization via graph cuts. ieee trans. on pattern analysis and machine intelligence brand m. coupled hidden markov models for modeling interacting processes. technical report mit lab for perceptual computing. breiman l. bagging predictors. machine learning breiman l. arcing classifiers. annals of statistics breiman l. random forests. machine learning breiman l. statistical modeling the two cultures. statistical science breiman l. j. friedman and r. olshen classification and regression trees. wadsworth. breslow n. e. and d. g. clayton approximate inference in generalized linear mixed models. j. of the am. stat. assoc. briers m. a. doucet and s. maskel smoothing algorithms for state-space models. annals of the institute of statistical mathematics brochu e. m. cora and n. de freitas november. a tutorial on bayesian optimization of expensive cost functions with application to active user modeling and hierarchical learning. technical report department of computer science university of british columbia. reinforcement brooks s. and g. roberts assessing convergence of markov chain monte carlo algorithms. statistics and computing brown l. t. cai and a. dasgupta interval estimation for a binomial proportion. statistical science brown m. p. r. hughey a. krogh i. s. mian k. sj lander and d. haussler using dirichlet mixtures priors to derive hidden markov models for protein families. in intl. conf. on intelligent systems for molecular biology pp. bruckstein a. d. donoho and m. elad from sparse solutions of systems of equations to sparse modeling of signals and images. siam review bryson a. and y.-c. ho applied optimal control optimization estimation and control. blaisdell publishing company. buhlmann p. and t. hothorn boosting algorithms regularization prediction and model fitting. statistical science buhlmann s. p. de and geer statistics for highdimensional data methodology theory and applications. springer. van buhlmann p. and b. yu boosting with the loss regression and classification. j. of the am. stat. assoc. buhlmann p. and b. yu sparse j. of machine learning boosting. research bui h. s. venkatesh and g. west policy recognition in the abstract hidden markov model. j. of ai research buntine w. variational extensions to em and multinomial pca. in intl. conf. on machine learning. buntine w. and a. jakulin applying discrete pca in data analysis. in uai. buntine w. and a. jakulin discrete component analysis. in subspace latent structure and feature selection statistical and optimization perspectives workshop. buntine w. and a. weigend bayesian backpropagation. complex systems burges c. j. t. shaked e. renshaw a. lazier m. deeds n. hamilton and g. hullender learning to rank using gradient descent. in intl. conf. on machine learning pp. burkard r. m. dell amico and assignment s. martello problems. siam. bibliography byran k. and t. leise the eigenvector the linear algebra behind google. siam review calvetti d. and e. somersalo introduction to bayesian scientific computing. springer. candes e. j. romberg and t. tao robust uncertainty principles exact signal reconstruction from highly incomplete frequency information. ieee. trans. inform. theory candes e. and m. wakin march. an introduction to compressive sampling. ieee signal processing magazine candes e. m. wakin and s. boyd enhancing sparsity by reweighted minimization. j. of fourier analysis and applications cannings c. e. a. thompson and m. h. skolnick probability functions in complex pedigrees. advances in applied probability canny j. gap a factor model in proc. anintl. acm sigir conference for discrete data. nual pp. cao z. t. qin t.-y. liu m.-f. tsai and h. li learning to rank from pairwise approach to listwise approach. in intl. conf. on machine learning pp. a cappe o. online expectation maximisation. in k. mengersen m. titterington and c. robert mixtures. cappe o. and e. mouline june. online em algorithm for latent data models. j. of royal stat. soc. series b cappe o. e. moulines and t. ryden inference in hidden markov models. springer. carbonetto p. unsupervised statistical models for general object recognition. master s thesis university of british columbia. caron f. and a. doucet sparse bayesian nonparametric regression. in intl. conf. on machine learning. carreira-perpinan m. and c. williams an isotropic gaussian mixture can have more modes than components. technical report school of informatics u. edinburgh. carter c. and r. kohn on gibbs sampling for state space models. biometrika carterette b. p. bennett d. chickering and s. dumais here or there preference judgments for relevance. in proc. ecir. caruana r. a dozen tricks with in g. orr and multitask learning. k.-r. mueller neural networks tricks of the trade. springerverlag. caruana r. and a. niculescu-mizil an empirical comparison of supervised learning algorithms. in intl. conf. on machine learning. carvahlo c. n. polson and j. scott the horseshoe estimator for sparse signals. biometrika carvahlo l. and c. lawrence centroid estimation in discrete high-dimensional spaces with applications in biology. proc. of the national academy of science usa carvalho c. m. and m. west dynamic matrix-variate graphical models. bayesian analysis casella g. and r. berger statistical inference. duxbury. edition. castro m. m. coates and r. d. nowak likelihood based hierarchical clustering. ieee trans. in signal processing celeux g. and j. diebolt the sem algorithm a probabilistic teacher derive from the em algorithm for the mixture problem. computational statistics quarterly carlin b. p. and t. a. louis bayes and empirical bayes methods for data analysis. chapman and hall. cemgil a. t. a technique for painless derivation of kalman filtering recursions. technical report u. nijmegen. cesa-bianchi n. and g. lugosi learning and games. prediction cambridge university press. cevher v. learning with com pressible priors. in nips. chai k. m. a. multi-task learning with gaussian processes. ph.d. thesis u. edinburgh. chang h. y. weiss and w. freeman informative sensing. technical report hebrew u. submitted to ieee transactions on info. theory. chang j. and d. blei hierarchical relational models for document networks. the annals of applied statistics chang j. j. boyd-graber s. gerrish c. wang and d. blei reading tea leaves how humans interpret topic models. in nips. chapelle o. and l. li an empirical evaluation of thompson sampling. in nips. chartrand r. and w. yin iteratively reweighted algorithms for compressive sensing. in intl. conf. on acoustics speech and signal proc. chechik g. a. g. n. tishby and information boty. weiss tleneck for gaussian variables. j. of machine learning research a cheeseman p. j. kelly m. self j. stutz w. taylor and d. freeman autoclass a bayesian classification system. in proc. of the fifth intl. workshop on machine learning. cheeseman p. and j. stutz bayesian classification in fayyad theory and results. and pratetsky-shapiro uthurasamy advances in knowledge discovery and data mining. mit press. smyth chen b. k. swersky b. marlin and n. de freitas sparsity priors and boosting for learning localized distributed feature representations. technical report ubc. chen b. j.-a. ting b. marlin and n. de freitas deep learning of features from video. in nips workshop on deep learning. invariant spatio-temporal bibliography chen m. d. carlson a. zaas c. woods g. ginsburg a. hero j. lucas and l. carin march. the bayesian elastic net classifying multi-task geneexpression data. ieee trans. biomed. eng. chen r. and s. liu mixture kalman filters. j. royal stat. soc. b. chen s. and j. goodman an empirical study of smoothing techniques for language modeling. in proc. acl pp. chen s. and j. goodman an empirical study of smoothing techniques for language modeling. technical report dept. comp. sci. harvard. chen s. and j. wigger july. fast orthogonal least squares algorithm for efficient subset model selection. ieee trans. signal processing chen s. s. d. l. donoho and m. a. saunders atomic decomposition by basis pursuit. siam journal on scientific computing chen x. s. kim q. lin j. g. carbonell and e. p. xing graph-structured multi-task regression and an efficient optimization method for general fused lasso. technical report cmu. chib s. marginal from the gibbs output. am. stat. assoc. likelihood j. of the chickering d. learning bayesian networks is np-complete. in aistats v. chickering d. and d. heckerman efficient approximations for the marginal likelihood of incomplete data given a bayesian network. machine learning chickering d. m. optimal structure identification with greedy search. journal of machine learning research chipman h. e. george and r. mcbayesian cart j. of the am. stat. culloch model search. assoc. chipman h. e. george and r. mcculloch the practical implementation of bayesian model selection. model selection. ims lecture notes. chipman h. e. george and r. mcculloch bayesian ensemble learning. in nips. chipman h. e. george and r. mcculloch bart bayesian additive regression trees. ann. appl. stat. choi m. v. tan a. anandkumar and a. willsky learning latent tree graphical models. j. of machine learning research. j. trees and bechoi m. exploiting and improving yond tree-structured graphical models. ph.d. thesis mit. choset h. and k. nagatani topological simultaneous localization and mapping toward exact localization without explicit localization. ieee trans. robotics and automation chow c. k. and c. n. liu approximating discrete probability distributions with dependence trees. ieee trans. on info. theory christensen o. g. roberts and m. sk uld robust markov chain monte carlo methods for spatial generalized linear mixed models. j. of computational and graphical statistics chung f. spectral graph the ory. ams. cimiano p. a. schultz s. sizov p. sorg and s. staab explicit versus latent concept models for cross-language information retrieval. in intl. joint conf. on ai. cipra b. the ising model is np-complete. siam news ciresan d. c. u. meier l. m. gambardella and j. schmidhuber deep big simple neural nets for handwritten digit recognition. neural computation clarke b. bayes model averaging and stacking when model approximation error cannot be ignored. j. of machine learning research cleveland w. and s. devlin locally-weighted regression an approach to regression analysis by local fitting. j. of the am. stat. assoc. collins m. discriminative training methods for hidden markov models theory and experiments with perceptron algorithms. in emnlp. collins m. s. dasgupta and r. e. schapire a generalization of principal components analysis to the exponential family. in collins m. and n. duffy conlan volution kernels for natural guage. in nips. collobert r. and j. weston a unified architecture for natural language processing deep neural networks with multitask learning. in intl. conf. on machine learning. combettes p. and v. wajs signal recovery by proximal forwardbackward splitting. siam j. multiscale model. simul. cook j. exact calculation of beta inequalities. technical report m. d. anderson cancer center dept. biostatistics. cooper g. and e. herskovits a bayesian method for the induction of probabilistic networks from data. machine learning cooper g. and c. yoo causal discovery from a mixture of experimental and observational data. in uai. cover t. and p. hart nearest neighbor pattern classification. ieee trans. inform. theory cover t. m. and j. a. thomas elements of information theory. john wiley. cover t. m. and j. a. thomas information theory. john wiley. edition. elements of clarke b. e. fokoue and h. h. zhang principles and theory for data mining and machine learning. springer. cowles m. and b. carlin markov chain monte carlo convergence diagnostics a comparative review. j. of the am. stat. assoc. bibliography crisan d. p. d. moral and t. lyons discrete filtering using branching and interacting particle systems. markov processes and related fields dawid a. p. and s. l. lauritzen hyper-markov laws in the statistical analysis of decomposable graphical models. the annals of statistics dempster a. p. n. m. laird and d. b. rubin maximum likelihood from incomplete data via the em algorithm. j. of the royal statistical society series b cui y. x. z. fern and j. g. dy learning multiple nonredundant clusterings. acm transactions on knowledge discovery from data de freitas n. r. dearden f. hutter r. morales-menendez j. mutch and d. poole diagnosis by a waiter and a mars explorer. proc. ieee cukier k. february. data data everywhere. dagum p. and m. luby approximating probabilistic inference in bayesian belief networks is nphard. artificial intelligence dahl j. l. vandenberghe and v. roychowdhury august. covariance selection for non-chordal graphs via chordal embedding. optimization methods and software dahlhaus r. and m. eichler causality and graphical models for time series. in p. green n. hjort and s. richardson highly structured stochastic systems. oxford university press. dallal s. and w. hall approximating priors by mixtures of natural conjugate priors. j. of royal stat. soc. series b darwiche a. modeling and reasoning with bayesian networks. cambridge. de freitas n. m. niranjan and a. gee hierarchical bayesian models for regularisation in sequential learning. neural computation dechter r. bucket elimination a unifying framework for probabilistic inference. in uai. dechter r. constraint process ing. morgan kaufmann. decoste d. and b. schoelkopf training invariant support vector machines. machine learnng deerwester s. s. dumais g. furnas t. landauer and r. harshman indexing by latent semantic analysis. j. of the american society for information science degroot m. optimal statistical decisions. mcgraw-hill. deisenroth m. c. rasmussen and j. peters gaussian process dynamic programming. neurocomputing daume h. fast search for dirichlet process mixture models. in aistatistics. daume h. frustratingly easy domain adaptation. in proc. the assoc. for comp. ling. dawid a. p. applications of a general propagation algorithm for probabilistic expert systems. statistics and computing dawid a. p. influence diagrams for causal modelling and inference. intl. stat. review corrections dellaportas p. p. giudici and g. roberts bayesian inference for nondecomposable graphical gaussian models. sankhya ser. a dellaportas p. and a. f. m. smith bayesian inference for generalized linear and proportional hazards models via gibbs sampling. the royal statistical society. series c statistics j. of delyon lavielle b. m. and e. moulines convergence of a stochastic approximation version of the em algorithm. annals of statistics dawid a. p. beware of the dag! j. of machine learning research dempster a. covariance selec tion. biometrics denison d. c. holmes b. mallick and a. smith bayesian methods for nonlinear classification and regression. wiley. denison d. b. mallick and a. smith a bayesian cart algorithm. biometrika desjardins g. and y. bengio empirical evaluation of convolutional rbms for vision. technical report u. montreal. dey d. s. ghosh and b. mallick generalized linear models a bayesian perspective. chapman hallcrc biostatistics series. diaconis p. s. holmes and r. montgomery dynamical bias in the coin toss. siam review diaconis p. and d. ylvisaker in quantifying prior opinion. bayesian statistics dietterich t. g. and g. bakiri solving multiclass learning problems via ecocs. j. of ai research diggle p. and p. ribeiro model based geostatistics. springer. ding y. and r. harrison a sparse multinomial probit model for classification. pattern analysis and applications dobra a. dependency networks for genome-wide data. technical report u. washington. dobra a. and h. massam the mode oriented stochastic search algorithm for log-linear models with conjugate priors. statistical methodology domingos p. and d. lowd markov logic an interface layer for ai. morgan claypool. domingos p. and m. pazzani on the optimality of the simple bayesian classifier under zero-one loss. machine learning bibliography domke j. a. karapurkar and y. aloimonos who killed the directed model? in cvpr. duda r. o. p. e. hart and d. g. stork pattern classification. wiley interscience. edition. doucet a. n. de freitas and n. j. gordon sequential monte carlo methods in practice. springer verlag. doucet a. n. gordon and v. krishnamurthy particle filters for state estimation of jump markov linear systems. ieee trans. on signal processing dow j. and j. endersby multinomial probit and multinomial logit a comparison of choice models for voting research. electoral studies drineas p. a. frieze r. kannan s. vempala and v. vinay clustering large graphs via the singular value decomposition. machine learning drugowitsch j. bayesian linear regression. technical report u. rochester. druilhet p. and j.-m. marin invariant hpd credible sets and map estimators. bayesian analysis duane s. a. kennedy b. pendleton and d. roweth hybrid monte carlo. physics letters b duchi j. s. gould and d. koller projected subgradient methods for learning sparse gaussians. in uai. duchi for online learning j. e. hazan and y. singer adaptive subgradient methand ods stochastic optimization. in proc. of the workshop on computational learning theory. duchi j. s. shalev-shwartz y. singer and t. chandra efficient for projections onto the learning in high dimensions. in intl. conf. on machine learning. duchi j. and y. singer boosting with structural sparsity. in intl. conf. on machine learning. duchi j. d. tarlow g. elidan and d. koller using combinatorial optimization within maxproduct belief propagation. in nips. dumais s. and t. landauer a solution to plato s problem the latent semantic analysis theory of acquisition induction and representation of knowledge. psychological review elad m. and i. yavnch a plurality of sparse representations is better than the sparsest one alone. ieee trans. on info. theory elidan g. and s. gould learning bounded treewidth bayesian networks. j. of machine learning research dunson d. j. palomo and k. bollen bayesian structural equation modeling. technical report samsi. elidan g. n. lotner n. friedman and d. koller discovering hidden variables a structure-based approach. in nips. durbin j. and s. j. koopman time series analysis by state space methods. oxford university press. durbin r. s. eddy a. krogh and g. mitchison biological sequence analysis probabilistic models of proteins and nucleic acids. cambridge cambridge university press. earl d. and m. deem parallel tempering theory applications and new perspectives. phys. chem. chem. phys. eaton d. and k. murphy exact bayesian structure learning from uncertain interventions. in aistatistics. elidan g. i. mcgraw and d. koller residual belief gation informed scheduling for asynchronous message passing. in uai. elkan c. using the triangle inin equality to accelerate k-means. intl. conf. on machine learning. elkan c. deriving tf-idf as a fisher kernel. in proc. intl. symp. on string processing and information retrieval pp. elkan c. clustering documents with an exponential fmaily the dirichlet approximation of compoind multinomial model. in intl. conf. on machine learning. edakunni n. s. schaal and s. vijayakumar probabilistic incremental locally weighted learning using randomly varying coefficient model. technical report usc. ellis b. and w. h. wong learning causal bayesian network structures from experimental data. j. of the am. stat. assoc. edwards d. g. de abreu and r. labouriau selecting highdimensional mixed graphical models using minimal aic or bic forests. bmc bioinformatics efron b. why isn t everyone a bayesian? the american statistician efron b. large-scale inference empirical bayes methods for estimation testing and prediction. cambridge. efron b. i. johnstone t. hastie and r. tibshirani least angle regression. annals of statistics efron b. and c. morris data analysis using stein s estimator and its generalizations. j. of the am. stat. assoc. engel y. s. mannor and r. meir reinforcement learning in intl. with gaussian processes. conf. on machine learning. erhan d. y. bengio a. courville p.-a. manzagol p. vincent and s. bengio why does unsupervised pre-training help deep learning? j. of machine learning research erosheva s. e. fienberg joutard and c. describing disability through individual-level mixture models for multivariate binary data. annals of applied statistics. erosheva e. s. fienberg and j. lafferty mixed-membership models of scientific publications. proc. of the national academy of science usa bibliography escobar m. d. and m. west bayesian density estimation and inference using mixtures. j. of the am. stat. assoc. ewens w. population genetics theory the past and the future. in s.lessard mathemetical and statistica developments of evolutionary theory pp. reidel. fan j. and r. z. li variable selection via non-concave penalized likelihood and its oracle properties. j. of the am. stat. assoc. fearnhead p. exact bayesian curve fitting and signal segmentation. ieee trans. signal processing felzenszwalb p. and d. huttenlocher efficient belief propagation for early vision. intl. j. computer vision ferrucci d. e. brown j. chu-carroll j. fan d. gondek a. kalyanpur a. lally j. w. murdock e. n. amd j. prager n. schlaefter and c. welty building watson an overview of the deepqa project. ai magazine fienberg s. an iterative procedure for estimation in contingency tables. annals of mathematical statistics a figueiredo m. adaptive sparseness for supervised learning. ieee trans. on pattern analysis and machine intelligence figueiredo m. r. nowak and s. wright gradient projection for sparse reconstruction application to compressed sensing and other inverse problems. ieee. j. on selected topics in signal processing. figueiredo m. a. t. and a. k. jain unsupervised learning of finite mixture models. ieee trans. on pattern analysis and machine intelligence matlab code at httpwww.lx.it.pt mtfmixturecode.zip. fine s. y. singer and n. tishby the hierarchical hidden markov model analysis and applications. machine learning finkel j. and c. manning hierarchical bayesian domain adaptation. in proc. naacl pp. fischer b. and j. schumann autobayes a system for generating data analysis programs from statistical models. j. functional programming fishelson m. and d. geiger exact genetic linkage computations for general pedigrees. bmc bioinformatics fletcher r. on the barzilaiapplied opti borwein method. mization fokoue e. mixtures of factor analyzers an extension with covariates. j. multivariate analysis forbes j. t. huang k. kanazawa and s. russell the batmobile towards a bayesian automated taxi. in intl. joint conf. on ai. forsyth d. and j. ponce computer vision a modern approach. prentice hall. fraley c. and a. raftery model-based clustering discriminant analysis and density estimation. j. of the am. stat. assoc. fraley c. and a. raftery bayesian regularization for normal mixture estimation and modelbased clustering. j. of classification franc v. a. zien and b. schoelkopf support vector machines as probabilistic models. in intl. conf. on machine learning. frank i. and j. friedman a statistical view of some chemometrics regression tools. technometrics fraser a. hidden markov models and dynamical systems. siam press. freund y. and r. r. schapire experiments with a new boosting algorithm. in intl. conf. on machine learning. frey b. graphical models for machine learning and digital communication. mit press. frey b. extending factor graphs so as to unify directed and undirected graphical models. in uai. frey b. and d. dueck february. clustering by passing messages between data points. science a friedman j. multivariate adaptive regression splines. ann. statist. friedman j. on bias variance loss and the curse of dimensionality. j. data mining and knowledge discovery friedman j. greedy function approximation a gradient boosting machine. annals of statistics friedman j. t. hastie and r. tibshirani additive logistic regression a statistical view of boosting. annals of statistics friedman j. t. hastie and r. tibshirani sparse inverse covariance estimation the graphical lasso. biostatistics friedman j. t. hastie and r. tibshirani februrary. regularization paths for generalized linear models via coordinate descent. j. of statistical software friedman n. learning bayesian networks in the presence of missing values and hidden variables. in uai. friedman n. d. geiger and m. goldszmidt bayesian network classifiers. machine learning j. friedman n. d. geiger and n. lotner likelihood computation with value abstraction. in uai. friedman n. and d. koller being bayesian about network structure a bayesian approach to structure discovery in bayesian networks. machine learning friedman n. m. ninion i. pe er and t. pupko a structural em algorithm for phylogenetic inference. j. comp. bio. friedman n. and y. singer efficient bayesian parameter estimation in large discrete domains. in bibliography fruhwirth-schnatter s. finite mixture and markov switching models. springer. fruhwirth-schnatter s. and r. fruhwirth data augmentation and mcmc for binary and multinomial logit models. in t. kneib and g. tutz statistical modelling and regression structures pp. springer. fu w. penalized regressions the bridge verus the lasso. j. computational and graphical statistics. gelman a. j. carlin h. stern and d. rubin bayesian data analysis. chapman and hall. edition. gelman a. and j. hill data analysis using regression and multilevel hierarchical models. cambridge. gelman a. and x.-l. meng simulating normalizing constants from importance to bridge sampling to path sampling. statisical science sampling fukushima k. cognitron a self-organizing multilayered neural network. biological cybernetics gelman a. and t. raghunathan using conditional distributions for missing-data imputation. statistical science. fung r. and k. chang weighting and integrating evidence for stochastic simulation in bayesian networks. in uai. gelman a. and d. rubin inference from iterative simulation using multiple sequences. statistical science efficient gabow h. z. galil and t. spencer implementation of graph algorithms using contraction. in ieee symposium on the foundations of computer science. gales m. maximum likelihood multiple subspace projections for hidden markov models. ieee. trans. on speech and audio processing gales m. j. f. semi-tied covariance matrices for hidden markov models. ieee trans. on speech and audio processing gamerman d. efficient sampling from the posterior distribution in generalized linear mixed models. statistics and computing geiger d. and d. heckerman in learning gaussian networks. uai volume pp. geiger d. and d. heckerman a characterization of dirchlet distributions through local and global independence. annals of statistics gelfand a. model determination using sampling-based methods. in gilks richardson and spiegelhalter markov chain monte carlo in practice. chapman hall. geman s. e. bienenstock and r. doursat neural networks and the bias-variance dilemma. neural computing geman s. and d. geman stochastic relaxation gibbs distributions and the bayesian restoration of images. ieee trans. on pattern analysis and machine intelligence geoffrion a. lagrangian relaxation for integer programming. mathematical programming study george e. and d. foster calibration and empirical bayes variable selection. biometrika getoor l. and b. taskar introduction to relational statistical learning. mit press. geyer c. practical markov chain monte carlo. statistical science ghahramani z. and m. beal inference for bayesian in variational mixtures of factor analysers. ghahramani z. and m. beal propagation algorithms for variational bayesian learning. in gelfand a. and a. smith sampling-based approaches to calculating marginal densities. j. of the am. stat. assoc. ghahramani z. and g. hinton the em algorithm for mixtures of factor analyzers. technical report dept. of comp. sci. uni. toronto. ghahramani z. and g. hinton parameter estimation for linear dynamical systems. technical report dept. comp. sci. univ. toronto. ghahramani z. and m. jordan factorial hidden markov models. machine learning a moving gilks w. and c. berzuini following target monte carlo infernece for dynamic bayesian models. j. of royal stat. soc. series b gilks w. n. best and k. tan adaptive rejection metropolis sampling. applied statistics gilks w. and p. wild adaptive rejection sampling for gibbs sampling. applied statistics girolami m. b. calderhead and s. chin riemannian manifold hamiltonian monte carlo. j. of royal stat. soc. series b. to appear. girolami m. and s. rogers hierarchic bayesian models for kernel learning. in intl. conf. on machine learning pp. girolami m. and s. rogers variational bayesian multinomial probit regression with gaussian process priors. neural comptuation girshick r. p. felzenszwalb and d. mcallester object detection with grammar models. in nips. gittins j. multi-armed bandit allocation indices. wiley. giudici p. and p. green gausdetermination. decomposable sian model biometrika graphical givoni i. e. and b. j. frey june. a binary variable model for affinity propagation. neural computation globerson a. and t. jaakkola fixing max-product convergent message passing algorithms for map lp-relaxations. in nips. glorot x. and y. bengio may. understanding the difficulty of training deep feedforward neural networks. in aistatistics volume pp. bibliography gogate v. w. a. webb and p. dominlearning efficient gos markov networks. in nips. goldenberg a. a. x. zheng s. e. fienberg and e. m. airoldi a survey of statistical network models. foundations and trends in machine learning golub g. and c. f. van loan johns hop matrix computations. kins university press. gonen m. w. johnson y. lu and p. westfall august. the bayesian two-sample t test. the american statistician gonzales t. clustering to minimize the maximum intercluster distance. theor. comp. sci. gorder p. f. novdec. neural networks show new promise for machine vision. computing in science engineering gordon n. approach to nonlinearnon-gaussian bayesian state estimation. iee proceedings novel graepel t. j. quinonero-candela t. borchert and r. herbrich web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft a zs bing search engine. in intl. conf. on machine learning. grauman k. and t. darrell april. the pyramid match kernel efficient learning with sets of features. j. of machine learning research green p. reversible jump markov chain monte carlo computation and bayesian model determination. biometrika green p. tutorial on transdimensional mcmc. in p. green n. hjort and s. richardson highly structured stochastic systems. oup. greenshtein e. and j. park application of non parametric empirical bayes estimation to high dimensional classification. j. of machine learning research greig d. b. porteous and a. seheult exact maximum a posteriori estimation for binary images. j. of royal stat. soc. series b griffin j. and p. brown bayesian adaptive lassos with nonconvex penalization. technical report u. kent. griffin j. and p. brown inference with normal-gamma prior distributions in regression problems. bayesian analysis griffiths t. and j. tenenbaum theory-based causal induction. psychological review griffiths t. and m. steyvers finding scientific topics. proc. of the national academy of science usa griffiths t. m. steyvers d. blei and integrating j. tenenbaum topics and syntax. in nips. griffiths t. and j. tenenbaum using vocabulary knowledge in bayesian multinomial estimation. in nips pp. griffiths t. and j. tenenbaum structure and strength in causal induction. cognitive psychology grimmett g. and d. stirzaker probability and random processes. oxford. guan y. j. dy d. niu and z. ghahramani variational inference for nonparametric multiple clustering. in intl. workshop on discovering summarizing and using multiple clustering guedon y. estimating hidden semi-markov chains from discrete sequences. j. of computational and graphical statistics gustafsson m. bilistic derivation of least-squares algorithm. chemical ing a probathe partial journal of information and model guyon i. s. gunn m. nikravesh and l. zadeh feature extraction foundations and applications. springer. hacker j. and p. pierson winner-take-all how washington made the rich richer and turned its back on the middle class. simon schuster. politics halevy a. p. norvig and f. pereira the unreasonable effectiveness of data. ieee intelligent systems hall p. j. t. ormerod and m. p. wand theory of gaussian variational approximation for a generalised linear mixed model. statistica sinica hamilton j. analysis of time series subject to changes in regime. j. econometrics hans c. bayesian lasso regression. biometrika hansen m. and b. yu model selection and the principle of minimum description length. j. of the am. stat. assoc.. hara h. and a. takimura a localization approach to improve iterative proportional scaling in gaussian graphical models. communications in statistics theory and method. to appear. hardin j. and j. hilbe generalized estimating equations. chapman and hallcrc. harmeling s. and c. k. i. williams greedy learning of binary latent trees. ieee trans. on pattern analysis and machine intelligence harnard s. the symbol grounding problem. physica d green p. and b. silverman nonparametric regression and generalized linear models. chapman and hall. guo y. supervised exponential family principal component analysis via convex optimization. in nips. harvey a. c. forecasting structural time series models and the kalman filter. cambridge univerity press. bibliography hastie t. s. rosset r. tibshirani and j. zhu the entire regularization path for the support vector machine. j. of machine learning research hastie t. and r. tibshirani generalized additive models. chapman and hall. hastie t. r. tibshirani and j. friedman the elements of statistical learning. springer. hastie t. r. tibshirani and j. friedman the elements of statistical learning. springer. edition. hastings w. monte carlo sampling methods using markov chains applications. biometrika their and haykin s. neural networks a comprehensive foundation. prentice hall. edition. haykin s. kalman filter ing and neural networks. wiley. hazan t. and a. shashua convergent message-passing algorithms for inference over general graphs with convex free energy. in uai. hazan t. and a. shashua norm-product belief propagation primal-dual message passing for approximate inference. ieee trans. on info. theory he y.-b. and z. geng active learning of causal networks with intervention experiments and optimal designs. j. of machine learning research heaton m. and j. bayesian computation and the linear model. technical report duke. scott heckerman d. d. chickering and c. meek r. rounthwaite c. kadie dependency networks for density estimation collaborative filtering and data visualization. j. of machine learning research heckerman d. d. geiger and learning m. chickering bayesian networks the combination of knowledge and statistical data. machine learning heckerman d. c. meek february. and g. cooper a bayesian approach to causal discovery. technical report microsoft research. heckerman d. c. meek and d. koller probabilistic models for relational data. technical report microsoft research. heller k. and z. ghahramani bayesian hierarchical clustering. in intl. conf. on machine learning. henrion m. propagation of uncertainty by logic sampling in bayes networks. in uai pp. herbrich r. t. minka and t. graepel trueskill a bayesian skill rating system. in nips. hertz j. a. krogh and r. g. palmer an introduction to the theory of neural comptuation. addisonwesley. hillar c. j. sohl-dickstein and k. koepsell april. efficient and optimal binary hopfield associative memory storage using minimum probability flow. technical report. hinton g. products of experts. in proc. intl. conf. on artif. neural networks volume pp. hinton g. training products of experts by minimizing contrastive divergence. neural computation hinton g. a practical guide to training restricted boltzmann machines. technical report u. toronto. hinton g. and d. v. camp keeping neural networks simple by minimizing the description length of the weights. in in proc. of the ann. acm conf. on computational learning theory pp. acm press. hinton g. s. osindero and y. teh a fast learning algorithm for deep belief nets. neural computation hinton g. and r. salakhutdinov july. reducing the dimensionality of data with neural networks. science hinton g. e. p. dayan and m. revow modeling the manifolds of images of handwritten digits. ieee trans. on neural networks hinton g. e. and y. teh discovering multiple constraints that are frequently approximately satis n aed. in uai. hjort n. c. holmes p. muller and bayesian s. walker nonparametrics. cambridge. hoefling h. a path algorithm for the fused lasso signal approximator. technical report stanford. hoefling h. r. and tibshirani estimation of sparse binary pairwise markov networks using pseudo-likelihoods. j. of machine learning research hoeting j. d. madigan a. raftery and c. volinsky bayesian model averaging a tutorial. statistical science hoff p. d. a first course in bayesian statistical methods. springer. july. hoffman m. d. blei and f. bach online learning for latent dirichlet allocation. in nips. hoffman m. and a. gelman the no-u-turn sampler adaptively setting path lengths in hamiltonian monte carlo. technical report columbia u. hofmann t. probabilistic latent semantic indexing. research and development in information retrieval holmes c. and l. held bayesian auxiliary variable models for binary and multinomial regression. bayesian analysis honkela a. and h. valpola variational learning and bits-back coding an information-theoretic view to bayesian learning. ieee. trans. on neural networks honkela a. h. valpola and j. karhunen accelerating cyclic update algorithms for parameter estimation by pattern searches. neural processing letters bibliography hopfield j. j. april. neural networks and physical systems with emergent collective computational abilities. proc. of the national academy of science usa a hornik k. approximation capabilities of multilayer feedforward networks. neural networks a horvitz e. j. apacible r. sarin and l. liao prediction expectation and surprise methods designs and study of a deployed traffic forecasting service. in uai. howard r. and j. matheson influence diagrams. in r. howard and j. matheson readings on the principles and applications of decision analysis volume ii. strategic decisions group. hoyer p. non-negative matrix factorizaton with sparseness constraints. j. of machine learning research hsu c.-w. c.-c. chang and c.-j. lin a practical guide to support vector classification. technical report dept. comp. sci. national taiwan university. hu d. l. van der maaten y. cho l. saul and s. lerner latent variable models for predicting file dependencies in large-scale software development. in nips. hu m. c. ingram m.sirski c. pal s. swamy and c. patten a hierarchical hmm implementation for vertebrate gene splice site prediction. technical report dept. computer science univ. waterloo. huang j. q. morris and b. frey bayesian inference of microrna targets from sequence and expression data. j. comp. bio.. hubel d. and t. wiesel receptive fields binocular itneraction and functional architecture in the cat s visual cortex. j. physiology huber p. robust estimation of a location parameter. annals of statistics a hubert l. and p. arabie comj. of classifica paring partitions. tion hunter d. and r. li variable selection using mm algorithms. annals of statistics jacob l. f. bach and j.-p. vert a clustered multi-task learning convex formulation. in nips. hunter d. r. and k. lange a tutorial on mm algorithms. the american statistician jain a. and r. dubes algorithms for clustering data. prentice hall. hyafil l. and r. rivest constructing optimal binary decision trees is np-complete. information processing letters james g. and t. hastie the error coding method and picts. j. of computational and graphical statistics hyvarinen a. j. hurri and p. hoyer natural image statistics a probabilistic approach to early computational vision. springer. hyvarinen a. and e. oja independent component analysis algorithms and applications. neural networks ilin a. and t. raiko practical approaches to principal component analysis in the presence of missing values. j. of machine learning research insua d. r. and f. ruggeri robust bayesian analysis. springer. isard m. pampas real-valued graphical models for computer vision. in cvpr volume pp. isard m. and a. blake condensation conditional density propagation for visual tracking. intl. j. of computer vision jaakkola t. tutorial on variational approximation methods. in m. opper and d. saad advanced mean field methods. mit press. jaakkola t. and d. haussler exploiting generative models in discriminative classifiers. in nips pp. jaakkola t. and m. upper computing and bounds on likelihoods tractable networks. in uai. jordan lower in in jaakkola t. and m. jordan a variational approach to bayesian logistic regression problems and their extensions. in ai statistics. japkowicz n. s. hanson and m. gluck nonlinear autoassociation is not equivalent to pca. neural computation jaynes e. t. probability theory the logic of science. cambridge university press. jebara t. r. kondor and a. howard probability product kernels. j. of machine learning research jeffreys h. theory of probability. oxford. jelinek f. statistical methods for speech recognition. mit press. jensen c. s. a. kong and u. kjaerulff blocking-gibbs sampling in very large probabilistic expert systems. intl. j. human-computer studies jermyn i. invariant bayesian estimation on manifolds. annals of statistics jerrum m. and a. sinclair polynomial-time approximation algorithms for the ising model. siam j. on computing jerrum m. and a. sinclair the markov chain monte carlo method an approach to approximate counting and integration. in d. s. hochbaum approximation algorithms for np-hard problems. pws publishing. jerrum m. a. sinclair and e. vigoda a polynomial-time approximation algorithm for the permanent of a matrix with non-negative entries. journal of the acm jaakkola t. s. and m. i. jordan bayesian parameter estimation via variational methods. statistics and computing ji and l. carin s. d. dunson multi-task compressive sensing. ieee trans. signal processing bibliography ji s. l. tang s. yu and j. ye a shared-subspace learning framework for multi-label classification. acm trans. on knowledge discovery from data jirousek r. and s. preucil on the effective implementation of the iterative proportional fitting procedure. computational statistics data analysis joachims t. training linear svms in linear time. in proc. of the int l conf. on knowledge discovery and data mining. joachims t. t. finley and c.-n. yu cutting-plane training of structural svms. machine learning johnson j. k. d. m. malioutov and a. s. willsky walk-sum interpretation and analysis of gaussian belief propagation. in nips pp. johnson m. capacity and complexity of hmm duration modeling techniques. signal processing letters johnson n. a study of the nips feature selection challenge. technical report stanford. johnson v. and j. albert ordi nal data modeling. springer. jones b. a. dobra c. carvalho c. hans c. carter and m. west experiments in stochastic computation for high-dimensional graphical models. statistical science jordan m. i. an introduction to in probabilistic graphical models. preparation. jordan m. i. the era of big in isba bulletin volume data. pp. jordan m. i. z. ghahramani t. s. jaakkola and l. k. saul an introduction to variational methods for graphical models. in m. jordan learning in graphical models. mit press. journee m. y. nesterov p. richtarik and r. sepulchre generalized power method for sparse principal components analysis. j. of machine learning research julier s. and j. uhlmann a new extension of the kalman filter to nonlinear systems. in proc. of aerosense the intl. symp. on aerospacedefence sensing simulation and controls. jurafsky d. and j. h. martin speech and language processing an introduction to natural language processing computational linguistics and speech recognition. prentice-hall. jurafsky d. and j. h. martin speech and language processing an introduction to natural language processing computational linguistics and speech recognition. prentice-hall. edition. kaariainen m. and j. langford a comparison of tight generalization bounds. in intl. conf. on machine learning. kaelbling l. m. a. moore learning a survey. search and littman reinforcement j. of ai re kaelbling l. p. m. littman and planning a. cassandra and acting in partially observable stochastic domains. artificial intelligence kaiser h. the varimax criterion for analytic rotation in factor analysis. psychometrika kakade s. y. w. teh and s. roweis an alternate objective function for markovian fields. in intl. conf. on machine learning. kanazawa k. d. koller and s. russell stochastic simulation algorithms for dynamic probabilistic networks. in uai. kandel e. j. schwarts and t. jessell principles of neural science. mcgraw-hill. jordan m. i. and r. a. jacobs hierarchical mixtures of experts and the em algorithm. neural computation kappen h. and f. rodriguez boltzmann machine learning using mean field theory and linear response correction. in nips. karhunen j. and j. joutsensalo generalizations of principal component analysis optimization problems and neural networks. neural networks kass r. and l. wasserman a reference bayesian test for nested hypotheses and its relationship to the schwarz criterio. j. of the am. stat. assoc. katayama t. subspace methods for systems identification. springer verlag. kaufman l. and p. rousseeuw finding groups in data an introduction to cluster analysis. wiley. kawakatsu h. and a. largey em algorithms for ordered probit models with endogenous regressors. the econometrics journal kearns m. j. and u. v. vazirani an introduction to computational learning theory. mit press. kelley j. e. the cutting-plane method for solving convex programs. j. of the soc. for industrial and applied math. kemp c. j. tenenbaum s. niyogi and t. griffiths a probabilistic model of theory formation. cognition kemp c. j. tenenbaum t. y. t. griffiths and and n. ueda learning systems of concepts with an infinite relational model. in aaai. kersting k. s. natarajan and d. poole statistical relational ai logic probability and computation. technical report ubc. khan m. e. b. marlin g. bouchard and k. p. murphy variational bounds for mixed-data factor analysis. in nips. khan z. t. balch and f. dellaert mcmc data association and sparse factorization updating for real time multitarget tracking with merged and multiple measurements. ieee trans. on pattern analysis and machine intelligence kirkpatrick s. c. g. jr. and m. vecchi optimization by simulated annealing. science bibliography kitagawa g. the two-filter formula for smoothing and an implementation of the gaussian-sum smoother. annals of the institute of statistical mathematics koller d. and u. lerner sampling in factored dynamic systems. in a. doucet n. de freitas and n. gordon sequential monte carlo methods in practice. springer. kjaerulff u. triangulation of graphs algorithms giving small total state space. technical report dept. of math. and comp. sci. aalborg univ. denmark. kjaerulff u. and a. madsen bayesian networks and influence diagrams a guide to construction and analysis. springer. klaassen c. and j. a. wellner efficient estimation in the bivariate noramal copula model normal margins are least favorable. bernoulli klami a. and s. kaski probabilistic approach to detecting dependencies between data sets. neurocomputing klami a. s. virtanen and s. kaski bayesian exponential family projections for coupled data sources. in uai. kleiner a. a. talwalkar p. sarkar and m. i. jordan a scalable bootstrap for massive data. technical report uc berkeley. kneser r. and h. ney improved backing-off for n-gram language modeling. in intl. conf. on acoustics speech and signal proc. volume pp. ko j. and d. fox gpbayesfilters bayesian filtering using gaussian process prediction and observation models. autonomous robots journal. kohn r. m. smith and d. chan nonparametric regression using linear combinations of basis functions. statistical computing koivisto m. advances in exact bayesian structure discovery in bayesian networks. in uai. koivisto m. and k. sood exact bayesian structure discovery in bayesian networks. j. of machine learning research kolmogorov v. october. convergent tree-reweighted message passing for energy minimization. ieee trans. on pattern analysis and machine intelligence kolmogorov v. and m. wainwright on optimality properties of tree-reweighted message passing. in uai pp. kolmogorov v. and r. zabin what energy functions can be minimized via graph cuts? ieee trans. on pattern analysis and machine intelligence komodakis n. n. paragios and g. tziritas mrf energy minimization and beyond via dual decomposition. ieee trans. on pattern analysis and machine intelligence koo t. a. m. rush m. collins t. jaakkola and d. sontag dual decomposition for parsing with non-projective head automata. in proc. emnlp pp. a koren y. the bellkor solution to the netflix grand prize. technical report yahoo! research. koren y. collaborative filtering with temporal dynamics. in proc. of the int l conf. on knowledge discovery and data mining. koren y. r. bell and c. volinsky matrix factorization techniques for recommender systems. ieee computer krishnapuram l. b. carin m. figueiredo and a. hartemink learning sparse bayesian classifiers multi-class formulation fast algorithms and generalization bounds. ieee transaction on pattern analysis and machine intelligence. kschischang f. b. frey and h.-a. loeliger february. factor graphs and the sum-product algorithm. ieee trans info. theory. kuan p. g. pan j. a. thomson r. stewart and s. keles a hierarchical semi-markov model for detecting enrichment with application to chip-seq experiments. technical report u. wisconsin. kulesza a. and b. taskar learning determinantal point processes. in uai. kumar n. and a. andreo heteroscedastic discriminant analysis and reduced rank hmms for improved speech recognition. speech communication kumar s. and m. hebert discriminative random fields a discriminative framework for contextual interaction in classification. in intl. conf. on computer vision. kuo l. and b. mallick variable selection for regression models. sankhya series b kurihara k. m. welling and n. vlassis accelerated variational dp mixture models. in nips. kushner h. and g. yin stochastic approximation and recursive algorithms and applications. springer. kuss and c. rasmussen assessing approximate inference for binary gaussian process classification. j. of machine learning research kwon j. and k. murphy modeling freeway traffic with coupled hmms. technical report univ. california berkeley. kyung m. j. gill m. ghosh and g. casella penalized regression standard errors and bayesian lassos. bayesian analysis lacoste-julien s. f. huszar and z. ghahramani approximate inference for the loss-calibrated bayesian. in aistatistics. koller d. and n. friedman probabilistic graphical models principles and techniques. mit press. krizhevsky a. and g. hinton using very deep autoencoders for content-based image retrieval. submitted. lacoste-julien s. f. sha and m. i. jordan disclda discriminative learning for dimensionality reduction and classification. in nips. bibliography lafferty j. a. mccallum and f. pereira conditional random fields probabilistic models for segmenting and labeling sequence data. in intl. conf. on machine learning. lange k. r. little and j. taylor robust statistical modeling using the t disribution. j. of the am. stat. assoc. langville a. and c. meyer updating markov chains with an eye on google s pagerank. siam j. on matrix analysis and applications larranaga p. c. m. h. kuijpers m. poza and r. h. murga decomposing bayesian networks triangulation of the moral graph with genetic algorithms. statistics and computing lashkari d. and p. golland convex clustering with examplarbased models. in nips. lasserre j. c. bishop and t. minka principled hybrids of generative and discriminative models. in cvpr. lau j. and p. green bayesian model-based clustering procedures. journal of computational and graphical statistics lauritzen s. graphical models. oup. lauritzen s. causal inference from graphical models. in d. r. c. o. e. barndoff-nielsen and c. klueppelberg complex stochastic systems. chapman and hall. lauritzen s. and d. nilsson representing and solving decision problems with limited information. management science lauritzen s. l. december. propagation of probabilities means and variances in mixed graphical association models. j. of the am. stat. assoc. lauritzen s. l. and d. j. spiegelhalter local computations with probabilities on graphical structures and their applications to expert systems. j. r. stat. soc. b law e. b. settles and t. mitchell learning to tag from open vocabulary labels. in proc. european conf. on machine learning. law m. m. figueiredo and a. jain simultaneous feature selection and clustering using mixture models. ieee trans. on pattern analysis and machine intelligence lawrence n. d. probabilistic non-linear principal component analysis with gaussian process latent variable models. j. of machine learning research lawrence n. d. a unifying probabilistic perspective for specintral dimensionality reduction sights and new models. j. of machine learning research learned-miller e. hyperspacings and the estimation of information theoretic quantities. technical report u. mass. amherst comp. sci. dept. lecun y. b. boser j. s. denker d. henderson r. e. howard w. hubbard and l. d. jackel winter. backpropagation applied to handwritten zip code recognition. neural computation lecun y. l. bottou y. bengio and p. haffner november. gradient-based learning applied to document recognition. proceedings of the ieee lecun y. s. chopra r. hadsell f.-j. huang and m.-a. ranzato a tutorial on energy-based learning. predicting structured outputs. mit press. in b. et al. ledoit o. and m. wolf honey i shrunk the sample covariance matrix. j. of portfolio management lee a. f. caron a. doucet and c. holmes a hierarchical bayesian framework for constructing sparsity-inducing priors. technical report u. oxford. lee a. f. caron a. doucet and c. holmes bayesian sparsitypath-analysis of genetic association signal using generalized t prior. technical report u. oxford. lee d. and s. seung algorithms for non-negative matrix factorization. in nips. lee h. r. grosse r. ranganath and a. ng convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. in intl. conf. on machine learning. lee h. y. largman p. pham and a. ng unsupervised feature learning for audio classification using convolutional deep belief networks. in nips. lee s.-i. v. ganapathi and d. koller efficient structure learning of markov networks using in nips. lee t. s. and d. mumford hierarchical bayesian inference in the visual cortex. j. of optical society of america a lenk p. w. s. desarbo p. green and m. young hierarchical bayes conjoint analysis recovery of partworth heterogeneity from reduced experimental designs. marketing science lenkoski a. and a. dobra bayesian structural learning and estimation in gaussian graphical models. technical report department of statistics university of washington. of comparison lepar v. and p. p. shenoy a lauritzenspiegelhalter hugin and shenoyshafer architectures for computing marginals of probability distributions. in g. cooper and s. moral uai pp. morgan kaufmann. lauritzen s. l. the em algorithm for graphical association models with missing data. computational statistics and data analysis ledoit o. and m. wolf a wellconditioned estimator largedimensional covariance matrices. j. of multivariate analysis for lerner u. and r. parr inference in hybrid networks theoretical limits and practical algorithms. in uai. bibliography leslie c. e. eskin a. cohen j. weston and w. noble mismatch string kernels for discriminative protein classification. bioinformatics levy s. in the plex how google thinks works and shapes our lives. simon schuster. li l. w. chu j. langford and x. wang unbiased offline evaluation of contextual-banditbased news article recommendation algorithms. in wsdm. liang f. s. mukherjee and m. west understanding the use of unlabelled data in predictive modelling. statistical science liang f. r. paulo g. molina m. clyde and j. berger mixtures of g-priors for bayesian variable selection. j. of the am. stat. assoc. i. liang p. and m. jordan an asymptotic analysis of generative discriminative and pseudolikelihood estimators. in international conference on machine learning liang p. and d. klein. online em in proc. for unsupervised models. naacl conference. liao l. d. j. patterson d. fox and h. kautz learning and inferring transportation routines. artificial intelligence lindley d. scoring rules and isi the inevetability of probability. review lindley d. v. bayesian statistics a review. siam. lindley d. v. and l. d. phillips inference for a bernoulli process bayesian view. the american statistician lindsay b. composite likecontemporary lihood methods. mathematics lipton r. j. and r. e. tarjan theorem for planar siam journal of applied a separator graphs. math little. r. j. and d. b. rubin statistical analysis with missing data. new york wiley and son. liu c. and d. rubin ml estimation of the t distribution using em and its extensions ecm and ecme. statistica sinica liu h. j. lafferty and l. wasserman the nonparanormal semiparametric estimation of high dimensional undirected graphs. j. of machine learning research liu j. monte carlo strategies in scientific computation. springer. liu j. s. w. h. wong and a. kong covariance structure of the gibbs sampler with applications to the comparisons of estimators and augmentation schemes. biometrika liu t.-y. learning to rank for information retrieval. foundations and trends in information retrieval lizotte d. practical bayesian optimization. ph.d. thesis u. alberta. ljung l. system identificiation theory for the user. prentice hall. lo c. h. statistical methods for high throughput genomics. ph.d. thesis ubc. lo k. f. hahne r. brinkman r. ryan and r. gottardo may. flowclust a bioconductor package for automated gating of flow cytometry data. bmc bioinformatics lopes h. and m. west bayesian model assessment in factor analysis. statisica sinica lowe d. g. object recognition from local scale-invariant features. in proc. of the international conference on computer vision iccv corfu pp. luce r. individual choice behavior a theoretical analysis. wiley. lunn d. n. best and j. whittaker generic reversible jump mcmc using graphical models. statistics and computing lunn d. a. thomas n. best and d. spiegelhalter winbugs a bayesian modelling framework concepts structure and extensibility. statistics and computing ma h. h. yang m. lyu and i. king sorec social recommendation using probabilistic matrix factorization. in proc. of conf. on information and knowledge management. ma s. c. ji and j. farmer an efficient em-based training algorithm for feedforward neural networks. neural networks maathuis m. d. colombo m. kalisch and p. b ijhlmann predicting causal effects in large-scale systems from observational data. nature methods maathuis m. m. kalisch and p. b ijhlmann estimating high-dimensional intervention effects from observational data. annals of statistics mackay d. bayesian interpolation. neural computation mackay d. developments in probabilistic modeling with neural networks ensemble learning. in proc. ann. symp. neural networks. mackay d. probable networks and plausible predictions a review of practical bayesian methods for supervised neural networks. network. mackay d. ensemble learning for hidden markov models. technical report u. cambridge. mackay d. comparision of approximate methods for handling hyperparameters. neural computation mackay d. information theory inference and learning algorithms. cambridge university press. macnaughton-smith p. w. t. williams m. b. dale and g. mockett dissimilarity analysis a new technique of hierarchical sub-division. nature bibliography madeira s. c. and a. l. oliveira biclustering algorithms for biological data analysis a survey. ieeeacm transactions on computational biology and bioinformatics madigan d. and a. raftery model selection and accounting for model uncertainty in graphical models using occam s window. j. of the am. stat. assoc. madsen r. d. kauchak and c. elkan modeling word burstiness using the dirichlet distribution. in intl. conf. on machine learning. mairal j. f. bach j. ponce and g. sapiro online learning for matrix factorization and sparse coding. j. of machine learning research mairal j. m. elad and g. sapiro sparse representation for color image restoration. ieee trans. on image processing malioutov d. j. johnson and a. willsky walk-sums and belief propagation in gaussian graphical models. j. of machine learning research july. mallat s. g. davis and z. zhang adaptive frequency decompositions. spie journal of optical engineering mallat s. and z. zhang matching pursuits with time-frequency dictionaries. ieee transactions on signal processing malouf r. a comparison of algorithms for maximum entropy parameter estimation. in proc. sixth conference on natural language learning pp. manning c. p. raghavan and h. schuetze introduction to information retrieval. cambridge university press. manning c. and h. schuetze statistical natural foundations of language processing. mit press. mansinghka v. d. roy r. rifkin and j. tenenbaum aclass an online algorithm for generative classification. in aistatistics. mansinghka v. p. shafto e. cross-categorization jonas c. petschulat and j. tenenbaum a nonparametric bayesian method for modeling heterogeneous high dimensional data. technical report mit. margolin a. i. nemenman k. basso c. wiggins g. stolovitzky and r. f. abd a. califano aracne an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context. bmc bionformatics marin j.-m. and c. robert bayesian core a practical approach to computational bayesian statistics. springer. marks t. k. and j. r. movellan diffusion networks products of experts and factor analysis. technical report university of california san diego. marlin b. modeling user rating profiles for collaborative filtering. in nips. marlin b. missing data problems in machine learning. ph.d. thesis u. toronto. marlin b. e. khan and k. murphy piecewise bounds for estimating bernoulli-logistic latent gaussian models. in intl. conf. on machine learning. marlin b. and r. zemel collaborative prediction and ranking with non-random missing data. in proc. of the acm conference on recommender systems. marlin b. m. k. swersky b. chen and n. de freitas inductive principles for restricted boltzmann machine learning. in aistatistics. marroquin j. s. mitter and t. poggio probabilistic solution of ill-posed problems in computational vision. j. of the am. stat. assoc. martens j. deep learning via in intl. hessian-free optimization. conf. on machine learning. maruyama y. and e. george a g-prior extension for p n. technical report u. tokyo. mason l. j. baxter p. bartlett and boosting algoin m. frean rithms as gradient descent. nips volume pp. matthews r. bayesian critique of statistics in health the great health hoax. maybeck p. stochastic models estimation and control. academic press. mazumder r. and t. hastie the graphical lasso new insights and alternatives. technical report. mcauliffe j. d. blei and m. jordan nonparametric empirical bayes for the dirichlet process mixture model. statistics and computing mccallum a. efficiently inducing features of conditional random fields. in uai. mccallum freitag a. d. and f. pereira maximum entropy markov models for information extraction and segmentation. in intl. conf. on machine learning. mccallum a. and k. nigam a comparison of event models for naive bayes text classification. in aaaiicml workshop on learning for text categorization. mccray a. an upper level ontology for the biomedical domain. comparative and functional genomics mccullagh p. and j. nelder generalized linear models. chapman and hall. edition. mccullich w. and w. pitts a logical calculus of the ideas immanent in nervous activity. bulletin of mathematical biophysics mcdonald j. and w. newey partially adaptive estimation of regression models via the generalized t distribution. econometric theory mceliece r. j. d. j. c. mackay and j. f. cheng turbo decoding as an instance of pearl s belief propagation algorithm. ieee j. on selected areas in comm. mcfadden d. conditional logit analysis of qualitative choice behavior. in p. zarembka frontiers in econometrics pp. academic press. bibliography mcgrayne s. b. the theory that would not die how bayes rule cracked the enigma code hunted down russian submarines and emerged triumphant from two centuries of controversy. yale university press. mckay b. d. f. e. oggier g. f. royle n. j. a. sloane i. m. wanless and h. s. wilf acyclic digraphs and eigenvalues of j. integer sequences mckay d. and l. c. b. peto language a hierarchical dirichlet model. natural language engineering mclachlan g. j. and t. krishnan the em algorithm and extensions. wiley. meek c. and d. heckerman learnindependence and in uai structure and parameter ing for causal causal interaction models. pp. meek c. b. thiesson and d. heckerman staged mixture modelling and boosting. in uai san francisco ca pp. morgan kaufmann. meila m. a random walks view in ais of spectral segmentation. tatistics. meila m. comparing clusterin intl. ings an axiomatic view. conf. on machine learning. meila m. and t. jaakkola tractable bayesian learning of tree belief networks. statistics and computing i. meila m. and m. jordan learning with mixtures of trees. j. of machine learning research meinshausen n. a note on the lasso for gaussian graphical model selection. technical report eth seminar fur statistik. meinshausen n. and p. buhlmann high dimensional graphs and variable selection with the lasso. the annals of statistics globally optimal meltzer t. c. yanover and y. weiss solutions for energy minimization in stereo vision using reweighted belief propagation. in iccv pp. meng x. l. and d. van dyk the em algorithm an old folk song sung to a fast new tune discussion. j. royal stat. soc. b mesot b. and d. barber a simple alternative derivation of the expectation correction algorithm. ieee signal processing letters metropolis a. n. rosenbluth m. rosenbluth a. teller and e. teller equation of state calculations by fast computing machines. j. of chemical physics metz c. google behavioral ad targeter is a smart ass. the register. miller a. subset selection in regression. chapman and hall. edition. mimno d. and a. mccallum topic models conditioned on arbitrary features with dirichletmultinomial regression. in uai. minka t. pathologies of orthodox statisics. technical report mit media lab. minka t. automatical choice of dimensionality for pca. technical report mit. minka t. bayesian linear re gression. technical report mit. minka t. bayesian model averaging is not model combination. technical report mit media lab. minka t. empirical risk minimization is an incomplete inductive principle. technical report mit. minka t. estimating a dirichlet distribution. technical report mit. minka t. inferring a gaussian distribution. technical report mit. meinshausen n. and p. b ijhlmann stability selection. j. of royal stat. soc. series b minka t. bayesian inference of a uniform distribution. technical report mit. minka t. empirical risk minimization is an incomplete inductive principle. technical report mit. minka t. expectation propagation for approximate bayesian inference. in uai. minka t. a family of algorithms for approximate bayesian inference. ph.d. thesis mit. minka t. statistical approaches to learning and discovery homework assignment question technical report cmu. minka t. a comparison of numerical optimizers for logistic regression. technical report msr. minka t. divergence measures and message passing. technical report msr cambridge. minka t. and y. qi treestructured approximations by expectation propagation. in nips. minka t. j. winn d. knowles microsoft httpresearch.microsoft.cominfernet. j. guiver and infer.net research cambridge. minsky m. and s. papert per ceptrons. mit press. mitchell t. machine learning. mcgraw hill. mitchell t. and j. beauchamp bayesian variable selection in linear regression. j. of the am. stat. assoc. mobahi h. r. collobert and j. weston deep learning from temporal coherence in video. in intl. conf. on machine learning. mockus j. w. eddy a. mockus l. mockus and g. reklaitis bayesian heuristic approach to discrete and global optimization algorithms visualization software and applications. kluwer. moghaddam b. a. gruber y. weiss and s. avidan sparse regression as a sparse eigenvalue problem. in information theory applications workshop moghaddam b. b. marlin e. khan and k. murphy accelerating bayesian structural inference for non-decomposable gaussian graphical models. in nips. bibliography moghaddam b. and a. pentland probabilistic visual learning for object detection. in intl. conf. on computer vision. mohamed s. and z. ghahramani bayesian exponential family pca. in nips. k. heller moler c. numerical computing with matlab. siam. morris r. d. x. descombes and j. zerubia the isingpotts model is not well suited to segmentation tasks. in ieee dsp workshop. mosterman p. j. and g. biswas diagnosis of continuous valued systems in transient operating regions. ieee trans. on systems man and cybernetics part a moulines e. j.-f. cardoso and e. gassiat maximum likelihood for blind separation and deconvolution of noisy signals using mixture models. ieee int. conf. on acoustics speech and signal processing munich germany pp. in proc. muller p. g. parmigiani c. robert and j. rousseau optimal sample size for multiple testing the case of gene expression microarrays. j. of the am. stat. assoc. mumford d. neuronal architectures for pattern-theoretic problems. in c. koch and j. davis large scale neuronal theories of the brain. mit press. murphy k. bayesian map learning in dynamic environments. in nips volume murphy k. and m. paskin linear time inference in hierarchical hmms. in nips. murphy k. y. weiss and m. jordan loopy belief propagation for approximate inference an empirical study. in uai. murphy k. p. filtering and smoothing in linear dynamical systems using the junction tree algorithm. technical report u.c. berkeley dept. comp. sci. murray i. and z. ghahramani a note on the evidence and bayesian occam s razor. technical report gatsby. musso c. n. oudjane and f. legland improving regularized ticle filters. in a. doucet j. f. g. de freitas and n. gordon sequential monte carlo methods in practice. springer. nabney i. netlab algorithms for pattern recognition. springer. neal r. connectionist learning of belief networks. artificial intelligence neal r. probabilistic inference using markov chain monte carlo methods. technical report univ. toronto. neal r. bayesian learning for neural networks. springer. neal r. monte carlo implementation of gaussian process models for bayesian regression and classification. technical report u. toronto. neal r. erroneous results in marginal likelihood from the gibbs output technical report u. toronto. neal r. markov chain sampling methods for dirichlet process mixture models. j. of computational and graphical statistics neal r. slice sampling. an nals of statistics neal r. mcmc using hamilin s. brooks tonian dynamics. a. gelman g. jones and x.-l. meng handbook of markov chain monte carlo. chapman hall. neal r. and d. mackay likelihood-based boosting. technical report u. toronto. neal r. and j. zhang high dimensional classification bayesian neural networks and dirichlet diffusion trees. in i. guyon s. gunn m. nikravesh and l. zadeh feature extraction. springer. neal r. m. annealed importance sampling. statistics and computing neal r. m. density modeling and clustering using dirichlet diffusion trees. in j. m. bernardo et al. bayesian statistics pp. oxford university press. neal r. m. and g. e. hinton a new view of the em algorithm that justifies incremental and other variants. in m. jordan learning in graphical models. mit press. neapolitan r. bayesian networks. prentice hall. learning nefian a. l. liang x. pi x. liu and k. murphy dynamic for audiobayesian networks visual speech recognition. j. applied signal processing. nemirovski a. and d. yudin on cezari s convergence of the steepest descent method for approximating saddle points of convexconcave functions. soviet math. dokl. nesterov y. introductory lectures on convex optimization. a basic course. kluwer. newton m. d. noueiry d. sarkar and p. ahlquist detecting differential gene expression with a semiparametric hierarchical mixture method. biostatistics newton m. and a. raftery approximate bayesian inference with the weighted likelihood bootstrap. j. of royal stat. soc. series b ng a. m. jordan and y. weiss on spectral clustering analysis and an algorithm. in nips. ng a. y. and m. i. jordan on discriminative vs. generative classifiers a comparison of logistic regression and naive bayes. in nickisch h. and c. rasmussen approximations for binary gaussian process classification. j. of machine learning research nilsson d. an efficient algorithm for finding the m most probable configurations in a probabilistic expert system. statistics and computing nilsson d. and j. goldberger sequentially finding the n-best list in hidden markov models. in intl. joint conf. on ai pp. nocedal j. and s. wright nu merical optimization. springer. bibliography nowicki k. and t. a. b. snijders estimation and prediction for stochastic blockstructures. journal of the american statistical association nowlan s. and g. hinton simplifying neural networks by soft weight sharing. neural computation nummiaro k. e. koller-meier and an adaptive l. v. gool color-based particle filter. image and vision computing obozinski g. b. taskar and m. i. jordan joint covariate selection for grouped classification. technical report uc berkeley. oh m.-s. and j. berger adaptive importance sampling in monte carlo integration. j. of statistical computation and simulation oh s. s. russell and s. sastry markov chain monte carlo data association for multi-target tracking. ieee trans. on automatic control o hagan a. curve fitting and optimal design for prediction. j. of royal stat. soc. series b o hara r. and m. sillanpaa a review of bayesian variable selection methods what how and which. bayesian analysis olshausen b. a. and d. j. field emergence of simple cell receptive field properties by learning a sparse code for natural images. nature opper m. a bayesian approach to online learning. in d. saad on-line learning in neural networks. cambridge. opper m. and c. archambeau the variational gaussian approximation revisited. neural computation opper m. and d. saad advanced mean field methods theory and practice. mit press. osborne m. r. b. presnell and b. a. turlach a new approach to variable selection in least squares problems. ima journal of numerical analysis osborne m. r. b. presnell and b. a. turlach on the lasso and its dual. j. computational and graphical statistics ostendorf m. v. digalakis and o. kimball from hmms to segment models a unified view of stochastic modeling for speech recognition. ieee trans. on speech and audio processing overschee p. v. and b. d. moor subspace identification for linear systems theory implementation applications. kluwer academic publishers. paatero p. and u. tapper positive matrix factorization a nonnegative factor model with optimal utilization of error estimates of data values. environmetrics padadimitriou c. and k. steiglitz combinatorial optimization algorithms and complexity. prentice hall. paisley j. and l. carin nonparametric factor analysis with beta process priors. in intl. conf. on machine learning. palmer s. vision science photons to phenomenology. mit press. parise s. and m. welling random learning fields an empirical study. in joint statistical meeting. in markov park t. and g. casella the j. of the am. stat. bayesian lasso. assoc. parviainen p. and m. koivisto ancestor relations in the presence of unobserved variables. in proc. european conf. on machine learning. paskin m. thin junction tree filters for simultaneous localization and mapping. in intl. joint conf. on ai. pearl j. probabilistic reasoning in intelligent systems networks of plausible inference. morgan kaufmann. pearl j. and t. verma a theory of inferred causation. in knowledge representation pp. pe er d. april. bayesian network analysis of signaling networks a primer. science stke peng f. r. jacobs and m. tanner bayesian inference in mixtures-of-experts and hierarchical mixtures-of-experts models with an application to speech recognition. j. of the am. stat. assoc. petris g. s. petrone and p. campagnoli dynamic linear models with r. springer. pham d.-t. and p. garrat blind separation of mixture of independent sources through a quasiapproach. maximum likelihood ieee trans. on signal processing pietra s. d. v. d. pietra and j. lafferty inducing features of random fields. ieee trans. on pattern analysis and machine intelligence plackett r. the analysis of permutations. applied stat. platt j. using analytic qp and sparseness to speed training of support vector machines. in nips. platt j. probabilities for sv main a. smola p. bartlett chines. b. schoelkopf and d. schuurmans advances in large margin classifiers. mit press. platt j. n. cristianini and j. shawetaylor large margin dags for multiclass classification. in nips volume pp. plummer m. jags a program for analysis of bayesian graphical models using gibbs sampling. in proc. intl. workshop on distributed statistical computing. polson n. and s. scott data augmentation for support vector machines. bayesian analysis pontil m. s. mukherjee and f. girosi on the noise model of support vector machine regression. technical report mit ai lab. pearl j. causality models reasoning and inference. cambridge univ. press. poon h. and p. domingos sumproduct networks a new deep architecture. in uai. bibliography pourahmadi m. simultaneous modelling of covariance matrices glm bayesian and nonparametric perspectives. technical report northern illinois university. prado r. and m. west time series modelling computation and inference. crc press. press s. j. applied multivariate analysis using bayesian and frequentist methods of inference. dover. second edition. press w. w. vetterling s. teukolosky and b. flannery numerical recipes in c the art of scientific computing ed.. cambridge university press. prince s. computer vision models learning and inference. cambridge. pritchard j. m. m. stephens and p. donnelly inference of population structure using multilocus genotype data. genetics qi y. and t. jaakkola parameter expanded variational bayesian methods. in nips. qi y. m. szummer and t. minka bayesian conditional random fields. in intl. workshop on aistatistics. quinlan j. learning logical definitions from relations. machine learning quinlan j. r. induction of decision trees. machine learning quinlan j. r. programs for machine learning. morgan kauffman. quinonero-candela j. c. rasmussen and c. williams approximation methods for gaussian process regression. in l. bottou o. chapelle d. decoste and j. weston large scale kernel machines pp. mit press. rabiner l. r. a tutorial on hidden markov models and selected applications in speech recognition. proc. of the ieee rai p. and h. daume multilabel prediction via sparse infinite cca. in nips. raiffa h. decision analysis. ad dison wesley. raina r. a. madhavan and a. ng large-scale deep unsupervised learning using graphics processors. in intl. conf. on machine learning. raina r. a. ng and d. koller transfer learning by constructing informative priors. in nips. rajaraman a. and j. ullman self mining of massive datasets. published. rajaraman a. and j. ullman mining of massive datasets. cambridge. rakotomamonjy a. f. bach s. canu and y. grandvalet simplemkl. j. of machine learning research ramage d. d. hall r. nallapati and c. manning labeled lda a supervised topic model for credit attribution in multi-labeled corpora. in emnlp. ramage d. c. manning and s. dumais partially labeled topic models for interpretable text mining. in proc. of the int l conf. on knowledge discovery and data mining. ramaswamy s. p. tamayo r. rifkin s. mukherjee c. yeang m. angelo c. ladd m. reich e. latulippe j. mesirov t. poggio w. gerald m. loda e. lander and t. golub multiclass cancer diagnosis using tumor gene expression signature. proc. of the national academy of science usa ranzato m. and g. hinton modeling pixel means and covariances using factored third-order boltzmann machines. in cvpr. ranzato m. f.-j. huang y.-l. boureau and y. lecun unsupervised learning of invariant feature hierarchies with applications to object recognition. in cvpr. ranzato m. c. poultney s. chopra and y.lecun efficient learning of sparse representations with an energy-based model. in nips. rao a. and k. rose february. deterministically annealed design of hidden markov model speech recognizers. ieee trans. on speech and audio proc. rasmussen c. the infinite gaussian mixture model. in nips. rasmussen c. e. and j. qui onerocandela healing the relevance vector machine by augmentation. in intl. conf. on machine learning pp. rasmussen c. e. and c. k. i. williams gaussian processes for machine learning. mit press. ratsch g. t. onoda and k. muller soft margins for adaboost. machine learning rattray m. o. stegle k. sharp and j. winn inference algorithms and learning theory for bayesian sparse factor analysis. in proc. intl. workshop on statisticalmechanical informatics. rauch h. e. f. tung and c. t. striebel maximum likelihood estimates of linear dynamic systems. aiaa journal ravikumar p. j. lafferty h. liu and l. wasserman sparse additive models. j. of royal stat. soc. series b raydan m. the barzilai and borwein gradient method for the large scale unconstrained minimization problem. siam j. on optimization rennie j. why sums are bad. technical report mit. rennie j. l. shih j. teevan and d. karger tackling the poor assumptions of naive bayes text classifiers. in intl. conf. on machine learning. reshed d. y. reshef h. finucane s. grossman g. mcvean p. turnbaugh e. lander m. mitzenmacher and p. sabeti december. detecting novel associations in large data sets. science resnick s. i. adventures in stochastic processes. birkhauser. rice j. mathematical statistics and data analysis. duxbury. edition. bibliography richardson s. and p. green on bayesian analysis of mixtures with an unknown number of components. j. of royal stat. soc. series b riesenhuber m. and t. poggio hierarchical models of object recognition in cortex. nature neuroscience rish i. g. grabarnik g. cecchi f. pereira and g. gordon closed-form supervised dimensionality reduction with generalized linear models. in intl. conf. on machine learning. ristic b. s. arulampalam and n. gordon beyond the kalman filter particle filters for tracking applications. artech house radar library. robert c. simulation of truncated normal distributions. statistics and computing robert c. and g. casella monte carlo statisical methods. springer. edition. roberts g. and j. rosenthal scaling optimal metropolis-hastings statistical science for various algorithms. roberts g. o. and s. k. sahu updating schemes correlation structure blocking and parameterization for the gibbs sampler. j. of royal stat. soc. series b robinson r. w. counting labeled acyclic digraphs. in f. harary new directions in the theory of graphs pp. academic press. roch s. a short proof that phylogenetic tree reconstrution by maximum likelihood is hard. ieeeacm trans. comp. bio. bioinformatics rodriguez a. and k. modeling through nested data models. biometrika. to appear. ghosh relational partition rose k. november. deterministic annealing for clustering compression classification regression and related optimization problems. proc. ieee rosenblatt f. the perceptron a probabilistic model for information storage and organization in the brain. psychological review a ross s. introduction to proba bility models. academic press. rosset s. j. zhu and t. hastie boosting as a regularized path to a maximum margin classifier. j. of machine learning research rossi p. g. allenby and r. mcculloch bayesian statistics and marketing. wiley. roth d. apr. on the hardness of approximate reasoning. artificial intelligence rother c. p. kohli w. feng and j. jia minimizing sparse higher order energy functions of discrete variables. in cvpr pp. rouder j. p. speckman d. sun and r. morey bayesian t tests for accepting and rejecting the null hypothesis. pyschonomic bulletin review roverato a. hyper inverse wishart distribution for nondecomposable graphs and its application to bayesian inference for gaussian graphical models. scand. j. statistics roweis s. em algorithms for pca and spca. in nips. rubin d. using the sir algorithm to simulate posterior distributions. in bayesian statistics rue h. and l. held gaussian markov random fields theory and applications volume of monographs on statistics and applied probability. london chapman hall. rue h. s. martino and n. chopin approximate bayesian inference for latent gaussian models using integrated nested laplace approximations. j. of royal stat. soc. series b rumelhart d. g. hinton and r. williams learning internal representations by error propagation. in d. rumelhart j. mcclelland and the pdd research group parallel distributed processing explorations in the microstructure of cognition. mit press. ruppert d. m. wand and r. carroll semiparametric regression. cambridge university press. rush a. m. and m. collins a tutorial on lagrangian relaxation and dual decomposition for nlp. technical report columbia u. russell s. j. binder d. koller and k. kanazawa local learning in probabilistic networks with hidden variables. in intl. joint conf. on ai. russell s. and p. norvig artificial intelligence a modern approach. englewood cliffs nj prentice hall. russell s. and p. norvig artificial intelligence a modern approach. prentice hall. edition. russell s. and p. norvig artificial intelligence a modern approach. prentice hall. edition. s. and m. black april. fields j. computer vi intl. of experts. sion sachs k. o. perez d. pe er d. lauffenburger and g. nolan causal protein-signaling networks derived from multiparameter science single-cell data. sahami m. and t. heilman a web-based kernel function for measuring the similarity of short text snippets. in www conferenec. salakhutdinov r. erative models. toronto. deep genthesis u. ph.d. salakhutdinov r. and g. hinton deep boltzmann machines. in aistatistics volume pp. salakhutdinov r. and g. hinton an undirected topic model. in nips. replicated softmax salakhutdinov r. and h. larochelle efficient learning of deep boltzmann machines. in aistatistics. salakhutdinov r. and a. mnih probabilistic matrix factorization. in nips volume bibliography salakhutdinov r. and s. roweis adaptive overrelaxed bound optimization methods. in proceedings of the international conference on machine learning volume pp. schaefer j. and k. strimmer a shrinkage approach to largescale covariance matrix estimation and implications for functional genomics. statist. appl. genet. mol. biol salakhutdinov r. j. tenenbaum and a. torralba learning to learn with compound hd models. in nips. salakhutdinov r. r. a. mnih and g. e. hinton restricted boltzmann machines for collaborative filtering. in intl. conf. on machine learning volume pp. salojarvi j. k. puolamaki and s. klaski on discriminative joint density modeling. in proc. european conf. on machine learning. sampson f. a novitiate in a period of change an experimental and case study of social relationships. ph.d. thesis cornell. santner t. b. williams and w. notz the design and analysis of computer experiments. springer. sarkar j. one-armed bandit problems with covariates. the annals of statistics sato m. and s. ishii on-line em algorithm for the normalized gaussian network. neural computation saul l. t. jaakkola and m. jordan mean field theory for sigmoid belief networks. j. of ai research saul l. and m. jordan exploiting tractable substructures in intractable networks. in nips volume saul l. and m. jordan attractor dynamics in feedforward neural networks. neural computation saunders c. j. shawe-taylor and a. vinokourov string kernels fisher kernels and finite state automata. in nips. savage r. k. heller y. xi z. ghahramani w. truman m. grant k. denby and d. wild rbhc fast bayesian hierarchical clustering for microarray data. bmc bioinformatics schapire r. the strength of weak learnability. machine learning schapire r. and y. freund foundations and algo boosting rithms. mit press. schapire r. y. freund p. bartlett and w. lee boosting the margin a new explanation for the effectiveness of voting methods. annals of statistics scharstein d. and r. szeliski a taxonomy and evaluation of dense two-frame stereo correspondence algorithms. j. computer vision intl. schaul t. s. zhang and y. lecun no more pesky learning rates. technical report courant instite of mathematical sciences. schmee j. and g. hahn a simple method for regresssion analysis with censored data. technometrics schmidt m. graphical model structure learning with regularization. ph.d. thesis ubc. schmidt m. g. fung and r. rosales optimization methods for regularization. technical report u. british columbia. schmidt m. and k. murphy modeling discrete interventional data using directed cyclic graphical models. in uai. schmidt m. k. murphy g. fung and r. rosales structure learning in random fields for heart motion abnormality detection. in cvpr. schmidt m. a. niculescu-mizil and k. murphy learning graphical model structure using paths. in aaai. schmidt m. e. van den berg m. friedlander and k. murphy optimizing costly functions with simple constraints a limited-memory projected quasinewton algorithm. in ai statistics. schniter p. l. c. potter and j. ziniel fast bayesian matching pursuit model uncertainty and parameter estimation for sparse linear models. technical report u. ohio. submitted to ieee trans. on signal processing. schnitzspan p. s. roth and b. schiele automatic discovery of meaningful object parts with latent crfs. in cvpr. schoelkopf b. and a. smola learning with kernels support vector machines regularization optimization and beyond. mit press. schoelkopf b. a. smola and k.-r. mueller nonlinear component analysis as a kernel eigenvalue problem. neural computation schraudolph n. n. j. yu and s. g nter a stochastic quasinewton method for online convex optimization. in aistatistics pp. schwarz g. estimating the dimension of a model. annals of statistics a schwarz r. and y. chow the n-best algorithm an efficient and exact procedure for finding the n most in intl. conf. on acoustics speech and signal proc. likely hypotheses. schweikerta g. a. zien g. zeller j. behr c. dieterich c. ong p. philips f. d. bona l. hartmann a. bohlen n. kr ijger s. sonnenburg and g. r d tsch mgene accurate svm-based gene finding with an application to nematode genomes. genome research scott d. and biometrika data-based on optimal histograms. scott j. g. and c. m. carvalho feature-inclusion stochastic search for gaussian graphical models. j. of computational and graphical statistics scott s. data augmentation frequentist estimation and the bayesian analysis of multinomial logit models. statistical papers. scott s. a modern bayesian look at the multi-armed bandit. applied stochastic models in business and industry bibliography sedgewick r. and k. wayne al gorithms. addison wesley. seeger m. bayesian inference and optimal design in the sparse linear model. j. of machine learning research seeger m. and h. nickish compressed sensing and bayesian experimental design. in intl. conf. on machine learning. segal d. february. the dirty little secrets of search. new york times. seide f. g. li and d. yu conversational speech transcription using context-dependent deep neural networks. in interspeech. sejnowski t. and c. rosenberg parallel networks that learn to pronounce english text. complex systems sellke t. m. j. bayarri and j. berger calibration of p values for testing precise null hypotheses. the american statistician serre t. l. wolf and t. poggio recognition with features object inspired by visual cortex. in cvpr pp. shachter r. bayes-ball the rational pastime determining irrelevance and requisite information in belief networks and influence diagrams. in uai. r. and c. r. kenley shachter gaussian influence diagrams. managment science shachter r. d. and m. a. peot simulation approaches to general probabilistic inference on belief networks. in uai volume shafer g. r. and p. p. shenoy probability propagation. annals of mathematics and ai shafto p. c. kemp v. mansinghka m. gordon and j. b. tenenbaum learning cross-cutting systems of categories. in cognitive science conference. shahaf d. a. chechetka and c. guestrin learning thin junction trees via graph cuts. in aistats. shalev-shwartz s. y. singer and n. srebro pegasos primal estimated sub-gradient solver for svm. in intl. conf. on machine learning. shalizi c. cs lecture principal components mathematics example interpretation. shan h. and a. banerjee residual bayesian co-clustering for matrix approximation. in siam intl. conf. on data mining. shawe-taylor j. and n. cristianini kernel methods for pattern analysis. cambridge. sheng q. y. moreau and b. d. moor biclustering microarray data by gibbs sampling. bioinformatics shi j. and j. malik normalized cuts and image segmentation. ieee trans. on pattern analysis and machine intelligence. shoham y. and k. leyton-brown multiagent systems algorithmic game- theoretic and logical foundations. cambridge university press. shotton j. a. fitzgibbon m. cook t. sharp m. finocchio r. moore a. kipman and a. blake real-time human pose recognition in parts from a single depth image. in cvpr. shwe m. b. middleton d. heckerman m. henrion e. horvitz h. lehmann and g. cooper probabilistic diagnosis using a reformulation of the knowledge base. inf. med methods. siddiqi s. b. boots and g. gordon a constraint generation approach to learning stable linear dynamical systems. in nips. siepel a. and d. haussler combining phylogenetic and hidden markov models in biosequence analysis. in proc. intl. conf. on computational molecular biology silander t. p. kontkanen and p. myllym d ki on sensitivity of the map bayesian network structure to the equivalent sample size parameter. in uai pp. silander t. and p. myllmaki a simple approach for finding the globally optimal bayesian network structure. in uai. sill j. g. takacs l. mackey and d. lin feature-weighted linear stacking. technical report silverman b. w. spline smoothing the equivalent variable kernel method. annals of statistics simard p. d. steinkraus and j. platt best practices for convolutional neural networks applied to visual document analysis. in intl. conf. on document analysis and recognition simon d. optimal state estimation kalman h infinity and nonlinear approaches. wiley. singliar t. and m. hauskrecht noisy-or component analysis and its application to link analysis. j. of machine learning research smidl v. and a. quinn the variational bayes method in signal processing. springer. smith a. f. m. and a. e. gelfand bayesian statistics without tears a sampling-resampling perspective. the american statistician smith r. and p. cheeseman on the representation and estimation of spatial uncertainty. intl. j. robotics research smith t. v. j. yu smulders a. hartemink and e. jarvis computational inference of neural information flow networks. plos computational biology smolensky p. information processing in dynamical systems foundations of harmony theory. in d. rumehart and j. mcclelland parallel distributed processing explorations in the microstructure of cognition. volume mcgraw-hill. smyth p. d. heckerman and m. i. jordan probabilistic independence networks for hidden markov probability models. neural computation sohl-dickstein m. deweese on machine learning. j. p. battaglino and in intl. conf. bibliography sollich p. bayesian methods for support vector machines evidence and predictive class probabilities. machine learning sontag d. a. globerson and t. jaakkola introduction to dual decomposition for inference. in s. sra s. nowozin and s. j. wright optimization for machine learning. mit press. sorenson h. and d. alspach recursive bayesian estimation using gaussian sums. automatica a s soussen c. j. iier d. brie and j. duan from bernoulligaussian deconvolution to sparse signal restoration. technical report centre de recherche en automatique de nancy. spaan m. and n. vlassis perseus randomized point-based value iteration for pomdps. j. of ai research spall j. introduction to stochastic search and optimization estimation simulation and control. wiley. speed t. december. a correlation for the century. science speed t. and h. kiiveri gaussian markov distributions over finite graphs. annals of statistics spiegelhalter d. j. and s. l. lauritzen sequential updating of conditional probabilities on directed graphical structures. networks spirtes p. scheines c. glymour and r. causation prediction and search. mit press. edition. srebro n. maximum likelihood bounded tree-width markov networks. in uai. srebro n. and t. jaakkola approximain intl. conf. on machine low-rank weighted tions. learning. steinbach m. g. karypis and v. kumar a comparison of document clustering techniques. in kdd workshop on text mining. stephens m. dealing with label-switching in mixture models. j. royal statistical society series b stern d. r. herbrich and t. graepel matchbox large scale bayesian recommendations. in proc. intl. world wide web conference. steyvers m. and t. griffiths probabilistic topic models. in t. landauer d. mcnamara s. dennis and w. kintsch latent semantic analysis a road to meaning. laurence erlbaum. stigler s. the history of statis tics. harvard university press. stolcke a. and s. m. omohundro hidden markov model induction by bayesian model merging. in stoyanov v. a. ropson and j. eisner empirical risk minimization of graphical model parameters given approximate inference decoding and model structure. in aistatistics. sudderth e. graphical models for visual object recognition and tracking. ph.d. thesis mit. sudderth e. and w. freeman march. signal and image processing with belief propagation. ieee signal processing magazine. sudderth e. a. ihler w. freeman and a. willsky nonparametric belief propagation. in cvpr. sudderth e. a. ihler m. isard w. freeman and a. willsky nonparametric belief propagation. comm. of the acm sudderth e. and m. jordan shared segmentation of natural scenes using dependent pitmanyor processes. in nips. sudderth e. m. wainwright and a. willsky loop series and bethe variational bounds for attractive graphical models. in nips. sun l. s. ji s. yu and j. ye on the equivalence between canonical correlation analysis and orthonormalized partial in intl. joint conf. on ai. least squares. sunehag p. j. trumpf s. v. n. vishwanathan and n. n. schraudolph variable metric stochastic approximation theory. in aistatistics pp. sutton c. and a. mccallum improved dynamic schedules for belief propagation. in uai. sutton r. and a. barto reinforcment learning an introduction. mit press. swendsen r. and j.-s. wang nonuniversal critical dynamics in monte carlo simulations. physical review letters swersky k. b. chen b. marlin and n. de freitas a tutorial on stochastic approximation algorithms for training restricted boltzmann machines and deep belief nets. in information theory and applications workshop. szeliski r. computer vision algorithms and applications. springer. szeliski r. r. zabih d. scharstein o. veksler v. kolmogorov a. agarwala m. tappen and c. rother a comparative study of energy minimization methods for markov random fields with smoothness-based priors. ieee trans. on pattern analysis and machine intelligence szepesvari c. algorithms for reinforcement learning. morgan claypool. taleb n. the black swan the impact of the highly improbable. random house. talhouk a. k. murphy and a. doucet efficient bayesian inference for multivariate probit models with sparse inverse correlation matrices. j. comp. graph. statist.. tanner m. tools for statistical inference. springer. sun j. n. zheng and h. shum stereo matching using belief propagation. ieee trans. on pattern analysis and machine intelligence tanner m. and w. wong the calculation of posterior distributions by data augmentation. j. of the am. stat. assoc. bibliography tarlow d. i. givoni and r. zemel hop-map efficient message passing with high order potentials. in aistatistics. taskar b. c. guestrin and d. koller max-margin markov networks. in nips. taskar b. d. klein m. collins d. koller and c. manning max-margin parsing. in proc. empirical methods in natural language processing. teh y. w. a hierarchical bayesian language model based on pitman-yor processes. in proc. of the assoc. for computational linguistics pp. teh y.-w. m. jordan m. beal and d. blei hierarchical dirichlet processes. j. of the am. stat. assoc. tenenbaum j. framework for ph.d. thesis mit. a bayesian learning. concept tenenbaum j. b. and f. xu word learning as bayesian inference. in proc. annual conf.of the cognitive science society. theocharous g. k. murphy and l. kaelbling representing hierarchical pomdps as dbns for multi-scale robot localization. in ieee intl. conf. on robotics and automation. thiesson b. c. meek d. chickering and d. heckerman learning mixtures of dag models. in uai. thomas a. and p. green enumerating the decomposable neighbours of a decomposable graph under a simple perturbation scheme. comp. statistics and data analysis thrun s. w. burgard and d. fox probabilistic robotics. mit press. thrun s. m. montemerlo d. koller b. wegbreit j. nieto and e. nebot fastslam an efficient solution to the simultaneous localization and mapping problem with unknown data association. j. of machine learning research thrun s. and l. pratt learning to learn. kluwer. tibshirani r. regression shrinkage and selection via the lasso. j. royal. statist. soc b tibshirani r. g. walther and t. hastie estimating the number of clusters in a dataset via the gap statistic. j. of royal stat. soc. series b tieleman t. training restricted boltzmann machines using approximations to the likelihood gradient. in proceedings of the international conference on machine learning pp. acm new york ny usa. ting j. a. d souza s. vijayakumar and s. schaal efficient learning and feature selection in high-dimensional regression. neural computation tipping m. probabilistic visualization of high-dimensional binary data. in nips. tipping m. sparse bayesian learning and the relevance vector machine. j. of machine learning research tipping m. and c. bishop probabilistic principal component analysis. j. of royal stat. soc. series b tipping m. and a. faul fast likelihood maximisation marginal for sparse bayesian models. in aistats. tishby n. f. pereira and w. biale the information neck method. in the annual allerton conf. on communication control and computing pp. a tomas m. d. anoop k. stefan b. lukas and c. jan empirical evaluation and combination of advanced language modeling techniques. in proc. annual conf. of the intl. speech communication association torralba a. r. fergus and y. weiss small codes and large image databases for recognition. in cvpr. train k. discrete choice methcambridge ods with simulation. university press. second edition. tseng p. on accelerated proximal gradient methods for convexconcave optimization. technical report u. washington. tsochantaridis i. t. joachims t. hofmann and y. altun september. large margin methods for structured and interdependent output variables. j. of machine learning research tu z. and s. zhu image segmentation by data-driven markov chain monte carlo. ieee trans. on pattern analysis and machine intelligence turian j. l. ratinov and y. bengio word representations a simple and general method for semi-supervised learning. in proc. acl. turlach b. w. venables and s. wright simultaneous variable selection. technometrics turner r. p. berkes m. sahani and d. mackay counterexamples to variational free energy compactness folk theorems. technical report u. cambridge. ueda n. and r. nakano deterministic annealing em algorithm. neural networks usunier n. d. buffoni and p. gallinari ranking with ordered weighted pairwise classification. vaithyanathan s. and b. dom model selection in unsupervised learning with applications to document clustering. in intl. conf. on machine learning. van der merwe r. a. doucet n. de freitas and e. wan the unscented particle filter. in van dyk d. and x.-l. meng the art of data augmentation. j. computational and graphical statistics vandenberghe l. applied numerical computing lecture notes. vandenberghe l. optimization methods for large-scale systems. vanhatalo j. speeding up the inference in gaussian process models. ph.d. thesis helsinki univ. technology. bibliography vanhatalo j. v. pietil d inen and a. vehtari approximate inference for disease mapping with sparse gaussian processes. statistics in medicine vapnik v. statistical learning theory. wiley. vapnik v. s. golowich and a. smola support vector method for function approximation regression estimation and signal processing. in nips. varian h. structural time series in r a tutorial. technical report google. verma t. and j. pearl equivalence and synthesis of causal models. in uai. viinikanoja j. a. klami and s. kaski variational bayesian mixture of robust cca models. in proc. european conf. on machine learning. vincent p. a connection between score matching and denoising autoencoders. neural computation vincent p. h. larochelle i. lajoie y. bengio and p.-a. manzagol stacked denoising autoencoders learning useful representations in a deep network with a local denoising criterion. j. of machine learning research vinh n. j. epps and j. bailey information theoretic measures for is a corclusterings comparison rection for chance necessary? in intl. conf. on machine learning. vinyals m. j. cerquides j. rodriguezaguilar and a. farinelli worst-case bounds on the quality of max-product fixed-points. in nips. viola p. and m. jones rapid object detection using a boosted cascade of simple classifiers. in cvpr. virtanen s. bayesian exponential family projections. master s thesis aalto university. vishwanathan s. v. n. and a. smola fast kernels for string and tree matching. in nips. viterbi a. error bounds for convolutional codes and an asymptotically optimum decoding algorithm. ieee trans. on information theory a von luxburg u. a tutorial on statistics and spectral clustering. computing wagenmakers e.-j. r. wetzels d. borsboom and h. van der maas why psychologists must change the way they analyze their data the case of psi. journal of personality and social psychology. wagner d. and f. wagner between min cut and graph bisection. in proc. intl. symp. on math. found. of comp. sci. pp. wainwright m. t. jaakkola and a. willsky tree-based reparameterization for approximate estimation on loopy graphs. in jaakkola and wainwright m. t. a. willsky a new class of upper bounds on the log partition function. ieee trans. info. theory wainwright m. p. ravikumar and inferring graphj. lafferty ical model structure using pseudo-likelihood. in nips. wainwright m. j. t. s. jaakkola and a. s. willsky tree-based reparameterization framework for analysis of sum-product and related algorithms. ieee trans. on information theory wainwright m. j. and m. jordan graphical models exponential families and variational inference. foundations and trends in machine learning i. wainwright m. j. and m. jordan graphical models exponential families and variational inference. foundations and trends in machine learning i. wallach h. i. murray r. salakhutdinov and d. mimno evaluation methods for topic models. in intl. conf. on machine learning. wan e. a. and r. v. der merwe the unscented kalman filter. in s. haykin kalman filtering and neural networks. wiley. wand m. semiparametric regression and graphical models. aust. n. z. j. stat. wand m. p. j. t. ormerod s. a. padoan and r. fruhrwirth mean field variational bayes for elaborate distributions. bayesian analysis wang c. variational bayesian approach to canonical correlation analysis. ieee trans. on neural networks wasserman l. all of statistics. a concise course in statistical inference. springer. wei g. and m. tanner a monte carlo implementation of the em algorithm and the poor man s data augmentation algorithms. j. of the am. stat. assoc. weinberger k. a. dasgupta j. attenberg j. langford and a. smola feature hashing for large scale multitask learning. in intl. conf. on machine learning. weiss d. b. sapp and b. taskar sidestepping intractable inference with structured ensemble cascades. in nips. weiss y. correctness of local probability propagation in graphical models with loops. neural computation weiss y. comparing the mean field method and belief propagation for approximate inference in mrfs. in saad and opper advanced mean field methods. mit press. weiss y. and w. t. freeman correctness of belief propagation in gaussian graphical models of arbitrary topology. in weiss y. and w. t. freeman correctness of belief propagation in gaussian graphical models of arbitrary topology. neural computation weiss y. and w. t. freeman on the optimality of solutions of the max-product belief propagation algorithm in arbitrary graphs. ieee trans. information theory special issue on codes on graphs and iterative algorithms weiss y. a. torralba and r. fergus spectral hashing. in nips. bibliography welling m. c. chemudugunta and n. sutter deterministic latent variable models and their pitfalls. in intl. conf. on data mining. welling m. t. minka and y. w. teh structured region graphs morphing ep into gbp. in uai. welling m. m. rosen-zvi and g. hinton exponential family harmoniums with an application to information retrieval. in welling m. and c. sutton learning in markov random fields with contrastive free energies. in tenth international workshop on artificial intelligence and statistics welling m. and y.-w. teh belief optimization for binary networks a stable alternative to loopy belief propagation. in uai. werbos p. beyond regression new tools for prediction and analysis in the behavioral sciences. ph.d. thesis harvard. west m. of tures biometrika normal on scale mixdistributions. west m. bayesian factor regression models in the p small n paradigm. bayesian statistics west m. and j. harrison bayesian forecasting and dynamic models. springer. weston j. s. bengio and n. usunier large scale image annotation learning to rank with joint word-image embeddings. in proc. european conf. on machine learning. weston j. f. ratle and r. collobert deep learning via semisupervised embedding. in intl. conf. on machine learning. weston j. and c. watkins support vector ma multi-lcass chines. in esann. wiering m. and m. van otterlo reinforcement learning state-of-the-art. springer. wilkinson d. and s. yeung conditional simulation from highly structured gaussian systems with application to blocking-mcmc for the bayesian analysis of very large linear models. statistics and computing williams c. computation with infinite networks. neural computation williams c. a mcmc approach to hierarchical mixture modelling in s. a. solla t. k. leen and k.-r. m ller nips. mit press. williams c. on a connection between kernel pca and metric multidimensional scaling. machine learning j. williams o. and a. fitzgibbon gaussian process implicit surfaces. in gaussian processes in practice. williamson s. and z. ghahramani probabilistic models for data combination in recommender systems. in nips workshop on learning from multiple sources. winn j. and c. bishop variational message passing. j. of machine learning research wipf d. and s. nagarajan a new view of automatic relevancy determination. in nips. wipf d. and s. nagarajan april. iterative reweighted and methods for finding sparse solutions. j. of selected topics in signal processing issue on compressive sensing wipf d. b. rao and s. nagarajan latent variable bayesian models for promoting sparsity. ieee transactions on information theory. witten d. r. tibshirani and t. hastie a penalized matrix decomposition with applications to sparse principal components and canonical correlation analysis. biostatistics wolpert d. stacked generalization. neural networks wolpert d. the lack of a priori distinctions between learning algorithms. neural computation wong f. c. carter and r. kohn efficient estimation of models. covariance biometrika selection wood f. c. archambeau j. gasthaus l. james and y. w. teh a stochastic memoizer for sequence data. in intl. conf. on machine learning. wright r. s. nowak and m. figueiredo sparse reconstruction by separable approximation. ieee trans. on signal processing wu t. t. and k. lange coordinate descent algorithms for lasso penalized regression. ann. appl. stat wu y. h. tjelmeland and m. west bayesian cart prior structure and mcmc computations. j. of computational and graphical statistics xu f. and j. tenenbaum word learning as bayesian inference. psychological review xu z. v. tresp a. rettinger and k. kersting social network mining with nonparametric relational models. in acm workshop on social network mining and analysis xu z. v. tresp k. yu and h.-p. kriegel infinite hidden relational models. in uai. xu z. v. tresp s. yu k. yu and h.-p. kriegel fast inference in infinite hidden relational models. in workshop on mining and learning with graphs. xue y. x. liao l. carin and b. krishnapuram multi-task learning for classification with dirichlet process priors. j. of machine learning research yadollahpour p. d. batra and g. shakhnarovich diverse mbest solutions in mrfs. in nips workshop on disrete optimization in machine learning. yan d. l. huang and m. i. jordan fast approximate spectral clustering. in acm conf. on knowledge discovery and data mining. fast feb. yang a. a. ganesh s. sastry and y. ma algorithms and an application in robust face recognition a review. technical report eecs department university of california berkeley. bibliography yang c. r. duraiswami and l. david efficient kernel machines using the improved fast gauss transform. in nips. yang s. b. long a. smola h. zha and z. zheng collaborative competitive filtering learning recommender using context of user choice. in proc. annual intl. acm sigir conference. yanover c. o. schueler-furman and y. weiss minimizing and learning energy functions for side-chain prediction. in recomb. yaun g.-x. k.-w. chang c.-j. hsieh and c.-j. lin a comparison of optimization methods and software for large-scale linear classification. j. of machine learning research yedidia j. w. t. freeman and y. weiss understanding belief propagation and its generalizations. in intl. joint conf. on ai. yoshida r. and m. west bayesian learning in sparse graphical factor models via annealed entropy. j. of machine learning research younes l. parameter estimation for imperfectly observed gibbsian fields. probab. theory and related fields yu c. and t. joachims learning structural svms with latent variables. in intl. conf. on machine learning. yu s. k. yu v. tresp k. h-p. and m. wu supervised probabilistic principal component analysis. in proc. of the int l conf. on knowledge discovery and data mining. yu s.-z. and h. kobayashi practical implementation of an efficient forward-backward algorithm for an explicit-duration hidden markov model. ieee trans. on signal processing yuan m. and y. lin model selection and estimation in regression with grouped variables. j. royal statistical society series b y. lin yuan m. and model selection and estimation in the gaussian graphical model. biometrika yuille a. cccp algorithms to minimze the bethe and kikuchi free energies convergent alternatives to belief propagation. neural computation yuille a. and a. rangarajan concave-convex procedure. the neural computation yuille a. and s. zheng compositional noisy-logical learning. in intl. conf. on machine learning. yuille a. l. and x. he probabilistic models of vision and maxmargin methods. frontiers of electrical and electronic engineering zellner a. on assessing prior distributions and bayesian regression analysis with g-prior distributions. in bayesian inference and decision techniques studies of bayesian and econometrics and statistics volume north holland. zhai c. and j. lafferty a study of smoothing methods for language models applied to information retrieval. acm trans. on information systems zhang n. hierarchical latnet class models for cluster analysis. j. of machine learning research zhang n. and d. poole exindependence in j. of ploiting causal bayesian network inference. ai research zhang t. adaptive forwardbackward greedy algorithm for sparse learning with linear models. in nips. zhang x. t. graepel and r. herbrich bayesian online learning for multi-label and multi-variate performance measures. in aistatistics. zhao j.-h. and p. l. h. yu november. fast ml estimation for the mixture of factor analyzers via an ecm algorithm. ieee. trans. on neural networks zhao p. and b. yu stagewise j. of machine learning re lasso. search zhou h. d. karakos s. khudanpur a. andreou and c. priebe on projections of gaussian distributions using maximum likelihood criteria. in proc. of the workshop on information theory and its applications. zhou m. h. chen j. paisley l. ren g. sapiro and l. carin non-parametric bayesian dictionary learning for sparse image representations. in nips. zhou x. and x. liu the em algorithm for the extended finite mixture of the factor analyzers model. computational statistics and data analysis zhu c. s. n. y. wu and d. mumford november. minimax entropy principle and its application to texture modeling. neural computation zhu j. and e. xing conditional in intl. conf. topic random fields. on machine learning. zhu l. y. chen a.yuille and w. freeman latent hierarchical structure learning for object detection. in cvpr. zhu m. and a. ghodsi automatic dimensionality selection from the scree plot via the use of profile likelihood. computational statistics data analysis zhu m. and a. lu the counterintuitive non-informative prior for the bernoulli family. j. statistics education. zinkevich m. online convex programming and generalized infinitesimal gradient ascent. in intl. conf. on machine learning pp. a zobay o. mean field inference for the dirichlet process mixture model. electronic j. of statistics zhao p. g. rocha and b. yu grouped and hierarchical model selection through composite absolute penalties. technical report uc berkeley. zoeter o. bayesian generalized linear models in a terabyte world. in proc. international symposium on image and signal processing and analysis. bibliography zou h. the adaptive lasso j. of the and its oracle properties. am. stat. assoc. nent analysis. j. of computational and graphical statistics cave penalized likelihood models. annals of statistics zou h. and t. hastie regularization and variable selection via the elastic net. j. of royal stat. soc. series b zou h. t. hastie and r. tibshirani on the of freedom of the lasso. annals of statistics zou h. t. hastie and r. tibshirani sparse principal compo zou h. and r. li onestep sparse estimates in noncon zweig g. and m. padmanabhan exact alpha-beta computation in logarithmic space with application to map word graph construction. in proc. intl. conf. spoken lang. index to code agglomdemo amazonsellerdemo arsdemo arsenvelope bayeschangeofvar bayesttestdemo beliefpropagation bernoullientropyfig besselk betabinompostpreddemo betacredibleint betahpd betaplotdemo bimodaldemo binaryfademotipping binomdistplot binomialbetaposteriordemo bleildaperplexityplot bolassodemo boostingdemo bootstrapdemober cancerhighdimclassifdemo cancerrateseb casinodemo centrallimitdemo chowliutreedemo coinsmodelseldemo contoursssedemo convexfnhand cursedimensionality demard depnetfit dirichlethistogramdemo discreteprobdistfig discrimanalysisdboundariesdemo discrimanalysisfit discrimanalysisheightweightdemo discrimanalysispredict dpmsampledemo dtfit dtreedemoiris elasticdistortionsdemo emloglikelihoodmax fabiplotdemo fisherdiscrimvoweldemo fisheririsdemo fisherldademo fmgibbs gammaplotdemo gammarainfalldemo gampdf gaussheightweight gaussimputationdemo gaussinterpdemo gaussinterpnoisydemo gaussmissingfitem gaussmissingfitgibbs gaussplotdemo generativevsdiscrim geomridge ggmfitdemo ggmfithtf ggmfitminfunc ggmlassodemo ggmlassohtf gibbsdemoising gibbsgaussdemo ginidemo gpnndemo gprdemoard gprdemochangehparams gprdemomarglik gprdemonoisefree gpspatialdemolaplace grouplassodemo hclustyeastdemo hingelossplot hmmfilter hmmfwdback hmmlillypaddemo hmmselfloopdist hopfielddemo huberlossdemo icabasisdemo icademo icademouniform isingimagedenoisedemo kalmanfilter kalmantrackingdemo kernelbinaryclassifdemo kernelregrdemo kernelregressiondemo klfwdreversemixgauss klpqgauss kmeansheightweight kmeansyeastdemo knnclassifydemo knnvoronoi kpcascholkopf lassopathprostate lassoshooting leastsquaresprojection linregallsubsetsgraycodedemo linregbayescaterpillar linregcensoredschmeehahndemo indexes linregebmodelselvsn linregonlinedemokalman linregpolylassodemo linregpolyvsdegree linregpolyvsn linregpolyvsregdemo linregpostpreddemo linregrbfdemo linregrobustdemocombined lmsdemo logregfit logreglaplacegirolamidemo logregmultinomkerneldemo logregsatdemo logregsatdemobayes logregsatmhdemo logregxordemo logsumexp lossfunctionfig lsicode marsdemo mcaccuracydemo mcestimatepi mcmcgmmdemo mcquantiledemo mcstatdist mimixeddemo mixbermnistem mixbetademo mixexpdemo mixexpdemoonetomany mixgaussdemofaithful mixgaussliksurfacedemo mixgaussmlvsmap mixgaussoverrelaxedemdemo mixgaussplotdemo mixgausssingularity mixgaussvbdemofaithful mixppcademonetlab mixstudentbankruptcydemo mlppriorsdemo mlpregevidencedemo mlpreghmcdemo multilevellinregdemo mutualinfoallpairsmixed naivebayesbowdemo naivebayesfit naivebayespredict netflixresultsplot newsgroupsvisualize newtonsmethodminquad newtonsmethodnonconvex ngramplot normalgammapenaltyplotdemo normalgammathresholdplotdemo numbersgame pagerankdemo pagerankdemopmtk paretoplot pcademoheightweight pcaemstepbystep pcaimagedemo pcaoverfitdemo pcapmtk pfcolortrackerdemo poissonplotdemo postdensityintervals prhand probitplot probitregdemo prostatecomparison prostatesubsets quantiledemo rbpfmaneuverdemo rbpfslamdemo rdafit regtreesurfacedemo rejectionsamplingdemo relevancenetworknewsgroupdemo residualsdemo ridgepathprostate riskfngauss robustdemo robustpriordemo sademopeaks samplecdf samplingdistgaussshrinkage sensorfusionunknownprec seqlogodemo shrinkagedemobaseball shrinkcov shrinkcovdemo shrunkencentroidsfit shrunkencentroidssrbctdemo shuffleddigitsdemo sigmoidlowerbounds sigmoidplot simpsonsparadoxgraph smoothingkernelplot sparsa sparsedictdemo sparsennetdemo sparsepostplot sparsesensingdemo spectralclusteringdemo splinebasisdemo ssmtimeseriessimple steepestdescentdemo stickbreakingdemo studentlaplacepdfplot subgradientplot subsupergaussplot surfacefitdemo svdimagedemo svmcgammademo tanhplot trueskilldemo index to code trueskillplot unigaussvbdemo varembound variableelimination visdirichletgui visualizealarmnetwork vqdemo wiplotdemo index to keywords loss a star search absorbing state accept action action nodes action space actions activation active learning active set active set activity recognition adagrad adaline adaptive basis-function model adaptive importance sampling adaptive lasso adaptive mcmc adaptive rejection metropolis sampling adaptive rejection sampling add-one smoothing adf adjacency matrix adjust for adjusted rand index admissible admixture mixture adsense adwords affinity propagation agglomerative clustering agglomerative hierarchical clustering aha ai aic akaike information criterion alarm network alignment all pairs alleles alpha divergence alpha expansion alpha-beta swap alternative hypothesis analysis view analysis-synthesis ancestors ancestral graph ancestral sampling and-or graphs annealed importance sampling annealing annealing importance sampling anova anti-ferromagnets aperiodic approximate inference approximation error ard ard kernel area under the curve arma array cgh association rules associative associative markov network associative memory associative mrf assumed density filter assumed density filtering asymptotically normal asymptotically optimal asynchronous updates atom atomic bomb attractive mrf attributes auc audio-visual speech recognition augmented dag auto-encoder auto-encoders auto-regressive hmm autoclass autocorrelation function automatic relevance determination automatic relevancy determination automatic speech recognition automatic speech recognition auxiliary function auxiliary variables average link clustering average precision average precision at k axis aligned axis parallel splits back-propagation backdoor path backfitting background knowledge backoff smoothing backpropagation backpropagation algorithm backslash operator backwards selection bag of words bag-of-characters bag-of-words bagging bandwidth barren node removal bart barzilai-borwein base distribution base learner base measure base rate fallacy basic feasible solution basis function expansion basis functions basis pursuit denoising batch baum-welch bayes ball algorithm bayes decision rule bayes estimator index to keywords bayes factor bayes model averaging bayes point bayes risk bayes rule bayes theorem bayesian xxvii bayesian adaptive regression trees bayesian factor regression bayesian hierarchical clustering bayesian information criterion bayesian ipf bayesian lasso bayesian model selection bayesian network structure learning bayesian networks bayesian occam s razor bayesian statistics bde bdeu beam search belief networks belief propagation belief state belief state mdp belief updating bell curve berkson s paradox bernoulli bernoulli product model bernoulli-gaussian bessel function beta distribution beta function beta process beta-binomial bethe bethe energy functional bethe free energy bfgs bhattacharya distance bi-directed graph bias bias term bias-variance tradeoff bic biclustering big data bigram model binary classification binary entropy function binary independence model binary mask binary tree bing binomial binomial coefficient binomial distribution binomial regression binomialboost bio biosequence analysis bipartite graph biplot birth moves bisecting k-means bits bits-back black swan paradox black-box blackwell-macqueen blank slate blind signal separation blind source separation blocked gibbs sampling blocking gibbs sampling bloodtype bolasso boltzmann distribution boltzmann machine bond variables boosting boosting bootstrap bootstrap filter bootstrap lasso bootstrap resampling borrow statistical strength bottleneck bottleneck layer bound optimization box constraints box-muller boxcar kernel boyen-koller bp bpdn bradley terry branch and bound branching factor bridge regression brownian motion bucket elimination bugs buried markov models burn-in phase burned in burstiness bursty calculus of variations calibration candidate method canonical correlation analysis canonical form canonical link function canonical parameters cardinality constraints cart cartesian cascade case analysis categorical categorical pca categorical variables cauchy causal markov assumption causal models causal mrf causal networks causal sufficiency causality cca cccp cd cdf censored regression censored regression indexes centering matrix central composite design central interval central limit theorem central moment central-limit theorem centroid centroids certainty factors chain graph chain rule chance nodes change of variables channel coding chapman-kolmogorov characteristic length scale cheeseman-stutz approximation chi-squared distribution chi-squared statistic children chinese restaurant process chip-seq cholesky decomposition chomsky normal form chordal chordal graph chow-liu algorithm ci circuit complexity city block distance clamped phase clamped term clamping class imbalance class-conditional density classical classical statistics classification classification and regression trees clausal form clause click-through rate clique cliques closing the loop closure cluster variational method clustering clustering clusters clutter problem co-clustering co-occurrence matrix co-parents coarse-to-fine grid cocktail party problem coclustering codebook collaborative filtering collapsed gibbs sampler collapsed gibbs sampling collapsed particles collect evidence collect-to-root collider colt committee method commutative semi-ring commutative semiring compactness compelled edges complementary prior complete complete data complete data assumption complete data log likelihood complete link clustering completing the square composite likelihood compressed sensing compressive sensing computation tree computational learning theory computationalism concave concave-convex procedure concentration matrix concentration parameter concept concept learning condensation conditional entropy conditional gamma poisson conditional gaussian conditional independence conditional likelihood conditional logit model conditional probability conditional probability distribution conditional probability tables conditional random field conditional random fields conditional topic random field conditionally conjugate conditionally independent conditioning conditioning case conductance confidence interval confidence intervals confounder confounders confounding variable confusion matrix conjoint analysis conjugate gradients conjugate prior conjugate priors conjunctive normal form connectionism consensus sequence conservation of probability mass consistent consistent estimator consistent estimators constant symbols constraint satisfaction problems constraint-based approach content addressable memory context free grammar context specific independence context-specific independence contextual bandit contingency table continuation method contrastive divergence contrastive term control signal converge convex index to keywords convex belief propagation convex combination convex hull convolutional dbns convolutional neural nets convolutional neural network cooling schedule corpus correlated topic model correlation coefficient correlation matrix correspondence cosine similarity cost-benefit analysis coupled hmm covariance covariance graph covariance matrix covariance selection covariates cpd cpts cramer-rao inequality cramer-rao lower bound credible interval crf critical temperature critical value cross entropy cross over rate cross validation cross-entropy cross-language information retrieval crosscat crowd sourcing crp ctr cubic spline cumulant function cumulants cumulative distribution function curse of dimensionality curved exponential family cutting plane cv cycle cyclic permutation property d-prime d-separated dace dag damped updates damping dasher data association data augmentation data compression data fragmentation data fusion data overwhelms the prior data-driven mcmc data-driven proposals dbm dbn dcm dct death moves debiasing decision decision boundary decision diagram decision nodes decision problem decision procedure decision rule decision trees decoding decomposable decomposable graphs decomposes deebn deebns deep deep auto-encoders deep belief network deep boltzmann machine deep directed networks deep learning deep networks defender s fallacy deflated matrix degeneracy problem degenerate degree degrees of freedom deleted interpolation delta rule dendrogram denoising auto-encoder dense stereo reconstruction density estimation dependency network dependency networks derivative free filter descendants descriptive design matrix detailed balance detailed balance equations determinism deterministic annealing deviance dgm diagonal diagonal covariance lda diagonal lda diameter dictionary digamma digital cameras dimensionality reduction dirac delta function dirac measure dirchlet process direct posterior probability approach directed directed acyclic graph directed graphical model directed local markov property directed mixed graph directed mixed graphical model dirichlet dirichlet compound multinomial dirichlet distribution dirichlet multinomial regression lda dirichlet process dirichlet process mixture models discontinuity preserving discounted cumulative gain indexes discrete discrete adaboost discrete choice modeling discrete random variable discrete with probability one discretize discriminability discriminant analysis discriminant function discriminative discriminative classifier discriminative lda discriminative random field disease mapping disease transmission disparity dispersion parameter dissimilarity analysis dissimilarity matrix distance matrix distance transform distorted distortion distribute evidence distribute-from-root distributed encoding distributed representation distributional particles distributive law divisive clustering dna sequences do calculus document classification document classification domain adaptation domain adaptation dominates double loop algorithms double pareto distribution double sided exponential drum dual decomposition dual variables dummy encoding dyadic dybn dybns dynamic bayes net dynamic bayesian network dynamic linear model dynamic programming dynamic topic model e step e-commerce early stopping eb ecm ecme ecoc econometric forecasting economy sized svd edge appearance probability edges edit distance eer effective sample size efficient ipf efficiently pac-learnable eigendecomposition eigenfaces eigengap eigenvalue spectrum ekf elastic net elimination order em email spam filtering embedding empirical bayes empirical distribution empirical measure empirical risk empirical risk minimization end effector energy based models energy function energy functional ensemble ensemble learning ensemble learning entanglement entanglement problem entropy entropy ep epanechnikov kernel epca epigraph epistemological uncertainty epoch epsilon insensitive loss function epsr equal error rate equilibrium distribution equivalence class equivalent kernel equivalent sample size erf ergodic erlang distribution erm error bar error correcting codes error correction error function error signal error-correcting output codes ess essential graph estimated potential scale reduction estimator euclidean distance evidence evidence procedure evolutionary mcmc exchangeable exclusive or expectation correction expectation maximization expectation proagation expectation propagation expectation propagation expected complete data log likelihood expected profit expected sufficient statistics expected value explaining away explicit duration hmm exploration-exploitation index to keywords exploratory data analysis exponential cooling schedule exponential distribution exponential family exponential family harmonium exponential family pca exponential loss exponential power distribution extended kalman filter extension external field f score score fa face detection face detector face recognition facebook factor factor analysis factor analysis distance factor graph factor loading matrix factorial hmm factorial prior factors faithful false alarm false alarm rate false discovery rate false negative false positive false positive rate family family marginal fan-in fantasy data farthest point clustering fast fourier transform fast gauss transform fast ica fast iterative shrinkage thesholding algorithm fastslam fat hand fault diagnosis feature construction feature extraction feature function feature induction feature maps feature matrix feature selection feature-based clustering features feedback loops feedforward neural network ferro-magnets fft fields of experts fill-in edges filtering filtering finite difference matrix finite mixture model first-order logic fisher information fisher information matrix fisher kernel fisher scoring method fisher s linear discriminant analysis fista fit-predict cycle fixed effect fixed lag smoothing fixed point flat clustering flda flow cytometry folds forest forward stagewise additive modeling forward stagewise linear regression forwards kl forwards model forwards selection forwards-backwards forwards-backwards algorithm founder model founder variables fourier basis fraction of variance explained free energy free-form optimization frequent itemset mining frequentist frequentist statistics frobenius norm frustrated frustrated system full full conditional function approximation functional data analysis functional gradient descent furthest neighbor clustering fused lasso fuzzy clustering fuzzy set theory g-prior game against nature game theory gamma gamma distribution gamma function gap gap statistic gating function gauss-seidel gaussian gaussian approximation gaussian bayes net gaussian copulas gaussian graphical models gaussian kernel gaussian mixture model gaussian mrf gaussian process gaussian processes gaussian random fields gaussian rbm gaussian scale mixture gaussian sum filter gda gee gem gene finding gene finding gene knockout experiment indexes gene microarrays generalization generalization error generalization gradient generalize generalized additive model generalized belief propagation generalized cross validation generalized eigenvalue generalized em generalized estimating equations generalized linear mixed effects model generalized linear model generalized linear models generalized pseudo bayes filter generalized t distribution generate and test generative approach generative classifier generative pre-training generative weights genetic algorithms genetic linkage analysis genome genotype geometric distribution gibbs distribution gibbs sampler gibbs sampling gini index gist gittins indices glasso glauber dynamics glm glmm glmnet global balance equations global convergence global localization global markov property global minimum global prior parameter independence globally normalized gm gmm gp-lvm gps gpus gradient boosting gradient descent gram matrix grammars grandmother cells graph graph cuts graph laplacian graph surgery graph-guided fused lasso graphcuts graphical lasso graphical model graphical models xxviii gray code greatest common divisor greedy equivalence search ground network ground states group lasso grouping effect gumbel hadamard product haldane prior ham hamiltonian mcmc hammersley-clifford hamming distance handwriting recognition haplotype hard clustering hard em hard thresholding harmonic mean harmonium hastings correction hat matrix hdi heat bath heavy ball method heavy tails hellinger distance helmholtz free energy hessian heteroscedastic lda heuristics hidden hidden layer hidden markov model hidden nodes hidden semi-markov model hidden units hidden variable hidden variables hierarchical adaptive lasso hierarchical bayesian model hierarchical bayesian models hierarchical clustering hierarchical dirichlet process hierarchical hmm hierarchical latent class model hierarchical mixture of experts high throughput high variance estimators highest density interval highest posterior density hill climbing hindsight hinge loss hinton diagram hinton diagrams histogram hit rate hmm hmm filter hmms hoeffding s inequality homogeneous homotopy hopfield network horizon horn clauses hpd hsmm huber loss hugin hungarian algorithm hybrid mcmc hybrid monte carlo hybrid systems index to keywords hyper-parameters hypothesis space i-map i-projection ica ida identifiable identifiable in the limit iff iid ill-conditioned image classification image compression image denoising image inpainting image segmentation image tagging imm implicit feedback importance sampling importance weights impression log improper prior imputation imputation posterior in-degree inclusion probabilities incremental em independence sampler independent and identically distributed independent component analysis indicator function induced width induction inductive bias infer.net inference infinite hidden relational model infinite hmm infinite mixture models infinite relational model influence diagram influence model infomax information information arc information bottleneck information extraction information filter information form information gain information inequality information projection information retrieval information theory inheritance model inner approximation innovation inside outside inside-outside algorithm instance-based learning integrate out integrated likelihood integrated risk intensive care unit inter-causal reasoning interaction effects interactive multiple models interest point detector interpolate interpolated kneser-ney interpolator interval censored interventional data interventions intrinsic gaussian random field invariant invariant distribution invariant features inverse chi-squared distribution inverse gamma inverse gamma inverse gaussian inverse probability transform inverse problem inverse problems inverse reinforcement learning inverse wishart inverted index inverted indices ip ipf iris irls irm irreducible ising model isotropic iterated ekf iterative conditional modes iterative proportional fitting iterative scaling iterative shrinkage and thresholding algorithm iterative soft thresholding iteratively reweighted least squares jacobi jacobian jacobian matrix jags jambayes james stein estimator james-stein estimator jc penney jeffreys prior jeffreys-lindley paradox jensen s inequality jensen-shannon divergence jeopardy jittered jj bound joint distribution joint probability distribution jta jump markov linear system junction tree junction tree algorithm junction trees k-centers k-means algorithm k-means k-medoids algorothm k-spectrum kernel algorithm kalman filter kalman gain matrix kalman smoother kalman smoothing karhunen loeve karl popper kde kendall s kernel kernel density estimation kernel density estimator kernel function kernel machine kernel pca kernel regression kernel smoothing kernel trick kernelised feature vector kikuchi free energy kinect kinematic tracking kink kl divergence kleene star knee knn knots knowledge base knowledge discovery knowledge engineering kolmogorov smirnov kriging kronecker product kruskal s algorithm kullback-leibler divergence kurtosis l-bfgs pseudo-norm regularization loss regularization loss norm regularization label label bias label switching label taxonomy labeled lda lag lagrange multiplier lagrange multipliers lagrangian lagrangian relaxation lanczos algorithm language model language modeling language models laplace laplace approximation laplace distribution laplace s rule of succession lar large margin classifier large margin principle lars lasso latent indexes latent class model latent crf latent dirichlet allocation latent factors latent semantic analysis latent semantic indexing latent svms latent variable models lattice lauritzen-spiegelhalter lbp lda lda-hmm ldpc lds leaf leak node leapfrog steps learning learning curve learning rate learning to learn learning to rank least favorable prior least mean squares least squares least squares boosting leave one out cross validation leave-one out cross validation leaves left censored left-to-right left-to-right transition matrix leptokurtic letor level sets levenberg marquardt levinson-durbin lg-ssm likelihood likelihood equivalence likelihood equivalent likelihood principle likelihood ratio likelihood weighting limited memory bfgs limiting distribution line minimization line search linear discriminant analysis linear dynamical system linear gaussian linear gaussian system linear kernel linear program linear programming relaxtion linear regression linear smoother linear threshold unit linear trend linear-gaussian cpd linear-gaussian ssm linearity of expectation linearly separable link farms link function lisrel listnet lms index to keywords local consistency local evidence local level model local prior parameter independence local variational approximation localist encoding locally decodable locally normalized locally weighted regression loess log partition function log-linear log-loss log-odds ratio log-sum-exp logic sampling logical reasoning problems logistic logistic distribution logistic normal logistic regression logit logitboost long tail long tails loocv look-ahead rbpf loop loopy belief propagation lorentz loss loss function loss matrix loss-augmented decoding loss-calibrated inference lossy compression low density parity check low-level vision lowess lsa lse lsi lvm m step m-projection machine learning macro-averaged mahalanobis distance mammogram maneuvering target tracking manifest map estimate mar margin margin re-rescaling marginal distribution marginal likelihood marginal polytope marginalizing out marginally independent marker market basket analysis markov markov assumption markov blanket markov chain markov chain monte carlo markov chain monte carlo markov decision process markov equivalence markov equivalent markov logic network markov mesh markov model markov models markov network markov random field markov switching models mars mart master matching pursuit matching pursuits matern kernel matlab xxviii matrix completion matrix determinant lemma matrix factorization matrix inversion lemma matrix permanent matrix tree theorem max flowmin cut max margin markov networks max pooling max product linear programming max-product max-product belief propagation maxent maximal branching maximal clique maximal information coefficient maximal weight bipartite matching maximizer of the posterior marginals maximum a posteriori maximum entropy maximum entropy classifier maximum entropy markov model maximum expected utility principle maximum likelihood estimate maximum risk maximum weight spanning tree mcar mcem mcmc mdl mdp mds mean mean absolute deviation mean average precision mean field mean field energy functional mean function mean precision mean reciprocal rank mean squared error mechanical turk median median model memm memory-based learning mendelian inheritance mercer kernel mercer s theorem message passing metric metric crf metric mrf indexes metropolis hastings metropolis-hastings algorithm mfcc mh mi micro-averaged microsoft mini-batch minimal minimal i-map minimax rule minimum description length minimum entropy prior minimum mean squared error minimum spanning tree minorize-maximize misclassification loss misclassification rate misclassification rate missed detection missing missing at random missing completely at random missing data missing data problem mixed directed graphs mixed membership model mixed membership stochastic block model mixed model mixing matrix mixing time mixing weights mixture mixture density network mixture model mixture of conjugate priors mixture of experts mixture of factor analysers mixture of gaussians mixture of kalman filters mixture of trees mixture proposal mle mlp mm mmse mnist mobious numbers mode model based clustering model selection model selection consistent model-based approach xxvii model-based clustering moderated output modularity xxviii moe moment matching moment parameters moment projection momentum monks monte carlo monte carlo em monte carlo integration monte carlo localization moralization motes motif mpca mpe mpm mrf mse multi label classification multi net multi-armed bandit multi-class logistic regression multi-clust multi-grid techniques multi-information multi-label classification multi-layer perceptron multi-level model multi-level modeling multi-stage multi-target tracking multi-task feature selection multi-task learning multiclass classification multidimensional scaling multinomial multinomial coefficient multinomial logistic regression multinomial pca multinomial probit multinomial regression lda multinomial resampling multinoulli distribution multiple hypothesis testing multiple hypothesis tracking multiple imputation multiple kernel learning multiple lda multiple output model multiple random restarts multiple restarts multivariate adaptive regression splines multivariate bernoulli naive bayes multivariate delta method multivariate gamma function multivariate gamma function multivariate gaussian multivariate normal multivariate probit multivariate student t mutual information mutual inhibition mutually independent mvn n-best list n-gram n-gram models nadaraya-watson naive bayes classifier naive bayes classifiers named entity extraction nan nats natural exponential family natural gradient natural parameters ndcg nearest centroids classifier nearest medoid classification nearest neighbor nearest neighbor clustering nearest neighbor data association nearest shrunken centroids index to keywords negative binomial negative binomial distribution negative examples negative log likelihood negative transfer negentropy neighbors neocognitron nested plate nesterov s method netflix nettalk neural network neural networks neutral process newton s algorithm nhst niw nix nll nmar nmf no forgetting no free lunch theorem nodes nodes that fire together should wire together noise floor noisy-or nominal non-descendants non-factorial non-informative non-negative matrix factorization non-negative sparse coding non-null recurrent non-parametric bayes non-parametric bootstrap non-parametric bp non-parametric model non-parametric prior non-serial dynamic programming non-smooth non-terminals nonparanormal norm of a function normal normal equation normal gamma normal inverse chi-squared normal-inverse-wishart normalized cut normalized discounted cumulative gain normalized mutual information not missing at random noun phrase chunking np-complete np-hard classifier nuisance variables null hypothesis null hypothesis significance testing number game numerical underflow object detection object localization observation observation model observed data log likelihood observed information observed information matrix occam factor occam s razor occasionally dishonest casino occupancy grid octave xxviii offline oil wild-catter ols omp one-armed bandit one-hot encoding one-of-c encoding one-shot decision problem one-standard error rule one-step-ahead predictive density one-versus-one one-versus-the-rest one-vs-all online em online gradient descent online learning ontological uncertainty ontology open class open directory project open universe optimal action optimism of the training error optimization ordered markov property ordinal ordinal regression ordinal variables ordinary least squares ornstein-uhlenbeck process orthodox statistics orthogonal least squares orthogonal matching pursuits orthogonal projection out-degree out-of-clique query outer approximation outliers over-complete overcomplete overcounting number overdispersed overfit overfitting overrelaxed em algorithm p-value pac pagerank paired t-test pairwise independent pairwise markov property pairwise mrf parallel tempering parameter parameter expansion parameter modularity parameter sharing parameter tying parametric bootstrap parametric model parents pareto distribution part of speech indexes part of speech tagging partial dependence plot partial least squares partially directed acyclic graph partially labeled lda partially observed markov decision process partially observed mrf particle filtering particle filtering partition function partitional clustering partitioned inverse formula partitioning partitions of the integers parzen window density estimator passing a flow path path diagrams pathologies pattern pattern completion pattern recognition pattern search pca pcfg pdag pdf pedigree graph peeling algorithm pegasos penalized least squares penalized log likelihood penalized splines penetrance model perception-action perceptron perceptron algorithm perceptual aliasing perfect intervention perfect map period permanent perplexity persistent cd persistent contrastive divergence personalized recommendation personalized spam filtering perturbation theory phase phase transition phenotypes phone phonemes phylogenetic hmm phylogenetic tree piecewise polynomial pilot runs pipeline pitman-koopman-darmois theorem pitman-yor process plackett-luce plates platykurtic pls plsi plug-in plug-in approximation plutocracies pmf pmtk xxviii point estimate pointwise approach pointwise marginal credibility intervals pointwise mutual information poisson poisson regression polar policy polya urn polyak-ruppert averaging polynomial kernel polynomial regression polynomial time approximation schemes polysemy polytree pomdp pooled pooled empirical variance population minimizer positive definite positive definite kernel positive examples posterior expected loss posterior mean posterior median posterior mode posterior predictive density posterior predictive distribution potential function potts model power law power method ppca precision precision at k precision matrix precision recall curve predict-update cycle predict-update-project predictive preferences preposterior risk prevalence prim s algorithm primal variables principal component principal components principal components analysis principal components regression principle of insufficient reason probabilistic decision tree probabilistic expert system probabilistic inference probabilistic latent semantic indexing probabilistic matrix factorization probabilistic pca probabilistic principal components analysis probabilistic relational modeling probability density function probability mass function probability of the evidence probability product kernel probability simplex probability theory xxvii probably approximately correct probe probit probit regression product of experts product rule index to keywords production rules profile hmm profile log likelihood projected gradient descent projection projection pursuit prolog proposal distribution propose prosecutor s fallacy protein sequence alignment protein-protein interaction networks prototype proximal operator pruning pseudo counts pseudo likelihood pseudo marginals pseudo random number generator pseudo-likelihood pure purity pushing sums inside products pyramid match kernel qaly qmr qp qq-plot qr decomposition quadratic discriminant analysis quadratic loss quadratic program quantile quantize quartiles quasi-newton query logs query variables quick medical reference radar radial basis function rand index random accelerations model random effects random effects mixture of experts random forests random probability measure random utility model random walk metropolis algorithm random walk on the integers random walk proposal rank correlation rank one update ranking ranknet rao-blackwell rao-blackwellisation rao-blackwellised particle filtering rao-blackwellized particle filtering rare event rate rational behavior rbf rbf kernel rbf network rbm rbpf real adaboost recall receiver operating characteristic receptive fields recognition weights recombination model reconstruction error recurrent recurrent neural network recurrent neural networks recursive recursive least squares reflecting pair regime switching regime switching markov model regression regression spline regret regular regularization regularization path regularized discriminant analysis regularized estimation regularized particle filter regularized risk minimization reinforcement learning reject action rejection sampling rejuvenation relation relational probabilistic models relational topic model relative entropy relative importance of predictor variables relative risk relevance network relevance vector machine rephil replicated softmax model representer theorem reproducing kernel hilbert space reproducing property rerank resample-move residual residual analysis residual belief propagation residual error residual resampling residual sum of squares response variable responsibility restricted boltzmann machine reverse kl reversible jump mcmc reward ricatti equations rich get richer ridge regression right censored risk risk averse rjmcmc rkhs rls robbins-monro robust robust priors robustness roc rocking indexes root root mean square error rosenblatt rotamers rts smoother rule of iterated expectation rule of total probability rules rum running intersection property rvm saddle point approximation sample impoverishment sample standard deviation samples sampling distribution sampling importance resampling sampling period satisfying assignment saturated model sbl scalar product scale invariant prior scale of evidence scatter plot scfgs schedule schur complement scientific method scope score function score matching score vector scores scree plot screening search engine optimization second order second order markov chain second-order markov model self loops semantic hashing semantic network semantic role labeling semi-conjugate semi-continuous hmm semi-markov model semi-metric semi-parametric model semi-supervised semi-supervised embedding semi-supervised learning sensible pca sensitivity sensitivity analysis sensor fusion sentiment analysis separating set separation oracle sequence logo sequential sequential minimal optimization sequential trbp sgd shafer-shenoy shallow parsing shared sherman-morrison-woodbury formula shooting shrinkage shrinkage estimation shrinkage factor side chains side information sift sifting property sigma points sigmoid sigmoid belief net sigmoid belief nets sigmoid kernel signal detection theory signal processing signal-to-noise ratio signal-to-symbol similar similarity-based clustering simple cells simple linear regression simplex factor model simpon s paradox simulated annealing simulated annealing simulation based simultaneous localization and mapping single best replacement single link clustering single site updating singular value decomposition singular values sir size principle skewness skip arcs skip-chain crf slack re-scaling slack variables slam slaves slice sampling sliding window detector slippage slot machine small n large d smartass sml smo smoothing smoothing kernel smoothing splines social networks soft clustering soft margin constraints soft thresholding soft weight sharing softmax source coding spam spam spanning tree polytope sparsa sparse sparse bayesian learning sparse boosting sparse coding sparse data problem sparse kernel machine sparse matrix factorization sparse pca index to keywords sparse representation sparse vector machine sparsity sparsity-promoting prior spectral spectral clustering spectral graph theory speech recognition sphereing spherical spike and slab spin spline split merge split variable square root filter squared error squared exponential kernel squared loss squashing function ssm ssvms stability selection stable stacked denoising auto-encoder stacking standard deviation standard error standard error of the mean standard errors standard model standard normal standard overcomplete representation standardized standardizing state state estimation state space state space model state transition diagram state transition matrix stationary stationary distribution statistical learning theory statistical relational ai statistical relational learning statistically significant steepest descent stein s paradox stemming step size stepping out stepwise em stick-breaking construction sticky stochastic algorithm stochastic approximation stochastic approximation em stochastic automaton stochastic block model stochastic context free grammars stochastic em stochastic gradient boosting stochastic gradient descent stochastic matrix stochastic maximum likelihood stochastic optimization stochastic process stochastic processes stochastic search stochastic volatility stop words stopping rule stratified cv stratified sampling streaming data streetview strict strictly convex string kernel strong local optimum strong sampling assumption structural em structural equation model structural equation models structural error structural risk minimization structural signatures structural support vector machines structural time series structural zeros structure learning structured mean field structured output structured perceptron algorithm structured-output classification problems student t student t distribution sub-gaussian subderivative subdifferential subgradient subgraph subjective subjective probability submodular subsampling subspace method sufficiency principle sufficient statistics suffix trees sum of squared errors sum of squares sum rule sum-product sum-product algorithm super efficient super-gaussian supermodular supervised lda supervised learning supervised pca support support vector machine support vector machines support vectors surrogate loss surrogate loss function surrogate splits survival of the fittest suspicious coincidence suspicious coincidences svd svm svmstruct swendsen wang switching linear dynamical system switching state space model symbol grounding symmetric synchronous updates syntactic sugar synthesis view systematic resampling systems biology systems identification systolic array t statistic t-test tabula rasa tail area probabilities tail area probability tan tasa taylor series taylor series expansion taylor s theorem temperature template template matching tensor product tensor product basis terminals test statistic tf-idf thin junction tree filter thin junction trees thin plate spline thin svd thinning thompson sampling tied tied-mixture hmm tikhonov regularization time reversible time-invariant time-series forecasting tobit model toeplitz tokens topic topic model topological ordering total ordering trace trace plot trace trick traceback tracking tracking by detection tractable substructure trail training set trans-dimensional mcmc transfer function transfer learning transient transition matrix transition model translation invariance translation invariant translation invariant prior trbp trbp-s tree tree ep tree reparameterization tree reweighted belief propagation tree-augmented naive bayes classifier indexes treewidth trellis trellis diagram tri-cube kernel triangle inequality triangulated tridiagonal trigram model true positive rate trueskill truncated gaussian truncated gaussian potential truncated newton truncated svd trw trw-s tube tuples turbo codes two-filter smoothing two-slice marginal type i type i error rate type ii maximum likelihood type-ii maximum likelihood u-shaped curve ucb ugm ukf unbiased uncertainty unclamped phase unclamped term unconditionally independent underfits undirected undirected graphical model undirected local markov property unfaithful unidentifiable unified medical language system uniform distribution unigram statistics unigrams uninformative union bound unit information prior universal approximator unk unknown unrolled unscented kalman filter unscented particle filter unscented transform unstable unsupervised learning up-down user rating profile utilities utility function utility nodes v-structure validation set value nodes value of perfect information vanishing gradient vapnik-chervonenkis var zipf s law index to keywords variable duration hmm variable elimination variance variance stabilizing transform variation of information variational bayes variational bayes em variational em variational free energy variational inference variational message passing varimax vb vbem vc vc dimension vector auto-regressive vector quantization version space vertices vibes views visible visible nodes visible variables visual words visualizing viterbi viterbi decoding viterbi training vmp voronoi tessellation vq wald wald interval warm starting warp watson wavelet wavelet transforms weak conditionality weak learner weak marginalization web crawling web spam weight decay weight function weight vector weighted approximate-rank pairwise weighted average weighted least squares weighted least squares problem whitening whitening widrow-hoff rule wishart working response world health organization wrapper method xbox xor zellner s g-prior zero avoiding zero count problem zero forcing zero temperature limit zig-zag