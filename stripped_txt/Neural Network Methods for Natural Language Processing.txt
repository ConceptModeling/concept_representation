series issn g o l d b e r g series editor graeme hirst university of toronto neural network methods for natural language processing yoav goldberg bar ilan university neural networks are a family of powerful machine learning models. this book focuses on the application of neural network models to natural language data. the first half of the book i and ii covers the basics of supervised machine learning and feed-forward neural networks the basics of working with machine learning over language data and the use of vector-based rather than symbolic representations for words. it also covers the computation-graph abstraction which allows to easily define and train arbitrary neural networks and is the basis behind the design of contemporary neural network software libraries. the second part of the book iii and iv introduces more specialized neural network architectures including convolutional neural networks recurrent neural networks conditionedgeneration models and attention-based models. these architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation syntactic parsing and many other applications. finally we also discuss tree-shaped networks structured prediction and the prospects of multi-task learning. about synthesis this volume is a printed version of a work that appears in the synthesis digital library of engineering and computer science. synthesis lectures provide concise original presentations of important research and development topics published quickly in digital and print formats. for more information visit our website httpstore.morganclaypool.com store.morganclaypool.com f o r n a t u r a l l a n g u a g e p r o c e s s i n g n e u r a l n e t w o r k m e t h o d s m o r g a n c l a y p o o l neural network methods for natural language processing synthesis lectures on human language technologies editor graeme hirst university of toronto synthesis lectures on human language technologies is edited by graeme hirst of the university of toronto. e series consists of to monographs on topics relating to natural language processing computational linguistics information retrieval and spoken language understanding. emphasis is on important new techniques on new applications and on topics that combine two or more hlt subfields. neural network methods for natural language processing yoav goldberg syntax-based statistical machine translation philip williams rico sennrich matt post and philipp koehn domain-sensitive temporal tagging jannik str tgen and michael gertz linked lexical knowledge bases foundations and applications iryna gurevych judith eckle-kohler and michael matuschek bayesian analysis in natural language processing shay cohen metaphor a computational perspective tony veale ekaterina shutova and beata beigman klebanov grammatical inference for computational linguistics jeffrey heinz colin de la higuera and menno van zaanen iii automatic detection of verbal deception eileen fitzpatrick joan bachenko and tommaso fornaciari natural language processing for social media atefeh farzindar and diana inkpen semantic similarity from natural language and ontology analysis s bastien harispe sylvie ranwez stefan janaqi and jacky montmain learning to rank for information retrieval and natural language processing second edition hang li ontology-based interpretation of natural language philipp cimiano christina unger and john mccrae automated grammatical error detection for language learners second edition claudia leacock martin chodorow michael gamon and joel tetreault web corpus construction roland sch fer and felix bildhauer recognizing textual entailment models and applications ido dagan dan roth mark sammons and fabio massimo zanzotto linguistic fundamentals for natural language processing essentials from morphology and syntax emily m. bender semi-supervised learning and domain adaptation in natural language processing anders s gaard semantic relations between nominals vivi nastase preslav nakov diarmuid s aghdha and stan szpakowicz iv computational modeling of narrative inderjeet mani natural language processing for historical texts michael piotrowski sentiment analysis and opinion mining bing liu discourse processing manfred stede bitext alignment j rg tiedemann linguistic structure prediction noah a. smith learning to rank for information retrieval and natural language processing hang li computational modeling of human language acquisition afra alishahi introduction to arabic natural language processing nizar y. habash cross-language information retrieval jian-yun nie automated grammatical error detection for language learners claudia leacock martin chodorow michael gamon and joel tetreault data-intensive text processing with mapreduce jimmy lin and chris dyer v semantic role labeling martha palmer daniel gildea and nianwen xue spoken dialogue systems kristiina jokinen and michael mctear introduction to chinese natural language processing kam-fai wong wenjie li ruifeng xu and zheng-sheng zhang introduction to linguistic annotation and text analytics graham wilcock dependency parsing sandra k bler ryan mcdonald and joakim nivre statistical language models for information retrieval chengxiang zhai copyright by morgan claypool all rights reserved. no part of this publication may be reproduced stored in a retrieval system or transmitted in any form or by any means electronic mechanical photocopy recording or any other except for brief quotations in printed reviews without the prior permission of the publisher. neural network methods for natural language processing yoav goldberg www.morganclaypool.com isbn isbn paperback ebook doi a publication in the morgan claypool publishers series synthesis lectures on human language technologies lecture series editor graeme hirst university of toronto series issn print electronic neural network methods for natural language processing yoav goldberg bar ilan university synthesis lectures on human language technologies cmclaypoolmorganpublishers abstract neural networks are a family of powerful machine learning models. is book focuses on the application of neural network models to natural language data. e first half of the book i and ii covers the basics of supervised machine learning and feed-forward neural networks the basics of working with machine learning over language data and the use of vector-based rather than symbolic representations for words. it also covers the computation-graph abstraction which allows to easily define and train arbitrary neural networks and is the basis behind the design of contemporary neural network software libraries. e second part of the book iii and iv introduces more specialized neural network architectures including convolutional neural networks recurrent neural networks conditioned-generation models and attention-based models. ese architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation syntactic parsing and many other applications. finally we also discuss tree-shaped networks structured prediction and the prospects of multi-task learning. keywords natural language processing machine learning supervised learning deep learning neural networks word embeddings recurrent neural networks sequence to sequence models contents ix preface xvii acknowledgments. xxi introduction e challenges of natural language processing. neural networks and deep learning deep learning in nlp success stories. coverage and organization what s not covered a note on terminology mathematical notation part i supervised classification and feed-forward neural networks learning basics and linear models supervised learning and parameterized functions train test and validation sets linear models binary classification log-linear binary classification multi-class classification representations one-hot and dense vector representations log-linear multi-class classification. training as optimization loss functions regularization x gradient-based optimization stochastic gradient descent worked-out example beyond sgd from linear models to multi-layer perceptrons limitations of linear models e xor problem nonlinear input transformations kernel methods trainable mapping functions feed-forward neural networks a brain-inspired metaphor in mathematical notation representation power. common nonlinearities. loss functions regularization and dropout similarity and distance layers embedding layers neural network training e computation graph abstraction forward computation backward computation backprop software. implementation recipe network composition practicalities choice of optimization algorithm initialization restarts and ensembles vanishing and exploding gradients saturation and dead neurons shu ing learning rate minibatches xi part ii working with natural language data features for textual data typology of nlp classification problems features for nlp problems directly observable properties inferred linguistic properties core features vs. combination features ngram features distributional features case studies of nlp features document classification language identification document classification topic classification document classification authorship attribution word-in-context part of speech tagging. word-in-context named entity recognition word in context linguistic features preposition sense disambiguation relation between words in context arc-factored parsing. from textual features to inputs encoding categorical features one-hot encodings dense encodings embeddings dense vectors vs. one-hot representations combining dense vectors window-based features variable number of features continuous bag of words relation between one-hot and dense vectors odds and ends distance and position features padding unknown words and word dropout feature combinations vector sharing. dimensionality embeddings vocabulary network s output xii example part-of-speech tagging. example arc-factored parsing language modeling e language modeling task evaluating language models perplexity traditional approaches to language modeling further reading limitations of traditional language models. neural language models using language models for generation byproduct word representations pre-trained word representations random initialization supervised task-specific pre-training unsupervised pre-training using pre-trained embeddings word embedding algorithms distributional hypothesis and word representations. from neural language models to distributed representations connecting the worlds other algorithms e choice of contexts window approach sentences paragraphs or documents syntactic window multilingual. character-based and sub-word representations dealing with multi-word units and word inflections limitations of distributional methods using word embeddings obtaining word vectors word similarity word clustering xiii finding similar words similarity to a group of words odd-one out short document similarity word analogies retrofitting and projections practicalities and pitfalls case study a feed-forward architecture for sentence meaning inference natural language inference and the snli dataset a textual similarity network part iii specialized architectures ngram detectors convolutional neural networks. basic convolution pooling convolutions over text vector pooling variations. alternative feature hashing hierarchical convolutions recurrent neural networks modeling sequences and stacks e rnn abstraction rnn training common rnn usage-patterns acceptor encoder transducer bidirectional rnns multi-layer rnns rnns for representing stacks a note on reading the literature xiv concrete recurrent neural network architectures cbow as an rnn simple rnn. gated architectures lstm gru other variants dropout in rnns modeling with recurrent networks acceptors sentiment classification subject-verb agreement grammaticality detection rnns as feature extractors. part-of-speech tagging. rnn cnn document classification arc-factored dependency parsing conditioned generation rnn generators training generators conditioned generation sequence to sequence models applications other conditioning contexts. unsupervised sentence similarity conditioned generation with attention computational complexity interpretability attention-based models in nlp machine translation morphological inflection syntactic parsing xv part iv additional topics modeling trees with recursive neural networks formal definition. extensions and variations training recursive neural networks a simple alternative linearized trees. outlook structured output prediction search-based structured prediction structured prediction with linear models nonlinear structured prediction probabilistic objective approximate search reranking see also greedy structured prediction conditional generation as structured output prediction examples search-based structured prediction first-order dependency parsing neural-crf for named entity recognition approximate ner-crf with beam-search cascaded multi-task and semi-supervised learning model cascading multi-task learning training in a multi-task setup selective sharing word-embeddings pre-training as multi-task learning multi-task learning in conditioned generation multi-task learning as regularization caveats semi-supervised learning examples gaze-prediction and sentence compression arc labeling and syntactic parsing xvi preposition sense disambiguation and preposition translation prediction conditioned generation multilingual machine translation parsing and image captioning outlook conclusion what have we seen? e challenges ahead bibliography author s biography preface xvii natural language processing is a collective term referring to automatic computational processing of human languages. is includes both algorithms that take human-produced text as input and algorithms that produce natural looking text as outputs. e need for such algorithms is ever increasing human produce ever increasing amounts of text each year and expect computer interfaces to communicate with them in their own language. natural language processing is also very challenging as human language is inherently ambiguous ever changing and not well defined. natural language is symbolic in nature and the first attempts at processing language were symbolic based on logic rules and ontologies. however natural language is also highly ambiguous and highly variable calling for a more statistical algorithmic approach. indeed the currentday dominant approaches to language processing are all based on statistical machine learning. for over a decade core nlp techniques were dominated by linear modeling approaches to supervised learning centered around algorithms such as perceptrons linear support vector machines and logistic regression trained over very high dimensional yet very sparse feature vectors. around the field has started to see some success in switching from such linear models over sparse inputs to nonlinear neural network models over dense inputs. some of the neuralnetwork techniques are simple generalizations of the linear models and can be used as almost drop-in replacements for the linear classifiers. others are more advanced require a change of mindset and provide new modeling opportunities. in particular a family of approaches based on recurrent neural networks alleviates the reliance on the markov assumption that was prevalent in sequence models allowing to condition on arbitrarily long sequences and produce effective feature extractors. ese advances led to breakthroughs in language modeling automatic machine translation and various other applications. while powerful the neural network methods exhibit a rather strong barrier of entry for various reasons. in this book i attempt to provide nlp practitioners as well as newcomers with the basic background jargon tools and methodologies that will allow them to understand the principles behind neural network models for language and apply them in their own work. i also hope to provide machine learning and neural network practitioners with the background jargon tools and mindset that will allow them to effectively work with language data. finally i hope this book can also serve a relatively gentle somewhat incomplete introduction to both nlp and machine learning for people who are newcomers to both fields. xviii preface intended readership is book is aimed at readers with a technical background in computer science or a related field who want to get up to speed with neural network techniques for natural language processing. while the primary audience of the book is graduate students in language processing and machine learning i made an effort to make it useful also to established researchers in either nlp or machine learning including some advanced material and to people without prior exposure to either machine learning or nlp covering the basics from the grounds up. is last group of people will obviously need to work harder. while the book is self contained i do assume knowledge of mathematics in particular undergraduate level of probability algebra and calculus as well as basic knowledge of algorithms and data structures. prior exposure to machine learning is very helpful but not required. is book evolved out of a survey paper which was greatly expanded and somewhat re-organized to provide a more comprehensive exposition and more in-depth coverage of some topics that were left out of the survey for various reasons. is book also contains many more concrete examples of applications of neural networks to language data that do not exist in the survey. while this book is intended to be useful also for people without nlp or machine learning backgrounds the survey paper assumes knowledge in the field. indeed readers who are familiar with natural language processing as practiced between roughly and with heavy reliance on machine learning and linear models may find the journal version quicker to read and better organized for their needs. however such readers may also appreciate reading the chapters on word embeddings and the chapter on conditioned generation with rnns and the chapters on structured prediction and multi-task learning and focus of this book is book is intended to be self-contained while presenting the different approaches under a unified notation and framework. however the main purpose of the book is in introducing the neuralnetworks machinery and its application to language data and not in providing an in-depth coverage of the basics of machine learning theory and natural language technology. i refer the reader to external sources when these are needed. likewise the book is not intended as a comprehensive resource for those who will go on and develop the next advances in neural network machinery it may serve as a good entry point. rather it is aimed at those readers who are interested in taking the existing useful technology and applying it in useful and creative ways to their favorite language-processing problems. preface xix further reading for in-depth general discussion of neural networks the theory behind them advanced optimization methods and other advanced topics the reader is referred to other existing resources. in particular the book by bengio et al. is highly recommended. for a friendly yet rigorous introduction to practical machine learning the freely available book of daum iii is highly recommended. for more theoretical treatment of machine learning see the freely available textbook of shalev-shwartz and ben-david and the textbook of mohri et al. for a strong introduction to nlp see the book of jurafsky and martin e information retrieval book by manning et al. also contains relevant information for working with language data. finally for getting up-to-speed with linguistic background the book of bender in this series provides a concise but comprehensive coverage directed at computationally minded readers. e first chapters of the introductory grammar book by sag et al. are also worth reading. as of this writing the progress of research in neural networks and deep learning is very fast paced. e state-of-the-art is a moving target and i cannot hope to stay up-to-date with the latest-and-greatest. e focus is thus with covering the more established and robust techniques that were proven to work well in several occasions as well as selected techniques that are not yet fully functional but that i find to be established andor promising enough for inclusion. yoav goldberg march acknowledgments xxi is book grew out of a survey paper i ve written on the topic which in turn grew out of my frustration with the lack organized and clear material on the intersection of deep learning and natural language processing as i was trying to learn it and teach it to my students and collaborators. i am thus indebted to the numerous people who commented on the survey paper its various forms from initial drafts to post-publication comments as well as to the people who commented on various stages of the book s draft. some commented in person some over email and some in random conversations on twitter. e book was also influenced by people who did not comment on it per-se some never read it but discussed topics related to it. some are deep learning experts some are nlp experts some are both and others were trying to learn both topics. some contributed through very detailed comments others by discussing small details others in between. but each of them influenced the final form of the book. ey are in alphabetical order yoav artzi yonatan aumann jason baldridge miguel ballesteros mohit bansal marco baroni tal baumel sam bowman jordan boyd-graber chris brockett mingwei chang david chiang kyunghyun cho grzegorz chrupala alexander clark raphael cohen ryan cotterell hal daum iii nicholas dronen chris dyer jacob eisenstein jason eisner michael elhadad yad faeq manaal faruqui amir globerson fr deric godin edward grefenstette matthew honnibal dirk hovy moshe koppel angeliki lazaridou tal linzen ang luong chris manning stephen merity paul michel margaret mitchell piero molino graham neubig joakim nivre brendan o connor nikos pappas fernando pereira barbara plank ana-maria popescu delip rao tim rockt schel dan roth alexander rush naomi saphra djam seddah erel segal-halevi avi shmidman shaltiel shmidman noah smith anders s gaard abe stanway emma strubell sandeep subramanian liling tan reut tsarfaty peter turney tim vieira oriol vinyals andreas vlachos wenpeng yin and torsten zesch. e list excludes of course the very many researchers i ve communicated with through their academic writings on the topic. e book also benefited a lot from and was shaped by my interaction with the natural language processing group at bar-ilan university its soft extensions yossi adi roee aharoni oded avraham ido dagan jessica ficler jacob goldberger hila gonen joseph keshet eliyahu kiperwasser ron konigsberg omer levy oren melamud gabriel stanovsky ori shapira micah shlain vered shwartz hillel taub-tabib and rachel wities. most of them belong in both lists but i tried to keep things short. e anonymous reviewers of the book and the survey paper while unnamed sometimes annoying provided a solid set of comments suggestions and corrections which i can safely say dramatically improved many aspects of the final product. anks whoever you are! xxii acknowledgments and thanks also to graeme hirst michael morgan samantha draper and c.l. tondo for orchestrating the effort. as usual all mistakes are of course my own. do let me know if you find any though and be listed in the next edition if one is ever made. finally i would like to thank my wife noa who was patient and supportive when i disappeared into writing sprees my parents esther and avner and brother nadav who were in many cases more excited about the idea of me writing a book than i was and the staff at e streets cafe george branch and shne or cafe who kept me well fed and served me drinks throughout the writing process with only very minimal distractions. yoav goldberg march c h a p t e r introduction the challenges of natural language processing natural language processing is the field of designing methods and algorithms that take as input or produce as output unstructured natural language data. human language is highly ambiguous the sentence i ate pizza with friends and compare it to i ate pizza with olives and also highly variable core message of iatepizzawithfriends can also be expressed as friendsandisharedsomepizza. it is also ever changing and evolving. people are great at producing language and understanding language and are capable of expressing perceiving and interpreting very elaborate and nuanced meanings. at the same time while we humans are great users of language we are also very poor at formally understanding and describing the rules that govern language. understandingand producinglanguageusing computersis thus highly challenging.indeed the best known set of methods for dealing with language data are using supervised machine learning algorithms that attempt to infer usage patterns and regularities from a set of pre-annotated input and output pairs. consider for example the task of classifying a document into one of four categories s p g and e obviously the words in the documents provide very strong hints but which words provide what hints? writing up rules for this task is rather challenging. however readers can easily categorize a document into its topic and then based on a few hundred human-categorized examples in each category let a supervised machine learning algorithm come up with the patterns of word usage that help categorize the documents. machine learning methods excel at problem domains where a good set of rules is very hard to define but annotating the expected output for a given input is relatively simple. besides the challenges of dealing with ambiguous and variable inputs in a system with illdefined and unspecified set of rules natural language exhibits an additional set of properties that make it even more challenging for computational approaches including machine learning it is discrete compositional and sparse. language is symbolic and discrete. e basic elements of written language are characters. characters form words that in turn denote objects concepts events actions and ideas. both characters and words are discrete symbols words such as hamburger or pizza each evoke in us a certain mental representations but they are also distinct symbols whose meaning is external to them and left to be interpreted in our heads. ere is no inherent relation between hamburger and pizza that can be inferred from the symbols themselves or from the individual letters they introduction are made of. compare that to concepts such as color prevalent in machine vision or acoustic signals these concepts are continuous allowing for example to move from a colorful image to a gray-scale one using a simple mathematical operation or to compare two different colors based on inherent properties such as hue and intensity. is cannot be easily done with words there is no simple operation that will allow us to move from the word red to the word pink without using a large lookup table or a dictionary. language is also compositional letters form words and words form phrases and sentences. e meaning of a phrase can be larger than the meaning of the individual words that comprise it and follows a set of intricate rules. in order to interpret a text we thus need to work beyond the level of letters and words and look at long sequences of words such as sentences or even complete documents. e combination of the above properties leads to data sparseness. e way in which words symbols can be combined to form meanings is practically infinite. e number of possible valid sentences is tremendous we could never hope to enumerate all of them. open a random book and the vast majority of sentences within it you have not seen or heard before. moreover it is likely that many sequences of four-words that appear in the book are also novel to you. if you were to look at a newspaper from just years ago or imagine one years in the future many of the words in particular names of persons brands and corporations but also slang words and technical terms will be novel as well. ere is no clear way of generalizing from one sentence to another or defining the similarity between sentences that does not depend on their meaning which is unobserved to us. is is very challenging when we come to learn from examples even with a huge example set we are very likely to observe events that never occurred in the example set and that are very different than all the examples that did occur in it. neural networks and deep learning deep learning is a branch of machine learning. it is a re-branded name for neural networks a family of learning techniques that was historically inspired by the way computation works in the brain and which can be characterized as learning of parameterized differentiable mathematical functions. e name deep-learning stems from the fact that many layers of these differentiable function are often chained together. while all of machine learning can be characterized as learning to make predictions based on past observations deep learning approaches work by learning to not only predict but also to correctly represent the data such that it is suitable for prediction. given a large set of desired inputoutput mapping deep learning approaches work by feeding the data into a network that produces successive transformations of the input data until a final transformation predicts the output. e transformations produced by the network are learned from the given input-output mappings such that each transformation makes it easier to relate the data to the desired label. in this book we take the mathematical view rather than the brain-inspired view. deep learning in nlp while the human designer is in charge of designing the network architecture and training regime providing the network with a proper set of input-output examples and encoding the input data in a suitable way a lot of the heavy-lifting of learning the correct representation is performed automatically by the network supported by the network s architecture. deep learning in nlp neural networks provide a powerful learning machinery that is very appealing for use in natural language problems. a major component in neural networks for language is the use of an embedding layer a mapping of discrete symbols to continuous vectors in a relatively low dimensional space. when embedding words they transform from being isolated distinct symbols into mathematical objects that can be operated on. in particular distance between vectors can be equated to distance between words making it easier to generalize the behavior from one word to another. is representation of words as vectors is learned by the network as part of the training process. going up the hierarchy the network also learns to combine word vectors in a way that is useful for prediction. is capability alleviates to some extent the discreteness and data-sparsity problems. ere are two major kinds of neural network architectures that can be combined in various ways feed-forward networks and recurrentrecursive networks. feed-forward networks in particular multi-layer perceptrons allow to work with fixed sized inputs or with variable length inputs in which we can disregard the order of the elements. when feeding the network with a set of input components it learns to combine them in a meaningful way. mlps can be used whenever a linear model was previously used. e nonlinearity of the network as well as the ability to easily integrate pre-trained word embeddings often lead to superior classification accuracy. convolutional feed-forward networks are specialized architectures that excel at extracting local patterns in the data they are fed arbitrarily sized inputs and are capable of extracting meaningful local patterns that are sensitive to word order regardless of where they appear in the input. ese work very well for identifying indicative phrases or idioms of up to a fixed length in long sentences or documents. recurrent neural networks are specialized models for sequential data. ese are network components that take as input a sequence of items and produce a fixed size vector that summarizes that sequence. as summarizing a sequence means different things for different tasks the information needed to answer a question about the sentiment of a sentence is different from the information needed to answer a question about its grammaticality recurrent networks are rarely used as standalone component and their power is in being trainable components that can be fed into other network components and trained to work in tandem with them. for example the output of a recurrent network can be fed into a feed-forward network that will try to predict some value. e recurrent network is used as an input-transformer that is trained to produce informative representations for the feed-forward network that will operate on top of it. recurrent networks are very impressive models for sequences and are arguably the most exciting introduction offer of neural networks for language processing. ey allow abandoning the markov assumption that was prevalent in nlp for decades and designing models that can condition on entire sentences while taking word order into account when it is needed and not suffering much from statistical estimation problems stemming from data sparsity. is capability leads to impressive gains in language-modeling the task of predicting the probability of the next word in a sequence equivalently the probability of a sequence which is a cornerstone of many nlp applications. recursive networks extend recurrent networks from sequences to trees. many of the problems in natural language are structured requiring the production of complex output structures such as sequences or trees and neural network models can accommodate that need as well either by adapting known structured-prediction algorithms for linear models or by using novel architectures such as sequence-to-sequence models which we refer to in this book as conditioned-generation models. such models are at the heart of stateof-the-art machine translation. finally many language prediction tasks are related to each other in the sense that knowing to perform one of them will help in learning to perform the others. in addition while we may have a shortage of supervised training data we have ample supply of raw text data. can we learn from related tasks and un-annotated data? neural network approaches provide exciting opportunities for both mtl from related problems and semi-supervised learning from external unannotated data. success stories fully connected feed-forward neural networks can for the most part be used as a drop-in replacement wherever a linear learner is used. is includes binary and multi-class classification problems as well as more complex structured prediction problems. e nonlinearity of the network as well as the ability to easily integrate pre-trained word embeddings often lead to superior classification accuracy. a series of works managed to obtain improved syntactic parsing results by simply replacing the linear model of a parser with a fully connected feed-forward network. straightforward applications of a feed-forward network as a classifier replacement coupled with the use of pre-trained word vectors provide benefits for many language tasks including the very well basic task of language modeling ccg supertagging dialog state tracking and pre-ordering for statistical machine translation. iyyer et al. demonstrate that multi-layer feed-forward networks can provide competitive results on sentiment classification and factoid question answering. zhou et al. and andor et al. integrate them in a beam-search structured-prediction system achieving stellar accuracies on syntactic parsing sequence tagging and other tasks. and manning durrett and klein pei et al. weiss et al. see chapter as well as bengio et al. vaswani et al. and steedman et al. gispert et al. deep learning in nlp networks with convolutional and pooling layers are useful for classification tasks in which we expect to find strong local clues regarding class membership but these clues can appear in different places in the input. for example in a document classification task a single key phrase an ngram can help in determining the topic of the document and zhang we would like to learn that certain sequences of words are good indicators of the topic and do not necessarily care where they appear in the document. convolutional and pooling layers allow the model to learn to find such local indicators regardless of their position. convolutional and pooling architecture show promising results on many tasks including document classification shorttext categorization sentiment classification relation-type classification between entities event detection paraphrase identification semantic role labeling question answering predicting box-office revenues of movies based on critic reviews modeling text interestingness and modeling the relation between character-sequences and part-of-speech tags. in natural language we often work with structured data of arbitrary sizes such as sequences and trees. we would like to be able to capture regularities in such structures or to model similarities between such structures. recurrent and recursive architectures allow working with sequences and trees while preserving a lot of the structural information. recurrent networks are designed to model sequences while recursive networks and k chler are generalizations of recurrent networks that can handle trees. recurrent models have been shown to produce very strong results for language modeling as well as for sequence tagging machine translation parsing and many other tasks including noisy text normalization dialog state tracking response generation and modeling the relation between character sequences and part-of-speech tags. and zhang et al. et al. kim santos et al. zeng et al. et al. nguyen and grishman and sch tze et al. et al. and cohn et al. santos and zadrozny some notable works are adel et al. auli and gao auli et al. duh et al. jozefowicz et al. mikolov mikolov et al. and cardie ling et al. xu et al. et al. sundermeyer et al. sutskever et al. tamura et al. et al. kiperwasser and goldberg watanabe and sumita i et al. et al. sordoni et al. et al. introduction recursive models were shown to produce state-of-the-art or near state-of-the-art results for constituency and dependency parse re-ranking discourse parsing semantic relation classification political ideology detection based on parse trees sentiment classification targetdependent sentiment classification and question answering. coverage and organization e book consists of four parts. part i introduces the basic learning machinery we ll be using throughout the book supervised learning mlps gradient-based training and the computationgraph abstraction for implementing and training neural networks. part ii connects the machinery introduced in the first part with language data. it introduces the main sources of information that are available when working with language data and explains how to integrate them with the neural networks machinery. it also discusses word-embedding algorithms and the distributional hypothesis and feed-forward approaches to language modeling. part iii deals with specialized architectures and their applications to language data convolutional networks for working with ngrams and rnns for modeling sequences and stacks. rnns are the main innovation of the application of neural networks to language data and most of part iii is devoted to them including the powerful conditioned-generation framework they facilitate and attention-based models. part iv is a collection of various advanced topics recursive networks for modeling trees structured prediction models and multi-task learning. part i covering the basics of neural networks consists of four chapters. chapter introduces the basic concepts of supervised machine learning parameterized functions linear and log-linear models regularization and loss functions training as optimization and gradient-based training methods. it starts from the ground up and provides the needed material for the following chapters. readers familiar with basic learning theory and gradient-based learning may consider skipping this chapter. chapter spells out the major limitation of linear models motivates the need for nonlinear models and lays the ground and motivation for multi-layer neural networks. chapter introduces feed-forward neural networks and the mlps. it discusses the definition of multi-layer networks their theoretical power and common subcomponents such as nonlinearities and loss functions. chapter deals with neural network training. it introduces the computationgraph abstraction that allows for automatic gradient computations for arbitrary networks back-propagation algorithm and provides several important tips and tricks for effective network training. et al. and zuidema zhu et al. et al. et al. liu et al. et al. and blunsom socher et al. et al. et al. coverage and organization part ii introducing language data consists of seven chapters. chapter presents a typology of common language-processing problems and discusses the available sources of information available for us when using language data. chapter provides concrete case studies showing how the features described in the previous chapter are used for various natural language tasks. readers familiar with language processing can skip these two chapters. chapter connects the material of chapters and with neural networks and discusses the various ways of encoding language-based features as inputs for neural networks. chapter introduces the language modeling task and the feed-forward neural language model architecture. is also paves the way for discussing pre-trained word embeddings in the following chapters. chapter discusses distributed and distributional approaches to word-meaning representations. it introduces the word-context matrix approach to distributional semantics as well as neural language-modeling inspired wordembedding algorithms such as g v and w and discusses the connection between them and the distributional methods. chapter deals with using word embeddings outside of the context of neural networks. finally chapter presents a case study of a task-specific feedforward network that is tailored for the natural language inference task. part iii introducing the specialized convolutional and recurrent architectures consists of five chapters. chapter deals with convolutional networks that are specialized at learning informative ngram patterns. e alternative hash-kernel technique is also discussed. e rest of this part chapters is devoted to rnns. chapter describes the rnn abstraction for modeling sequences and stacks. chapter describes concrete instantiations of rnns including the simple rnn known as elman rnns and gated architectures such as the long shortterm memory and the gated recurrent unit chapter provides examples of modeling with the rnn abstraction showing their use within concrete applications. finally chapter introduces the conditioned-generation framework which is the main modeling technique behind state-of-the-art machine translation as well as unsupervised sentence modeling and many other innovative applications. part iv is a mix of advanced and non-core topics and consists of three chapters. chapter introduces tree-structured recursive networks for modeling trees. while very appealing this family of models is still in research stage and is yet to show a convincing success story. nonetheless it is an important family of models to know for researchers who aim to push modeling techniques beyond the state-of-the-art. readers who are mostly interested in mature and robust techniques can safely skip this chapter. chapter deals with structured prediction. it is a rather technical chapter. readers who are particularly interested in structured prediction or who are already familiar with structured prediction techniques for linear models or for language processing will likely appreciate the material. others may rather safely skip it. finally chapter presents multitask and semi-supervised learning. neural networks provide ample opportunities for multi-task and semi-supervised learning. ese are important techniques that are still at the research stage. however the existing techniques are relatively easy to implement and do provide real gains. e chapter is not technically challenging and is recommended to all readers. introduction dependencies for the most part chapters depend on the chapters that precede them. an exception are the first two chapters of part ii which do not depend on material in previous chapters and can be read in any order. some chapters and sections can be skipped without impacting the understanding of other concepts or material. ese include section and chapter that deal with the details of word embedding algorithms and the use of word embeddings outside of neural networks chapter describing a specific architecture for attacking the stanford natural language inference dataset and chapter describing convolutional networks. within the sequence on recurrent networks chapter dealing with the details of specific architectures can also be relatively safely skipped. e chapters in part iv are for the most part independent of each other and can be either skipped or read in any order. what s not covered e focus is on applications of neural networks to language processing tasks. however some subareas of language processing with neural networks were deliberately left out of scope of this book. specifically i focus on processing written language and do not cover working with speech data or acoustic signals. within written language i remain relatively close to the lower level relatively well-defined tasks and do not cover areas such as dialog systems document summarization or question answering which i consider to be vastly open problems. while the described techniques can be used to achieve progress on these tasks i do not provide examples or explicitly discuss these tasks directly. semantic parsing is similarly out of scope. multi-modal applications connecting language data with other modalities such as vision or databases are only very briefly mentioned. finally the discussion is mostly english-centric and languages with richer morphological systems and fewer computational resources are only very briefly discussed. some important basics are also not discussed. specifically two crucial aspects of good work in language processing are proper evaluation and data annotation. both of these topics are left outside the scope of this book but the reader should be aware of their existence. proper evaluation includes the choice of the right metrics for evaluating performance on a given task best practices fair comparison with other work performing error analysis and assessing statistical significance. data annotation is the bread-and-butter of nlp systems. without data we cannot train supervised models. as researchers we very often just use standard annotated data produced by someone else. it is still important to know the source of the data and consider the implications resulting from its creation process. data annotation is a very vast topic including proper formulation of the annotation task developing the annotation guidelines deciding on the source of annotated data its coverage and class proportions good train-test splits and working with annotators consolidating decisions validating quality of annotators and annotation and various similar topics. a note on terminology a note on terminology e word feature is used to refer to a concrete linguistic input such as a word a suffix or a partof-speech tag. for example in a first-order part-of-speech tagger the features might be current word previous word next word previous part of speech. e term input vector is used to refer to the actual input that is fed to the neural network classifier. similarly input vector entry refers to a specific value of the input. is is in contrast to a lot of the neural networks literature in which the word feature is overloaded between the two uses and is used primarily to refer to an input-vector entry. mathematical notation we use bold uppercase letters to represent matrices y z and bold lowercase letters to represent vectors when there are series of related matrices and vectors example where each matrix corresponds to a different layer in the network superscript indices are used w for the rare cases in which we want indicate the power of a matrix or a vector a pair of brackets is added around the item to be exponentiated we use as the index operator of vectors and matrices is the ith element of vector b and w is the element in the ith column and jth row of matrix w when unambiguous we sometimes adopt the more standard mathematical notation and use bi to indicate the ith element of vector b and pi wi vi dpi we use to indicate a sequence of vectors xn and similarly similarly wij for elements of a matrix w we use to denote the dot-product operator w v d is the sequence of items xn. we use to indicate the reverse sequence. d xi d we use to denote vector concatenation. while somewhat unorthodox unlessotherwisestatedvectorsareassumedtoberowvectors. e choice to use row vectors which are right multiplied by matrices c b is somewhat non standard a lot of the neural networks literature use column vectors that are left multiplied by matrices x c b. we trust the reader to be able to adapt to the column vectors notation when reading the literature. e choice to use the row vectors notation was inspired by the following benefits it matches the way input vectors and network diagrams are often drawn in the literature it makes the hierarchicallayered structure of the network more transparent and puts the input as the left-most variable rather than being nested it results in fully connected layer dimensions being din dout rather than dout din and it maps better to the way networks are implemented in code using matrix libraries such as numpy. part i supervised classification and feed-forward neural networks c h a p t e r learning basics and linear models neural networks the topic of this book are a class of supervised machine learning algorithms. is chapter provides a quick introduction to supervised machine learning terminology and practices and introduces linear and log-linear models for binary and multi-class classification. e chapter also sets the stage and notation for later chapters. readers who are familiar with linear models can skip ahead to the next chapters but may also benefit from reading sections and supervised machine learning theory and linear models are very large topics and this chapter is far from being comprehensive. for a more complete treatment the reader is referred to texts such as daum iii shalev-shwartz and ben-david and mohri et al. supervised learning and parameterized functions e essence of supervised machine learning is the creation of mechanisms that can look at examples and produce generalizations. more concretely rather than designing an algorithm to perform a task distinguish spam from non-spam email we design an algorithm whose input is a set of labeled examples is pile of emails are spam. is other pile of emails are not spam. and its output is a function a program that receives an instance email and produces the desired label or not-spam. it is expected that the resulting function will produce correct label predictions also for instances it has not seen during training. as searching over the set of all possible programs all possible functions is a very hard rather ill-defined problem we often restrict ourselves to search over specific families of functions e.g. the space of all linear functions with din inputs and dout outputs or the space of all decision trees over din variables. such families of functions are called hypothesis classes. by restricting ourselves to a specific hypothesis class we are injecting the learner with inductive bias a set of assumptions about the form of the desired solution as well as facilitating efficient procedures for searching for the solution. for a broad and readable overview of the main families of learning algorithms and the assumptions behind them see the book by domingos e hypothesis class also determines what can and cannot be represented by the learner. one common hypothesis class is that of high-dimensional linear function i.e. functions of the learning basics and linear models form f d x w c b x rdin w b rdout here the vector x is the input to the function while the matrix w and the vector b are the parameters. e goal of the learner is to set the values of the parameters w and b such that the function behaves as intended on a collection of input values d xk and the corresponding desired outputs y d y i y k. e task of searching over the space of functions is thus reduced to one of searching over the space of parameters. it is common to refer to parameters of the function as for the linear model case d w b. in some cases we want the notation to make the parameterization explicit in which case we include the parameters in the function s definition f w b d x w c b. as we will see in the coming chapters the hypothesis class of linear functions is rather restricted and there are many functions that it cannot represent it is limited to linear relations. in contrast feed-forward neural networks with hidden layers to be discussed in chapter are also parameterized functions but constitute a very strong hypothesis class they are universal approximators capable of representing any borel-measurable function. however while restricted linear models have several desired properties they are easy and efficient to train they often result in convex optimization objectives the trained models are somewhat interpretable and they are often very effective in practice. linear and log-linear models were the dominant approaches in statistical nlp for over a decade. moreover they serve as the basic building blocks for the more powerful nonlinear feed-forward networks which will be discussed in later chapters. train test and validation sets before delving into the details of linear models let s reconsider the general setup of the machine learning problem. we are faced with a dataset of k input examples and their corresponding gold labels y and our goal is to produce a function f that correctly maps inputs x to outputs oy as evidenced by the training set. how do we know that the produced function f is indeed a good one? one could run the training examples through f record the answers oy compare them to the expected labels y and measure the accuracy. however this process will not be very informative our main concern is the ability of f to generalize well to unseen examples. a function f that is implemented as a lookup table that is looking for the input x in its memory and returning the corresponding value y for instances is has seen and a random value otherwise will get a perfect score on this test yet is clearly not a good classification function as it has zero generalization ability. we rather have a function f that gets some of the training examples wrong providing that it will get unseen examples correctly. as discussed in section is book takes a somewhat un-orthodox approach and assumes vectors are row vectors rather than column vectors. see further discussion in section train test and validation sets leave-one out we must assess the trained function s accuracy on instances it has not seen during training. one solution is to perform leave-one-out cross-validation train k functions each time leaving out a different input example xi and evaluating the resulting function fi on its ability to predict xi. en train another function f on the entire trainings set assuming that the training set is a representative sample of the population this percentage of functions fi that produced correct prediction on the left-out samples is a good approximation of the accuracy of f on new inputs. however this process is very costly in terms of computation time and is used only in cases where the number of annotated examples k is very small than a hundred or so. in language processing tasks we very often encounter training sets with well over examples. held-out set a more efficient solution in terms of computation time is to split the training set into two subsets say in a split train a model on the larger subset training set and test its accuracy on the smaller subset held-out set. is will give us a reasonable estimate on the accuracy of the trained function or at least allow us to compare the quality of different trained models. however it is somewhat wasteful in terms training samples. one could then re-train a model on the entire set. however as the model is trained on substantially more data the error estimates of the model trained on less data may not be accurate. is is generally a good problem to have as more training data is likely to result in better rather than worse predictors. some care must be taken when performing the split in general it is better to shu e the examples prior to splitting them to ensure a balanced distribution of examples between the training and held-out sets example you want the proportion of gold labels in the two sets to be similar. however sometimes a random split is not a good option consider the case where your input are news articles collected over several months and your model is expected to provide predictions for new stories. here a random split will over-estimate the model s quality the training and held-out examples will be from the same time period and hence on more similar stories which will not be the case in practice. in such cases you want to ensure that the training set has older news stories and the held-out set newer ones to be as similar as possible to how the trained model will be used in practice. a three-way split e split into train and held-out sets works well if you train a single model and wants to assess its quality. however in practice you often train several models compare their quality and select the best one. here the two-way split approach is insufficient selecting the best model according to the held-out set s accuracy will result in an overly optimistic estimate of the model s quality. you don t know if the chosen settings of the final classifier are good in general or are just good for the particular examples in the held-out sets. e problem will be even worse if you perform error analysis based on the held-out set and change the features or the architecture of the model based on the observed errors. you don t know if your improvements based on the held note however that some setting in the training procedure in particular the learning rate and regularization weight may be sensitive to the training set size and tuning them based on some data and then re-training a model with the same settings on larger data may produce sub-optimal results. learning basics and linear models out sets will carry over to new instances. e accepted methodology is to use a three-way split of the data into train validation called development and test sets. is gives you two held-out sets a validation set called development set and a test set. all the experiments tweaks error analysis and model selection should be performed based on the validation set. en a single run of the final model over the test set will give a good estimate of its expected quality on unseen examples. it is important to keep the test set as pristine as possible running as few experiments as possible on it. some even advocate that you should not even look at the examples in the test set so as to not bias the way you design your model. now that we have established some methodology we return to describe linear models for binary and multi-class classification. linear models binary classification in binary classification problems we have a single output and thus use a restricted version of equation in which dout d making w a vector and b a scalar. f d x w c b e range of the linear function in equation is in order to use it for binary classification it is common to pass the output of f through the sig n function mapping negative values to negative class and non-negative values to positive class. consider the task of predicting which of two neighborhoods an apartment is located at based on the apartment s price and size. figure shows a plot of some apartments where the x-axis denotes the monthly rent price in usd while the y-axis is the size in square feet. e blue circles are for dupont circle dc and the green crosses are in fairfax va. it is evident from the plot that we can separate the two neighborhoods using a straight line apartments in dupont circle tend to be more expensive than apartments in fairfax of the same size. e dataset is linearly separable the two classes can be separated by a straight line. each data-point apartment can be represented as a vector x where is the apartment s size and is its price. we then get the following linear model oy d sign.f d sign.x w c b d sign.size c price c b where is the dot-product operation b and w d are free parameters and we predict fairfax if oy and dupont circle otherwise. e goal of learning is setting the values of note that looking at either size or price alone would not allow us to cleanly separate the two groups. linear models housing data rent price in usd vs. size in square ft. data source craigslist ads collected from june and b such that the predictions are correct for all data-points we observe. we will discuss learning in section but for now consider that we expect the learning procedure to set a high value to and a low value to once the model is trained we can classify new data-points by feeding them into this equation. it is sometimes not possible to separate the data-points using a straight line in higher dimensions a linear hyperplane such datasets are said to be nonlinearly separable and are beyond the hypothesis class of linear classifiers. e solution would be to either move to a higher dimension more features move to a richer hypothesis class or allow for some mis-classification. geometrically for a given w the points x w c b d define a hyperplane in two dimensions corresponds to a line that separates the space into two regions. e goal of learning is then finding a hyperplane such that the classification induced by it is correct. misclassifying some of the examples is sometimes a good idea. for example if we have reason to believe some of the datapoints are outliers examples that belong to one class but are labeled by mistake as belonging to the other class. learning basics and linear models feature representations in the example above each data-point was a pair of size and price measurements. each of these properties is considered a feature by which we classify the datapoint. is is very convenient but in most cases the data-points are not given to us directly as lists of features but as real-world objects. for example in the apartments example we may be given a list of apartments to classify. we then need to make a concious decision and select the measurable properties of the apartments that we believe will be useful features for the classification task at hand. here it proved effective to focus on the price and the size. we could also look at additional properties such as the number of rooms the height of the ceiling the type of floor the geo-location coordinates and so on. after deciding on a set of features we create a feature extraction function that maps a real world object an apartment to a vector of measurable quantities and size which can be used as inputs to our models. e choice of the features is crucial to the success of the classification accuracy and is driven by the informativeness of the features and their availability to us geo-location coordinates are much better predictors of the neighborhood than the price and size but perhaps we only observe listings of past transactions and do not have access to the geo-location information. when we have two features it is easy to plot the data and see the underlying structures. however as we see in the next example we often use many more than just two features making plotting and precise reasoning impractical. a central part in the design of linear models which we mostly gloss over in this text is the design of the feature function called feature engineering. one of the promises of deep learning is that it vastly simplifies the feature-engineering process by allowing the model designer to specify a small set of core basic or natural features and letting the trainable neural network architecture combine them into more meaningful higher-level features or representations. however one still needs to specify a suitable set of core features and tie them to a suitable architecture. we discuss common features for textual data in chapters and we usually have many more than two features. moving to a language setup consider the task of distinguishing documents written in english from documents written in german. it turns out that letter frequencies make for quite good predictors for this task. even more informative are counts of letter bigrams i.e. pairs of consecutive letters. assuming we have an alphabet of letters z space and a special symbol for all other characters including digits punctuations etc. we represent a document as a dimensional vector x where each entry represents a count of a particular letter combination in the document normalized by the document s length. for example denoting by xab the entry of x corresponding to the while one may think that words will also be good predictors letters or letter-bigrams are far more robust we are likely to encounter a new document without any of the words we observed in the training set while a document without any of the distinctive letter-bigrams is significantly less likely. letter-bigram ab where is the number of times the bigram ab appears in the document and jdj is the total number of bigrams in the document document s length. xab d linear models jdj figure character-bigram histograms for documents in english blue and german green. underscores denote spaces. figure shows such bigram histograms for several german and english texts. for readability we only show the top frequent character-bigrams and not the entire feature vectors. on the left we see the bigrams of the english texts and on the right of the german ones. ere are clear patterns in the data and given a new item such as learning basics and linear models you could probably tell that it is more similar to the german group than to the english one. note however that you couldn t use a single definite rule such as if it has th its english or if it has ie its german while german texts have considerably less th than english the th may and does occur in german texts and similarly the ie combination does occur in english. e decision requires weighting different factors relative to each other. let s formalize the problem in a machine-learning setup. we can again use a linear model oy d sign.f d sign.x w c b d sign.xaa waa c xab wab c xac wac c b a document will be considered english if f and as german otherwise. intuitively learning should assign large positive values to w entries associated with letter pairs that are much more common in english than in german th negative values to letter pairs that are much more common in german than in english en and values around zero to letter pairs that are either common or rare in both languages. note that unlike the case of the housing data vs. size here we cannot easily visualize the points and the decision boundary and the geometric intuition is likely much less clear. in general it is difficult for most humans to think of the geometries of spaces with more than three dimensions and it is advisable to think of linear models in terms of assigning weights to features which is easier to imagine and reason about. log-linear binary classification e output f is in the range and we map it to one of two classes using the sig n function. is is a good fit if all we care about is the assigned class. however we may be interested also in the confidence of the decision or the probability that the classifier assigns to the class. an alternative that facilitates this is to map instead to the range by pushing the output through a squashing function such as the sigmoid d resulting in oy d d c figure shows a plot of the sigmoid function. it is monotonically increasing and maps values when used with a suitable loss function to the range with being mapped to in section the binary predictions made through the log-linear model can be interpreted as class membership probability estimates d p oy d j x of x belonging to the positive class. we also get p oy d j x d p oy d j x d e closer the value is to or the more certain the model is in its class membership prediction with the value of indicating model uncertainty. linear models figure e sigmoid function multi-class classification e previous examples were of binary classification where we had two possible classes. binaryclassification cases exist but most classification problems are of a multi-class nature in which we should assign an example to one of k different classes. for example we are given a document and asked to classify it into one of six possible languages english french german italian spanish other. a possible solution is to consider six weight vectors we wf and biases one for each language and predict the language resulting in the highest score oy d f d argmax x wl c bl e six sets of parameters wl bl can be arranged as a matrix w and vector b and the equation re-written as oy d f d x w c b oy prediction d oy d argmax i here oy is a vector of the scores assigned by the model to each language and we again determine the predicted language by taking the argmax over the entries of oy. ere are many ways to model multi-class classification including binary-to-multi-class reductions. ese are beyond the scope of this book but a good overview can be found in allwein et al. learning basics and linear models representations consider the vector oy resulting from applying equation of a trained model to a document. e vector can be considered as a representation of the document capturing the properties of the document that are important to us namely the scores of the different languages. e representation oy contains strictly more information than the prediction oy d argmaxi oy for example oy can be used to distinguish documents in which the main language in german but which also contain a sizeable amount of french words. by clustering documents based on their vector representations as assigned by the model we could perhaps discover documents written in regional dialects or by multilingual authors. e vectors x containing the normalized letter-bigram counts for the documents are also representations of the documents arguably containing a similar kind of information to the vectors oy. however the representations in oy is more compact entries instead of and more specialized for the language prediction objective by the vectors x would likely reveal document similarities that are not due to a particular mix of languages but perhaps due to the document s topic or writing styles. e trained matrix w can also be considered as containing learned representations. as demonstrated in figure we can consider two views of w as rows or as columns. each of the columns of w correspond to a particular language and can be taken to be a vector representation of this language in terms of its characteristic letter-bigram patterns. we can then cluster the language vectors according to their similarity. similarly each of the rows of w correspond to a particular letter-bigram and provide a vector representation of that bigram in terms of the languages it prompts. representations are central to deep learning. in fact one could argue that the main power of deep-learning is the ability to learn good representations. in the linear case the representations are interpretable in the sense that we can assign a meaningful interpretation to each dimension in the representation vector each dimension corresponds to a particular language or letter-bigram. is is in general not the case deep learning models often learn a cascade of representations of the input that build on top of each other in order to best model the problem at hand and these representations are often not interpretable we do not know which properties of the input they capture. however they are still very useful for making predictions. moreover at the boundaries of the model i.e. at the input and the output we get representations that correspond to particular aspects of the input a vector representation for each letter-bigram or the output a vector representation of each of the output classes. we will get back to this in section after discussing neural networks and encoding categorical features as dense vectors. it is recommended that you return to this discussion once more after reading that section. one-hot and dense vector representations figure two views of the w matrix. each column corresponds to a language. each row corresponds to a letter bigram. one-hot and dense vector representations e input vector x in our language classification example contains the normalized bigram counts in the document d. is vector can be decomposed into an average of jdj vectors each corresponding to a particular document position i jdjx x d jdj here is the bigram at document position i and each vector is a one-hot vector in which all entries are zero except the single entry corresponding to the letter bigram which is e resulting vector x is commonly referred to as an averaged bag of bigrams generally averaged bag of words or just bag of words. bag-of-words representations contain information about the identities of all the words bigrams of the document without considering their order. a one-hot representation can be considered as a bag-of-a-single-word. e view of the rows of the matrix w as representations of the letter bigrams suggests an alternative way of computing the document representation vector oy in equation denoting ospitgrfrenaaabacadaeafagahzyzzabww learning basics and linear models by w the row of w corresponding to the bigram we can take the representation y of a document d to be the average of the representations of the letter-bigrams in the document jdjx oy d jdj w is representation is often called a continuous bag of words as it is composed of a sum of word representations where each word representation is a low-dimensional continuous vector. we note that equation and the term x w in equation are equivalent. to see why consider y d x w jdj jdjx jdjx jdjx jdj jdj d d d w w w in other words the continuous-bag-of-words representation can be obtained either by summing word-representation vectors or by multiplying a bag-of-words vector by a matrix in which each row corresponds to a dense word representation matrices are also called embedding matrices. we will return to this point in chapter particular section when discussing feature representations in deep learning models for text. in the binary case we transformed the linear prediction into a probability estimate by passing it through the sigmoid function resulting in a log-linear model. e analog for the multi-class case is passing the score vector through the softmax function log-linear multi-class classification d resulting in training as optimization oy d softmax.xw c b pj e.xw oy d e.xw e softmax transformation forces the values in oy to be positive and sum to making them interpretable as a probability distribution. training as optimization recall that the input to a supervised learning algorithm is a training set of n training examples d xn together with corresponding labels d yn. without loss of generality we assume that the desired inputs and outputs are vectors y e goal of the algorithm is to return a function f that accurately maps input examples to their desired labels i.e. a function f such that the predictions oy d f over the training set are accurate. to make this more precise we introduce the notion of a loss function quantifying the loss suffered when predicting oy while the true label is y. formally a loss function l. oy y assigns a numerical score scalar to a predicted output oy given the true expected output y. e loss function should be bounded from below with the minimum attained only for cases where the prediction is correct. e parameters of the learned function matrix w and the biases vector b are then set in order to minimize the loss l over the training examples it is the sum of the losses over the different training examples that is being minimized. concretely given a labeled training set y a per-instance loss function l and a parameterized function f we define the corpus-wide loss with respect to the parameters as the average loss over all training examples d n l.f y i in this view the training examples are fixed and the values of the parameters determine the loss. e goal of the training algorithm is then to set the values of the parameters such that the value of l is minimized d argmin d argmin n l.f y i equation attempts to minimize the loss at all costs which may result in overfitting the training data. to counter that we often pose soft restrictions on the form of the solution. is in many cases it is natural to think of the expected output as a scalar assignment rather than a vector. in such cases y is simply the corresponding one-hot vector and argmaxi y is the corresponding class assignment. nx nx n nx d argmin loss regularization l.f y i c learning basics and linear models is done using a function taking as input the parameters and returning a scalar that reflect their complexity which we want to keep low. by adding r to the objective the optimization problem needs to balance between low loss and low complexity e function r is called a regularization term. different combinations of loss functions and regularization criteria result in different learning algorithms with different inductive biases. we now turn to discuss common loss functions followed by a discussion of regularization and regularizers en in section we present an algorithm for solving the minimization problem loss functions e loss can be an arbitrary function mapping two vectors to a scalar. for practical purposes of optimization we restrict ourselves to functions for which we can easily compute gradients subgradients. in most cases it is sufficient and advisable to rely on a common loss function rather than defining your own. for a detailed discussion and theoretical treatment of loss functions for binary classification see zhang we now discuss some loss functions that are commonly used with linear models and with neural networks in nlp. hinge for binary classification problems the classifier s output is a single scalar qy and the intended output y is in e classification rule is oy d sign. qy and a classification is considered correct if y qy meaning that y and qy share the same sign. e hinge loss also known as margin loss or svm loss is defined as e loss is when y and qy share the same sign and j qyj otherwise the loss is linear. in other words the binary hinge loss attempts to achieve a correct classification with a margin of at least hinge e hinge loss was extended to the multi-class setting by crammer and singer let oy d oy oy be the classifier s output vector and y be the one-hot vector for the correct output class. lhingebinary. qy y d y qy e classification rule is defined as selecting the class with the highest score prediction d argmax i oy a gradient of a function with k variables is a collection of k partial derivatives one according to each of the variables. gradients are discussed further in section lhingemulti-class. oy y d oy oy class such that k t. e multi-class hinge loss is defined as training as optimization denote by t d argmaxi y the correct class and by k d argmaxi t oy the highest scoring e multi-class hinge loss attempts to score the correct class above all other classes with a margin of at least both the binary and multi-class hinge losses are intended to be used with linear outputs. e hinge losses are useful whenever we require a hard decision rule and do not attempt to model class membership probability. log loss e log loss is a common variation of the hinge loss which can be seen as a soft version of the hinge loss with an infinite margin et al. llog oy y d c oy oy binary cross entropy e binary cross-entropy loss also referred to as logisticloss is used in binary classification with conditional probability outputs. we assume a set of two target classes labeled and with a correct label y e classifier s output qy is transformed using the sigmoid called the logistic function d c to the range and is interpreted as the conditional probability oy d qy d p d e prediction rule is prediction d oy oy training example y. e logistic loss is defined as llogistic. oy y d log oy oy e network is trained to maximize the log conditional probability log p d for each e logistic loss is useful when we want the network to produce class conditional probability for a binary classification problem. when using the logistic loss it is assumed that the output layer is transformed using the sigmoid function. categorical cross-entropy loss e categorical cross-entropy loss referred to as negative log likelihood is used when a probabilistic interpretation of the scores is desired. let y d y y be a vector representing the true multinomial distribution over the labels n and let oy d oy oy be the linear classifier s output which was transformed by the softmax function and represent the class membership conditional distribution oy d p d ijx. e categorical cross entropy loss measures the dissimilarity between the true label distribution y and the predicted label distribution oy and is defined as cross entropy lcross-entropy. oy y d y log. oy i is formulation assumes an instance can belong to several classes with some degree of certainty. learning basics and linear models for hard-classification problems in which each training example has a single correct class assignment y is a one-hot vector representing the true class. in such cases the cross entropy can be simplified to lcross-entropyhard classification. oy y d oy where t is the correct class assignment. is attempts to set the probability mass assigned to the correct class t to because the scores oy have been transformed using the softmax function to be non-negative and sum to one increasing the mass assigned to the correct class means decreasing the mass assigned to all the other classes. e cross-entropy loss is very common in the log-linear models and the neural networks literature and produces a multi-class classifier which does not only predict the one-best class label but also predicts a distribution over the possible labels. when using the cross-entropy loss it is assumed that the classifier s output is transformed using the softmax transformation. ranking losses in some settings we are not given supervision in term of labels but rather as pairs of correct and incorrect items x and and our goal is to score correct items above incorrect ones. such training situations arise when we have only positive examples and generate negative examples by corrupting a positive example. a useful loss in such scenarios is the margin-based ranking loss defined for a pair of correct and incorrect examples where f is the score assigned by the classifier for input vector x. e objective is to score correct inputs over incorrect ones with a margin of at least lrankingmargin.x d f a common variation is to use the log version of the ranking loss lrankinglog.x d c f examples using the ranking hinge loss in language tasks include training with the auxiliary tasks used for deriving pre-trained word embeddings section in which we are given a correct word sequence and a corrupted word sequence and our goal is to score the correct sequence above the corrupt one and weston similarly van de cruys used the ranking loss in a selectional-preferences task in which the network was trained to rank correct verb-object pairs above incorrect automatically derived ones and weston et al. trained a model to score correct relation tail triplets above corrupted ones in an informationextraction setting. an example of using the ranking log loss can be found in gao et al. a variation of the ranking log loss allowing for a different margin for the negative and positive class is given in dos santos et al. training as optimization regularization consider the optimization problem in equation it may admit multiple solutions and especially in higher dimensions it can also over-fit. consider our language identification example and a setting in which one of the documents in the training set it xo is an outlier it is actually in german but is labeled as french. in order to drive the loss down the learner can identify features bigrams in xo that occur in only few other documents and give them very strong weights toward the french class. en for other german documents in which these features occur which may now be mistakenly classified as french the learner will find other german letter bigrams and will raise their weights in order for the documents to be classified as german again. is is a bad solution to the learning problem as it learns something incorrect and can cause test german documents which share many words with xo to be mistakenly classified as french. intuitively we would like to control for such cases by driving the learner away from such misguided solutions and toward more natural ones in which it is ok to mis-classify a few examples if they don t fit well with the rest. is is achieved by adding a regularization term r to the optimization objective whose job is to control the complexity of the parameter value and avoid cases of overfitting d argmin d argmin c nx n l.f y i c e regularization term considers the parameter values and scores their complexity. we then look for parameter values that have both a low loss and low complexity. a hyperparameter is used to control the amount of regularization do we favor simple model over low loss ones or vice versa. e value of has to be set manually based on the classification performance on a development set. while equation has a single regularization function and value for all the parameters it is of course possible to have a different regularizer for each item in in practice the regularizers r equate complexity with large weights and work to keep the parameter values low. in particular the regularizers r measure the norms of the parameter matrices and drive the learner toward solutions with low norms. common choices for r are the norm the norm and the elastic-net. regularization in regularization r takes the form of the squared norm of the parameters trying to keep the sum of the squares of the parameter values low dx ij d jjw a hyperparameter is a parameter of the model which is not learned as part of the optimization process but needs to be set by hand. learning basics and linear models e regularizer is also called a gaussian prior or weight decay. note that regularized models are severely punished for high parameter weights but once the value is close enough to zero their effect becomes negligible. e model will prefer to decrease the value of one parameter with high weight by than to decrease the value of ten parameters that already have relatively low weights by each. regularization in regularization r takes the form of the norm of the parameters trying to keep the sum of the absolute values of the parameters low d jjw dx ij jw in contrast to the regularizer is punished uniformly for low and high values and has an incentive to decrease all the non-zero parameter values toward zero. it thus encourages a sparse solutions models with many parameters with a zero value. e regularizer is also called a sparse prior or lasso elastic-net e elastic-net regularization and hastie combines both and regularization relastic-net.w d c dropout another form of regularization which is very effective in neural networks is dropout which we discuss in section gradient-based optimization in order to train the model we need to solve the optimization problem in equation a common solution is to use a gradient-based method. roughly speaking gradient-based methods work by repeatedly computing an estimate of the loss l over the training set computing the gradients of the parameters with respect to the loss estimate and moving the parameters in the opposite directions of the gradient. e different optimization methods differ in how the error estimate is computed and how moving in the opposite direction of the gradient is defined. we describe the basic algorithm stochastic gradient descent and then briefly mention the other approaches with pointers for further reading. motivating gradient-based optimization consider the task of finding the scalar value x that minimizes a function y d f e canonical approach is computing the second derivative f of the function and solving for f d to get the extrema points. for the sake of example assume this approach cannot be used it is challenging to use this approach in function of multiple variables. an alternative approach is a numeric one compute the first derivative f en start with an initial guess value xi. evaluating u d f gradient-based optimization will give the direction of change. if u d then xi is an optimum point. otherwise move in the opposite direction of u by setting xi where is a rate parameter. with a small enough value of f will be smaller than f repeating this process properly decreasing values of will find an optimum point xi. if the function f is convex the optimum will be a global one. otherwise the process is only guaranteed to find a local optimum. gradient-based optimization simply generalizes this idea for functions with multiple variables. a gradient of a function with k variables is the collections of k partial derivatives one according to each of the variables. moving the inputs in the direction of the gradient will increase the value of the function while moving them in the opposite direction will decrease it. when optimizing the loss y the parameters are considered as inputs to the function while the training examples are treated as constants. in gradient-based optimization it is common to distinguish between convex convexity concave functions and non-convex functions. a convex function is a function whose second-derivative is always non-negative. as a consequence convex functions have a single minimum point. similarly concave functions are functions whose second-derivatives are always negative or zero and as a consequence have a single maximum point. convex functions have the property that they are easy to minimize using gradientbased optimization simply follow the gradient until an extremum point is reached and once it is reached we know we obtained the global extremum point. in contrast for functions that are neither convex nor concave a gradient-based optimization procedure may converge to a local extremum point missing the global optimum. stochastic gradient descent an effective method for training linear models is using the sgd algorithm lecun et al. or a variant of it. sgd is a general optimization algorithm. it receives a function f parameterized by a loss function l and desired input and output pairs y it then attempts to set the parameters such that the cumulative loss of f on the training examples is small. e algorithm works as shown in algorithm dpn e goal of the algorithm is to set the parameters so as to minimize the total loss l.f yi over the training set. it works by repeatedly sampling a training example and computing the gradient of the error on the example with respect to the parameters the input and expected output are assumed to be fixed and the loss is treated as a function of the parameters e parameters are then updated in the opposite direction of the gradient scaled by a learning rate e learning rate can either be fixed throughout the learning basics and linear models algorithm online stochastic gradient descent training. input function f parameterized with parameters training set of inputs xn and desired outputs yn. loss function l. while stopping criteria not met do sample a training example xi yi compute the loss l.f yi og gradients of l.f yi w.r.t og return training process or decay as a function of the time step t. for further discussion on setting the learning rate see section note that the error calculated in line is based on a single training example and is thus just a rough estimate of the corpus-wide loss l that we are aiming to minimize. e noise in the loss computation may result in inaccurate gradients. a common way of reducing this noise is to estimate the error and the gradients based on a sample of m examples. is gives rise to the minibatch sgd algorithm in lines the algorithm estimates the gradient of the corpus loss based on the minibatch. after the loop og contains the gradient estimate and the parameters are updated toward og. e minibatch size can vary in size from m d to m d n. higher values provide better estimates of the corpus-wide gradients while smaller values allow more updates and in turn faster convergence. besides the improved accuracy of the gradients estimation the minibatch algorithm provides opportunities for improved training efficiency. for modest sizes of m some computing architectures gpus allow an efficient parallel implementation of the computation in lines with a properly decreasing learning rate sgd is guaranteed to converge to a global optimum if the function is convex which is the case for linear and log-linear models coupled with the loss functions and regularizers discussed in this chapter. however it can also be used to optimize non-convex functions such as multi-layer neural network. while there are no longer guarantees of finding a global optimum the algorithm proved to be robust and performs well in practice. learning rate decay is required in order to prove convergence of sgd. recent work from the neural networks literature argue that the non-convexity of the networks is manifested in a proliferation of saddle points rather than local minima et al. is may explain some of the success in training neural networks despite using local search techniques. gradient-based optimization algorithm minibatch stochastic gradient descent training. input function f parameterized with parameters training set of inputs xn and desired outputs yn. loss function l. sample a minibatch of m examples ymg og for i d to m do while stopping criteria not met do return compute the loss l.f yi og og c gradients of m l.f yi w.r.t og worked-out example as an example consider a multi-class linear classifier with hinge loss oy oy d argmax oy d f d xw c b i l. oy y d oy oy d c c t d argmax k d argmax oy i t y i i we want to set the parameters w and b such that the loss is minimized. we need to compute the gradients of the loss with respect to the values w and b. e gradient is the collection of the learning basics and linear models partial derivatives according to each of the variables oy y d oy y d oy oy oy oy oy oy oy oy oy oy oy oy more concretely we will compute the derivate of the loss w.r.t each of the values w and we begin by expanding the terms in the loss calculation l. oy y d oy oy x x d c c w cx d max d max w c i i w c w c i i t d argmax k d argmax i i y oy i t e first observation is that if oy oy then the loss is and so is the gradient derivative of the max operation is the derivative of the maximal value. otherwise consider the derivative of for the partial derivative is treated as a variable and all others are ered as constants. for i k t the term does not contribute to the loss and its derivative it is for i d k and i d t we trivially get d i d t i d k otherwise more advanced derivation techniques allow working with matrices and vectors directly. here we stick to high-school level techniques. similarly for w only j d k and j d t contribute to the loss. we get gradient-based optimization d d j d t d j d k otherwise is concludes the gradient calculation. as a simple exercise the reader should try and compute the gradients of a multi-class linear model with hinge loss and regularization and the gradients of multi-class classification with softmax output transformation and cross-entropy loss. beyond sgd while the sgd algorithm can and often does produce good results more advanced algorithms are also available. e sgdmomentum and nesterov momentum sutskever et al. algorithms are variants of sgd in which previous gradients are accumulated and affect the current update. adaptive learning rate algorithms including adagrad et al. adadelta rmsprop and hinton and adam and ba are designed to select the learning rate for each minibatch sometimes on a per-coordinate basis potentially alleviating the need of fiddling with learning rate scheduling. for details of these algorithms see the original papers or et al. sections c h a p t e r from linear models to multi-layer perceptrons limitations of linear models the xor problem e hypothesis class of linear log-linear models is severely restricted. for example it cannot represent the xor function defined as d d d d at is there is no parameterization w b r such that w c b w c b w c b w c b to see why consider the following plot of the xor function where blue os denote the positive class and green xs the negative class. from linear models to multi-layer perceptrons it is clear that no straight line can separate the two classes. nonlinear input transformations however if we transform the points by feeding each of them through the nonlinear function d c the xor problem becomes linearly separable. e function mapped the data into a representation that is suitable for linear classification. having at our disposal we can now easily train a linear classifier to solve the xor problem. oy d f d c b in general one can successfully train a linear classifier over a dataset which is not linearly separable by defining a function that will map the data to a representation in which it is linearly separable and then train a linear classifier on the resulting representation. in the xor example the transformed data has the same dimensions as the original one but often in order to make the data linearly separable one needs to map it to a space with a much higher dimension. is solution has one glaring problem however we need to manually define the function a process which is dependent on the particular dataset and requires a lot of human intuition. kernel methods kernelized support vectors machines and et al. and kernel methods in general and cristianini approach this problem by defining a set of generic mappings each of them mapping the data into very high dimensional and sometimes even infinite spaces and then performing linear classification in the transformed space. working in very high dimensional spaces significantly increase the probability of finding a suitable linear separator. one example mapping is the polynomial mapping d for d d we get d is gives us all combinations of the two variables allowing to solve the xor problem using a linear classifier with a polynomial increase in the number of parameters. in the xor problem the mapping increased the dimensionality of the input trainable mapping functions hence the number of parameters from for the language identification example the input dimensionality would have increased from to dimensions. working in very high dimensional spaces can become computationally prohibitive and the ingenuity in kernel methods is the use of the kernel trick et al. sch lkopf that allows one to work in the transformed space without ever computing the transformed representation. e generic mappings are designed to work on many common cases and the user needs to select the suitable one for its task often by trial and error. a downside of the approach is that the application of the kernel trick makes the classification procedure for svms dependent linearly on the size of the training set making it prohibitive for use in setups with reasonably large training sets. another downside of high dimensional spaces is that they increase the risk of overfitting. trainable mapping functions a different approach is to define a trainable nonlinear mapping function and train it in conjunction with the linear classifier. at is finding the suitable representation becomes the responsibility of the training algorithm. for example the mapping function can take the form of a parameterized linear model followed by a nonlinear activation function g that is applied to each of the output dimensions oy d c b d g.xw c by taking g.x d x and w d we get an equivalent mapping to c for the our points of interest and successfully solving the xor problem. e entire expression g.xw c c b is differentiable not convex making it possible to apply gradient-based techniques to the model training learning both the representation function and the linear classifier on top of it at the same time. is is the main idea behind deep learning and neural networks. in fact equation describes a very common neural network architecture called a multi-layer perceptron having established the motivation we now turn to describe multi-layer neural networks in more detail. c h a p t e r feed-forward neural networks a brain-inspired metaphor as the name suggests neural networks were inspired by the brain s computation mechanism which consists of computation units called neurons. while the connections between artificial neural networks and the brain are in fact rather slim we repeat the metaphor here for completeness. in the metaphor a neuron is a computational unit that has scalar inputs and outputs. each input has an associated weight. e neuron multiplies each input by its weight and then sums them applies a nonlinear function to the result and passes it to its output. figure shows such a neuron. figure a single neuron with four inputs. e neurons are connected to each other forming a network the output of a neuron may feed into the inputs of one or more neurons. such networks were shown to be very capable computational devices. if the weights are set correctly a neural network with enough neurons and a nonlinear activation function can approximate a very wide range of mathematical functions will be more precise about this later. a typical feed-forward neural network may be drawn as in figure each circle is a neuron with incoming arrows being the neuron s inputs and outgoing arrows being the neuron s outputs. each arrow carries a weight reflecting its importance shown. neurons are arranged in layers reflecting the flow of information. e bottom layer has no incoming arrows and is while summing is the most common operation other functions such as a max are also possible. feed-forward neural networks the input to the network. e top-most layer has no outgoing arrows and is the output of the network. e other layers are considered hidden. e sigmoid shape inside the neurons in the middle layers represent a nonlinear function the logistic function c that is applied to the neuron s value before passing it to the output. in the figure each neuron is connected to all of the neurons in the next layer this is called a fully connected layer or an affine layer. figure feed-forward neural network with two hidden layers. while the brain metaphor is sexy and intriguing it is also distracting and cumbersome to manipulate mathematically. we therefore switch back to using more concise mathematical notation. as will soon become apparent a feed-forward network as the one in figure is simply a stack of linear models separated by nonlinear functions. e values of each row of neurons in the network can be thought of as a vector. in figure the input layer is a vector and the layer above it is a vector e fully connected layer can be thought of as a linear transformation from dimensions to dimensions. a fully connected layer implements a vector-matrix multiplication h d xw where the weight of the connection from the ith neuron in the input row to the jth neuron in the output row is w e values of h are then transformed by a nonlinear function g that is applied to each value before being passed on as input to the next layer. e whole computation from input to output can be written as where w are the weights of the first layer and w are the weights of the second one. taking this view the single neuron in figure is equivalent to a logistic binary classifier without a bias term to see why this is the case denote the weight of the ith input of the jth neuron in h as w e value of is then w output layerhidden layerhidden layerinput in mathematical notation from this point on we will abandon the brain metaphor and describe networks exclusively in terms of vector-matrix operations. e simplest neural network is called a perceptron. it is simply a linear model in mathematical notation nnperceptron.x d xw c b x rdin w b rdout where w is the weight matrix and b is a bias term. in order to go beyond linear functions we introduce a nonlinear hidden layer network in figure has two such layers resulting in the multi layer perceptron with one hidden-layer a feed-forward neural network with one hidden-layer has the form d g.xw c c x rdin w w here w and are a matrix and a bias term for the first linear transformation of the input g is a nonlinear function that is applied element-wise called a nonlinearity or an activation function and w and are the matrix and bias term for a second linear transform. breaking it down xw c is a linear transformation of the input x from din dimensions to dimensions. g is then applied to each of the dimensions and the matrix w together with bias vector are then used to transform the result into the dimensional output vector. e nonlinear activation function g has a crucial role in the network s ability to represent complex functions. without the nonlinearity in g the neural network can only represent linear transformations of the input. taking the view in chapter the first layer transforms the data into a good representation while the second layer applies a linear classifier to that representation. we can add additional linear-transformations and nonlinearities resulting in an mlp with two hidden-layers network in figure is of this form d c c it is perhaps clearer to write deeper networks like this using intermediary variables dy c c y e network in figure does not include bias terms. a bias term can be added to a layer by adding to it an additional neuron that does not have any incoming connections whose value is always to see why consider that a sequence of linear transformations is still a linear transformation. feed-forward neural networks e vector resulting from each linear transform is referred to as a layer. e outer-most linear transform results in the output layer and the other linear transforms result in hidden layers. each hidden layer is followed by a nonlinear activation. in some cases such as in the last layer of our example the bias vectors are forced to dropped layers resulting from linear transformations are often referred to as fully connected or affine. other types of architectures exist. in particular image recognition problems benefit from convolutional and pooling layers. such layers have uses also in language processing and will be discussed in chapter networks with several hidden layers are said to be deep networks hence the name deep learning. when describing a neural network one should specify the dimensions of the layers and the input. a layer will expect a din dimensional vector as its input and transform it into a dout dimensional vector. e dimensionality of the layer is taken to be the dimensionality of its output. for a fully connected layer l.x d xw c b with input dimensionality din and output dimensionality dout the dimensions of x is din of w is din dout and of b is dout. like the case with linear models the output of a neural network is a dout dimensional vector. in case dout d the network s output is a scalar. such networks can be used for regression scoring by considering the value of the output or for binary classification by consulting the sign of the output. networks with dout d k can be used for k-class classification by associating each dimension with a class and looking for the dimension with maximal value. similarly if the output vector entries are positive and sum to one the output can be interpreted as a distribution over class assignments output normalization is typically achieved by applying a softmax transformation on the output layer see section e matrices and the bias terms that define the linear transformations are the parameters of the network. like in linear models it is common to refer to the collection of all parameters as together with the input the parameters determine the network s output. e training algorithm is responsible for setting their values such that the network s predictions are correct. unlike linear models the loss function of multi-layer neural networks with respect to their parameters is not convex making search for the optimal parameter values intractable. still the gradient-based optimization methods discussed in section can be applied and perform very well in practice. training neural networks is discussed in detail in chapter representation power in terms of representation power it was shown by hornik et al. and cybenko that is a universal approximator it can approximate with any desired non-zero amount of error a family of functions that includes all continuous functions on a closed and bounded subset of rn and any function mapping from any finite dimensional discrete space to another. is strictly convex functions have a single optimal solution making them easy to optimize using gradient-based methods. specifically a feed-forward network with linear output layer and at least one hidden layer with a squashing activation function can approximate any borel measurable function from one finite dimensional space to another. e proof was later extended by leshno et al. to a wider range of activation functions including the relu function g.x d x. common nonlinearities may suggest there is no reason to go beyond to more complex architectures. however the theoretical result does not discuss the learnability of the neural network states that a representation exists but does not say how easy or hard it is to set the parameters based on training data and a specific learning algorithm. it also does not guarantee that a training algorithm will find the correct function generating our training data. finally it does not state how large the hidden layer should be. indeed telgarsky show that there exist neural networks with many layers of bounded size that cannot be approximated by networks with fewer layers unless these layers are exponentially large. in practice we train neural networks on relatively small amounts of data using local search methods such as variants of stochastic gradient descent and use hidden layers of relatively modest sizes to several thousands. as the universal approximation theorem does not give any guarantees under these non-ideal real-world conditions there is definitely benefit to be had in trying out more complex architectures than in many cases however does indeed provide strong results. for further discussion on the representation power of feed-forward neural networks see bengio et al. section common nonlinearities e nonlinearity g can take many forms. ere is currently no good theory as to which nonlinearity to apply in which conditions and choosing the correct nonlinearity for a given task is for the most part an empirical question. i will now go over the common nonlinearities from the literature the sigmoid tanh hard tanh and the rectified linear unit some nlp researchers also experimented with other forms of nonlinearities such as cube and tanh-cube. sigmoid e sigmoid activation function d c also called the logistic function is an s-shaped function transforming each value x into the range e sigmoid was the canonical nonlinearity for neural networks since their inception but is currently considered to be deprecated for use in internal layers of neural networks as the choices listed below prove to work much better empirically. activation function is an hyperbolic tangent e hyperbolic tangent tanh.x d s-shaped function transforming the values x into the range hard tanh e hard-tanh activation function is an approximation of the tanh function which is faster to compute and to find derivatives thereof x x otherwise hardtanh.x d x rectifier e rectifier activation function et al. also known as the rectified linear unit is a very simple activation function that is easy to work with and was shown many feed-forward neural networks times to produce excellent results. e relu unit clips each value x at despite its simplicity it performs well for many tasks especially when combined with the dropout regularization technique section relu.x d x d x x otherwise as a rule of thumb both relu and tanh units work well and significantly outperform the sigmoid. you may want to experiment with both tanh and relu activations as each one may perform better in different settings. figure shows the shapes of the different activations functions together with the shapes of their derivatives. figure activation functions and their derivatives loss functions when training a neural network on training in chapter much like when training a linear classifier one defines a loss function l. oy y stating the loss of predicting oy when the true output is y. e training objective is then to minimize the loss across the different training examples. e loss l. oy y assigns a numerical score scalar to the network s output oy given the true expected output y. e loss functions discussed for linear models in section are relevant and widely used also for neural networks. for further discussion on loss functions in the e technical advantages of the relu over the sigmoid and tanh activation functions is that it does not involve expensiveto-compute functions and more importantly that it does not saturate. e sigmoid and tanh activation are capped at and the gradients at this region of the functions are near zero driving the entire gradient near zero. e relu activation does not have this problem making it especially suitable for networks with multiple layers which are susceptible to the vanishing gradients problem when trained with the saturating units. f x f x f x f x regularization and dropout context of neural networks see lecun and huang lecun et al. and bengio et al. regularization and dropout multi-layer networks can be large and have many parameters making them especially prone to overfitting. model regularization is just as important in deep neural networks as it is in linear models and perhaps even more so. e regularizers discussed in section namely and the elastic-net are also relevant for neural networks. in particular regularization also called weight decay is effective for achieving good generalization performance in many cases and tuning the regularization strength is advisable. another effective technique for preventing neural networks from overfitting the training data is dropout training et al. srivastava et al. e dropout method is designed to prevent the network from learning to rely on specific weights. it works by randomly dropping to half of the neurons in the network in a specific layer in each training example in the stochastic-gradient training. for example consider the multi-layer perceptron with two hidden layers dy c c y when applying dropout training to we randomly set some of the values of and to at each training round dy c c y d here and are random masking vectors with the dimensions of and respectively and is the element-wise multiplication operation. e values of the elements in the masking feed-forward neural networks vectors are either or and are drawn from a bernouli distribution with parameter r r d e values corresponding to zeros in the masking vectors are then zeroed out replacing the hidden layers h with qh before passing them on to the next layer. work by wager et al. establishes a strong connection between the dropout method and regularization. another view links dropout to model averaging and ensemble techniques et al. e dropout technique is one of the key factors contributing to very strong results of neuralnetwork methods on image classification tasks et al. especially when combined with relu activation units et al. e dropout technique is effective also in nlp applications of neural networks. we sometimes wish to calculate a scalar value based on two vectors such that the value reflects the similarity compatibility or distance between the two vectors. for example vectors rd and rd may be the output layers of two mlps and we would like to train the network to produce similar vectors for some training examples and dissimilar vectors for others. in what follows we describe common functions that take two vectors u rd and v rd and return a scalar. ese functions can often are integrated in feed-forward neural networks. dot product a very common options is to use the dot-product similarity and distance layers dx simdot.u v du v d vuut dx dp.u v v d jju euclidean distance another popular options is the euclidean distance disteuclidean.u v d note that this is a distance metric and not a similarity here small zero values indicate similar vectors and large values dissimilar ones. e square-root is often omitted. trainable forms e dot-product and the euclidean distance above are fixed functions. we sometimes want to use a parameterized function that can be trained to produce desired similarity dissimilarity values by focusing on specific dimensions of the vectors. a common trainable similarity function is the bilinear form embedding layers simbilinear.u v d um v m where the matrix m is a parameter that needs to be trained. similarly for a trainable distance function we can use dist.u v d vm v finally a multi-layer perceptron with a single output neuron can also be used for producing a scalar from two vectors by feeding it the concatenation of the two vectors. embedding layers as will be further discussed in chapter when the input to the neural network contains symbolic categorical features features that take one of k distinct symbols such as words from a closed vocabulary it is common to associate each possible feature value each word in the vocabulary with a d-dimensional vector for some d. ese vectors are then considered parameters of the model and are trained jointly with the other parameters. e mapping from a symbolic feature values such as word number to d-dimensional vectors is performed by an embedding layer called a lookup layer. e parameters in an embedding layer are simply a matrix e where each row corresponds to a different word in the vocabulary. e lookup operation is then simply indexing d e if the symbolic feature is encoded as a one-hot vector x the lookup operation can be implemented as the multiplication xe. e word vectors are often concatenated to each other before being passed on to the next layer. embeddings are discussed in more depth in chapter when discussing dense representations of categorical features and in chapter when discussing pre-trained word representations. c h a p t e r neural network training similar to linear models neural network are differentiable parameterized functions and are trained using gradient-based optimization section e objective function for nonlinear neural networks is not convex and gradient-based methods may get stuck in a local minima. still gradient-based methods produce good results in practice. gradient calculation is central to the approach. e mathematics of gradient computation for neural networks are the same as those of linear models simply following the chain-rule of differentiation. however for complex networks this process can be laborious and error-prone. fortunately gradients can be efficiently and automatically computed using the backpropagation algorithm et al. rumelhart et al. e backpropagation algorithm is a fancy name for methodically computing the derivatives of a complex expression using the chainrule while caching intermediary results. more generally the backpropagation algorithm is a special case of the reverse-mode automatic differentiation algorithm section et al. bengio e following section describes reverse mode automatic differentiation in the context of the computation graph abstraction. e rest of the chapter is devoted to practical tips for training neural networks in practice. the computation graph abstraction while one can compute the gradients of the various parameters of a network by hand and implement them in code this procedure is cumbersome and error prone. for most purposes it is preferable to use automatic tools for gradient computation e computationgraph abstraction allows us to easily construct arbitrary networks evaluate their predictions for given inputs pass and compute gradients for their parameters with respect to arbitrary scalar losses pass. a computation graph is a representation of an arbitrary mathematical computation as a graph. it is a directed acyclic graph in which nodes correspond to mathematical operations or variables and edges correspond to the flow of intermediary values between the nodes. e graph structure defines the order of the computation in terms of the dependencies between the different components. e graph is a dag and not a tree as the result of one operation can be the input of several continuations. consider for example a graph for the computation of b c b c neural network training e computation of a b is shared. we restrict ourselves to the case where the computation graph is connected a disconnected graph each connected component is an independent function that can be evaluated and differentiated independently of the other connected components. figure input expected output and a final loss node. graph with unbound input. graph with concrete input. graph with concrete since a neural network is essentially a mathematical expression it can be represented as a computation graph. for example figure presents the computation graph for an mlp with one hidden-layer and a softmax output transformation. in our notation oval nodes represent the black dog the black dog the computation graph abstraction mathematical operations or functions and shaded rectangle nodes represent parameters variables. network inputs are treated as constants and drawn without a surrounding node. input and parameter nodes have no incoming arcs and output nodes have no outgoing arcs. e output of each node is a matrix the dimensionality of which is indicated above the node. is graph is incomplete without specifying the inputs we cannot compute an output. figure shows a complete graph for an mlp that takes three words as inputs and predicts the distribution over part-of-speech tags for the third word. is graph can be used for prediction but not for training as the output is a vector a scalar and the graph does not take into account the correct answer or the loss term. finally the graph in figure shows the computation graph for a specific training example in which the inputs are the of the words the black dog and the expected output is noun index is e pick node implements an indexing operation receiving a vector and an index this case and returning the corresponding entry in the vector. once the graph is built it is straightforward to run either a forward computation the result of the computation or a backward computation the gradients as we show below. constructing the graphs may look daunting but is actually very easy using dedicated software libraries and apis. forward computation e forward pass computes the outputs of the nodes in the graph. since each node s output depends only on itself and on its incoming edges it is trivial to compute the outputs of all nodes by traversing the nodes in a topological order and computing the output of each node given the already computed outputs of its predecessors. more formally in a graph of n nodes we associate each node with an index i according to their topological ordering. let fi be the function computed by node i multiplication. addition etc.. let be the parent nodes of node i and d fj j i the children nodes of node i are the arguments of fi. denote by v.i the output of node i that is the application of fi to the output values of its arguments for variable and input nodes fi is a constant function and is empty. e computation-graph forward pass computes the values v.i for all i n algorithm computation graph forward pass. for i to n do let am d v.i fi v.am neural network training backward computation backprop e backward pass begins by designating a node n with scalar output as a loss-node and running forward computation up to that node. e backward computation computes the gradients of the parameters with respect to that node s value. denote by d.i the quantity e backpropagation algorithm is used to compute the values d.i for all nodes i. e backward pass fills a table of values d.n as in algorithm algorithm computation graph backward pass d.n for i to do d.i d.j f f d x d e backpropagation algorithm is essentially following the chain-rule of differentiation. e quantity is the partial derivative of fj w.r.t the argument i is value depends on the function fj and the values v.am am d of its arguments which were computed in the forward pass. us in order to define a new kind of node one needs to define two methods one for calculating the forward value v.i based on the node s inputs and the another for calculating for each x derivatives of non-mathematical functions while defining for mathematical funcx tions such is as log or c is straightforward some find it challenging to think about the derivative of operations as as pi ck.x that selects the fifth element of a vector. e answer is to think in terms of the contribution to the computation. after picking the ith element of a vector only that element participates in the remainder of the computation. us the gradient of pi ck.x is a vector g with the dimensionality of x where d and d similarly for the function x the value of the gradient is for x and otherwise. for further information on automatic differentiation see neidinger section and baydin et al. for more in depth discussion of the backpropagation algorithm and computation graphs called flow graphs see bengio et al. section and bengio lecun et al. for a popular yet technical presentation see chris olah s description at the computation graph abstraction software several software packages implement the computation-graph model including eano et al. tensorflow et al. chainer and dynet et al. all these packages support all the essential components types for defining a wide range of neural network architectures covering the structures described in this book and more. graph creation is made almost transparent by use of operator overloading. e framework defines a type for representing graph nodes called expressions methods for constructing nodes for inputs and parameters and a set of functions and mathematical operations that take expressions as input and result in more complex expressions. for example the python code for creating the computation graph from figure using the dynet framework is i n i t i a l i z a t i o n import dynet as dy model model dy model model add_parameters model add_parameters model add_parameters model add_parameters lookup model add_lookup_parameters trainer dy simplesgdtrainermodel def get_index pass logic omitted maps words to numeric ids the following builds and executes the computation graph and updates model parameters only one data point i s shown should run in a data feeding loop in practice the following building the computation graph dy renew_cg create a new graph wrap the model parameters as graph nodes dy parameter dy parameter dy parameter dy parameter generate the embeddings layer vthe vblack dy lookup get_index black vdog dy lookup get_index the dy lookup get_index dog connect the l e a f nodes into a complete graph x dy concatenate vthe vblack vdog output dy softmax tanh l o s s log pick output httpdeeplearning.netsoftwaretheano httpswww.tensorflow.org httpchainer.org httpsgithub.comclabdynet neural network training loss_value l o s s forward l o s s backward the gradient i s computed and stored in the corresponding parameters trainer update update the parameters according to the gradients most of the code involves various initializations the first block defines model parameters that are be shared between different computation graphs that each graph corresponds to a specific training example. e second block turns the model parameters into the graph-node types. e third block retrieves the expressions for the embeddings of the input words. finally the fourth block is where the graph is created. note how transparent the graph creation is there is an almost a one-to-one correspondence between creating the graph and describing it mathematically. e last block shows a forward and backward pass. e equivalent code in the tensorflow package is import tensorflow as t f t f get_variable t f get_variable t f get_variable t f get_variable lookup t f get_variable w def get_index pass logic omitted t f placeholder t f t f placeholder t f t f placeholder t f target t f placeholder t f t f nn embedding_lookup lookup t f nn embedding_lookup lookup t f nn embedding_lookup lookup x t f concat output t f nn softmax t f einsum i j j i t f tanh t f einsum i j j i x l o s s t f log output target trainer t f train gradientdescentoptimizer minimize l o s s graph d e f i n i t i o n done compile i t and feed concrete data only one data point a data feeding loop with t f session as sess in practice we w i l l use i s shown sess run t f global_variables_initializer feed_dict get_index the get_index black get_index dog tensorflow code provided by tim rockt schel. anks tim! the computation graph abstraction target loss_value sess run loss feed_dict update no c a l l of backward necessary sess run trainer feed_dict e main difference between dynet chainer to tensorflow eano is that the formers use dynamic graph construction while the latters use static graph construction. in dynamic graph construction a different computation graph is created from scratch for each training sample using code in the host language. forward and backward propagation are then applied to this graph. in contrast in the static graph construction approach the shape of the computation graph is defined once in the beginning of the computation using an api for specifying graph shapes with place-holder variables indicating input and output values. en an optimizing graph compiler produces an optimized computation graph and each training example is fed into the optimized graph. e graph compilation step in the static toolkits and eano is both a blessing and a curse. on the one hand once compiled large graphs can be run efficiently on either the cpu or a gpu making it ideal for large graphs with a fixed structure where only the inputs change between instances. however the compilation step itself can be costly and it makes the interface more cumbersome to work with. in contrast the dynamic packages focus on building large and dynamic computation graphs and executing them on the fly without a compilation step. while the execution speed may suffer compared to the static toolkits in practice the computation speeds of the dynamic toolkits are very competitive. e dynamic packages are especially convenient when working with the recurrent and recursive networks described in chapters and as well as in structured prediction settings as described in chapter in which the graphs of different data-points have different shapes. see neubig et al. for further discussion on the dynamic-vs.-static approaches and speed benchmarks for the different toolkits. finally packages such as keras provide a higher level interface on top of packages such as eano and tensorflow allowing the definition and training of complex neural networks with even fewer lines of code provided that the architectures are well established and hence supported in the higher-level interface. implementation recipe using the computation graph abstraction and dynamic graph construction the pseudo-code for a network training algorithm is given in algorithm here build_computation_graph is a user-defined function that builds the computation graph for the given input output and network structure returning a single loss node. update_parameters is an optimizer specific update rule. e recipe specifies that a new graph is created for each training example. is accommodates cases in which the network structure varies between training examples such as recurrent and recursive neural networks to be discussed in httpskeras.io neural network training for training example xi yi in dataset do algorithm neural network training with computation graph abstraction minibatches of size define network parameters. for iteration to t do return parameters. loss_node build_computation_graphxi yi parameters loss_node.forward gradients loss_node.backward parameters update_parametersparameters gradients chapters for networks with fixed structures such as an mlps it may be more efficient to create one base computation graph and vary only the inputs and expected outputs between examples. network composition as long as the network s output is a vector k matrix it is trivial to compose networks by making the output of one network the input of another creating arbitrary networks. e computation graph abstractions makes this ability explicit a node in the computation graph can itself be a computation graph with a designated output node. one can then design arbitrarily deep and complex networks and be able to easily evaluate and train them thanks to automatic forward and gradient computation. is makes it easy to define and train elaborate recurrent and recursive networks as discussed in chapters and as well as networks for structured outputs and multi-objective training as we discuss in chapters and once the gradient computation is taken care of the network is trained using sgd or another gradient-based optimization algorithm. e function being optimized is not convex and for a long time training of neural networks was considered a black art which can only be done by selected few. indeed many parameters affect the optimization process and care has to be taken to tune these parameters. while this book is not intended as a comprehensive guide to successfully training neural networks we do list here a few of the prominent issues. for further discussion on optimization techniques and algorithms for neural networks refer to bengio et al. chapter for some theoretical discussion and analysis refer to glorot and bengio for various practical tips and recommendations see bottou lecun et al. practicalities practicalities initialization choice of optimization algorithm while the sgd algorithm works well it may be slow to converge. section lists some alternative more advanced stochastic-gradient algorithms. as most neural network software frameworks provide implementations of these algorithms it is easy and often worthwhile to try out different variants. in my research group we found that when training larger networks using the adam algorithm and ba is very effective and relatively robust to the choice of the learning rate. e non-convexity of the objective function means the optimization procedure may get stuck in a local minimum or a saddle point and that starting from different initial points different random values for the parameters may result in different results. us it is advised to run several restarts of the training starting at different random initializations and choosing the best one based on a development set. e amount of variance in the results due to different random seed selections is different for different network formulations and datasets and cannot be predicted in advance. e magnitude of the random values has a very important effect on the success of training. an effective scheme due to glorot and bengio called xavier initialization after glorot s first name suggests initializing a weight matrix w as pdin c dout pdin c dout ation isq where u is a uniformly sampled random value in the range e suggestion is based on properties of the tanh activation function works well in many situations and is the preferred default initialization method by many. analysis by he et al. suggests that when using relu nonlinearities the weights should be initialized by sampling from a zero-mean gaussian distribution whose standard devidin. is initialization was found by he et al. to work better than xavier initial ization in an image classification task especially when deep networks were involved. restarts and ensembles when training complex networks different random initializations are likely to end up with different final solutions exhibiting different accuracies. us if your computational resources allow it is advisable to run the training process several times each with a different random initialization and choose the best one on the development set. is technique is called random restarts. e average model accuracy across random seeds is also interesting as it gives a hint as to the stability of the process. when debugging and for reproducibility of results it is advised to used a fixed random seed. w u neural network training while the need to tune the random seed used to initialize models can be annoying it also provides a simple way to get different models for performing the same task facilitating the use model ensembles. once several models are available one can base the prediction on the ensemble of models rather than on a single one example by taking the majority vote across the different models or by averaging their output vectors and considering the result as the output vector of the ensembled model. using ensembles often increases the prediction accuracy at the cost of having to run the prediction step several times for each model. vanishing and exploding gradients in deep networks it is common for the error gradients to either vanish exceedingly close to or explode exceedingly high as they propagate back through the computation graph. e problem becomes more severe in deeper networks and especially so in recursive and recurrent networks et al. dealing with the vanishing gradients problem is still an open research question. solutions include making the networks shallower step-wise training train the first layers based on some auxiliary output signal then fix them and train the upper layers of the complete network based on the real task signal performing batch-normalization and szegedy every minibatch normalizing the inputs to each of the network layers to have zero mean and unit variance or using specialized architectures that are designed to assist in gradient flow the lstm and gru architectures for recurrent networks discussed in chapter dealing with the exploding gradients has a simple but very effective solution clipping the gradients if their norm exceeds a given threshold. let og be the gradients of all parameters og if in the network and k ogk be their norm. pascanu et al. suggest to set og threshold k ogk k ogk threshold. layers with tanh and sigmoid activations can become saturated resulting in output values for that layer that are all close to one the upper-limit of the activation function. saturated neurons have very small gradients and should be avoided. layers with the relu activation cannot be saturated but can die most or all values are negative and thus clipped at zero for all inputs resulting in a gradient of zero for that layer. if your network does not train well it is advisable to monitor the network for layers with many saturated or dead neurons. saturated neurons are caused by too large values entering the layer. is may be controlled for by changing the initialization scaling the range of the input values or changing the learning rate. dead neurons are caused by all signals entering the layer being negative example this can happen after a large gradient update. reducing the learning rate will help in this situation. for saturated layers another option is to normalize the values in the saturated layer after the activation i.e. instead of g.h d tanh.h using g.h d layer normalization is an effective measure for countering saturation but is also expensive in terms of gradient computation. a related technique is batch normalization due saturation and dead neurons tanh.h ktanh.hk shuffling practicalities to ioffe and szegedy in which the activations at each layer are normalized so that they have mean and variance across each mini-batch. e batch-normalization techniques became a key component for effective training of deep networks in computer vision. as of this writing it is less popular in natural language applications. e order in which the training examples are presented to the network is important. e sgd formulation above specifies selecting a random example in each turn. in practice most implementations go over the training example in random order essentially performing random sampling without replacement. it is advised to shu e the training examples before each pass through the data. learning rate selection of the learning rate is important. too large learning rates will prevent the network from converging on an effective solution. too small learning rates will take a very long time to converge. as a rule of thumb one should experiment with a range of initial learning rates in range e.g. monitor the network s loss over time and decrease the learning rate once the loss stops improving on a held-out development set. learning rate scheduling decreases the rate as a function of the number of observed minibatches. a common schedule is dividing the initial learning rate by the iteration number. l on bottou recommends using a learning rate of the form d c where is the initial learning rate is the learning rate to use on the tth training example and is an additional hyperparameter. he further recommends determining a good value of based on a small sample of the data prior to running on the entire dataset. minibatches parameter updates occur either every training example of size or every k training examples. some problems benefit from training with larger minibatch sizes. in terms of the computation graph abstraction one can create a computation graph for each of the k training examples and then connecting the k loss nodes under an averaging node whose output will be the loss of the minibatch. large minibatched training can also be beneficial in terms of computation efficiency on specialized computing architectures such as gpus and replacing vector-matrix operations by matrix-matrix operations. is is beyond the scope of this book. part ii working with natural language data c h a p t e r features for textual data in the previous chapters we discussed the general learning problem and saw some machine learning models and algorithms for training them. all of these models take as input vectors x and produce predictions. up until now we assumed the vectors x are given. in language processing the vectors x are derived from textual data in order to reflect various linguistic properties of the text. e mapping from textual data to real valued vectors is called feature extraction or feature representation and is done by a feature function. deciding on the right features is an integral part of a successful machine learning project. while deep neural networks alleviate a lot of the need in feature engineering a good set of core features still needs to be defined. is is especially true for language data which comes in the form of a sequence of discrete symbols. is sequence needs to be converted somehow to a numerical vector in a non-obvious way. we now diverge from the training machinery in order to discuss the feature functions that are used for language data which will be the topic of the next few chapters. is chapter provides an overview of the common kinds of information sources that are available for use as features when dealing with textual language data. chapter discusses feature choices for some concrete nlp problems. chapter deals with encoding the features as input vectors that can be fed to a neural network. typology of nlp classification problems generally speaking classification problems in natural language can be categorized into several broad categories depending on the item being classified problems in natural language processing do not fall neatly into the classification framework. for example problems in which we are required to produce sentences or longer texts i.e. in document summarization and machine translation. ese will be discussed in chapter word in these problems we are faced with a word such as dog magnificent magnifficant or parlez and need to say something about it does it denote a living thing? what language is it in? how common is it? what other words are similar to it? is it a mis-spelling of another word? and so on. ese kind of problems are actually quite rare as words seldom appear in isolation and for many words their interpretation depends on the context in which they are used. texts in these problems we are faced with a piece of text be it a phrase a sentence a paragraph or a document and need to say something about it. is it spam or not? is it about politics or features for textual data sports? is it sarcastic? is it positive negative or neutral some issue? who wrote it? is it reliable? which of a fixed set of intents does this text reflect none? will this text be liked by years old males? and so on. ese types of problems are very common and we ll refer to them collectively as document classification problems. paired texts in these problems we are given a pair of words or longer texts and need to say something about the pair. are words a and b synonyms? is word a a valid translation for word b? are documents a and b written by the same author? can the meaning of sentence a be inferred from sentence b? word in context here we are given a piece of text and a particular word phrase or letter etc. within it and we need to classify the word in the context of the text. for example is the word book in i want to book a flight a noun a verb or an adjective? is the word apple in a given context referring to a company or a fruit? is on the right preposition to use in i read a book on london? does a given period denote a sentence boundary or an abbreviation? is the given word part of a name of a person location or organization? and so on. ese types of questions often arise in the context of larger goals such as annotating a sentence for parts-of-speech splitting a document into sentences finding all the named entities in a text finding all documents mentioning a given entity and so on. relation between two words here we are given two words or phrases within the context of a larger document and need to say something about the relations between them. is word a the subject of verb b in a given sentence? does the purchase relation hold between words a and b in a given text? and so on. many of these classification cases can be extended to structured problems in which we are interested in performing several related classification decisions such that the answer to one decision can influence others. ese are discussed in chapter what is a word? we are using the term word rather loosely. e question what is a word? is a matter of debate among linguists and the answer is not always clear. one definition is the one being loosely followed in this book is that words are sequences of letters that are separated by whitespace. is definition is very simplistic. first punctuation in english is not separated by whitespace so according to our definition dog dog? dog. and dog are all different words. our corrected definition is then words separated by whitespace or punctuation. a process called tokenization is in charge of splitting text into tokens we call here words based on whitespace and punctuation. in english the job of the tokenizer is quite simple although it does need to consider cases such as abbreviations and titles that needn t be split. in other languages things can become much tricker in hebrew and arabic some words attach to the next one without whitespace and in chinese there are no whitespaces at all. ese are just a few examples. features for nlp problems when working in english or a similar language this book assumes tokenizing on whitespace and punctuation handling a few corner cases can provide a good approximation of words. however our definition of word is still quite technical it is derived from the way things are written. another common better definition take a word to be the smallest unit of meaning. by following this definition we see that our whitespace-based definition is problematic. after splitting by whitespace and punctuation we still remain with sequences such as don t that are actually two words do not that got merged into one symbol. it is common for english tokenizers to handle these cases as well. e symbols cat and cat have the same meaning but are they the same word? more interestingly take something like new york is it two words or one? what about ice cream? is it the same as ice-cream or icecream? and what about idioms such as kick the bucket? in general we distinguish between words and tokens. we refer to the output of a tokenizer as a token and to the meaning-bearing units as words. a token may be composed of multiple words multiple tokens can be a single word and sometimes different tokens denote the same underlying word. having said that in this book we use the term word very loosely and take it to be interchangeable with token. it is important to keep in mind however that the story is more complex than that. features for nlp problems in what follows we describe the common features that are used for the above problems. as words and letters are discrete items our features often take the form of indicators or counts. an indicator feature takes a value of or depending on the existence of a condition a feature taking the value of if the word dog appeared at least once in the document and otherwise. a count takes a value depending on the number of times some event occurred e.g. a feature indicating the number of times the word dog appears in the text. directly observable properties features for single words when our focus entity is a word outside of a context our main source of information is the letters comprising the word and their order as well as properties derived from these such as the length of the word the orthographic shape of the word the first letter capitalized? are all letters capitalized? does the word include a hyphen? does it include a digit? and so on and prefixes and suffixes of the word it start with un? does it end with ing?. we may also look at the word with relation to external sources of information how many times does the word appear in a large collection of text? does the word appear in a list of common person names in the u.s.? and so on. lemmas and stems we often look at the lemma dictionary entry of the word mapping forms such as booking booked books to their common lemma book. is mapping is usually per features for textual data formed using lemma lexicons or morphological analyzers that are available for many languages. e lemma of a word can be ambiguous and lemmatizing is more accurate when the word is given in context. lemmatization is a linguistically defined process and may not work well for forms that are not in the lemmatization lexicon or for mis-spelling. a coarser process than lemmatization that can work on any sequence of letters is called stemming. a stemmer maps sequences of words to shorter sequences based on some language-specific heuristics such that different inflections will map to the same sequence. note that the result of stemming need not be a valid word picture and pictures and pictured will all be stemmed to pictur. various stemmers exist with different levels of aggressiveness. lexical resources an additional source of information about word forms are lexical resources. ese are essentially dictionaries that are meant to be accessed programmatically by machines rather than read by humans. a lexical resource will typically contain information about words linking them to other words andor providing additional information. for example for many languages there are lexicons that map inflected word forms to their possible morphological analyses telling you that a certain word may be either a plural feminine noun or a past-perfect verb. such lexicons will typically also include lemma information. a very well-known lexical resource in english is wordnet wordnet is a very large manually curated dataset attempting to capture conceptual semantic knowledge about words. each word belongs to one or several synsets where each synsets describes a cognitive concept. for example the word star as a noun belongs to the synsets astronomicalcelestialbody someone who is dazzlingly skilled any celestial body visible from earth and an actor who plays a principle role among others. e second synset of star contains also the words ace adept champion sensation maven virtuoso among others. synsets are linked to each other by means of semantic relations such as hypernymy and hyponymy specific or less specific words. for example for the first synset of star these would include sun and nova and celestial body other semantic relations in wordnet contain antonyms words and holonyms and meronyms and whole-part relations. wordnet contains information about nouns verbs adjectives and adverbs. framenet et al. and verbnet et al. are manually curated lexical resources that focus around verbs listing for many verbs the kinds of argument they take that giving involves the core arguments d r and t thing that is being given and may have non-core arguments such as t p p and m among others. e paraphrase database et al. pavlick et al. is a large automatically created dataset of paraphrases. it lists words and phrases and for each one provides a list of words and phrases that can be used to mean roughly the same thing. lexical resources such as these contain a lot of information and can serve a good source of features. however the means of using such symbolic information effectively is task dependent features for nlp problems and often requires non-trivial engineering efforts andor ingenuity. ey are currently not often used in neural network models but this may change. distributional information another important source of information about words is distributional which other words behave similar to it in the text? ese deserve their own separate treatment and are discussed in section below. in section we discuss how lexical resources can be used to inject knowledge into distributional word vectors that are derived from neural network algorithms. features for text when we consider a sentence a paragraph or a document the observable features are the counts and the order of the letters and the words within the text. bag of words a very common feature extraction procedures for sentences and documents is the bag-of-words approach in this approach we look at the histogram of the words within the text i.e. considering each word count as a feature. by generalizing from words to basic elements the bag-of-letter-bigrams we used in the language identification example in section is an example of the bag-of-words approach. wecan alsocomputequantitiesthataredirectlyderivedfromthewordsand theletterssuch as the length of the sentence in terms of number of letters or number of words. when considering individual words we may of course use the word-based features from above counting for example the number of words in the document that have a specific prefix or suffix or compute the ratio of short words length below a given length to long words in a document. weighting as before we can also integrate statistics based on external information focusing for example on words that appear many times in the given document yet appear relatively few times in an external set of documents will distinguish words that have high counts in the documents because they are generally common like a and for from words that have a high count because they relate to the document s topic. when using the bag-of-words approach it is common to use tf-idf weighting et al. chapter consider a document d which is part of a larger corpus d. rather than representing each word w in d by its normalized count term frequency tf-idf weighting represent it instead by in the document log e second term is the inverse document frequency the inverse of the number of distinct documents in the corpus in which this word occurred. is highlights words that are distinctive of the current text. besides words one may also look at consecutive pairs or triplets of words. ese are called ngrams. ngram features are discussed in depth in section features of words in context when considering a word within a sentence or a document the directly observable features of the word are its position within the sentence as well as the words or letters surrounding it. words that are closer to the target word are often more informative about it than words that are further apart. however note that this is a gross generalization and in many cases language exhibit a long-range dependencies between words a word at the end of a text may well be influenced by a word at the beginning. jdj features for textual data windows for this reason it is often common to focus on the immediate context of a word by considering a window surrounding it k words to each side with typical values of k being and and take the features to be the identities of the words within the window a feature will be word x appeared within a window of five words surrounding the target word for example consider the sentence thebrownfoxjumpedoverthelazydog with the target word jumped. a window of words to each side will produce the set of features wordbrown wordfox wordover wordthe e window approach is a version of the bag-of-words approach but restricted to items within the small window. e fixed size of the window gives the opportunity to relax the bag-of-word assumption that order does not matter and take the relative positions of the words in the window into account. is results in relative-positional features such as word x appeared two words to the left of the target word. for example in the example above the positional window approach will result in the set of features encoding of window-based features as vectors is discussed in section in chapters and we will introduce the birnn architecture that generalizes window features by providing a flexible adjustable and trainable window. position besides the context of the word we may be interested in its absolute position within a sentence. we could have features such as the target word is the word in the sentence or a binned version indicating more coarse grained categories does it appear within the first words between word and and so on. features for word relations when considering two words in context besides the position of each one and the words surrounding them we can also look at the distance between the words and the identities of the words that appear between them. inferred linguistic properties sentences in natural language have structures beyond the linear order of their words. e structure follows an intricate set of rules that are not directly observable to us. ese rules are collectively referred to as syntax and the study of the nature of these rules and regularities in natural language is the study-object of linguistics. while the exact structure of language is still a mystery and rules governing many of the more intricate patterns are either unexplored or still open for debate among linguists a subset of phenomena governing language are well documented and well understood. ese include concepts such as word classes tags morphology syntax and even parts of semantics. while the linguistic properties of a text are not observable directly from the surface forms of words in sentences and their order they can be inferred from the sentence string with vary is last sentence is of course a gross simplification. linguistics has much wider breadth than syntax and there are other systems that regulate the human linguistic behavior besides the syntactic one. but for the purpose of this introductory book this simplistic view will be sufficient. for a more in depth overview see the further reading recommendations at the end of this section. features for nlp problems ing degrees of accuracy. specialized systems exist for the prediction of parts of speech syntactic trees semantic roles discourse relations and other linguistic properties with various degrees of accuracy and these predictions often serve as good features for further classification problems. linguistic annotation let s explore some forms of linguistic annotations. consider the sentence the boy with the black shirt opened the door with a key. one level of annotation assigns to each word its part of speech the opened key v d n p d n door with the boy the shirt d n p d a n black with a going further up the chain we mark syntactic chunk boundaries indicating the the boy is a noun phrase. the boy with the black shirt opened the door with a key note that the word opened is marked as a verbal-chunk is may not seem very useful because we already know its a verb. however vp chunks may contain more elements covering also cases such as will opened and did not open. e chunking information is local. a more global syntactic structure is a constituency tree also called a phrase-structure tree constituency trees are nested labeled bracketing over the sentence indicating the hierarchy of syntactic units the noun phrase the boy with the black shirt is made of the noun indeed for many researchers improving the prediction of these linguistic properties is the natural language processing problem they are trying to solve. snpdttheboyopenedininnnnpnpdtdtwithwiththetheblackshirtakeydoorjjnnnndtnnnpppppvbdvp features for textual data phrase the boy and the preposition phrase with the black shirt. e latter itself contains the noun phrase the black shirt. having with a key nested under the vp and not under the np the door signals that with a key modifies the verb opened with a key rather than the np door with a key. a different kind of syntactic annotation is a dependency tree. under dependency syntax each word in the sentence is a modifier of another word which is called its head. each word in the sentence is headed by another sentence word except for the main word usually a verb which is the root of the sentence and is headed by a special root node. while constituencytreesmake explicitthegroupingofwordsinto phrasesdependency trees make explicit the modification relations and connections between words. words that are far apart in the surface form of the sentence may be close in its dependency tree. for example boy and opened have four words between them in the surface form but have a direct nsubj edge connecting them in the dependency tree. e dependency relations are syntactic they are concerned with the structure of the sentence. other kinds of relations are more semantic. for example consider the modifiers of the verb open also called the arguments of the verb. e syntactic tree clearly marks the boy the black shirt the door and with a key as arguments and also tells us that with a key is an argument of open rather than a modifier of door. it does not tell us however what are the semantic-roles of the arguments with respect to the verb i.e. that the boy is the a performing the action and that a key is an i that to the boy opened the door with a smile. here the sentence will have the same syntactic structure but unless we are in a magical-world a smile is a m rather than an i e semantic role labeling annotations reveal these structures the boy with the black shirt opened the door with a keydetprepprepamoddetpobjnsubjrootdobjpobjdetdet features for nlp problems besides the observable properties words counts lengths linear distances frequencies etc. we can also look such inferred linguistic properties of words sentences and documents. for example we could look at the part-of-speech tag of a word within a document it a noun a verb adjective or a determiner? the syntactic role of a word it serve as a subject or an object of a verb? is it the main verb of the sentence? is it used as an adverbial modifier? or the semantic role of it in the key opened the door key acts as an i while in the boy opened the door boy is an a when given two words in a sentence we can consider the syntactic dependency tree of the sentence and the subtree or paths that connect the two words within the this tree as well as properties of that path. words that are far apart in the sentence in terms of the number of words separating them can be close to each other in the syntactic structure. when moving beyond the sentence we may want to look at the discourse relations that connect sentences together such as e c c e and so on. ese relations are often expressed by discourse-connective words such as moreover however and and but are also expressed with less direct cues. another important phenomena is that of anaphora consider the sentence sequence the boy opened the door with a key. wasn t locked and entered the room. saw a man. was smiling. anaphora resolution called coreference resolution will tell us that refers to the door not the key or the boy refers to the boy and is likely to refer to the man. part of speech tags syntactic rolesdiscourse relationsanaphora and so onare conceptsthat are based on linguistic theories that were developed by linguists over a long period of time with the aim of capturing the rules and regularities in the very messy system of the human language. while many aspects of the rules governing language are still open for debate and others may seem overly rigid or simplistic the concepts explored here others do indeed capture a wide and important array of generalizations and regularities in language. are linguistic concepts needed? some proponents of deep-learning argue that such inferred manually designed linguistic properties are not needed and that the neural network will learn these intermediate representations equivalent or better ones on its own. e jury is still out on this. my current personal belief is that many of these linguistic concepts can indeed be inferred by the boy with the black shirt opened the door with a smilethe boy with the black shirt opened the door with a keya!ipmpa! features for textual data the network on its own if given enough data and perhaps a push in the right direction. however for many other cases we do not have enough training data available for the task we care about and in these cases providing the network with the more explicit general concepts can be very valuable. even if we do have enough data we may want to focus the network on certain aspects of the text and hint to it that it should ignore others by providing the generalized concepts in addition to or even instead of the surface forms of the words. finally even if we do not use these linguistic properties as input features we may want to help guide the network by using them as additional supervision in a multi-task learning setup chapter or by designing network architecture or training paradigms that are more suitable for learning certain linguistic phenomena. overall we see enough evidence that the use of linguistic concepts help improve language understanding and production systems. further reading when dealing with natural language text it is well advised to be aware of the linguistic concepts beyond letters and words as well as of the current computational tools and resources that are available. is book barely scratches the surface on this topic. e book of bender provides a good and concise overview of linguistic concepts directed at computationalminded people. for a discussion on current nlp methods tools and resources see the book by jurafsky and martin as well as the various specialized titles in this series. core features vs. combination features in many cases we are interested in a conjunction of features occurring together. for example knowing that the two indicators the word book appeared in a window and the part-of-speech v appeared in a window is strictly less informative than knowing the word book with the assigned part of speech v appeared in a window. similarly if we assign a distinct parameter weight for each indicator feature is the case in linear models then knowing that the two distinct features word in position is like word in position is not occur is almost useless compared to the very indicative combined indicator word in position is like and word in position is not. similarly knowing that a document contains the word paris is an indication toward the document being in the t category and the same holds for the word hilton. however if the document contains both words it is an indication away from the t category and toward the c or g categories. linear models cannot assign a score to a conjunction of events occurred and y occurred and that is not a sum of their individual scores unless the conjunction itself is modeled as its own feature. us when designing features for a linear model we must define not only the core features but also many combination features. e set of possible combination is very large and see for example the experiment in section in which a neural networks learns the concept of subject-verb agreement in english inferring the concepts of nouns verbs grammatical number and some hierarchical linguistics structures. syntactic dependency structures are discussed in k bler et al. and semantic roles in palmer et al. is is a direct manifestation of the xor problem discussed in chapter and the manually defined combination-features are the mapping function that maps the nonlinearly separable vectors of core-features to a higher dimensional space in which the data is more likely to be separable by a linear model. features for nlp problems human expertise coupled with trial and error is needed in order to construct a set of combinations that is both informative and relatively compact. indeed a lot of effort has gone into design decisions such as include features of the form word at position is x and at position is y but do not include features of the form word at position is x and at position is y. neural networks provide nonlinear models and do not suffer from this problem. when using a neural network such as a multi-layer perceptron the model designer can specify only the set of core features and rely on the network training procedure to pick up on the important combinations on its own. is greatly simplifies the work of the model designer. in practice neural networks indeed manage to learn good classifiers based on core features only sometimes surpassing the best linear classifier with human-designed feature combinations. however in many other cases a linear classifier with a good hand-crafted feature-set is hard to beat with the neural network models with core features getting close to but not surpassing the linear models. ngram features a special case of feature combinations is that of ngrams consecutive word sequences of a given length. we already saw letter-bigram features in the language classification case word-bigrams as well as trigrams of three items of letters or words are also common. beyond that and are sometimes used for letters but rarely for words due to sparsity issues. it should be intuitively clear why word-bigrams are more informative than individual words it captures structures such as new york not good and paris hilton. indeed a bag-of-bigrams representation is much more powerful than bag-of-words and in many cases proves very hard to beat. of course not all bigrams are equally informative bigrams such as of the on a the boy etc. are very common and for most tasks not more informative than their individual components. however it is very hard to know a-priori which ngrams will be useful for a given task. e common solution is to include all ngrams up to a given length and let the model regularization discard of the less interesting ones by assigning them very low weights. note that vanilla neural network architectures such as the mlp cannot infer ngram features from a document on their own in the general case a multi-layer perceptron fed with a bag-of-words feature vector of a document could learn combinations such as word x appear in the document and word y appears in the document but not the bigram x y appears in the document. us ngram features are useful also in the context of nonlinear classification. multi-layer perceptrons can infer ngrams when applied to a fixed size windows with positional information the combination of word at position is x and word at position is y is in effect the bigram xy. more specialized neural network architectures such as convolutional networks are designed to find informative ngram features for a given task based on a sequence of words of varying lengths. bidirectional rnns and generalize the ngram concept even further and can be sensitive to informative ngrams of varying lengths as well as ngrams with gaps in them. features for textual data distributional features up until now our treatment of words was as discrete and unrelated symbols the words pizza burger and chair are all equally similar equally dis-similar to each other as far as the algorithm is concerned. we did achieve some form of generalization across word types by mapping them to coarsergrained categories such as parts-of-speech or syntactic roles the a an some are all determiners generalizing from inflected words forms to their lemmas book booking booked all share the lemma book looking at membership in lists or dictionaries john jack and ralph appear in a list of common u.s. first names or looking at their relation to other words using lexical resources such as wordnet. however these solutions are quite limited they either provide very coarse grained distinctions or otherwise rely on specific manually compiled dictionaries. unless we have a specialized list of foods we will not learn that pizza is more similar to burger than it is to chair and it will be even harder to learn that pizza is more similar to burger than it is to icecream. e distributional hypothesis of language set forth by firth and harris states that the meaning of a word can be inferred from the contexts in which it is used. by observing cooccurrence patterns of words across a large body of text we can discover that the contexts in which burger occur are quite similar to those in which pizza occurs less similar to those in which icecream occurs and very different from those in which chair occurs. many algorithms were derived over the years to make use of this property and learn generalizations of words based on the contexts in which they occur. ese can be broadly categorized into clustering-based methods which assign similar words to the same cluster and represent each word by its cluster membership et al. miller et al. and to embedding-based methods which represent each word as a vector such that similar words having a similar distribution have similar vectors and weston mikolov et al. turian et al. discuss and compare these approaches. esealgorithmsuncovermanyfacetsofsimilaritybetweenwordsandcanbeusedtoderive good word features for example one could replace words by their cluster id replacing both the words june and aug by replace rare or unseen words with the common word most similar to them or just use the word vector itself as the representation of the word. however care must be taken when using such word similarity information as it can have unintended consequences. for example in some applications it is very useful to treat london and berlin as similar while for others example when booking a flight or translating a document the distinction is crucial. we will discuss word embeddings methods and the use of word vectors in more detail in chapters and c h a p t e r case studies of nlp features after discussing the different sources of information available for us for deriving features from natural language text we will now explore examples of concrete nlp classification tasks and suitable features for them. while the promise of neural networks is to alleviate the need for manual feature engineering we still need to take these sources of information into consideration when designing our models we want to make sure that the network we design can make effective use of the available signals either by giving it direct access to them by use of feature-engineering by designing the network architecture to expose the needed signals or by adding them as an additional loss signals when training the models. document classification language identification in the language identification task we are given a document or a sentence and want to classify it into one of a fixed set of languages. as we saw in chapter a bag of letter-bigrams is a very strong feature representation for this task. concretely each possible letter-bigram each letter bigram appearing at least k times in at least one language is a core feature and the value of a core feature for a given document is the count of that feature in the document. a similar task is the one of encodingdetection. here a good feature representation is a bag-of byte-bigrams. document classification topic classification in the topic classification task we are given a document and need to classify it into one of a predefined set of topics economy politics sports leisure gossip lifestyle other. here the letter level is not very informative and our basic units will be words. word order is not very informative for this task maybe for consecutive word pairs such as bigrams. us a good set of features will be the bag-of-words in the document perhaps accompanied by a bag-of-word-bigrams word and each word-bigram is a core feature. additionally linear or log-linear models with manually designed features are still very effective for many tasks. ey can be very competitive in terms of accuracy as well as being very easy to train and deploy at scale and easier to reason about and debug than neural networks. if nothing else such models should be considered as strong baselines for whatever networks you are designing. case studies of nlp features if we do not have many training examples we may benefit from pre-processing the document by replacing each word with its lemma. we may also replace or supplement words by distributional features such as word clusters or word-embedding vectors. when using a linear classifier we may want to also consider word pairs i.e. consider each pair of words necessarily consecutive that appear in the same document as a core feature. is will result in a huge number of potential core features and the number will need to be trimmed down by designing some heuristic such as considering only word pairs which appear in a specified number of documents. nonlinear classifiers alleviate this need. when using a bag-of-words it is sometimes useful to weight each word with proportion to its informativeness for example using tf-idf weighting however the learning algorithm is often capable of coming up with the weighting on its own. another option is to use word indicators rather than word counts each word in the document each word above a given count will be represented once regardless of its number of occurrences in the document. document classification authorship attribution in the authorship attribution task et al. we are given a text and need to infer the identify of its author a fixed set of possible authors or other characteristics of the author of the text such as their gender their age or their native language. e kind of information used to solve this task is very different than that of topic classification the clues are subtle and involve stylistic properties of the text rather than content words. us our choice of features should shy away from content words and focus on more stylistic properties. a good set for such tasks focus on parts of speech tags and function words. ese are words like on of the and before and so on that do not carry much content on their own but rather serve to connect to content-bearing words and assign meanings to their compositions as well as pronouns she i they etc. a good approximation of function words is the list of or so most frequent words in a large corpus. by focusing on such features we can learn to capture subtle stylistic variations in writing that are unique to an author and very hard to fake. a good feature set for authorship attribution task include a bag-of-function-words-andpronouns bag-of-pos-tags and bags of pos bigrams trigrams and additionally we may want to consider the density of function words the ratio between the number of function words and content words in a window of text a bag of bigrams of function words after removing the content words and the distributions of the distances between consecutive function words. one could argue that for age or gender identification we may as well observe also the content-words as there are strong correlation between age and gender of a person and the topics they write about and the language register they use. is is generally true but if we are interested in a forensic or adversarial setting in which the author has an incentive to hide their age or gender we better not rely on content-based features as these are rather easy to fake compared to the more subtle stylistic cues. word-in-context part of speech tagging word-in-context part of speech tagging in the parts-of-speech tagging task we are given a sentence and need to assign the correct partof-speech to each word in the sentence. e parts-of-speech come from a pre-defined set for this example assume we will be using the tagset of the universal treebank project et al. nivre et al. containing tags. part-of-speech tagging is usually modeled as a structured task the tag of the first word may depend on the tag of the third one but it can be approximated quite well by classifying each word in isolation into a pos-tag based on a window of two words to each side of the word. if we tag the words in a fixed order for example from left to right we can also condition each tagging prediction on tag predictions made on previous tags. our feature function when classifying a word wi has access to all the words in the sentence their letters as well as all the previous tagging decisions the assigned tags for words here we discuss features as if they are used in an isolated classification task. in chapter we discuss the structured learning case using the same set of features. e sources of information for the pos-tagging task can be divided into intrinsic cues on the word itself and extrinsic cues on its context. intrinsic cues include the identify of the word words are more likely than others to be nouns for example prefixes suffixes and orthographic shape of the word english words ending in are likely past-tense verbs words starting with un- are likely to be adjectives and words starting with a capital letter are likely to be proper names and the frequency of the word in a large corpus example rare words are more likely to be nouns. extrinsic cues include the word identities prefixes and suffixes of words surrounding the current word as well as the part-of-speech prediction for the previous words. overlapping features if we have the word form as a feature why do we need the prefixes and suffixes? after all they are deterministic functions of the word. e reason is that if we encounter a word that we have not seen in training of vocabulary or oov word or a word we ve seen only a handful of times in training rare word we may not have robust enough information to base a decision on. in such cases it is good to back-off to the prefixes and suffixes which can provide useful hints. by including the prefix and suffix features also for words that are observed many times in training we allow the learning algorithms to better adjust their weights and hopefully use them properly when encountering oov words. adjective adposition adverb auxiliary verb coordinating conjunction determiner interjection noun numeral particle pronoun proper noun punctuation subordinating conjunction symbol verb other. case studies of nlp features an example of a good set of core features for pos tagging is wordx word-is-capitalized word-contains-hyphen word-contains-digit for p in word at position px of word at position px of word at position px of word at position px of word at position px word at position px is capitalized word at position px contains hyphen word at position px contains digit predicted pos of word at position predicted pos of word at position in addition to these distributional information such as word clusters or word-embedding vectors of the word and of surrounding words can also be useful especially for words not seen in the training corpus as words with similar pos-tags tend to occur in more similar contexts to each other than words of different pos-tags. word-in-context named entity recognition word-in-context named entity recognition in the named-entity recognition task we are given a document and need to find named entities such as milan john smith mccormik industries and paris as well as to categorize them into a pre-defined set of categories such as l o p or o note that this task is context dependent as milan can be a location city or an organization sports team milan played against barsa wednesday evening and paris can be the name of a city or a person. a typical input to the problem would be a sentence such as john smith president of mccormik industries visited his niece paris in milan reporters say and the expected output would be john smith president of mccormik industries visited his niece paris in milan reporters say while ner is a sequence segmentation task it assigns labeled brackets over nonoverlapping sentence spans it is often modeled as a sequence tagging task like pos-tagging. e use of tagging to solve segmentation tasks is performed using bio encoded tags. each word is assigned one of the following tags as seen in table table bio tags for named entity recognition variants on the bio tagging scheme are explored in the literature and some perform somewhat better than it. see lample et al. ratinov and roth tagmeaningonot part of a named entityb-peri-perfirst word of a person namecontinuation of a person nameb-loci-locfirst word of a location namecontinuation of a location nameb-orgi-orgfirst word of an organization namecontinuation of an organization nameb-misci-miscfirst word of another kind of named entitycontinuation of another kind of named entity case studies of nlp features e sentence above would be tagged as johnb-per smithi-per presidento ofo mccormikb-org industriesi-org visitedo hiso nieceo parisb-per ino milanb-loc reporterso sayo e translation from non-overlapping segments to bio tags and back is straightforward. like pos-tagging the ner task is a structured one as tagging decisions for different words interact with each other is more likely to remain within the same entity type than to switch it is more likely to tag john smith inc. as b-org i-org i-org than as b-per i-per b-org. however we again assume it can be approximated reasonably well using independent classification decisions. e core feature set for the ner task is similar to that of the pos-tagging task and relies on words within a window to each side of the focus word. in addition to the features of the pos-tagging task which are useful for ner as well is a suffix indicating a location mc- is a prefix indicating a person we may want to consider also the identities of the words that surround other occurrences of the same word in the text as well as indicator functions that check if the word occurs in pre-compiled lists of persons locations and organizations. distributional features such word clusters or word vectors are also extremely useful for the ner task. for a comprehensive discussion on features for ner see ratinov and roth word in context linguistic features preposition sense disambiguation prepositions words like on in with and for serve for connecting predicates with their arguments and nouns with their prepositional modifiers. preposions are very common and also very ambiguous. consider for example the word for in the following sentences. a. we went there for lunch. b. he paid for me. c. we ate for two hours. d. he would have left for home but it started raining. e word for plays a different role in each of them in it indicates a p in a b in a d and in a l in order to fully understand the meaning of a sentence one should arguablly know the correct senses of the prepositions within it. e preposition-sense disambiguation task deals with assigning the correct sense to a preposition in context from a finite inventory of senses. schneider et al. discuss the task present a unified sense inventory that covers many preposi wordincontextlinguisticfeaturesprepositionsensedisambiguation tions and provide a small annotated corpus of sentences from online reviews covering preposition mentions each annotated with its sense. which are a good set of features for the preposition sense disambiguation task? we follow here the feature set inspired by the work of hovy et al. obviously the preposition itself is a useful feature distribution of possible senses for in is very different from the distribution of senses for with or about for example. besides that we will look in the context in which the word occurs. a fixed window around the preposition may not be ideal in terms of information content thought. consider for example the following sentences. a. he liked the round object from the very first time he saw it. b. he saved the round object from him the very first time they saw it. e two instances of from have different senses but most of the words in a window around the word are either not informative or even misleading. we need a better mechanism for selecting informative contexts. one option would be to use a heuristic such as the first verb on the left and the first noun on the right. ese will capture the triplets hlikedfromtimei and hsavedfromhimi which indeed contain the essence of the preposition sense. in linguistic terms we say that this heuristic helps us capture the governor and the and object of the preposition. by knowing the identify of the preposition as well as its governor and objects humans can in many cases infer the sense of the preposition using reasoning processes about the fine-grained semantics of the words. e heuristic for extracting the object and governor requires the use of a pos-tagger in order to identify the nouns and verbs. it is also somewhat brittle it is not hard to imagine cases in which it fails. we could refine the heuristic with more rules but a more robust approach would be to use a dependency parser the governor and object information is easily readable from the syntactic tree reducing the need for complex heuristics of course the parser used for producing the tree may be wrong too. for robustness we may look at both the governor and object extracted from the parser and the governor and object extracted using the heuristic and use all four as sources for features parse_govx parse_objy earlier sense inventories and annotated corpora for the task are also available. see for example litkowski and hargraves srikumar and roth he liked the round object from the very time he saw itnsubjnsubjdobjdetamodamodamoddetrcmoddobjpreppobjroot case studies of nlp features heur_govz heur_objw letting the learning process decide which of the sources is more reliable and how to balance between them. after extracting the governor and the object perhaps also words adjacent to the governor and the object we can use them as the basis for further feature extraction. for each of the items we could extract the following pieces of information the actual surface form of the word the lemma of the word the part-of-speech of the word prefixes and suffixes of the word adjectives of degree number order etc such as ultra- poly- post- as well as some distinctions between agentive and non-agentive verbs and word cluster or distributional vector of the word. if we allow the use of external lexical resources and don t mind greatly enlarging the feature space hovy et al. found the use of wordnet-based features to be helpful as well. for each of the governor and the object we could extract many wordnet indicators such as does the word have a wordnet entry? hypernyms of the first synset of the word hypernyms of all synsets of the word synonyms for first synset of the word synonyms for all synsets of the word all terms in the definition of the word the super-sense of the word also called lexicographer-files in the wordnet jargon are relatively high levels in the wordnet hierarchy indicating concepts such as being an animal being a body part being an emotion being food etc. and various other indicators. is process may result in tens or over a hundred of core features for each preposition instance i.e. hyper_all_syn_gova hyper_all_syn_govb hyper_all_syn_govc term_in_def_govq term_in_def_govw etc. see the work of hovy et al. for the finer details. hyper_all_syn_objy relation between words in context arc-factored parsing e preposition-sense disambiguation task is an example of a high-level semantic classification problem for which we need a set of features that cannot be readily inferred from the surface forms and can benefit from linguistic pre-processing pos-tagging and syntactic parsing as well as from selected pieces of information from manually curated semantic lexicons. relation between words in context arc-factored parsing in the dependency parsing task we are given a sentence and need to return a syntactic dependency tree over it such as the tree in figure each word is assigned a parent word except for the main word of the sentence whose parent is a special symbol. figure dependency tree. for more information on the dependency parsing task its linguistic foundations and approaches to its solution see the book by k bler et al. one approach to modeling the task is the arc-factored approach et al. where each of the possible word-word relations is assigned a score independent of the others and then we search for the valid tree with the maximal overall score. e score assignment is made by a trained scoring function a s m se nt receiving a sentence as well as the indices h and m of two words within it that are considered as candidates for attachment is the index of the candidate head-word and m is the index of the candidate modifier. training the scoring function such that it works well with the search procedure will be discussed in chapter here we focus on the features used in the scoring function. assume a sentence of n words and their corresponding parts-of-speech se nt d wn pn when looking at an arc between words wh and wm we can make use of the following pieces of information. we begin with the usual suspects the boy with the black shirt opened the door with a keydetprepprepamoddetpobjnsubjrootdobjpobjdetdet case studies of nlp features e word form pos-tag of the head word. e word form pos-tag of the modifier word. words are less likely to be heads or modifiers regardless to who they are connected to. for example determiners the a are often modifiers and are never heads. words in a window of two words to each side of the head word including the relative positions. words in a window of two words to each side of the modifier word including the relative positions. e window information is needed to give some context to the word. words behave differently in different contexts. we use the parts-of-speech as well as the word forms themselves. word-forms give us very specific information example that cake is a good candidate object for ate while the parts-of-speech provide lower level syntactic information that is more generalizable example that determiners and adjectives are good modifiers for nouns and that nouns are good modifiers for verbs. as the training corpora for dependency-trees are usually rather limited in size it could be a good idea to supplement or replace the words using distributional information in the form of word clusters or pre-trained word embeddings that will capture generalizations across similar words also for words that may not have a good coverage in the training data. we do not look at prefixes and suffixes of words because these are not directly relevant to the parsing task. while the affixes of words indeed carry important syntactic information the word likely to be a noun? a past verb? this information is already available to us in through the pos-tags. if we were parsing without access to pos-tag features example if the parser was in charge for both parsing and pos-tag assignments it would be wise to include the suffix information as well. of course if we use a linear classifier we need to take care also of feature combinations with features such as head candidate word is x and modifier word candidate is y and head part-of-speech is z and the word before the modifier word is w. indeed it is common for dependency parsers based on linear models to have hundreds of such feature combinations. in addition to these usual suspects it is also informative to consider the following. e distance between words wh and wm in the sentence dist d jh mj. some distances are more likely to stand in a dependency relation than others. e direction between the words. in english assume wm is a determiner the and wh is a noun boy it is quite likely that there will be an arc between them if m h and very unlikely if m h. all the words of words that appear between the head and the modifier words in the sentence. is information is useful as it hints at possible competing attachments. relation between words in context arc-factored parsing for example a determiner at wm is likely to modify a noun at whm but not if a word wk k h between them is also a determiner. note that the number of words between the head and the modifier is potentially unbounded also changes from instance to instance and so we need a way to encode a variable number of features hinting at a bag-of-words approach. c h a p t e r from textual features to inputs in chapters and we discussed classifiers that accept feature vectors as input without getting into much details about the contents of these vectors. in chapters and we discussed the sources of information which can serve as the core features for various natural language tasks. in this chapter we discuss the details of going from a list of core-features to a feature-vector that can serve as an input to a classifier. to recall in chapters and we presented machine-trainable models linear loglinear or multi-layer perceptron. e models are parameterized functions f that take as input a din dimensional vector x and produce a dout dimensional output vector. e function is often used as a classifier assigning the input x a degree of membership in one or more of dout classes. e function can be either simple a linear model or more complex arbitrary neural networks. in this chapter we focus on the input x. encoding categorical features when dealing with natural language most of the features represent discrete categorical features such as words letters and part-of-speech tags. how do we encode such categorical data in a way which is amenable for use by a statistical classifier? we discuss two options one-hot encodings and dense embedding vectors as well as the trade-offs and the relations between them. one-hot encodings in linear and log-linear models of the form f d xw c b it is common to think in term of indicator functions and assign a unique dimension for each possible feature. for example when considering a bag-of-words representation over a vocabulary of items x will be a vector where dimension number corresponds to the word dog and dimension number corresponds to the word cat. a document of words will be represented by a very sparse vector in which at most dimensions have non-zero values. correspondingly the matrix w will have rows each corresponding to a particular vocabulary word. when the core features are the words in a words window surrounding and including a target word words to each side with positional information and a vocabulary of words is features of the form or x will be a vector with non-zero entries with dimension number corresponding to and dimension number corresponding to is is called a one-hot encoding as each dimension corresponds to a unique feature and the resulting feature from textual features to inputs vector can be thought of as a combination of high-dimensional indicator vectors in which a single dimension has a value of and all others have a value of dense encodings embeddings perhaps the biggest conceptual jump when moving from sparse-input linear models to deeper nonlinear models is to stop representing each feature as a unique dimension in a one-hot representation and representing them instead as dense vectors. at is each core feature is embedded into a d dimensional space and represented as a vector in that space. e dimension d is usually much smaller than the number of features i.e. each item in a vocabulary of items as one-hot vectors can be represented as or dimensional vector. e embeddings vector representation of each core feature are treated as parameters of the network and are trained like the other parameters of the function f figure shows the two approaches to feature representation. e general structure for an nlp classification system based on a feed-forward neural network is thus. extract a set of core linguistic features fk that are relevant for predicting the output class. for each feature fi of interest retrieve the corresponding vector v.fi combine the vectors by concatenation summation or a combination of both into an input vector x. feed x into a nonlinear classifier neural network. e biggest change in the input when moving from linear to deeper classifier is then the move from sparse representations in which each feature is its own dimension to a dense representation in which each feature is mapped to a vector. another difference is that we mostly need to extract only core features and not feature combinations. we will elaborate on both these changes briefly. dense vectors vs. one-hot representations what are the benefits of representing our features as vectors instead of as unique ids? should we always represent features as dense vectors? let s consider the two kinds of representations. different feature types may be embedded into different spaces. for example one may represent word features using dimensions and part-of-speech features using dimensions. encoding categorical features sparsevs.densefeaturerepresentations. two encodings of the information currentword is dog previous word is the previous pos-tag is det. sparse feature vector. each dimension represents a feature. feature combinations receive their own dimensions. feature values are binary. dimensionality is very high. dense embeddings-based feature vector. each core feature is represented as a vector. each feature corresponds to several input vector entries. no explicit encoding of feature combinations. dimensionality is low. e feature-to-vector mappings come from an embedding table. one hot each feature is its own dimension. dimensionality of one-hot vector is same as number of distinct features. features are completely independent from one another. e feature word is dog is as dissimilar to word is thinking than it is to word is cat x word embeddingspos embeddingsab from textual features to inputs dense each feature is a d-dimensional vector. dimensionality of vector is d. model training will cause similar features to have similar vectors information is shared between similar features. one benefit of using dense and low-dimensional vectors is computational the majority of neural network toolkits do not play well with very high-dimensional sparse vectors. however this is just a technical obstacle which can be resolved with some engineering effort. e main benefit of the dense representations is in generalization power if we believe some features may provide similar clues it is worthwhile to provide a representation that is able to capture these similarities. for example assume we have observed the word dog many times during training but only observed the word cat a handful of times or not at all. if each of the words is associated with its own dimension occurrences of dog will not tell us anything about the occurrences of cat. however in the dense vectors representation the learned vector for dog may be similar to the learned vector for cat allowing the model to share statistical strength between the two events. is argument assumes that we saw enough occurrences of the word cat such that its vector will be similar to that of dog or otherwise that good vectors are somehow given to us. such good word-vectors called pre-trained embeddings can be obtained from a large corpus of text through algorithms that make use of the distributional hypothesis. such algorithms are discussed in more depth in chapter in cases where we have relatively few distinct features in the category and we believe there are no correlations between the different features we may use the one-hot representation. however if we believe there are going to be correlations between the different features in the group example for part-of-speech tags we may believe that the different verb inflections vb and vbz may behave similarly as far as our task is concerned it may be worthwhile to let the network figure out the correlations and gain some statistical strength by sharing the parameters. it may be the case that under some circumstances when the feature space is relatively small and the training data is plentiful or when we do not wish to share statistical information between distinct words there are gains to be made from using the one-hot representations. however this is still an open research question and there is no strong evidence to either side. e majority of work by chen and manning collobert and weston collobert et al. advocate the use of dense trainable embedding vectors for all features. for work using neural network architecture with sparse vector encodings see johnson and zhang combining dense vectors each feature corresponds to a dense vector and the different vectors need to be combined somehow. e prominent options are concatenation summation averaging and combinations of the two. combining dense vectors d. window-based features consider the case of encoding a window of size k words to each side of a focus word at position i. assume k d we need to encode the words at positions i i i c and i c assume the window items are the words a b c and d and let abc and d be the corresponding word vectors. if we do not care about the relative positions of the words within the window we will encode the window as a sum a c b c c c d. if we do care about the relative positions we rather use concatenation bi ci d here even though a word will have the same vector regardless of its position within the window the word s position is reflected by its position within the concatenation. we may not care much about the order but would want to consider words further away from the context word less than words that are closer to it. is can be encoded as a weighted sum i.e. ese encodings can be mixed and matched. assume we care if the feature occurs before or after the focus word but do not care about the distance as long as it is within the window. is can be encoded using a combination of summation and concatenation c bi c d a note on notation when describing network layers that get concatenated vectors x y and z as input some authors use explicit concatenation yi c b while others use an affine transformation c yv c zw c b. if the weight matrices u v w in the affine transformation are different than one another the two notations are equivalent. a c b c c c variable number of features continuous bag of words feed-forward networks assume a fixed dimensional input. is can easily accommodate the case of a feature-extraction function that extracts a fixed number of features each feature is represented as a vector and the vectors are concatenated. is way each region of the resulting input vector corresponds to a different feature. however in some cases the number of features is not known in advance example in document classification it is common that each word in the sentence is a feature. we thus need to represent an unbounded number of features using a fixed size vector. one way of achieving this is through a so-called continuous bag of words representation et al. e cbow is very similar to the traditional bag-of-words representation in which we discard order information and works by either summing or averaging the embedding alternatively we could have a separate embedding for each wordposition pair i.e. and will represent the word a when it appears in relative positions and respectively. following this approach we could then use a sum and still be sensitive to position information c c c is approach will not share information between instances of words when they appear in different positions and may be harder to use with externally trained word vectors. e matrices should be different in the sense that a change to one will not be reflected in the others. it is ok for the matrices to happen to share the same values of course. from textual features to inputs vectors of the corresponding features fk d k kx v.fi ai kx a simple variation on the cbow representation is weighted cbow in which different vectors receive different weights fk d ai v.fi here each feature fi has an associated weight ai indicating the relative importance of the feature. for example in a document classification task a feature fi may correspond to a word in the document and the associated weight ai could be the word s tf-idf score. relation between one-hot and dense vectors representing features as dense vectors is an integral part of the neural network framework and consequentially the differences between using sparse and dense feature representations are subtler than they may appear at first. in fact using sparse one-hot vectors as input when training a neural network amounts to dedicating the first layer of the network to learning a dense embedding vector for each feature based on the training data. when using dense vectors each categorical feature value fi is mapped to a dense ddimensional vector v.fi is mapping is performed through the use of an embedding layer or a lookup layer. consider a vocabulary of jv j words each embedded as a d dimensional vector. e collection of vectors can then be thought of as a jv j d embedding matrix e in which each row corresponds to an embedded feature. let fi be the one-hot representation of feature fi that is a jv j-dimensional vector which is all zeros except for one index corresponding to the value of the ith feature in which the value is e multiplication fi e will then select the corresponding row of e. us v.fi can be defined in terms of e and fi and similarly v.fi d fi e kx fk d e d kx fi e note that if the v.fi were one-hot vectors rather than dense feature representations the cbow and wcbow would reduce to the traditional bag-of-words representations which is in turn equivalent to a sparse feature-vector representation in which each binary indicator feature corresponds to a unique word. odds and ends e input to the network is then considered to be a collection of one-hot vectors. while this is elegant and well-defined mathematically an efficient implementation typically involves a hashbased data structure mapping features to their corresponding embedding vectors without going through the one-hot representation. consider a network which uses a traditional sparse representation for its input vectors and no embedding layer. assuming the set of all available features is v and we have k on features fk fi v the network s input is kx fi and so the first layer the nonlinear activation is x d x njv j c kx xw c b d fi w c b w rjv b rd is layer selects rows of w corresponding to the input features in x and sums them then adding a bias term. is is very similar to an embedding layer that produces a cbow representation over the features where the matrix w acts as the embedding matrix. e main difference is the introduction of the bias vector b and the fact that the embedding layer typically does not undergo a nonlinear activation but rather is passed on directly to the first layer. another difference is that this scenario forces each feature to receive a separate vector in w while the embedding layer provides more flexibility allowing for example for the features next word is dog and previous word is dog to share the same vector. however these differences are small and subtle. when it comes to multi-layer feed-forward networks the difference between dense and sparse inputs is smaller than it may seem at first sight. odds and ends distance and position features e linear distance in between two words in a sentence may serve as an informative feature. for example in an event extraction task we may be given a trigger word and a candidate argument word and asked to predict if the argument word is indeed an argument of the trigger. similarly in a coreference-resolution task if which of the previously mentioned entities if at all a pronoun such as he or she refers to we may be given a pair of candidate word and asked to predict if they co-refer or not. e distance relative position between the trigger e event extraction task involves identification of events from a predefined set of event types. for example identification of purchase events or terror-attack events. each event type can be triggered by various triggering words verbs and has several slots that needs to be filled who purchased? what was purchased? at what amount?. from textual features to inputs and the argument is a strong signal for these prediction tasks. in the traditional nlp setup distances are usually encoded by binning the distances into several groups and associating each bin with a one-hot vector. in a neural architecture where the input vector is not composed of binary indicator features it may seem natural to allocate a single input entry to the distance feature where the numeric value of that entry is the distance. however this approach is not taken in practice. instead distance features are encoded similarly to the other feature types each bin is associated with a d-dimensional vector and these distance-embedding vectors are then trained as regular parameters in the network santos et al. nguyen and grishman zeng et al. zhu et al. padding unknown words and word dropout padding in some cases your feature extractor will look for things that do not exist. for example when working with parse trees you may have a feature looking for the left-most dependant of a given word but the word may not have any dependents to its left. perhaps you are looking at the word to positions to the right of the current one but you are at the end of the sequence and two positions to the right is past the end. what should be done in such situations? when using a bag-of-features approach summing you could just leave the feature out of the sum. when using a concatenation you may provide a zero-vector in the place. ese two approaches work fine technically but could be sub-optimal for your problem domain. maybe knowing that there is no left-modifier is informative? e suggested solution would be to add a special symbol symbol to your embedding vocabulary and use the associated padding vector in these cases. depending on the problem at hand you may want to use different padding vectors for different situations no-left-modifier may be a different vector than no-right-modifier. such paddings are important for good prediction performance and are commonly used. unfortunately their use is not often reported or quickly glossed over in many research papers. unknown words another case where a requested feature vector will not be available is for outof-vocabulary items. you are looking for the word on the left observe the value variational but this word was not a part of your training vocabulary so you don t have an embedding vector for it. is case is different from the padding case because the item is there but you just don t know it. e solution is similar however reserve a special symbol u representing an unknown token for use in such cases. again you may or may not want to use different unknown symbols for different vocabularies. in any case it is advised to not share the padding and the unknown vectors as they reflect two very different conditions. word signatures another technique for dealing with unknown words is backing-off from the word forms to word signatures. using the u symbol for unknown words is essentially backingoff from all unknown words to the same signature. but depending on the task one is trying to solve one may come up with more fine-grained strategies. for example we may replace unknown words that end with ing with an symbol words that end with ed with an sym odds and ends bol words that start with un with an symbol numbers with a symbol and so on. e list of mappings is hand-crafted to reflect informative backing-off patterns. is approach is often used in practice but rarely reported in deep-learning papers. while there are approaches that allow to automatically learn such backing-off behavior as part of the model training without needing to manually define the backing-off patterns discussion on sub-word units in section they are in many cases an overkill and hard-coding the patterns is as effective and more computationally efficient. word dropout reserving a special embedding vector for unknown words is not enough if all the features in the training set have their own embedding vectors the unknown-word condition will not be observed in training the associated vector will not receive any updates and the model will not be tuned to handle the unknown condition. is is equivalent to just using a random vector when an unknown word is encountered at test time. e model needs to be exposed to the unknown-word condition during training. a possible solution would be to replace all some of the features with a low frequency in the training with the unknown symbol pre-process the data replacing words with a frequency below a threshold with is solution works but has the disadvantage of losing some training data these rare words will not receive any signal. a better solution is the use of word-dropout when extracting featuresin training randomly replace words with the unknown symbol. e replacement should be based on the word s frequency less frequent words will be more likely to be replaced by the unknown symbol than frequent ones. e random replacement should be decided on runtime a word that was dropped once may or may not be dropped when it is encountered again in different iterations over the training data. ere is no established formula for deciding on the word dropout rate. works in my group where is a parameter for controlling the aggressiveness of the dropout use and goldberg word dropout as regularization besides better adaptation to unknown words word dropout may also be beneficial for preventing overfitting and improving robustness by not letting the model rely too much on any single word being present et al. when used this way word dropout should be applied frequently also to frequent words. indeed the suggestion of iyyer et al. is to drop word instances according to a bernoulli trial with probability p regardless of their frequency. when word dropout is a applied as a regularizer you may not want to replace the dropped words with the unknown symbol in some circumstances. for example when the feature representation is a bag-of-words over the document and more than a quarter of the words are dropped replacing the dropped words with the unknown word symbol will create a feature representation that is not likely to occur at test time where such a large concentration of unknown words is unlikely. from textual features to inputs feature combinations note that the feature extraction stage in the neural network settings deals only with extraction of core features. is is in contrast to the traditional linear-model-based nlp systems in which the feature designer had to manually specify not only the core features of interest but also interactions between them introducing not only a feature stating word is x and a feature stating tag is y but also combined feature stating word is x and tag is y or sometimes even word is x tag is y and previous word is z e combination features are crucial in linear models because they introduce more dimensions to the input transforming it into a space where the data-points are closer to being linearly separable. on the other hand the space of possible combinations is very large and the feature designer has to spend a lot of time coming up with an effective set of feature combinations. one of the promises of the nonlinear neural network models is that one needs to define only the core features. e nonlinearity of the classifier as defined by the network structure is expected to take care of finding the indicative feature combinations alleviating the need for feature combination engineering. as discussed in section kernel methods and cristianini and in particular polynomial kernels and matsumoto also allow the feature designer to specify only core features leaving the feature combination aspect to the learning algorithm. in contrast to neural network models kernels methods are convex admitting exact solutions to the optimization problem. however the computational complexity of classification in kernel methods scales linearly with the size of the training data making them too slow for most practical purposes and not suitable for training with large datasets. on the other hand the computational complexity of classification using neural networks scales linearly with the size of the network regardless of the training data size. vector sharing consider a case where you have a few features that share the same vocabulary. for example when assigning a part-of-speech to a given word we may have a set of features considering the previous word and a set of features considering the next word. when building the input to the classifier we will concatenate the vector representation of the previous word to the vector representation of the next word. e classifier will then be able to distinguish the two different indicators and treat them differently. but should the two features share the same vectors? should the vector for dogprevious-word be the same as the vector of dognext-word or should we assign them two distinct vectors? is again is mostly an empirical question. if you believe words behave differently when they appear in different positions word x behaves like word y when in the previous position but x behaves like z when in the next position then it may be a good idea to use two different vocabularies and assign a different set of vectors for each feature type. however of course one still needs to go over the entire dataset when training and sometimes go over the dataset several times. is makes training time scale linearly with the dataset size. however each example in either training or test time is processed in a constant time a given network. is is in contrast to a kernel classifier in which each example is processed in a time that scales linearly with the dataset size. odds and ends if you believe the words behave similarly in both locations then something may be gained by using a shared vocabulary for both feature types. dimensionality how many dimensions should we allocate for each feature? unfortunately there are no theoretical bounds or even established best-practices in this space. clearly the dimensionality should grow with the number of the members in the class probably want to assign more dimensions to word embeddings than to part-of-speech embeddings but how much is enough? in current research the dimensionality of word-embedding vectors range between about to a few hundreds and in some extreme cases thousands. since the dimensionality of the vectors has a direct effect on memory requirements and processing time a good rule of thumb would be to experiment with a few different sizes and choose a good trade-off between speed and task accuracy. embeddings vocabulary what does it mean to associate an embedding vector for every word? clearly we cannot associate one with all possible values and need to restrict ourselves to every value from a finite vocabulary. is vocabulary is usually based on the training set or if we use pre-trained embeddings on the training on which the pre-trained embeddings were trained. it is recommended that the vocabulary will also include a designated u symbol associating a special vector to all words that are not in the vocabulary. network s output for multi-class classification problems with k classes the network s output is a k-dimensional vector in which every dimension represents the strength of a particular output class. at is the output remains as in the traditional linear models scalar scores to items in a discrete set. however as we saw in chapter there is a d k matrix associated with the output layer. e columns of this matrix can be thought of as d dimensional embeddings of the output classes. e vector similarities between the vector representations of the k classes indicate the model s learned similarities between the output classes. historical note representing words as dense vectors for input to a neural network was popularized by bengio et al. in the context of neural language modeling. it was introduced to nlp tasks in the pioneering work of collobert weston and colleagues and weston collobert et al. using embeddings for representing not only words but arbitrary features was popularized following chen and manning while the work by bengio collobert weston and colleagues popularized the approaches they were not the first to use them. earlier authors that use dense continuous-space vectors for representing word inputs to neural networks include lee et al. and forcada and eco similarly continuous-space language models were used for machine-translation already by schwenk et al. from textual features to inputs example part-of-speech tagging e pos-tagging task we are given a sentence of n words wn and a word position i and need to predict the tag of wi. assuming we tag the words from left to right we can also look at the previous tag predictions a list of concrete core features is given in section here we discuss encoding them as an input vector. we need a feature function x d i getting a sentence s comprised of words and previous tagging decisions and an input position i and returning a feature vector x. we assume a function suf k that returns the k-letter suffix of word w and similarly pref k that returns the prefix. we begin with the three boolean questions word-is-capitalized word-contains-hyphen and word-contains-digit. e most natural way to encode these is to associate each of them with its own dimension with a value of if the condition holds for word wi and otherwise. we will put these in a vector associated with word i ci. next we need to encode words prefixes suffixes and part-of-speech tags in various positions in the window. we associate each word wi with an embedding vector vw rdw. similarly we associate each two-letter suffix suf with an embedding vector vs.suf and similarly for three-letter suffixes vs.suf rds. prefixes get the same treatment with embeddings rdp. finally each pos-tag receives an embedding vt rdt. each position i can be associated with a vector vi of the relevant word information form prefixes suffixes boolean features vi d vw vs.suf vs.suf vp.pref vp.pref our input vector x is then a concatenation of the following vectors vi x d i d vii vt vt x discussion note that the words in each position share the same embedding vectors when creating vi and we read from the same embedding tables and that a vector vi does not know its relative position. however because of the vector concatenation the vector x knows that which relative position each v is associated with because of its relative position within x. is allows us to share some information between the words in the different positions vector of the word dog will receive updates when the word is at relative position as well as when it is in relative position but will also be treated differently by the model when it appears in different relative positions because it will be multiplied by a different part of the matrix in the first layer of the network. a value of for the negative condition is also a possible choice. i.e. example arc-factored parsing an alternative approach would be to associate each word-and-position pair with its own embedding instead of a single table vw we will have five embedding tables and use appropriate one for each relative word position. is approach will substantially increase the number of parameters in the model will need to learn five-times as many embedding vectors and will not allow sharing between the different words. it will also be somewhat more wasteful in terms of computation as in the previous approach we could compute the vector vi for each word in the sentence once and then re-use them when looking at different positions i while in the alternative approach the vectors vi will need to be re-computed for each position i we are looking at. finally it will be harder to use pre-trained word vectors because the pre-trained vectors do not have location information attached to them. however this alternative approach would allow us to treat each word position completely independently from the others if we wanted to. another point to consider is capitalization. should the words dog and dog receive different embeddings? while capitalization is an important clue for tagging in our case the capitalization status of word wi is already encoded in the boolean features ci. it is thus advisable to lower-case all words in the vocabulary before creating or querying the embedding table. finally the and features are redundant with each other contains the other and similarly for the suffixes. do we really need both? can we make them share information? indeed we could use letter embeddings instead of suffix embeddings and replace the two suffix embeddings with a vector composed of the concatenation of the three last letters in the word. in section we will see an alternative approach that uses character-level recurrent neural networks to capture prefix suffix and various other properties of the word form. example arc-factored parsing in the arc-factored parsing task we are given a sentence of n words and their parts-of-speech and need to predict a parse tree. here we are concerned with the features for scoring a single attachment decision between words wh and wm where wh is the candidate head-word and wm is the candidate modifier word. a list of concrete core features was given in section and here we discuss encoding them as an input vector. we define a feature function x d m se nt receiving a sentence comprised of word and pos-tags and the positions of a head-word and a modifier-word first we need to consider the head word its pos-tag and the words and pos-tags in a five-word window around the head word words to each side. we associate each word w in our vocabulary with an embedding vector vw rdw and similarly each part-of-speech tag p with an embedding vector vt rdt. we then define the vector representation of a word at position i to be vi d vt rdwcdt the concatenation of the word and pos vector. note that mathematically even that last benefit is not a real benefit when used as an input to a neural network the first layer could potentially learn to look only at certain dimensions of the embeddings when they are used at position and at different dimensions when they are used at position achieving the same separation as in the alternative approach. us the original approach is just as expressive as the alternative one at least in theory. from textual features to inputs en we associate the head word with a vector h representing the word within its context and associate the modifier word with a similar vector m h d vhi m d vmi is takes care of the elements in the first block of features. note that like with the partof-speech tagging features this encoding cares about the relative position of each of the context words. if we didn t care about the positions we could have instead represented the head word as d c c c is sums the context words into a bag-of-words losing their positional information yet concatenates the context and the focus words retaining the distinction between them. we now turn to the distance and direction features. while we could assign the distance a single dimension with the numeric distance value it is common to bin the distance into k discrete bins and associate each bin with a dd-dimensional embedding. e direction is a boolean feature and we represent it as its own dimension. we denote the vector containing the binned distance embedding concatenated with the boolean direction feature as d. finally we need to represent the words and pos-tags between the head and the modifier. eir number is unbounded and varies between different instances so we cannot use concatenation. fortunately we do not care about the relative positions of the intervening items so we can use a bag-of-words encoding. concretely we represent the between-context words as a vector c defined as the average of the words and pos between vectors mx idh c d vi note that this sum potentially captures also the number of elements between the head and our final representation of an attachment decision to be scored x is then encoded as the modifier words making the distance feature potentially redundant. concatenation of the various elements while this encoding of the dimension is very natural pei et al. follow a different approach in their parser. perhaps motivated by the importance of distance information they chose to not mark it as an input feature but instead to use two different scoring functions one for scoring left-to-right arcs and another for scoring right-to-left arcs. is gives a lot of power to the direction information while substantially increasing the number of parameters in the model. example arc-factored parsing where x d m se nt d mi ci d vi d vt h d vhi m d vmi c d d d binned distance embeddings direction indicator. idh mx vi note how we combine positional window-based features with bag-of-word features by simple concatenation. e neural network layers on top of x can then infer transformation and feature combinations between the elements in the different windows as well as between the different elements in the bag-of-words representation. e process of creating the representation x the embedding tables for the words pos-tags and binned distances as well as the different concatenations and summations is also part of the neural network. it is reflected in the computationgraph construction and its parameters are trained jointly with the network. e features creation part of the network could be even more complex. for example if we had reasons to believe that the interactions between a word and its pos-tag and the interactions within a context window are more important than the interactions across elements of different entities we could have reflected that in the input encoding by creating further nonlinear transformations in the feature-encoding process i.e. replacing vi with d g.vi w v c bv and h with d h c bh and setting x d ci d c h a p t e r language modeling the language modeling task language modeling is the task of assigning a probability to sentences in a language what is the probability of seeing the sentence the lazy dog barked loudly? besides assigning a probability to each sequence of words the language models also assigns a probability for the likelihood of a given word a sequence of words to follow a sequence of words what is the probability of seeing the word barked after the seeing sequence the lazy dog? perfect performance at the language modeling task namely predicting the next word in a sequence with a number of guesses that is the same as or lower than the number of guesses required by a human participant is an indication of human level intelligence and is unlikely to be achieved in the near future. even without achieving human-level performance language modeling is a crucial components in real-world applications such as machine-translation and automatic speech recognition where the system produces several translation or transcription hypotheses which are then scored by a language model. for these reasons language modeling plays a central role in natural-language processing ai and machine-learning research. formally the task of language modeling is to assign a probability to any sequence of words i.e. to estimate p using the chain-rule of probability this can be rewritten as p d p j j j p j at is a sequence of word-prediction tasks where each word is predicted conditioned on the preceding words. while the task of modeling a single word based on its left context seem more manageable than assigning a probability score to an entire sentence the last term in the equation note that the ability to assign a probability for a word following a sequence of words and the ability to assign a probabilities to arbitrary sequences of words wk are equivalent as one can be derived from the other. assume we can model probabilities of sequences. en the conditional probability of a word can be expressed as a fraction of two sequences wi d alternatively if we could model the conditional probability of a word following a sequence of words we could use the chain rule to express the probability of sequences as a product of conditional probabilities wk d p.wkjs where is a special start-of-sequence symbol. indeed any question can be posed as a next-word-guessing task e.g. the answer to question x is even without such pathological cases predicting the next word in the text requires knowledge of syntactic and semantic rules of the language as well as vast amounts of world knowledge. language modeling still requires conditioning on n words which is as hard as modeling an entire sentence. for this reason language models make use of the markov-assumption stating that the future is independent of the past given the present. more formally a kth order markov-assumption assumes that the next word in a sequence depends only on the last k words p j p j estimating the probability of the sentence then becomes ny p j where are defined to be special padding symbols. p our task is then to accurately estimate p j given large amounts of text. while the kth order markov assumption is clearly wrong for any k can have arbitrarily long dependencies as a simple example consider the strong dependence between the first word of the sentence being what and the last one being it still produces strong language modeling results for relatively small values of k and was the dominant approach for language modeling for decades. is chapter discusses kth order language models. in chapter we discuss language modeling techniques that do not make the markov assumption. evaluating language models perplexity ere are several metrics for evaluating language modeling. e application-centric ones evaluate them in the context of performing a higher-level task for example by measuring the improvement in translation quality when switching the language-modeling component in a translation system from model a to model b. a more intrinsic evaluation of language models is using perplexity over unseen sentences. perplexity is an information theoretic measurement of how well a probability model predicts a sample. low perplexity values indicate a better fit. given a text corpus of n words wn can be in the millions and a language model function lm assigning a probability to a word based on its history the perplexity of lm with respect to the corpus is npn good language models reflective of real language usage will assign high probabilities to the events in the corpus resulting in lower perplexity values. e perplexity measure is a good indicator of the quality of a language model. perplexities are corpus specific perplexities of two language models are only comparable with respect to the same evaluation corpus. it is important to note however that in many cases improvement in perplexity scores do not transfer to improvement in extrinsic task-quality scores. in that sense the perplexity measure is good for comparing different language models in terms of their ability to pick-up regularities in sequences but is not a good measure for assessing progress in language understanding or language-processing tasks. traditional approaches to language traditional approaches to language modeling modeling e traditional approach to language models assumes a k-order markov property and model p d p d e role of the language model then is to provide good estimates d e estimates are usually derived from corpus counts. let be the count of the sequence of words wiwj in a corpus. e maximum likelihood estimate of d is then opmle d d while effective this baseline approach has a big shortcoming if the event was never observed in the corpus d the probability assigned to it will be and this in turn will result in a assignment to the entire corpus because of the multiplicative nature of the sentence probability calculation. a zero probability translates to an infinite perplexity which is very bad. zero events are quite common consider a trigram language model which only conditions on words and a vocabulary of words is rather small. ere are d possible word triplets it is clear that many of them won t be observed in training corpora of say words. while many of these events don t occur because they do not make sense many others just did not occur in the corpus. one way of avoiding zero-probability events is using smoothing techniques ensuring an allocation of a small probability mass to every possible event. e simplest example is probably additive smoothing also called smoothing and goodman goodman lidstone it assumes each event occurred at least times in addition to its observations in the corpus. e estimate then becomes d d c c j where jv j is the vocabulary size and many more elaborate smoothing techniques exist. another popular family of approaches is using back-off if the kgram was not observed compute an estimate based on a a representative sample of this family is jelinek mercer interpolated smoothing and goodman jelinek and mercer opint d d c opint d for optimal performance the values should depend on the content of the conditioning context rare contexts should receive different treatments than frequent ones. e current state-of-the-art non-neural language modeling technique uses modified kneser ney smoothing and goodman which is a variant of the technique proposed by kneser and ney for details see chen and goodman and goodman language modeling further reading language modeling is a very vast topic with decades of research. a good formal overview of the task as well as motivations behind the perplexity measure can be found in the class notes by michael collins. a good overview and empirical evaluation of smoothing techniques can be found in the works of chen and goodman and goodman another review of traditional language modeling techniques can be found in the background chapters of the ph.d. thesis of mikolov for a recent advance in non-neural language modeling see pelemans et al. limitations of traditional language models language modeling approaches based on smoothed mle estimates traditional are easy to train scale to large corpora and work well in practice. ey do however have several important shortcomings. e smoothing techniques are intricate and based on back off to lower-order events. is assumes a fixed backing-up order that needs to be designed by hand and makes it hard to add more creative conditioning contexts if one wants to condition on the k previous words and on the genre of the text should the backoff first discard of the kth previous word or of the genre variable?. e sequential nature of the backoff also makes it hard to scale toward larger ngrams in order to capture long-range dependencies in order to capture a dependency between the next word and the word positions in the past one needs to see a relevant in the text. in practice this very rarely happens and the model backs off from the long history. it could be that a better option would be to back off from the intervening words i.e. allow for ngrams with holes in them. however these are tricky to define while retaining a proper generative probabilistic framework. scaling to larger ngrams is an inherent problem for mle-based language models. e nature of natural language and the large number of words in the vocabulary means that statistics for larger ngrams will be sparse. moreover scaling to larger ngrams is very expensive in terms of memory requirements. e number of possible ngrams over a vocabulary v is jv jn increasing the order by one will result in a jv j-fold increase to that number. while not all of the theoretical ngrams are valid or will occur in the text the number of observed events does grow at least multiplicatively when increasing the ngram size by is makes it very taxing to work larger conditioning contexts. finally mle-based language models suffer from lack of generalization across contexts. having observed black car and blue car does not influence our estimates of the event red car if we haven t see it before. although see lines of work on factored language models a. bilmes and kirchhoff and on maximum-entropy language models starting with rosenfeld as well as recent work by pelemans et al. class-based language models et al. try to tackle this by clustering the words using distributional algorithms and conditioning on the induced word-classes instead of or in addition to the words. neural language models neural language models nonlinear neural network models solve some of the shortcomings of traditional language models they allow conditioning on increasingly large context sizes with only a linear increase in the number of parameters they alleviate the need for manually designing backoff orders and they support generalization across different contexts. a model of the form presented in this chapter was popularized by bengio et al. e input to the neural network is a kgram of words and the output is a probability distribution over the next word. e k context words are treated as a word window each word w is associated with an embedding vector v.w rdw and the input vector x a concatenation of the k words x d e input x is then fed to an mlp with one or more hidden layers oy d p d d softmax.hw c h d g.xw c x d v.w d e wi v e rjv w rdhid w j rjv j v is a finite vocabulary including the unique symbols u for unknown words for sentence initial padding and for end-of-sequence marking. e vocabulary size jv j ranges between words with the common sizes revolving around training e training examples are simply word kgrams from the corpus where the identities of the first k words are used as features and the last word is used as the target label for the classification. conceptually the model is trained using cross-entropy loss. working with cross entropy loss works very well but requires the use of a costly softmax operation which can be prohibitive for very large vocabularies prompting the use of alternative losses andor approximations below. memory and computational efficiency each of the k input words contributes dw dimensions to x and moving from k to k c words will increase the dimensions of the weight matrix w from k dw dhid to c dw dhid a small linear increase in the number of parameters in contrast to a polynomial increase in the case of the traditional count-based language models. is is possible because the feature combinations are computed in the hidden layer. increasing the order k will likely require enlarging the dimension of dhid as well but this is still a very a similar model was presented as early as by nakamura and shikano in their work on word class prediction with neural networks. language modeling modest increase in the number of parameters compared to the traditional modeling case. adding additional nonlinear layers to capture more complex interactions is also relatively cheap. each of the vocabulary words is associated with one dw dimensional vector row in e and one dhid dimensional vector column in w us a new vocabulary items will result in a linear increase in the number of parameters again much better than the traditional case. however while the input vocabulary matrix e requires only lookup operations and can grow without affecting the computation speed the size of the output vocabulary greatly affects the computation time the softmax at the output layer requires an expensive matrix-vector multiplication with the matrix w j followed by jv j exponentiations. is computation dominates the runtime and makes language modeling with large vocabulary sizes prohibitive. large output spaces working with neural probabilistic language models with large output spaces efficiently computing the softmax over the vocabulary can be prohibitive both at training time and at test time. dealing with large output spaces efficiently is an active research question. some of the existing solutions are as follows. hierarchical softmax and bengio allows to compute the probability of a single word in o.logjv j time rather than o.jv j. is is achieved by structuring the softmax computation as tree traversal and the probability of each word as the product of branch selection decisions. assuming one is interested in the probability of a single word than getting the distribution over all words this approach provides clear benefits in both training and testing time. self-normalizing aproaches such as noise-contrastive estimation and teh vaswani et al. or adding normalizing term to the training objective et al. e nce approach improves training time performance by replacing the cross-entropy objective with a collection of binary classification problems requiring the evaluation of the assigned scores for k random words rather than the entire vocabulary. it also improves test-time prediction by pushing the model toward producing approximately normalized exponentiated scores making the model score for a word a good substitute for its probability. e normalization term approach of devlin et al. similarly improves test time efficiency by adding a term to the training objective that encourages the exponentiated model scores to sum to one making the explicit summation at test time unnecessary approach does not improve training time efficiency. sampling approaches approximate the training-time softmax over a smaller subset of the vocabulary et al. a good review and comparison of these and other techniques for dealing with large output vocabularies is available in chen et al. an orthogonal line of work is attempting to sidestep the problem by working at the characters level rather than words level. desirable properties putting aside the prohibitive cost of using the large output vocabulary the model has very appealing properties. it achieves better perplexities than state-of-the-art traditional language models such as kneser-ney smoothed models and can scale to much larger orders neural language models than is possible with the traditional models. is is achievable because parameters are associated only with individual words and not with kgrams. moreover the words in different positions share parameters making them share statistical strength. e hidden layers of the models are in charge of finding informative word combinations and can at least in theory learn that for some words only sub-parts of the kgram are informative it can learn to back-up to smaller kgrams if needed like a traditional language model and do it in a context-dependent way. it can also learn skip-grams i.e. that for some combinations it should look at words and skipping words and another appealing property of the model besides the flexibility of the kgram orders is the ability to generalize across contexts. for example by observing that the words blue green red black etc. appear in similar contexts the model will be able to assign a reasonable score to the event green car even though it never observed this combination in training because it did observe blue car and red car. e combination of these properties the flexibility of considering only parts of the conditioning context and the ability to generalize to unseen contexts together with the only linear dependence on the conditioning context size in terms of both memory and computation make it very easy to increase the size of the conditioning context without suffering much from data sparsity and computation efficiency. e ease in which the neural language model can incorporate large and flexible conditioning contexts allow for creative definitions of contexts. for example devlin et al. propose a machine translation model in which the probability of the next word is conditioned on the previous k words in the generated translation as well as on m words in the source language that the given position in the translation is based on. is allows the model to be sensitive to topic-specific jargon and multi-word expressions in the source language and indeed results in much improved translation scores. limitations neural language models of the form presented here do have some limitations predicting the probability of a word in context is much more expensive than using a traditional language model and working with large vocabulary sizes and training corpora can become prohibitive. however they do make better use of the data and can get very competitive perplexities even with relatively small training set sizes. when applied in the context of a machine-translation system neural language models do not always improve the translation quality over traditional kneser-ney smoothed language models. however translation quality does improve when the probabilities from a traditional language model and a neural one are interpolated. it seems that the models complement each other the neural language models generalize better to unseen events but sometimes this generalization can hurt the performance and the rigidity of the traditional models is preferred. as an example consider the opposite of the colors example above a model is asked to assign a probability to the sentence red horse. a traditional model will assign it a very low score as it likely observed such such skip-grams were explored also for non-neural language models see pelemans et al. and the references therein. language modeling an event only a few times if at all. on the other hand a neural language model may have seen brown horse black horse and white horse and also learned independently that black white brown and red can appear in similar contexts. such a model will assign a much higher probability to the event red horse which is undesired. using language models for generation language models can also be used for generating sentences. after training a language model on a given collection of text one can generate sample random sentences from the model according to their probability using the following process predict a probability distribution over the first word conditioned on the start symbol and draw a random word according to the predicted distribution. en predict a probability distribution over the second word conditioned on the first and so on until predicting the end-of-sequence symbol. already with k d this produces very passable text and the quality improves with higher orders. when decoding a sentence from a trained language-model in this way one can either choose the highest scoring prediction at each step or sample a random word according to the predicted distribution. another option is to use beam search in order to find a sequence with a globally high probability the highest-prediction at each step may result in sub-optimal overall probability as the process may trap itself into a corner leading to prefixes that are followed by low-probability events. is is called the label-bias problem discussed in depth by andor et al. and lafferty et al. associates a probability value pi for each item i jv j such thatpjv j sampling from a multinomial distribution a multinomial distribution over jv j elements pi d in order to sample a random item from a multinomial distribution according to its probability the following algorithm can be used i s u while s do i i c s s pi return i f a uniform random number between and is is a naive algorithm with a computational complexity linear in the vocabulary size o.jv j. is can be prohibitively slow using large vocabularies. for peaked distributions where the values are sorted by decreasing probability the average time would be much faster. e alias method and peterson jr. is an efficient algorithm for sampling from arbitrary multinomial distributions with large vocabularies allowing sampling in after linear time pre-processing. byproduct word representations byproduct word representations language models can be trained on raw text for training a k-order language model one just needs to extract c from running text and treat the c word as the supervision signal. us we can generate practically unlimited training data for them. consider the matrix w appearing just before the final softmax. each column in this matrix is a dhid dimensional vector that is associated with a vocabulary item. during the final score computation each column in w is multiplied by the context representation h and this produces the score of the corresponding vocabulary item. intuitively this should cause words that are likely to appear in similar contexts to have similar vectors. following the distributional hypothesis according to which words that appear in similar contexts have similar meanings words with similar meanings will have similar vectors. a similar argument can be made about the rows of the embedding matrix e. as a byproduct of the language modeling process we also learn useful word representations in the rows and columns of the matrices e and w in the next chapter we further explore the topic of learning useful word representations from raw text. c h a p t e r pre-trained word representations uniformly sampled values i. a main component of the neural network approach is the use of embeddings representing each feature as a vector in a low dimensional space. but where do the vectors come from? is chapter surveys the common approaches. random initialization when enough supervised training data is available one can just treat the feature embeddings the same as the other model parameters initialize the embedding vectors to random values and let the network-training procedure tune them into good vectors. some care has to be taken in the way the random initialization is performed. e method used by the effective w implementation et al. is to initialize the word where d is the number of vectors to uniformly sampled random numbers in the range dimensions. another option is to use xavier initialization section and initialize with in practice one will often use the random initialization approach to initialize the embedding vectors of commonly occurring features such as part-of-speech tags or individual letters while using some form of supervised or unsupervised pre-training to initialize the potentially rare features such as features for individual words. e pre-trained vectors can then either be treated as fixed during the network training process or more commonly treated like the randomly initialized vectors and further tuned to the task at hand. supervised task-specific pre-training if we are interested in task a for which we only have a limited amount of labeled data example syntactic parsing but there is an auxiliary task b part-of-speech tagging for which we have much more labeled data we may want to pre-train our word vectors so that they perform well as predictors for task b and then use the trained vectors for training task a. in this way we can utilize the larger amounts of labeled data we have for task b. when training task a we can either treat the pre-trained vectors as fixed or tune them further for task a. another option is to train jointly for both objectives see chapter for more details. pre-trained word representations unsupervised pre-training e common case is that we do not have an auxiliary task with large enough amounts of annotated data maybe we want to help bootstrap the auxiliary task training with better vectors. in such cases we resort to unsupervised auxiliary tasks which can be trained on huge amounts of unannotated text. e techniques for training the word vectors are essentially those of supervised learning but instead of supervision for the task that we care about we instead create practically unlimited number of supervised training instances from raw text hoping that the tasks that we created will match be close enough to the final task we care about. e key idea behind the unsupervised approaches is that one would like the embedding vectors of similar words to have similar vectors. while word similarity is hard to define and is usually very task-dependent the current approaches derive from the distributional hypothesis stating that words are similar if they appear in similar contexts. e different methods all create supervised training instances in which the goal is to either predict the word from its context or predict the context from the word. in the final section of chapter we saw how language modeling creates word vectors as a byproduct of training. indeed language modeling can be treated as an unsupervised approach in which a word is predicted based on the context of the k preceding words. historically the algorithm of collobert and weston and weston collobert et al. and the w family of algorithms described below et al. were inspired by this property of language modeling. e w algorithms are designed to perform the same side effects as language modeling using a more efficient and more flexible framework. e g v algorithm by pennington et al. follows a similar objective. ese algorithms are also deeply connected to another family of algorithms which evolved in the nlp and ir communities and that are based on matrix factorization and goldberg word embeddings algorithms are discussed in section an important benefit of training word embeddings on large amounts of unannotated data is that it provides vector representations for words that do not appear in the supervised training set. ideally the representations for these words will be similar to those of related words that do appear in the training set allowing the model to generalize better on unseen events. it is thus desired that the similarity between word vectors learned by the unsupervised algorithm captures the same aspects of similarity that are useful for performing the intended task of the network. arguably the choice of auxiliary problem is being predicted based on what kind of context affects the resulting vectors much more than the learning method that is being used to train them. section surveys different choices of auxiliary problems. word embeddings derived by unsupervised training algorithms have applications in nlp beyond using them for initializing the word-embeddings layer of neural network model. ese are discussed in chapter e interpretation of creating auxiliary problems from raw text is inspired by ando and zhang word embedding algorithms using pre-trained embeddings when using pre-trained word embeddings there are some choices that should be taken. e first choice is about pre-processing should the pre-trained word vectors be used as is or should each vector be normalized to unit length? is is task dependent. for many word embedding algorithms the norm of the word vector correlates with the word s frequency. normalizing the words to unit length removes the frequency information. is could either be a desirable unification or an unfortunate information loss. e second choice regards fine-tuning the pre-trained vectors for the task. consider an embedding matrix e rjv associating words from vocabulary v with d-dimensional vectors. a common approach is to treat e as model parameters and change it with the rest of the network. while this works well it has the potential undesirable effect of changing the representations for words that appear in the training data but not for other words that used to be close to them in the original pre-trained vectors e. is may hurt the generalization properties we aim to get from the pre-training procedure. an alternative is to leave the pre-trained vectors e fixed. is keeps the generalization but prevents the model from adapting the representations for the given task. a middle ground is to keep e fixed but use an additional matrix t instead of looking at the rows of e we look at rows of a transformed matrix d e t e transformation matrix t is tuned as part of the network allowing to fine-tune some aspects of the pre-trained vectors for the task. however the task-specific adaptations are in the form of linear transformations that apply to all words not just those seen in training. e downside of this approach is the inability to change the representations of some words but not others example if hot and cold received very similar vectors it could be very hard for a linear transformation t to separate them. another option is to keep e fixed but use an additional matrix rjv and take the embedding matrix to be d e c or d e t c e matrix is initialized to and trained with the network allowing to learn additive changes to specific words. adding a strong regularization penalty over will encourage the fine-tuned representations to stay close to the original ones. word embedding algorithms e neural networks community has a tradition of thinking in terms of distributed representations et al. in contrast to local representations in which entities are represented as discrete symbols and the interactions between entities are encoded as a set of discrete relations between symbols forming a graph in distributed representations each entity is instead represented as a vector of value a pattern of activations and the meaning of the entity and its relation to other entities are captured by the activations in the vector and the similarities between different vectors. in the context of language processing it means that words sentences should not be mapped to discrete dimensions but rather mapped to a shared low dimensional space where note that all the updates during gradient-based training are additive and so without regularization updating e during training and keeping e fixed but updating and looking at e c will result in the same final embeddings. e approaches only differ when regularization is applied. pre-trained word representations each word will be associated with a d-dimensional vector and the meaning of the word will be captured by its relation to other words and the activation values in its vector. e natural language processing community has a tradition in thinking in terms of distributional semantics in which a meaning of a word could be derived from its distribution in a corpus i.e. from the aggregate of the contexts in which it is being used. words that tend to occur in similar contexts tend to have similar meanings. ese two approaches to representing words in terms of patterns of activations that are learned in the context of a larger algorithm and in terms of co-occurrence patterns with other words or syntactic structures give rise to seemingly very different views of word representations leading to different algorithmic families and lines of thinking. in section we will explore the distributional approach to word representation and in section we ll explore the distributed approaches. section will connect the two worlds and show that for the most part current state-of-the-art distributed representations of words are using distributional signals to do most of their heavy lifting and that the two algorithmic families are deeply connected. distributional hypothesis and word representations e distributional hypothesis about language and word meaning states that words that occur in the same contexts tend to have similar meanings e idea was popularized by firth through the saying you shall know a word by the company it keeps. intuitively when people encounter a sentence with an unknown word such as the word wampimuk in marco saw a hairy little wampinuk crouching behind a tree they infer the meaning of the word based on the context in which it occurs. is idea has given rise to the field of distributional semantics a research area interested in quantifying semantic similarities between linguistic items according to their distributional properties in large text corpora. for a discussion of the linguistic and philosophical basis of the distributional hypothesis see sahlgren word-context matrices in nlp a long line of research captures the distributional properties of words using word-context matrices in which each row i represents a word each column j represents a linguistic context in which words can occur and a matrix entry m quantifies the strength of association between a word and a context in a large corpus. in other words each word is represented as a sparse vector in high dimensional space encoding the weighted bag of contexts in which it occurs. different definitions of contexts and different ways of measuring the association between a word and a context give rise to different word representations. different distance functions can be used to measure the distances between word vectors which are taken to represent the semantic distances between the associated words. see the survey of turney and pantel and baroni and lenci for an overview. word embedding algorithms more formally denote by vw the set of words words vocabulary and by vc the set of possible contexts. we assume each word and each context are indexed such that wi is the ith word in the words vocabulary and cj is the jth word in the context vocabulary. e matrix m f d f cj where f is an association rjvw is the word-context matrix defined as m f measure of the strength between a word and a context. similarity measures once words are represented as vectors one can compute similarities between words by computing the similarities between the corresponding vectors. a common and effective measure is the cosine similarity measuring the cosine of the angle between the vectors pi ppi pi pi another popular measure is the generalized jacaard similarity defined as simcos.u v d u v d simjacaard.u v d word-context weighting and pmi e function f is usually based on counts from a large corpus. denote by c the number of it is intuitive to define f c to be the count f c d c or the times word w occurred in the context c in the corpus d and let jdj be the corpus size d normalized count f c d p c d however this has the undesired effect of assigning high weights to word-context pairs involving very common contexts example consider the context of a word to be the previous word. en for a word such as cat the events the cat and a cat will receive much higher scores than cute cat and small cat even though the later are much more informative. to counter this effect it is better to define f to favor informative contexts for a given word contexts that co-occur more with the given word than with other words. an effective metric that captures this behavior is the pointwise mutual information an information-theoretic association measure between a pair of discrete outcomes x and y defined as jdj pmi.x y d log p y in our case pmi.w c measures the association between a word w and a context c by calculating the log of the ratio between their joint probability frequency in which they cooccur together and their marginal probabilities frequencies in which they occur individually. pmi can be estimated empirically by considering the actual number of observations in a corpus p f c d pmi.w c d log c jdj when thinking of u and v as sets the jacaard similarity is defined as juvj juvj pre-trained word representations ppmi.w c d max c where and c are the corpus frequencies of w and c respectively. e use of pmi as a measure of association in nlp was introduced by church and hanks and widely adopted for word similarity and distributional semantic tasks et al. turney turney and pantel working with the pmi matrix presents some computational challenges. e rows of mpmi contain many entries of word-context pairs c that were never observed in the corpus for which pmi.w c d log d a common solution is to use the positive pmi metric in which all negative values are replaced by systematic comparisons of various weighting schemes for entries in the word-context similarity matrix show that the pmi and more so the positive-pmi metrics provide the best results for a wide range of word-similarity tasks and levy kiela and clark a deficiency of pmi is that it tends to assign high value to rare events. for example if two events occur only once but occur together they will receive a high pmi value. it is therefore advisable to apply a count threshold before using the pmi metric or to otherwise discount rare events. dimensionality reduction through matrix factorization a potential obstacle of representing words as the explicit set of contexts in which they occur is that of data sparsity some entries in the matrix m may be incorrect because we did not observe enough data points. additionally the explicit word vectors are of a very high dimensions on the definition of context the number of possible contexts can be in the hundreds of thousands or even millions. both issues can be alleviated by considering a low-rank representation of the data using a dimensionality reduction technique such as the singular value decomposition svd works by factorizing the matrix m rjvw into two narrow matrices a w rjvw word matrix and a c context matrix such that w c d rjvw is the best rank-d approximation of m in the sense that no other rank-d matrix has a closer distance to m than e low-rank representation can be seen as a smoothed version of m based on robust patterns in the data some of the measurements are fixed. is has the effect for example of adding words to contexts that they were not seen with if other words in this context seem to when representing words there is some intuition behind ignoring negative values humans can easily think of positive associations canada and snow but find it much harder to invent negative ones canada and desert is suggests that the perceived similarity of two words is influenced more by the positive context they share than by the negative context they share. it therefore makes some intuitive sense to discard the negatively associated contexts and mark them as uninformative instead. a notable exception would be in the case of syntactic similarity. for example all verbs share a very strong negative association with being preceded by determiners and past tense verbs have a very strong negative association to be preceded by be verbs and modals. word embedding algorithms co-locate with each other. moreover the matrix w allows to represent each word as a dense ddimensional vector instead of a sparse jvcj-dimensional one where d jvcj choices are d such that the d-dimensional vectors captures the most important directions of variation in the original matrix. one can then compute similarities based on the dense d-dim vectors instead of the sparse high-dimensional ones. e mathematics of svd e singular value decomposition is an algebraic technique by which an m n real or complex matrix m is factorized into three matrices m d u dv where u is an m m real or complex matrix d is an m n real or complex matrix and v is an n n matrix. e matrices u and v are orthonormal meaning that their rows are unit-length and orthogonal to each other. e matrix d is diagonal where the elements on the diagonal are the singular values of m in decreasing order. e factorization is exact. e svd has many uses in machine learning and elsewhere. for our purposes svd is used for dimensionality reduction finding low-dimensional representations of high-dimensional data that preserve most of the information in the original data. consider the multiplication u qdv where qd is a version of d in which all but the first k elements on the diagonal are replaced by zeros. we can now zero out all but the first k rows of u and columns of v as they will be zeroed out by the multiplication anyhow. deleting the rows and columns leaves us with three matrices qu k k diagonal and v n. e product is a n matrix of rank k. and n and can be thought of as a low rank approximation of m. the best rank-k approximation of m under loss. at is is the minimizer of e matrix is the product of thin matrices qu and qv with k much smaller than m according to the eckart-young theorem and young the matrix is d qu qd qv d argmin kx st x is rank-k e matrix can be thought of as a smoothed version of m in the sense that it uses only the k most influential directions in the data. approximatingrowdistances e low-dimensional rows of e d qu qd are low-rank approximations of the high-dimensional rows of the original matrix m in the sense that computing the dot product between rows of e is equivalent to computing the dot-product between the rows of the reconstructed matrix at is e e d pre-trained word representations to see why consider the m m matrix s e d e e. an entry j in this matrix is d e e similarly for the equal to the dot product between rows i and j in e s e matrix s d we will show that s e d s recall that qv qv d i because qv is orthonormal. now s d d qu qd qv qu qd qv d qu qd qv qv qd qu d d qu qd. qv qv qd qu d qu qd. qu qd d e e d s e we can thus use the rows of e instead of the high-dimensional rows of instead of the high-dimensional rows of m. using a similar argument we can also use the rows of qd qv instead of the columns of when using svd for word similarity the rows of m correspond to words the columns to contexts and the vectors comprising the rows of e are low-dimensional word representations. in practice it is often better to not use e d qu qd but instead to use the more balanced version e d qup qd or even ignoring the singular values qd completely and taking e d qu. from neural language models to distributed representations in contrast to the so-called count-based methods described above the neural networks community advocates the use of distributed representations of word meanings. in a distributed representation each word is associated with a vector in rd where the meaning of the word with respect to some task is captured in the different dimensions of the vector as well as in the dimensions of other words. unlike the explicit distributional representations in which each dimension corresponds to a specific context the word occurs in the dimensions in the distributed representation are not interpretable and specific dimensions do not necessarily correspond to specific concepts. e distributed nature of the representation means that a given aspect of meaning may be captured by over a combination of many dimensions and that a given dimension may contribute to capturing several aspects of meaning. consider the language modeling network in equation in chapter e context of a word is the kgram of words preceding it. each word is associated with a vector and their concatenation is encoded into a dhid dimensional vector h using a nonlinear transformation. e vector h is then multiplied by a matrix w in which each column corresponds to a word and interactions between h and columns in w determine the probabilities of the different words given the context. e columns of w well as the rows of the embeddings matrix e are distributed we note that in many ways the explicit distributional representations is also distributed different aspects of the meaning of a word are captured by groups of contexts the word occurs in and a given context can contribute to different aspects of meaning. moreover after performing dimensionality reduction over the word-context matrix the dimensions are no longer easily interpretable. word embedding algorithms representations of words the training process determines good values to the embeddings such that they produce correct probability estimates for a word in the context of a kgram capturing the meaning of the words in the columns of w associated with them. collobert and weston e design of the network in equation is driven by the language modeling task which poses two important requirements the need to produce a probability distributions over words and the need to condition on contexts that can be combined using the chain-rule of probability to produce sentence-level probability estimates. e need to produce a probability distribution dictates the need to compute an expensive normalization term involving all the words in the output vocabulary while the need to decompose according to the chain-rule restricts the conditioning context to preceding kgrams. if we only care about the resulting representations both of the constraints can be relaxed as was done by collobert and weston in a model which was refined and presented in greater depth by bengio et al. e first change introduced by collobert and weston was changing the context of a word from the preceding kgram words to its left to a window surrounding it computing p e generalization to other kinds of fixed-sized contexts is straightforward. e second change introduced by collobert and weston is to abandon the probabilistic output requirement. instead of computing a probability distribution over target words given a context their model only attempts to assign a score to each word such that the correct word scores above incorrect ones. is removes the need to perform the computationally expensive normalization over the output vocabulary making the computation time independent of the output vocabulary size. is not only makes the network much faster to train and use but also makes it scalable to practically unlimited vocabularies only cost of increasing the vocabulary is a linear increase in memory usage. let w be a target word be an ordered list of context items and vw and vc.c embedding functions mapping word and context indices to demb dimensional vectors now on we assume the word and context vectors have the same number of dimensions. e model of collobert and weston computes a score s.w of a word-context pair by concatenating the word and the context embeddings into a vector x which is fed into an mlp with one hidden layer whose single output is the score assigned to the word-context combination instead of p s.w d g.xu v x d vc.cki vw u v rdh e network is trained with a margin-based ranking loss to score correct word-context pairs above incorrect word-context pairs with a margin of at least e loss pre-trained word representations l.w for a given word-context pair is given by l.w c d where is a random word from the vocabulary. e training procedure repeatedly goes over word-context pairs from the corpus and for each one samples a random word computes the loss l.w c using and updates parameters u v and the word and context embeddings to minimize the loss. e use of randomly sampled words to produce negative examples of incorrect word-context to drive the optimization is also at the core of the w algorithm to be described next. e widely popular w algorithm was developed by tom mikolov and colleagues over a series of papers et al. like the algorithm of collobert and weston w also starts with a neural language model and modifies it to produce faster results. is not a single algorithm it is a software package implementing two different context representations and skip-gram and two different optimization objectives and hierarchical softmax. here we focus on the negative-sampling objective like collobert and weston s algorithm the ns variant of w works by training the network to distinguish good word-context pairs from bad ones. however w replaces the margin-based ranking objective with a probabilistic one. consider a set d of correct word-context pairs and a set nd of incorrect word-context pairs. e goal of the algorithm is to estimate the probability p d c that the word-context pair came from the correct set d. is should be high for pairs from d and low for pairs from nd. e probability constraint dictates that p d c d p d c. e probability function is modeled as a sigmoid over the score s.w c e corpus-wide objective of the algorithm is to maximize the log-likelihood of the data p d c d c d nd d nd d x log p d c c x log p d c nd e positive examples d are generated from a corpus. e negative examples nd can be generated in many ways. in w they are generated by the following process for each good pair c d sample k words and add each of c as a negative example to nd. is results in the negative samples data nd being k times larger than d. e number of negative samples k is a parameter of the algorithm. word embedding algorithms before normalizing e negative words w can be sampled according to their corpus-based frequency or as done in the w implementation according to a smoothed version in which the is second version gives counts are raised to the power of more relative weight to less frequent words and results in better word similarities in practice. cbow other than changing the objective from margin-based to a probabilistic one w also considerably simplify the definition of the word-context scoring function vector c to be a sum of the embedding vectors of the context components c dpk s.w c. for a multi-word context the cbow variant of w defines the context ci. it then defines the score to be simply s.w c d w c resulting in p d d c e cbow variant loses the order information between the context s elements. in return it allows the use of variable-length contexts. however note that for contexts with bound length the cbow can still retain the order information by including the relative position as part of the content element itself i.e. by assigning different embedding vector to context elements in different relative positions. skip-gram e skip-gram variant of w scoring decouples the dependence between the context elements even further. for a k-elements context the skip-gram variant assumes that the elements ci in the context are independent from each other essentially treating them as k different contexts i.e. a word-context pair ciwk will be represented in d as k different contexts ck. e scoring function s.w c is defined as in the cbow version but now each context is single embedding vector ky c ky kx p d ci d c p d d log p d d p d ci d log c while introducing strong independence assumptions between the elements of the context the skip-gram variant is very effective in practice and very commonly used. connecting the worlds both the distributional count-based method and the distributed neural ones are based on the distributional hypothesis attempting capture the similarity between words based on the similarity pre-trained word representations between the contexts in which they occur. in fact levy and goldberg show that the ties between the two worlds are deeper than appear at first sight. e training of w models result in two embedding matrices e w rjvw and e c representing the words and the contexts respectively. e context embeddings are discarded after training and the word embeddings are kept. however imagine keeping the context embedding matrix e c and consider the product e w e c d rjvw viewed this way w is factorizing an implicit word-context matrix what are the elements of matrix an entry corresponds to the dot product of the word and context embedding vectors w c. levy and goldberg show that for the combination of skip-grams contexts and the negative sampling objective with k negative samples the global objective is minimized by setting w c d d pmi.w c log k. at is w is implicitly factorizing a matrix which is closely related to the well-known word-context pmi matrix! remarkably it does so without ever explicitly constructing the matrix e above analysis assumes that the negative samples are sampled according to the recall that the w implementation sampus frequency of the words p d under this sampling scheme ples instead from a modified distribution p d p log k. indeed using this the optimal value changes to c log k d log modified version of pmi when constructing sparse and explicit distributional vectors improves the similarity in that setup as well. e w algorithms are very effective in practice and are highly scalable allowing to train word representations with very large vocabularies over billions of words of text in a matter of hours with very modest memory requirements. e connection between the sgns variant of w and word-context matrix-factorization approaches ties the neural methods and the traditional count-based ones suggesting that lessons learned in the study of distributional representation can transfer to the distributed algorithms and vice versa and that in a deep sense the two algorithmic families are equivalent. p other algorithms many variants on the w algorithms exist none of which convincingly produce qualitatively or quantitatively superior word representations. is sections list a few of the popular ones. nce e noise-contrastive estimation approach of mnih and kavukcuoglu is very similar to the sgns variant of w but instead of modeling p d j w ci as in if the optimal assignment was satisfiable the skip-grams with negative-sampling solution is the same as the svd over word-context matrix solution. of course the low dimensionality demb of w and c may make it impossible to satisfy w c d pmi.w c log k for all w and c pairs and the optimization procedure will attempt to find the best achievable solution while paying a price for each deviation from the optimal assignment. is is where the sgns and the svd objectives differ svd puts a quadratic penalty on each deviation while sgns uses a more complex penalty term. equation it is modeled as the choice of contexts p d j w ci d p d j w ci d c k q.w k q.w c k q.w jdj is the observed unigram frequency of w in the corpus. is algorithm is based where q.w d on the noise-contrastive estimation probability modeling technique and hyv rinen according to levy and goldberg this objective is equivalent to factorizing the word-context matrix whose entries are the log conditional probabilities log p log k. glove e g v algorithm et al. constructs an explicit word-context matrix and trains the word and context vectors w and c attempting to satisfy w c c c d log.w c c d where and are word-specific and context-specific trained biases. e optimization procedure looks at observed word context pairs while skipping zero count events. in terms of matrix factorization if we fix d log.w and d log.c we ll get an objective that is very similar to factorizing the word-context pmi matrix shifted by log.jdj. however in glove these parameters are learned and not fixed giving it another degree of freedom. e optimization objective is weighted least-squares loss assigning more weight to the correct reconstruction of frequent items. finally when using the same word and context vocabularies the glove model suggests representing each word as the sum of its corresponding word and context embedding vectors. the choice of contexts e choice of context by which a word is predicted has a profound effect on the resulting word vectors and the similarities they encode. in most cases the contexts of a word are taken to be other words that appear in its surrounding either in a short window around it or within the same sentence paragraph or document. in some cases the text is automatically parsed by a syntactic parser and the contexts are derived from the syntactic neighborhood induced by the automatic parse trees. sometimes the definitions of words and context change to include also parts of words such as prefixes or suffixes. window approach e most common approach is a sliding window approach in which auxiliary tasks are created by looking at a sequence of c words. e middle word is called the focus word and the m words to each side are the contexts. en either a single task is created in which the goal is to predict the focus word based on all of the context words either using cbow et al. pre-trained word representations or vector concatenation and weston or distinct tasks are created each pairing the focus word with a different context word. e tasks approach popularized by mikolov et al. is referred to as a skip-gram model. skip-gram-based approaches are shown to be robust and efficient to train et al. pennington et al. and often produce state of the art results. effect of window size e size of the sliding window has a strong effect on the resulting vector similarities. larger windows tend to produce more topical similarities dog bark and leash will be grouped together as well as walked run and walking while smaller windows tend to produce more functional and syntactic similarities poodle pitbull rottweiler or walking running approaching positional windows when using the cbow or skip-gram context representations all the different context words within the window are treated equally. ere is no distinction between context words that are close to the focus words and those that are farther from it and likewise there is no distinction between context words that appear before the focus words to context words that appear after it. such information can easily be factored in by using positional contexts indicating for each context word also its relative position to the focus words instead of the context word being the it becomes indicating the word appears two positions to the right of the focus word. e use of positional context together with smaller windows tend to produce similarities that are more syntactic with a strong tendency of grouping together words that share a part of speech as well as being functionally similar in terms of their semantics. positional vectors were shown by ling et al. to be more effective than window-based vectors when used to initialize networks for part-of-speech tagging and syntactic dependency parsing. variants many variants on the window approach are possible. one may lemmatize words before learning apply text normalization filter too short or too long sentences or remove capitalization e.g. the pre-processing steps described by dos santos and gatti one may subsample part of the corpus skipping with some probability the creation of tasks from windows that have too common or too rare focus words. e window size may be dynamic using a different window size at each turn. one may weigh the different positions in the window differently focusing more on trying to predict correctly close word-context pairs than further away ones. each of these choices is a hyperparameter to be manually set before training and will effect the resulting vectors. ideally these will be tuned for the task at hand. much of the strong performance of the w implementation can be attributed to specifying good default values for these hyperparameters. some of these hyperparameters others are discussed in detail in levy et al. the choice of contexts sentences paragraphs or documents using a skip-gram cbow approach one can consider the contexts of a word to be all the other words that appear with it in the same sentence paragraph or document. is is equivalent to using very large window sizes and is expected to result in word vectors that capture topical similarity from the same topic i.e. words that one would expect to appear in the same document are likely to receive similar vectors. syntactic window some work replace the linear context within a sentence with a syntactic one et al. levy and goldberg e text is automatically parsed using a dependency parser and the context of a word is taken to be the words that are in its proximity in the parse tree together with the syntactic relation by which they are connected. such approaches produce highly functional similarities grouping together words than can fill the same role in a sentence colors names of schools verbs of movement. e grouping is also syntactic grouping together words that share an inflection and goldberg e effect of context e following table taken from levy and goldberg shows the most similar words to some seed words when using bag-of-words windows of size and and b as well as dependency-based contexts using the same underlying corpora and the same embeddings algorithm notice how for some words batman the induced word similarities are somewhat agnostic to the contexts while for others there is a clear trend the larger window contexts result in more topical similarities is similar to other terms in the harry potter universe turing is related to computability dancing is similar to other inflections of the word while the syntactic-dependency contexts result in more functional similarities similar to other fictional or non-fictional schools turing is similar to other scientists and dancing to other gerunds of entrainment activities. e smaller context window is somewhere in between the two. is re-affirms that context choices strongly affects the resulting word representations and stresses the need to take the choice of context into consideration when using unsupervised word embeddings. pre-trained word representations multilingual another option is using multilingual translation-based contexts and dyer hermann and blunsom for example given a large amount of sentence-aligned parallel text target nite-statenondeterministicbuchiprimalitypaulinghotellinghetinglessinghamming oridagainesville ajacksonvilletampalauderdale caspect-orientedevent-drivenobjective-cdata crule-baseddata-drivenhuman-centereddancingsingingdancedancesdancerstap-dancingsingingdancedancesbreakdancingclowningsingingrappingbreakdancingmimingbusking the choice of contexts one can run a bilingual alignment model such as the ibm model or model using the giza software and then use the produced alignments to derive word contexts. here the context of a word instance is the foreign language words that are aligned to it. such alignments tend to result in synonym words receiving similar vectors. some authors work instead on the sentence alignment level without relying on word alignments et al. or train an end-to-end machine-translation neural network and use the resulting word embeddings et al. an appealing method is to mix a monolingual window-based approach with a multilingual approach creating both kinds of auxiliary tasks. is is likely to produce vectors that are similar to the window-based approach but reducing the somewhat undesired effect of the window-based approach in which antonyms hot and cold high and low tend to receive similar vectors and dyer for further discussion on multilingual word embeddings and a comparison of different methods see levy et al. character-based and sub-word representations an interesting line of work attempts to derive the vector representation of a word from the characters that compose it. such approaches are likely to be particularly useful for tasks which are syntactic in nature as the character patterns within words are strongly related to their syntactic function. ese approaches also have the benefit of producing very small model sizes one vector for each character in the alphabet together with a handful of small matrices needs to be stored and being able to provide an embedding vector for every word that may be encountered. dos santos and gatti dos santos and zadrozny and kim et al. model the embedding of a word using a convolutional network chapter over the characters. ling et al. model the embedding of a word using the concatenation of the final states of two rnn encoders one reading the characters from left to right and the other from right to left. both produce very strong results for part-of-speech tagging. e work of ballesteros et al. show that the two-lstms encoding of ling et al. is beneficial also for representing words in dependency parsing of morphologically rich languages. deriving representations of words from the representations of their characters is motivated by the unknown words problem what do you do when you encounter a word for which you do not have an embedding vector? working on the level of characters alleviates this problem to a large extent as the vocabulary of possible characters is much smaller than the vocabulary of possible words. however working on the character level is very challenging as the relationship between form and function semantics in language is quite loose. restricting oneself to stay on the character level may be an unnecessarily hard constraint. some researchers propose a middle-ground in which a word is represented as a combination of a vector for the word itself with vectors of sub-word units that comprise it. e sub-word embeddings then help in sharing information between different words with similar forms as well as allowing back-off to the subword level when the word is not observed. at the same time the models are not forced to rely solely on form when enough observations of the word are available. botha and blunsom pre-trained word representations suggest to model the embedding vector of a word as a sum of the word-specific vector if such vector is available with vectors for the different morphological components that comprise it components are derived using morfessor and lagus an unsupervised morphological segmentation method. gao et al. suggest using as core features not only the word form itself but also a unique feature a unique embedding vector for each of the letter-trigrams in the word. another middle ground between characters and words is breaking up words into meaningful units which are larger than characters and are automatically derived from the corpus. one such approach is to use byte-pair encoding which was introduced by sennrich et al. in the context of machine translation and proved to be very effective. in the b approach one decides on a vocabulary size and then looks for units that can represent all the words in the corpus vocabulary according to the following algorithm taken from sennrich et al. we initialize the symbol vocabulary with the character vocabulary and represent each word as a sequence of characters plus a special end-of-word symbol which allows us to restore the original tokenization after translation. we iteratively count all symbol pairs and replace each occurrence of the most frequent pair b with a new symbol ab. each merge operation produces a new symbol which represents a character n-gram. frequent character n-grams whole words are eventually merged into a single symbol thus b requires no shortlist. e final symbol vocabulary size is equal to the size of the initial vocabulary plus the number of merge operations the latter is the only hyperparameter of the algorithm. for efficiency we do not consider pairs that cross word boundaries. e algorithm can thus be run on the dictionary extracted from a text with each word being weighted by its frequency. dealing with multi-word units and word inflections two issues that are still under-explored with respect to word representations have to do with the definition of a word. e unsupervised word embedding algorithms assume words correspond to tokens characters without whitespace or punctuation see the what is a word? discussion in section is definition often breaks. in english we have many multi-token units such as new york and ice cream as well as looser cases such as boston university or volga river that we may want to assign to single vectors. in many languages other than english rich morphological inflection systems make forms that relate to the same underlying concept look differently. for example in many languages adjectives are inflected for number and gender causing the word yellow describing a plural masculine noun to have a different form from the word yellow describing a singular feminine noun. even worse as the inflection system also dictates the forms of the neighboring words near the limitations of distributional methods singular feminine form of yellow are themselves in a singular feminine form different inflections of the same word often do not end up similar to each other. while there are no good solutions to either of these problems they can both be addressed to a reasonable degree by deterministically pre-processing the text such that it better fits the desired definitions of words. in the multi-token units case one can derive a list of such multi-token items and replace them in the text with single entities replacing occurrences of new york with new_york. mikolov et al. proposes a pmi-based method for automatically creating such a list by considering the pmi of a word pair and merging pairs with pmi scores that pass some predefined thresholds. e process then iteratively repeats to merge pairs words into triplets and so on. en the embedding algorithm is run over the pre-processed corpus. is coarse but effective heuristic is implemented as part of the w package allowing to derive embeddings also for some prominent multi-token items. in the inflections case one can mitigate the problem to a large extent by pre-processing the corpus by lemmatizing some or all of the words embedding the lemmas instead of the inflected forms. a related pre-processing is pos-tagging the corpus and replacing words with pairs creating for example the two different token types booknoun and bookverb that will each receive a different embedding vector. for further discussion on the interplay of morphological inflections and word embeddings algorithms see avraham and goldberg cotterell and schutze limitations of distributional methods e distributional hypothesis offers an appealing platform for deriving word similarities by representing words according to the contexts in which they occur. it does however have some inherent limitations that should be considered when using the derived representations. definition of similarity e definition of similarity in distributional approaches is completely operational words are similar if used in similar contexts. but in practice there are many facets of similarity. for example consider the words dog cat and tiger. on the one hand cat is more similar to dog than to tiger as both are pets. on the other hand cat can be considered more similar to tiger than to dog as they are both felines. some facets may be preferred over others in certain use cases and some may not be attested by the text as strongly as others. e distributional methods provide very little control over the kind of similarities they induce. is could be controlled to some extent by the choice of conditioning contexts but it is far from being a complete solution. blacksheeps when using texts as the conditioning contextsmany of the more trivial properties of the word may not be reflected in the text and thus not captured in the representation. is happens because of a well-documented bias in people s use of language stemming from efficiency constraints on communication people are less likely to mention known information than they are for in-depth discussion of heuristics for finding informative word collocations see manning and sch tze chapter pre-trained word representations to mention novel one. us when people talk of white sheep they will likely refer to them as sheep while for black sheep they are much more likely to retain the color information and say black sheep. a model trained on text data only can be greatly misled by this. antonyms words that are the opposite of each other vs. bad buy vs. sell hot vs cold tend to appear in similar contexts that can be hot can also be cold things that are bought are often sold. as a consequence models based on the distributional hypothesis tend to judge antonyms as very similar to each other. corpus biases for better or worse the distributional methods reflect the usage patterns in the corpora on which they are based and the corpora in turn reflect human biases in the real world or otherwise. indeed caliskan-islam et al. found that distributional word vectors encode every linguistic bias documented in psychology that we have looked for including racial and gender stereotypes european american names are closer to pleasant terms while african american names are closer to unpleasant terms female names are more associated with family terms than with career terms it is possible to predict the percentage of women in an occupation according to u.s. census based on the vector representation of the occupation name. like with the antonyms case this behavior may or may not be desired depending on the use case if our task is to guess the gender of a character knowing that nurses are stereotypically females while doctors are stereotypically males may be a desired property of the algorithm. in many other cases however we would like to ignore such biases. in any case these tendencies of the induced word similarities should be taken into consideration when using distributional representations. for further discussion see caliskan-islam et al. and bolukbasi et al. lack of context e distributional approaches aggregate the contexts in which a term occurs in a large corpus. e result is a word representation which is context independent. in reality there is no such thing as a context-independent meaning for a word. as argued by firth the complete meaning of a word is always contextual and no study of meaning apart from context can be taken seriously an obvious manifestation of this is the case of polysemy some words have obvious multiple senses a bank may refer to a financial institution or to the side of a river a star may an abstract shape a celebrity an astronomical entity and so on. using a single vector for all forms is problematic. in addition to the multiple senses problem there are also much subtler contextdependent variations in word meaning. c h a p t e r using word embeddings in chapter we discussed algorithms for deriving word vectors from large quantities of unannotated text. such vectors can be very useful as initialization for the word embedding matrices in dedicated neural networks. ey also have practical uses on their own outside the context of neural networks. is chapter discusses some of these uses. notation in this chapter we assume each word is assigned an integer index and use symbols such as w or wi to refer to both a word and its index. e is then the row in e corresponding to word w. we sometimes use w wi to denote the vectors corresponding to w and wi. obtaining word vectors word-embedding vectors are easy to train from a corpus and efficient implementations of training algorithms are available. moreover one can also download pre-trained word vectors that were trained on very large quantities of text in mind that differences in training regimes and underlying corpora have a strong influence on the resulting representations and that the available pre-trained representations may not be the best choice for the particular use case. as the time of this writing efficient implementations of the w algorithms are available as a stand-alone binary as well as in the g s python package. a modification of the w binary that allows using arbitrary contexts is also available. an efficient implementation of the glove model is available as well. pre-trained word vectors for english can be obtained from google and stanford as well as other sources. pre-trained vectors in languages other than english can be obtain from the polyglot project. word similarity given pre-trained word embedding vectors the major use aside from feeding them into a neural network is to compute the similarity between two words using a similarity function over vectors si m.u v. a common and effective choice for similarity between vectors is the cosine similarity httpsradimrehurek.comgensim httpnlp.stanford.eduprojectsglove httpnlp.stanford.eduprojectsglove httppolyglot.readthedocs.org using word embeddings corresponding to the cosine of the angle between the vectors u v simcos.u v d duces to a dot-product simcos.u v d u v dpi working with dot-products is very conwhen the vectors u and v are of unit-length d d the cosine similarity revenient computationally and it is common to normalize the embeddings matrix such that each row has unit length. from now on we assume the embeddings matrix e is normalized in this way. word clustering e word vectors can be easily clustered using clustering algorithms such as kmeans that are defined over euclidean spaces. e clusters can then be used as features in learning algorithms that work with discrete features or in other systems that require discrete symbols such as ir indexing systems. finding similar words with row-normalized embeddings matrix as described above the cosine similarity between two words and is given by d e e we are often interested in the k most similar words to a given word. let w d e be the vector corresponding to word w. e similarity to all other words can be computed by the matrixvector multiplication s d e w. e result s is a vector of similarities where is the similarity of w to the ith word in the vocabulary ith row in e. e k most similar words can be extracted by finding the indices corresponding to the k highest values in s. in a optimized modern scientific computing library such as numpy such matrix-vector multiplication is executed in milliseconds for embedding matrices with hundreds of thousands of vectors allowing rather rapid calculation of similarities. word similarities that result from distributional measures can be combined with other forms of similarity. for example we can define a similarity measure that is based on orthographic similarity that share the same letters. by filtering the list of top-k distributional-similar words to contain words that are also orthographically similar to the target word we can find spelling variants and common typos of the target word. httpwww.numpy.org odd-one out similarity to a group of words we may be interested in finding the most similar word to a group of words. is need arises when we have a list of related words and want to expand it example we have a list of four countries and want to extend it with names of more countries or we have a list of gene names and want find names of additional genes. another use case is when we want to direct the similarity to be to a given sense of a word. by creating a list of words that are related to that sense we can direct the similarity query toward that sense. ere are many way of defining similarity of an item to a group here we take the definition kpk to be the average similarity to the items in the group i.e. given a group of words we define simcos.w wi its similarity to word w as sim.w d anks to linearity computing the average cosine similarity from a group of words to all other words can be again done using a single matrix-vector multiplication this time between the embedding matrix and the average word vector of the words in the group. e vector s in which d sim.w is computed by s d e c c c wkk odd-one out consider an odd-one-out question in which we are given a list of words and need to find the one that does not belong. is can be done by computing the similarity between each word to the average word vector of the group and returning the least similar word. short document similarity sometimes we are interested in computing a similarity between two documents. while the best results are likely to be achieved using dedicated models solutions based on pre-trained word embeddings are often very competitive especially when dealing with short documents as such web queries newspaper headlines or tweets. e idea is to represent each document as a bag-ofwords and define the similarity between the documents to be the sum of the pairwise similarities between the words in the documents. formally consider two documents d m and d n and define the document similarity as d i j mx nx using basic linear algebra it is straightforward to show that for normalized word vectors this similarity function can be computed as the dot product between the continuous-bag-of-words using word embeddings representations of the documents i mx j nx d consider a document collection and let d be a matrix in which each row i is the continuous bag-of-words representation of document di. en the similarity between a new matrix-vector product s d d document d and each of the documents in the collection can be computed using a single word analogies an interesting observation by mikolov and colleagues et al. mikolov et al. that greatly contributed to the popularity of word embeddings is that one can perform algebra on the word vectors and get meaningful results. for example for word embeddings trained using w one could take the vector of the word king subtract the word man add the word woman and get that the closest vector to the result excluding the words king man and woman belongs to the word queen. at is in vector space wking wman c wwoman wqueen. similar results are obtained for various other semantic relations for example wfrance wparis c wlondon wengland and the same holds for many other cities and countries. is has given rise to the analogy solving task in which different word embeddings are evaluated on their ability to answer analogy questions of the form manwoman king? by solving analogy.m w w k d argmax nfmwkg cos.v k m c w levy and goldberg observe that for normalized vectors solving the maximization in equation is equivalent to solving equation that is searching for a word that is similar to king similar to man and dissimilar to woman cos.v k cos.v m c cos.v w analogy.m w w k d argmax nfmwkg levy and goldberg refer to this method as a e move from arithmetics between words in vector space to arithmetics between word similarities helps to explain to some extent the ability of the word embeddings to solve analogies as well as suggest which kinds of analogies can be recovered by this method. it also highlights a possible deficiency of the a analogy recovery method because of the additive nature of the objective one term in the summation may dominate the expression effectively ignoring the others. as suggested by levy and goldberg this can be alleviated by changing to a multiplicative objective m analogy.m w w k d argmax nfmwkg cos.v kcos.v w cos.v m c retrofitting and projections while the analogy-recovery task is somewhat popular for evaluating word embeddings it is not clear what success on a benchmark of analogy tasks says about the quality of the word embeddings beyond their suitability for solving this specific task. retrofitting and projections more often than not the resulting similarities do not fully reflect the similarities one has in mind for their application. often one can come up with or have access to a representative and relatively large list of word pairs that reflects the desired similarity better than the word embeddings but has worse coverage. e retrofitting method of faruqui et al. allows to use such data in order to improve the quality of the word embeddings matrix. faruqui et al. show the effectiveness of the approach by using information derived from wordnet and ppdb to improve pre-trained embedding vectors. e method assumes pre-trained word embedding matrix e as well as a graph g that encodes binary word to word similarities nodes in the graph are words and words are similar if they are connected by an edge. note that the graph representation is very general and a list of word pairs that are considered similar easily fits within the framework. e method works by solving an optimization problems that searches for a new word embeddings matrix oe whose rows are close both to the corresponding rows in e but also to the rows corresponding to their neighbors in the graph g. concretely the optimization objective is oe e c x nx argmin oe oe oe where and reflect the importance of a word being similar to itself or to another word. in practice are typically set uniformly to while is set to the inverse of the degree of wi in the graph a word has many neighbors it has smaller influence on each of them. e approach works quite well in practice. a related problem is when one has two embedding matrices one with a small vocabulary es rjvs and another one with a large vocabulary el that were trained separately and are hence incompatible. perhaps the smaller vocabulary matrix was trained using a more expensive algorithm as part of a larger and more complex network and the larger one was downloaded from the web. ere is some overlap in the vocabularies and one is interested in using word vectors from the larger matrix el for representing words that are not available in the smaller one es. one can then bridge the gap between the two embedding spaces using a linear projection et al. mikolov et al. e training objective is searching for a good projection matrix m that will map rows in el such that they are close to of course for this to work one needs to assume a linear relation between the two spaces. e linear projection method often works well in practice. using word embeddings corresponding rows in es by solving the following optimization problem argmin m x kel m es e learned matrix can then be used to project also the rows of el that do not have corresponding rows in es. is approach was successfully used by kiros et al. to increase the vocabulary size of an lstm-based sentence encoder sentence encoding model of kiros et al. is discussed in section another cute somewhat less robust application of the projection approach was taken by mikolov et al. who learned matrices to project between embedding vectors trained on language a english to embedding vectors trained on language b spanish based on a seed list of known word-word translation between the languages. practicalities and pitfalls while off-the-shelf pre-trained word embeddings can be downloaded and used it is advised to not just blindly download word embeddings and treat them as a black box. choices such as the source of the training corpus not necessarily its size larger is not always better and a smaller but cleaner or smaller but more domain-focused corpora are often more effective for a given use case the contexts that were used for defining the distributional similarities and many hyperparameters of the learning can greatly influence the results. in presence of an annotated test set for the similarity task one cares about it is best to experiment with several setting and choose the setup that works best on a development set. for discussion on the possible hyper-parameters and how they may affect the resulting similarities see the work of levy et al. when using off-the-shelf embedding vectors it is better to use the same tokenization and text normalization schemes that were used when deriving the corpus. finally the similarities induced by word vectors are based on distributional signals and therefore susceptible to all the limitations of distributional similarity methods described in section one should be aware of these limitations when using word vectors. c h a p t e r case study a feed-forward architecture for sentence meaning inference in section we introduced the sum of pairwise word similarities as a strong baseline for the short document similarity task. given two sentences the first one with words and the second one with words each word is associated with a corresponding pre-trained word vector and the similarity between the documents is given by i j while this is a strong baseline it is also completely unsupervised. here we show how a document similarity score can be greatly improved if we have a source of training data. we will follow the network presented by parikh et al. for the stanford natural language inference semantic inference task. other than providing a strong model for the snli task this model also demonstrates how the basic network components described so far can be combined in various layers resulting in a complex and powerful network that is trained jointly for a task. natural language inference and the snli dataset in the natural language inference task also called recognizing textual entailment you are given two texts and and need to decide if entails is can you infer from contradicts it cannot both be true or if the texts are neutral second one neither entails nor contradicts the first. example sentences for the different conditions are given in table e entailment task was introduced by dagan and glickman and subsequently established through a series of benchmarks known as the pascal rte challenges et al. e task is very challenging and solving it perfectly entails human level understanding of e snli dataset described here focuses on descriptions of scenes that appear in images and is easier than the general and un-restricted rte task which may require rather complex inference steps in order to solve. an example of an entailing pair in the un-restricted rte task is about two weeks before the trial started i was in shapiro s office in century city shapiro worked in century city. case study a feed-forward architecture for sentence e natural language inference entailment task. e examples are taken from the development set of the snli dataset. language. for in-depth discussion on the task and approaches to its solution that do not involve neural networks see the book by dagan roth sammons and zanzotto in this series et al. snli is a large dataset created by bowman et al. containing human-written sentence pairs each pair manually categorized as entailing contradicting or neutral. e sentences were created by presenting image captions to annotators and asking them without seeing the image to write a caption that is definitely a true description of the image a caption that is might be a true description of the image and a caption that is a definitely false description of the image after collecting sentence pairs this way of them were further validated by presenting sentence pairs to different annotators and asking them to categorize the pair into entailing neutral or contradicting. e validated sentences are then used for the test and validation sets. e examples in table are from the snli dataset. while simpler than the previous rte challenge datasets it is also much larger and still not trivial to solve particular for distinguishing the entailing from the neutral events. e snli dataset is a popular dataset for assessing meaning inference models. notice that the task goes beyond mere pairwise word similarity for example consider the second sentence in table the neutral sentence is much more similar terms of average word similarity to the original one than the entailed sentence. we need the ability to highlight some similarities degrade the strength of others and also to understand which kind of similarities are meaning preserving going from man to patient in the context of a surgery and which add new information going from patient to man. e network architecture is designed to facilitate this kind of reasoning. a textual similarity network e network will work in several stages. in the first stage our goal is to compute pairwise word similarities that are more suitable for the task. e similarity function for two word vectors is two men on bicycles competing in a race.entailpeople are riding bikes.neutralmen are riding bicycles on the street.contradicta few people are catching sh.two doctors perform surgery on patient.entaildoctors are performing surgery.neutraltwo doctors are performing surgery on a man.contradicttwo surgeons are having lunch. defined to be a textual similarity network d mlptransform mlptransform rds rdemb mlptransform at is we first transform each word vector by use of a trained nonlinear transformation and then take the dot-product of the transformed vectors. each word in sentence a can be similar to several words in sentence b and vice versa. for each word wa in sentence a we compute a b-dimensional vector of its similarities to words in sentence b normalized via softmax so that all similarities are positive and sum to one. is is called the alignment vector for the word i i d softmax.sim.wa i wb i wb b we similarly compute an alignment vector for each word in b for every word wa b that are aligned to wa i wb a wb i i d softmax.sim.wa i nc b a i nc i we compute a vector nwb i composed of a weighted-sum of the words in i and similarly for every word wb j bx ax nwb i d wb j i wa i i nwa j d a vector nwb i captures the weighted mixture of words in sentence b that are triggered by the ith word in sentence a. such weighted sum representations of a sequence of vectors where the weights are computed by a softmax over scores such as the one in equation are often referred to as an attention mechanism. e name comes from the fact that the weights reflect how important is each item in the target sequence to the given source item how much attention should be given to each of the items in the target sequence with respect to the source item. we will discuss attention in more details in chapter when discussing conditioned-generation models. i in sentence b is not necessarily relevant for the nli task. we attempt to transform each such pair into a representation i and the corresponding triggered mixture nwb e similarity between wa case study a feed-forward architecture for sentence i that focuses on the important information for the task. is is done using another feed vector va forward network va vb i d mlppair j d mlppair note that unlike the similarity function in equation that considered each term individually here the function can handle both terms differently. finally we sum the resulting vectors and pass them into a final mlp classifier for predicting the relation between the two sentences contradict or neutral i i nwb i ji nwa j i va i va dx vb dx oy d mlpdecide vb j j in the work of parikh et al. all the mlps have two hidden layers of size and a relu activation function. e entire process is captured in the same computation graph and the network is trained end-to-end using the cross-entropy loss. e pre-trained word embeddings themselves were not changed with the rest of the network relying on mlpt ransf orm to do the needed adaptation. as of the time of this writing this architecture is the best performing network on the snli dataset. to summarize the architecture the transform network learns a similarity function for wordlevel alignment. it transforms each word into a space that preserves important word-level similarities. after the transform network each word vector is similar to other words that are likely to refer to the same entity or the same event. e goal of this network is to find words that may contribute to the entailment. we get alignments in both directions from each word in a to multiple words in b and from each word in b to multiple words in a. e alignments are soft and are manifested by weighted group membership instead of by hard decisions so a word can participate in many pairs of similarities. is network is likely to put men and people next to each other men and two next to each other and man and patient next to each other and likewise for inflected forms perform and performing. e pair network then looks at each aligned pair group using a weighted-cbow representation and extracts information relevant to the pair. is this pair useful for the entailment prediction task? it also looks at sentence each component of the pair came from and will likely learn that patient and man are entailing in one direction and not the other. finally the decide network looks at the aggregated data from the word pairs and comes up with a decision based on all the evidence. we have three stages of reasoning first one recovers weak local evidence in terms of similarity alignment the second one looks at weighted multi-word units and also adds directionality and the third integrates all the local evidence into a global decision. a textual similarity network e details of the network are tuned for this particular task and dataset and it is not clear if they will generalize to other settings. e idea of this chapter was not to introduce a specific network architecture but rather to demonstrate that complex architectures can be designed and that it is sometimes worth the effort to do so. a new component introduced in this chapter that is worth noting is the use of the soft alignment weights i sometimes called attention in i we will encounter this idea order to compute a weighted sum of elements nwb again when discussing attention-based conditioned generation with rnns in chapter part iii specialized architectures in the previous chapters we ve discussed supervised learning and feed-forward neural networks and how they can be applied to language tasks. e feed-forward neural networks are for the most part general-purpose classification architectures nothing in them is tailored specifically for language data or sequences. indeed we mostly structured the language tasks to fit into the mlp framework. in the following chapters we will explore some neural architectures that are more specialized for dealing with language data. in particular we will discuss convolutional-and-pooling architectures and recurrent neural networks cnns are neural architectures that are specialized at identifying informative ngrams and gappy-ngrams in a sequence of text regardless of their position but while taking local ordering patterns into account. rnns are neural architectures that are designed to capture subtle patterns and regularities in sequences and that allow modeling non-markovian dependencies looking at infinite windows around a focus word while zooming-in on informative sequential patterns in that window. finally we will discuss sequence-generation models and conditioned generation. feature extraction e cnn and rnn architectures explored in this part of the book are primarily used as feature extractors. a cnn or an rnn network are not a standalone component but rather each such network produces a vector a sequence of vectors that are then fed into further parts of the network that will eventually lead to predictions. e network is trained endto-end predicting part and the convolutionalrecurrent architectures are trained jointly such that the vectors resulting from the convolutional or recurrent part of the network will capture the aspects of the input that are useful for the given prediction task. in the following chapters we introduce feature extractors that are based on the cnn and the rnn architectures. as the time of this writing rnn-based feature extractors are more established than cnns as feature extractors for text-based applications. however the different architectures have different strengths and weaknesses and the balance between them may shift in the future. both are worth knowing and hybrid approaches are also likely to become popular. chapters and discuss the integration of rnn-based feature extractors in different nlp prediction and generation architectures. large parts of the discussion in these chapters are applicable also to convolutional networks. cnns and rnns as lego bricks when learning about the cnn and rnn architectures it is useful to think about them as lego bricks that can be mixed and matched to create a desired structure and to achieve a desired behavior. is lego-bricks-like mixing-and-matching is facilitated by the computation-graph mechanism and gradient-based optimization. it allows treating network architectures such as mlps cnns and rnns as components or blocks that can be mixed and matched to create larger and larger structures one just needs to make sure that that input and output dimensions of the different components match and the computation graph and gradient-based training will take care of the rest. is allows us to create large and elaborate network structures with multiple layers of mlps cnns and rnns feeding into each other and training the entire network in an endto-end fashion. several examples are explored in later chapters but many others are possible and different tasks may benefit from different architectures. when learning about a new architecture don t think which existing component does it replace? or how do i use it to solve a task? but rather how can i integrate it into my arsenal of building blocks and combine it with the other components in order to achieve a desired result? c h a p t e r ngram detectors convolutional neural networks sometimes we are interested in making predictions based on ordered sets of items the sequence of words in a sentence the sequence of sentences in a document and so on. consider for example predicting the sentiment negative or neutral of sentences such as the following. part of the charm of satin rouge is that it avoids the obvious with humor and lightness. still this flick is fun and host to some truly excellent sequences. some of the sentence words are very informative of the sentiment fun excellent other words are less informative host flick lightness obvious avoids and to a good approximation an informative clue is informative regardless of its position in the sentence. we would like to feed all of the sentence words into a learner and let the training process figure out the important clues. one possible solution is feeding a cbow representation into a fully connected network such as an mlp. however a downside of the cbow approach is that it ignores the ordering information completely assigning the sentences it was not good it was actually quite bad and it was not bad it was actually quite good the exact same representation. while the global positions of the indicators not good and not bad do not matter for the classification task the local ordering of the words the word not appears right before the word bad is very important. similarly in the corpus-based example montias pumps a lot of energy into his nuanced narative and surrounds himself with a cast of quirky but not stereotyped street characters there is a big difference between not stereotyped indicator and not nuanced indicator. while the examples above are simple cases of negation some patterns are not as obvious e.g. avoids the obvious vs. obvious or vs. avoids the charm in the first example. in short looking at ngrams is much more informative than looking at a bag-of-words. a naive approach would suggest embedding word-pairs or word-triplets rather than words and building a cbow over the embedded ngrams. while such an architecture is indeed quite effective it will result huge embedding matrices will not scale for longer ngrams and will suffer from data sparsity problems as it does not share statistical strength between different ngrams embedding of quite good and very good are completely independent of one another so if the learner saw only one of them during training it will not be able to deduce anything about the other based on its component words. ngram detectors convolutional neural networks e cnn architecture is chapter introduces the convolution-and-pooling called convolutional neural networks or cnns architecture which is tailored to this modeling problem. a convolutional neural network is designed to identify indicative local predictors in a large structure and to combine them to produce a fixed size vector representation of the structure capturing the local aspects that are most informative for the prediction task at hand. i.e. the convolutional architecture will identify ngrams that are predictive for the task at hand without the need to pre-specify an embedding vector for each possible ngram. section we discuss an alternative method that allows working with unbounded ngram vocabularies while keeping a bounded size embedding matrix. e convolutional architecture also allows to share predictive behavior between ngrams that share similar components even if the exact ngram was never seen at test time. e convolutional architecture could be expanded into a hierarchy of convolution layers each one effectively looking at a longer range of ngrams in the sentence. is also allows the model to be sensitive to some non-contiguous ngrams. is is discussed in section as discussed in the opening section of this part of the book the cnn is in essence a feature-extracting architecture. it does not constitute a standalone useful network on its own but rather is meant to be integrated into a larger network and to be trained to work in tandem with it in order to produce an end result. e cnn layer s responsibility is to extract meaningful sub-structures that are useful for the overall prediction task at hand. history and terminology convolution-and-pooling architectures and bengio evolved in the neural networks vision community where they showed great success as object detectors recognizing an object from a predefined category cat bicycles regardless of its position in the image et al. when applied to images the architecture is using convolutions. when applied to text we are mainly concerned with convolutions. convolutional networks were introduced to the nlp community in the pioneering work of collobert et al. who used them for semantic-role labeling and later by kalchbrenner et al. and kim who used them for sentiment and question-type classification. because of their origins in the computer-vision community a lot of the terminology around convolutional neural networks is borrowed from computer vision and signal processing including terms such as filter channel and receptive-field which are often used also in the context of text processing. we will mention these terms when introducing the corresponding concepts. basic convolution pooling e main idea behind a convolution and pooling architecture for language tasks is to apply a nonlinear function over each instantiation of a k-word sliding window over the sentence. is function called filter transforms a window of k words into a scalar value. several such filters can be applied resulting in dimensional vector dimension corresponding to a filter e window-size k is sometimes referred to as the receptive field of the convolution. basic convolution pooling that captures important properties of the words in the window. en a pooling operation is used to combine the vectors resulting from the different windows into a single vector by taking the max or the average value observed in each of the dimensions over the different windows. e intention is to focus on the most important features in the sentence regardless of their location each filter extracts a different indicator from the window and the pooling operation zooms in on the important indicators. e resulting vector is then fed further into a network that is used for prediction. e gradients that are propagated back from the network s loss during the training process are used to tune the parameters of the filter function to highlight the aspects of the data that are important for the task the network is trained for. intuitively when the sliding window of size k is run over a sequence the filter function learns to identify informative kgrams. figure illustrates an application of a convolution-and-pooling network over a sentence. convolutions over text we begin by focusing on the one-dimensional convolution operation. e next section will focus on pooling. consider a sequence of words d wn each with their corresponding demb dimensional word embedding e d wi. a convolution of width-k works by moving a sliding-window of size k over the sentence and applying the same filter to each window in the sequence where a filter is a dot-product with a weight vector u which is often followed by a nonlinear activation function. define the operator to be the concatenation of the vectors wi e concatenated vector of the ith window is then xi d d xi we then apply the filter to each window-vector resulting scalar values pi pi dg.xi u xi d pi r xi u where g is a nonlinear activation. and a bias vector b is often added it is customary to use different filters u which can be arranged into a matrix u pi d g.xi u c b pi r xi u b r each vector pi is a collection of values that represent summarize the ith window. ideally each dimension captures a different kind of indicative information. here refers to a convolution operating over inputs such as sequences as opposed to convolutions which are applied to images. ngram detectors convolutional neural networks narrow vs. wide convolutions how many vectors pi do we have? for a sentence of length n with a window of size k there are n k c positions in which to start the sequence and we get n k c vectors is is called a narrow convolution. an alternative is to pad the sentence with k padding-words to each side resulting in n c k c vectors is is called a wide convolution et al. we use m to denote the number of resulting vectors. in our description of convolutions over a sequence an alternative formulation of convolutions of n items each item is associated with a d-dimensional vector and the vector are concatenated into a large d n sentence vector. e convolution network with a window of size k and output values is then based on a k d matrix. is matrix is applied to segments of the d n sentence matrix that correspond to k-word windows. each such multiplication results in values. each of these k values can be thought of as the result of a dot product between a k d vector row in the matrix and a sentence segment. another formulation that is often used in the literature is one in which the n vectors are stacked on top of each other resulting in an n d sentence matrix. e convolution operation is then performed by sliding different k d matrices kernels or filters over the sentence matrix and performing a matrix convolution between each kernel and the corresponding sentence-matrix segment. e matrix convolution operation between two matrices is defined as performing element-wise multiplication of the two matrices and summing the results. each of the sentence-kernel convolution operations produces a single value for a total of values. it is easy to convince oneself that the two approaches are indeed equivalent by observing that each kernel corresponds to a row in the k d matrix and the convolution with a kernel corresponds to a dot-product with a matrix row. figure show narrow and wide convolutions in the two notations. figure e inputs and outputs of a narrow and a wide convolution in the vector-concatenation and the vector-stacking notations. a narrow convolution with a window of size k d and output d in the vector-concatenation notation. a wide convolution with a window of size k d a output d in the vector-stacking notation. thethe actualactual serviceservice waswas notnot veryvery goodgood theactualservicewasnotverygoodpadthe actual service was not very goodthe actualactual serviceservice waswas notnot veryvery goodab basic convolution pooling channels in computer vision a picture is represented as a collection of pixels each representing the color intensity of a particular point. when using an rgb color scheme each pixel is a combination of three intensity values one for each of the red green and blue components. ese are then stored in three different matrices. each matrix provides a different view of the image and is referred to as a channel. when applying a convolution to an image in computer vision it is common to apply a different set of filters to each channel and then combine the three resulting vectors into a single vector. taking the different-views-of-the-data metaphor we can have multiple channels in text processing as well. for example one channel will be the sequence of words while another channel is the sequence of corresponding pos tags. applying the and applying it over the pos-tags will result lution over the words will result in m vectors pw in m vectors pt i c pt or by concatenation pi d to summarize e main idea behind the convolution layer is to apply the same parameterized function over all k-grams in the sequence. is creates a sequence of m vectors each representing a particular k-gram in the sequence. e representation is sensitive to the identity and order of the words within a k-gram but the same representation will be extracted for a k-gram regardless of its position within the sequence. ese two views can then be combined either by summation pi d pw i i i pt i vector pooling applying the convolution over the text results in m vectors each pi r ese vectors are then combined into a single vector c r representing the entire sequence. ideally the vector c will capture the essence of the important information in the sequence. e nature of the important information that needs to be encoded in the vector c is task dependent. for example if we are performing sentiment classification the essence are informative ngrams that indicate sentiment and if we are performing topic-classification the essence are informative ngrams that indicate a particular topic. during training the vector c is fed into downstream network layers an mlp culminating in an output layer which is used for prediction. e training procedure of the network calculates the loss with respect to the prediction task and the error gradients are propagated all the way back through the pooling and convolution layers as well as the embedding layers. e training process tunes the convolution matrix u the bias vector b the downstream network and potentially also the embeddings matrix e such that the vector c resulting from the convolution and pooling process indeed encodes information relevant to the task at hand. e input to the downstream network can be either c itself or a combination of c and other vectors. besides being useful for prediction a by-product of the training procedure is a set of parameters w b and embeddings e that can be used in a convolution and pooling architecture to encode arbitrary length sentences into fixed-size vectors such that sentences that share the same kind of predictive information will be close to each other. ngram detectors convolutional neural networks max-pooling e most common pooling operation is max pooling taking the maximum value across each dimension. d max pi pi denotes the jth component of pi. e effect of the max-pooling operation is to get the most salient information across window positions. ideally each dimension will specialize in a particular sort of predictors and max operation will pick on the most important predictor of each type. figure provides an illustration of the convolution and pooling process with a maxpooling operation. figure convolutionpooling over the sentence the quick brown fox jumped over the lazy dog. is is a narrow convolution padding is added to the sentence with a window size of each word is translated to a embedding vector shown. e embedding vectors are then concatenated resulting in window representations. each of the seven windows is transfered through a filter transformation followed by element-wise tanh resulting in seven filtered representations. en a max-pooling operation is applied taking the max over each dimension resulting in a final pooled vector. average pooling e second most common pooling type being average-pooling taking the average value of each index instead of the max mx c d m pi the quick brown fox jumped over the lazy dogthe quick brown quick brown fox brown fox jumped fox jumped over jumped over the over the lazy the lazy basic convolution pooling one view of average-pooling is that of taking a continuous bag-of-words of the k-gram representations resulting from the convolutions rather than from the sentence words. k-max pooling another variation introduced by kalchbrenner et al. is k-max pooling operation in which the top k values in each dimension are retained instead of only the best one while preserving the order in which they appeared in the text. for example consider the following matrix a pooling over the column vectors will result in while a pooling will result in the following matrix whose rows will then be concatenated to e k-max pooling operation makes it possible to pool the k most active indicators that may be a number of positions apart it preserves the order of the features but is insensitive to their specific positions. it can also discern more finely the number of times the feature is highly activated et al. dynamic pooling rather than performing a single pooling operation over the entire sequence we may want to retain some positional information based on our domain understanding of the prediction problem at hand. to this end we can split the vectors pi into r distinct groups apply the pooling separately on each group and then concatenate the r resulting vectors cr. e division of the pis into groups is performed based on domain knowledge. for example we may conjecture that words appearing early in the sentence are more indicative than words appearing late. we can then split the sequence into r equally sized regions applying a separate max-pooling to each region. for example johnson and zhang found that when classifying documentsintotopicsit isusefulto the initial sentences the topic is usually introduced from later ones while for a sentiment classification task a single max-pooling operation over the entire sentence was optimal that one or two very strong signals are enough to determine the sentiment regardless of the position in the sentence. similarly in a relation extraction kind of task we may be given two words and asked to determine the relation between them. we could argue that the words before the first word the words after the second word and the words between them provide three different kinds of infor in this chapter we use k to denote the window-size of the convolution. e k in k-max pooling is a different and unrelated value. we use the letter k for consistency with the literature. ngram detectors convolutional neural networks mation et al. we can thus split the pi vectors accordingly pooling separately the windows resulting from each group. variations rather than a single convolutional layer several convolutional layers may be applied in parallel. for example we may have four different convolutional layers each with a different window size in the range capturing k-gram sequences of varying lengths. e result of each convolutional layer will then be pooled and the resulting vectors concatenated and fed to further processing e convolutional architecture need not be restricted into the linear ordering of a sentence. for example ma et al. generalize the convolution operation to work over syntactic dependency trees. ere each window is around a node in the syntactic tree and the pooling is performed over the different nodes. similarly liu et al. apply a convolutional architecture on top of dependency paths extracted from dependency trees. le and zuidema propose performing max pooling over vectors representing the different derivations leading to the same chart item in a chart parser. alternative feature hashing convolutional networks for text work as very effective feature detectors for consecutive k-grams. however they require many matrix multiplications resulting in non-negligible computation. a more time-efficient alternative would be to just use k-gram embeddings directly and then pool the k-grams using average pooling in a continuous-bag-of-ngrams representations or max pooling. a downside of the approach is that it requires allocating a dedicated embedding vector for each possible k-gram which can be prohibitive in terms of memory as the number of k-grams in the training corpus can be very large. a solution to the problems is the use of the feature hashing technique that originated in linear models and dredze shi et al. weinberger et al. and recently adopted to neural networks et al. e idea behind feature hashing is that we don t pre-compute vocabulary-to-index mapping. instead we allocate an embedding matrix e with n rows should be sufficiently large but not prohibitive say in the millions or tens of millions. when a k-gram is seen in training we assign it to a row in e by applying a hash function h that will deterministically map it into a number in the range n i d h.k-gram n we then use the corresponding row e as the embedding vector. every k-gram will be dynamically assigned a row index this way without the need to store an explicit kgram-to-index mapping or to dedicate an embedding vector to each k-gram. some k-grams may share the same embedding vector due to hash collisions with the space of possible k-grams being much larger than the number of allocated embedding vectors such collisions are bound to happen but as most kgrams are not informative for the task the collisions will be smoothed out by the training process. if one wants to be more careful several distinct hash functions hr can be used and each k-gram represented as the sum of the rows corresponding to its hashes hierarchical convolutions e is way if an informative k-gram happens to collide with another informative k-gram using one hash it still likely to have a non-colliding representation from one of the other hashes. is hashing trick called hash kernel works very well in practice resulting in very efficient bag-of-ngrams models. it is recommended as a go-to baseline before considering more complex approaches or architectures. hierarchical convolutions e convolution approach described so far can be thought of as an ngram detector. a convolution layer with a window of size k is learning to identify indicative k-grams in the input. e approach can be extended into a hierarchy of convolutional layers in which a sequence of convolution layers are applied one after the other. let convk be the result of applying a convolution with window size k and parameters to each k-size window in the sequence u dconvk k c pi u c b narrow convolution n c k c wide convolution m d we can now have a succession of r convolutional layers that feed into each other as follows u u pr dconvkr u r capture increasingly larger effective windows receptive-fields of the e resulting vectors pr sentence. for r layers with a window of size k each vector pr i will be sensitive to a window i can be sensitive to gappy-ngrams of k c r of r.k c words. moreover the vector pr plot words potentially capturing patterns such as not where stands for a short sequence of words as well more specialized patterns where the gaps to see why consider that the first convolution layer transforms each sequence of k neighboring word-vectors into vectors representing k-grams. en the second convolution layer will combine each k consecutive k-gram-vectors into vectors that capture a window of k c words and so on until the rth convolution will capture k c d r.k c words. good or obvious predictable ngram detectors convolutional neural networks can be further specialized a sequence of words that do not contain not or a sequence of words that are adverb-like figure shows a two-layer hierarchical convolution with k d figure two-layer hierarchical convolution with strides dilation and pooling so far the convolution operation is applied to each k-word window in the sequence i.e. windows starting at indices is is said to have a stride of size larger strides are also possible i.e. with a stride of size the convolution operation will be applied to windows starting at indices more generally we define convks as dconvks pi u c b u where s is the stride size. e result will be a shorter output sequence from the convolutional layer. in a dilated convolution architecture et al. yu and koltun the hierarchy of convolution layers each has a stride size of k is allows an exponential growth in the effective window size as a function of the number of layers. figure shows convolution layers with different stride lengths. figure shows a dilated convolution architecture. an alternative to the dilation approach is to keep the stride-size fixed at but shorten the sequence length between each layer by applying local pooling i.e consecutive of vectors to see why consider a sequence of two convolution layer each with a window of size over the sequence funny and appealing. e first convolution layer will encode funny and and and appealing as vectors and may choose to retain the equivalent of appealing in the resulting vectors. e second convolution layer can then combine these into funny funny and appealing. appealing funny or the actual servicethe actual actual serviceservice waswas notnot veryvery goodactual service wasservice was notwas not verynot very goodthe actual service was not very good hierarchical convolutions figure strides. c convolution layer with and stride sizes can be converted into a single vector using max pooling or averaged pooling. even if we pool just every two neighboring vectors each convolutional-and-pooling layer in the hierarchy will halve the length of the sequence. similar to the dilation approach we again gain an exponential decrease in sequence length as a function of the number of layers. parameter tying and skip-connections another variation that can be applied to the hierarchical convolution architecture is performing parameter-tying using the same set of parameters u b in all the parameter layers. is results in more parameter sharing as well as allowing to use an unbounded number of convolution layers all the convolution layers share the same parameters the number of convolution layers need not be set in advance which in turn allows to reduce arbitrary length sequences into a single vector by using a sequence of narrow convolutions each resulting in a shorter sequence of vectors. when using deep architectures skip-connections are sometimes useful these work by feeding into the ith layer not only the vectors resulting from the i layer but also vectors from k s s s ngram detectors convolutional neural networks figure ree-layer dilated hierarchical convolution with previous layers which are combined to the vectors of the i layer using either concatenation averaging or summation. further reading e use of hierarchical and dilated convolution and pooling architectures is very common in the computer-vision community where various deep architectures comprising of arrangements of many convolutions and pooling layers with different strides have been proposed resulting in very strong image classification and object recognition results et al. krizhevsky et al. simonyan and zisserman e use of such deep architectures for nlp is still more preliminary. zhang et al. provide initial experiments with text classification with hierarchical convolutions over characters and conneau et al. provide further results this time with very deep convolutional networks. e work of strubell et al. provides a good overview of hierarchical and dilated architectures for a sequence labeling task. kalchbrenner et al. use dilated convolutions as encoders in an encoder-decoder architecture for machine translation. e hierarchy of convolutions with local pooling approach is used by xiao and cho who apply it to a sequence of character in a documentclassification task and then feed the resulting vectors into a recurrent neural network. we return to this example in section after discussing recurrent-neural-networks. c h a p t e r recurrent neural networks modeling sequences and stacks when dealing with language data it is very common to work with sequences such as words of letters sentences of words and documents. we saw how feed-forward networks can accommodate arbitrary feature functions over sequences through the use of vector concatenation and vector addition in particular the cbow representations allows to encode arbitrary length sequences as fixed sized vectors. however the cbow representation is quite limited and forces one to disregard the order of features. e convolutional networks also allow encoding a sequence into a fixed size vector. while representations derived from convolutional networks are an improvement over the cbow representation as they offer some sensitivity to word order their order sensitivity is restricted to mostly local patterns and disregards the order of patterns that are far apart in the sequence. recurrent neural networks allow representing arbitrarily sized sequential inputs in fixed-size vectors while paying attention to the structured properties of the inputs. rnns particularly ones with gated architectures such as the lstm and the gru are very powerful at capturing statistical regularities in sequential inputs. ey are arguably the strongest contribution of deep-learning to the statistical natural-language processing tool-set. is chapter describes rnns as an abstraction an interface for translating a sequence of inputs into a fixed sized output that can then be plugged as components in larger networks. various architectures that use rnns as a component are discussed. in the next chapter we deal with concrete instantiations of the rnn abstraction and describe the elman rnn called simple rnn the long-short-term memory and the gated recurrent unit en in chapter we consider examples of modeling nlp problems using with rnns. in chapter we discussed language modeling and the markov assumption. rnns allow for language models that do not make the markov assumption and condition the next word on the entire sentence history the words preceding it. is ability opens the way to conditioned generation models where a language model that is used as a generator is conditioned on some other signal such as a sentence in another language. such models are described in more depth in chapter however as discussed in section hierarchical and dilated convolutional architectures do have the potential of capturing relatively long-range dependencies within a sequence. yn d xi rdin yn rdout d yi d xi rdin yi rdout recurrent neural networks modeling sequences and stacks the rnn abstraction we use xiwj to denote the sequence of vectors xi xj. on a high-level the rnn is a function that takes as input an arbitrary length ordered sequence of n din-dimensional vectors d xn rdin and returns as output a single dout dimensional vector yn rdout is implicitly defines an output vector yi for each prefix of the sequence we denote by rnn? the function returning this sequence e output vector yn is then used for further prediction. for example a model for predicting the conditional probability of an event e given the sequence can be defined as p.e d d w c the jth element in the output vector resulting from the softmax operation over a linear transformation of the rnn encoding yn d e rnn function provides a framework for conditioning on the entire history xi without resorting to the markov assumption which is traditionally used for modeling sequences described in chapter indeed rnn-based language models result in very good perplexity scores when compared to ngram-based models. looking in a bit more detail the rnn is defined recursively by means of a function r taking as input a state vector and an input vector xi and returning a new state vector si. e state vector si is then mapped to an output vector yi using a simple deterministic function e base of the recursion is an initial state vector which is also an input to the rnn. for brevity we often omit the initial vector or assume it is the zero vector. when constructing an rnn much like when constructing a feed-forward network one has to specify the dimension of the inputs xi as well as the dimensions of the outputs yi. e dimensions of the states si are a function of the output dimension. using the o function is somewhat non-standard and is introduced in order to unify the different rnn models to to be presented in the next chapter. for the simple rnn rnn and the gru architectures o is the identity mapping and for the lstm architecture o selects a fixed subset of the state. while rnn architectures in which the state dimension is independent of the output dimension are possible the current popular architectures including the simple rnn the lstm and the gru do not follow this flexibility. the rnn abstraction yi do.si si xi xi rdin yi rdout si rf e functions r and o are the same across the sequence positions but the rnn keeps track of the states of computation through the state vector si that is kept and being passed across invocations of r. graphically the rnn has been traditionally presented as in figure figure graphical representation of an rnn is presentation follows the recursive definition and is correct for arbitrarily long sequences. however for a finite sized input sequence all input sequences we deal with are finite one can unroll the recursion resulting in the structure in figure while not usually shown in the visualization we include here the parameters in order to highlight the fact that the same parameters are shared across all time steps. different instantiations of r and o will result in different network structures and will exhibit different properties in terms of their running times and their ability to be trained effectively using gradient-based methods. however they all adhere to the same abstract interface. we will provide details of concrete instantiations of r and o the simple rnn the lstm and the gru in chapter before that let s consider working with the rnn abstraction. yir recurrent neural networks modeling sequences and stacks figure graphical representation of an rnn first we note that the value of si hence yi is based on the entire input xi. for example by expanding the recursion for i d we get dr. dr.r. dr.r.r. us sn and yn can be thought of as encoding the entire input sequence. is the encoding useful? is depends on our definition of usefulness. e job of the network training is to set the parameters of r and o such that the state conveys useful information for the task we are tying to solve. rnn training viewed as in figure it is easy to see that an unrolled rnn is just a very deep neural network rather a very large computation graph with somewhat complex nodes in which the same parameters are shared across many parts of the computation and additional input is added at various layers. to train an rnn network then all we need to do is to create the unrolled computation graph for a given input sequence add a loss node to the unrolled graph and then use the backward note that unless r is specifically designed against this it is likely that the later elements of the input sequence have stronger effect on sn than earlier ones. common rnn usage-patterns algorithm to compute the gradients with respect to that loss. is procedure is referred to in the rnn literature as backpropagation through time what is the objective of the training? it is important to understand that the rnn does not do much on its own but serves as a trainable component in a larger network. e final prediction and loss computation are performed by that larger network and the error is back-propagated through the rnn. is way the rnn learns to encode properties of the input sequences that are useful for the further prediction task. e supervision signal is not applied to the rnn directly but through the larger network. some common architectures of integrating the rnn within larger networks are given below. common rnn usage-patterns acceptor one option is to base the supervision signal only at the final output vector yn. viewed this way the rnn is trained as an acceptor. we observe the final state and then decide on an outcome. for example consider training an rnn to read the characters of a word one by one and then use the final state to predict the part-of-speech of that word is inspired by ling et al. an rnn that reads in a sentence and based on the final state decides if it conveys positive or negative sentiment is inspired by wang et al. or an rnn that reads in a sequence of words and decides whether it is a valid noun-phrase. e loss in such cases is defined in terms of a function of yn d o.sn. typically the rnn s output vector yn is fed into a fully connected layer or an mlp which produce a prediction. e error gradients are then backpropagated through the rest of the sequence figure e loss can take any familiar form cross entropy hinge margin etc. encoder similar to the acceptor case an encoder supervision uses only the final output vector yn. however unlike the acceptor where a prediction is made solely on the basis of the final vector here the variants of the bptt algorithm include unrolling the rnn only for a fixed number of input symbols at each time first unroll the rnn for inputs resulting in compute a loss and backpropagate the error through the network steps back. en unroll the inputs this time using sk as the initial state and again backpropagate the error for k steps and so on. is strategy is based on the observations that for the simple rnn variant the gradients after k steps tend to vanish large enough k and so omitting them is negligible. is procedure allows training of arbitrarily long sequences. for rnn variants such as the lstm or the gru that are designed specifically to mitigate the vanishing gradients problem this fixed size unrolling is less motivated yet it is still being used for example when doing language modeling over a book without breaking it into sentences. a similar variant unrolls the network for the entire sequence in the forward step but only propagates the gradients back for k steps from each position. e terminology is borrowed from finite-state acceptors. however the rnn has a potentially infinite number of states making it necessary to rely on a function other than a lookup table for mapping states to decisions. is kind of supervision signal may be hard to train for long sequences especially so with the simple rnn because of the vanishing gradients problem. it is also a generally hard learning task as we do not tell the process on which parts of the input to focus. yet it does work very well in many cases. recurrent neural networks modeling sequences and stacks figure acceptor rnn training graph. final vector is treated as an encoding of the information in the sequence and is used as additional information together with other signals. for example an extractive document summarization system may first run over the document with an rnn resulting in a vector yn summarizing the entire document. en yn will be used together with other features in order to select the sentences to be included in the summarization. transducer another option is to treat the rnn as a transducer producing an output oti for each input it reads in. modeled this way we can compute a local loss signal llocal. oti ti for each of the outpn puts oti based on a true label ti. e loss for unrolled sequence will then be d llocal.oti ti or using another combination rather than a sum such as an average or a weighted average figure one example for such a transducer is a sequence tagger in which we take xiwn to be feature representations for the n words of a sentence and ti as an input for predicting the tag assignment of word i based on words a ccg super-tagger based on such an architecture provides very strong ccg super-tagging results et al. although in many cases a transducer based on a bi-directional rnn see section below is a better fit for such tagging problems. a very natural use-case of the transduction setup is for language modeling in which the sequence of words is used to predict a distribution over the c word. rnn-based language models are shown to provide vastly better perplexities than traditional language models et al. mikolov mikolov et al. sundermeyer et al. using rnns as transducers allows us to relax the markov assumption that is traditionally taken in language models and hmm taggers and condition on the entire prediction history. andcalculate lossloss bidirectional rnns figure transducer rnn training graph. special cases of the rnn transducer is the rnn generator and the related conditionedgeneration called encoder-decoder and the conditioned-generation with attention architectures. ese will be discussed in chapter bidirectional rnns a useful elaboration of an rnn is a bidirectional-rnn commonly referred to as birnn schuster and paliwal consider the task of sequence tagging over a sentence xn. an rnn allows us to compute a function of the ith word xi based on the past the words up to and including it. however the following words may also be useful for prediction as is evident by the common sliding-window approach in which the focus word is categorized based on a window of k words surrounding it. much like the rnn relaxes the markov assumption and allows looking arbitrarily back into the past the birnn relaxes the fixed window size assumption allowing to look arbitrarily far at both the past and the future within the sequence. consider an input sequence e birnn works by maintaining two separate states is based on xi while the i and sb sf backward state sb i is based on xn xi. e forward and backward states are generated by two different rnns. e first rnn o f is fed the input sequence as is while the second rnn o b is fed the input sequence in reverse. e state representation si is when used with a specific rnn architecture such as an lstm the model is called bilstm. i for each input position i. e forward state sf i andcalculate losspredict andcalculate losspredict andcalculate losspredict andcalculate losspredict andcalculate losslosssum recurrent neural networks modeling sequences and stacks then composed of both the forward and backward states. e output at position i is based on the concatenation of the two output vectors yi d f i taking into account both the past and the future. in other words yi the birnn encoding of the ith word in a sequence is the concatenation of two rnns one reading the sequence from the beginning and the other reading it from the end. we define i to be the output vector corresponding to the ith sequence position i d f i o b.sb i i y b i d yi d e vector yi can then be used directly for prediction or fed as part of the input to a more complex network. while the two rnns are run independently of each other the error gradients at position i will flow both forward and backward through the two rnns. feeding the vector yi through an mlp prior to prediction will further mix the forward and backward signals. visual representation of the birnn architecture is given in figure figure computing the birnn representation of the word jumped in the sentence the brown fox jumped over the dog. around including the focus vector xjumped. note how the vector y corresponding to the word jumped encodes an infinite window similarly to the rnn case we also define as the sequence of vectors d yiwn d n e birnn vector can either a simple concatenation of the two rnn vectors as in equation or followed by another linear-transformation to reduce its dimension often back to the dimension of the single rnn input i d yi d is is variant is often used when stacking several birnns on top of each other as discussed in section fo f r fo f r fo f r fo f rbob rbobrbob multi-layer rnns e n output vectors yiwn can be efficiently computed in linear time by first running the forward and backward rnns and then concatenating the relevant outputs. is architecture is depicted in figure figure computing the birnn? for the sentence the brown fox jumped. e birnn is very effective for tagging tasks in which each input vector corresponds to one output vector. it is also useful as a general-purpose trainable feature-extracting component that can be used whenever a window around a given word is required. concrete usage examples are given in chapter e use of birnns for sequence tagging was introduced to the nlp community by irsoy and cardie multi-layer rnns rnns can be stacked in layers forming a grid and bengio consider k rnns where the jth rnn has states sj e input for the first rnn are while the input of the jth rnn are the outputs of the rnn below it y such layered architectures are often called deep rnns. a visual representation of a three-layer rnn is given in figure birnns can be stacked in a similar fashion. e term deep-birnn is used in the literature to describe to different architecture in the first the birnn state is a concatenation of two deep rnns. in the second the output sequence of on birnn is fed as input to another. my research group found the second variant to often performs better. e output of the entire formation is the output of the last rnn y k and outputs y j xtherbob rbob rbob rbob rbob fo f r fo f r fo f r fo f r fo f recurrent neural networks modeling sequences and stacks figure a three-layer deep rnn architecture. while it is not theoretically clear what is the additional power gained by the deeper architecture it was observed empirically that deep rnns work better than shallower ones on some tasks. in particular sutskever et al. report that a four-layers deep architecture was crucial in achieving good machine-translation performance in an encoder-decoder framework. irsoy and cardie also report improved results from moving from a one-layer birnn to an architecture with several layers. many other works report result using layered rnn architectures but do not explicitly compare to one-layer rnns. in the experiment of my research group using two or more layers indeed often improves over using a single one. rnns for representing stacks some algorithms in language processing including those for transition-based parsing require performing feature extraction over a stack. instead of being confined to looking at the k top-most elements of the stack the rnn framework can be used to provide a fixed-sized vector encoding of the entire stack. e main intuition is that a stack is essentially a sequence and so the stack state can be represented by taking the stack elements and feeding them in order into an rnn resulting in a final encoding of the entire stack. in order to do this computation efficiently performing an o.n stack encoding operation each time the stack changes the rnn state is maintained together with the stack state. if the stack was push-only this would be trivial whenever a new rnns for representing stacks element x is pushed into the stack the corresponding vector x will be used together with the rnn state si in order to obtain a new state dealing with pop operation is more challenging but can be solved by using the persistent-stack data-structure et al. okasaki persistent or immutable data-structures keep old versions of themselves intact when modified. e persistent stack construction represents a stack as a pointer to the head of a linked list. an empty stack is the empty list. e push operation appends an element to the list returning the new head. e pop operation then returns the parent of the head but keeping the original list intact. from the point of view of someone who held a pointer to the previous head the stack did not change. a subsequent push operation will add a new child to the same node. applying this procedure throughout the lifetime of the stack results in a tree where the root is an empty stack and each path from a node to the root represents an intermediary stack state. figure provides an example of such a tree. e same process can be applied in the computation graph construction creating an rnn with a tree structure instead of a chain structure. backpropagating the error from a given node will then affect all the elements that participated in the stack when the node was created in order. figure shows the computation graph for the stack-rnn corresponding to the last state in figure is modeling approach was proposed independently by dyer et al. and watanabe and sumita for transition-based dependency parsing. figure an immutable stack construction for the sequence of operations push a push b push c pop push d pop pop push e push f. push push push push push push f recurrent neural networks modeling sequences and stacks figure e stack-rnn corresponding to the final state in figure a note on reading the literature unfortunately it is often the case that inferring the exact model form from reading its description in a research paper can be quite challenging. many aspects of the models are not yet standardized and different researchers use the same terms to refer to slightly different things. to list a few examples the inputs to the rnn can be either one-hot vectors which case the embedding matrix is internal to the rnn or embedded representations the input sequence can be padded with start-of-sequence andor end-of-sequence symbols or not while the output of an rnn is usually assumed to be a vector which is expected to be fed to additional layers followed by a softmax for prediction is the case in the presentation in this tutorial some papers assume the softmax to be part of the rnn itself in multi-layer rnn the state vector can be either the output of the top-most layer or a concatenation of the outputs from all layers when using the encoder-decoder framework conditioning on the output of the encoder can be interpreted in various different ways and so on. on top of that the lstm architecture described in the next osasaxbr osabsabsaexcr osabcyayabyabcxdr osabdxer oyabdsaefyaefyaexfr o a note on reading the literature section has many small variants which are all referred to under the common name lstm. some of these choices are made explicit in the papers other require careful reading and others still are not even mentioned or are hidden behind ambiguous figures or phrasing. as a reader be aware of these issues when reading and interpret model descriptions. as a writer be aware of these issues as well either fully specify your model in mathematical notation or refer to a different source in which the model is fully specified if such a source is available. if using the default implementation from a software package without knowing the details be explicit of that fact and specify the software package you use. in any case don t rely solely on figures or natural language text when describing your model as these are often ambiguous. c h a p t e r concrete recurrent neural network architectures after describing the rnn abstraction we are now in place to discuss specific instantiations of it. recall that we are interested in a recursive function si d r.xi such that si encodes the sequence we will present several concrete instantiations of the abstract rnn architecture providing concrete definitions of the functions r and o. ese include the simple rnn the long short-term memory and the gated recurrent unit cbow as an rnn on particularly simple choice of r is the addition function si dr d c xi yi do d si si yi rds xi rds following the definition in equation we get the continuous-bag-of-words model the state resulting from inputs is the sum of these inputs. while simple this instantiation of the rnn ignores the sequential nature of the data. e elman rnn described next adds dependence on the sequential ordering of the elements. simple rnn e simplest rnn formulation that is sensitive to the ordering of elements in the sequence is known as an elman network or simple-rnn e s-rnn was proposed by elman and explored for use in language modeling by mikolov e s-rnn takes the following form si dr d s c xi w x c b yi do d si si yi rds xi rdx w x w s b rds e view of the cbow representation as an rnn is not a common one in the literature. however we find it to be a good stepping stone into the elman rnn definition. it is also useful to have the simple cbow encoder in the same framework as the rnns as it can also serve the role of an encoder in a conditioned generation network such as those described in chapter concrete recurrent neural network architectures at is the state and the input xi are each linearly transformed the results are added with a bias term and then passed through a nonlinear activation function g tanh or relu. e output at position i is the same as the hidden state in that position. an equivalent way of writing equation is equation both are used in the literature si dr d xi c b yi do d si si yi rds xi rdx w r.dxcds b rds e s-rnn is only slightly more complex than the cbow with the major difference being the nonlinear activation function g. however this difference is a crucial one as adding the linear transformation followed by the nonlinearity makes the network sensitive to the order of the inputs. indeed the simple rnn provides strong results for sequence tagging et al. as well as language modeling. for comprehensive discussion on using simple rnns for language modeling see the ph.d. thesis by mikolov gated architectures e s-rnn is hard to train effectively because of the vanishing gradients problem et al. error signals in later steps in the sequence diminish quickly in the backpropagation process and do not reach earlier input signals making it hard for the s-rnn to capture long-range dependencies. gating-based architectures such as the lstm and schmidhuber and the gru et al. are designed to solve this deficiency. consider the rnn as a general purpose computing device where the state si represents a finite memory. each application of the function r reads in an input reads in the current memory si operates on them in some way and writes the result into memory resulting in a new memory state viewed this way an apparent problem with the s-rnn architecture is that the memory access is not controlled. at each step of the computation the entire memory state is read and the entire memory state is written. how does one provide more controlled memory access? consider a binary vector g such a vector can act as a gate for controlling access to n-dimensional vectors using the hadamard-product operation x g consider a memory s rd an input x rd and a gate g e computation g x c g reads the entries in x that correspond to the values in g and writes them to the new memory en locations that weren t some authors treat the output at position i as a more complicated function of the state e.g. a linear transformation or an mlp. in our presentation such further transformation of the output are not considered part of the rnn but as separate computations that are applied to the rnns output. e hadamard-product is a fancy name for element-wise multiplication of two vectors the hadamard product x d u v results in d gated architectures read to are copied from the memory s to the new memory through the use of the gate g. figure shows this process for updating the memory with positions and from the input. figure using binary gate vector g to control access to memory e gating mechanism described above can serve as a building block in our rnn gate vectors can be used to control access to the memory state si. however we are still missing two important related components the gates should not be static but be controlled by the current memory state and the input and their behavior should be learned. is introduced an obstacle as learning in our framework entails being differentiable of the backpropagation algorithm and the binary values used in the gates are not differentiable. a solution to the above problem is to approximate the hard gating mechanism with a soft but differentiable gating mechanism. to achieve these differentiable gates we replace the requirement that g and allow arbitrary real numbers rn which are then pass through a sigmoid function is bounds the value in the range with most values near the borders. when using the gate x indices in x corresponding to near-one values in are allowed to pass while those corresponding to near-zero values are blocked. e gate values can then be conditioned on the input and the current memory and trained using a gradient-based method to perform a desired behavior. is controllable gating mechanism is the basis of the lstm and the gru architectures to be defined next at each time step differentiable gating mechanisms decide which parts of the inputs will be written to memory and which parts of memory will be overwritten is rather abstract description will be made concrete in the next sections. lstm e long short-term memory architecture and schmidhuber was designed to solve the vanishing gradients problem and is the first to introduce the gating mechanism. e lstm architecture explicitly splits the state vector si into two halves where one half it is in principle possible to learn also models with non-differentiable components such as binary gates using reinforcementlearning techniques. however as the time of this writing such techniques are brittle to train. reinforcement learning techniques are beyond the scope of this book. gxs concrete recurrent neural network architectures is treated as memory cells and the other is working memory. e memory cells are designed to preserve the memory and also the error gradients across time and are controlled through differentiable gating components smooth mathematical functions that simulate logical gates. at each input state a gate is used to decide how much of the new input should be written to the memory cell and how much of the current content of the memory cell should be forgotten. mathematically the lstm architecture is defined as sj d r xj hj cj df c i z hj do tanh.cj i w xi c hi f w xf c hf o w xo c ho z dtanh.xj w xz c hz yj d o dhj sj xi rdx cj hj i f o z rdh w w e state at time j is composed of two vectors cj and hj where cj is the memory component and hj is the hidden state component. ere are three gates i f and o controlling for input forget and output. e gate values are computed based on linear combinations of the current input xj and the previous state passed through a sigmoid activation function. an update candidate z is computed as a linear combination of xj and passed through a tanh activation function. e memory cj is then updated the forget gate controls how much of the previous memory to keep and the input gate controls how much of the proposed update to keep z. finally the value of hj is also the output yj is determined based on the content of the memory cj passed through a tanh nonlinearity and controlled by the output gate. e gating mechanisms allow for gradients related to the memory part cj to stay high across very long time ranges. for further discussion on the lstm architecture see the ph.d. thesis by alex graves as well as chris olah s description. for an analysis of the behavior of an lstm when used as a character-level language model see karpathy et al. ere are many variants on the lstm architecture presented here. for example forget gates were not part of the original proposal in hochreiter and schmidhuber but are shown to be an important part of the architecture. other variants include peephole connections and gate-tying. for an overview and comprehensive empirical comparison of various lstm architectures see greff et al. gated architectures e vanishing gradients problem in recurrent neural networks and its solution intuitively recurrent neural networks can be thought of as very deep feed-forward networks with shared parameters across different layers. for the simple-rnn the gradients then include repeated multiplication of the matrix w making it very likely for the values to vanish or explode. e gating mechanism mitigate this problem to a large extent by getting rid of this repeated multiplication of a single matrix. for further discussion of the exploding and vanishing gradient problem in rnns see section in bengio et al. for further explanation of the motivation behind the gating mechanism in the lstm the gru and its relation to solving the vanishing gradient problem in recurrent neural networks see sections and in the detailed course notes of cho lstms are currently the most successful type of rnn architecture and they are responsible for many state-of-the-art sequence modeling results. e main competitor of the lstmrnn is the gru to be discussed next. practical considerations when training lstm networks jozefowicz et al. strongly recommend to always initialize the bias term of the forget gate to be close to one. gru e lstm architecture is very effective but also quite complicated. e complexity of the system makes it hard to analyze and also computationally expensive to work with. e gated recurrent unit was recently introduced by cho et al. as an alternative to the lstm. it was subsequently shown by chung et al. to perform comparably to the lstm on several textual datasets. like the lstm the gru is also based on a gating mechanism but with substantially fewer gates and without a separate memory component. sj d xj z c z qsj z w xz c sz r w xr c sr qsj dtanh.xj w xs c sg yj d ogru.sj dsj sj qsj rds xi rdx z r rds w w concrete recurrent neural network architectures one gate is used to control access to the previous state and compute a proposed update qsj. e updated state sj also serves as the output yj is then determined based on an interpolation of the previous state and the proposal qsj where the proportions of the interpolation are controlled using the gate z. e gru was shown to be effective in language modeling and machine translation. however the jury is still out between the gru the lstm and possible alternative rnn architectures and the subject is actively researched. for an empirical exploration of the gru and the lstm architectures see jozefowicz et al. other variants improvements to non-gated architectures e gated architectures of the lstm and the gru help in alleviating the vanishing gradients problem of the simple rnn and allow these rnns to capture dependencies that span long time ranges. some researchers explore simpler architectures than the lstm and the gru for achieving similar benefits. mikolov et al. observed that the matrix multiplication s coupled with the nonlinearity g in the update rule r of the simple rnn causes the state vector si to undergo large changes at each time step prohibiting it from remembering information over long time periods. ey propose to split the state vector si into a slow changing component ci context units and a fast changing component hi. e slow changing component ci is updated according to a linear interpolation of the input and the previous component ci d w c where is update allows ci to accumulate the previous inputs. e fast changing component hi is updated similarly to the simple rnn update rule but changed to take ci into account as well hi d w c h c ci w c. finally the output yi is the concatenation of the slow and the fast changing parts of the state yi d hi mikolov et al. demonstrate that this architecture provides competitive perplexities to the much more complex lstm on language modeling tasks. e approach of mikolov et al. can be interpreted as constraining the block of the matrix w s in the s-rnn corresponding to ci to be a multiple of the identity matrix mikolov et al. for the details. le et al. propose an even simpler approach set the activation function of the s-rnn to a relu and initialize the biases b as zeroes and the matrix w s as the identify matrix. is causes an untrained rnn to copy the previous state to the current state add the effect of the current input xi and set the negative values to zero. after setting this initial bias toward state copying the training procedure allows w s to change freely. le et al. demonstrate that this simple modification makes the s-rnn comparable to an lstm with the same number of parameters on several tasks including language modeling. e states s are often called h in the gru literature. we depart from the notation in mikolov et al. and reuse the symbols used in the lstm description. e update rule diverges from the s-rnn update rule also by fixing the nonlinearity to be a sigmoid function and by not using a bias term. however these changes are not discussed as central to the proposal. dropout in rnns beyond differential gates e gating mechanism is an example of adapting concepts from the theory of computation access logical gates into differentiable and hence gradienttrainable systems. ere is considerable research interest in creating neural network architectures to simulate and implement further computational mechanisms allowing better and more fine grained control. one such example is the work on a differentiable stack et al. in which a stack structure with push and pop operations is controlled using an end-to-end differentiable network and the neural turing machine et al. which allows read and write access to content-addressable memory again in a differentiable system. while these efforts are yet to result in robust and general-purpose architectures that can be used in non-toy language processing applications they are well worth keeping an eye on. dropout in rnns applying dropout to rnns can be a bit tricky as dropping different dimensions at different time steps harms the ability of the rnn to carry informative signals across time. is prompted pham et al. zaremba et al. to suggest applying dropout only on the non-recurrent connection i.e. only to apply it between layers in deep-rnns and not between sequence positions. more recently following a variational analysis of the rnn architecture gal suggests applying dropout to all the components of the rnn recurrent and non-recurrent but crucially retain the same dropout mask across time steps. at is the dropout masks are sampled once per sequence and not once per time step. figure contrasts this form of dropout variational rnn with the architecture proposed by pham et al. zaremba et al. e variational rnn dropout method of gal is the current best-practice for applying dropout in rnns. concrete recurrent neural network architectures figure gal s proposal for rnn dropout vs. the previous suggestion by pham et al. zaremba et al. figure from gal used with permission. each square represents an rnn unit with horizontal arrows representing time dependence connections. vertical arrows represent the input and output to each rnn unit. colored connections represent dropped-out inputs with different colors corresponding to different dropout masks. dashed lines correspond to standard connections with no dropout. previous techniques dropout left use different masks at different time steps with no dropout on the recurrent layers. gal s proposed technique rnn right uses the same dropout mask at each time step including the recurrent layers. yt naive dropout rnnb variational rnn c h a p t e r modeling with recurrent networks after enumerating common usage patterns in chapter and learning the details of concrete rnn architectures in chapter we now explore the use of rnns in nlp applications through some concrete examples. while we use the generic term rnn we usually mean gated architectures such as the lstm or the gru. e simple rnn consistently results in lower accuracies. acceptors e simplest use of rnns is as acceptors read in an input sequence and produce a binary or multi-class answer at the end. rnns are very strong sequence learners and can pick-up on very intricate patterns in the data. is power is often not needed for many natural language classification tasks the wordorder and sentence structure turn out to not be very important in many cases and bag-of-words or bag-of-ngrams classifier often works just as well or even better than rnn-acceptors. is section presents two examples of acceptor usages for language problems. e first is a canonical one sentiment classification. e approach works well but less powerful approaches can also prove competitive. e second is a somewhat contrived example it does not solve any useful task on its own but demonstrates the power of rnns and the kind of patterns they are capable of learning. sentiment classification sentence-level sentiment classification in the sentence-level sentiment classification task we are given a sentence often as part of a review and need to assign it one of two values p or n is is a somewhat simplistic view of the sentiment detection task but one which is often used nonetheless. is is also the task that motivated our discussion of convolutional neural networks in chapter an example of naturally occurring p and n sentences in the movie-reviews domain would be the following in a more challenging variant the goal is a three-way classification into p n and n ese examples are taken from the stanford sentiment treebank et al. modeling with recurrent networks p it s not life-affirming it s vulgar and mean but i liked it. n it s a disappointing that it only manages to be decent instead of dead brilliant. note that the positive example contains some negative phrases life affirming vulgar and mean while the negative examples contains some positive ones brilliant. correctly predicting the sentiment requires understanding not only the individual phrases but also the context in which they occur linguistic constructs such as negation and the overall structure of the sentence. sentiment classification is a tricky and challenging task and properly solving it involves handling such issues as sarcasm and metaphor. e definition of sentiment is also not straightforward. for a good overview of the challenges in sentiment classification and its definition see the comprehensive review by pang and lee for our current purpose however we will ignore the complexities in definition and treat it as a data-driven binary classification task. e task is straightforward to model using an rnn-acceptor after tokenization the rnn reads in the words of the sentence one at a time. e final rnn state is then fed into an mlp followed by a softmax-layer with two outputs. e network is trained with cross-entropy loss based on the gold sentiment labels. for a finer-grained classification task where one needs to assign a sentiment on scale of or star rating it is straightforward to change the mlp to produce outputs instead of to summarize the architecture p.label d k j d oy oy d d e e e word embeddings matrix e is initialized using pre-trained embeddings learned over a large external corpus using an algorithm such as w or g v with a relatively wide window. it is often helpful to extend the model in equation by considering two rnns one reading the sentence in its given order and the other one reading it in reverse. e end states of the two rnns are then concatenated and fed into the mlp for classification p.label d k j d oy oy d d e e ese bidirectional models produce strong results for the task et al. for longer sentences li et al. found it useful to use a hierarchical architecture in which the sentence is split into smaller spans based on punctuation. en each span is fed into a forward and a backward rnn as described in equation sequence of resulting vectors for each span are then fed into an rnn acceptor such as the one in equation formally given a sentence which is split into m spans p.label d k j d oy acceptors the architecture is given by wm m oy d zi d xi i d e i i e i i each of the m different spans may convey a different sentiment. e higher-level acceptor reads the summary produced by the lower level encoders and decides on the overall sentiment. sentiment classification is also used as a test-bed for hierarchical tree-structured recursive neural networks as described in chapter document level sentiment classification e document level sentiment classification is similar to the sentence level one but the input text is much longer consisting of multiple sentences and the supervision signal label is given only at the end not for the individual sentences. e task is harder than sentence-level classification as the individual sentences may convey different sentiments than the overall one conveyed by the document. tang et al. found it useful to use a hierarchical architecture for this task similar to the one used by li et al. each sentence si is encoded using a gated rnn producing a vector zi and the vectors are then fed into a second gated rnn producing a vector h d which is then used for prediction oy d softmax.mlp.h. e authors experimented also with a variant in which all the intermediate vectors from npn the document-level rnn are kept and their average is fed into the mlp d hi is produced slightly higher results in some cases. oy d softmax.mlp. subject-verb agreement grammaticality detection grammatical english sentences obey the constraint that the head of the subject of a present-tense verb must agree with in on the number inflection denote ungrammatical sentences a. e key is on the table. e key are on the table. b. e keys is on the table. c. d. e keys are on the table. is relationship is non-trivial to infer from the sequence alone as the two elements can be separated by arbitrary long sentential material which may include nouns of the opposite number modeling with recurrent networks a. e keys to the cabinet in the corner of the room are on the table. e keys to the cabinet in the corner of the room is on the table. b. given the difficulty in identifying the subject from the linear sequence of the sentence dependencies such as subject-verb agreement serve as an argument for structured syntactic representations in humans et al. indeed given a correct syntactic parse tree of the sentence the relation between the verb and its subject becomes trivial to extract in work with linzen and dupoux et al. we set out to find if rnns which are sequential learners can pick up on this rather syntactic regularity by learning from word sequences alone. we set up several prediction tasks based on naturally occurring text from wikipedia to test this. one of the tasks was grammaticality detection the rnn is tasked with reading a sentence and at the end deciding if it is grammatical or not. in our setup grammatical sentences were wikipedia sentences that contain a present-tense verb while ungrammatical ones are wikipedia sentences with a present-tense verb in which we picked up one of the present-tense verbs at random and flipped its form from singular to plural or the other way around. note that a bag-of-words or bag-of-ngrams model is likely to have a very hard time solving this particular problem as the dependency between the verb and the subject relies on the structure of the sentence which is lost when moving to a bag-of-words representation and can also span more than any number of words n. e model was trained as a straightforward acceptor oy d softmax.mlp.rnn.e e using cross-entropy loss. we had tens of thousands training sentences and hundreds of thousands test sentences of the agreement cases are not hard and we wanted the test set to contain a substantial amount of hard cases. is is a hard task with very indirect supervision the supervision signal did not include any clue as to where the grammaticality clue is. e rnn had to learn the concept of number plural and singular words belong to different groups the concept of agreement the form of the verb should match the form of the subject and the concept of subjecthood identify which some fine details we identified verbs using automatically assigned pos-tags. we used a vocabulary of the most frequent words in the corpus and words not in the vocabulary were replaced with their automatically assigned pos tag. keys to the cabinet in the corner of the room are on the tabledetdetpobjpobjnsubjpobjrootpobjdetdetdetprepprepprepprep rnns as feature extractors of the nouns preceding the verb determines the verb s form. identification of the correct subject requires learning to identify syntactic markers of nested structures in order to be able to skip over distracting nouns in nested clauses. e rnn handled the learning task remarkably well and managed to solve the vast majority accuracy of the test set cases. when focusing on the really hard cases in which the verb and its subject were separated by nouns of the opposite number the rnn still managed to get accuracy of over note that if it were to learn a heuristic of predicting the number of the last noun its accuracy would have been on these cases and for a heuristic of choosing a random preceding noun the accuracy would have been to summarize this experiment demonstrates the learning power of gated rnns and the kinds of subtle patterns and regularities in the data they can pick up on. rnns as feature extractors a major use case of rnns is as flexible trainable feature extractors that can replace parts of the more traditional feature extraction pipelines when working with sequences. in particular rnns are good replacements for window-based extractors. part-of-speech tagging let s re-consider the part-of-speech tagging problem under the rnn setup. e skeleton deep birnn pos-tagging is a special case of the sequence tagging task assigning an output tag to each of the n input words. is makes a birnn an ideal candidate for the basic structure. given a sentence with words s d we will translate them into input vectors using a feature function xi d i e input vectors will be fed into a deep birnn producing output vectors d each of the vectors yi will then be fed into an mlp which will predict one of k possible output tags for the word. each vector yi is focused on position i in the sequence but also has information regarding the entire sequence surrounding that position infinite window rough the training procedure the birnn will learn to focus on the sequential aspects that are informative for predicting the label for wi and encode them in the vector yi. from words to inputs with character-level rnns how do we map a word wi to an input vector xi? one possibility is to use an embedding matrix which can be either randomly initialized or pre-trained using a technique such as w with positional window contexts. such mapping will be performed through an embedding matrix e mapping words to embedding vectors ei d e while this works well it can also suffer from coverage problems for vocabulary items not seen during training or pre-training. words are made of characters and certain suffixes and prefixes as well as other orthographic cues such as the presence of capitalization hyphens or dig modeling with recurrent networks its can provide strong hints regarding the word s ambiguity class. in chapters and we discussed integrating such information using designated features. here we will replace these manually designed feature extractors with rnns. specifically we will use two character-level rnns. for a word w made of characters c we will map each character into a corresponding embedding vector ci. e word will then be encoded using a forward rnn and reverse rnn over the characters. ese rnns can then either replace the word embedding vector or better yet be concatenated to it xi d i d note that the forward-running rnn focuses on capturing suffixes the backward-running rnn focuses on prefixes and both rnns can be sensitive to capitalization hyphens and even word length. e final model e tagging models then becomes p.ti d wn d i xi d i d e model is trained using cross-entropy loss. making use of word dropout for the word embeddings is beneficial. an illustration of the architecture is given in figure a similar tagging model is described in the work of plank et al. in which it was shown to produce very competitive results for a wide range of languages. character-level convolution and pooling in the architecture above words are mapped to vectors using forward-moving and backward-moving rnns over the word s characters. an alternative is to represent words using character-level convolution and pooling neural networks chapter ma and hovy demonstrate that using a one-layer convolutional-and-pooling layer with a window-size of k d over each word s characters is indeed effective for part-ofspeech tagging and named-entity recognition tasks. structured models in the above model the tagging prediction for word i is performed independently of the other tags. is may work well but one could also condition the ith tag on the previous model predictions. e conditioning can be either the previous k tags a markov assumption in which case we use tag embeddings e resulting in p.ti d wn d i e e or on the entire sequence of previous predictions in which case an rnn is used for encoding the tag sequence p.ti d wn d i rnns as feature extractors figure illustration of the rnn tagging architecture. each word wi is converted into a vector which is a concatenation of an embedding vector and the end states of forward- and backwardmoving character level rnns. e word vectors are then fed into a deep birnn. e output of each of the outer layer birnn states is then fed into a predicting network followed by softmax resulting in a tag prediction. note that each tagging prediction can conditions on the entire input sentence. in both cases the model can be run in greedy mode predicting the tags ti in sequence or using dynamic programming search the markov case or beam-search both cases to find a high-scoring tagging sequence. such a model was used for ccg-supertagging each word one of a large number of tags encoding a rich syntactic structure by vaswani et al. structured prediction training for such models is discussed in chapter rnn cnn document classification in the sentiment classification examples in section we had embedding vectors feeding into a forward-moving rnn and a backward-moving rnn followed by a classification layer in the tagger example in section we saw that the word embeddings can be supplemented replaced with character-level models such as rnns or cnns over preddetadjnnvbinbipredconcatbipredbipredbipredbibibibibibibibibibibi rfcscbrfcrrfcorfcwrfcnrfcecerfrbcnrbcwrbcorbcrrbcbrbcsrb modeling with recurrent networks the characters in order to improve the model s coverage and help it deal with unseen words inflections and typos. e same approach can be effective also for document classification instead of feeding word-embeddings into the two rnns we feed vectors that result either from character-level rnns over each word or from a convolutional-and-pooling layer applied over each word. another alternative is to apply a hierarchical convolution-and-pooling network on the characters in order to get a shorter sequence of vectors that represent units that are beyond characters but are not necessarily words captured information may capture either more or less than a single word and then feed the resulting sequence of vectors into the two rnns and the classification layer. such an approach is explored by xiao and cho on several document classification tasks. more specifically their hierarchical architecture includes a series of convolutional and pooling layers. at each layer a convolution with window size k is applied to the sequence of input vectors and then max-pooling is applied between each two neighboring resulting vectors halving the sequence length. after several such layers window sizes varying between and as a function of the layer i.e. widths of the resulting vectors are fed into forward-running and backward-running gru rnns which are then fed into a classification component fully connected layer followed by softmax. ey also apply dropout between the last convolutional layer and the rnns and between the rnn and the classification component. is approach is effective for several document classification tasks. arc-factored dependency parsing we revisit the arc-factored dependency-parsing task from section recall that we are given a sentence sent with words and corresponding pos-tags and need to assign for each word pair wj a score indicating the strength assigned to word wi being the head of word wj. in section we derived an intricate feature function for the task based on windows surrounding the head and modifier words the words between the head and modifier words and their pos tags. is intricate feature function can be replaced by a concatenation of two birnn vectors corresponding to the head and the modifier words. specifically given words and pos-tags and with the corresponding embedding vectors and we create a birnn encoding vi for each sentence position by concatenating the word and pos vectors and feeding them into a deep-birnn d xi d ti we then score a head-modifier candidate by passing the concatenation of the birnn vectors through an mlp a s m d mlp m s d mlp rnns as feature extractors illustration of the architecture is given in figure notice that the birnn vectors vi encode the words in context essentially forming an infinite window to each side of the word wi which is sensitive to both the pos-tag sequence and the word sequence. moreover the concatenation include rnns running up to each word in each direction and in particular it covers the sequence of positions between wh and wm and the distance between them. e birnn is trained as part of the larger network and learns to focus on the important aspects of the sequence the syntactic parsing task of the arc-factored parser is explained in section figure illustration of the arc-factored parser feature extractor for the arc between fox and over. such a feature extractor was used in the work of kiperwasser and goldberg in which it was shown to produce state-of-the-art parsing results for the arc-factored approach rivaling the scores of much more complex parsing models. a similar approach was taken also by zhang et al. achieving similar results with a different training regime. in general whenever one is using words as features in a task that is sensitive to word order or sentence structure the words can be replaced by their trained bilstm vectors. such an approach was taken by kiperwasser and goldberg and cross and huang in the context of transition-based syntactic parsing with impressive results. biconcatbibiconcatbibiconcatbibiconcatconcatbibiconcatbibiconcatbibiconcatbibiconcatbibiconcatbi fox setheedefoxenewhoepelikeseveapplesenejumpedeveoverepeaededogenthedfoxnwhoplikesvapplesnjumpedvoverpaddogn c h a p t e r conditioned generation as discussed in chapter rnns can act as non-markovian language models conditioning on the entire history. is ability makes them suitable for use as generators natural language sequences and conditioned generators in which the generated output is conditioned on a complex input. is chapter discusses these architectures. rnn generators a special case of using the rnn-transducer architecture for language modeling is sequence generation. any language model can be used for generation as described in section for the rnn-transducer generation works by tying the output of the transducer at time i with its input at time i c after predicting a distribution over the next output symbols p.ti d a token ti is chosen and its corresponding embedding vector is fed as the input to the next step. e process stops when generating a special end-of-sequence symbol often denoted as e process is depicted in figure figure transducer rnn used as a generator. similar to the case of generation from an ngram language model when generating from a trained rnn transducer one can either choose the highest probability item at opredictpredictpredictpredictpredicttheblackfoxjumpeds conditioned generation each step sample an item according to the model s predicted distribution or use beam-search for finding a globally high-probability output. an impressive demonstration of the ability of gated rnn to condition on arbitrarily long histories is through a rnn-based language model that is trained on characters rather than on words. when used as a generator the trained rnn language model is tasked with generating random sentences character by character each character conditioning on the previous ones et al. working on the character level forces the model to look further back into the sequence in order to connect letters to words and words to sentences and to form meaningful patterns. e generated texts not only resemble fluent english but also show sensitivity to properties that are not captured by ngram language models including line lengths and nested parenthesis balancing. when trained on c source code the generated sequences adhere to indentation patterns and the general syntactic constraints the c language. for an interesting demonstration and analysis of the properties of rnn-based character level language models see karpathy et al. training generators when training the generator the common approach is to simply train it as a transducer that aims to put a large probability mass on the next token in the observed sequence based on the previously observed tokens training as a language model. more concretely for every n words sentence wn in the training corpus we produce an rnn transducer with n c inputs and n c corresponding outputs where the first input is the start-of-sentence symbol followed by the n words of the sentence. e first expected output is then the second expected output is and so on and the n c expected output is the end-of-sentence symbol. is training approach is often called teacher-forcing as the generator is fed the observed word even if its own prediction put a small probability mass on it and in test time it would have generated a different word at this state. while this works it does not handle well deviations from the gold sequences. indeed when applied as a generator feeding on its own predictions rather than on gold sequences the generator will be required to assign probabilities given states not observed in training. searching for a high-probability output sequence using beam-search may also benefit from a dedicated training procedure. as of this writing coping with these situations is still an open research question which is beyond the scope of this book. we briefly touch upon this when discussing structured prediction in chapter conditioned generation while using the rnn as a generator is a cute exercise for demonstrating its strength the power of the rnn transducer is really revealed when moving to a conditioned generation framework. e generation framework generates the next token based on the previously generated conditioned generation tokens is is modeled in the rnn framework as d k j d k j d f otj p.tj j or if using the more detailed recursive definition d k j d f d r.otj sj otj p.tj j where f is a parameterized function that maps the rnn state to a distribution over words for example f d softmax.xw c b or f d softmax.mlp.x. in the conditioned generation framework the next token is generated based on the previously generated tokens and an additional conditioning context c. when using the rnn framework the context c is represented as a vector c d k j c d k j c d f vi d otj p.tj j c or using the recursive definition d k j c d f d r.sj otj p.ti j c at each stage of the generation process the context vector c is concatenated to the input otj and the concatenation is fed into the rnn resulting in the next prediction. figure illustrates the architecture. conditioned generation figure conditioned rnn generator. what kind of information can be encoded in the context c? pretty much any data we can put our hands on during training and that we find useful. for example if we have a large corpus of news items categorized into different topics we can treat the topic as a conditioning context. our language model will then be able to generate texts conditioned on the topic. if we are interested in movie reviews we can condition the generation on the genre of the movie the rating of the review and perhaps the geographic region of the author. we can then control these aspects when generating text. we can also condition on inferred properties that we automatically extract from the text. for example we can derive heuristics to tell us if a given sentence is written in first person if it contains a passive-voice construction and the level of vocabulary used in it. we can then use these aspects as conditioning context for training and later for text generation. sequence to sequence models e context c can have many forms. in the previous subsection we described some fixed-length set-like examples of conditioning contexts. another popular approach takes c to be itself a sequence most commonly a piece of text. is gives rise to the sequence to sequence conditioned generation framework also called the encoder-decoder framework et al. sutskever et al. opredictpredictpredictpredictpredictconcatconcatconcatconcatconcattheblackfoxjumpedsccccc conditioned generation in sequence to sequence conditioned generation we have a source sequence example reflecting a sentence in french and we are interested in generating a target output sequence example the translation of the sentence into english. is works by encoding the source sentence into a vector using an encoder function c d e commonly an rnn c d rnnenc a conditioned generator rnn is then used to generate the desired output according to equation e architecture is illustrated in figure figure sequence-to-sequence rnn generator. issetup isusefulformappingsequencesof length n tosequences oflength m. eencoder summarizes the source sentence as a vector c and the decoder rnn is then used to predict a language modeling objective the target sequence words conditioned on the previously predicted words as well as the encoded sentence c. e encoder and decoder rnns are trained jointly. e odrd odrd odrd odrd oere oere oere oere oepredictpredictpredictpredictpredictconcatconcatconcatconcatconcattheblackfoxjumpedscccccessethetheeblackblackefoxfoxejumpedjumped conditioned generation supervision happens only for the decoder rnn but the gradients are propagated all the way back to the encoder rnn figure figure sequence-to-sequence rnn training graph. applications e sequence-to-sequence approach is very general and can potentially fit any case where a mapping from an input sequence to an output sequence is needed. we list some example use cases from the literature. odrd odrd odrd odrd oere oere oere oere oepredict andcalculatelosspredict andcalculatelosspredict andcalculatelosspredict andcalculatelosspredict conditioned generation machine translation e sequence-to-sequence approach was shown to be surprisingly effective for machine translation et al. using deep lstm rnns. in order for the technique to work sutskever et al. found it effective to input the source sentence in reverse such that xn corresponds to the first word of the sentence. in this way it is easier for the second rnn to establish the relation between the first word of the source sentence to the first word of the target sentence. while the success of the sequence-to-sequence approach in french-to-english translation is impressive it is worth noting that the approach of sutskever et al. required eight layers of high-dimensional lstms is very computationally expensive and is non-trivial to train well. later in this chapter we describe attention-based architectures an elaboration on the sequence-to-sequence architecture that is much more useful for machine translation. email auto-response here the task is to map an email that can be potentially long into a short answer such as yes i ll do it great see you on wednesday or it won t work out. kannan et al. describe an implementation of the auto-response feature for the google inbox product. e core of the solution is a straightforward sequence to sequence conditioned generation model based on an lstm encoder that reads in the email and an lstm decoder that generates an appropriate response. is component is trained on many email-response pairs. of course in order to successfully integrate the response generation component into a product it needs to be supplemented by additional modules to schedule the triggering of the response component to ensure diversity of responses and balance negative and positive responses maintain user privacy and so on. for details see kannan et al. morphological inflection in the morphological inflection task the input is a base word and a desired inflection request and the output is an inflected form of the word. for example for the finnish word bruttoarvo and the desired inflection posncaseinablnumpl the desired output is bruttoarvoista. while the task has traditionally been approached using hand-crafted lexicons and finite-state transducers it is also a very good fit for character level sequence-to-sequence conditioned generation models et al. results of the sigmorphon shared task on inflection generation indicate that recurrent neural network approaches outperform all other participating approaches et al. e second-place system et al. used a sequence-to-sequence model with a few enhancements for the task while the winning system and sch tze used an ensemble of attentive sequence-to-sequence models such as the ones described in section other uses mapping a sequence of n items to a sequence of m items is very general and almost any task can be formulated in an encode-and-generate solution. however the fact that a task can be formulated this way does not mean that it should be perhaps better architectures are more suitable for it or are easier to learn. we now describe several applications that seem to be needlessly hard to learn under the encoder-decoder framework and for which other better conditioned generation suited architectures exist. e fact that the authors managed to get decent accuracies with the encoder-decoder framework attests to the power of the framework. filippova et al. use the architecture for performing sentence compression by deletion. in this task we are given a sentence such as alan turing known as the father of computer science the codebreaker that helped win world war and the man tortured by the state for being gay is to receive a pardon nearly years after his death and are required to produce a shorter compressed version containing the main information in the sentence by deleting words from the original sentence. an example compression would be alan turing is to receive a pardon. filippova et al. model the problem as a sequence-to-sequence mapping in which the input sequence is the input sentence coupled with syntactic information derived from an automatically produced parse-tree and the output is a sequence of k d and s decisions. e model was trained on a corpus of about million sentence-and-compression pairs extracted automatically from news articles and altun producing state-of-the-art results. gillick et al. perform part-of-speech tagging and named-entity recognition by treating it as a sequence-to-sequence problem mapping a sequence of unicode bytes to a sequence of spans predictions of the form indicating a long p entity starting at offset and an long l entity starting at offset vinyals et al. perform syntactic parsing as a sequence-to-sequence task mapping a sentence to a set of constituency bracketing decisions. other conditioning contexts e conditioned-generation approach is very flexible the encoder needn t be an rnn. indeed the conditioning context vector can be based on a single word a cbow encoding be generated by a convolutional network or based on some other complex computation. furthermore the conditioning context need not even be text-based. in a dialog setting which the rnn is trained to produce responses to messages in a dialog li et al. use as context a trainable embedding vector which is associated with the user who wrote the response. e intuition is that different users have different communication styles based on their age gender social role background knowledge personality traits and many other latent factors. by conditioning on the user when generating the response the network can learn to adapt its predictions while still using an underlying language model as a backbone. moreover as a side effect of training the generator the network also learns user embeddings producing similar vectors to users who have similar communication styles. at test time one can influence the style of the generated response by feeding in a particular user average user vector as a conditioning context. while impressive the sequence-to-sequence approach is arguably an overkill for this task in which we map a sequence of n words into a sequence of n decisions where the ith decision relates directly to the ith word. is is in essence a sequence tagging task and a bilstm transducer such as those described in the previous chapter could be a better fit. indeed the work of klerke et al. shows that similar a bit lower accuracies can be obtained using a birnn transducer trained on several orders of magnitude less data. is is again a sequence tagging task which can be performed well using a bilstm transducer or a structured bilstm transducer as described in section unsupervised sentence similarity departing further away from language a popular use-case is in image captioning an input image is encoded as a vector using a multi-layer convolutional network and this vector is used as a conditioning context for an rnn generator that is trained to predict image descriptions and li mao et al. vinyals et al. work by huang et al. extend the captioning task to the more elaborate one of visual story telling in which the input is a series of images and the output is a story describing the progression in the images. here the encoder is an rnn that reads in a sequence of image vectors. unsupervised sentence similarity it is often desired to have vector representations of sentences such that similar sentences have similar vectors. is problem is somewhat ill defined does it mean for sentences to be similar? and is still an open research question but some approaches produce reasonable results. here we focus on unsupervised approaches in the sense that they can be trained from un-annotated data. e result of the training is an encoder function e such that similar sentences are encoded to similar vectors. most approaches are based on the sequence-to-sequence framework an encoder rnn is trained to produced context vectors c that will then be used by an rnn decoder to perform a task. as a consequence the important information from the sentence with respect to the task must be captured in c. en the decoder rnn is thrown away and the encoder is used to generate sentence representations c under the premise that similar sentences will have similar vectors. e resulting similarity function across sentences then crucially relies on the task of the decoder was trained to perform. auto encoding e auto-encoding approach is a conditioned generation model in which a sentence is encoded using an rnn and then the decoder attempts to reconstruct the input sentence. is way the model is trained to encode the information that is needed to reconstruct the sentence again hopefully resulting in similar sentences having similar vectors. e sentence reconstruction objective may not be ideal for general sentence similarity however as it is likely to push apart representations of sentences that convey similar meanings but use different words. machine translation here a sequence-to-sequence network is trained to translate sentences from english to another language. intuitively the vectors produced by the encoder are useful for translation and so they encode the essence of the sentence that is needed to translate it properly resulting in sentences that will be translated similarly to have similar vectors. is method requires a large corpus for the conditioned generation task such as a parallel corpus used in machine translation. mapping images to vectors using neural architectures is a well-studied topic with established best practices and many success stories. it also falls outside the scope of this book. conditioned generation skip-thoughts e model of kiros et al. assigned the name skip-thought vectors by its authors presents an interesting objective to the sentence similarity problem. e model extend the distributional hypothesis from words to sentences arguing that sentences are similar if they appear in similar contexts where a context of a sentence are the sentences surrounding it. e skip-thoughts model is thus a conditioned generation model where an rnn encoder maps a sentence to a vector and then one decoder is trained to reconstruct the previous sentence based on the encoded representation and a second decoder is trained to reconstruct the following sentence. e trained skip-thought encoder produces impressive results in practice mapping sentences such as he ran his hand inside his coat double-checking that the unopened letter was still there and he slipped his hand between his coat and his shirt where the folded copies lay in a brown envelope. to similar vectors. syntactic similarity e work of vinyals et al. demonstrate that an encoder-decoder can produce decent results for phrase-based syntactic parsing by encoding the sentence and requiring the decoder to reconstruct a linearized parse tree as a stream of bracketing decisions i.e. mapping from the boy opened the door to dt nn vbd dt nn e encoded sentence representations under such training are likely to capture the syntactic structure of the sentence. conditioned generation with attention in the encoder-decoder networks described in section the input sentence is encoded into a single vector which is then used as a conditioning context for an rnn-generator. is architectures forces the encoded vector c d rnnenc to contain all the information required for generation and requires the generator to be able to extract this information from the fixed-length vector. given these rather strong requirements the architecture works surprisingly well. however in many cases it can be substantially improved by the addition of an attention mechanism. e conditioned generation with attention architecture et al. relaxes the condition that the entire source sentence be encoded as a single vector. instead the input sentence is encoded as a sequence of vectors and the decoder uses a soft attention mechanism in order to decide on which parts of the encoding input it should focus. e encoder decoder and attention mechanism are all trained jointly in order to play well with each other. conditioned generation with attention more concretely the encoder-decoder with attention architecture encodes a length n input sequence using a birnn producing n vectors d e d e generator can then use these vectors as a read-only memory representing the conditioning sentence at any stage j of the generation process it chooses which of the vectors it should attend to resulting in a focused context vector cj d e focused context vector cj is then used for conditioning the generation at step j d k j d f d r.sj cj cj d otj p.tj j in terms of representation power this architectures subsumes the previous encoder-decoder architecture by setting d cn we get equation how does the function look like? as you may have guessed by this point it is a trainable parameterized function. is text follows the attention mechanism described by bahdanau et al. who were the first to introduce attention in the context of sequence to sequence generation. while this particular attention mechanism is popular and works well many variants are possible. e work of luong et al. explores some of them in the context of machine translation. e implemented attention mechanism is soft meaning that at each stage the decoder sees a weighted average of the vectors where the weights are chosen by the attention mechanism. more formally at stage j the soft attention produces a mixture vector cj nx cj d ci rn c sum to one. e values is the vector of attention weights for stage j whose elements are all positive and are produced in a two stage process first unnormalized attention weights are produced using a feed-forward network mlpatt taking into account the decoder state at time j and each of the vectors ci d d d mlpatt e description of the decoder part of the model differs in some small aspects from that of bahdanau et al. and is more similar to that of luong et al. conditioned generation e unnormalized weights are then normalized into a probability distribution using the softmax function d in the context of machine translation one can think of mlpatt as computing a soft alignment between the current decoder state sj the recently produced foreign words and each of the source sentence components ci. e complete attend function is then d cj nx cj d ci d d mlpatt ci and the entire sequence-to-sequence generation with attention is given by d k j d f d rdec.sj cj nx ci cj d d d d mlpatt ci otj p.tj j f d softmax.mlpout mlpatt ci d v ci c b a sketch of the architecture is given in figure why use the birnn encoder to translate the conditioning sequence into the context vectors instead of letting the attention mechanism look directly at couldn t we just use xi we could but we get important benefits from the encoding process. first the birnn vectors ci represent the items xi in their sentential context cj dpn xi and d mlpatt conditioned generation with attention figure sequence-to-sequence rnn generator with attention. that is they represent a window focused around the input item xi and not the item itself. second by having a trainable encoding component that is trained jointly with the decoder the encoder and decoder evolve together and the network can learn to encode relevant properties of the input that are useful for decoding and that may not be present at the source sequence directly. for example the birnn encoder may learn to encode the position of xi within the sequence and the decoder could use this information to access the elements in order or learn to pay more attention to elements in the beginning of the sequence then to elements at its end. odrd odrd odrd odrd odbiebiebiebiebieesseaaeconditioningconditioningesequencesequenceessessethetheeblackblackefoxfoxejumpedjumped conditioned generation attentive conditioned generation models are very powerful and work very well on many sequence to sequence generation tasks. computational complexity e conditioned generation without attention is relatively cheap the encoding is performed in linear time in the input length and the decoding is performed in linear time in the output length while generating a distribution over words from a large vocabulary is in itself expensive this is an orthogonal issue to this analysis in which we consider the vocabulary scoring as a constant time operation. e overall complexity of the sequence to sequence generation process is then o.m c n. what is the cost of adding the attention mechanism? e encoding of the input sequence remains an o.n linear time operation. however each step of the decoding process now needs to compute cj. is entails n evaluations of mlpatt followed by a normalization step and a summation of n vectors. e complexity of a decoding step grew from a constant time operation to linear in the length of the conditioning sentence resulting in a total runtime of o.m n. interpretability non-attentive encoder-decoder networks like most other neural architectures are extremely opaque we do not have a clear understanding on what exactly is encoded in the encoded vector how this information is exploited in the decoder and what prompted a particular decoder behavior. an important benefit of the attentive architecture is that it provides a simple way of peeking inside some of the reasoning in the decoder and what it learned. at each stage of the decoding process one can look at the produced attention weights and see which areas of the source sequence the decoder found relevant when producing the given output. while this is still a weak form of interpretability it is leaps and bounds beyond the opaqueness of the non-attentive models. attention-based models in nlp conditioned-generation with attention is a very powerful architecture. it is the main algorithm driving state-of-the-art machine translation and provides strong results on many other nlp tasks. is section provides a few examples of its usage. while the output length m is in principle not bounded in practice the trained decoders do learn to produce outputs with a length distribution similar to lengths in the training dataset and in the worst case one can always put a hard limit on the length of generated sentences. attention-based models in nlp machine translation while we initially described machine translation in the context of plain sequence to sequence generation current state-of-the-art machine translation systems are powered by models that employ attention. e first results with attentive sequence-to-sequence models for machine translation are due to bahdanau et al. who essentially used the architecture described in the previous section as is a gru-flavored rnn employing beam-search when generating from the decoder at test time. while luong et al. explored variations on the attention mechanism leading to some gains most progress in neural machine translation use the attentive sequenceto-sequence architecture as is with lstms or grus while changing its inputs. while we cannot expect to cover neural machine translation in this rather short section we list some improvements due to sennrich and colleagues that push the boundaries of the stateof-the-art. in order to deal with vocabularies of highly inflected languages well as to sub-word units restrict the vocabulary size in general sennrich et al. propose moving to working with sub-word units that are smaller than a token. eir algorithm processes the source and target side texts using an algorithm called b in search for prominent subword units algorithm itself is described at the end of section when run on english this stage is likely to find units such as er est un low and wid. e source and target sentences are then processed to split words according to the induced segmentation converting the widest network into the wid_ net_ is processed corpus is then fed into an attentive sequence-to-sequence training. after decoding test sentences the output is processed once more to un-split the sub-word units back into words. is process reduces the number of unknown tokens makes it easier to generalize to new vocabulary items and improves translation quality. related research effort attempt to work directly on the character level and decoding characters instead of words with notable success et al. incorporating monolingual data e sequence-to-sequence models are trained on parallel corpora of aligned sentences in the source and target languages. such corpora exist but are naturally much smaller than available monolingual data which is essentially infinite. indeed the previous generation of statistical machine translation systems train a translation model on the parallel data and a separate language model on much larger monolingual data. e sequence-to-sequence architecture does not currently allow such a separation training the language model and translation model interaction jointly. how can we make use of target-side monolingual data in a sequence-to-sequence framework? sennrich et al. propose the following training protocol when attempting to translate from source to target first train a translation model from target to source and use it to for an overview see the book of koehn as well as the book on syntax-based machine translation in this series et al. conditioned generation translate a large monolingual corpus of target sentences. en add the resulting pairs to the parallel corpus as examples. train a source to target mt system on the combined corpus. note that while the system now trains on automatically produced examples all of the target side sentences it sees are original so the language modeling component is never trained on automatically produced text. while somewhat of a hack this training protocol brings substantial improvements in translation quality. further research will likely yield cleaner solutions for integrating monolingual data. linguistic annotations finally sennrich and haddow show that the attentive sequence-to-sequence architecture can learn better translation model if its input is supplemented with linguistic annotations. at is given a source sentence wn rather than creating the input vectors by simply assigning an embedding vector to each word d e the sentence is run through a linguistic annotation pipeline that includes part-of-speech tagging syntactic dependency parsing and lemmatization. each word is then supplemented with an encoding vector of its part of speech tag it s dependency label with respect to its head its lemma and morphological features e input vectors is then defined as concatenation of these features xi d pii rii lii mi ese additional features consistently improve translation quality indicating that linguistic information is helpful even in the presence of powerful models than can in theory learn the linguistic concepts on their own. similarly aharoni and goldberg show that by training the decoder in a german to english translation system to produce linearized syntactic trees instead of a sequence of words the resulting translations exhibit more consistent reordering behavior and better translation quality. ese works barely scratch the surface with respect to integrating linguistic information. further research may come up with additional linguistic cues that could be integrated or improved ways of integrating the linguistic information. open issues as of the time of this writing major open issues in neural machine translation include scaling up the size of the output vocabulary removing the dependence on it by moving to character-based outputs training while taking the beam-search decoding into account and speeding up training and decoding. another topic that becomes popular is the move to models that make use of syntactic information. at said the field is moving extremely fast and this paragraph may not be relevant by the time the book gets to press. morphological inflection e morphological inflection task discussed above in the context of sequence to sequence models also work better when used with an attentive sequence-to-sequence architecture as evident by the architecture of the winning system in the sigmorphon shared task on morphological reinflection et al. e winning system and sch tze essentially use an off-the-shelf attentive sequence to sequence model. e input to the shared task is a word form and a desired inflection given as a list of target part-of-speech tags and morphological attention-based models in nlp features e.g. noun gendermale numberplural and the desired output as an inflected form. is is translated to a sequence to sequence model by creating an input sequence that is the list of inflection information followed by the list of characters of the input word. e desired output is then the list of characters in the target word. syntactic parsing while more suitable architectures exist the work of vinyals et al. show that attentive sequence to sequence models can produce competitive syntactic parsing results by reading in a sentence word at a time and outputting a sequence of bracketing decisions. is may not seem like an ideal architecture for parsing indeed one can get superior results with better tailored architectures as evident by the work of cross and huang however considering the generality of the architecture the system works surprisingly well and produces impressive parsing results. in order to get fully competitive results some extra steps must be taken the architecture needs a lot of training data. it is trained on parse-trees produced by running two treebank-trained parsers on a large text corpus and selecting trees on which the two parsers agree parses. in addition for the final parser an ensemble of several attention networks is used. part iv additional topics c h a p t e r modeling trees with recursive neural networks e rnn is very useful for modeling sequences. in language processing it is often natural and desirable to work with tree structures. e trees can be syntactic trees discourse trees or even trees representing the sentiment expressed by various parts of a sentence et al. we may want to predict values based on specific tree nodes predict values based on the root nodes or assign a quality score to a complete tree or part of a tree. in other cases we may not care about the tree structure directly but rather reason about spans in the sentence. in such cases the tree is merely used as a backbone structure which helps guide the encoding process of the sequence into a fixed size vector. e recursive neural network abstraction popularized in nlp by richard socher and colleagues socher et al. is a generalization of the rnn from sequences to trees. much like the rnn encodes each sentence prefix as a state vector the recnn encodes each tree-node as a state vector in rd. we can then use these state vectors either to predict values of the corresponding nodes assign quality values to each node or as a semantic representation of the spans rooted at the nodes. e main intuition behind the recursive neural networks is that each subtree is represented as a d-dimensional vector and the representation of a node p with children and is a function of the representation of the nodes vec.p d f where f is a composition function taking two d-dimensional vectors and returning a single d-dimensional vector. much like the rnn state si is used to encode the entire sequence the recnn state associated with a tree node p encodes the entire subtree rooted at p. see figure for an illustration. formal definition consider a binary parse tree t over an n-word sentence. as a reminder an ordered unlabeled tree over a string xn can be represented as a unique set of triplets k j s.t. i k j. each such triplet indicates that a node spanning words xiwj is parent of the nodes spanning xiwk and triplets of the form i i correspond to terminal symbols at the tree leaves words xi. moving from the unlabeled case to the labeled one we can represent a tree as a set of while presented in terms of binary parse trees the concepts easily transfer to general recursively defined data structures with the major technical challenge is the definition of an effective form for r the combination function. modeling trees with recursive neural networks figure illustration of a recursive neural network. e representations of v and are combined to form the representation of vp. e representations of vp and are then combined to form the representation of s. b c i k j whereas i k and j indicate the spans as before and a b and c are the node labels of of the nodes spanning xiwj xiwk and respectively. here leaf nodes have the form a a i i i where a is a pre-terminal symbol. we refer to such tuples as production rules. for an example consider the syntactic tree for the sentence the boy saw her duck. its corresponding unlabeled and labeled representations are as shown in table s v vp combinecombinesnpdetnounboyverbsawnpvpdetthenounherduck table unlabeled and labeled representations formal definition e set of production rules above can be uniquely converted to a set of tree nodes qa iwj a node with symbol a over the span xiwj by simply ignoring the elements c k in each production rule. we are now in position to define the recursive neural network. a recursive neural network is a function that takes as input a parse tree over an nword sentence xn. each of the sentence s words is represented as a d-dimensional vector xi and the tree is represented as a set t of production rules b c i j k. denote the nodes of t by qa iwj where each inside state vector sa iwj and encodes the entire structure rooted at that node. like the sequence rnn the tree-shaped recnn is defined recursively using a function r where the inside vector of a given node is defined as a function of the inside vectors of its direct children. formally iwj. e recnn returns as output a corresponding set of inside state vectors sa iwj rd represents the corresponding tree node qa xnt dfsa sa iwi dv.xi sa iwj dr.a b c sb iwj rd j qa t g iwj iwk sc qb iwk t qc t e function r usually takes the form of a simple linear transformation which may or may not be followed by a nonlinear activation function g r.a b c sb le and zuidema extend the recnn definition such that each node has in addition to its inside state vector also an outside state vector representing the entire structure around the subtree rooted at that node. eir formulation is based on the recursive computation of the classic inside-outside algorithm and can be thought of as the birnn counterpart of the tree recnn. for details see le and zuidema d iwk sc iwki sc unlabeledlabeledcorresponding det det noun noun verb verb det det noun noun det noun her verb np saw her det nound the np vp the boy saw her duck modeling trees with recursive neural networks is formulation of r ignores the tree labels using the same matrix w for all combinations. is may be a useful formulation in case the node labels do not exist when the tree does not represent a syntactic structure with clearly defined labels or when they are unreliable. however if the labels are available it is generally useful to include them in the composition function. one approach would be to introduce label embeddings v.a mapping each non-terminal symbol to a dnt dimensional vector and change r to include the embedded symbols in the combination function r.a b c sb iwk sc iwki sc r.a b c sb iwk sc d iwki sc bc v.ai w such approach is taken by qian et al. an alternative approach due to socher et al. is to untie the weights according to the non-terminals using a different composition matrix for each b c pair of symbols d is formulation is useful when the number of non-terminal symbols the number of possible symbol combinations is relatively small as is usually the case with phrase-structure parse trees. a similar model was also used by hashimoto et al. to encode subtrees in semantic-relation classification task. extensions and variations as all of the definitions of r above suffer from the vanishing gradients problem of the simple rnn several authors sought to replace it with functions inspired by the lstm gated architecture resulting in tree-shaped lstms et al. zhu et al. e question of optimal tree representation is still very much an open research question and the vast space of possible combination functions r is yet to be explored. other proposed variants on tree-structured rnns includes a recursive matrix-vector model et al. and recursive neural tensor network et al. in the first variant each word is represented as a combination of a vector and a matrix where the vector defines the word s static semantic content as before while the matrix acts as a learned operator for the word allowing more subtle semantic compositions than the addition and weighted averaging implied by the concatenation followed by linear transformation function. in the second variant words are associated with vectors as usual but the composition function becomes more expressive by basing it on tensor instead of matrix operations. in our own work and goldberg we propose a tree encoder that is not restricted to binary trees but instead can work with arbitrary branching trees. e encoding is based on rnns lstms where each subtree encoding is recursively defined as the merging of two rnn states one running over the encodings of the left subtrees left to while not explored in the literature a trivial extension would condition the transformation matrix also on a. training recursive neural networks right and ending in the root node and the other running over the encodings of the right subtrees right to left and ending in the root node. training recursive neural networks e training procedure for a recursive neural network follows the same recipe as training other forms of networks define a loss spell out the computation graph compute gradients using backpropagation and train the parameters using sgd. with regard to the loss function similar to the sequence rnn one can associate a loss either with the root of the tree with any given node or with a set of nodes in which case the individual node s losses are combined usually by summation. e loss function is based on the labeled training data which associates a label or other quantity with different tree nodes. additionally one can treat the recnn as an encoder whereas the inside-vector associated with a node is taken to be an encoding of the tree rooted at that node. e encoding can potentially be sensitive to arbitrary properties of the structure. e vector is then passed as input to another network. for further discussion on recursive neural networks and their use in natural language tasks refer to the ph.d. thesis of socher a simple alternative linearized trees e recnn abstraction provides a flexible mechanism for encoding trees as vectors using a recursive compositional approach. e recnn encodes not only the given tree but also all of its subtrees. if this recursiveness of the encoding is not needed and all we need is a vector representation of an entire tree that is sensitive to the tree structure simpler alternatives may work well. in particular linearizing trees into linear sequence that is then fed into a gated rnn acceptor a birnn encoder has proven to be very effective in several works and charniak luong et al. vinyals et al. concretely the tree for the sentence the boy saw her duck presented above will be translated into the linear string the det boy noun np saw verb her det duck noun np vp s which will then be fed into a gated rnn such as an lstm. e final state of the rnn can then be used as the vector representation of the tree. alternatively the tree structure can be scored by training an rnn language model over such linearized parse-trees and taking the language-model probability of the linearized parse tree to stand for its quality. before the introduction of the computation graph abstraction the specific backpropagation procedure for computing the gradients in a recnn as defined above was referred to as the back-propagation through structure algorithm and k chler modeling trees with recursive neural networks outlook e concept of recursive tree-structured networks is powerful intriguing and seems very suited for dealing with the recursive nature of language. however as of the end of it is safe to say that they don t yet show any real and consistent benefits over simpler architectures. indeed in many cases sequence-level models such as rnns capture the desired regularities just as well. either we have not yet found the killer-application for tree-structured networks or we have not yet found the correct architecture or training regimes. some comparison and analysis of the use of tree-structured vs. sequence-structured networks for language tasks can be found in the work of li et al. as it stands the use of tree-structured networks for processing language data is still an open research area. finding the killer-app for such networks providing better training regimes or showing that tree-like architectures are not needed are all exciting research directions. c h a p t e r structured output prediction many problems in nlp involve structured outputs cases where the desired output is not a class label or distribution over class labels but a structured object such as a sequence a tree or a graph. canonical examples are sequence tagging part-of-speech tagging sequence segmentation ner syntactic parsing and machine translation. in this chapter we discuss the application of neural network models for structured tasks. search-based structured prediction e common approach to structured data prediction is search based. for in-depth discussion of search-based structure prediction in pre-deep-learning nlp see the book by smith e techniques can easily be adapted to use a neural network. in the neural networks literature such models were discussed under the framework of energy-based learning et al. section ey are presented here using setup and terminology familiar to the nlp community. search-based structured prediction is formulated as a search problem over possible structures where x is an input structure y is an output over x a typical example x is a sentence and y is a tag-assignment or a parse-tree over the sentence y.x is the set of all valid structures over x and we are looking for an output y that will maximize the score of the x y pair. structured prediction with linear models in the rich literature on structure prediction with linear and log-linear models the scoring function is modeled as a linear function predict.x d argmax scoreglobal.x y scoreglobal.x y d w y where is a feature extraction function and w is a weight vector. in order to make the search for the optimal y tractable the structure y is decomposed into parts and the feature function is defined in terms of the parts where is a part-local feature extraction function y dx structured output prediction scores each part is scored separately and the structure score is the sum of the component parts dx w dx scoreglobal.x y dw y d w where p y is a shorthand for p parts.x y. e decomposition of y into parts is such that there exists an inference algorithm that allows for efficient search for the best scoring structure given the scores of the individual parts. nonlinear structured prediction one can now trivially replace the linear scoring function over parts with a neural network scorelocal.p scoreglobal.x y dx scoreglobal.x y dx scorelocal.p dx dx where maps the part p into a di n dimensional vector. in case of a one hidden-layer feed-forward network c rdin w w a common objective in structured prediction is making the gold structure y score higher than any other structure leading to the following perceptron loss max scoreglobal.x scoreglobal.x y e maximization is performed using a dedicated search algorithm which is often based on dynamic programming or a similar search technique. in terms of implementation this means create a computation graph c gp for each of the possible parts and calculate its score. en run inference search to find the best scoring structure according to the scores of its parts. connect the output nodes of the computation graphs corresponding to parts in the gold structure y into a summing node cgy connect cgy and using a minus node cgl and compute the gradients. as argued in lecun et al. section the generalized perceptron loss may not be a good loss function when training structured prediction neural networks as it does not have a margin and a margin-based hinge loss is preferred m c max y scoreglobal.x scoreglobal.x y search-based structured prediction it is trivial to modify the implementation above to work with the hinge loss. note that in both cases we lose the nice properties of the linear model. in particular the model is no longer convex. is is to be expected as even the simplest nonlinear neural network is already non-convex. nonetheless we could still use standard neural network optimization techniques to train the structured model. training and inference is slower as we have to evaluate the neural network take gradients once for each part a total of jparts.x yj times. cost augmented training structured prediction is a vast field and this book does not attempt to cover it fully. for the most part the loss functions regularizers and methods described in e.g. smith are easily transferable to the neural network framework although losing convexity and many of the associated theoretical guarantees. one technique that is worth mentioning specifically is cost augmented training also called loss augmented inference. while it brings modest gains when used in linear structured prediction my research group found it essential to successfully training neural network-based structured-prediction models using the generalized perceptron or the margin based losses especially when using strong feature extractors such as rnns. e maximization term in equations and is looking for a structure that receives a high score according to the current model and is also wrong. en the loss reflects the difference in scores between and the gold structure y. once the model is sufficiently well trained the incorrect structure and the correct one y are likely to be similar to each other the model learned to assign high scores to structures that are reasonably good. recall that the global score function is in fact composed of a sum of local part scores. parts that appear in both scoring terms and of y will cancel each other out and will result in gradients of for the associated network parameters. if y and are similar to each other then most parts will overlap and cancel out this way leading to an overall very small update for the example. e idea behind cost-augmented training is to change the maximization to find structures that score well under the model and are also relatively wrong in the sense that they have many incorrect parts. formally the hinge objective changes to where is a scalar hyperparameter indicating the relative importance of vs. the model score and is a function counting the number of incorrect parts in with respect to y d jfp w p p ygj practically the new maximization can be implemented by increasing the local score of each incorrect part by before calling the maximization procedure. e use of cost augmented inference surfaces highly incorrect examples and result in more loss terms that do not cancel out causing more effective gradient updates. m c max y c scoreglobal.x structured output prediction probabilistic objective e error-based and margin-based losses above attempt to score the correct structure above incorrect ones but does not tell anything about the ordering of the structures below the highest scoring one or the score distances between them. in contrast a discriminative probabilistic loss attempts to assign a probability to each possible structure given the input such that the probability of the correct structure is maximized. e probabilistic losses are concerned with the scores of all possible structures not just the highest scoring one. in a probabilistic framework known as conditional random fields or crf each of the parts scores is treated as a clique potential lafferty et al. smith and the score of each structure y is defined to be score y d p d d escoreglobal escoreglobal scorelocal.p scorelocal.p d eters of the network such that corpus conditional log likelihoodp.xi log p is e scoring function defines a conditional distribution p and we wish to set the parammaximized. e loss for a given training example y is then y d y at is the loss is related to the distance of the probability of the correct structure from e crf loss can be seen as an extension of the hard-classification cross-entropy loss to the structured case. taking the gradient with respect to the loss in equation is as involved as building the associated computation graph. e tricky part is the denominator partition function which requires summing over the potentially exponentially many structures in y. however for some problems a dynamic programming algorithm exists for efficiently solving the summation in polynomial time the forward-backward viterbi recurrences for sequences and the cky inside-outside recurrences for tree structures. when such an algorithm exists it can be adapted to also create a polynomial-size computation graph. approximate search sometimes efficient search algorithms for the prediction problem are not available. we may not have an efficient way of finding the best scoring structure the maximization in equa search-based structured prediction tions or or not have an efficient algorithm for computing the partition function in equation in such cases one can resort to approximate inference algorithms such as beam search. when using beam search the maximization and summation are with respect to the items in the beam. for example one may use beam search for looking for a structure with an approximately high score and for the partition function sum over the structures remaining in the beam instead of over the exponentially large y.x. a related technique when working with inexact search is early-update instead of computing the loss over complete structures compute it over partial structures as soon as the gold items falls off the beam. for an analysis of the early update techniques and alternative loss-computation and update strategies when learning under approximate search see huang et al. reranking when searching over all possible structures is intractable inefficient or hard to integrate into a model another alternative to beam search is the use of reranking. in the reranking framework and johnson collins and koo a base model is used to produce a list of the k-best scoring structures. a more complex model is then trained to score the candidates in the k-best list such that the best structure with respect to the gold one is scored highest. as the search is now performed over k items rather than over an exponential space the complex model can condition on features from arbitrary aspects of the scored structure. e base model that is used for predicting the k-best structures can be based on a simpler model with stronger independence assumptions which can produce reasonable but not great results. reranking methods are natural candidates for structured prediction using neural network models as they allow the modeler to focus on the feature extraction and network structure while removing the need to integrate the neural network scoring into a decoder. indeed reranking methods are often used for experimenting with neural models that are not straightforward to integrate into a decoder such as convolutional recurrent and recursive networks. works using the reranking approach include auli et al. le and zuidema schwenk et al. socher et al. zhu et al. and choe and charniak see also beyond the examples in section sequence-level crfs with neural network clique potentials are discussed in peng et al. and do et al. where they are applied to sequence labeling of biological data ocr data and speech signals and by wang and manning who apply them on traditional natural language tagging tasks and ner. similar sequence tagging architecture is also described in collobert and weston collobert et al. a hinge-based approach was used by pei et al. for arc-factored dependency parsing with a manually defined feature extractor and by kiperwasser and goldberg using a bilstm feature extractor. e probabilistic approach was used by durrett and klein for a crf constituency parser. e approximate beam-based partition function crf was ef structured output prediction fectively used by zhou et al. in a transition-based parser and later by andor et al. for various tasks. greedy structured prediction in contrast to the search-based structured prediction approaches there are greedy approaches that decompose the structured problem into a sequence of local prediction problems and training a classifier to perform each local decision well. at test time the trained classifier is used in a greedy manner. examples of this approach are left-to-right tagging models nez and m rquez and greedy transition-based parsing because they do not assume search greedy approaches are not restricted in the kind of features that are available to them and can use rich conditioning structures. is make greedy approaches quite competitive in terms of prediction accuracies for many problems. however the greedy approaches are heuristic by definition and have the potential of suffering from error-propagation prediction errors that are made early in the sequence cannot be fixed and can propagate into larger errors later on. e problem is especially severe when using a method with a limited horizon into the sentence such as common with window-based feature extractors. such methods process the sentence tokens in a fixed order and only see a local window around the prediction point. ey have no way of knowing what the future of the sequence hold and are likely to be misled by the local context into incorrect decisions. fortunately the use of rnns especially birnns mitigate the effect considerably. a feature extractor which is based on a birnn can essentially see through the end of the input and be trained to extract useful information from arbitrarily far sequence positions. is ability turn greedy local models that are trained with birnn extractor into greedy global models each decision can condition on the entire sentence making the process less susceptible to being surprised later on by an unexpected output. as each prediction can become more accurate the overall accuracy grows considerably. indeed works in syntactic parsing show that greedy prediction models that are trained with global birnn feature extractors rival the accuracy of search-based methods that combine global search with local feature extractors and huang dyer et al. kiperwasser and goldberg lewis et al. vaswani et al. in addition to global feature extractors the greedy methods can benefit from training techniques that aim to mitigate the error propagation problem by either attempting to take easier predictions before harder ones easy-first approach and elhadad or making training conditions more similar to testing conditions by exposing the training procedure to inputs that result from likely mistakes daum iii et al. goldberg and nivre ese are effective also for training greedy neural network models as demonstrated by ma et al. transition-based parsers are beyond the scope of this book but see k bler et al. nivre and goldberg and nivre for an overview. conditional generation as structured output prediction tagger and ballesteros et al. kiperwasser and goldberg oracle training for greedy dependency parsing. conditional generation as structured output prediction finally rnn generators especially in the conditioned generator setup can also be seen as an instance of structured-prediction. e series of predictions made by the generator produces a structured output each individual prediction has an associated score probability ability i.e. such thatpn score.oti j and we are interested in output sequence with maximal score maximal is maximized. unfortunately the non-markovian nature of the rnn means that the scoring function cannot be decomposed into factors that allow for exact search using standard dynamic programming techniques and approximate search must be used. one popular approximate technique is using greedy prediction taking the highest scoring item at each stage. while this approach is often effective it is obviously non-optimal. indeed using beam search as an approximate search often works far better than the greedy approach. at this stage it is important to consider how conditioned generators are trained. as described in section generators are trained using a teacher-forcing technique they are trained using a probabilistic objective that attempts to assign high probability mass to gold observed sequences. given a gold sequence at each stage i the model is trained to assign a high probability mass to the gold event oti d ti conditioned on the gold history ere are two shortcomings with this approach first it is based on the gold history while in practice the generator will be tasked with assigning scores based on its predicted history second it is a locally normalized model the model assigns a probability distribution over each event and thus susceptible to the label bias problem which can hurt the quality of solutions returned by beam search. both of these problems were tackled in the nlp and machine-learning communities but are not yet fully explored in the rnn generation setting. e first problem can be mitigated using training protocols such searn daum iii et al. dagger and bagnell ross et al. and exploration-training with dynamic oracles and nivre application of these techniques in the context of rnn generators is proposed by bengio et al. under the term scheduled sampling. e second problem can be treated by discarding of the locally normalized objective and moving to global sequence-level objectives that are more suitable for beam decoding. such objectives include the beam approximations of the structured hinge loss and the crf loss discussed in section above. wiseman and rush discuss global sequence-level scoring objectives for rnn generators. for a discussion of the label bias problem see section of andor et al. and the references therein. structured output prediction examples search-based structured prediction first-order dependency parsing consider the dependency-parsing task described in section e input is an n-words sentence s d wn and we are interested in finding a dependency parse tree y over the sentence a dependency parse tree is a rooted directed tree over the words in the sentence. every word in the tree is assigned a single parent head that can be either another word in the sentence or special root element. e parent word is called a head and its daughter words are called modifiers. dependency parsing fits nicely in the search-based structured prediction framework described in section specifically equation states that we should assign scores to trees by decomposing them into parts and scoring each part individually. e parsing literature describes many possible factorizations and collins zhang and mcdonald here we focus on the simplest one due to mcdonald et al. the arc-factored decomposition. each part will be an arc in the tree pair of head word wh and modifier word wm. each arc wm will be scored individually based on a local scoring function that will asses the quality of the attachment. after assigning a score to each of the possible arcs we can run an inference alogorithm such as the eisner algorithm and satta k bler et al. mcdonald et al. to find the valid projective tree whose sum of arc scores is maximal. equation then becomes scoreglobal.x y d x scorelocal.wh wm d x m s where m s is a feature function translating the sentence indices h and m into real-valued vectors. we discussed feature extractors for the parsing task in sections and manually designed features and in section a birnn feature extractor. here assume the feature extractor is given and focus on the training procedure. once we decide on a particular form for the nn component an mlp nn.x d c b v we can easily compute the score of each possible arch the index of root is d m su c b v n n parsing people talk about projective and non-projective trees. projective trees pose additional constraints on the form of the tree that it can be drawn over a linearization of the words in the sentence in their original order without crossing arcs. while the distinction is an important one in the parsing world it is beyond the scope of this book. for more details see k bler et al. and nivre we then run the eisner algorithm resulting in a predicted tree with maximal score examples if we were to use cost-augmented inference we would have used instead the scores na x d max d c d eisner.n a if m y otherwise x c x once we have the predicted tree and gold tree y we can create a computation graph for the structured hinge loss of the trees according to su c b v y m su c b v scoreglobal.sy we then compute the gradients with respect to the loss using backprop update the parameters accordingly and move to the next tree in the training set. is parsing approach is described in pei et al. the manually designed feature function from section and kiperwasser and goldberg the birnn feature extractor from section neural-crf for named entity recognition independent classification consider the named entity recognition task described in section it is a sequence segmentation task which is often modeled as sequence tagging each word in the sentence is assigned one of k bio-tags described in table and the tagging decisions are then deterministically translated into spans. in section we treated ner as a word-in-context classification problem assuming each tagging decision for each word is performed independently of the others. under the independent classification framework we are given a sentence s d wn and use a feature function s to create a feature vector representing the word wi in the context of the sentence. en a classifier such as an mlp is used to predict a score a probability to each tag oti d s ni here oti is a vector of predicted tag scores and oti is the score of tagging word i with tag k. e predicted tagging oyn for the sentence is then obtained by independently choosing the highest scoring tag for each sentence position oti oyi d argmax n k structured output prediction and the score of the assignment oy d oyn is nx score.s oy d ti oyi structured tagging by coupling tag-pair decisions e independent classification approach may work reasonably well in many cases but is sub-optimal because neighboring decisions influence each other. consider a sequence such as paris hilton the first word can be either a location or a person and the second word can be either an organization or a person but if we chose one of them to be a person the second one should be tagged person with certainty. we would like to have the different tagging decisions influence each other and have this reflected in the score. a common way to do this is by introducing tag-tag factors compatibility scores for pairs of neighboring tags. intuitively a pair such as b-per i-per should receive a high score while a pair b-per i-org should receive a very low or even negative score. for a tagset of k possible tags we introduce a scoring matrix a in which is the compatibility score of the tag sequence g h. e scoring function for a tagging assignment is updated to take the tagging factors into account score.s oy d ti oyi c oyi nx where the tags at locations and n c are special and symbols. given tagging scores for individual words and the values in a one can find the sequence oy maximizing equation using the viterbi dynamic-programming algorithm. as we do not need the tag scores in each position to be positive and sum to one we remove the softmax when computing the scores ti oti d s n e tagging scores ti are determined by a neural network according to equation and the matrix a can be considered as additional model parameters. we can now proceed to train a structured model using the structured hinge-loss or the cost-augmented structured hinge loss instead we will follow lample et al. and use the probabilistic crf objective. structured crf training under the crf objective our goal is to assign a probability to each possible tag sequence y d yn over a sentence s. is is modeled by taking a softmax over all the possible taggings score y d p j s d d examples escore.sy exp.pn exp.pn ti cpn ti cpn e denominator is the same for all possible taggings y so finding the best sequence its probability amounts to finding the sequence that maximizes score.s y and can be done using viterbi as above. e loss is then defined as the negative log likelihood of the correct structure y p d d ti c ti c score of gold exp c log x c m ti c using dynamic program ti c is algorithm is known as the forward algorithm which is different than the algorithm for computing the forward pass in the computation graph. wherel denotes addition in log-space andl.a b c d d log.ea c eb c ec c ed e first term can be easily constructed as a computation graph but the second is a bit less trivial to construct as it requires summing over the nk different sequences in y.s. fortunately it can be solved using a variant of the viterbi algorithm which we describe below. properties of log-addition e log-add operation performs addition in log-space. it has the following properties that we use in constructing the dynamic program. ey are trivial to prove with basic mathematic manipulation and the reader is encouraged to do so. m.a b dm.b a m.am.b c dm.a b c m.a c c b c c dm.a c b c c commutativity associativity distributivity f structured output prediction f denote by y.s r k the set of sequences of length r that end with symbol k. e set of all possible sequences over jsj is then y.s d y.s n c further denote by y.s r k d the sequences of length r where the last symbol is k and the second to last symbol is let c our goal is computing c as a shorthand define f d ti c we now get rx d m m c dm m dm m dm f dm c f c d d dm c c c dm rx c f c d d k rx c c c f c d d k we obtained the recurrence which we can use to construct the computation graph for computing the denominator c after building the computation graph we can compute the gradients using backpropagation. approximate ner-crf with beam-search in the previous section we transformed the ner prediction into a structured task by coupling the output tags at positions i and i using a score matrix a assigning a score to each consecutive tag pair. is is akin to using a first-order markov assumption in which the tag in position i is independent of the tags at positions i given the tag at i is independence assumption allowed us to decompose the sequence scoring and derive efficient algorithms for finding the highest scoring sequence as well as the sum over all possible tag sequences. observe that this recursion is the same as the one for the best-path viterbi algorithm withl replaced with a max. examples we may want to relax this markov independence assumption and instead condition the tag yi at all previous tags is can be incorporated into the tagging model by adding an additional rnn over the tag history. we now score a tag sequence y d yn as score.s oy d f i where f is a parametric function such as a linear transformation or an mlp and is a feature function mapping the word as position i in the sentence s to a vector. in words we compute the local score of tagging position i with tag k by considering features of sentence position i as well as an rnn encoding of the tag sequence k. we then compute the global score as a sum of local scores. unfortunately the rnn component ties the different local scores over all previous tagging decisions preventing us from using efficient dynamic programming algorithms for finding the exact best tagging sequence or the sum of all possible tag sequences under the model. instead we must resort to approximation such as beam search. using a beam of size r we can develop r different tag sequences oy oy r. e approximate best tag sequence is then the highest scoring of the r beam sequences argmax score.s oy i for training we can use the approximate crf objective qy.sr scorea c y d qp d escore.sy y d log qp d score.s y c log x qy.sr qy.s r dfy y rg fyg instead of normalizing by summing over the entire set of sequences y.s we sum over qy.s r the union of the gold sequence and the r beam sequences. r is a small number making the summation is trivial. as r approaches nk we approach the true crf objective. e feature function can be based on a word window or a bilstm similar to the feature functions for pos-tagging in sections and e beam search algorithm works in stages. after obtaining r possible tag sequences of length i oy sponding to the first i words of the sentence we extend each sequence with all possible tags score each of the resulting r k sequences and retain the top scoring r sequences. e process continues until we have r tag sequences over the entire sentence. oy r c h a p t e r cascaded multi-task and semi-supervised learning when processing natural language it is often the case that we have several tasks that feed into each other. for example the syntactic parser we discussed in sections and takes as input parts of speech tags that are in themselves automatically predicted by a statistical model. feeding the predictions of one model as the input of another when the two models are independent is called a pipeline system. an alternative approach is model cascading. in model cascading rather than feeding the predictions of model a tagger into model b parser we instead feed into the parser the intermediate representations that are informative for predicting the tags. at is rather than committing to a particular tagging decision we pass on the tagging uncertainty to the parser. model cascading is very easy to implement in deep learning system by simply passing the vector before the argmax or even one of the hidden vectors. a related technique is multi-task learning in which we have several related predictions tasks may or may not feed into each other. we would like to leverage the information in one of the tasks in order to improve the accuracy on the other tasks. in deep learning the idea is to have different networks for the different tasks but let the networks share some of their structure and parameters. is way a common predictive core shared structure is influenced by all the tasks and training data for one task may help improve the predictions of the other ones. a cascading approach lends itself naturally to the multi-task learning framework instead of just passing in the intermediate output of the tagger to the parser we can instead plug in the subgraph of the computation graph that is responsible for the intermediate tagging representation as input to the parser s computation graph and backpropagate the parser s error all the way back to the shared base of the tagging component. another related and similar case is that of semi-supervised learning in which we have supervised training data for task a and what to use annotated or unannotated data for other tasks in order to improve the performance on task a. is chapter deals with these three techniques. model cascading in model-cascading large networks are built by composing them out of smaller component networks. for example in section we describe an rnn-based neural network for predicting cascaded multi-task and semi-supervised learning the part of speech of a word based on its sentential context and the characters that compose it. in a pipeline approach we would use this network for predicting parts of speech and then feed the predictions as input features to a neural network that does syntactic chunking or parsing. instead we could think of the hidden layers of this network as an encoding that captures the relevant information for predicting the part of speech. in a cascading approach we take the hidden layers of this network and connect them not the part of speech prediction themselves as the inputs for the syntactic network. we now have a larger network that takes as input sequences of words and characters and outputs a syntactic structure. as a concrete example consider the tagging and parsing networks described in sections and e tagging network reproduced here predicts the tag of the ith word according to ti d argmax i xi d i d while the parsing network assigns arc-scores according to j a s m d mlp m s d mlp d xi d ti e important thing to note here is that the parser takes as input words and tags and then converts the words and tags into embedding vectors and concatenates them to form its corresponding input representations in the cascading approach we ll feed the tagger s pre-prediction state directly into the parser in one joint network. concretely denote by zi the tagger s pre-prediction for word i zi d i we can now use zi as the input representation of the ith word in the parser resulting in a s m d m s d d zi d i xi d i dhe e computation graph abstraction allows us to easily propagate the error gradients from the syntactic task loss all the way back to the characters. figure presents a sketch of the entire network. depending on the situation we may or may not want to backpropagate the error all the way back. model cascading figure tagging-parsing cascade network biparsebiparsebiparsebiparsebiparsebiparsebitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbiparsebiparsebiparsebiparseconcat rfcscbrfcrrfcorfcwrfcnrfcecerfrbcnrbcwrbcorbcrrbcbrbcsrbconcatdetadjnnvbinpredpredpredpredpred brown s cascaded multi-task and semi-supervised learning while the parser has access to the word identities they may be diluted by the time they pass through all the tagger rnn layers. to remedy this we may use a skip-connection and pass the word embeddings e directly to the parser in addition to the tagger s output a s m d m s d d zi d d i xi d i dhe is architecture is depicted in figure to combat the vanishing gradient problem of deep networks as well as to make better use of available training material the individual component network s parameters can be bootstrapped by training them separately on a relevant task before plugging them in to the larger network for further tuning. for example the part-of-speech predicting network can be trained to accurately predict parts-of-speech on a relatively large annotated corpus before plugging its hidden layer into the syntactic parsing network for which less training data is available. in case the training data provide direct supervision for both tasks we can make use of it during training by creating a network with two outputs one for each task computing a separate loss for each output and then summing the losses into a single node from which we backpropagate the error gradients. model cascading is very common when using convolutional recursive and recurrent neural networks where for example a recurrent network is used to encode a sentence into a fixed sized vector which is then used as the input of another network. e supervision signal of the recurrent network comes primarily from the upper network that consumes the recurrent network s output as it inputs. in our example both the tagger and the parser were based on a birnn backbone. is is not necessary either or both of the networks could just as well be a feed-forward network that gets a word-window as input a convolutional network or any other architecture that produces vectors and that can pass gradients. multi-task learning multi-task learning is a related technique in which we have several related tasks that we assume are correlated in the sense that learning to solve one is likely to provide intuitions about solving the other. for example consider the syntactic chunking task linguistic annotation frame in section in which we annotate a sentence with chunk boundaries producing output such as the boy with the black shirt opened the door with a key multi-task learning figure tagging-parsing cascade with skip-connections for the word embeddings biparsebiparsebiparsebiparsebiparsebiparsebitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbitagbiparsebiparsebiparsebiparseconcat rfcscbrfcrrfcorfcwrfcnrfcecerfrbcnrbcwrbcorbcrrbcbrbcsrbconcatpredconcatconcatconcatconcatconcatpredpredpredpred brown sdetadjnnvbin cascaded multi-task and semi-supervised learning like named-entity recognition chunking is a sequence-segmentation task and can be reduced to a tagging task using the bio encoding scheme section a network for chunking then may be modeled as a deep birnn followed by an mlp for individual tag predictions p.chunktagi d j d i xi d i d ecnk brevity we removed the character-level rnns from the input but they can be trivially added. note that this is very similar to a pos-tagging network p.postagi d j d i xi d i d etag figure left pos tagging network. right chunk tagging network. both networks are depicted in figure different colors indicate different sets of parameters. e syntactic chunking task is synergistic with part-of-speech tagging. information for predicting chunk boundaries or the part-of-speech of a word rely on some shared underlying syntactic representation. instead of training a separate network for each task we can create a single network with several outputs. e common approach would be to share the birnn parameters but have a dedicated mlp predictor for each task have also a shared mlp in which only the final matrix and bias terms are specialized for a task. is will result in the following shared network p.chunktagi d j d i p.postagi d j d i xi d i d eshared e two networks use the same deep birnn and embedding layers but separate final output predictors. is is depicted in figure predpredpredpredpreddetadjnnvbinpredpredpredpredpredb-npi-npi-npb-vpb-ppxthexbrownxfoxxjumpedxoverxthexbrownxfoxxjumpedxoverbibibibibibibibibibibibibibibibibibibibibibibibibibibibibibi multi-task learning figure a joint pos-tagging and chunking network. e birnn parameters are shared and the birnn component is specialized for both tasks. e final predictors are separate. most of the parameters of the network are shared between the different tasks. useful in formation learned from one task can then help to disambiguate other tasks. training in a multi-task setup e computation graph abstraction makes it very easy to construct such networks and compute the gradients for them by computing a separate loss for each available supervision signals and then summing the losses into a single loss that is used for computing the gradients. in case we have several corpora each with different kind of supervision signal we have one corpus for pos and another for chunking the preferred training protocol would be to choose a corpus at random pass the example through the relevant part of the computation graph compute the loss backpropagate the error and update the parameters. en on the next step again choose a corpus at random and so on. in practice this is often achieved by shu ing all the available training examples and going through them in order. e important part is that we potentially compute the gradients with respect to a different loss using a different sub-network for each training example. in some cases we may have several tasks but care more about one of them. at is we have one or more main tasks and a few other supporting task which we believe can help the main task but whose predictions we do not care about. in such cases we may want to scale the loss of the supporting task to be smaller than the loss of the main tasks. another option is to first pre-train predpredpredpredpredpredpredpredpredpreddetadjnnvbinxthexbrownxfoxxjumpedxoverb-npi-npi-npb-vpb-ppbibibibibibibibibibibibibibibi cascaded multi-task and semi-supervised learning a network on the supporting tasks and then take the shared components of this network and continue training it only on the main task. selective sharing going back to the pos-tagging and chunking example we could argue that while the tasks share information the pos-tagging task is in fact somewhat more low level than the chunking task the information needed for performing chunking is more refined than that needed for postagging. in such cases we may prefer to not share the entire deep birnn between the two tasks but rather have the lower layer of the birnn be shared and the upper layers be dedicated to the chunking task figure a selectively shared pos-tagging and chunking network. e lower layer of the birnn is shared between the two tasks but the upper layers are dedicated to chunking. e lower layer in the birnn is shared between the two tasks. it is primarily supervised by the pos task but also receives gradients from the chunking supervision. e upper layers of the network are dedicated to the chunking task but are trained to work well with the representation of the lower layers. is selective sharing suggestion follows the work of s gaard and goldberg a similar approach using feed-forward rather than recurrent networks is taken by zhang and weiss under the name stack propagation. e selectively shared mtl network in figure is very similar in spirit to the cascaded setup in discussed in the previous section indeed it is often hard to properly draw the boundary between the two frameworks. predpredpredpredpredpredpredpredpredpreddetadjnnvbinxthexbrownxfoxxjumpedxoverb-npi-npi-npb-vpb-ppbibibibibibibibibibibibibibibi multi-task learning input-output inversion another view of multi-task and cascaded learning is one of inputoutput inversion. instead of thinking of some signal pos-tags as inputs to a higher level task parsing we can think of them as outputs of intermediate layers in the network for the higher level tasks. at is instead of using the parts-of-speech tags as inputs they are used instead as a supervision signal to intermediate layers of the network. word-embeddings pre-training as multi-task learning e chunking and pos-tagging tasks indeed many others are also synergistic with the language modeling task. information for predicting the chunk boundary are the part-of-speech tag of a word is intimately connected with the ability to predict the identity of the next word or the previous one the tasks share a common syntactic-semantic backbone. viewed this way the use of pre-trained word vectors for initializing the embedding layer of a task-specific network is an instance of mtl with language modeling as a supporting task. e word embedding algorithms are trained with an distributional objective that is a generalization of language modeling and the word embedding layer of the embedding algorithms is then shared with the other task. e kind of supervision for the pre-training algorithm the choice of contexts should be matched to the task the specialized network is trying to solve. closer tasks results in larger benefits from mtl. multi-task learning in conditioned generation mtl can be seamlessly integrated into the conditioned generation framework discussed in chapter is is done by having a shared encoder feeding into different decoders each attempting to perform a different task. is will force the encoder to encode information that is relevant to each of the tasks. not only can this information then be shared by the different decoders it also will potentially allow for training different decoders on different training data enlarging the overall number of examples available for training. we discuss a concrete example in section multi-task learning as regularization another view of multi-task learning is one of a regularizer. e supervision from the supporting tasks prevent the network from overfitting on the main task by forcing the shared representation to be more general and useful for prediction beyond the training instances of the main task. viewed this way and when the supporting tasks are meant to be used as regularizers one should not perform the mtl in a sequence where the supporting tasks are tuned first followed by adapting the representation to the main task suggested in section rather all tasks should be learned in parallel. cascaded multi-task and semi-supervised learning caveats while the prospect of mtl is very appealing some caveats are in order. mtl often does not work well. for example if the tasks are not closely related you may not see gains from mtl and most tasks are indeed not related. choosing the related tasks for performing mtl can be more of an art than a science. even if the tasks are related but the shared network doesn t have the capacity to support all the tasks the performance of all of them can degrade. when taking the regularization view this means that the regularization is too strong and prevents the model from fitting the individual tasks. in such cases it is better to increase the model capacity increase the number of dimensions in the shared components of the network. if an mtl network with k tasks needs a k-fold increase in capacity close to it in order to support all tasks it means that there is likely no sharing of predictive structure at all between the tasks and one should forgo the mtl idea. when the tasks are very closely related such as the pos tagging and chunking tasks the benefits from mtl could be very small. is is especially true when the networks are trained on a single dataset in which each sentence is annotated for both pos-tag and chunk label. e chunking network can learn the representation it needs without the help of the intermediate pos supervision. we do start to see the benefits of mtl when the pos-training data and the chunking data are disjoint share sizable portions of the vocabulary or when the pos-tag data is a superset of the chunk data. in this situation the mtl allows to effectively enlarge the amount of supervision for the chunking task by training on data with related labels for the pos-tagging task. is lets the chunk part of the network leverage on and influence the shared representation that was learned based on the pos annotations on the additional data. semi-supervised learning a related framework to both multi-task and cascaded learning is semi-supervised learning in which we have a small amount of training data for a task we care about and additional training data for other tasks. e other tasks can be either supervised or unsupervised where the supervision can be generated from unannotated corpora such as in language modeling word embeddings or sentence encodings as discussed in section chapters and we would like to use the supervision for the additional tasks to invent suitable additional tasks in order to improve the prediction accuracy on the main task. is is a very common scenario which is an active and important research area we never have enough supervision for the tasks we care about. for an overview of non-neural networks semi-supervised learning methods in nlp see the book of s gaard in this series. within the deep-learning framework semi-supervised learning can be performed much like mtl by learning a representation based on the additional tasks that can then be used as supplement input or as initialization to the main task. concretely one can pre-train word em examples beddings or sentence representations on unannotated data and use these to initialize or feed into a pos-tagger parser or a document summarization system. in a sense we have been doing semi-supervised learning ever since we introduced distributional representations pre-trained word embeddings in chapter sometimes problems lend themselves to more specialized solutions as we explore in section e similarities and connections to multi-task learning are also clear we are using supervision data from one task to improve performance on another. e main difference seem to be in how the different tasks are integrated into the final model and in the source of the annotated data for the different tasks but the border between the approaches is rather blurry. in general it is probably best not to debate about the boundaries of cascaded learning multi-task learning and semi-supervised learning but rather see them as a set of complimentary and overlapping techniques. other approaches to semi-supervised learning include various regimes in which one or more models are trained on the small labeled data and are then used to assign labels to large amounts of unlabeled data. e automatically annotated data following some quality filtering stage based on agreement between the models are other confidence measures is then used to train a new model or provide additional features to an existing on. ese approaches can be grouped under the collective term self-training. other methods specify constraints on the solution that should help guide the model specifying that some words can only be tagged with certain tags or that each sentence must contain at least one word tagged as x. such methods are not specialized for neural networks and are beyond the scope of this book. for an overview see the book by s gaard examples we now describe a few examples in which we mtl was shown to be effective. gaze-prediction and sentence compression in the sentence compression by deletion task we are given a sentence such as alan turing known as the father of computer science the codebreaker that helped win world war ii and the man tortured by the state for being gay is to receive a pardon nearly years after his death and are required to produce a shorter compressed version containing the main information in the sentence by deleting words from the original sentence. an example compression would be alan turing is to receive a pardon. is can be modeled as a deep birnn followed by an mlp in which the inputs to the birnn are the words of the sentence and the outputs of the mlps are k or d decisions for each word. in work with klerke et al. we showed that the performance on the sentence deletion by compression task can be improved by using two additional sequence prediction tasks ccg supertagging and gaze prediction. e two tasks are added in a selective-sharing architecture as individual mlps that feed from the lower layer of the birnn. cascaded multi-task and semi-supervised learning e ccg supertagging task assigns each work with a ccg supertag which is a complex syntactic tag such as indicating its syntactic role with respect to the rest of the sentence. e gaze prediction task is a cognitive task that relates to the way people read written language. when reading our eyes move across the page fixating on some words skipping others and often jumping back to previous words. it is widely believed that eye movement when reading reflects on the sentence processing mechanisms in the brain which in turn reflects on the sentence structure. eye-trackers are machines that can accurately track eye-movement while reading and some eye-tracked corpora are available in which sentences are paired with exact eye-movement measurements of several human subjects. in the gaze-prediction task the network was trained to predict aspects of the eye-movement behavior on the text long of a fixation each word would receive or which words will trigger back movements. e intuition being that parts of the sentence which are less important are more likely to be skipped or glossed over and parts that are more important are likely to be fixated more upon when processing the sentence. e compression data the syntactic ccg tagging data and the eye-movement data were completely disjoint from each other but we observed clear improvements to the compression accuracy when including the additional tasks as supervision. arc labeling and syntactic parsing roughout the book we described an architecture for arc-standard dependency parsing. in particular in section we described birnn based features and in section a structured prediction learning framework. e parser we described was an unlabeld parser the model assigned a score to each possible head-modifier pair and the final prediction by the parser was a collection of arcs representing the best tree over the sentence. however the scoring function and the resulting arcs only took into consideration which words are syntactically connected to each other and not the nature of the relation between the words. recall from section that a dependency parse-tree usually contains also the relation information in term of a dependency label on each arc i.e. the det prep pobj nsubj etc. label annotations in figure given an unlabeled parsing the arc-labels can be assigned using an architecture in which a birnn is used to read the words of the sentence and then for arc m in the tree concatenate the corresponding birnn encodings and feed them into an mlp for predicting the arc s label. rather than training a separate network for the label prediction we can treat unlabeled parsing and arc-labeling as related tasks in a multi-task setting. we then have a single birnn for the arc-labeler and the parser and use the encoded birnn states as inputs both to the arcscorer and to the arc-labeler. in training the arc-labeler will only see gold arcs we do not ccg and ccg supertags are beyond the scope of this book. a good pointer to start learning about ccg in nlp is the ph.d. thesis of julia hockenmaier e concept of supertagging is introduced by joshi and srinivas examples figure labeled dependency tree. have label information for the other hypothetical arcs while the arc-scorer will see all possible arcs. indeed in the work of kiperwasser and goldberg we observe that the tasks are indeed closely related. training the joint network for performing both unlabeled arc-scoring and arc-labeling using the same shared birnn encoder not only results in accurate arc labeling but also substantially improves the accuracy of the unlabeled parse trees. preposition sense disambiguation and preposition translation prediction consider the preposition-sense disambiguation task discussed in section to recall this is a word-in-context problem in which we need to assign each preposition with one of k possible sense labels p l d etc.. annotated corpora for the task exist and hargraves schneider et al. but are small. in section we discussed a rich set of core features that can be used for training a preposition-sense disambiguator. let s denote the feature extractor taking a preposition instance and returning an encoding of these features as a vector as i where s is the input sentence words part-of-speech tags lemmas and syntactic parse-tree information and i is the index of the preposition within the sentence. e feature extractor based on features similar to those in features in section without the wordnet-based features but with pre-trained wordembeddings is a strong one. feeding it into an mlp for prediction performs reasonably well still disappointingly low below accuracy and attempts to replace or to supplement it with a birnn-based feature extractor does not improve the accuracies. here we show how the accuracy of the sense prediction can be improved further using a semi-supervised approach which is based on learning a useful representation from large amounts of unannotated data that we transform into related and useful prediction tasks. specifically we will be using tasks derived from sentence-aligned multilingual data. ese are pairs of english sentences and their translation into other languages. when translating from such resources are readily available from e.g. proceedings of the european union europarl corpus or can be mined from the web et al. ese are the resources that drive statistical machine translation. the boy with the black shirt opened the door with a keydetprepprepamoddetpobjnsubjrootdobjpobjdetdet cascaded multi-task and semi-supervised learning english to a different language a preposition can be translated into one of several possible alternatives. e choice of the foreign preposition will be based on the english preposition sense as reflected by the sentential context in which it appears. while prepositions are ambiguous in all languages the ambiguity patterns differ across languages. us predicting the foreign preposition into which a given english preposition will be translated based on the english sentential context is a good auxiliary task for preposition sense disambiguation. is is the approach taken by gonen and goldberg we provide here a high-level overview. for details refer to the original paper. trainingdataiscreatedbasedonamultilingualsentence-alignedparallelcorpus. ecorpus is word-aligned using a word-alignment algorithm et al. and tuples of hsentence preposition-position foreign-language foreign-prepositionsi are extracted as training examples. given such a tuple hs d i l f i the prediction task is to predict the translation of the preposition wi in the context of the sentence s. e possible outputs are taken from a set of language specific options pl and the correct output is f e hope is that a representation of the context of wi that is good at predicting the foreign preposition f will also be helpful for predicting the preposition sense. we model the task as an encoder e i that encodes the sentential context of wi into a vector and a predictor which attempts to predict the right preposition. e encoder is very similar to a birnn but does not include the preposition itself in order to force the network to pay more attention to the context while the predictor is a language specific mlp. p.foreign d f js i l d softmax.mlplforeign.e i e i d e encoder is shared across the different languages. after training the network on several million henglish sentence foreign-prepositioni pairs we are left with a pre-trained context-encoder that can then be used in the preposition-sense disambiguation network by concatenating it to the supervised feature representation. our semi-supervised disambiguator is then p.sense d jjs i d i i where e is the pre-trained encoder that is further trained by the sense prediction network and is the supervised feature extractor. e approach substantially and consistently improve the accuracy of the sense prediction by about accuracy points depending on details of the setup. while an increase of accuracy points may not seem very impressive it is unfortunately in the upper range of what one could realistically expect in semi-supervised scenarios in which the baseline supervised system is already relatively strong using current semi-supervision techniques. improvements over weaker baselines are larger. conditioned generation multilingual machine translation parsing and image captioning examples mtl can also be easily performed in an encoder-decoder framework. e work of luong et al. demonstrated this in the context of machine translation. eir translation system follows the sequence-to-sequence architecture without attention. while better translation systems exist systems that make use of attention the focus of the work was to show that improvements from the multi-task setup are possible. luong et al explore different setups of multi-task learning under this system. in the first setup the encoder component english sentences into vectors is shared and is used with two different decoders one decoder is generating german translations and the other decoder is generating linearized parse-trees for the english sentences the predicted sequence for the boy opened the door should be dt nn vbd dt nn e system is trained on a parallel corpus of henglishgermani translation pairs and on gold parse trees from the penn treebank et al. e translation data and the parsing data are disjoint. rough the multi-task setting the shared encoder learns to produce vectors that are informative for both tasks. e multi-task encoder-decoder network is effective the network that is trained for both tasks encoder two decoders works better than the individual networks consisting of a single encode-decoder pair. is setup likely works because encoding basic elements of the syntactic structure of a sentence are informative for selecting the word order and syntactic structures in the resulting translation and vice versa. e translation and parsing tasks are indeed synergistic. in the second setup there is a single decoder but several different encoders. e tasks here are machine translation to english translation and image captioning to english description. e decoder is tasked at producing english sentences. one encoder is encoding german sentences while the other is encoding images. like before the datasets for the translation and for the image captioning are disjoint. again with some tuning of parameters training the joint system improves over the individual ones though the gains are somewhat smaller. here there is no real connection between the task of encoding german sentences express elaborate predications and complicated syntactic structures and encoding image contents encodes the main components of simple scenes. e benefit seem to be from the fact that both tasks provide supervision for the language-modeling part of the decoder network allowing it to produce better sounding english sentences. additionally the improvement may stem from a regularization effect in which one hencoderdecoderi pair prevents the other pair from overfitting to its training data. despite the rather low baseline the results of luong et al. are encouraging suggesting there are gains to be had from multi-task learning in the conditional generation framework when suitable synergistic tasks are chosen. cascaded multi-task and semi-supervised learning outlook cascaded multi-task and semi-supervised learning are exciting techniques. e neural networks framework driven by gradients-based training over a computation graph provide many seamless opportunities for using these techniques. in many cases such approaches bring real and consistent gains in accuracy. unfortunately as of the time of this writing the gains are often relatively modest compared to the baseline performance especially when the baselines are high. is should not discourage you from using the techniques as the gains often times are real. it should also encourage you to actively work on improving and refining the techniques so that we see could expect to see greater gains in the future. c h a p t e r conclusion what have we seen? e introduction of neural networks methods has been transformative for nlp. it prompted a move from linear-models with heavy feature engineering in particular the engineering of backoff and combination features to multi-layer perceptrons that learn the feature combinations discussed in the first part of the book to architectures like convolutional neural networks that can identify generalizable ngrams and gappy-ngrams discussed in chapter to architectures like rnns and bidirectional rnns that can identify subtle patterns and regularities in sequences of arbitrary lengths and to recursive neural networks that can represent trees. ey also brought about methods for encoding words as vectors based on distributional similarity which can be effective for semi-supervised learning and methods for non-markovian language modeling which in turn pave the way to flexible conditioned language generation models and revolutionized machine translation. e neural methods also present many opportunities for multi-task learning moreover established pre-neural structured-prediction techniques can be readily adapted to incorporate neural network based feature extractors and predictors the challenges ahead all in all the field is progressing very quickly and it is hard to predict what the future will hold. one thing is clear though at least in my view with all their impressive advantages neural networks are not a silver bullet for natural-language understanding and generation. while they provide many improvements over the previous generation of statistical nlp techniques the core challenges remain language is discrete and ambiguous we do not have a good understanding of how it works and it is not likely that a neural network will learn all the subtleties on its own without careful human guidance. e challenges mentioned in the introduction are ever-present also with the neural techniques and familiarity with the linguistic concepts and resources presented in chapter is still as important as ever for designing good language processing systems. e actual performance on many natural language tasks even low-level and seemingly simple ones such as pronominal coreference resolution and manning wiseman et al. or coordination boundary disambiguation and goldberg is still very far from being perfect. designing learning systems to target such low-level language understanding tasks is as important a research challenge as it was before the introduction of neural nlp methods. conclusion another important challenge is the opaqueness of the learned representations and the lack of rigorous theory behind the architectures and the learning algorithms. research into the interpretability of neural network representations as well as into better understanding of the learning capacity and training dynamics of various architectures is crucially needed in order to progress even further. as of the time of this writing neural networks are in essence still supervised learning methods and require relatively large amounts of labeled training data. while the use of pre-trained word-embeddings provides a convenient platform for semi-supervised learning we are still in very preliminary stages of effectively utilizing unlabeled data and reducing the reliance on annotated examples. remember that humans can often generalize from a handful of examples while neural networks usually require at least hundreds of labeled examples in order to perform well even in the most simple language tasks. finding effective ways of leveraging small amounts of labeled data together with large amounts of un-annotated data as well as generalizing across domains will likely result in another transformation of the field. finally an aspect which was only very briefly glossed over in this book is that language is not an isolated phenomena. when people learn perceive and produce language they do it with a reference to the real world and language utterances are more often than not grounded in real world entities or experiences. learning language in a grounded setting either coupled with some other modality such as images videos or robot movement control or as part of an agent that interacts with the world in order to achieve concrete goals is another promising research frontier. bibliography mart n abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg s. corrado andy davis jeffrey dean matthieu devin et al. tensorflow large-scale machine learning on heterogeneous systems httptensorflow.org heike adel ngoc ang vu and tanja schultz. combination of recurrent neural networks and factored language models for code-switching language modeling. in proc. of the annual meeting of the association for computational linguistics short papers pages sofia bulgaria august roee aharoni yoav goldberg and yonatan belinkov. proc. of the sigmorphon workshop on computational research in phonetics phonology and morphology chapter improving sequence to sequence learning for morphological inflection generation e biu-mit systems for the sigmorphon shared task for morphological reinflection pages association for computational linguistics doi roee aharoni and yoav goldberg. towards string-to-tree neural machine translation. proc. of acl m. a. aizerman e. a. braverman and l. rozonoer. eoretical foundations of the potential function method in pattern recognition learning. in automationandremotecontrol number in automation and remote control pages erin l. allwein robert e. schapire and yoram singer. reducing multiclass to binary a unifying approach for margin classifiers. journal of machine learning research rie ando and tong zhang. a high-performance semi-supervised learning method for text chunking. in proc. of the annual meeting of the association for computational linguistics pages ann arbor michigan june doi rie kubota ando and tong zhang. a framework for learning predictive structures from multiple tasks and unlabeled data. e journal of machine learning research daniel andor chris alberti david weiss aliaksei severyn alessandro presta kuzman ganchev slav petrov and michael collins. globally normalized transition-based neural networks. in proc. of the annual meeting of the association for computational linguistics long papers pages doi bibliography michael auli and jianfeng gao. decoder integration and expected bleu training for recurrent neural network language models. in proc. of the annual meeting of the association for computational linguistics short papers pages baltimore maryland june doi michael auli michel galley chris quirk and geoffrey zweig. joint language and translation modeling with recurrent neural networks. in proc. of the conference on empirical methods in natural language processing pages seattle washington. association for computational linguistics october oded avraham and yoav goldberg. e interplay of semantics and morphology in word embed dings. eacl dzmitry bahdanau kyunghyun cho and yoshua bengio. neural machine translation by jointly learning to align and translate. stat september miguel ballesteros chris dyer and noah a. smith. improved transition-based parsing by modeling characters instead of words with lstms. in proc. of the conference on empirical methods in natural language processing pages lisbon portugal. association for computational linguistics september doi miguel ballesteros yoav goldberg chris dyer and noah a. smith. training with exploration improves a greedy stack-lstm parser emnlp march doi mohit bansal kevin gimpel and karen livescu. tailoring continuous word representations for dependency parsing. in proc. of the annual meeting of the association for computational linguistics short papers pages baltimore maryland june doi marco baroni and alessandro lenci. for corpus-based semantics. distributional memory a general framework computational linguistics doi atilim gunes baydin barak a. pearlmutter alexey andreyevich radul and jeffrey mark siskind. automatic differentiation in machine learning a survey. february emily m. bender. linguistic fundamentals for natural language processing essentials from morphology and syntax. synthesis lectures on human language technologies. morgan claypool publishers samy bengio oriol vinyals navdeep jaitly and noam shazeer. scheduled sampling for sequence prediction with recurrent neural networks. corr http bibliography yoshua bengio. practical recommendations for gradient-based training of deep architectures. june doi yoshua bengio r jean ducharme pascal vincent and christian janvin. a neural probabilistic issn journal of machine learning research march language model. doi yoshua bengio j r me louradour ronan collobert and jason weston. curriculum learning. in proc. of the annual international conference on machine learning pages acm doi yoshua bengio ian j. goodfellow and aaron courville. deep learning. mit press james bergstra olivier breuleux fr d ric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley and yoshua bengio. eano a cpu and gpu math expression compiler. in proc.ofthepythonforscientificcomputingconferencescipy june jeff a. bilmes and katrin kirchhoff. factored language models and generalized parallel in companion volume of the proc. of hlt-naacl short papers doi backoff. zsolt bitvai and trevor cohn. non-linear text regression with a deep convolutional neural network. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing short papers pages beijing china july doi tolga bolukbasi kai-wei chang james y. zou venkatesh saligrama and adam tauman kalai. quantifying and reducing stereotypes in word embeddings. corr bernhard e. boser isabelle m. guyon and vladimir n. vapnik. a training algorithm for optimal margin classifiers. in proc. of the annual acm workshop on computational learning eory pages acm press doi jan a. botha and phil blunsom. compositional morphology for word representations and language modelling. in proc. of the international conference on machine learning beijing china june l on bottou. stochastic gradient descent tricks. in neural networks tricks of the trade pages springer doi r. samuel bowman gabor angeli christopher potts and d. christopher manning. a large annotated corpus for learning natural language inference. in methods in natural language processing pages association for computational linguistics doi bibliography peter brown peter desouza robert mercer t. watson vincent della pietra and jenifer lai. class-based n-gram models of natural language. computational linguistics december john a. bullinaria and joseph p. levy. extracting semantic representations from word cooccurrence statistics a computational study. behavior research methods doi a. caliskan-islam j. j. bryson and a. narayanan. semantics derived automatically from lan guage corpora necessarily contain human biases. corr rich caruana. multitask learning. machine learning doi eugene charniak and mark johnson. coarse-to-fine n-best parsing and maxent disin proc. of the annual meeting of the association for comcriminative reranking. putational linguistics pages ann arbor michigan june doi danqi chen and christopher manning. a fast and accurate dependency parser using neural networks. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi stanley f. chen and joshua goodman. an empirical study of smoothing techniques for language modeling. in annual meeting of the association for computational linguistics http doi stanley f. chen and joshua goodman. an empirical study of smoothing techniques for language modeling. computer speech and language doi wenlin chen david grangier and michael auli. strategies for training large vocabulary neural language models. in proc. of the annual meeting of the association for computational linguistics long papers pages httpaclweb.organthology doi yubo chen liheng xu kang liu daojian zeng and jun zhao. event extraction via dynamic multi-pooling convolutional neural networks. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi kyunghyun cho. natural bibliography language understanding with distributed representation. stat november kyunghyun cho bart van merrienboer dzmitry bahdanau and yoshua bengio. on the properties of neural machine translation encoder-decoder approaches. in proc. of workshop on syntax semantics and structure in statistical translation pages doha qatar. association for computational linguistics october doi kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio. learning phrase representations using rnn encoderdecoder for statistical machine translation. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi do kook choe and eugene charniak. parsing as language modeling. in proc. of the conference on empirical methods in natural language processing pages austin texas. association for computational linguistics november doi grzegorz chrupala. normalizing tweets with edit scripts and recurrent neural embeddings. in proc. of the annual meeting of the association for computational linguistics short papers pages baltimore maryland june doi junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio. empirical evaluation of gated recurrent neural networks on sequence modeling. december junyoung chung kyunghyun cho and yoshua bengio. a character-level decoder without explicit segmentation for neural machine translation. in proc. of the annual meeting of the association for computational linguistics long papers pages doi kenneth ward church and patrick hanks. word association norms mutual information and lexicography. computational linguistics doi kevin clark and christopher d. manning. improving coreference resolution by learning entityin association for computational linguistics level distributed representations. doi michael collins. discriminative training methods for hidden markov models eory and experiments with perceptron algorithms. in proc. of the conference on empirical methods in natural language processing pages association for computational linguistics july doi bibliography michael collins and terry koo. computational linguistics march ing. discriminative reranking for natural language parsissn doi ronan collobert and jason weston. a unified architecture for natural language processing deep neural networks with multitask learning. in learning pages acm doi ronan collobert jason weston l on bottou michael karlen koray kavukcuoglu and pavel kuksa. natural language processing from scratch. e journal of machine learning research alexis conneau holger schwenk lo c barrault and yann lecun. very deep convolutional networks for natural language processing. corr httparxiv.org ryan cotterell and hinrich schutze. morphological word embeddings. naacl ryan cotterell christo kirov john sylak-glassman david yarowsky jason eisner and mans hulden. proc. of the sigmorphon workshop on computational research in phonetics phonologyandmorphology chapter e sigmorphon shared task morphological reinflection pages association for computational linguistics httpaclweb doi koby crammer and yoram singer. on the algorithmic implementation of multiclass kernel based vector machines. e journal of machine learning research mathias creutz and krista lagus. unsupervised models for morpheme segmentation and morphology learning. acmtransactionsofspeechandlanguageprocessing february issn doi james cross and liang huang. incremental parsing with minimal features using bi-directional lstm. in proc. of the annual meeting of the association for computational linguistics short papers pages doi james cross and liang huang. span-based constituency parsing with a structure-label sysin proc. of the conference on empirical methods in natutem and dynamic oracles. ral language processing association for computational linguistics doi g. cybenko. approximation by superpositions of a sigmoidal function. mathematics of control signals and systems december issn doi bibliography ido dagan and oren glickman. probabilistic textual entailment generic applied modeling of language variability. in pascal workshop on learning methods for text understanding and mining ido dagan fernando pereira and lillian lee. similarity-based estimation of word cooccurrence probabilities. in acl doi ido dagan oren glickman and bernardo magnini. e pascal recognising textual entailment challenge. in machine learning challenges evaluating predictive uncertainty visual object classification and recognizing textual entailment first pascal machine learning challenges workshop mlcw pages southampton uk april selected papers. doi ido dagan dan roth mark sammons and fabio massimo zanzotto. recognizing textual entailment models and applications. synthesis lectures on human language technologies. morgan claypool publishers doi g. e. dahl t. n. sainath and g. e. hinton. improving deep neural networks for lvcsr in ieee international conference on acoususing rectified linear units and dropout. tics speech and signal processing pages may doi hal daum iii john langford and daniel marcu. search-based structured prediction. machine learning journal doi hal daum iii. a course in machine learning. self published yann n. dauphin razvan pascanu caglar gulcehre kyunghyun cho surya ganguli and yoshua bengio. identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. in z. ghahramani m. welling c. cortes n. d. lawrence and k. q. weinberger eds. advances in neural information processing systems pages curran associates inc. adri de gispert gonzalo iglesias and bill byrne. fast and accurate preordering for smt using neural networks. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages denver colorado doi jacob devlin rabih zbib zhongqiang huang omas lamar richard schwartz and john makhoul. fast and robust neural network joint models for statistical machine translation. in proc. of the annual meeting of the association for computational linguistics longpapers pages baltimore maryland june doi bibliography trinh do ierry arti and others. neural conditional random fields. in international conference on artificial intelligence and statistics pages pedro domingos. e master algorithm. basic books li dong furu wei chuanqi tan duyu tang ming zhou and ke xu. adaptive recursive neural in proc. of the annual network for target-dependent twitter sentiment classification. meeting of the association for computational linguistics short papers pages baltimore maryland june doi li dong furu wei ming zhou and ke xu. question answering over freebase with multiin proc. of the annual meeting of the associacolumn convolutional neural networks. tion for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi cicero dos santos and maira gatti. deep convolutional neural networks for sentiment analysis of short texts. in proc. of coling the international conference on computational linguistics technical papers pages dublin city university dublin ireland. association for computational linguistics august cicero dos santos and bianca zadrozny. learning character-level representations for part-ofin proc. of the international conference on machine learning speech tagging. pages cicero dos santos bing xiang and bowen zhou. classifying relations by ranking with convolutional neural networks. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi john duchi elad hazan and yoram singer. adaptive subgradient methods for online learning and stochastic optimization. e journal of machine learning research kevin duh graham neubig katsuhito sudoh and hajime tsukada. adaptation data selection using neural language models experiments in machine translation. in proc. of the annual meeting of the association for computational linguistics short papers pages sofia bulgaria august greg durrett and dan klein. neural crf parsing. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural languageprocessing pages beijing china july doi bibliography chris dyer victor chahuneau and a. noah smith. a simple fast and effective reparameterization of ibm model in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages chris dyer miguel ballesteros wang ling austin matthews and noah a. smith. transitionbased dependency parsing with stack long short-term memory. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi c. eckart and g. young. e approximation of one matrix by another of lower rank. psychome trika doi jason eisner and giorgio satta. efficient parsing for bilexical context-free grammars and head automaton grammars. in doi jeffrey l. elman. finding structure in time. cognitive science march issn doi martin b. h. everaert marinus a. c. huybregts noam chomsky robert c. berwick and johan j. bolhuis. structures not strings linguistics as part of the cognitive sciences. trends in cognitive sciences doi manaal faruqui and chris dyer. improving vector space word representations using multilingual correlation. in proc. of the conference of the european chapter of the association for computational linguistics pages gothenburg sweden april doi manaal faruqui jesse dodge kumar sujay jauhar chris dyer eduard hovy and a. noah in proc. of the conference smith. retrofitting word vectors to semantic lexicons. of the north american chapter of the association for computational linguistics human language technologies pages doi manaal faruqui yulia tsvetkov graham neubig and chris dyer. morphological inflection in proc. of the conference generation using character sequence to sequence learning. of the north american chapter of the association for computational linguistics human language technologies pages doi christiane fellbaum. wordnet an electronic lexical database. bradford books bibliography jessica ficler and yoav goldberg. a neural network for coordination boundary prediction. in proc. of the conference on empirical methods in natural language processing pages austin texas. association for computational linguistics november httpsaclweb doi katja filippova and yasemin altun. overcoming the lack of parallel data in sentence compresin proc. of the conference on empirical methods in natural language processing sion. pages association for computational linguistics httpaclweb.organt katja filippova enrique alfonseca carlos a. colmenares lukasz kaiser and oriol vinyals. sentence compression by deletion with lstms. in proc. of the conference on empirical methods in natural language processing pages lisbon portugal. association for computational linguistics september doi charles j. fillmore josef ruppenhofer and collin f. baker. framenet and representing the link between semantic and syntactic relations. language and linguistics monographs series b pages institute of linguistics academia sinica taipei john r. firth. a synopsis of linguistic theory in studies in linguistic analysis special volume of the philological society pages firth john rupert haas william halliday michael a. k. oxford blackwell ed. john r. firth. e technique of semantics. transactions of the philological society issn doi mikel l. forcada and ram n p. eco. recursive hetero-associative memories for translation. in biological and artificial computation from neuroscience to technology pages springer doi philip gage. a new algorithm for data compression. c users journal february issn yarin gal. a theoretically grounded application of dropout in recurrent neural networks. corr december kuzman ganchev and mark dredze. proc. of the hlt workshop on mobile language processing chapter small statistical models by random feature mixing pages association for computational linguistics juri ganitkevitch benjamin van durme and chris callison-burch. ppdb e paraphrase database. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages httpaclw bibliography jianfeng gao patrick pantel michael gamon xiaodong he and li deng. modeling interestingness with deep neural networks. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi dan gillick cliff brunk oriol vinyals and amarnag subramanya. multilingual language processing from bytes. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages doi jes s gim nez and lluis m rquez. svmtool a general pos tagger generator based on support vector machines. in proc. of the lrec lisbon portugal xavier glorot and yoshua bengio. understanding the difficulty of training deep feedforward neural networks. in international conference on artificial intelligence and statistics pages xavier glorot antoine bordes and yoshua bengio. deep sparse rectifier neural networks. in international conference on artificial intelligence and statistics pages yoav goldberg. a primer on neural network models for natural language processing. journal of artificial intelligence research yoav goldberg and michael elhadad. an efficient algorithm for easy-first non-directional dependency parsing. in human language technologies e annual conference of the north american chapter of the association for computational linguistics pages los angeles california june yoav goldberg and joakim nivre. training deterministic parsers with non-deterministic oracles. transactionsoftheassociationforcomputationallinguistics october issn yoav goldberg kai zhao and liang huang. efficient implementation of beam-search inin proc. of the annual meeting of the association for computational cremental parsers. linguistics short papers pages sofia bulgaria august christoph goller and andreas k chler. learning task-dependent distributed representations by backpropagation through structure. in in proc. of the pages ieee hila gonen and yoav goldberg. semi supervised preposition-sense disambiguation using multilingual data. in proc. of coling the international conference on computational linguistics technical papers pages osaka japan december e coling organizing committee. bibliography joshua goodman. a bit of progress in language modeling. corr http doi stephan gouws yoshua bengio and greg corrado. bilbowa fast bilingual distributed representations without word alignments. in proc. of the international conference on machine learning pages a. graves. supervised sequence labelling with recurrent neural networks. ph.d. thesis technis che universit t m nchen doi alex graves greg wayne and ivo danihelka. neural turing machines. corr edward grefenstette karl moritz hermann mustafa suleyman and phil blunsom. learning to transduce with unbounded memory. in c. cortes n. d. lawrence d. d. lee m. sugiyama and r. garnett eds. advances in neural information processing systems pages curran associates inc. klaus greff rupesh kumar srivastava jan koutn k bas r. steunebrink and j rgen march doi lstm a search space odyssey. huber. michael gutmann and aapo hyv rinen. noise-contrastive estimation a new estimation principle for unnormalized statistical models. in international conference on artificial intelligence and statistics pages distributional word doi zellig harris. structure. kazuma hashimoto makoto miwa yoshimasa tsuruoka and takashi chikayama. simple cusin proc. of the tomization of recursive neural networks for semantic relation classification. conference on empirical methods in natural language processing pages seattle washington. association for computational linguistics october kaiming he xiangyu zhang shaoqing ren and jian sun. delving deep into rectifiers surpassing human-level performance on imagenet classification. february doi kaiming he xiangyu zhang shaoqing ren and jian sun. deep residual learning for image recognition. in e ieee conference on computer vision and pattern recognition june doi bibliography matthew henderson blaise omson and steve young. deep neural network approach for the dialog state tracking challenge. in proc. of the sigdial conference pages metz france. association for computational linguistics august karl moritz hermann and phil blunsom. e role of syntax in vector space models of compositional semantics. in proc. of the annual meeting of the association for computational linguistics long papers pages sofia bulgaria august karl moritz hermann and phil blunsom. multilingual models for compositional distributed semantics. in proc. of the annual meeting of the association for computational linguistics pages baltimore maryland june doi salah el hihi and yoshua bengio. hierarchical recurrent neural networks for long-term dependencies. in d. s. touretzky m. c. mozer and m. e. hasselmo eds. advances in neural information processing systems pages mit press felix hill kyunghyun cho sebastien jean coline devin and yoshua bengio. embedding word similarity with neural machine translation. december geoffrey e. hinton j. l. mcclelland and d. e. rumelhart. distributed representations. in d. e. rumelhart j. l. mcclelland et al. eds. parallel distributed processing volume foundations pages mit press cambridge geoffrey e. hinton nitish srivastava alex krizhevsky ilya sutskever and ruslan r. salakhutdinov. improving neural networks by preventing co-adaptation of feature detectors. july sepp hochreiter and j rgen schmidhuber. long short-term memory. neural computation doi julia hockenmaier. data and models for statistical parsing with combinatory categorial grammar. ph.d. thesis university of edinburgh doi kurt hornik maxwell stinchcombe and halbert white. multilayer feedforward networks are universal approximators. neural networks issn doi dirk hovy stephen tratz and eduard hovy. what s in a preposition? dimensions of sense disambiguation for an interesting word class. in coling posters pages beijing china august coling organizing committee. httpwww.aclweb.organthologyc bibliography ting-hao huang francis ferraro nasrin mostafazadeh ishan misra aishwarya agrawal jacob devlin ross girshick xiaodong he pushmeet kohli dhruv batra lawrence c. zitnick devi parikh lucy vanderwende michel galley and margaret mitchell. visual storytelling. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages doi liang huang suphan fayong and yang guo. structured perceptron with inexact search. in proc.oftheconferenceofthenorthamericanchapteroftheassociationforcomputationallinguisticshumanlanguagetechnologies pages sergey ioffe and christian szegedy. batch normalization accelerating deep network training by reducing internal covariate shift. february ozan irsoy and claire cardie. opinion mining with deep recurrent neural networks. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi mohit iyyer jordan boyd-graber leonardo claudino richard socher and hal daum iii. a neural network for factoid question answering over paragraphs. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi mohit iyyer peter enns jordan boyd-graber and philip resnik. political ideology detection using recursive neural networks. in proc. of the annual meeting of the association for computational linguistics long papers pages baltimore maryland june doi mohit iyyer varun manjunatha jordan boyd-graber and hal daum iii. deep unordered composition rivals syntactic methods for text classification. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi s bastien jean kyunghyun cho roland memisevic and yoshua bengio. on using very large target vocabulary for neural machine translation. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages httpaclweb.organtho doi frederick jelinek and robert mercer. interpolated estimation of markov source parameters from sparse data. in workshop on pattern recognition in practice bibliography rie johnson and tong zhang. effective use of word order for text categorization with convolutional neural networks. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages denver colorado doi aravind k. joshi and bangalore srinivas. disambiguation of super parts of speech supertags allnost parsing. in coling volume e international conference on computational linguistics doi armand joulin edouard grave piotr bojanowski and tomas mikolov. bag of tricks for efficient text classification. corr rafal jozefowicz wojciech zaremba and ilya sutskever. an empirical exploration of recurrent network architectures. in proc. of the international conference on machine learning pages rafal jozefowicz oriol vinyals mike schuster noam shazeer and yonghui wu. exploring the limits of language modeling. february daniel jurafsky and james h. martin. speech and language processing ed. prentice hall nal kalchbrenner edward grefenstette and phil blunsom. a convolutional neural network for modelling sentences. in proc. of the annual meeting of the association for computational linguistics long papers pages baltimore maryland june doi nal kalchbrenner lasse espeholt karen simonyan a ron van den oord alex graves and koray kavukcuoglu. neural machine translation in linear time. corr katharina kann and hinrich sch tze. proc. of the sigmorphon workshop on computational research in phonetics phonology and morphology chapter med e lmu system for the sigmorphon shared task on morphological reinflection pages association for computational linguistics doi anjuli kannan karol kurach sujith ravi tobias kaufmann andrew tomkins balint miklos greg corrado laszlo lukacs marina ganea peter young and vivek ramavajjala. smart reply automated response suggestion for email. in proc. of the acm sigkdd conference on knowledge discovery and data mining doi bibliography andrej karpathy and fei-fei li. deep visual-semantic alignments for generating image descriptions. in ieee conference on computer vision and pattern recognition cvpr pages boston ma june doi andrej karpathy justin johnson and fei-fei li. visualizing and understanding recurrent net works. june douwe kiela and stephen clark. a systematic study of semantic vector space model parameters. in workshop on continuous vector space models and their compositionality doi yoon kim. convolutional neural networks for sentence classification. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi yoon kim yacine jernite david sontag and alexander m. rush. character-aware neural lan guage models. stat august diederik kingma and jimmy ba. december adam a method for stochastic optimization. eliyahu kiperwasser and yoav goldberg. easy-first dependency parsing with hierarchical tree lstms. transactions of the association of computational linguistics issue pages eliyahu kiperwasser and yoav goldberg. simple and accurate dependency parsing using bidirectional lstm feature representations. transactions of the association of computational linguistics issue pages karin kipper hoa t. dang and martha palmer. class-based construction of a verb lexicon. in aaaiiaai pages ryan kiros yukun zhu ruslan r salakhutdinov richard zemel raquel urtasun antonio torralba and sanja fidler. skip-thought vectors. in c. cortes n. d. lawrence d. d. lee m. sugiyama and r. garnett eds. advances in neural information processing systems pages curran associates inc. sigrid klerke yoav goldberg and anders s gaard. improving sentence compression by learning to predict gaze. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages httpac doi reinhard kneser and hermann ney. bibliography improved backing-off for m-gram language modeling. in acoustics speech and signal processing international conference on volume pages may doi philipp koehn. europarl a parallel corpus for statistical machine translation. in proc. of mt summit volume pages philipp koehn. statistical machine translation. cambridge university press doi terry koo and michael collins. efficient third-order dependency parsers. in proc. of the annual meeting of the association for computational linguistics pages httpac moshe koppel jonathan schler and shlomo argamon. computational methods in authorship attribution. journal of the american society for information science and technology doi alex krizhevsky ilya sutskever and geoffrey e. hinton. imagenet classification with deep convolutional neural networks. in f. pereira c. j. c. burges l. bottou and k. q. weinberger eds. advances in neural information processing systems pages curran associates inc. doi r. a. kronmal and a. v. peterson jr. on the alias method for generating random variables from a discrete distribution. e american statistician doi sandra k bler ryan mcdonald and joakim nivre. dependency parsing. synthesis lectures on human language technologies. morgan claypool publishers doi taku kudo and yuji matsumoto. fast methods for kernel-based text analysis. in proc. of the annual meeting on association for computational linguistics pages stroudsburg pa doi john lafferty andrew mccallum and fernando cn pereira. conditional random fields prob abilistic models for segmenting and labeling sequence data. in proc. of icml guillaume lample miguel ballesteros sandeep subramanian kazuya kawakami and chris dyer. neural architectures for named entity recognition. in proc. of the conference of the north americanchapteroftheassociationforcomputationallinguisticshumanlanguagetechnologies pages doi bibliography phong le and willem zuidema. e inside-outside recursive neural network model for dependency parsing. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi phong le and willem zuidema. e forest convolutional network compositional distributional semantics with a neural chart and without binarization. in proc. of the conference on empirical methods in natural language processing pages lisbon portugal. association for computational linguistics september doi quoc v. le navdeep jaitly and geoffrey e. hinton. a simple way to initialize recurrent net works of rectified linear units. april yann lecun and yoshua bengio. convolutional networks for images speech and time-series. in m. a. arbib ed. e handbook of brain eory and neural networks. mit press yann lecun leon bottou g. orr and k. muller. efficient backprop. in g. orr and muller k eds. neural networks tricks of the trade. springer doi yann lecun leon bottou yoshua bengio and patrick haffner. gradient based learning applied to pattern recognition. proc. of the ieee november yann lecun and f. huang. loss functions for discriminative training of energy-based models. in proc. of aistats yann lecun sumit chopra raia hadsell m. ranzato and f. huang. a tutorial on energy based learning. predicting structured data geunbae lee margot flowers and michael g. dyer. learning distributed representations of conceptual knowledge and their application to script-based story processing. in connectionist natural language processing pages springer doi moshe leshno vladimir ya. lin allan pinkus and shimon schocken. multilayer feedforward networks with a nonpolynomial activation function can approximate any function. neural networks issn httpwww.sciencedirect.comscience doi omer levy and yoav goldberg. dependency-based word embeddings. in proc. of the annual meeting of the association for computational linguistics short papers pages baltimore maryland june doi omer levy and yoav goldberg. linguistic regularities in sparse and explicit word representations. in proc. of the conference on computational natural language learning pages bibliography association for computational linguistics doi omer levy and yoav goldberg. neural word embedding as implicit matrix factorization. in z. ghahramani m. welling c. cortes n. d. lawrence and k. q. weinberger eds. advances in neural information processing systems pages curran associates inc. omer levy yoav goldberg and ido dagan. improving distributional similarity with lessons learned from word embeddings. transactions of the association for computational linguistics may issn omer levy anders s gaard and yoav goldberg. a strong baseline for learning cross-lingual word embeddings from sentence alignments. in proc. of the conference of the european chapter of the association for computational linguistics mike lewis and mark steedman. improved ccg parsing with semi-supervised supertagging. transactionsoftheassociationforcomputationallinguistics october issn mike lewis kenton lee and luke zettlemoyer. lstm ccg parsing. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages doi jiwei li rumeng li and eduard hovy. recursive deep models for discourse parsing. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi jiwei li ang luong dan jurafsky and eduard hovy. when are tree structures necessary for deep learning of representations? in proc. of the conference on empirical methods in natural languageprocessing associationfor computationallinguistics http doi jiwei li michel galley chris brockett georgios spithourakis jianfeng gao and bill dolan. a persona-based neural conversation model. in proc. of the annual meeting of the association for computational linguistics long papers pages httpaclw doi g. j. lidstone. note on the general case of the bayes-laplace formula for inductive or a posteriori probabilities. transactions of the faculty of actuaries bibliography wang ling chris dyer alan w. black and isabel trancoso. twotoo simple adaptations of in proc. of the conference of the north american chapter of for syntax problems. the association for computational linguistics human language technologies pages denver colorado doi wang ling chris dyer alan w. black isabel trancoso ramon fermandez silvio amir luis marujo and tiago luis. finding function in form compositional character models for open vocabulary word representation. in proc. of the conference on empirical methods in natural language processing pages lisbon portugal. association for computational linguistics september doi tal linzen emmanuel dupoux and yoav goldberg. assessing the ability of lstms to learn syntax-sensitive dependencies. transactions of the association for computational linguistics issn httpswww.transacl.orgojsindex.phptaclarticle ken litkowski and orin hargraves. e preposition project. in proc. of the acl-sigsem workshop on the linguistic dimensions of prepositions and eir use in computational linguistics formalisms and applications pages ken litkowski and orin hargraves. task word-sense disambiguation of prepositions. in proc. of the international workshop on semantic evaluations pages doi yang liu furu wei sujian li heng ji ming zhou and houfeng wang. a dependencyin proc. of the annual meeting of the based neural network for relation classification. association for computational linguistics and the international joint conference on natural languageprocessing minh- ang luong hieu pham and christopher d. manning. effective approaches to attention-based neural machine translation. august minh- ang luong quoc v. le ilya sutskever oriol vinyals and lukasz kaiser. multi-task sequence to sequence learning. in proc. of iclr ji ma yue zhang and jingbo zhu. tagging the web building a robust web tagger with in proc. of the annual meeting of the association for computational neural network. linguistics long papers pages baltimore maryland june doi mingbo ma liang huang bowen zhou and bing xiang. dependency-based convolutional neural networks for sentence embedding. in proc. of the annual meeting of the associ bibliography ation for computational linguistics and the international joint conference on natural language processing short papers pages beijing china july doi xuezhe ma and eduard hovy. end-to-end sequence labeling via bi-directional lstm-cnnscrf. in proc. of the annual meeting of the association for computational linguistics long papers pages berlin germany august httpwww.aclw doi christopher manning and hinrich sch tze. foundations of statistical natural language process ing. mit press christopher manning prabhakar raghavan and hinrich sch tze. introduction to information retrieval. cambridge university press doi junhua mao wei xu yi yang jiang wang and alan l. yuille. explain images with multimodal recurrent neural networks. corr ryan mcdonald koby crammer and fernando pereira. online large-margin training of dein proc. of the annual meeting of the association for computational pendency parsers. linguistics pages doi ryan mcdonald joakim nivre yvonne quirmbach-brundage yoav goldberg dipanjan das kuzman ganchev keith b. hall slav petrov hao zhang oscar t ckstr m claudia bedini n ria bertomeu castell and jungmee lee. universal dependency annotation for multilingual parsing. in acl pages tom mikolov. statisticallanguagemodelsbasedonneuralnetworks. ph.d. thesis brno university of technology tom mikolov. martin karafi t lukas burget jan cernocky and sanjeev khudanpur. recurrent neural network based language model. in interspeech annual conference of the international speech communication association pages makuhari chiba japan september tom mikolov stefan kombrink luk burget jan honza ernocky and sanjeev khudanpur. extensions of recurrent neural network language model. in acoustics speech and signal processing ieee international conference on pages doi tom mikolov. kai chen greg corrado and jeffrey dean. efficient estimation of word rep resentations in vector space. january bibliography tom mikolov. quoc v. le and ilya sutskever. exploiting similarities among languages for machine translation. corr tom mikolov. ilya sutskever kai chen greg s corrado and jeff dean. distributed representations of words and phrases and their compositionality. in c. j. c. burges l. bottou m. welling z. ghahramani and k. q. weinberger eds. advances in neural information processing systems pages curran associates inc. tom mikolov. wen-tau yih and geoffrey zweig. linguistic regularities in continuous space word representations. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages httpac tom mikolov. armand joulin sumit chopra michael mathieu and marc aurelio ranzato. learning longer memory in recurrent neural networks. december scott miller jethran guinness and alex zamanian. name tagging with word clusters and in proc. of the human language technology conference of the north discriminative training. american chapter of the association for computational linguistics hlt-naacl http andriy mnih and koray kavukcuoglu. learning word embeddings efficiently with noisecontrastive estimation. in c. j. c. burges l. bottou m. welling z. ghahramani and k. q. weinberger eds. advances in neural information processing systems pages curran associates inc. andriy mnih and yee whye teh. a fast and simple algorithm for training neural probabilistic language models. in john langford and joelle pineau eds. proc. of the international conference on machine learning pages new york ny july omnipress. mehryar mohri afshin rostamizadeh and ameet talwalkar. foundations of machine learning. mit press frederic morin and yoshua bengio. hierarchical probabilistic neural network language model. in robert g. cowell and zoubin ghahramani eds. proc. of the international workshop on artificial intelligence and statistics pages httpwww.iro.umontreal.ca nikola mrk i diarmuid s aghdha blaise omson milica gasic pei-hao su david vandyke tsung-hsien wen and steve young. multi-domain dialog state tracking using bibliography recurrent neural networks. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing short papers pages beijing china. association for computational linguistics july doi masami nakamura and kiyohiro shikano. a study of english word category prediction based on neural networks. e journal of the acoustical society of america doi r. neidinger. programming. introduction to automatic differentiation and matlab object-oriented siam review january issn doi y. nesterov. a method of solving a convex programming problem with convergence rate o in soviet mathematics doklady y. nesterov. introductory lectures on convex optimization. kluwer academic publishers doi graham neubig chris dyer yoav goldberg austin matthews waleed ammar antonios anastasopoulos miguel ballesteros david chiang daniel clothiaux trevor cohn kevin duh manaal faruqui cynthia gan dan garrette yangfeng ji lingpeng kong adhiguna kuncoro gaurav kumar chaitanya malaviya paul michel yusuke oda matthew richardson naomi saphra swabha swayamdipta and pengcheng yin. dynet e dynamic neural network toolkit. corr ien huu nguyen and ralph grishman. event detection and domain adaptation with convolutional neural networks. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing short papers pages beijing china july doi joakim nivre. algorithms for deterministic incremental dependency parsing. computaissn doi tional linguistics december joakim nivre eljko agi maria jesus aranzabe masayuki asahara aitziber atutxa miguel ballesteros john bauer kepa bengoetxea riyaz ahmad bhat cristina bosco sam bowman giuseppe g. a. celano miriam connor marie-catherine de marneffe arantza diaz de ilarraza kaja dobrovoljc timothy dozat toma erjavec rich rd farkas jennifer foster daniel galbraith filip ginter iakes goenaga koldo gojenola yoav goldberg berta gonzales bruno guillaume jan haji dag haug radu ion elena irimia anders johannsen hiroshi kanayama jenna kanerva simon krek veronika laippala alessandro lenci nikola bibliography ljube i teresa lynn christopher manning c t lina m r nduc david mare ek h ctor mart nez alonso jan ma ek yuji matsumoto ryan mcdonald anna missil verginica mititelu yusuke miyao simonetta montemagni shunsuke mori hanna nurmi petya osenova lilja vrelid elena pascual marco passarotti cenel-augusto perez slav petrov jussi piitulainen barbara plank martin popel prokopis prokopidis sampo pyysalo loganathan ramasamy rudolf rosa shadi saleh sebastian schuster wolfgang seeker mojgan seraji natalia silveira maria simi radu simionescu katalin simk kiril simov aaron smith jan t p nek alane suhr zsolt sz nt takaaki tanaka reut tsarfaty sumire uematsu larraitz uria viktor varga veronika vincze zden k abokrtsk daniel zeman and hanzhi zhu. universal dependencies lindatclarin digital library at institute of formal and applied linguistics charles university in prague. chris okasaki. purely functional data structures. cambridge university press cambridge uk june doi mitchell p. marcus beatrice santorini and mary ann marcinkiewicz. building a large annotated corpus of english e penn treebank. computational linguistics june special issue on using large corpora ii martha palmer daniel gildea and nianwen xue. semantic role labeling. synthesis lectures on human language technologies. morgan claypool publishers doi bo pang and lillian lee. opinion mining and sentiment analysis. foundation and trends in information retrieval doi ankur p. parikh oscar t ckstr m dipanjan das and jakob uszkoreit. a decomposable attention model for natural language inference. in proc. of emnlp doi razvan pascanu tomas mikolov and yoshua bengio. on the difficulty of training recurrent neural networks. november ellie pavlick pushpendre rastogi juri ganitkevitch benjamin van durme and chris callisonburch. ppdb better paraphrase ranking fine-grained entailment relations word embeddings and style classification. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing short papers pages association for computational linguistics doi wenzhe pei tao ge and baobao chang. an effective neural network model for graph-based dependency parsing. in proc. of the annual meeting of the association for computational lin bibliography guistics and the international joint conference on natural language processing long papers pages beijing china july doi joris pelemans noam shazeer and ciprian chelba. sparse non-negative matrix language modeling. transactions of the association of computational linguistics jian peng liefeng bo and jinbo xu. conditional neural fields. in y. bengio d. schuurmans j. d. lafferty c. k. i. williams and a. culotta eds. advances in neural information processing systems pages curran associates inc. jeffrey pennington richard socher and christopher manning. glove global vectors for word representation. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi vu pham christopher kermorvant and j r me louradour. dropout improves recurrent neural networks for handwriting recognition. corr httparxiv.orgabs doi barbara plank anders s gaard and yoav goldberg. multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss. in proc. of the annual meeting of the association for computational linguistics short papers pages association for computational linguistics doi jordan b. pollack. recursive distributed representations. artificial intelligence doi b. t. polyak. some methods of speeding up the convergence of iteration methods. ussr computational mathematics and mathematical physics issn doi qiao qian bo tian minlie huang yang liu xuan zhu and xiaoyan zhu. learning tag embeddings and tag-specific composition functions in recursive neural network. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi lev ratinov and dan roth. proc. of the conference on computational natural language learning chapter design challenges and misconceptions in named entity recognition pages association for computational linguistics httpaclw bibliography ronald rosenfeld. a maximum entropy approach to adaptive statistical language modeling. computer speech and language longe version carnegie mellon technical report doi st phane ross and j. andrew bagnell. efficient reductions for imitation learning. in proc. of the international conference on artificial intelligence and statistics pages st phane ross geoffrey j. gordon and j. andrew bagnell. a reduction of imitation learning and structured prediction to no-regret online learning. in proc. of the international conference on artificial intelligence and statistics pages david e. rumelhart geoffrey e. hinton and ronald j. williams. learning representations by back-propagating errors. nature october doi ivan a. sag omas wasow and emily m. bender. syntactic eory ed. csli lecture note magnus sahlgren. e distributional hypothesis. italianjournaloflinguistics nathan schneider vivek srikumar jena d. hwang and martha palmer. a hierarchy with of and for preposition supersenses. in proc. of the linguistic annotation workshop pages doi nathan schneider jena d. hwang vivek srikumar meredith green abhijit suresh kathryn conger tim o gorman and martha palmer. a corpus of preposition supersenses. in proc. of the linguistic annotation workshop doi bernhard sch lkopf. e kernel trick for distances. in t. k. leen t. g. dietterich and v. tresp eds. advances in neural information processing systems pages mit press m. schuster and kuldip k. paliwal. bidirectional recurrent neural networks. actions on signal processing november ieee transissn doi holger schwenk daniel dchelotte and jean-luc gauvain. continuous space language modin proc. of the colingacl on main conferels for statistical machine translation. ence poster sessions pages association for computational linguistics doi rico sennrich and barry haddow. proc. of the conference on machine translation chapter linguistic input features improve neural machine translation pages association for computational linguistics httpaclweb.organtholo doi bibliography rico sennrich barry haddow and alexandra birch. neural machine translation of rare words with subword units. in proc. of the annual meeting of the association for computational linguistics long papers pages httpaclweb.organtholo doi rico sennrich barry haddow and alexandra birch. improving neural machine translation models with monolingual data. in proc. of the annual meeting of the association for computational linguistics long papers pages association for computational linguistics doi shai shalev-shwartz and shai ben-david. understanding machine learning from eory to algorithms. cambridge university press doi john shawe-taylor and nello cristianini. kernel methods for pattern analysis. cambridge uni versity press cambridge uk june doi q. shi j. petterson g. dror j. langford a. j. smola a. strehl and v. vishwanathan. hash kernels. in artificial intelligence and statistics aistats florida april karen simonyan and andrew zisserman. very deep convolutional networks for large-scale image recognition. in iclr noah a. smith. linguistic structure prediction. synthesis lectures on human language tech nologies. morgan claypool may doi richard socher. recursive deep learning for natural language processing and computer vision. ph.d. thesis stanford university august richard socher christopher manning and andrew ng. learning continuous phrase representations and syntactic parsing with recursive neural networks. in proc. of the deep learning and unsupervised feature learning workshop of pages richard socher cliff chiung-yu lin andrew y. ng and christopher d. manning. parsing natural scenes and natural language with recursive neural networks. in lise getoor and tobias scheffer eds. proc. of the international conference on machine learning icml pages bellevue washington june july omnipress richard socher brody huval christopher d. manning and andrew y. ng. semantic compositionality through recursive matrix-vector spaces. in proc. of the joint conference on empirical methods in natural language processing and computational natural language learning pages jeju island korea. association for computational linguistics july richard socher john bauer christopher d. manning and andrew y. ng. parsing with compositional vector grammars. in proc. of the annual meeting of the association for computational linguistics long papers pages sofia bulgaria august bibliography richard socher alex perelygin jean wu jason chuang christopher d. manning andrew ng and christopher potts. recursive deep models for semantic compositionality over a sentiment treebank. in proc. of the conference on empirical methods in natural language processing pages seattle washington. association for computational linguistics october anders s gaard. semi-supervised learning and domain adaptation in natural language processing. synthesis lectures on human language technologies. morgan claypool publishers doi anders s gaard and yoav goldberg. deep multi-task learning with low level tasks supervised at lower layers. in proc. of the annual meeting of the association for computational linguistics short papers pages httpaclweb.organthologyp doi alessandro sordoni michel galley michael auli chris brockett yangfeng ji margaret mitchell jian-yun nie jianfeng gao and bill dolan. a neural network approach to contextsensitive generation of conversational responses. in proc. of the conference of the north american chapter of the association for computational linguistics human language technologies pages denver colorado doi vivek srikumar and dan roth. an inventory of preposition relations. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov. dropout a simple way to prevent neural networks from overfitting. journal of machine learning research e. strubell p. verga d. belanger and a. mccallum. fast and accurate sequence labeling with iterated dilated convolutions. arxiv e-prints february martin sundermeyer ralf schl ter and hermann ney. lstm neural networks for language modeling. in interspeech martin sundermeyer tamer alkhouli joern wuebker and hermann ney. translation modeling with bidirectional recurrent neural networks. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi ilya sutskever james martens and geoffrey e. hinton. generating text with recurrent neural networks. in proc. of the international conference on machine learning pages doi ilya sutskever james martens george dahl and geoffrey hinton. on the importance of initialization and momentum in deep learning. in proc. of the international conference on machine learning pages bibliography ilya sutskever oriol vinyals and quoc v. v le. sequence to sequence learning with neural networks. in z. ghahramani m. welling c. cortes n. d. lawrence and k. q. weinberger eds. advances in neural information processing systems pages curran associates inc. kai sheng tai richard socher and christopher d. manning. improved semantic representations from tree-structured long short-term memory networks. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi akihiro tamura taro watanabe and eiichiro sumita. recurrent neural networks for word in proc. of the annual meeting of the association for computational alignment model. linguistics long papers pages baltimore maryland june doi duyu tang bing qin and ting liu. document modeling with gated recurrent neural network for sentiment classification. in proc. of the conference on empirical methods in natural language processing pages association for computational linguistics httpaclw doi matus telgarsky. benefits of depth in neural networks. stat february robert tibshirani. regression shrinkage and selection via the lasso. journal of the royal statistical society series b doi t. tieleman and g. hinton. lecture rmsprop divide the gradient by a running average of its recent magnitude. coursera neural networks for machine learning joseph turian lev-arie ratinov and yoshua bengio. word representations a simple and general method for semi-supervised learning. in proc. of the annual meeting of the association for computational linguistics pages peter d. turney. mining the web for synonyms pmi-ir vs. lsa on toefl. in ecml doi peter d. turney and patrick pantel. from frequency to meaning vector space models of seman tics. journal of artificial intelligence research jakob uszkoreit jay ponte ashok popat and moshe dubiner. large scale parallel document mining for machine translation. in proc. of the international conference on computational linguistics pages organizing committee httpaclweb.o bibliography tim van de cruys. a neural network approach to selectional preference acquisition. in proc. of the conference on empirical methods in natural language processing pages doha qatar. association for computational linguistics october doi ashish vaswani yinggong zhao victoria fossum and david chiang. decoding with largescale neural language models improves translation. in proc.oftheconferenceonempiricalmethods in natural language processing pages seattle washington. association for computational linguistics october ashishvaswani yinggongzhaovictoria fossumand david chiang. decoding with large-scale neural language models improves translation. in proc. of the conference on empirical methods in natural language processing pages association for computational linguistics ashish vaswani yonatan bisk kenji sagae and ryan musa. supertagging with lstms. in proc.oftheconferenceofthenorthamericanchapteroftheassociationforcomputationallinguistics human language technologies pages association for computational linguistics doi oriol vinyals lukasz kaiser terry koo slav petrov ilya sutskever and geoffrey hinton. grammar as a foreign language. stat december oriol vinyals alexander toshev samy bengio and dumitru erhan. show and tell a neural in ieee conference on computer vision and pattern recognition image caption generator. cvpr pages boston ma june doi stefan wager sida wang and percy s liang. dropout training as adaptive regularization. in c. j. c. burges l. bottou m. welling z. ghahramani and k. q. weinberger eds. advances in neural information processing systems pages curran associates inc. mengqiu wang and christopher d. manning. effect of non-linear deep architecture in sequence labeling. in ijcnlp pages peng wang jiaming xu bo xu chenglin liu heng zhang fangyuan wang and hongwei hao. semantic clustering and convolutional neural network for short text categorization. in joint conference on natural language processing short papers pages beijing china july doi xin wang yuanchao liu chengjie sun baoxun wang and xiaolong wang. predicting polarities of tweets by composing word embeddings with long short-term memory. in proc. of the annual meeting of the association for computational linguistics and the international bibliography joint conference on natural language processing long papers pages beijing china july doi taro watanabe and eiichiro sumita. transition-based neural constituent parsing. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi k. weinberger a. dasgupta j. attenberg j. langford and a. j. smola. feature hashing for large scale multitask learning. in international conference on machine learning doi david weiss chris alberti michael collins and slav petrov. structured training for neural network transition-based parsing. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi p. j. werbos. backpropagation through time what it does and how to do it. proc. of the ieee issn doi jason weston antoine bordes oksana yakhnenko and nicolas usunier. connecting language and knowledge bases with embedding models for relation extraction. in proc. of the conference on empirical methods in natural language processing pages seattle washington. association for computational linguistics october philip williams rico sennrich matt post and philipp koehn. syntax-based statistical machine translation. synthesis lectures on human language technologies. morgan claypool publishers doi sam wiseman and alexander m. rush. sequence-to-sequence learning as beam-search opin proc. of the conference on empirical methods in natural language processing timization. association for computational linguistics doi sam wiseman m. alexander rush and m. stuart shieber. learning global features for in proc. of the conference of the north american chapter of the assocoreference resolution. ciation for computational linguistics human language technologies pages doi yijun xiao and kyunghyun cho. efficient character-level document classification by combining convolution and recurrent layers. corr bibliography wenduan xu michael auli and stephen clark. ccg supertagging with a recurrent neural network. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing short papers pages beijing china july doi wenpeng yin and hinrich sch tze. convolutional neural network for paraphrase identificain proc. of the conference of the north american chapter of the association for compution. tational linguistics human language technologies pages denver colorado doi fisher yu and vladlen koltun. multi-scale context aggregation by dilated convolutions. in iclr wojciech zaremba ilya sutskever and oriol vinyals. recurrent neural network regularization. september matthew d. zeiler. adadelta an adaptive learning rate method. december daojian zeng kang liu siwei lai guangyou zhou and jun zhao. relation classification via convolutional deep neural network. in proc. of coling the international conference on computational linguistics technical papers pages dublin ireland dublin city university and association for computational linguistics august hao zhang and ryan mcdonald. generalized higher-order dependency parsing with cube pruning. in proc. of the joint conference on empirical methods in natural language processing andcomputationalnaturallanguagelearning pages association for computational linguistics tong zhang. statistical behavior and consistency of classification methods based on convex risk minimization. e annals of statistics doi xiang zhang junbo zhao and yann lecun. character-level convolutional networks for text classification. in c. cortes n. d. lawrence d. d. lee m. sugiyama and r. garnett eds. advances in neural information processing systems pages curran associates inc. r-text-classification.pdf xingxing zhang jianpeng cheng and mirella lapata. dependency parsing as head selection. corr yuan zhang and david weiss. stack-propagation improved representation learning for synin proc. of the annual meeting of the association for computational linguistics tax. long papers pages doi bibliography hao zhou yue zhang shujian huang and jiajun chen. a neural probabilistic structuredprediction model for transition-based dependency parsing. in proc. of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing long papers pages beijing china july doi chenxi zhu xipeng qiu xinchi chen and xuanjing huang. a re-ranking model for dependency parser with recursive convolutional neural network. in proc. of the annual meeting of language processing long papers pages beijing china july doi xiaodan zhu parinaz sobhani and hongyu guo. long short-term memory over tree structures. march hui zou and trevor hastie. regularization and variable selection via the elastic net. journal of the royal statistical society series b doi author s biography yoav goldberg yoavgoldberg has been working in natural language processing for over a decade. he is a senior lecturer at the computer science department at bar-ilan university israel. prior to that he was a researcher at google research new york. he received his ph.d. in computer science and natural language processing from ben gurion university he regularly reviews for nlp and machine learning venues and serves at the editorial board of computational linguistics. he published over research papers and received best paper and outstanding paper awards at major natural language processing conferences. his research interests include machine learning for natural language structured prediction syntactic parsing processing of morphologically rich languages and in the past two years neural network models with a focus on recurrent neural networks.