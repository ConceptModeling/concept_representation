bayesian reasoning and machine learning david barber notation list v domx x x px tr px fa px y px y px y pxy x fx i y pa ch ne x yz dim x b dim x s y t d n n y s erfx i j im ii a calligraphic symbol typically denotes a set of random variables domain of a variable the variable x is in the state x probability of eventvariable x being in the state true probability of eventvariable x being in the state false probability of x and y probability of x and y probability of x or y the probability of x conditioned on y for continuous variables this is shorthand fxdx and for discrete variables means summation over the states of x fx indicator has value if x y otherwise the parents of node x. the children of node x. neighbours of node x variables x are independent of variables y conditioned on variables z. variables x are dependent on variables y conditioned variables z. for a discrete variable x this denotes the number of states x can take the average of the function fx with respect to the distribution px. delta function. for discrete a b this is the kronecker delta ab and for continuous a b the dirac delta function b the dimension of the vectormatrix x. the number of times variable x is in state s and y in state t simultaneously. dataset data index number of dataset training points the number of times variable x is in state y sample covariance matrix the logistic sigmoid exp x the error function the set of unique neighbouring edges on a graph the m m identity matrix draft march preface machine learning the last decade has seen considerable growth in interest in artificial intelligence and machine learning. in the broadest sense these fields aim to learn something useful about the environment within which the organism operates. how gathered information is processed leads to the development of algorithms how to process high dimensional data and deal with uncertainty. in the early stages of research in machine learning and related areas similar techniques were discovered in relatively isolated research communities. whilst not all techniques have a natural description in terms of probability theory many do and it is the framework of graphical models marriage between graph and probability theory that has enabled the understanding and transference of ideas from statistical physics statistics machine learning and information theory. to this extent it is now reasonable to expect that machine learning researchers are familiar with the basics of statistical modelling techniques. this book concentrates on the probabilistic aspects of information processing and machine learning. certainly no claim is made as to the correctness or that this is the only useful approach. indeed one might counter that this is unnecessary since biological organisms don t use probability theory whether this is the case or not it is undeniable that the framework of graphical models and probability has helped with the explosion of new algorithms and models in the machine learning community. one should also be clear that bayesian viewpoint is not the only way to go about describing machine learning and information processing. bayesian and probabilistic techniques really come into their own in domains where uncertainty is a necessary consideration. the structure of the book one aim of part i of the book is to encourage computer science students into this area. a particular difficulty that many modern students face is a limited formal training in calculus and linear algebra meaning that minutiae of continuous and high-dimensional distributions can turn them away. in beginning with probability as a form of reasoning system we hope to show the reader how ideas from logical inference and dynamical programming that they may be more familiar with have natural parallels in a probabilistic context. in particular computer science students are familiar with the concept of algorithms as core. however it is more common in machine learning to view the model as core and how this is implemented is secondary. from this perspective understanding how to translate a mathematical model into a piece of computer code is central. part ii introduces the statistical background needed to understand continuous distributions and how learning can be viewed from a probabilistic framework. part iii discusses machine learning topics. certainly some readers will raise an eyebrow to see their favourite statistical topic listed under machine learning. a difference viewpoint between statistics and machine learning is what kinds of systems we would ultimately iii like to construct capable of humanbiological information processing tasks rather than in some of the techniques. this section of the book is therefore what i feel would be useful for machine learners to know. part iv discusses dynamical models in which time is explicitly considered. in particular the kalman filter is treated as a form of graphical model which helps emphasise what the model is rather than focusing on it as a filter as is more traditional in the engineering literature. part v contains a brief introduction to approximate inference techniques including both stochastic carlo and deterministic techniques. the references in the book are not generally intended as crediting authors with ideas nor are they always to the most authoritative works. rather the references are largely to works which are at a level reasonably consistent with the book and which are readily available. whom this book is for my primary aim was to write a book for final year undergraduates and graduates without significant experience in calculus and mathematics that gave an inroad into machine learning much of which is currently phrased in terms of probabilities and multi-variate distributions. the aim was to encourage students that apparently unexciting statistical concepts are actually highly relevant for research in making intelligent systems that interact with humans in a natural manner. such a research programme inevitably requires dealing with high-dimensional data time-series networks logical reasoning modelling and uncertainty. other books in this area whilst there are several excellent textbooks in this area none currently meets the requirements that i personally need for teaching namely one that contains demonstration code and gently introduces probability and statistics before leading on to more advanced topics in machine learning. this lead me to build on my lecture material from courses given at aston edinburgh epfl and ucl and expand the demonstration software considerably. the book is due for publication by cambridge university press in the literature on machine learning is vast as is the overlap with the relevant areas of statistics engineering and other physical sciences. in this respect it is difficult to isolate particular areas and this book is an attempt to integrate parts of the machine learning and statistics literature. the book is written in an informal style at the expense of rigour and detailed proofs. as an introductory textbook topics are naturally covered to a somewhat shallow level and the reader is referred to more specialised books for deeper treatments. amongst my favourites are graphical models graphical models by s. lauritzen oxford university press bayesian networks and decision graphs by f. jensen and t. d. nielsen springer verlag probabilistic networks and expert systems by r. g. cowell a. p. dawid s. l. lauritzen and d. j. spiegelhalter springer verlag probabilistic reasoning in intelligent systems by j. pearl morgan kaufmann graphical models in applied multivariate statistics by j. whittaker wiley probabilistic graphical models principles and techniques by d. koller and n. friedman mit press machine learning and information processing information theory inference and learning algorithms by d. j. c. mackay cambridge uni versity press iv draft march pattern recognition and machine learning by c. m. bishop springer verlag an introduction to support vector machines n. cristianini and j. shawe-taylor cambridge university press gaussian processes for machine learning by c. e. rasmussen and c. k. i. williams mit press how to use this book part i would be suitable for an introductory course on graphical models with a focus on inference. part ii contains enough material for a short lecture course on learning in probabilistic models. part iii is reasonably self-contained and would be suitable for a course on machine learning from a probabilistic perspective particularly combined with the dynamical models material in part iv. part v would be suitable for a short course on approximate inference. accompanying code the matlab code is provided to help readers see how mathematical models translate into actual code. the code is not meant to be an industrial strength research tool rather a reasonably lightweight toolbox that enables the reader to play with concepts in graph theory probability theory and machine learning. in an attempt to retain readability no extensive error andor exception handling has been included. the code contains at the moment basic routines for manipulating discrete variable distributions along with a set of routines that are more concerned with continuous variable machine learning. one could in principle extend the graphical models part of the code considerably to support continuous variables. limited support for continuous variables is currently provided so that for example inference in the linear dynamical system may be written in conducted of operations on gaussian potentials. however in general potentials on continuous variables need to be manipulated with care and often specialised routines are required to ensure numerical stability. acknowledgements many people have helped this book along the way either in terms of reading feedback general insights allowing me to present their work or just plain motivation. amongst these i would like to thank massimiliano pontil mark herbster john shawe-taylor vladimir kolmogorov yuri boykov tom minka simon prince silvia chiappa bertrand mesot robert cowell ali taylan cemgil david blei jeff bilmes david cohn david page peter sollich chris williams marc toussaint amos storkey zakria hussain seraf n moral milan studen y tristan fletcher tom furmston ed challis and chris bracegirdle. i would also like to thank the many students that have helped improve the material during lectures over the years. i m particularly grateful to tom minka for allowing parts of his lightspeed toolbox to be bundled with the brmltoolbox and am similarly indebted to taylan cemgil for his graphlayout package. a final thankyou to my family and friends. website the code along with an electronic version of the book is available from httpwww.cs.ucl.ac.ukstaffd.barberbrml instructors seeking solutions to the exercises can find information at the website along with additional teaching material. the website also contains a feedback form and errata list. draft march v vi draft march contents i inference in probabilistic models probabilistic reasoning probability refresher probability tables interpreting conditional probability probabilistic reasoning prior likelihood and posterior two dice what were the individual scores? further worked examples code basic probability code general utilities an example notes exercises basic graph concepts graphs spanning tree numerically encoding graphs edge list adjacency matrix clique matrix code utility routines exercises belief networks probabilistic inference in structured distributions graphically representing distributions constructing a simple belief network wet grass uncertain evidence belief networks conditional independence the impact of collisions d-separation d-connection and dependence markov equivalence in belief networks belief networks have limited expressibility vii contents contents learning the direction of arrows causality simpson s paradox influence diagrams and the do-calculus parameterising belief networks further reading code naive inference demo conditional independence demo utility routines exercises graphical models graphical models markov networks markov properties gibbs networks markov random fields conditional independence using markov networks lattice models chain graphical models expressiveness of graphical models factor graphs conditional independence in factor graphs notes code exercises efficient inference in trees other forms of inference marginal inference variable elimination in a markov chain and message passing the sum-product algorithm on factor graphs computing the marginal likelihood the problem with loops max-product finding the n most probable states most probable path and shortest path mixed inference inference in multiply-connected graphs bucket elimination loop-cut conditioning message passing for continuous distributions notes code factor graph examples most probable and shortest path bucket elimination message passing on gaussians exercises viii draft march contents contents the junction tree algorithm clique graphs junction trees clustering variables reparameterisation absorption absorption schedule on clique trees the running intersection property constructing a junction tree for singly-connected distributions moralisation forming the clique graph forming a junction tree from a clique graph assigning potentials to cliques junction trees for multiply-connected distributions triangulation algorithms the junction tree algorithm remarks on the jta computing the normalisation constant of a distribution the marginal likelihood finding the most likely state reabsorption converting a junction tree to a directed network the need for approximations bounded width junction trees code utility routines exercises making decisions syntax of influence diagrams solving influence diagrams temporally unbounded mdps expected utility utility of money decision trees extending bayesian networks for decisions efficient inference using a junction tree markov decision processes maximising expected utility by message passing bellman s equation value iteration policy iteration a curse of dimensionality probabilistic inference and planning non-stationary markov decision process non-stationary probabilistic inference planner stationary planner utilities at each timestep code summax under a partial order junction trees for influence diagrams partially observable mdps restricted utility functions reinforcement learning further topics draft march ix contents contents party-friend example chest clinic with decisions markov decision processes exercises ii learning in probabilistic models statistics for machine learning estimator bias discrete distributions continuous distributions distributions summarising distributions bounded distributions unbounded distributions multivariate distributions multivariate gaussian conditioning as system reversal completing the square gaussian propagation whitening and centering maximum likelihood training bayesian inference of the mean and variance gauss-gamma distribution exponential family conjugate priors the kullback-leibler divergence klqp entropy code exercises learning as inference learning as inference learning the bias of a coin making decisions a continuum of parameters decisions based on continuous intervals maximum a posteriori and maximum likelihood summarising the posterior maximum likelihood and the empirical distribution maximum likelihood training of belief networks bayesian belief network training global and local parameter independence learning binary variable tables using a beta prior learning multivariate discrete tables using a dirichlet prior parents structure learning empirical independence network scoring maximum likelihood for undirected models the likelihood gradient decomposable markov networks non-decomposable markov networks constrained decomposable markov networks x draft march contents contents iterative scaling conditional random fields pseudo likelihood learning the structure properties of maximum likelihood training assuming the correct model class training when the assumed model is incorrect code pc algorithm using an oracle demo of empirical conditional independence bayes dirichlet structure learning exercises naive bayes naive bayes and conditional independence estimation using maximum likelihood binary attributes multi-state variables text classification bayesian naive bayes tree augmented naive bayes chow-liu trees learning tree augmented naive bayes networks code exercises learning with hidden variables hidden variables and missing data why hiddenmissing variables can complicate proceedings the missing at random assumption maximum likelihood identifiability issues expectation maximisation variational em classical em application to belief networks application to markov networks convergence extensions of em partial m step partial e step a failure case for em variational bayes em is a special case of variational bayes factorising the parameter posterior bayesian methods and ml-ii optimising the likelihood by gradient methods directed models undirected models code exercises draft march xi contents contents bayesian model selection comparing models the bayesian way illustrations coin tossing a discrete parameter space a continuous parameter space occam s razor and bayesian complexity penalisation a continuous example curve fitting approximating the model likelihood laplace s method bayes information criterion exercises iii machine learning machine learning concepts styles of learning supervised learning unsupervised learning anomaly detection online learning interacting with the environment semi-supervised learning supervised learning utility and loss what s the catch? using the empirical distribution bayesian decision approach learning lower-dimensional representations in semi-supervised learning features and preprocessing bayes versus empirical decisions representing data categorical ordinal numerical bayesian hypothesis testing for outcome analysis outcome analysis hdiff model likelihood hsame model likelihood dependent outcome analysis is classifier a better than b? code notes exercises nearest neighbour classification do as your neighbour does k-nearest neighbours a probabilistic interpretation of nearest neighbours when your nearest neighbour is far away code utility routines demonstration exercises xii draft march contents contents unsupervised linear dimension reduction latent semantic analysis lsa for information retrieval high-dimensional spaces low dimensional manifolds principal components analysis deriving the optimal linear reconstruction maximum variance criterion pca algorithm pca and nearest neighbours comments on pca high dimensional data eigen-decomposition for n d pca via singular value decomposition pca with missing data finding the principal directions collaborative filtering using pca with missing data matrix decomposition methods probabilistic latent semantic analysis extensions and variations applications of plsanmf kernel pca canonical correlation analysis svd formulation notes supervised linear dimension reduction supervised linear projections fisher s linear discriminant canonical variates dealing with the nullspace using non-gaussian data distributions code exercises linear models the dual representation and kernels introduction fitting a straight line linear parameter models for regression vector outputs regularisation radial basis functions regression in the dual-space positive definite kernels functions linear parameter models for classification logistic regression maximum likelihood training beyond first order gradient ascent avoiding overconfident classification multiple classes the kernel trick for classification support vector machines maximum margin linear classifier using kernels draft march xiii contents contents performing the optimisation probabilistic interpretation soft zero-one loss for outlier robustness notes code bayesian linear models regression with additive gaussian noise bayesian linear parameter models determining hyperparameters ml-ii learning the hyperparameters using em hyperparameter optimisation using the gradient validation likelihood prediction the relevance vector machine classification hyperparameter optimisation laplace approximation making predictions relevance vector machine for classification multi-class case code exercises gaussian processes non-parametric prediction from parametric to non-parametric from bayesian linear models to gaussian processes a prior on functions gaussian process prediction regression with noisy training outputs covariance functions making new covariance functions from old stationary covariance functions non-stationary covariance functions analysis of covariance functions smoothness of the functions mercer kernels fourier analysis for stationary kernels gaussian processes for classification binary classification laplace s approximation hyperparameter optimisation multiple classes further reading code exercises mixture models density estimation using mixtures expectation maximisation for mixture models unconstrained discrete tables mixture of product of bernoulli distributions the gaussian mixture model xiv draft march contents contents em algorithm practical issues classification using gaussian mixture models the parzen estimator k-means bayesian mixture models semi-supervised learning mixture of experts indicator models joint indicator approach factorised prior joint indicator approach polya prior latent dirichlet allocation graph based representations of data dyadic data monadic data cliques and adjacency matrices for monadic binary data further reading code exercises mixed membership models latent linear models factor analysis finding the optimal bias factor analysis maximum likelihood direct likelihood optimisation expectation maximisation interlude modelling faces probabilistic principal components analysis canonical correlation analysis and factor analysis independent components analysis code exercises latent ability models the rasch model maximum likelihood training bayesian rasch models competition models code exercises bradly-terry-luce model elo ranking model glicko and trueskill iv dynamical models discrete-state markov models markov models equilibrium and stationary distribution of a markov chain fitting markov models mixture of markov models the classical inference problems hidden markov models draft march xv contents contents learning hmms filtering parallel smoothing correction smoothing most likely joint state self localisation and kidnapped robots natural language models em algorithm mixture emission the hmm-gmm discriminative training related models explicit duration model input-output hmm linear chain crfs dynamic bayesian networks applications object tracking automatic speech recognition bioinformatics part-of-speech tagging code exercises continuous-state markov models stationary distribution with noise observed linear dynamical systems auto-regressive models training an ar model ar model as an olds time-varying ar model latent linear dynamical systems inference filtering smoothing rauch-tung-striebel correction method the likelihood most likely state time independence and riccati equations identifiability issues em algorithm subspace methods structured ldss bayesian ldss inference maximum likelihood learning using em code autoregressive models learning linear dynamical systems switching auto-regressive models exercises xvi draft march contents contents switching linear dynamical systems introduction the switching lds exact inference is computationally intractable gaussian sum filtering continuous filtering discrete filtering the likelihood collapsing gaussians relation to other methods gaussian sum smoothing continuous smoothing discrete smoothing collapsing the mixture using mixtures in smoothing relation to other methods reset models a poisson reset model hmm-reset code exercises distributed computation introduction stochastic hopfield networks learning sequences a single sequence multiple sequences boolean networks sequence disambiguation tractable continuous latent variable models deterministic latent variables an augmented hopfield network stochastically spiking neurons hopfield membrane potential dynamic synapses leaky integrate and fire models code exercises neural models v approximate inference sampling introduction univariate sampling multi-variate sampling ancestral sampling dealing with evidence perfect sampling for a markov network gibbs sampling gibbs sampling as a markov chain structured gibbs sampling remarks draft march xvii contents contents markov chain monte carlo markov chains metropolis-hastings sampling auxiliary variable methods hybrid monte carlo swendson-wang slice sampling importance sampling sequential importance sampling particle filtering as an approximate forward pass code exercises deterministic approximate inference introduction the laplace approximation properties of kullback-leibler variational inference bounding the normalisation constant bounding the marginal likelihood gaussian approximations using kl divergence moment matching properties of minimising klpq variational bounding using klqp pairwise markov random field general mean field equations asynchronous updating guarantees approximation improvement intractable energy structured variational approximation mutual information maximisation a kl variational approach the information maximisation algorithm linear gaussian decoder loopy belief propagation classical bp on an undirected graph loopy bp as a variational procedure expectation propagation map for mrfs map assignment attractive binary mrfs potts model further reading a background mathematics linear algebra vector algebra the scalar product as a projection lines in space planes and hyperplanes matrices linear transformations determinants matrix inversion computing the matrix inverse eigenvalues and eigenvectors matrix decompositions xviii draft march contents contents critical points matrix identities multivariate calculus interpreting the gradient vector higher derivatives chain rule matrix calculus inequalities convexity jensen s inequality optimisation gradient descent gradient descent with fixed stepsize gradient descent with momentum gradient descent with line searches exact line search condition multivariate minimization quadratic functions minimising quadratic functions using line search gram-schmidt construction of conjugate vectors the conjugate vectors algorithm the conjugate gradients algorithm newton s method quasi-newton methods constrained optimisation using lagrange multipliers draft march xix contents contents xx draft march part i inference in probabilistic models chapter probabilistic reasoning probability refresher variables states and notational shortcuts variables will be denoted using either upper case x or lower case x and a set of variables will typically be denoted by a calligraphic symbol for example v b c the domain of a variable x is written domx and denotes the states x can take. states will typically be represented using sans-serif font. for example for a coin c we might have domc tails and pc heads represents the probability that variable c is in state heads. the meaning of pstate will often be clear without specific reference to a variable. for example if we are discussing an experiment about a coin c the meaning of pheads is clear from the context being shortx fx the hand for pc heads. when summing performing some other operation over a interpretation is that all states of x are included i.e. x fx s domx fx s. for our purposes events are expressions about random variables such as two heads in coin tosses. two events are mutually exclusive if they cannot both simultaneously occur. for example the events the coin is heads and the coin is tails are mutually exclusive. one can think of defining a new variable named by the event so for example pthe coin is tails can be interpreted as pthe coin is tails true. we use px tr for the probability of eventvariable x being in the state true and px fa for the probability of eventvariable x being in the state false. the rules of probability definition of probability variables. the probability of an event x occurring is represented by a value between and px means that we are certain that the event does occur. conversely px means that we are certain that the event does not occur. the summation of the probability over all the states is px x x such probabilities are normalised. we will usually more conveniently x px probability refresher two events x and y can interact through px or y px py px and y we will use the shorthand px y for px and y. note that py x px y and px or y py or x. definition notation. an alternative notation in terms of set theory is to write px or y px y px y px y definition given a joint distribution px y the distribution of a single variable is given by px y px y here px is termed a marginal of the joint probability distribution px y. the process of computing a marginal from a joint distribution is called marginalisation. more generally one has xi xn xn xi an important definition that will play a central role in this book is conditional probability. definition probability bayes rule. the probability of event x conditioned on knowing event y more shortly the probability of x given y is defined as pxy px y py if py then pxy is not defined. probability density functions definition density functions. for a single continuous variable x the probability density px is defined such that px pxdx draft march probability refresher b a pa x b as shorthand we will sometimes b pxdx xa px particularly when we want an expression to be valid for either continuous or discrete variables. the multivariate case is analogous with integration over all real space and the probability that x belongs to a region of the space defined accordingly. for continuous variables formally speaking events are defined for the variable occurring within a defined region for example px fxdx where here fx is the probability density function of the continuous random variable x. unlike probabilities probability densities can take positive values greater than appear strange the nervous reader may simply replace our px x notation formally speaking for a continuous variable one should not speak of the probability that x since the probability of a single value is always zero. however we shall often write px for continuous variables thus not distinguishing between probabilities and probability density function values. whilst this may x fxdx where is a small region centred on x. this is well defined in a probabilistic sense and in the limit being very small this would give approximately fx. if we consistently use the same for all occurrences of pdfs then we will simply have a common prefactor in all expressions. our strategy is to simply ignore these values in the end only relative probabilities will be relevant and write px. in this way all the standard rules of probability carry over including bayes rule. interpreting conditional probability imagine a circular dart board split into equal sections labelled from to and randy a dart thrower who hits any one of the sections uniformly at random. hence the probability that a dart thrown by randy occurs in any one of the regions is pregion i a friend of randy tells him that he hasn t hit the region. what is the probability that randy has hit the region? conditioned on this information only regions to remain possible and since there is no preference for randy to hit any of these regions the probability is the conditioning means that certain states are now inaccessible and the original probability is subsequently distributed over the remaining accessible states. from the rules of probability pregion region pregion not region pnot region pregion pnot region giving the intuitive result. pregion in the above pregion not region pregion an important point to clarify is that pa ab b should not be interpreted as given the event b b has occurred pa ab b is the probability of the event a a occurring in most contexts no such explicit temporal causality is and the correct interpretation should be pa ab b is the probability of a being in state a under the constraint that b is in state b constant since pa a b b is not a distribution in a in other make it a distribution we need to divide pa a b the relation between the conditional pa ab b and the joint pa a b b is just a normalisation a pa a b b to a pa a b b which when summed over a does sum to indeed this is just the definition of pa ab b. will discuss issues related to causality further in draft march probability refresher definition events x and y are independent if knowing one event gives no extra information about the other event. mathematically this is expressed by px y pxpy provided that px and py independence of x and y is equivalent to pxy px pyx py if pxy px for all states of x and y then the variables x and y are said to be independent. if px y kfxgy for some constant k and positive functions f and g then x and y are independent. deterministic dependencies sometimes the concept of independence is perhaps a little strange. consider the following variables x and y are both binary domains consist of two states. we define the distribution such that x and y are always both in a certain joint state px a y px a y px b y px b y are x and y dependent? the reader may show that px a px b and py py hence pxpy px y for all states of x and y and x and y are therefore independent. this may seem strange we know for sure the relation between x and y namely that they are always in the same joint state yet they are independent. since the distribution is trivially concentrated in a single joint state knowing the state of x tells you nothing that you didn t anyway know about the state of y and vice versa. this potential confusion comes from using the term independent which in english suggests that there is no influence or relation between objects discussed. the best way to think about statistical independence is to ask whether or not knowing the state of variable y tells you something more than you knew before about variable x where knew before means working with the joint distribution of px y to figure out what we can know about x namely px. probability tables based on the populations and of england scotland and wales the a priori probability that a randomly selected person from these three countries would live in england scotland or wales would be approximately and respectively. we can write this as a vector probability table pcnt e pcnt s pcnt w whose component values sum to the ordering of the components in this vector is arbitrary as long as it is consistently applied. draft march probability refresher for the sake of simplicity let s assume that only three mother tongue languages exist english scottish and welsh with conditional probabilities given the country of residence england scotland and wales we write a conditional probability table pm t engcnt e pm t scotcnt e pm t welcnt e pm t engcnt s pm t welcnt s pm t welcnt w pm t engcnt w pm t scotcnt s pm t scotcnt w from this we can form a joint distribution pcnt m t pm tcntpcnt. this could be written as a matrix with rows indexed by country and columns indexed by mother tongue the joint distribution contains all the information about the model of this environment. by summing a column of this table we have the marginal pcnt. summing the row gives the marginal pm t similarly one could easily infer pcntm t pcntm t t from this joint distribution. for joint distributions over a larger number of variables xi i d with each variable xi taking ki entries. explicitly storing tables therefore requires space exponential in the number of variables which rapidly becomes impractical for a large number of variables. ki states the table describing the joint distribution is an array a probability distribution assigns a value to each of the joint states of the variables. for this reason pt j r s is considered equivalent to pj s r t any such reordering of the variables since in each case the joint setting of the variables is simply a different index to the same probability. this situation is more clear in the set theoretic notation pj s t r. we abbreviate this set theoretic notation by using the commas however one should be careful not to confuse the use of this indexing type notation with functions fx y which are in general dependent on the variable order. whilst the variables to the left of the conditioning bar may be written in any order and equally those to the right of the conditioning bar may be written in any order moving variables across the bar is not generally equivalent so that interpreting conditional probability together with the rules of probability conditional probability enables one to reason in a rational logical and consistent way. one could argue that much of science deals with problems of the form tell me something about the parameters given that i have observed data d and have some knowledge of the underlying data generating mechanism. from a modelling perspective this requires p pd pd pd pd this shows how from a forward or generative model pd of the dataset and coupled with a prior belief p about which parameter values are appropriate we can infer the posterior distribution p of parameters in light of the observed data. this use of a generative model sits well with physical models of the world which typically postulate how to generate observed phenomena assuming we know the correct parameters of the model. for example one might postulate how to generate a time-series of displacements for a swinging pendulum but with unknown mass length and damping constant. using this generative model and given only the displacements we could infer the unknown physical properties of the pendulum such as its mass length and friction damping constant. draft march probabilistic reasoning subjective probability probability is a contentious topic and we do not wish to get bogged down by the debate here apart from pointing out that it is not necessarily the axioms of probability that are contentious rather what interpretation we should place on them. in some cases potential repetitions of an experiment can be envisaged so that the long run frequentist definition of probability in which probabilities are defined with respect to a potentially infinite repetition of experiments makes sense. for example in coin tossing the probability of heads might be interpreted as if i were to repeat the experiment of flipping a coin random the limit of the number of heads that occurred over the number of tosses is defined as the probability of a head occurring. here s another problem that is typical of the kind of scenario one might face in a machine learning situation. a film enthusiast joins a new online film service. based on expressing a few films a user likes and dislikes the online company tries to estimate the probability that the user will like each of the films in their database. if we were to define probability as a limiting case of infinite repetitions of the same experiment this wouldn t make much sense in this case since we can t repeat the experiment. however if we assume that the user behaves in a manner consistent with other users we should be able to exploit the large amount of data from other users ratings to make a reasonable guess as to what this consumer likes. this degree of belief or bayesian subjective interpretation of probability sidesteps non-repeatability issues it s just a consistent framework for manipulating real values consistent with our intuition about probabilistic reasoning the axioms of probability combined with bayes rule make for a complete reasoning system one which includes traditional deductive logic as a special remark the central paradigm of probabilistic reasoning is to identify all relevant variables xn in the environment and make a probabilistic model xn of their interaction. reasoning is then performed by introducing evidence that sets variables in known states and subsequently computing probabilities of interest conditioned on this evidence. example consider the following fictitious scientific information doctors find that people with kreuzfeld-jacob disease almost invariably ate hamburgers thus phamburger eaterkj the probability of an individual having kj is currently rather low about one in assuming eating lots of hamburgers is rather widespread say phamburger eater what is the probability that a hamburger eater will have kreuzfeld-jacob disease? this may be computed as pkj eater phamburger eater kj phamburger eater phamburger eaterkj phamburger eater if the fraction of people eating hamburgers was rather small phamburger eater what is the probability that a regular hamburger eater will have kreuzfeld-jacob disease? repeating the above calculation this is given by draft march probabilistic reasoning intuitively this is much higher than in scenario since here we can be more sure that eating hamburgers is related to the illness. in this case only a small number of people in the population eat hamburgers and most of them get ill. example clouseau. inspector clouseau arrives at the scene of a crime. the victim lies dead in the room and the inspector quickly finds the murder weapon a knife the butler and maid are his main suspects. the inspector has a prior belief of that the butler is the murderer and a prior belief of that the maid is the murderer. these probabilities are independent in the sense that pb m pbpm. is possible that both the butler and the maid murdered the victim or neither. the inspector s prior criminal knowledge can be formulated mathematically as follows domb domm not murderer domk used knife not used pb murderer pknife usedb not murderer m not murderer pknife usedb not murderer m murderer m not murderer pknife usedb murderer pknife usedb murderer m murderer pm murderer what is the probability that the butler is the murderer? that it might be that neither is the murderer. using b for the two states of b and m for the two states of m pbk pb mk m m plugging in the values we have pb murdererknife used b m pkb mpm m pkb mpm pb m k pk the role of pknife used in the inspector clouseau example can cause some confusion. in the above pknife usedb mpm pknife used b m is computed to be but surely pknife used since this is given in the question! note that the quantity pknife used relates to the prior probability the model assigns to the knife being used the absence of any other information. if we know that the knife is used then the posterior pknife usedknife used pknife used knife used pknife used pknife used pknife used which naturally must be the case. another potential confusion is the choice pb murderer pm murderer which means that pb not murderer pm not murderer these events are not exclusive and it s just coincidence that the numerical values are chosen this way. for example we could have also chosen pb murderer pm murderer which means that pb not murderer pm not murderer draft march p pd pd prior likelihood evidence p posterior the evidence is also called the marginal likelihood. the term likelihood is used for the probability that a model generates observed data. more fully if we condition on the model m we have p m pd mp pdm where we see the role of the likelihood pd m and marginal likelihood pdm. the marginal likelihood is also called the model likelihood. the most probable a posteriori argmax p m. setting is that which maximises the posterior bayes rule tells us how to update our prior knowledge with the data generating mechanism. the prior distribution p describes the information we have about the variable before seeing any data. after data d arrives we update the prior distribution to the posterior p pd two dice what were the individual scores? two fair dice are rolled. someone tells you that the sum of the two scores is what is the probability distribution of the two dice the score of die a is denoted sa with domsa and similarly for sb. the three variables involved are then sa sb and the total score t sa sb. a model of these three variables naturally takes the form pt sa sb ptsa sb likelihood psa sb prior prior likelihood and posterior the prior likelihood and posterior are all probabilities. they are assigned these names due to their role in bayes rule described below. prior likelihood and posterior definition prior likelihood and posterior for data d and variable bayes rule tells us how to update our prior beliefs about the variable in light of the data to a posterior belief the prior psa sb is the joint probability of score sa and score sb without knowing anything else. assuming no dependency in the rolling mechanism psa sb psapsb since the dice are fair both psa and psb are uniform distributions psa s example is due to taylan cemgil. sa sb sb sb sb sb sb psapsb sa sa sa draft march sa sa further worked examples here the likelihood term is ptsa sb i sa sb which states that the total score is given by sa sb. here i y is the indicator function defined as i y if x y and otherwise. sa sb sb sb sb sb sb pt sb sa sa sa sa sa hence our complete model is pt sa sb ptsa sbpsapsb where the terms on the right are explicitly defined. our interest is then obtainable using bayes rule where pt psa sbt pt sbpsapsb pt the term pt sasb pt sbpsapsb pt sbpsapsb sa sa sa sa sa sa psa sbt sa sa sa sa sa sa sb sb sb sb sb sb sb sb sb sb sb sb equal mass in only non-zero elements as shown. sasb pt sbpsapsb hence the posterior is given by further worked examples example s in the bathroom?. consider a household of three people alice bob and cecil. cecil wants to go to the bathroom but finds it occupied. he then goes to alice s room and sees she is there. since cecil knows that only either alice or bob can be in the bathroom from this he infers that bob must be in the bathroom. to arrive at the same conclusion in a mathematical framework let s define the following events a alice is in her bedroom b bob is in his bedroom o bathroom occupied we can encode the information that if either alice or bob are not in their bedrooms then they must be in the bathroom might both be in the bathroom as po tra fa b po tra b fa the first term expresses that the bathroom is occupied if alice is not in her bedroom wherever bob is. similarly the second term expresses bathroom occupancy as long as bob is not in his bedroom. then pb fao tr a tr pb fa o tr a tr po tr a tr where po tra tr b fapa tr b fa po tr a tr po tr a tr po tra tr b fapa tr b fa po tra tr b trpa tr b tr draft march further worked examples using the fact po tra tr b fa and po tra tr b tr which encodes that if if alice is in her room and bob is not the bathroom must be occupied and similarly if both alice and bob are in their rooms the bathroom cannot be occupied pb fao tr a tr pa tr b fa pa tr b fa this example is interesting since we are not required to make a full probabilistic model in this case thanks to the limiting nature of the probabilities don t need to specify pa b. the situation is common in limiting situations of probabilities being either or corresponding to traditional logic systems. example resolution. we can represent the statement all apples are fruit by pf tra tr similarly all fruits grow on trees may be represented by pt trf tr additionally we assume that whether or not something grows on a tree depends only on whether or not it is a fruit pta f p from these we can compute pt tra tr pt trf fa pf faa tr pt trf a trpfa tr pf tra tr pt trf tr pt trf tr f f in other words we have deduced that all apples grow on trees is a true statement based on the information presented. kind of reasoning is called resolution and is a form of transitivity from the statements a f and f t we can infer a t example inverse modus ponens. according to logic from the statement if a is true then b is true one may deduce that if b is false then a is false let s see how this fits in with a probabilistic reasoning system. we can express the statement if a is true then b is true as pb tra tr then we may infer pa fab fa pa trb fa pb faa trpa tr pb faa trpa tr pb faa fapa fa this follows since pb faa tr pb tra tr annihilating the second term. both the above examples are intuitive expressions of deductive logic. the standard rules of aristotelian logic are therefore seen to be limiting cases of probabilistic reasoning. example xor gate. a standard xor logic gate is given by the table on the right. if we observe that the output of the xor gate is what can we say about a and b? in this case either a and b were both or a and b were both this means we don t know which state a was in it could equally likely have been or a b a xor b draft march further worked examples consider a soft version of the xor gate given on the right with additionally pa pb what is pa pa c pa b c b b pc bpa a b pc b pa b pc b pa c pa b c b b pc bpa pa b pc b then pa pa c pa c pa c example larry is typically late for school. if larry is late we denote this with l late otherwise l not late. when his mother asks whether or not he was late for school he never admits to being late. the response larry gives rl is represented as follows prl not latel not late prl latel late the remaining two values are determined by normalisation and are prl latel not late prl not latel late given that rl not late what is the probability that larry was late i.e. pl laterl not late? using bayes we have pl laterl not late pl late rl not late prl not late pl late rl not late pl late rl not late pl not late rl not late in the above pl late rl not late prl not latel late pl late and pl not late rl not late prl not latel not late pl not late hence pl laterl not late pl late pl late pl not late pl late draft march where we used normalisation in the last step pl late pl not late this result is intuitive larry s mother knows that he never admits to being late so her belief about whether or not he really was late is unchanged regardless of what larry actually says. further worked examples example and sue. continuing the example above larry s sister sue always tells the truth to her mother as to whether or not larry was late for school. prs not latel not late prs latel late the remaining two values are determined by normalisation and are prs latel not late prs not latel late we also assume prs rll prslprll. we can then write prl rs l prllprslpl given that rs late what is the probability that larry was late? using bayes rule we have pl laterl not late rs late z prs latel lateprl not latel latepl late where the normalisation z is given by prs latel lateprl not latel latepl late prs latel not lateprl not latel not latepl not late hence pl laterl not late rs late pl late pl late pl not late this result is also intuitive since larry s mother knows that sue always tells the truth no matter what larry says she knows he was late. example luke has been told he s lucky and has won a prize in the lottery. there are prizes available of value the prior probabilities of winning these prizes are with being the prior probability of winning no prize. luke asks eagerly did i win i m afraid not sir is the response of the lottery phone operator. did i win asks luke. again i m afraid not sir what is the probability that luke has won note first that we denote w for the first prize of and w for the remaining prizes and w for no prize. we need to compute pw w w pw w w w pw w w pw or w or w pw draft march code where the term in the denominator is computed using the fact that the events w are mutually exclusive can only win one prize. this result makes intuitive sense once we have removed the impossible states of w the probability that luke wins the prize is proportional to the prior probability of that prize with the normalisation being simply the total set of possible probability remaining. code the brmltoolbox code accompanying this book is intended to give the reader some insight into representing discrete probability tables and performing simple inference. the matlab code is written with only minimal error trapping to keep the code as short and hopefully reasonably basic probability code at the simplest level we only need two basic routines. one for multiplying probability tables together potentials in the code and one for summing a probability table. potentials are represented using a structure. for example in the code corresponding to the inspector clouseau example democlouseau.m we define a probability table as ans variables table double this says that the potential depends on the variables and the entries are stored in the array given by the table field. the size of the array informs how many states each variable takes in the order given by variables. the order in which the variables are defined in a potential is irrelevant provided that one indexes the array consistently. a routine that can help with setting table entries is setstate.m. for example means that for potential the table entry for variable being in state variable being in state and variable being in state should be set to value the philosophy of the code is to keep the information required to perform computations to a minimum. additional information about the labels of variables and their domains can be useful to check results but is not actually required to carry out computations. one may also specify the name and domain of each variable for example ans domain murderer not murderer name butler the variable name and domain information in the clouseau example is stored in the structure variable which can be helpful to display the potential table knife knife knife knife knife knife knife knife used not used used not used used not used used not used maid maid maid maid maid maid maid maid murderer murderer not murderer not murderer murderer murderer not murderer not murderer butler butler butler butler butler butler butler butler murderer murderer murderer murderer not murderer not murderer not murderer not murderer the time of writing some versions of matlab suffer from serious memory indexing bugs some of which may appear in the array structures used in the code provided. to deal with this turn off the jit accelerator by typing feature accel off. draft march multiplying potentials in order to multiply potentials for arrays the tables of each potential must be dimensionally consistent that is the number of states of variable i in potential must match the number of states of variable i in any other potential. this can be checked using potvariables.m. this consistency is also required for other basic operations such as summing potentials. multpots.m multiplying two or more potentials divpots.m dividing a potential by another code summing a potential sumpot.m sum a potential over a set of variables sumpots.m sum a set of potentials together making a conditional potential condpot.m make a potential conditioned on variables setting a potential setpot.m set variables in a potential to given states setevpot.m set variables in a potential to given states and return also an identity potential on the given states the philosophy of brmltoolbox is that all information about variables is local and is read off from a potential. using setevpot.m enables one to set variables in a state whilst maintaining information about the number of states of a variable. maximising a potential maxpot.m maximise a potential over a set of variables see also maxnarray.m and maxnpot.m which return the n-highest values and associated states. other potential utilities setstate.m set the a potential state to a given value table.m return a table from a potential whichpot.m return potentials which contain a set of variables potvariables.m variables and their number of states in a set of potentials orderpotfields.m order the fields of a potential structure uniquepots.m merge redundant potentials and return only unique ones numstates.m number of states of a variable in a domain squeezepots.m remove redundant potentials by merging normpot.m normalise a potential to form a distribution general utilities condp.m return a table pxy from px y condexp.m form a conditional distribution from a log value logsumexp.m compute the log of a sum of exponentials in a numerically precise way normp.m return a normalised table from an unnormalised table draft march exercises assign.m assign values to multiple variables maxarray.m maximize a multi-dimensional array over a subset an example the following code highlights the use of the above routines in solving the inspector clouseau democlouseau.m solving the inspector clouseau example notes the interpretation of probability is contentious and we refer the reader to for detailed discussions. a useful website that relates to understanding probability and bayesian reasoning is understandinguncertainty.org. exercises exercise prove px yz pxzpyx z and also pxy z pyx zpxz pyz exercise prove the bonferroni inequality pa b pa pb exercise from there are two boxes. box contains three red and five white balls and box contains two red and five white balls. a box is chosen at random pbox pbox and a ball chosen at random from this box turns out to be red. what is the posterior probability that the red ball came from box exercise from two balls are placed in a box as follows a fair coin is tossed and a white ball is placed in the box if a head occurs otherwise a red ball is placed in the box. the coin is tossed again and a red ball is placed in the box if a tail occurs otherwise a white ball is placed in the box. balls are drawn from the box three times in succession with replacing the drawn ball back in the box. it is found that on all three occasions a red ball is drawn. what is the probability that both balls in the box are red? exercise david spiegelhalter understandinguncertainty.org. a secret government agency has developed a scanner which determines whether a person is a terrorist. the scanner is fairly reliable of all scanned terrorists are identified as terrorists and of all upstanding citizens are identified as such. an informant tells the agency that exactly one passenger of aboard an aeroplane in which you are seated is a terrorist. the agency decide to scan each passenger and the shifty looking man sitting next to you is the first to test positive. what are the chances that this man is a terrorist? exercise monty hall problem. on a gameshow there are three doors. behind one door is a prize. the gameshow host asks you to pick a door. he then opens a different door to the one you chose and shows that there is no prize behind it. is is better to stick with your original guess as to where the prize is or better to change your mind? exercise consider three variable distributions which admit the factorisation pa b c pabpbcpc where all variables are binary. how many parameters are needed to specify distributions of this form? draft march exercise repeat the inspector clouseau scenario but with the restriction that either the maid or the butler is the murderer but not both. explicitly the probability of the maid being the murder and not the butler is the probability of the butler being the murder and not the maid is modify democlouseau.m to implement this. exercise prove exercises pa or c pa b pa c pa b c exercise prove pxz pxy zpyz y yw pxw y zpwy zpyz exercise as a young man mr gott visits berlin in he s surprised that he cannot cross into east berlin since there is a wall separating the two halves of the city. he s told that the wall was erected years previously. he reasons that the wall will have a finite lifespan his ignorance means that he arrives uniformly at random at some time in the lifespan of the wall. since only of the time one would arrive in the first or last of the lifespan of the wall he asserts that with confidence the wall will survive between and years. in the now professor gott is pleased to find that his prediction was correct and promotes his prediction method in elite journals. this delta-t method is widely adopted and used to form predictions in a range of scenarios about which researchers are totally ignorant would you buy a prediction from prof. gott? explain carefully your reasoning. exercise implement the soft xor gate using brmltoolbox. you may find condpot.m of use. exercise implement the hamburgers scenarios using brmltoolbox. to do so you will need to define the joint distribution phamburgers kj in which domhamburgers domkj fa. exercise implement the two-dice example using brmltoolbox. exercise a redistribution lottery involves picking the correct four numbers from to replacement so for example is not possible. the order of the picked numbers is irrelevant. every week a million people play this game each paying to enter with the numbers being the most popular in every people chooses these numbers. given that the million pounds prize money is split equally between winners and that any four numbers come up at random what is the expected amount of money each of the players choosing will win each week? the least popular set of numbers is with only in people choosing this. how much do they profit each week on average? do you think there is any skill involved in playing this lottery? exercise in a test of psychometry the car keys and wrist watches of people are given to a medium. the medium then attempts to match the wrist watch with the car key of each person. what is the expected number of correct matches that the medium will make chance? what is the probability that the medium will obtain at least correct match? draft march chapter basic graph concepts graphs definition a graph g consists of vertices and edges between the vertices. edges may be directed have an arrow in a single direction or undirected. a graph with all edges directed is called a directed graph and one with all edges undirected is called an undirected graph. definition ancestors descendants. a path a b from node a to node b is a sequence of vertices a an an b with an edge in the graph thereby connecting a to b. for a directed graph this means that a path is a sequence of nodes which when we follow the direction of the arrows leads us from a to b. the vertices a such that a b and b a are the descendants of and b a are the ancestors of b. the vertices b such that a b definition acyclic graph a dag is a graph g with directed edges on each link between the vertices such that by following a path of vertices from one node to another along the direction of each edge no path will revisit a vertex. in a dag the ancestors of b are those nodes who have a directed path ending at b. conversely the descendants of a are those nodes who have a directed path starting at a. definition in a dag. the children of the parents of are pa the family of a node is itself and its are ch parents. the markov blanket of a node is itself its parents children and the parents of its children. in this case the markov blanket of is c a f b g d e c b g e a f d graphs figure multiply-connected graph. singly-connected graph. definition graph. a d b c e an undirected graph g consists of undirected edges between nodes. definition for an undirected graph g the neighbours of x ne are those nodes directly connected to x. definition graph. an undirected graph is connected if there is a path between every set of vertices there are no isolated islands. for a graph which is not connected the connected components are those subgraphs which are connected. definition a d b c e given an undirected graph a clique is a maximally connected subset of vertices. all the members of the clique are connected to each other furthermore there is no larger clique that can be made from a clique. for example this graph has two cliques b c d and c e. whilst a b c are fully connected this is a non-maximal clique since there is a larger fully connected set a b c d that contains this. a non-maximal clique is sometimes called a cliquo. definition graph. a graph is singly-connected if there is only one path from a vertex a to another vertex b. otherwise the graph is multiply-connected. this definition applies regardless of whether or not the edges in the graph are directed. an alternative name for a singly-connected graph is a tree. a multiply-connected graph is also called loopy. draft march numerically encoding graphs spanning tree definition tree. a spanning tree of an undirected graph g is a singly-connected subset of the existing edges such that the resulting singlyconnected graph covers all vertices of g. on the right is a graph and an associated spanning tree. a maximum weight spanning tree is a spanning tree such that the sum of all weights on the edges of the tree is larger than for any other spanning tree of g. finding the maximal weight spanning tree a simple algorithm to find a spanning tree with maximal weight is as follows start by picking the edge with the largest weight and add this to the edge set. then pick the next candidate edge which has the largest weight and add this to the edge set if this results in an edge set with cycles then reject the candidate edge and find the next largest edge weight. note that there may be more than one maximal weight spanning tree. numerically encoding graphs to express the structure of gms we need to numerically encode the links on the graphs. for a graph of n vertices we can describe the graph structure in various equivalent ways. edge list as the name suggests an edge list simply lists which vertex-vertex pairs are in the graph. for an edge list is l where an undirected edge is represented by a bidirectional edge. adjacency matrix an alternative is to use an adjacency matrix a where aij if there is an edge from variable i to j in the graph and otherwise. some authors include self-connections in this definition. an undirected graph has a symmetric adjacency matrix. provided that the vertices are labelled in ancestral order always come before children a directed graph can be represented as a triangular adjacency matrix t draft march numerically encoding graphs figure an undirected graph can be represented as a symmetric adjacency matrix. a directed graph with vertices labelled in ancestral order corresponds to a triangular adjacency matrix. adjacency matrix powers for an n n adjacency matrix a powers of the adjacency if we include s on the diagonal of a then in the graph. if a corresponds to a dag the non-zero entries of the jth row correspond to the ij is non-zero when there is a path connecting j to i are from node i to node j in k edge hops. ij specify how many paths there descendants of node j. clique matrix for an undirected graph with n vertices and maximal cliques ck a clique matrix is an n k matrix in which each column ck has zeros expect for ones on entries describing the clique. a cliquo matrix relaxes the constraint that cliques are required to be for example c cinc is a clique matrix for a cliquo matrix containing only two-dimensional maximal cliques is called an incidence matrix. for example is an incidence matrix for it is straightforward to show that cincct inc is equal to the adjacency matrix except that the diagonals now contain the degree of each vertex number of edges it touches. similarly for any cliquo matrix the diagonal entry of expresses the number of cliquos that vertex i occurs in. off diagonal elements contain the number of cliquos that vertices i and j jointly inhabit. remark confusions. graphs are widely used but differ markedly in what they represent. two potential pitfalls are described below. state transition diagrams such graphical representations are common in markov chains and finite state automata. a set of states is written as set of nodesvertices of a graph and a directed edge between node i and node j an associated weight pij represents that a transition from state i to state j can occur with probability pij. from the graphical models perspective we would simply write down a directed graph xt xt to represent this markov chain. the state-transition diagram simply provides a graphical description of the conditional probability table pxt term cliquo for a non-maximal clique is attributed to julian besag. draft march exercises neural networks neural networks also have vertices and edges. in general however neural networks are graphical representations of functions whereas as graphical models are representations of distributions richer formalism. neural networks any other parametric description may be used to represent the conditional probability tables as in sigmoid belief code utility routines ancestors.m find the ancestors of a node in a dag edges.m edge list from an adjacency matrix ancestralorder.m ancestral order from a dag connectedcomponents.m connected components parents.m parents of a node given an adjacency matrix children.m children of a node given an adjacency matrix neigh.m neighbours of a node given an adjacency matrix a connected graph is a tree if the number of edges plus is equal to the number of nodes. however for a possibly disconnected graph this is not the case. the code below deals with the possibly disconnected case. the routine is based on the observation that any singly-connected graph must always possess a simplical node leaf node which can be eliminated to reveal a smaller singly-connected graph. istree.m if graph is singly connected return and elimination sequence spantree.m return a spanning tree from an ordered edge list singleparenttree.m find a directed tree with at most one parent from an undirected tree additional routines for basic manipulations in graphs are given at the end of exercises j in one timestep and otherwise. show that the exercise consider an adjacency matrix a with elements if one can reach state i from state ij represents the number of paths that lead from state j to i in k timesteps. hence derive an algorithm that will find the minimum number of steps to get from state j to state i. exercise for an n n symmetric adjacency matrix a describe an algorithm to find the connected components. you may wish to examine connectedcomponents.m. exercise show that for a connected graph that is singly-connected the number of edges e must be equal to the number of vertices minus e v give an example graph with e v that is not singly-connected. draft march exercises draft march chapter belief networks probabilistic inference in structured distributions consider an environment composed of n variables with a corresponding distribution xn writing e as the set of evidential variables and using evidence xe e e to denote all available evidence then inference and reasoning can be carried out automatically by the brute force xei pxe xe e e xi xi xei xe pxe xe e e xe pxi xievidence if all variables are binary two states these summations require operations. such exponential computation is impractical and techniques that reduce this burden by exploiting any structure in the joint probability table are the topic of our discussions on efficient inference. naively specifying all the entries of a table xn over binary variables xi takes space. we will need to deal with large numbers of variables in machine learning and related application areas with distributions on potentially hundreds if not millions of variables. the only way to deal with such large distributions is to constrain the nature of the variable interactions in some manner both to render specification and ultimately inference in such systems tractable. the key idea is to specify which variables are independent of others leading to a structured factorisation of the joint probability distribution. belief networks are a convenient framework for representing such factorisations into local conditional distributions. we will discuss belief networks more formally in first discussing their natural graphical representations of distributions. definition network. a belief network is a distribution of the form xd pxipa where pa represent the parental variables of variable xi. written as a directed graph with an arrow pointing from a parent variable to child variable a belief network is a directed acyclic graph with the ith vertex in the graph corresponding to the factor pxipa extension to continuous variables is straightforward replacing summation with integration over pdfs we defer treatment of this to later chapters since our aim is to here outline more the intuitions without needing to deal with integration of high dimensional distributions. graphically representing distributions graphically representing distributions belief networks called bayes networks or bayesian belief networks are a way to depict the independence assumptions made in a distribution their application domain is widespread ranging from and expert reasoning under uncertainty to machine learning. before we more formally define a bn an example will help motivate the constructing a simple belief network wet grass one morning tracey leaves her house and realises that her grass is wet. is it due to overnight rain or did she forget to turn off the sprinkler last night? next she notices that the grass of her neighbour jack is also wet. this explains away to some extent the possibility that her sprinkler was left on and she concludes therefore that it has probably been raining. making a model we can model the above situation using probability by following a general modelling approach. first we define the variables we wish to include in our model. in the above situation the natural variables are r means that it has been raining and otherwise. s means that tracey has forgotten to turn off the sprinkler and otherwise. j means that jack s grass is wet and otherwise. t means that tracey s grass is wet and otherwise. a model of tracey s world then corresponds to a probability distribution on the joint set of the variables of interest pt j r s order of the variables is irrelevant. since each of the variables in this example can take one of two states it would appear that we naively have to specify the values for each of the states e.g. pt j r s etc. however since there are normalisation conditions for probabilities we do not need to specify all the state probabilities. to see how many states need to be specified consider the following decomposition. without loss of generality and repeatedly using bayes rule we may write pt j r s ptj r spj r s ptj r spjr spr s ptj r spjr sprsps that is we may write the joint distribution as a product of conditional distributions. the first term ptj r s requires us to specify values we need pt r s for the joint states of j r s. the other value pt r s is given by normalisation pt r s pt r s. similarly we need values for the other factors making a total of values in all. in general for a distribution on n binary variables we need to specify values in the range the important point here is that the number of values that need to be specified in general scales exponentially with the number of variables in the model this is impractical in general and motivates simplifications. conditional independence the modeler often knows constraints on the system. for example in the scenario above we may assume that tracey s grass is wet depends only directly on whether or not is has been raining and whether or not her sprinkler was on. that is we make a conditional independence assumption ptj r s ptr s scenario is adapted from draft march graphically representing distributions r s b e j t r a figure belief network structure for the wet grass example. each node in the graph represents a variable in the joint distribution and the variables which feed in parents to another variable represent which variables are bn to the right of the conditioning bar. for the burglar model. similarly since whether or not jack s grass is wet is influenced only directly by whether or not it has been raining we write pjr s pjr and since the rain is not directly influenced by the sprinkler prs pr which means that our model now becomes pt j r s ptr spjrprps we can represent these conditional independencies graphically as in this reduces the number of values that we need to specify to a saving over the previous values in the case where no conditional independencies had been assumed. to complete the model we need to numerically specify the values of each conditional probability table let the prior probabilities for r and s be pr and ps we set the remaining probabilities to pj pj jack s grass is wet due to unknown effects other than rain pt s pt s s a small chance that even though the sprinkler was left on it didn t wet the grass noticeably pt s inference now that we ve made a model of an environment we can perform inference. let s calculate the probability that the sprinkler was on overnight given that tracey s grass is wet ps to do this we use bayes rule ps ps t jr pt j r s jrs pt j r s pt jr pjrpt s jrs pjrpt sprps r pt s rs pt sprps so the belief that the sprinkler is on increases above the prior probability due to the fact that the grass is wet. let us now calculate the probability that tracey s sprinkler was on overnight given that her grass is wet and that jack s grass is also wet ps j we use bayes rule again draft march graphically representing distributions ps j ps t j pt j r pt j r s rs pt j r s r pj s rs pj sprps the probability that the sprinkler is on given the extra evidence that jack s grass is wet is lower than the probability that the grass is wet given only that tracey s grass is wet. that is that the grass is wet due to the sprinkler is explained away by the fact that jack s grass is also wet this increases the chance that the rain has played a role in making tracey s grass wet. naturally we don t wish to carry out such inference calculations by hand all the time. general purpose algorithms exist for this such as the junction tree algorithm and we shall introduce these in later chapters. example it the burglar?. here s another example using binary variables adapted from sally comes home to find that the burglar alarm is sounding has she been burgled or was the alarm triggered by an earthquake she turns the car radio on for news of earthquakes and finds that the radio broadcasts an earthquake alert using bayes rule we can write without loss of generality pb e a r pab e rpb e r we can repeat this for pb e r and continue pb e a r pab e rprb epebpb however the alarm is surely not directly influenced by any report on the radio that is pab e r pab e. similarly we can make other conditional independence assumptions such that pb e a r pab eprepepb specifying conditional probability tables alarm burglar earthquake radio earthquake the remaining tables are pb and pe the tables and graphical structure fully specify the distribution. now consider what happens as we observe evidence. initial evidence the alarm is sounding pb er pb e a r ber pb e a r er pa epb ber pa epbpepre draft march graphically representing distributions additional evidence the radio broadcasts an earthquake warning a similar calculation gives pb r thus initially because the alarm sounds sally thinks that she s been burgled. however this probability drops dramatically when she hears that there has been an earthquake. that is the earthquake explains away to an extent the fact that the alarm is ringing. see demoburglar.m. uncertain evidence in soft or uncertain evidence the variable is in more than one state with the strength of our belief about each state being given by probabilities. for example if x has the states domx blue green the vector represents the probabilities of the respective states. in contrast for hard evidence we are certain that a variable is in a particular state. in this case all the probability mass is in one of the vector components for example performing inference with soft-evidence is straightforward and can be achieved using bayes rule. writing the soft evidence as y we have pxypy y px y y where py i y represents the probability that y is in state i under the soft-evidence. this is a generalisation of hard-evidence in which the vector py y has all zero component values except for all but a single component. note that the soft evidence py i y does not correspond to the marginal py i in the original joint distribution px y. a procedure to form a joint distribution known as jeffrey s rule is to begin with an original distribution y from which we can define y x y using the soft evidence py y we then define a new joint distribution y y y in the bn we use a dashed circle to represent that a variable is in a soft-evidence state. example revisiting the earthquake scenario imagine that we think we hear the burglar alarm sounding but are not sure specifically we are only sure we heard the alarm. for this binary variable case we represent this soft-evidence for the states as a what is the probability of a burglary under this soft-evidence? pb a pb pb pb a a the probabilities pb and pb are calculated using bayes rule as before to give pb a uncertain evidence versus unreliable modelling an entertaining example of uncertain evidence is given by draft march graphically representing distributions b a g w b a h w b a g w j n b a g w figure mr holmes burglary worries as given in mrs modified problem. mrs gibbon is not drinking but somewhat deaf we represent such uncertain by a circle. holmes gets additional information from his neighbour mrs and informant dodgy virtual evidence can be represented by a dashed line. mr holmes receives a telephone call from his neighbour dr watson who states that he hears the sound of a burglar alarm from the direction of mr holmes house. while preparing to rush home mr holmes recalls that dr watson is known to be a tasteless practical joker and he decides to first call another neighbour mrs gibbon who despite occasional drinking problems is far more reliable. when mr holmes calls mrs gibbon he soon realises that she is somewhat tipsy. instead of answering his question directly she goes on and on about her latest back operation and about how terribly noisy and crime-ridden the neighbourhood has become. when he finally hangs up all mr holmes can glean from the conversation is that there is probably an chance that mrs gibbon did hear an alarm sound from her window. a bn for this scenario is depicted in which deals with four binary variables house is has sounded hears alarm and mrs hears pb a g w pabpbpwapga holmes is interested in the likelihood that his house has been burgled. naively holmes might pb trw tr g tr however after finding out about mrs gibbon s state mr holmes no longer finds the above model reliable. he wants to ignore the effect that mrs gibbon s evidence has on the inference and replace it with his own belief as to what mrs gibbon observed. mr holmes can achieve this by replacing the term pg tra by a so-called virtual evidence term pg tra pha where pha a tr a fa here the state h is arbitrary and fixed. this is used to modify the joint distribution to pb a h w pabpbpwapha see when we then compute pb trw tr h the effect of mr holmes judgement will count for a factor of times more in favour of the alarm sounding than not. the values of the table entries are irrelevant up to normalisation since any constants can be absorbed into the proportionality constant. note also that pha is not a distribution in a and hence no normalisation is required. this form of evidence is also called likelihood evidence. a twist on pearl s scenario is that mrs gibbon has not been drinking. however she is a little deaf and cannot be sure herself that she heard the alarm. she is sure she heard it. in this case holmes would might be tempted to include an additional variable as a parent of g. this would then require us to specify the joint distribution pgt a for the parental joint states of t and a. here we assume that we do not have access to such information. notation tr is equivalent to and fa to from draft march trust the model however the observation itself is now uncertain this can be dealt with using the soft evidence technique. from jeffrey s rule one uses the original model equation to compute a pgapw trapab tr ba pgapw trapab graphically representing distributions pb trw tr g pb tr w tr g pw tr g and then uses the soft-evidence g tr g fa pg g to compute pb trw tr g pb trw tr g trpg tr g pb trw tr g fapg fa g the reader may show that an alternative way to represent an uncertain observation such as mrs gibbon being non-tipsy but hard-of-hearing above is to use a virtual evidence child from g. uncertain evidence within an unreliable model to highlight uncertain evidence in an unreliable model we introduce two additional characters. mrs nosy lives next door to mr holmes and is completely deaf but nevertheless an incorrigible curtain-peeker who seems to notice most things. unfortunately she s also rather prone to imagining things. based on his conversation with her mr holmes counts her story as times in favour of there not being a burglary to there being a burglary and therefore uses a virtual evidence term b tr b fa b tr b fa pnosyb pjoeb mr holmes also telephones dodgy joe his contact in the criminal underworld to see if he s heard of any planned burglary on mr holmes home. he summarises this information using a virtual evidence term when all this information is combined mrs gibbon is not tipsy but somewhat hard of hearing mrs nosy and dodgy joe we first deal with the unreliable model pb a w tr g nosy joe pbpnosybpjoebpabpw trapga from which we can compute pb trw tr g nosy joe finally we perform inference with the soft-evidence pb trw tr g nosy joe pb trw tr g nosy joepg g g an important consideration above is that the virtual evidence does not replace the prior pb with another prior distribution rather the virtual evidence terms modify the prior through the inclusion of extra factors. the usual assumption is that each virtual evidence acts independently although one can consider dependent scenarios if required. draft march belief networks figure two bns for a variable distribution. both graphs and represent the same distribution strictly speaking they represent the same of independence assumptions the graphs say nothing about the content of the cpts. the extension of this cascade to many variables is clear and always results in a directed acyclic graph. belief networks in the wet grass and burglar examples we had a choice as to how we recursively used bayes rule. in a general variable case we could choose the factorisation an equally valid choice is in general two different graphs may represent the same independence assumptions as we will discuss further in if one wishes to make independence assumptions then the choice of factorisation becomes significant. the observation that any distribution may be written in the cascade form gives an algorithm for constructing a bn on variables xn write down the n variable cascade graph assign any ordering of the variables to the nodes you may then delete any of the directed connections. more formally this corresponds to an ordering of the variables which without loss of generality we may write as xn. then from bayes rule we have xn xn xn pxn xn n the representation of any bn is therefore a directed acyclic graph every probability distribution can be written as a bn even though it may correspond to a fully connected cascade dag. the particular role of a bn is that the structure of the dag corresponds to a set of conditional independence assumptions namely which parental variables are sufficient to specify each conditional probability table. note that this does not mean that non-parental variables have no influence. for example for distribution with dag this does not imply the dag specifies conditional independence statements of variables on their ancestors namely which ancestors are causes for the variable. the dag corresponds to a statement of conditional independencies in the model. to complete the specification of the bn we need to define all elements of the conditional probability tables pxipa once the graphical structure is defined the entries of the conditional probability tables pxipa can be expressed. for every possible state of the parental variables pa a value for each of the states of xi needs to be specified one since this is determined by normalisation. for a large number of parents writing out a table of values is intractable and the tables are usually parameterised in a low dimensional manner. this will be a central topic of our discussion on the application of bns in machine learning. draft march belief networks conditional independence whilst a bn corresponds to a set of conditional independence assumptions it is not always immediately clear from the dag whether a set of variables is conditionally independent of a set of other variables. for example in are and independent given the state of the answer is yes since we have now combining the two results above we have so that and are indeed independent conditioned on definition independence. x yz denotes that the two sets of variables x and y are independent of each other provided we know the state of the set of variables z. for full conditional independence x and y must be independent given all states of z. formally this means that px pxzpyz for all states of x in case the conditioning set is empty we may also write x y for x y in which case x is independent of y. if x and y are not conditionally independent they are conditionally dependent. this is written to develop intuition about conditional independence consider the three variable distribution we may write this in any of the ways where is any of the permutations of whilst all different dags they represent the same distribution namely that which makes no conditional independence statements. to make an independence statement we need to drop one of the links. this gives rise to the dags in are any of these graphs equivalent in the sense that they represent the same distribution? figure draft march belief networks figure by dropping say the connection between variables and we reduce the possible bn graphs amongst three variables to fully connected cascade graphs correspond to with with with with with and with any other graphs would be cyclic and therefore not distributions. x y x y x y z z z x y w z variable z is a collider. graphs figure in graphs and variable z is not a collider. and represent conditional independence x y z. in graphs and x and y are graphically conditionally dependent given variable z. applying bayes rule gives graphc graphd graphb so that dags and represent the same ci assumptions namely that given the state of variable variables and are independent however graph represents something fundamentally different namely there is no way to transform the distribution into any of the others. remark dependence. belief networks are good for encoding conditional independence but are not well suited to describing dependence. for example consider the trivial network pa b pbapa which has the dag representation a b. this may appear to encode that a and b are dependent. however there are certainly instances when this is not the case. for example it may be that the conditional is such that pba pb so that pa b papb. in this case although generally there may appear to be a graphical dependence from the dag there can be instances of the distributions for which dependence does not follow. the same holds for markov networks in which pa b b. whilst this suggests graphical dependence between a and b for b the variables are independent. the impact of collisions in a general bn how could we check if x y z? in x and y are independent when conditioned on z. in they are dependent in this situation variable z is called a collider the arrows of its draft march belief networks a b e c d figure the variable d is a collider along the path a b d c but not along the path a b d e. is a e b? a and b are not d-connected since there are no colliders on the only path between a and e and since there is a non-collider b which is in the conditioning set. hence a and b are d-separated i.e. a e b. neighbours are pointing towards it. what about in when we condition on z x and y will be graphically dependent since w px yz px y z pz pz pzwpwx ypxpy pxzpyz intuitively variable w becomes dependent on the value of z and since x and y are conditionally dependent on w they are also conditionally dependent on z. roughly speaking if there is a non-collider z which is conditioned on along the path between x and y in then this path does not make x and y dependent. similarly if there is a path between x and y which contains a collider provided that this collider is not in the conditioning set neither are any of its descendants then this path does not make x and y dependent. if there is a path between x and y which contains no colliders and no conditioning variables then this path d-connects x and y. note that a collider is defined relative to a path. in the variable d is a collider along the path a b d c but not along the path a b d e relative to this path the two arrows do not point inwards to d. consider the bn a b c. here a and c are independent. however conditioning of b makes them graphically dependent. intuitively whilst we believe the root causes are independent given the value of the observation this tells us something about the state of both the causes coupling them and making them dependent. d-separation the dag concepts of d-separation and d-connection are central to determining conditional independence in any bn with structure given by the definition d-separation. if g is a directed graph in which x y and z are disjoint sets of vertices then x and y are d-connected by z in g if and only if there exists an undirected path u between some vertex in x and some vertex in y such that for every collider c on u either c or a descendent of c is in z and no non-collider on u is in z. x and y are d-separated by z in g if and only if they are not d-connected by z in g. one may also phrase this as follows. for every variable x x and y y check every path u between x and y. a path u is said to be blocked if there is a node w on u such that either w is a collider and neither w nor any of its descendants is in z. w is not a collider on u and w is in z. draft march belief networks if all such paths are blocked then x and y are d-separated by z. if the variable sets x and y are d-separated by z they are independent conditional on z in all probability distributions such a graph can represent. the bayes ball provides a linear time complexity algorithm which given a set of nodes x and z determines the set of nodes y such that x y z. y is called the set of irrelevant nodes for x given z. d-connection and dependence given a dag we can imply with certainty that two variables are independent provided they are d-separated. can we infer that they are dependent provided they are d-connected? consider the following situation pa b c pca bpapb for which we note that a and b are d-connected by c. for concreteness we assume c is binary with states the question is whether a and b are dependent conditioned on c a b c. to answer this consider pa bc pc bpapb ab pc bpapb in general the first term pc b does not need to be a factored function of a and b and therefore a and b are conditionally graphically dependent. however we can construct cases where this is not so. for example let pc b and pc b pc b where and are arbitrary potentials between and then z a b pa bc z which shows that pa bc is a product of a function in a and function in b so that a and b are independent conditioned on c a second example is given by the distribution pa b c pcbpbapa in which a and c are d-connected by b. the question is are a and c dependent a c for simplicity we assume b takes the two states then pa c b pa c pa papb pcbpba pa pcb pcb pcb pb pcb pcb for the setting pb for some constant for all states of a then which is a product of a function of a and a function of c. hence a and c are independent. draft march belief networks c e a c e a b t d f s b g b d f s b t g u figure examples for d-separation. b d-separates a from e. the joint variables d d-connect a and e. c and e are d-connected. b dconnects a and e. figure t and f are d-connected by g. b and f are d-separated by u. the moral of the story is that d-separation necessarily implies independence. however d-connection does not necessarily imply dependence. it might be that there are numerical settings for which variables are independent even though they are d-connected. for this reason we use the term graphical dependence when the graph would suggest that variables are dependent even though there may be numerical instantiations were dependence does not hold see example consider is a e b? if we sum out variable d then we see that a and e are independent given b since the variable e will appear as an isolated factor independent of all other variables hence indeed a e b. whilst b is a collider which is in the conditioning set we need all colliders on the path to be in the conditioning set their descendants for d-connectedness. in if we sum out variable d then c and e become intrinsically linked and pa b c e will not factorise into a function of a multiplied by a function of e hence they are dependent. example consider the graph in are the variables t and f unconditionally independent i.e. t f here there are two colliders namely g and s however these are not in the conditioning set is empty and hence they are d-separated and therefore unconditionally independent. what about t f g? there is a collider on the path between t and f which is in the conditioning set. hence t and f are d-connected by g and therefore t and f are not independent conditioned on g. what about b f s? since there is a collider s in the conditioning set on the path between t and f then b and f are graphically conditionally dependent given s. example is f u in since the conditioning set is empty and every path from either b or f to u contains a collider b f are unconditionally independent of u. markov equivalence in belief networks draft march definition properties of belief networks. belief networks b b b b b b b a a a a a a a c c c c c c c pa b c pca bpapb a and b are independent pa b papb. a and b are conditionally dependent on c pa bc pacpbc. a b marginalising over c makes a and b independent. a b conditioning on c makes a and b dependent. pa b c pacpbcpc a and b are dependent pa b papb. a and b are conditionally independent on c pa bc pacpbc. a marginalising over c makes a and b dependent. b a a b b conditioning on c makes a and b independent. a b c c definition equivalence. two graphs are markov equivalent if they both represent the same set of conditional independence statements. define the skeleton of a graph by removing the directions on the arrows. define an immorality in a dag as a configuration of three nodes abc such that c is a child of both a and b with a and b not directly connected. two dags represent the same set of independence assumptions are markov equivalent if and only if they have the same skeleton and the same set of immoralities using this rule we see that in bns have the same skeleton with no immoralities and are therefore equivalent. however bn has an immorality and is therefore not equivalent to dags draft march causality belief networks have limited expressibility h figure two treatments and corresponding outcomes the health of a patient is represented by h. this dag embodies the conditional independence statements namely that the treatments have no effect on each other. one could represent the marginalised latent variable using a bi-directional edge. consider the dag in this dag could be used to represent two successive experiments where and are two treatments and and represent two outcomes of interest h is the underlying health status of the patient the first treatment has no effect on the second outcome hence there is no edge from to now consider the implied independencies in the marginal distribution obtained by marginalising the full distribution over h. there is no dag containing only the vertices which represents the independence relations and does not also imply some other independence relation that is not implied by consequently any dag on vertices alone will either fail to represent an independence relation of or will impose some additional independence restriction that is not implied by the dag. in the above example h hph cannot in general be expressed as a product of functions defined on a limited set of the variables. however it is the case that the conditional independence conditions hold in they are there encoded in the form of the conditional probability tables. it is just that we cannot see this independence since it is not present in the structure of the marginalised graph one can naturally infer this in the larger graph h. this example demonstrates that bns cannot express all the conditional independence statements that could be made on that set of variables set of conditional independence statements can be increased by considering extra latent variables however. this situation is rather general in the sense that any graphical model has limited expressibility in terms of independence it is worth bearing in mind that belief networks may not always be the most appropriate framework to express one s independence assumptions and intuitions. a natural consideration is to use a bi-directional arrow when a latent variable is marginalised. for one could depict the marginal distribution using a bi-directional edge similarly a bn with a latent conditioned variable can be represented using an undirected edge. for a discussion of these and related issues see causality causality is a contentious topic and the purpose of this section is make the reader aware of some pitfalls that can occur and which may give rise to erroneous inferences. the reader is referred to and for further details. the word causal is contentious particularly in cases where the model of the data contains no explicit temporal information so that formally only correlations or dependencies can be inferred. for a distribution pa b we could write this as either pabpb or pbapa. in we might think that b causes a and in a causes b. clearly this is not very meaningful since they both represent exactly the same distribution. formally belief networks only make independence statements not causal ones. nevertheless in constructing bns it can be helpful to think about dependencies in terms of causation draft march causality a b a b r w r w figure both and represent the same distribution pa b pabpb pbapa. the graph represents prain grasswet pgrasswetrainprain. we could equally have written praingrasswetpgrasswet although this appears to be causally non-sensical. g d r fd g d a dag for the relation befigure tween gender drug and recovery see influence diagram. no decision variable is required for g since g has no parents. r since our intuitive understanding is usually framed in how one variable influences another. first we discuss a classic conundrum that highlights potential pitfalls that can arise. simpson s paradox simpson s paradox is a cautionary tale in causal reasoning in bns. consider a medical trial in which patient treatment and outcome are recovered. two trials were conducted one with females and one with males. the data is summarised in the question is does the drug cause increased recovery? according to the table for males the answer is no since more males recovered when they were not given the drug than when they were. similarly more females recovered when not given the drug than recovered when given the drug. the conclusion appears that the drug cannot be beneficial since it aids neither subpopulation. however ignoring the gender information and collating both the male and female data into one combined table we find that more people recovered when given the drug than when not. hence even though the drug doesn t seem to work for either males or females it does seem to work overall! should we therefore recommend the drug or not? resolution of the paradox the paradox occurs since we are asking a causal interventional question. the question we are intuitively asking is if we give someone the drug what happens? however the calculation we performed above was only an observational calculation. the calculation we really want is to first intervene setting males recovered not recovered rec. rate given drug not given drug females given drug not given drug combined given drug not given drug recovered not recovered rec. rate recovered not recovered rec. rate table table for simpson s paradox draft march causality the drug state and then observe what effect this has on recovery. describes this as the difference between given that we see evidence versus given that we do evidence. a model of the gender drug and recovery data makes no conditional independence assumptions is pg d r prg dpdgpg an observational calculation concerns computing prg d and prd. interpretation however if we intervene and give the drug then the term pdg in equation should play no role in the experiment the distribution models that given the gender we select a drug with probability pdg which is not the case we decide to give the drug or not independent of gender. in the causal case we are modelling the causal experiment in this case the term pdg needs to be replaced by a term that reflects the setup of the experiment. in an atomic intervention a single variable is set in a particular in our atomic causal intervention in setting d we are dealing with the modified distribution in a causal pg rd prg dpg where the terms on the right hand side of this equation are taken from the original bn of the data. to denote an intervention we use prg d prg d prg dpg r prg dpg prg d can also consider here g as being interventional in this case it doesn t matter since the fact that the variable g has no parents means that for any distribution conditional on g the prior factor pg will not be present. using equation for the males given the drug recover versus recovery when not given the drug. for the females given the drug recover versus recovery when not given the drug. similarly prd prd g prg dpg rg prg dpg g prg dpg using the above post intervention distribution we have precoverydrug and precoveryno drug hence we correctly infer that the drug is overall not helpful as we intuitively expect and is consistent with the results from both subpopulations. here prd means that we first choose either a male or female patient at random and then give them the drug or not depending on the state of d. the point is that we do not randomly decide whether or not to give the drug hence the absence of the term pdg from the joint distribution. one way to think about such models is to consider how to draw a sample from the joint distribution of the random variables in most cases this should clarify the role of causality in the experiment. in contrast to the interventional calculation the observational calculation makes no conditional independence assumptions. this means that for example the term pdg plays a role in the calculation reader might wish to verify that the result given in the combined data in is equivalent to inferring with the full distribution equation general experimental conditions can be modelled by replacing pdg by an intervention distribution draft march definition s do operator. causality in a causal inference in which the effect of setting variables xck ck c in states xck is to be inferred this is equivalent to standard evidential inference in the post intervention distribution doxck xck xck pxcipa p c where any parental states of pa of xj are set in their evidential states. an alternative notation is xck in words for those variables for which we causally intervene and set in a particular state the corresponding terms pxcipa are removed from the original belief network. for variables which are evidential but non-causal the corresponding factors are not removed from the distribution. the interpretation is that the post intervention distribution corresponds to an experiment in which the causal variables are first set and non-causal variables are subsequently observed. influence diagrams and the do-calculus in making causal inferences we must adjust the model to reflect any causal experimental conditions. in setting any variable into a particular state we need to surgically remove all parental links of that variable. pearl calls this the do operator and contrasts an observational see inference pxy with a causal make or do inference pxdoy. a useful alternative representation is to append variables x upon which an intervention can possibly be made with a parental decision variable for pd g r fd pdfd gpgprg dpfd where pdfd g pdpa pdfd d g for d d and otherwise hence if the decision variable fd is set to the empty state the variable d is determined by the standard observational term pdpa if the decision variable is set to a state of d then the variable puts all its probability in that single state of d d. this has the effect of replacing the conditional probability term a unit factor and any instances of d set to the variable in its interventional a potential advantage of the influence diagram approach over the do-calculus is that deriving conditional independence statements can be made based on standard techniques for the augmented bn. additionally for parameter learning standard techniques apply in which the decision variables are set to the condition under which each data sample was collected causal or non-causal sample. example and accidents a causal belief network. the influence diagram is a distribution over variables in including decision variables in contrast to the application of ids in general cases can be considered in which the variables are placed in a distribution of states draft march parameterising belief networks y y y figure if all variables are binary states are required to specify here only states are required. noisy logic gates. fd fa d a consider the following cpt entries pd bad pa trd bad if we intervene and use a bad driver what is the probability of an accident? pa trd bad fd bad fa pa trd bad on the other hand if we intervene and make an accident what is the probability the driver involved is bad? this is pd bada tr fd fa tr pd bad learning the direction of arrows in the absence of data from causal experiments one should be justifiably sceptical about learning causal networks. nevertheless one might prefer a certain direction of a link based on assumptions of the simplicity of the cpts. this preference may come from a physical intuition that whilst root causes may be uncertain the relationship from cause to effect is clear. in this sense a measure of the complexity of a cpt is required such as entropy. such heuristics can be numerically encoded and the directions learned in an otherwise markov equivalent graph. parameterising belief networks consider a variable y with many parental variables xn formally the structure of the graph implies nothing about the form of the parameterisation of the table xn. if each parent xi has dim states and there is no constraint on the table then the table xn contains i dim entries. if stored explicitly for each state this would require potentially huge storage. an alternative is to constrain the table to have a simpler parametric form. for example one might write a decomposition in which only a limited number of parental interactions are required is called divorcing parents in for example in assuming all variables are binary the number of states requiring specification is compared to the states in the unconstrained case. the distribution can be stored using only independent parameters. draft march logic gates exercises another technique to constrain cpts uses simple classes of conditional tables. for example in one could use a logical or gate on binary zi say if at least one of the zi is in state otherwise we can then make a cpt by including the additional terms pzi when each xi is binary there are in total only quantities required for specifying pyx. in this case can be used to represent any noisy logic gate such as the noisy or or noisy and where the number of parameters required to specify the noisy gate is linear in the number of parents x. the noisy-or is particularly common in disease-symptom networks in which many diseases x can give rise to the same symptom y provided that at least one of the diseases is present the probability that the symptom will be present is high. further reading an introduction to bayesian networks and graphical models in expert systems is to be found in which also discusses general inference techniques which will be discussed during later chapters. code naive inference demo demoburglar.m was it the burglar demo demochestclinic.m naive inference on chest clinic conditional independence demo the following demo determines whether x y z for the chest clinic network and checks the result the independence test is based on the markov method of this is preferred over the d-separation method since it is arguably simpler to code and also more general in that it deals also with conditional independence in markov networks as well as belief networks. running the demo code below it may happen that the numerical dependence is very low that is px pxzpyz even though this highlights the difference between structural and numerical independence. condindeppot.m numerical measure of conditional independence democondindep.m demo of conditional independence markov method utility routines dag.m find the dag structure for a belief network exercises exercise animal. the party animal problem corresponds to the network in the boss is angry and the worker has a headache what is the probability the worker has been to a party? to code for conditional independence is given in draft march exercises d u h p a figure party animal. here all variables are binary. when set to the statements are true p been to party h got a headache d demotivated at work u underperform at work a angry. shaded variables are observed in the true state. a t x l e b s d x positive x-ray d dyspnea of breath e either tuberculosis or lung cancer t tuberculosis l lung cancer b bronchitis a visited asia s smoker figure belief network structure for the chest clinic example. complete the specifications the probabilities are given as follows pu trp tr d tr pu trp fa d tr pu trp fa d fa pu trp tr d fa exercise consider the distribution pa b c pca bpapb. is a b is a b c? exercise the chest clinic network concerns the diagnosis of lung disease lung cancer or both or neither. in this model a visit to asia is assumed to increase the probability of tuberculosis. state if the following conditional independence relationships are true or false tuberculosis smoking shortness of breath lung cancer bronchitis smoking visit to asia smoking lung cancer visit to asia smoking lung cancer shortness of breath. exercise consider the network in which concerns the probability of a car starting. pf empty pb bad pg emptyb good f not empty pg emptyb good f empty pg emptyb bad f not empty pg emptyb bad f empty pt fab good pt fab bad ps fat tr f empty ps fat tr f not empty ps fat fa f not empty ps fat fa f empty calculate p emptys no the probability of the fuel tank being empty conditioned on the observation that the car does not start. exercise consider the chest clinic bayesian network in calculate by hand the values for pd pds tr pds fa. the table values are pa tr pt tra tr pl trs tr pb trs tr px tre tr pd tre tr b tr pd tre fa b tr ps tr pt tra fa pl trs fa pb trs fa px tre fa pd tre tr b fa pd tre fa b fa pe trt l only if both t and l are fa otherwise. draft march exercises battery fuel gauge figure belief network of car not see turn over start exercise if we interpret the chest clinic network causally how can we help a doctor answer the question if i could cure my patients of bronchitis how would this affect my patients s chance of being short of breath? how does this compare with pd trb fa in a non-causal interpretation and what does this mean? exercise there is a synergistic relationship between asbestos exposure smoking and cancer a model describing this relationship is given by pa s c pca spaps is a s is a s c? how could you adjust the model to account for the fact that people who work in the building industry have a higher likelihood to also be smokers and also a higher likelihood to asbestos exposure? exercise consider the three variable distribution pa b c pabpbcpc where all variables are binary. how many parameters are needed to specify distributions of this form? exercise consider the belief network on the right which represents mr holmes burglary worries as given in mrs all variables take the two states fa. the table entries are pb tr pa trb tr pa trb fa pw tra tr pw tra fa pg tra tr pg tra fa compute by hand show your working pb trw tr pb trw tr g fa b a g w consider the same situation as above except that now the evidence is uncertain. mrs gibbon thinks that the state is g fa with probability similarly dr watson believes in the state w fa with value compute by hand the posteriors under these uncertain evidences pb tr w pb tr w g exercise a doctor gives a patient a or no drug dependent on their or young and or female. whether or not the patient or doesn t recover depends on all d a g. in addition a g write down the belief network for the above situation. draft march exercises explain how to compute precoverdrug. explain how to compute precoverdodrug young. exercise implement the wet grass scenario numerically using the brmltoolbox. exercise burglar. consider the burglar scenario we now wish to model the fact that in los angeles the probability of being burgled increases if there is an earthquake. explain how to include this effect in the model. exercise given two belief networks represented as dags with associated adjacency matrices a and b write a matlab function markovequivab.m that returns if a and b are markov equivalent and zero otherwise. exercise the adjacency matrices of two belief networks are given below abmatrices.mat. state if they are markov equivalent. a b exercise there are three computers indexed by i computer i can send a message in one timestep to computer j if cij otherwise cij there is a fault in the network and the task is to find out some information about the communication matrix c is not necessarily symmetric. to do this thomas the engineer will run some tests that reveal whether or not computer i can send a message to computer j in t timesteps t this is expressed as cijt with cij. for example he might know that meaning that according to his test a message sent from computer will arrive at computer in at most timesteps. note that this message could go via different routes it might go directly from to in one timestep or indirectly from to and then from to or both. you may assume cii a priori thomas thinks there is a probability that cij given the test information c compute the a posteriori probability vector exercise a belief network models the relation between the variables oil inf eh bp rt which stand for the price of oil inflation rate economy health british petroleum stock price retailer stock price. each variable takes the states low high except for bp which has states low high normal. the belief network model for these variables has tables draw a belief network for this distribution. given that bp stock price is normal and the retailer stock price is high what is the probability that inflation is high? exercise there are a set of c potentials with potential c defined on a subset of variables xc. if xc xd then can merge potentials c and d since c is contained within d. with reference to suitable graph structures describe an efficient algorithm to merge a set of potentials so that for the new set of potentials no potential is contained within the other. draft march exercises draft march chapter graphical models graphical models graphical models are depictions of independencedependence relationships for distributions. each form of gm is a particular union of graph and probability constructs and details the form of independence assumptions represented. gms are useful since they provide a framework for studying a wide class of probabilistic models and associated algorithms. in particular they help to clarify modelling assumptions and provide a unified framework under which inference algorithms in different communities can be related. it needs to be emphasised that all forms of gm have a limited ability to graphically expresses conditional as we ve seen belief networks are useful for modelling ancestral conditional independence. in this chapter we ll introduce other types of gm that are more suited to representing different assumptions. markov networks for example are particularly suited to modelling marginal dependence and conditional independence. here we ll focus on markov networks chain graphs marry belief and markov networks and factor graphs. there are many more inhabitants of the zoo of graphical models see the general viewpoint we adopt is to describe the problem environment using a probabilistic model after which reasoning corresponds to performing probabilistic inference. this is therefore a two part process modelling after identifying all potentially relevant variables of a problem environment our task is to describe how these variables can interact. this is achieved using structural assumptions as to the form of the joint probability distribution of all the variables typically corresponding to assumptions of independence of variables. each class of graphical model corresponds to a factorisation property of the joint distribution. inference once the basic assumptions as to how variables interact with each other is formed the probabilistic model is constructed all questions of interest are answered by performing inference on the distribution. this can be a computationally non-trivial step so that coupling gms with accurate inference algorithms is central to successful graphical modelling. whilst not a strict separation gms tend to fall into two broad classes those useful in modelling and those useful in representing inference algorithms. for modelling belief networks markov networks chain graphs and influence diagrams are some of the most popular. for inference one typically compiles a model into a suitable gm for which an algorithm can be readily applied. such inference gms include factor graphs junction trees and region graphs. markov networks markov networks figure pa pb pc belief networks correspond to a special kind of factorisation of the joint probability distribution in which each of the factors is itself a distribution. an alternative factorisation is for example and z is a constant which ensures normalisation called the pa b c z b c where b and c are potentials partition function z b c abc we will typically use the convention that the ordering of the variables in the potential is not relevant for a distribution the joint variables simply index an element of the potential table. markov networks are defined as products of non-negative functions defined on maximal cliques of an undirected graph see case of a potential satisfying normalisation definition a potential is a non-negative function of the variable x a joint potential xn is a non-negative function of the set of variables. a distribution is a special x this holds similarly for continuous variables with summation replaced by integration. xn z definition network. for a set of variables x xn a markov network is defined as a product of potentials on subsets of the variables xc x cxc graphically this is represented by an undirected graph g with xc c c being the maximal cliques of g. the constant z ensures the distribution is normalised. the graph is said to satisfy the factorisation property. in the special case that the graph contains cliques of only size the distribution is called a pairwise markov network with potentials defined on each link between two variables. for the case in which clique potentials are strictly positive this is called a gibbs distribution. remark markov network. whilst a markov network is formally defined on maximal cliques in practice authors often use the term to refer to non-maximal cliques. for example in the graph on the right the maximal cliques are and so that the graph describes a distribution in a pairwise network though the poten tials are assumed to be over two-cliques giving draft march markov networks definition of markov networks. b b b a a a c c c pa b c aca c bcb cz a and b are unconditionally dependent pa b papb. a and b are conditionally independent on c pa bc pacpbc. a marginalising over c makes a and b dependent. b a b conditioning on c makes a and b independent. markov properties we here state some of the most useful results. the reader is referred to for proofs and more detailed discussion. consider the markov network in here we use the shorthand etc. we will use this undirected graph to demonstrate conditional independence properties. local markov property definition markov property. pxxx pxne when conditioned on its neighbours x is independent of the remaining variables of the graph. the conditional distribution is the last line above follows since the variable only appears in the cliques that border the generalisation of the above example is clear a mn with positive clique potentials defined with respect to an undirected graph g pxixi pxine pairwise markov property definition markov property. for any non-adjacent vertices x and y x yxx y notation xi is shorthand for the set of all variables x excluding variable xi namely xxi in set notation. draft march markov networks figure by the global markov property since every path from to passes through then where the last line follows since for fixed the function is a product of a function on and a function on implying independence. global markov property definition a subset s separates a subset a from a subset b if every path from any member of a to any member of b passes though s. definition markov property. for a disjoint subset of variables where s separates a from b in g then a bs. this implies that example machine. a boltzmann machine is a mn on binary variables domxi of the form i bixi ij wij xixj px zw b e where the interactions wij are the weights and the bi the biases. this model has been studied in the machine learning community as a basic model of distributed memory and the graphical model of the bm is an undirected graph with a link been nodes i and j for wij consequently for all but specially constrained w the graph is multiply-connected and inference will be typically intractable. gibbs networks for simplicity we assume that the potentials are strictly positive in which case mns are also termed gibbs networks. in this case a gn satisfies the following independence relations draft march markov networks figure local distributions. the markov network consistent with the local distributions. if the local distributions are positive by the hammersley-clifford theorem the only joint distribution that can be consistent with the local distributions must be a gibbs distribution with structure given by markov random fields definition random field. a mrf is defined by a set of distributions pxine where i n indexes the distributions and ne are the neighbours of variable xi namely that subset of the variables xn that the distribution of variable xi depends on. the term markov indicates that this is a proper subset of the variables. a distribution is an mrf with respect to an undirected graph g if pxixi pxine where ne are the neighbouring variables of variable xi according to the undirected graph g. hammersley clifford theorem the hammersley-clifford theorem helps resolve questions as to when a set of positive local distributions pxine could ever form a consistent joint distribution xn. local distributions pxine can form a consistent joint distribution if and only if xn factorises according to xn exp vcxc z indexed by c. equation is equivalent c where the sum is over all cliques and vcxc is a real function defined over the variables in the clique c namely a mn on positive clique potentials. the graph over which the cliques are defined is an undirected graph with a link between xi and xj if pxixi pxixij that is if xj has an effect on the conditional distribution of xi then add an undirected link between xi and xj. this is then repeated over all the variables see note that the hc theorem does not mean that given a set of conditional distributions we can always form a consistent joint distribution from them rather it states what the functional form of a joint distribution for the conditionals to be consistent with the joint see conditional independence using markov networks for x each being collections of variables in we discussed an algorithm to determine x yz. an alternative and more general method it handles directed and undirected graphs uses the following steps draft march a c e h i j b d f k g b d f a c e i markov networks figure belief network for which we are interested in checking conditional independence a b i. ancestral moralised graph for a b i. every path from a red to green node passes through a yellow node so a and b are independent given d i. alternatively if we consider a b i the variable d is uncoloured and we can travel from the red to the green without encountering in this case a is a yellow node the e f path. dependent on b conditioned on i. ancestral graph remove from the dag any node which is neither in x y z nor an ancestor of a node in this set together with any edges in or out of such nodes. moralisation add a line between any two remaining nodes which have a common child but are not already connected by an arrow. then remove remaining arrowheads. separation in the undirected graph so constructed look for a path which joins a node in x to one in y but does not intersect z. if there is no such path deduce that x yz. for markov networks only the final separation criterion needs to be applied. see for an example. lattice models undirected models have a long history in different branches of science especially statistical mechanics on lattices and more recently as models in visual processing in which the models encourage neighbouring variables to be in the same consider a model in which our desire is that states of the binary valued variables arranged on a lattice should prefer their neighbouring variables to be in the same state i j z ijxi xj where i j denotes the set of indices where i and j are neighbours in the undirected graph. the ising model a set of potentials for equation that encourages neighbouring variables to have the same state is ijxi xj e xj this corresponds to a well-known model of the physics of magnetic systems called the ising model which consists of mini-magnets which prefer to be aligned in the same state depending on the temperature figure onsagar magnetisation. as the temperature t decreases towards the critical temperature tc a phase transition occurs in which a large fraction of the variables become aligned in the same state. draft march chain graphical models a c b d a b a cd b e g d f c c aedf bg figure chain graphs. the chain components are identified by deleting the directed edges and idena chain components are d which can be tifying the remaining connected components. written as a bn on the cluster variables in chain components are e d f g which has the cluster bn representation t for high t the variables behave independently so that no global magnetisation appears. for low t there is a strong preference for neighbouring mini-magnets to become aligned generating a strong macromagnet. remarkably one can show that in a very large two-dimensional lattice below the so-called curie temperature tc variables the system admits a phase change in that a large fraction of the variables become aligned above tc on average the variables are unaligned. this is depicted in xin is the average alignment of the variables. that this phase change happens where m for non-zero temperature has driven considerable research in this and related global coherence effects such as this that arise from weak local constraints are present in systems that admit emergent behaviour. similar local constraints are popular in image restoration algorithms to clean up noise under the assumption that noise will not show any local spatial coherence whilst signal will. an example is given in where we discuss algorithms for inference under special constraints on the mrf. chain graphical models definition component. the chain components of a graph g are obtained by forming a graph with directed edges removed from g. then each connected component in constitutes a chain component. chain graphs contain both directed and undirected links. to develop the intuition consider the only terms that we can unambiguously specify from this depiction are pa and pb since there is no mixed interaction of directed and undirected edges at the a and b vertices. by probability therefore we must have pa b c d papbpc da b looking at the graph we might expect the interpretation to be pc da b dpcapdb cd however to ensure normalisation and also to retain generality we interpret this chain component as pc da b dpcapdb b with b dpcapdb this leads to the interpretation of a cg as a dag over the chain components. each chain component represents a distribution over the variables of the component conditioned on the parental components. the conditional distribution is itself a product over the cliques of the undirected component and moralised parental components including also a factor to ensure normalisation over the chain component. draft march expressiveness of graphical models definition graph distribution. the distribution associated with a chain graph g is found by first identifying the chain components then px and p p c c where c denotes the union of the cliques in component together with the moralised parental components of with being the associated functions defined on each clique. the proportionality factor is determined implicitly by the constraint that the distribution sums to bns are cgs in which the connected components are singletons. mns are cgs in which the chain components are simply the connected components of the undirected graph. cgs can be useful since they are more expressive of ci statements than either belief networks or markov networks alone. the reader is referred to and for further details. example graphs are more expressive than belief or markov networks. consider the chain graph in which has chain component decomposition pa b c d e f papbpc d e fa b where pc d e fa b c e f f b b with the normalisation requirement b c e f f b the marginal pc d e f is given by e f ab bpapb c b cdef since the marginal distribution of pc d e f is an undirected no dag can express the ci statements contained in the marginal pc d e f. similarly no undirected distribution on the same skeleton as could express that a and b are independent i.e. pa b papb. expressiveness of graphical models it is clear that directed distributions can be represented as undirected distributions since one can associate each factor in a directed distribution with a potential. for example the distribution pabpbcpc can be factored as b c where b pab and c pbcpc with z hence every belief network can be represented as some mn by simple identification of the factors in the distributions. however in general the associated undirected graph corresponds to the draft march expressiveness of graphical models a c e b d f c e d f c e d f figure the cg expresses a b and d e f. no directed graph could express both these conditions since the marginal distribution pc d e f is an undirected four cycle any dag on a cycle must contain a collider as in and therefore express a different set of ci statements than similarly no connected markov network can express unconditional independence and hence expresses ci statements that no belief network or markov network alone can express. moralised directed graph will contain additional links and independence information can be lost. for example the mn of pca bpapb if a single clique b c from which one cannot graphically infer that a b the converse question is whether every undirected model can be represented by a bn with a readily derived link structure? consider the example in in this case there is no directed model with the same link structure that can express the in the undirected graph. naturally every probability distribution can be represented by some bn though it may not necessarily have a simple structure and be simply a fully connected cascade style graph. in this sense the dag cannot graphically represent the independencedependence relations true in the distribution. definition maps. a graph is an independence map of a given distribution p if every conditional independence statement that one can derive from the graph g is true in the distribution p that is x yz g x yz p for all disjoint sets x similarly a graph is a dependence map of a given distribution p if every conditional independence statement that one can derive from p is true in the graph g. that is x yz g x yz p for all disjoint sets x a graph g which is both an i-map and a d-map for p is called a perfect map and x yz g x yz p for all disjoint sets x in this case the set of all conditional independence and dependence statements expressible in the graph g are consistent with p and vice versa. due to inverse modus ponens a dependence map is equivalent to g p although this is less useful since standard graphical model representations cannot express dependence. note that the above definitions are not dependent on the graph being directed or undirected. indeed some distributions may have a perfect directed map but no perfect undirected map. for example px y z pzx ypxpy draft march factor graphs b b a c a c d d figure an undirected model for which we wish to find a directed equivalent. every dag with the same structure as the undirected model must have a situation where two arrows will point to a node such as node d. summing over the states of variable d will leave a dag on the variables a b c with no link between a and c. this cannot represent the undirected model since when one marginals over d in the undirected this adds a link between a and c. has a directed perfect map x z y that pzx y xx yy but no perfect undirected map. example consider the distribution defined on variables h the bn hph is an i-map for distribution since every independence statement in the bn is true for the corresponding graph. however it is not a d-map since cannot be inferred from the bn. similarly no undirected graph can represent all independence statements true in in this case no perfect map bn or a mn can represent factor graphs factor graphs are mainly used as part of inference definition graph. given a function xn x i i the fg has a node by a square for each factor i and a variable node by a circle for each variable xj. for each xj x i an undirected link is made between factor i and variable xj. for a factor i parents to the factor node and a directed link from the factor node to the child. this has the same structure as an fg but preserves the information that the factors are distributions. x which is a conditional distribution pxipa we may use directed links from the factor graphs are useful since they can preserve more information about the form of the distribution than either a belief network or a markov network chain graph can do alone. consider the distribution pa b c b c c a fg is an alternative graphical depiction of a in which the vertices represent variables and a hyperedge a factor as a function of the variables associated with the hyperedge. a fg is therefore a hypergraph with the additional interpretation that the graph represents a function defined as products over the associated hyperedges. many thanks to robert cowell for this observation. draft march exercises a a a a a c b c b c b c b c b a c b d b c a. figure b c. b c. both and have the same undirected graphical model directed fg of the bn in is an undirected fg of the advantage of over is that information regarding the marginal independence of variables b and c is clear from graph whereas one could only ascertain this by examination of the numerical entries of the a partially directed fg of pab c c d. no directed undirected or factors in graph chain graph can represent both the conditional and marginal independence statements expressed by this graph and also the factored structure of the undirected terms. the mn representation is given in however could equally represent some unfactored clique potential b c. in this sense the fg representation in more precisely conveys the form of distribution equation an unfactored clique potential b c is represented by the fg hence different fgs can have the same mn since information regarding the structure of the clique potential is lost in the mn. conditional independence in factor graphs a rule which works with both directed and undirected partially directed fgs is as to determine whether two variables are independent given a set of conditioned variables consider all paths connecting the two variables. if all paths are blocked the variables are conditionally independent. a path is blocked if any one or more of the following conditions are satisfied one of the variables in the path is in the conditioning set. one of the variables or factors in the path has two incoming edges that are part of the path and neither the variable or factor nor any of its descendants are in the conditioning set. notes a detailed discussion of the axiomatic and logical basis of conditional independence is given in and code condindep.m conditional independence test px y pxzpy exercises exercise consider the pairwise markov network px express in terms of the following draft march exercises for a set of local distributions defined as is it always possible to find a joint distribution consistent with these local conditional distributions? exercise consider the markov network pa b c aba b bcb c nominally by summing over b the variables a and c are dependent. for binary b explain a situation in which this is not the case so that marginally a and c are independent. exercise show that for the boltzmann machine px zw b extwxxtb one may assume without loss of generality w wt. exercise the restricted boltzmann machine is a specially constrained boltzmann machine on a bipartite graph consisting of a layer of visible variables v vv and hidden variables h hh pv h zw a b evtwhatvbth all variables are binary taking states show that the distribution of hidden units conditional on the visible units factorises as bi j wjivj phv i phiv with phiv where ex. by symmetry arguments write down the form of the conditional pvh. is ph factorised? can the partition function zw a b be computed efficiently for the rbm? exercise consider px is it possible to compute argmax px efficiently? exercise you are given that x yz u u z derive the most general form of probability distribution px y z u consistent with these statements. does this distribution have a simple graphical model? draft march exercises exercise the undirected graph represents a markov network with nodes counting clockwise around the pentagon with potentials show that the joint distribution can be written as and express the marginal probability tables explicitly as functions of the potentials xj. exercise consider the belief network on the right. write down a markov network of is your markov network a perfect map of exercise two research labs work independently on the relationship between discrete variables x and y. lab a proudly announces that they have ascertained distribution paxy from data. lab b proudly announces that they have ascertained pbyx from data. is it always possible to find a joint distribution px y consistent with the results of both labs? is it possible to define consistent marginals px and py in the sense that px and py y paxypy x pbyxpx? if so explain how to find such marginals. if not explain why not. exercise research lab a states its findings about a set of variables xn as a list la of conditional independence statements. lab b similarly provides a list of conditional independence statements lb. is it possible to find a distribution which is consistent with la and lb? if the lists also contain dependence statements how could one attempt to find a distribution that is consistent with both lists? exercise consider the distribution px y w z pzwpwx ypxpy write pxz using a formula involving or some of pzw pwx y px py. write pyz using a formula involving or some of pzw pwx y px py. using the above results derive an explicit condition for x y z and explain if this is satisfied for this distribution. exercise consider the distribution h draw a belief network for this distribution. can the distribution h be written as a non-complete belief network? show that for as defined above draft march exercise consider the distribution pa b c d aba b bcb c cdc d dad a where the are potentials. draw a markov network for this distribution. exercises explain if the distribution can be represented as a non-complete belief network. derive explicitly if a c exercise show how for any singly-connected markov network one may construct a markov equivalent belief network. exercise consider a pairwise binary markov network defined on variables si i n ij e ijsi sj where e is a given edge set and the potentials ij are arbitrary. explain how to translate such a markov network into a boltzmann machine. with ps draft march chapter efficient inference in trees marginal inference given a distribution xn inference is the process of computing functions of the distribution. for example computing a marginal conditioned on a subset of variables being in a particular state would be an inference task. similarly computing the mean of a variable can be considered an inference task. the main focus of this chapter is on efficient inference algorithms for marginal inference in singly-connected structures. an efficient algorithm for multiply-connected graphs will be considered in marginal inference is concerned with the computation of the distribution of a subset of variables possibly conditioned on another subset. for example given a joint distribution a marginal inference given evidence calculation is tr tr marginal inference for discrete models involves summation and will be the focus of our development. in principle the algorithms carry over to continuous variable models although the lack of closure of most continuous distributions under marginalisation gaussian being a notable exception can make the direct transference of these algorithms to the continuous domain problematic. variable elimination in a markov chain and message passing a key concept in efficient inference is message passing in which information from the graph is summarised by local edge information. to develop this idea consider the four variable markov chain chains are discussed in more depth in pa b c d pabpbcpcdpd as given in for which our task is to calculate the marginal pa. for simplicity we assume that each of the variables has domain then pa pa b c d b b pa a b c d figure a markov chain is of the form pxt for some assignment of the variables to labels xt. variable elimination can be carried out in time linear in the number of variables in the chain. we could carry out this computation by simply summing each of the probabilities for the states of the variables bc and d. marginal inference a more efficient approach is to push the summation over d as far to the right as possible pa b pa d pcdpd dc where d is a state potential. similarly we can distribute the summation over c as far to the right as possible pa b pa c then finally pa b pa c pbc d cb by distributing the summations we have made additions compared to from the naive approach. whilst this saving may not appear much the important point is that the number of computations for a chain of length t would scale linearly with t as opposed to exponentially for the naive approach. this procedure is naturally enough called variable elimination since each time we sum over the states of a variable we eliminate it from the distribution. we can always perform variable elimination in a chain efficiently since there is a natural way to distribute the summations working inwards from the edges. note that in the above case the potentials are in fact always distributions we are just recursively computing the marginal distribution of the right leaf of the chain. one can view the elimination of a variable as passing a message to a neighbouring vertex on the graph. we can calculate a univariate-marginal of any singly-connected graph by starting at a leaf of the tree eliminating the variable there and then working inwards nibbling off each time a leaf of the remaining tree. provided we perform elimination from the leaves inwards then the structure of the remaining graph is simply a subtree of the original tree albeit with the conditional probability table entries modified to potentials which update under recursion. this is guaranteed to enable us to calculate any marginal pxi using a number of summations which scales linearly with the number of variables in the graph. finding conditional marginals for a chain consider the following inference problem given pa b c d pabpbcpcdpd find pda. this can be computed using bc bc pda pa b c d pabpbcpcdpd pcdpd c c b pabpbc bc the fact the missing proportionality constant is found by repeating the computation for all states of variable d. since we know that pda k c where c is the unnormalised result of the summation we can use d pda to infer that k d c draft march marginal inference in this example the potential b is not a distribution in c nor is c in general one may view variable elimination as the passing of messages in the form of potentials from nodes to their neighbours. for belief networks variable elimination passes messages that are distributions when following the direction of the edge and non-normalised potentials when passing messages against the direction of the edge. remark variable elimination in trees as matrix multiplication variable elimination is related to the associativity of matrix multiplication. for equation above we can define matrices pa ib j pc id j pb ic j pd i pa i then the marginal ma can be written ma mabmbcmcdmd mabmbcmcdmd since matrix multiplication is associative. this matrix formulation of calculating marginals is called the transfer matrix method and is particularly popular in the physics example will the fly be?. you live in a house with three rooms labelled there is a door between rooms and and another between rooms and one cannot directly pass between rooms and in one time-step. an annoying fly is buzzing from one room to another and there is some smelly cheese in room which seems to attract the fly more. using xt for which room the fly is in at time t with domxt the movement of the fly can be described by a transition ixt j mij where m is a transition matrix m t the transition matrix is stochastic in the sense that as required of a conditional probability distribution mij given that the fly is in room at time what is the probability of room occupancy at time t assume a markov chain which is defined by the joint distribution xt we are asked to compute which is given by since the graph of the distribution is a markov chain we can easily distribute the summation over the terms. this is most easily done using the transfer matrix method giving draft march where v is a vector with components reflecting the evidence that at time the fly is in room computing this we have decimal places of accuracy marginal inference xt similarly after time-steps the occupancy probabilities are the room occupancy probability is converging to a particular distribution the stationary distribution of the markov chain. one might ask where the fly is after an infinite number of time-steps. that is we are interested in the large t behaviour of at convergence pxt. writing p for the vector describing the stationary distribution this means p mp in other words p is the eigenvector of m with eigenvalue computing this numerically the stationary distribution is note that software packages usually return eigenvectors with ete the unit eigenvector therefore will usually require normalisation to make this a probability. the sum-product algorithm on factor graphs both markov and belief networks can be represented using factor graphs. for this reason it is convenient to derive a marginal inference algorithm for the fg since this then applies to both markov and belief networks. this is termed the sum-product algorithm since to compute marginals we need to distribute the sum over variable states over the product of factors. in older texts this is referred to as belief propagation. non-branching graphs variable to variable messages consider the distribution pa b c d b c d which has the factor graph represented in with factors as defined by the f above. to compute the marginal pa b c since the variable d only occurs locally we use pa b c pa b c d d d b c d b d similarly pa b c pa b c c hence c b c c d c c d c c bb it is clear how one can recurse this definition of messages so that for a chain of length n variables the marginal of the first node can be computed in time linear in n. the term c b can be interpreted as draft march d d cc marginal inference a b c d figure for singly-connected structures without branches simple messages from one variable to its neighbour may be defined to form an efficient marginal inference scheme. carrying marginal information from the graph beyond c. for any singly-connected structure the factors at the edge of the graph can be replaced with messages that reflect marginal information from the graph beyond that factor. for simple linear structures with no branching messages from variables to variables are sufficient. however as we will see below it is useful in more general structures with branching to consider two types of messages namely those from variables to factors and vice versa. general singly-connected factor graphs the slightly more complex example pabpbc dpcpdped has the factor graph b c d e if the marginal pa b is to be represented by the amputated graph with messages on the edges then e e in this case it is natural to consider messages from factors to variables. similarly we can break the message from the factor into messages arriving from the two branches through c and d namely pa b cd c d bb b c d c cd e d e similarly we can interpret d dd e e dd to complete the interpretation we identify c c in a non-branching link one can more simply use a variable to variable message. a b e c d figure for a branching singly-connected graph it is useful to define messages from both factors to variables and variables to factors. draft march for consistency of interpretation one also can view the above as a convenience of this approach is that the messages can be reused to evaluate other marginal inferences. for example it is clear that pb is given by marginal inference to compute the marginal pa we then have b b aa b b b b b b pa b a a pb c bb if we additionally desire pc we need to define the message from to c c d b d bd where b b this demonstrates the reuse of already computed message from d to to compute the marginal pc. definition schedule. a message schedule is a specified sequence of message updates. a valid schedule is that a message can be sent from a node only when that node has received all requisite messages from its neighbours. in general there is more than one valid updating schedule. sum-product algorithm the sum-product algorithm is described below in which messages are updated as a function of incoming messages. one then proceeds by computing the messages in a schedule that allows the computation of a new message based on previously computed messages until all messages from all factors to variables and vice-versa have been computed. definition messages on factor graphs. x provided the given a distribution defined as a product on subsets of the variables px factor graph is singly-connected we can carry out summation over the variables efficiently. f f z initialisation messages from extremal node factors are initialised to the factor. messages from extremal variable nodes are set to unity. variable to factor message x f g g x f xx x x x f x f f x draft march marginal inference factor to variable message f x f f y x fx y y f we y x fx to emphasise that we sum over all states in the set of variables x fx. marginal f nex px f x y fy f y f y xx x f x f f x x x x for marginal inference the important information is the relative size of the message states so that we may renormalise messages as we wish. since the marginal will be proportional to the incoming messages for that variable the normalisation constant is trivially obtained using the fact that the marginal must sum to however if we wish to also compute any normalisation constant using these messages we cannot normalise the messages since this global information will then be lost. to resolve this one may work with log messages to avoid numerical underoverflow problems. the sum-product algorithm is able to perform efficient marginal inference in both belief and markov networks since both are expressible as factor graphs. this is the reason for the preferred use of the factor graph since it requires only a single algorithm and is agnostic to whether or not the graph is a locally or globally normalised distribution. computing the marginal likelihood for a distribution defined as products over potentials f f f f x x px z z x f the normalisation is given by x to compute this summation efficiently we take the product of all incoming messages to an arbitrarily chosen variable x and then sum over the states of that variable z x f nex f x if the factor graph is derived from setting a subset of variables of a bn in evidential states px pxv pv then the summation over all non-evidential variables will yield the marginal on the visible variables pv. for this method to work the absolute relative values of the messages are required which prohibits renormalisation at each stage of the message passing procedure. however without normalisation the draft march marginal inference a d b c a b c figure factor graph with a loop. eliminating the variable d adds an edge between a and c demonstrating that in general one cannot perform marginal inference in loopy graphs by simply passing messages along existing edges in the original graph. numerical value of messages can become very small particularly for large graphs and numerical precision issues can occur. a remedy in this situation is to work with log messages for this the variable to factor messages become simply log g x g x g g x f x f f x f f f x log f f y x fx naively one may write more care is required for the factors to variable messages which are defined by y x fx y y f y y f however the exponentiation of the log messages will cause potential numerical precision problems. a solution to this numerical difficulty is obtained by finding the largest value of the incoming log messages y f max y y f then f x y x fx y f log f f y y f y f by construction the terms e contributions to the summation are computed accurately. y y f y f will be this ensures that the dominant numerical log marginals are readily found using log px f f x draft march other forms of inference the problem with loops loops cause a problem with variable elimination message passing techniques since once a variable is eliminated the structure of the amputated graph in general changes. for example consider the fg pa b c d b c d d the marginal pa b c is given by pa b c b d d d which adds a link ac in the amputated graph see this means that one cannot account for information from variable d by simply updating potentials on links in the original graph one needs to account for the fact that the structure of the graph changes. the junction tree algorithm is a widely used technique to deal with this and essentially combines variables together in order to make a new singly-connected graph for which the graph structure remains singly-connected under variable elimination. other forms of inference max-product a common interest is the most likely state of distribution. that is argmax p xn to compute this efficiently we exploit any factorisation structure of the distribution analogous to the sum-product algorithm. that is we aim to distribute the maximization so that only local computations are required. to develop the algorithm consider a function which can be represented as an undirected chain for which we wish to find the joint state x which maximises f. firstly we calculate the maximum value of f. since potentials are non-negative we may write max x fx max max max max max max max max the final equation corresponds to solving a single variable optimisation and determines both the optimal value of the function f and also the optimal state x given x the optimal is given by x and so on. this procedure is called backtracking. note that we could have equally started at the other end of the chain by defining messages that pass information from xi to and similarly x argmax argmax argmax the chain structure of the function ensures that the maximal value its state can be computed in time which scales linearly with the number of factors in the function. there is no requirement here that the function f corresponds to a probability distribution the factors must be non-negative. draft march other forms of inference example consider a distribution defined over binary variables pa b c pabpbcpc with pa trb tr pa trb fa pb trc tr pb trc fa pc tr what is the most likely joint configuration argmax pa b c? abc naively we could evaluate pa b c over all the joint states of a b c and select that states with highest probability. a message passing approach is to define max c pbcpc for the state b tr pb trc trpc tr pb trc fapc fa hence tr similarly for b fa pb fac trpc tr pb fac fapc fa hence fa we now consider max b pab for a tr the state b tr has value pa trb tr tr and state b fa has value pa trb fa fa hence tr similarly for a fa the state b tr has value pa fab tr tr and state b fa has value pa fab fa fa giving fa now we can compute the optimal state argmax a a fa given this optimal state we can backtrack giving argmax b b pa fab fa c argmax c pb facpc fa note that in the backtracking process we already have all the information required from the computation of the messages draft march other forms of inference using a factor graph one can also use the factor graph to compute the joint most probable state. provided that a full schedule of message passing has occurred the product of messages into a variable equals the maximum value of the joint function with respect to all other variables. one can then simply read off the most probable state by maximising this local potential. one then proceeds in computing the messages in a schedule that allows the computation of a new message based on previously computed messages until all messages from all factors to variables and vice-versa have been computed. the message updates are given below. definition messages on factor graphs. x provided the given a distribution defined as a product on subsets of the variables px factor graph is singly-connected we can carry out maximisation over the variables efficiently. f f z initialisation messages from extremal node factors are initialised to the factor. messages from extremal variable nodes are set to unity. variable to factor message x f g g x f xx x x f f x x x f factor to variable message f x max y x fx f f y maximal state x argmax x f nex f x y f f y f y xx x f x y fy f f x x x x in earlier literature this algorithm is called belief revision. finding the n most probable states it is often of interest to calculate not just the most likely joint state but the n most probable states particularly in cases where the optimal state is only slightly more probable than other states. this is an interesting problem in itself and can be tackled with a variety of methods. a general technique is given by which is based on the junction tree formalism and the construction of candidate lists see for example draft march other forms of inference figure state transition diagram not shown. the shortest path from state to state is considered as a markov chain walk the most probable path from state to state is the latter path is longer but more probable since for the path the probability of exiting from state into state is each transition is equally likely. see demomostprobablepath.m for singly-connected structures several approaches have been developed. for the hidden markov model a simple algorithm is the n-viterbi approach which stores the n-most probable messages at each stage of the propagation see for example a special case of nilsson s approach is available for hidden markov which is the particularly efficient for large state spaces. for more general singly-connected graphs one can extend the max-product algorithm to an n-max-product algorithm by retaining at each stage the n most probable messages see below. these techniques require n to be specified a-priori compared to anytime alternatives an alternative approach for singlyconnected networks was developed in of particular interest is the application of the singly-connected algorithms as an approximation when for example nilsson s approach on a multiply-connected graph is n-max-product the algorithm for n-max-product is a straightforward modification of the standard algorithms. computationally a straightforward way to accomplish this is to introduce an additional variable for each message that is used to index the most likely messages. for example consider the distribution pa b c d b c d for which we wish to find the two most probable values. using the notation imax x fx for the ith highest value of fx the maximisation over d can be expressed using the message db d d db d d using a similar message for the maximisation over c the most likely states of pa b c d can be computed using abcd abmbmd b db md cb mc where mc and md index the highest values. at the final stage we now have a table with dim a dim b entries from which we compute the highest two states. the generalisation of this to the factor graph formalism is straightforward and contained in maxnprodfg.m. essentially the only modification required is to define extended messages which contain the n-most likely messages computed at each stage. at a junction of the factor graph all the messages from the neighbours along with their n-most probable tables are multiplied together into a large table. for a factor to variable message the n-most probable messages are retained see maxnprodfg.m. the n-most probable states for each variable can then be read off by finding the variable state that maximises the product of incoming extended messages. draft march other forms of inference most probable path and shortest path what is the most likely path from state a to state b for an n state markov chain? note that this is not necessarily the same as the shortest path as explained in if assume that a length t path exists this has probability pst bst finding the most probable path can then be readily solved using the max-product max-sum algorithm for the log-transitions on a simple serial factor graph. to deal with the issue that we don t know the optimal t one approach is to redefine the probability transitions such that the desired state b is an absorbing state of the chain is one can enter this state but not leave it. with this redefinition the most probable joint state will correspond to the most probable state on the product of n transitions once the absorbing state is reached the chain will stay in this state and hence the most probable path can be read off from the sequence of states up to the first time the chain hits the absorbing state. this approach is demonstrated in demomostprobablepath.m along with the more direct approaches described below. an alternative cleaner approach is as follows for the markov chain we can dispense with variable-to-factor and factor-to-variable messages and use only variable-to-variable messages. if we want to find the most likely set of states a st b to get us there then this can be computed by defining the maximal path probability e b t to get from a to b in t e b t max max pst bst max pst bst to compute this efficiently we define messages t t t max st t a until the point e b t max st t t pst bst t t b we can now proceed to find the maximal path probability for timestep t since the messages up to time t will be the same as before we need only compute one additional message t t from which e b t max st t t pst bst t t b we can proceed in this manner until we reach e b n where n is the number of nodes in the graph. we don t need to go beyond this number of steps since those that do must necessarily contain non-simple paths. simple path is one that does not include the same state more than once. the optimal time t is then given by which of e b e b n is maximal. given t one can begin to since e b t max st t t pst bst we know the optimal state s t argmax st t t pst bst alternative to finding t is to define self-transitions with probability and then use a fixed time t n once the desired state b is reached the self-transition then preserves the chain in state b for the remaining timesteps. this procedure is used in mostprobablepathmult.m draft march we can then continue to backtrack s t argmax st t t ps t and so on. see mostprobablepath.m. other forms of inference in the above derivation we do not use any properties of probability except that p must be nonnegative sign changes can flip a whole sequence probability and the local message recursion no longer applies. one can consider the algorithm as finding the optimal product path from a to b. it is straightforward to modify the algorithm to solve the single-sink shortest weighted path problem. one way to do this is to replace the markov transition probabilities with edge weights exp ustst where ustst is infinite if there is no edge from st to st. this approach is taken in shortestpath.m which is able to deal with either positive or negative edge weights. this method is therefore more general than the well-known dijkstra s algorithm which requires weights to be positive. if a negative edge cycle exists the code returns the shortest weighted length n path where n is the number of nodes in the graph. see demoshortestpath.m. the above algorithm is efficient for the single-source single-sink scenario since the messages contain only n states meaning that the overall storage is on as it stands the algorithm is numerically impractical since the messages are recursively multiplied by values usually less than least for the case of probabilities. one will therefore quickly run into numerical underflow possibly overflow in the case of non-probabilities with this method. to fix the final point above it is best to work by defining the logarithm of e. since this is a monotonic transformation the most probable path defined through log e is the same as that obtained from e. in this case l b t max max log pst bst log a log pstst log pst bst t we can therefore define new messages t max st t t log one then proceeds as before by finding the most probable t defined on l and backtracks. graphical model corresponding to this simple markov chain is the belief network remark a possible confusion is that optimal paths can be efficiently found when the graph is loopy note that the graph in is a state-transition diagram not a graphical model. the t pstst a linear serial structure. hence the underlying graphical model is a simple chain which explains why computation is efficient. most probable path multiple-sink if we need the most probable path between all states a and b one could re-run the above single-sourcesingle-sink algorithm for all a and b. a computationally more efficient approach is to observe that one can define a message for each starting state a t max st t t draft march other forms of inference algorithm compute marginal from distribution px procedure bucket eliminationpx evidential variables are ordered xn. f f initialize all bucket potentials to unity. while there are potentials left in the distribution do for each potential f its highest variable xj to the ordering. multiply f with the potential in bucket j and remove f the distribution. end while for i bucket n to do for bucket i sum over the states of variable xi and call this potential i identify the highest variable xh of potential i multiply the existing potential in bucket h by i end for the marginal is proportional to return end procedure f f assumes non fill buckets empty buckets the conditional marginal. and continue until we find the maximal path probability matrix for getting from any state a to any state b in t timesteps e b t max st t t pst bst since we know the message t t for all states a we can readily compute the most probable path from all starting states a to all states b after t steps. this requires passing an n n matrix message we can then proceed to the next timestep t since the messages up to time t will be the same as before we need only compute one additional message t t from which e b t max st t t pst bst in this way one can then efficiently compute the optimal path probabilities for any starting state a and end state b after t timesteps. to find the optimal corresponding path backtracking proceeds as before see mostprobablepathmult.m. one can also use the same algorithm to solve the multiple-source multiple sink shortest path problem. this algorithm is a variant of the floyd-warshall-roy for finding shortest weighted summed paths on a directed graph above algorithm enumerates through time whereas the fwr algorithm enumerates through states. mixed inference an often encountered situation is to infer the most likely state of a joint marginal possibly given some evidence. for example given a distribution xn find argmax xm argmax xn in general even for tree structured xn the optimal marginal state cannot be computed efficiently. one way to see this is that due to the summation the resulting joint marginal does not have a structured factored form as products of simpler functions of the marginal variables. finding the most probable joint marginal then requires a search over all the joint marginal states a task exponential in m. an approximate solution is provided by the em algorithm and draft march inference in multiply-connected graphs e c b g a d f pepgd e pca pbpda b pbpda b e g e g pa pfd pa pa b a pa b a pfd pfd pfd g pfd g a d node is eliminated from the graph. the second stage of eliminating c is trivial since figure the bucket elimination algorithm applied to the graph at each stage at least one c pca and has therefore been skipped over since this bucket does not send any message. inference in multiply-connected graphs bucket elimination we consider here a general conditional marginal variable elimination method that works for any distribution multiply connected graphs. the algorithm assumes the distribution is in the form f xn and that the task is to compute for example for we could use the sets of variables here are in general the construction of potentials for a distribution is not unique. the task of computing a marginal in which a set of variables are clamped to their evidential states is evidence f f the algorithm is given in and can be considered a way to organise the distributed the algorithm is best explained by a simple example as given below. example elimination. consider the problem of calculating the marginal pf of pa b c d e f g pfdpgd epcapda bpapbpe see pf pa b c d e f g abcdeg abcdeg pfdpgd epcapda bpapbpe draft march inference in multiply-connected graphs pf terms we can write pf adg pf pf d d we can distribute the summation over the various terms as follows eb and c are end nodes so that we can sum over their values b pgd epe b pda bpb b d e pfdpa adg for convenience lets write the terms in the brackets as e g. the c pda bpb pca e pgd epe c pca is equal to unity and we therefore eliminate this node directly. rearranging pfdpa b d e g if we think of this graphically the effect of summing over b c e is effectively to remove or eliminate those variables. we can now carry on summing over a and g since these are end points of the new graph pfd pa b d e g a g again this defines new functions a g so that the final answer can be found from pfd a g we illustrate this in initially we define an ordering of the variables beginning with the one that we wish to find the marginal for a suitable ordering is therefore f d a g b c e. then starting with the highest bucket e to our ordering f d a g b c e we put all the functions that mention e in the e bucket. continuing with the next highest bucket c we put all the remaining functions that mention c in this c bucket etc. the result of this initialisation procedure is that terms distributions in the dag are distributed over the buckets as shown in the left most column of eliminating then the highest bucket e we pass a message to node g. immediately we can also eliminate bucket c since this sums to unity. in the next column we have now two less buckets and we eliminate the highest remaining bucket this time b passing a message to bucket a. there are some important observations we can make about bucket elimination to compute say we need to re-order the variables that the required marginal variable is labelled and repeat bucket elimination. hence each query of a marginal in this case requires re-running the algorithm. it would be more efficient to reuse messages rather than recalculating them each time. in general bucket elimination constructs multi-variable messages from bucket to bucket. the storage requirements of a multi-variable message are exponential in the number of variables of the message. for trees we can always choose a variable ordering to render the computational complexity to be linear in the number of variables. such an ordering is called perfect and indeed it can be shown that a perfect ordering can always easily be found for singly-connected graphs however orderings exist for which bucket elimination will be extremely inefficient. loop-cut conditioning for distributions which contain a loop is more than one path between two nodes in the graph when the directions are removed we run into some difficulty with the message passing routines such as the sum-product algorithm which are designed to work on singly-connected graphs only. one way to solve draft march c a f b g d e c b g e a f d notes figure a multiplyconnected graph reduced to a singly-connected graph by conditioning on the variable c. the difficulties of multiply connected graphs is to identify nodes that when removed would reveal a singly-connected consider the example if imagine that we wish to calculate a marginal say pd. then pd c abefg pcapa p pda bpb pfc d p pgd e where the p definitions are not necessarily distributions. for each state of c the form of the products of factors remaining as a function of a b e f g is singly-connected so that standard singly-connected message passing can be used to perform inference. we will need to do perform inference for each state of variable c each state defining a new singly-connected graph the same structure but with modified potentials. more generally we can define a set of variables c called the loop cut set and run singly-connected inference for each joint state of the cut-set variables c. this can also be used for finding the most likely state of a multiply-connected joint distribution as well. hence for a computational price exponential in the loopcut size we can calculate the marginals the most likely state for a multiply-connected distribution. however determining a small cut set is in general difficult and there is no guarantee that this will anyway be small for a given graph. whilst this method is able to handle loops in a general manner it is not particularly elegant since the concept of messages now only applies conditioned on the cut set variables and how to re-use messages for inference of additional quantities of interest becomes unclear. we will discuss an alternative method for handling multiply connected distributions in message passing for continuous distributions for parametric continuous distributions px x message passing corresponds to passing parameters of the distributions. for the sum-product algorithm this requires that the operations of multiplication and integration over the variables are closed with respect to the family of distributions. this is the case for example for the gaussian distribution the marginal of a gaussian is another gaussian and the product of two gaussians is a gaussian see this means that we can then implement the sum-product algorithm based on passing mean and covariance parameters. to implement this requires some tedious algebra to compute the appropriate message parameter updates. at this stage the complexities from performing such calculations are a potential distraction though the interested reader may refer to demosumprodgaussmoment.m demosumprodgausscanon.m and demosumprodgausscanonlds.m and also for examples of message passing with gaussians. for more general exponential family distributions message passing is essentially straightforward though again the specifics of the updates may be tedious to work out. in cases where the operations of marginalisation and products are not closed within the family the distributions need to be projected back to the chosen message family. expectation propagation is relevant in this case. notes a take-home message from this chapter is that inference in singly-connected structures is usually computationally tractable. notable exceptions are when the message passing operations are notclosed within the message family or representing messages explicitly requires an exponential amount of space. this happens for example when the distribution can contain both discrete and continuous variables draft march code such as the switching linear dynamical system which we discuss in broadly speaking inference in multiply-connected structures is more complex and may be intractable. however we do not want to give the impression that this is always the case. notable exceptions are finding the map state in an attractive pairwise mrf finding the map and mpm state in a binary planar mrf with pure interactions see for example for n variables in the graph a naive use of the junction tree algorithm for these inferences would result in an computation whereas clever algorithms are able to return the exact results in operations. of interest is bond which is an intuitive node elimination method to arrive at the mpm inference in pureinteraction ising models. code the code below implements message passing on a tree structured factor graph. the fg is stored as an adjacency matrix with the message between fg node i and fg node j given in aij. factorgraph.m return a factor graph adjacency matrix and message numbers sumprodfg.m sum-product algorithm on a factor graph in general it is recommended to work in log-space in the max-product case particularly for large graphs since the produce of messages can become very small. the code provided does not work in log space and as such may not work on large graphs writing this using log-messages is straightforward but leads to less readable code. an implementation based on log-messages is left as an exercise for the interested reader. maxprodfg.m max-product algorithm on a factor graph maxnprodfg.m n-max-product algorithm on a factor graph factor graph examples for the distribution from the following code finds the marginals and most likely joint states. the number of states of each variable is chosen at random. demosumprod.m test the sum-product algorithm demomaxprod.m test the max-product algorithm demomaxnprod.m test the max-n-product algorithm most probable and shortest path mostprobablepath.m most probable path demomostprobablepath.m most probable versus shortest path demo the shortest path demo works for both positive and negative edge weights. if negative weight cycles exist the code finds the best length n shortest path. demoshortestpath.m shortest path demo mostprobablepathmult.m most probable path multi-source multi-sink demomostprobablepathmult.m demo of most probable path multi-source multi-sink bucket elimination the efficacy of bucket elimination depends critically on the elimination sequence chosen. in the demonstration below we find the marginal of a variable in the chest clinic exercise using a randomly chosen elimination order. the desired marginal variable is specified as the last to be eliminated. for comparison we use an elimination sequence based on decimating a triangulated graph of the model as discussed in again under the constraint that the last variable to be decimated is the marginal variable of draft march exercises interest. for this smarter choice of elimination sequence the complexity of computing this single marginal is roughly the same as that for the junction tree algorithm using the same triangulation. bucketelim.m bucket elimination demobucketelim.m demo bucket elimination message passing on gaussians the following code hints at how message passing may be implemented for continuous distributions. the reader is referred to the brmltoolbox for further details and also for the algebraic manipulations required to perform marginalisation and products of gaussians. the same principal holds for any family of distributions which is closed under products and marginalisation and the reader may wish to implement specific families following the method outlined for gaussians. demosumprodgaussmoment.m sum-product message passing based on gaussian moment parameterisation exercises exercise given a pairwise singly connected markov network of the form xj i j px z explain how to efficiently compute the normalisation factor called the partition function z as a function of the potentials exercise you are employed by a web start up company that designs virtual environments in which players can move between rooms. the rooms which are accessible from another in one time step is given by the matrix m stored in virtualworlds.mat where mij means that there is a door between rooms i and j mji. mij means that there is no door between rooms i and j. mii meaning that in one time step one can stay in the same room. you can visualise this matrix by typing imagescm. write a list of rooms which cannot be reached from room after time steps. the manager complains that takes at least time steps to get from room to room is this true? find the most likely path of rooms to get from room to room if a single player were to jump randomly from one room to another stay in the same room with no preference between rooms what is the probability at time t the player will be in room assume that effectively an infinite amount of time has passed and the player began in room at t if two players are jumping randomly between rooms staying in the same room explain how to compute the probability that after an infinite amount of time at least one of them will be in room assume that both players begin in room exercise consider the hidden markov model vt ht pvthtphtht in which domht h and domvt v for all t t draw a belief network representation of the above distribution. draw a factor graph representation of the above distribution. draft march exercises use the factor graph to derive a sum-product algorithm to compute marginals vt explain the sequence order of messages passed on your factor graph. explain how to compute pht vt exercise for a singly connected markov network px xn the computation of a marginal pxi can be carried out efficiently. similarly the most likely joint state x arg px can be computed efficiently. explain when the most likely joint state of a marginal can be computed efficiently i.e. under what circumstances could one efficiently o time compute argmax xm for m n? exercise consider the internet with webpages labelled n. if webpage j has a link to webpage i then we place an element of the matrix lij otherwise lij by considering a random jump from webpage j to webpage i to be given by the transition probability mij i lij what is the probability that after an infinite amount of random surfing one ends up on webpage i? how could you relate this to the potential relevance of a webpage in terms of a search engine? exercise a special time-homogeneous hidden markov model is given by xt yt ht phtht the variable xt has states domxt c g t labelled as states the variable yt has states domyt c g t. the hidden or latent variable ht has states domht the hmm models the following process in humans z-factor proteins are a sequence on states of the variables xt in bananas zfactor proteins are also present but represented by a different sequence yt given a sequence xt from a human the task is to find the corresponding sequence yt in the banana by first finding the most likely joint latent sequence and then the most likely banana sequence given this optimal latent sequence. that is we require t h argmax yth where h h t argmax xt the file banana.mat contains the emission distributions pxgh pygh and transition phtghtm the initial hidden distribution is given in the observed x sequence is given in x. explain mathematically and in detail how to compute the optimal y-sequence using the two-stage procedure as stated above. write a matlab routine that computes and displays the optimal y-sequence given the observed x-sequence. your routine must make use of the factor graph formalism. explain whether or not it is computationally tractable to compute arg max xt bonus question by considering yt as parameters explain how the em algorithm may be used to find most likely marginal states. implement this approach with a suitable initialisation for the optimal parameters yt draft march exercises draft march chapter the junction tree algorithm clustering variables in we discussed efficient inference for singly-connected graphs for which variable elimination and message passing schemes are appropriate. in the multiply-connected case however one cannot in general perform inference by passing messages only along existing links in the graph. the idea behind the junction tree algorithm is to form a new representation of the graph in which variables are clustered together resulting in a singly-connected graph in the cluster variables on a different graph. the main focus of the development will be on marginal inference though similar techniques apply to difference inferences such as finding the maximal state of the distribution. at this stage it is important to point out that the junction tree algorithm is not a magic method to deal with intractabilities resulting from multiply connected graphs it is simply a way to perform correct inference on a multiply connected graph by transforming to a singly connected structure. carrying out the inference on the resulting junction tree may still be computationally intractable. for example the junction tree representation of a general two-dimensional ising model is a single supernode containing all the variables. inference in this case is exponentially complex in the number of variables. nevertheless even in cases where implementing the jta any other exact inference algorithm may be intractable the jta provides useful insight into the representation of distributions that can form the basis for approximate inference. in this sense the jta is key to understanding issues related to representations and complexity of inference and is central to the development of efficient inference algorithms. reparameterisation consider the chain pa b c d pabpbcpcdpd using bayes rule we can reexpress this as pa b c d pa b pb pb c pc pc d pd pd pa bpb cpc d pbpc a useful insight is that the distribution can therefore be written as a product of marginal distributions divided by a product of the intersection of the marginal distributions looking at the numerator pa bpb cpc d this cannot be a distribution over a b c d since we are overcounting b and c where this overcounting of b arises from the overlap between the sets a b and b c which have b as their intersection. similarly the overcounting of c arises from the overlap between the sets b c and c d. roughly speaking we need to correct for this overcounting by dividing by the distribution on the intersections. given the transformed representation a marginal such as pa b can be read off directly from the factors in the new clique graphs a b c d expression. abc bc bcd figure b c c d. graph of markov network equivalent clique the aim of the junction tree algorithm is to form a representation of the distribution which contains the marginals explicitly. we want to do this in a way that works for belief and markov networks and also deals with the multiply-connected case. in order to do so an appropriate way to parameterise the distribution is in terms of a clique graph as described in the next section. clique graphs definition graph. a clique graph consists of a set of potentials nx n each defined on a set of variables x i. for neighbouring cliques on the graph defined on sets of variables x i and x j the intersection x s x i x j is called the separator and has a corresponding potential sx s. a clique graph represents the function c cx c s sx s for notational simplicity we will usually drop the clique potential index c. graphically clique potentials are represented by circlesovals and separator potentials by squares. x x x x the graph on the left represents x clique graphs translate markov networks into structures convenient for carrying out inference. consider the markov network in pa b c d b c c d z which contains two clique potentials sharing the variables b c. an equivalent representation is given by the clique graph in defined as the product of the numerator clique potentials divided by the product of the separator potentials. in this case the separator potential may be set to the normalisation constant z. by summing we have zpa b c b b c d b cpb c d multiplying the two expressions we have d zpb c d c c a b c c d d a in other words pa b c d pa b cpb c d pc b b c b c ad pa b c d draft march clique graphs the important observation is that the distribution can be written in terms of its marginals on the variables in the original cliques and that as a clique graph it has the same structure as before. all that has changed is that the original clique potentials have been replaced by the marginals of the distribution and the separator by the marginal defined on the separator variables b c pa b c c d pb c d z pc b. the usefulness of this representation is that if we are interested in the marginal pa b c this can be read off from the transformed clique potential. to make use of this representation we require a systematic way of transforming the clique graph potentials so that at the end of the transformation the new potentials contain the marginals of the distribution. remark note that whilst visually similar a factor graph and a clique graph are different representations. in a clique graph the nodes contain sets of variables which may share variables with other nodes. absorption consider neighbouring cliques v and w sharing the variables s in common. in this case the distribution on the variables x v w is px and our aim is to find a new representation in which the potentials are given by pw ps in this example we can explicitly work out the new potentials as function of the old potentials by computing the marginals as follows px pv pw pv vs ws px px vs ws vs and and the refine the w potential using there is a symmetry present in the two equations above they are the same under interchanging v and w. one way to describe these equations is through absorption we say that the cluster w absorbs information from cluster v by the following updating procedure. first we define a new separator the advantage of this interpretation is that the new representation is still a valid clique graph representation of the distribution since draft march px a c e d junction trees figure an example absorption schedule on a clique tree. many valid schedules exist under the constraint that messages can only be passed to a neighbour when all other messages have been received. b f after w absorbs information from v then contains the marginal pw. similarly after v absorbs information from w then contains the marginal pv. after the separator s has participated in absorption along both directions then the separator potential will contain ps is not the case after only a single absorption. to see this consider continuing we have the new potential given by vs ws ws ps pv definition let v and w be neighbours in a clique graph let s be their separator and let and be their potentials. absorption from v to w through s replaces the tables and with vs we say that clique w absorbs information from clique v. absorption schedule on clique trees having defined the local message propagation approach we need to define an update ordering for absorption. in general a node v can send exactly one message to a neighbour w and it may only be sent when v has received a message from each of its other neighbours. we continue this sequence of absorptions until a message has been passed in both directions along every link. see for example note that the message passing scheme is not unique. definition schedule. a clique can send a message to a neighbour provided it has already received messages from all other neighbours. junction trees there are a few stages we need to go through in order to transform a distribution into an appropriate structure for inference. initially we explain how to do this for singly-connected structures before moving draft march junction trees figure singly-connected markov network. clique graph. clique tree. onto the multiply-connected case. consider the singly-connected markov network the clique graph of this singly-connected markov network is multiply-connected where the separator potentials are all set to unity. nevertheless let s try to reexpress the markov network in terms of marginals. first we have the relations taking the product of the three marginals we have this means that the markov network can be expressed in terms of marginals as hence a valid clique graph is also given by the representation indeed if a variable occurs on every separator in a clique graph loop one can remove that variable from an arbitrarily chosen separator in the loop. this leaves an empty separator which we can simply remove. this shows that in such cases we can transform the clique graph into a clique tree a singly-connected clique graph. provided that the original markov network is singly-connected one can always form a clique tree in this manner. the running intersection property sticking with the above example consider the clique tree in draft march as a representation of the distribution where we set to make this match. now perform absorption on this clique tree we absorb the new separator is junction trees and the new potential is now the new separator is and the new potential is since we ve hit the buffers in terms of message passing the potential cannot be updated further. let s examine more carefully the value of this new potential hence the new potential contains the marginal to complete a full round of message passing we need to have passed messages in a valid schedule along both directions of each separator. to do so we continue as follows we absorb the new separator is and and note that so that now after absorbing through both directions the separator contains the marginal the reader may show that finally we absorb the new separator is hence after a full round of message passing the new potentials all contain the correct marginals. draft march junction trees vi wi the new representation is consistent in the sense that for any necessarily neighbouring cliques v and w with intersection i and corresponding potentials and note that bidirectional absorption guarantees consistency for neighbouring cliques as in the example above provided that we started with a clique tree which is a correct representation of the distribution. in general the only possible source of non-consistency is if a variable occurs in two non-neighbouring cliques and is not present in all cliques on any path connection them. an extreme example would be if we removed the link between cliques and in this case this is still a clique tree however global consistency could not be guaranteed since the information required to make clique consistent with the rest of the graph cannot reach this clique. formally the requirement for the propagation of local to global consistency is that the clique tree is a junction tree as defined below. definition tree. a clique tree is a junction tree if for each pair of nodes v and w all nodes on the path between v and w contain the intersection v w. this is also called the running intersection property. from this definition local consistency will be passed on to any neighbours and the distribution will be globally consistent. proofs for these results are contained in example consistent junction tree. to gain some intuition about the meaning of consistency consider the junction tree in after a full round of message passing on this tree each link is consistent and the product of the potentials divided by the product of the separator potentials is just the original distribution itself. imagine that we are interested in calculating the marginal for the node abc. that requires summing over all the other variables def gh. if we consider summing over h then because the link is consistent h h so that the ratio c e h is unity and the effect of summing over node h is that the link between eh and dce can be removed along with the separator. the same happens for the link between node eg and dce and also for cf to abc. the only nodes remaining are now dce and abc and their separator c which have so far been unaffected by the summations. we still need to sum out over d and e. again because the link is consistent de so that the ratio summation over the other variables in that potential for example pf the result of the summation of all variables not in abc therefore produces unity for the cliques and their separators and the summed potential representation reduces simply to the potential b c which is the marginal pa b c. it is clear that a similar effect will happen for other nodes. we can then obtain the marginals for individual variables by simple brute force de c f. draft march constructing a junction tree for singly-connected distributions constructing a junction tree for singly-connected distributions moralisation for belief networks an initial step is required which is not required in the case of undirected graphs. definition for each variable x add an undirected link between all parents of x and replace the directed link from x to its parents by undirected links. this creates a moralised markov network. forming the clique graph the clique graph is formed by identifying the cliques in the markov network and adding a link between cliques that have a non-empty intersection. add a separator between the intersecting cliques. forming a junction tree from a clique graph for a singly-connected distribution any maximal weight spanning tree of a clique graph is a junction tree. definition tree. a junction tree is obtained by finding a maximal weight spanning tree of the clique graph. the weight of the tree is defined as the sum of all the separator weights of the tree where the separator weight is the number of variables in the separator. if the clique graph contains loops then all separators on the loop contain the same variable. by continuing to remove loop links until you have a tree is revealed we obtain a junction tree. example a junction tree. consider the belief network in the moralisation procedure gives identifying the cliques in this graph and linking them together gives the clique graph in there are several possible junction trees one could obtain from this clique graph and one is given in assigning potentials to cliques of a set of potentials definition potential assignment. given a junction tree and a function defined as the product x n a valid clique potential assignment places potentials in jt cliques whose variables can contain them such that the product of the jt clique potentials divided by the jt separator potentials is equal to the function. a simple way to achieve this assignment is to list all the potentials and order the jt cliques arbitrarily. then for each potential search through the jt cliques until the first is encountered for which the potential variables are a subset of the jt clique variables. subsequently the potential on each jt clique is taken as the product of all clique potentials assigned to the jt clique. lastly we assign all jt separators to unity. this approach is taken in jtassignpot.m. draft march junction trees for multiply-connected distributions c h c a e abc c eh d g c e b f d g a e c h cf dce e eg b f c e abc c cf eh dce e eg e figure belief network. a junction tree. this satisfies the running intersection property that for any two nodes which contain a variable in common any clique on the path linking the two nodes also contains that variable. moralised version of clique graph of example for the belief network of we wish to assign its potentials to the junction tree in this case the assignment is unique and is given by papbpca b pdped c pfc pge phe all separator potentials are initialised to unity. note that in some instances it can be that a junction tree clique is assigned to unity. junction trees for multiply-connected distributions when the distribution contains loops the construction outlined in does not result in a junction tree. the reason is that due to the loops variable elimination changes the structure of the remaining graph. to see this consider the following distribution pa b c d b c d a as shown in let s first try to make a clique graph. we have a choice about which variable first to marginalise over. let s choose d pa b c b draft march d d a junction trees for multiply-connected distributions a d a b c b c a d b c a d b c abc ac acd figure an undirected graph with a loop. c in the subgraph. representation. junction tree for the induced representation for the graph in eliminating node d adds a link between a and equivalent induced the remaining subgraph therefore has an extra connection between a and c see we can express the joint in terms of the marginals using to continue the transformation into marginal form let s try to replace the numerator terms with probabilities. we can do this by considering pa b c d pa b c d d a d a pa c d d b c b plugging this into the above equation we have pa b c d d d pa b cpa c d b b c we recognise that the denominator is simply pa c hence pa b c d pa b cpa c d pa c this means that a valid clique graph for the distribution must contain cliques larger than those in the original distribution. to form a jt based on products of cliques divided by products of separators we could start from the induced representation alternatively we could have marginalised over variables a and c and ended up with the equivalent representation generally the result from variable elimination and re-representation in terms of the induced graph is that a link is added between any two variables on a loop length or more which does not have a chord. this is called triangulation. a markov network on a triangulated graph can always be written in terms of the product of marginals divided by the product of separators. armed with this new induced representation we can form a junction tree. a f b e c a d f b e c d figure markov network. resentation. loopy ladder induced rep draft march junction trees for multiply-connected distributions a a f f b j b j c d e a g h i e a k d l c g h i k l f f b j b j c d e a g h i e a a k d l c g h i k l f f f b j b b j j c d e g h i k d d e e l c c g g h h i i k k l l figure markov network for which we seek a triangulation via greedy variable elimination. we we then eliminate variables b d since these only add first eliminate the simplical nodes a e l. f and i are now simplical and are eliminated. a single extra link to the induced graph. the remaining variables j k we eliminate g and h since this adds only single extra links. final triangulation. the variable elimination order is may be eliminated in any order. e l d i h j k where the brackets indicate that the order in which the variables inside the bracket are eliminated is irrelevant. compared with the triangulation produced by the maxcardinality checking approach in this triangulation is more parsimonious. example a slightly more complex loopy distribution is depicted in pa b c d e f b c d e f f e there are different induced representations depending on which variables we decide to eliminate. the reader may convince herself that one such induced representation is given by definition this is a link joining two non-consecutive vertices of a loop. definition graph. an undirected graph is triangulated if every loop of length or more has a chord. an equivalent term is that the graph is decomposable or chordal. an undirected graph is triangulated if and only if its clique graph has a junction tree. triangulation algorithms when a variable is eliminated from a graph links are added between all the neighbours of the eliminated variable. a triangulation algorithm is one that produces a graph for which there exists a variable elimination order that introduces no extra links in the graph. draft march junction trees for multiply-connected distributions figure junction tree formed from the triangulation one verify that this satisfies the running intersection property. abf bf bcf g cdhi di dei cf g cf gj chi chik cj ck cjk jk jkl for discrete variables the complexity of inference scales exponentially with clique sizes in the triangulated graph since absorption requires computing tables on the cliques. it is therefore of some interest to find a triangulated graph with small clique sizes. however finding the triangulated graph with the smallest maximal clique is an np-hard problem for a general graph and heuristics are unavoidable. below we describe two simple algorithms that are generically reasonable although there may be cases where an alternative algorithm may be considerably more remark does not mean putting triangles on the original graph. note that a triangulated graph is not one in which squares in the original graph have triangles within them in the triangulated graph whilst this is the case for this is not true for the term triangulation refers to the fact that every square loop of length must have a triangle with edges added until this criterion is satisfied. greedy variable elimination an intuitive way to think of triangulation is to first start with simplical nodes namely those which when eliminated do not introduce any extra links in the remaining graph. next consider a non-simplical node of the remaining graph that has the minimal number of neighbours. then add a link between all neighbours of this node and finally eliminate this node from the graph. continue until all nodes have been eliminated. procedure corresponds to rose-tarjan with a particular node elimination choice. by labelling the nodes eliminated in sequence we obtain a perfect ordering below in reverse. in the case that variables have different numbers of states a more refined version is to choose the non-simplical node i which when eliminated leaves the smallest clique table size product of the size of all the state dimensions of the neighbours of node i. see for an example. definition elimination. in variable elimination one simply picks any non-deleted node x in the graph and then adds links to all the neighbours of x. node x is then deleted. one repeats this until all nodes have been whilst this procedure guarantees a triangulated graph its efficiency depends heavily on the sequence of nodes chosen to be eliminated. several heuristics for this have been proposed including the one below which corresponds to choosing x to be the node with the minimal number of neighbours. maximum cardinality checking terminates with success if the graph is triangulated. not only is this a sufficient condition for a graph to be triangulated but is also necessary it processes each node and the time to process a node is quadratic in the number of adjacent nodes. this triangulation checking algorithm also suggests draft march the junction tree algorithm figure starting with the markov network in the maximum cardinality check algorithm proceeds until where an additional link is required one continues the algorithm until the fully triangulated graph is found. a triangulation construction algorithm we simply add a link between the two neighbours that caused the algorithm to fail and then restart the algorithm. the algorithm is restarted from the beginning not just continued from the current node. this is important since the new link may change the connectivity between previously labelled nodes. see for an definition elimination order. let the n variables in a markov network be ordered from to n. the ordering is perfect if for each node i the neighbours of i that are later in the ordering and i itself form a clique. this means that when we eliminate the variables in sequence from to n no additional links are induced in the remaining marginal graph. a graph which admits a perfect elimination order is decomposable and vice versa. algorithm a check if a graph is decomposable the graph is triangulated if after cycling through all the n nodes in the graph the fail criterion is not encountered. choose any node in the graph and label it for i to n do end for where there is more than one node with the most labeled neighbours the tie may be broken arbitrarily. choose the node with the most labeled neighbours and label it i. if any two labeled neighbours of i are not adjacent to each other fail. the junction tree algorithm we now have all the steps required for inference in multiply-connected graphs moralisation marry the parents. this is required only for directed distributions. triangulation ensure that every loop of length or more has a chord. junction tree form a junction tree from cliques of the triangulated graph removing any unnecessary links in a loop on the cluster graph. algorithmically this can be achieved by finding a tree with maximal spanning weight with weight wij given by the number of variables in the separator between cliques i and j. alternatively given a clique elimination order the lowest cliques eliminated first one may connect each clique i to the single neighbouring clique j i with greatest edge weight wij. example is due to david page www.cs.wisc.edu draft march the junction tree algorithm a c f b e d g b e a c f d g h i h i original figure loopy belief network. the moralisation links are between nodes e and f and between nodes f and g. the other additional links come from triangulation. the clique size of the resulting clique tree shown is four. from potential assignment assign potentials to junction tree cliques and set the separator potentials to unity. message propagation carry out absorption until updates have been passed along both directions of every link on the jt. the clique marginals can then be read off from the jt. an example is given in remarks on the jta the algorithm provides an upper bound on the computation required to calculate marginals in the graph. there may exist more efficient algorithms in particular cases although generally it is believed that there cannot be much more efficient approaches than the jta since every other approach must perform a one particular special case is that of marginal inference for a binary variable mrf on a two-dimensional lattice containing only pure quadratic interactions. in this case the complexity of computing a marginal inference is where n is the number of variables in the distribution. this is in contrast to the pessimistic exponential complexity suggested by the jta. such cases are highly specialised and it is unlikely that a general purpose algorithm that could consistently outperform the jta exists. one might think that the only class of distributions for which essentially a linear time algorithm is available are singly-connected distributions. however there are decomposable graphs for which the cliques have limited size meaning that inference is tractable. for example an extended version of the ladder in has a simple induced decomposable representation for which marginal inference would be linear in the number of rungs in the ladder. effectively these structures are hyper trees in which the complexity is then related to the tree width of the ideally we would like to find a triangulated graph which has minimal clique size. however it can be shown to be a hard-computation problem p to find the most efficient triangulation. in practice most general purpose triangulation algorithms are somewhat heuristic and chosen to provide reasonable but clearly not optimal generic performance. numerical overunder flow issues can occur in large cliques where many probability values are multiplied together. similarly in long chains since absorption will tend to reduce the numerical size of potential entries in a clique. if we only care about marginals we can avoid numerical difficulties by normalising potentials at each step these missing normalisation constants can always be found under the normalisation constraint. if required one can always store the values of these local renormalisations should for example the global normalisation constant of a distribution be required see after clamping variables in evidential states running the jta returns the joint distribution on the non-evidential variables in a clique with all the evidential variables clamped in their evidential states. from this conditionals are straightforward to calculate. imagine that we have run the jt algorithm and want to afterwards find the marginal pxevidence. we could do so by clamping the evidential variables. however if both x and the set of evidential draft march the junction tree algorithm variables are all contained within a single clique of the jt then we may use the consistent jt cliques to compute pxevidence. the reason is that since the jt clique contains the marginal on the set of variables which includes x and the evidential variables we can obtain the required marginal by considering the single jt clique alone. representing the marginal distribution of a set of variables x which are not contained within a single clique is in general computationally difficult. whilst the probability of any state of px may be computed efficiently there are in general an exponential number of such states. a classical example in this regard is the hmm with singly-connected joint distribution pvh. however the marginal distribution ph is fully connected. this means that for example whilst the entropy of pvh is straightforward to compute the entropy of the marginal ph is intractable. computing the normalisation constant of a distribution for a markov network px z how can we find z efficiently? if we used the jta on the unnormalised i have the equivalent representation i we would z c s c s px z x z xc since the distribution must normalise we can obtain z from for a consistent jt summing first over the variables of a simplical jt clique including the separator variables the marginal clique will cancel with the corresponding separator to give a unity term so that the clique and separator can be removed. this forms a new jt for which we then eliminate another simplical clique. continuing in this manner we will be left with a single numerator potential so that this is true for any clique c so it makes sense to choose one with a small number of states so that the resulting raw summation is efficient. hence in order to compute the normalisation constant of a distribution one runs the jt algorithm on an unnormalised distribution and the global normalisation is then given by the local normalisation of any clique. note that if the graph is disconnected are isolated cliques the normalisation is the product of the connected component normalisation constants. a computationally convenient way to find this is to compute the product of all clique normalisations divided by the product of all separator normalisations. the marginal likelihood our interest here is the computation of pv where v is a subset of the full variable set. naively one could carry out this computation by summing over all the non-evidential variables variables h xv explicitly. in cases where this is computationally impractical an alternative is to use phv pvh pv one can view this as a product of clique potentials divided by the normalisation pv for which the general method of may be directly applied. see demojtree.m. draft march the junction tree algorithm example simple example of the jta. consider running the jta on the simple graph pa b c pabpbcpc the moralisation and triangulation steps are trivial and the jta is given immediately by the figure on the right. a valid assignment is a ab b b c bc to find a marginal pb we first run the jta b pab c pbcpc absorbing from ab through b the new separator is a b a pab the new potential on c is given by pbcpc c c c c c absorbing from bc through b the new separator is pbcpc the new potential on b is given by b b c pbcpc this is therefore indeed equal to the marginal the new separator contains the marginal pb since pbcpc pb c pb c c c pa b c pa b. example a conditional marginal. continuing with the distribution in we consider how to compute pba c first we clamp the evidential variables in their states. then we claim that the effect of running the jta is to produce on a set of clique variables x the marginals on the cliques px we demonstrate this below a pab however since a is clamped in state a then the summation is not carried out over a and we have instead in general the new separator is given by pa the new potential on the c clique is given by a b c c c c c the new separator is normally given by pbcpc pbc however since c is clamped in state we have instead pbc draft march finding the most likely state the new potential on b is given by b b pa pa pa the effect of clamping a set of variables v in their evidential states and running the jta is that for a clique i which contains the set of non-evidential variables hi the consistent potential from the jta contains the marginal phiv. finding a conditional marginal is then straightforward by ensuring normalisation. example the likelihood pa c the effect of clamping the variables in their evidential states and running the jta produces the joint marginals such as b pa b c then calculating the likelihood is easy since we just sum out over the non-evidential variables of any converged potential pa c b b b pa b c finding the most likely state a quantity of interest is the most likely joint state of a distribution argmax px and it is natural to wonder how this can be efficiently computed in the case of a loopy distribution. since the development of the jta is based around a variable elimination procedure and the max operator distributes over the distribution as well eliminating a variable by maximising over that variable will have the same effect on the graph structure as summation did. this means that a junction tree is again an appropriate structure on which to perform max operations. once a jt has been constructed one then uses the max absorption procedure below to perform maximisation over the variables. after a full round of absorption has been carried out the cliques contain the distribution on the variables of the clique with all remaining variables set to their optimal states. the optimal local states can be found by explicit optimisation of each clique potential separately. note that this procedure holds also for non-distributions in this sense this is an example of a more general dynamic programming procedure applied in a case where the underlying graph is multiply-connected. this demonstrates how to efficiently compute the optimum of a multiply-connected function defined as the product on potentials. definition absorption. let v and w be neighbours in a clique graph let s be their separator and let and be their potentials. absorption replaces the tables and with maxvs once messages have been passed in both directions over all separators according to a valid schedule the most-likely joint state can be read off from maximising the state of the clique potentials. this is implemented in absorb.m and absorption.m where a flag is used to switch between either sum or max absorption. draft march reabsorption converting a junction tree to a directed network abc c c e dce e abc c c e cf dce e cf a b c e d f eg eh eg eh g h figure junction tree. directed junction tree in which all edges are consistently oriented away from the clique a set chain formed from the junction tree by reabsorbing each separator into its child clique. reabsorption converting a junction tree to a directed network it is sometimes useful to be able to convert the jt back to a bn of a desired form. for example if one wishes to draw samples from a markov network this can be achieved by ancestral sampling on an equivalent directed structure see revisiting the example from we have the jt given in to find a valid directed representation we first orient the jt edges consistently away from a chosen root node singleparenttree.m thereby forming a directed jt which has the property that each clique has at most one parent clique. definition v s w v w let v and w be neighbouring cliques in a directed jt in which each clique in the tree has at most one parent. furthermore let s be their separator and and be the potentials. reabsorption into w removes the separator and forms a conditional distribution pwv we say that clique w reabsorbs the separator s. in where one amongst many possible directed representations is formed from the jt. specifically represents pa b c d e f g h pe gpd c epa b cpc fpe h pepcpcpe we now have many choices as to which clique re-absorbs a separator. one such choice would give pa b c d e f g h pgepd ecpa b c this can be represented using a so-called set in chains generalise belief networks to a product of clusters of variables conditioned on parents. by writing each of the set conditional probabilities as local conditional bns one may also write full bn. for example one such would be given from the decomposition pca bpbapapgepfcphepde cpec draft march code figure diseases giving rise to symptoms. assuming the symptoms are all instantiated the triangulated graph of the diseases is a clique. the need for approximations the jta provides an upper bound on the complexity of inference and attempts to exploit the structure of the graph to reduce computations. however in a great deal of interesting applications the use of the jta algorithm would result in clique-sizes in the triangulated graph that are prohibitively large. a classical situation in which this can arise are disease-symptom networks. for example for the graph in the triangulated graph of the diseases is fully coupled meaning that no simplification can occur in general. this situation is common in such bipartite networks even when the children only have a small number of parents. intuitively as one eliminates each parent links are added between other parents mediated via the common children. unless the graph is highly regular analogous to a form of hidden markov model this fill-in effect rapidly results in large cliques and intractable computations. dealing with large clique in the triangulated graph is an active research topic and we ll discuss strategies to approximate the computations in bounded width junction trees in some applications we may be at liberty to choose the structure of the markov network. for example if we wish to fit a markov network to data we may wish to use as complex a markov network as we can computationally afford. in such cases we desire that the clique sizes of the resulting triangulated markov network are smaller than a specified tree width the corresponding junction tree as a hypertree. constructing such bounded width or thin junction trees is an active research topic. a simple way to do this is to start with a graph and include a randomly chosen edge provided that the size of all cliques in the resulting triangulated graph is below a specified maximal width. see demothinjt.m and makethinjt.m which assumes an initial graph g and a graph of candidate edges c iteratively expanding g until a maximal tree width limit is reached. see also for a discussion on learning an appropriate markov structure based on data. code absorb.m absorption update v s w absorption.m full absorption schedule over tree jtree.m form a junction tree triangulate.m triangulation based on simple node elimination utility routines knowing if an undirected graph is a tree and returning a valid elimination sequence is useful. a connected graph is a tree if the number of edges plus is equal to the number of nodes. however for a possibly disconnected graph this is not the case. the code deals with the possibly disconnected case returning a valid elimination sequence if the graph is singly-connected. the routine is based on the observation that any singly-connected graph must always possess a simplical node which can be eliminated to reveal a smaller singly-connected graph. istree.m if graph is singly connected return and elimination sequence elimtri.m vertexnode elimination on a triangulated graph with given end node demojtree.m junction tree chest clinic draft march exercises exercise show that the markov network elimination labelling for this graph. exercise consider the following distribution exercises is not perfect elimination ordered and give a perfect draw a clique graph that represents this distribution and indicate the separators on the graph. write down an alternative formula for the distribution in terms of the marginal probabilities exercise consider the distribution write down a junction tree for the above graph. carry out the absorption procedure and demonstrate that this gives the correct result for the marginal exercise consider the distribution pa b c d e f g h i papbapcapdapebpfcpgdphe fpif g draw the belief network for this distribution. draw the moralised graph. draw the triangulated graph. your triangulated graph should contain cliques of the smallest size possible. draw a junction tree for the above graph and verify that it satisfies the running intersection property. describe a suitable initialisation of clique potentials. describe the absorption procedure and write down an appropriate message updating schedule. exercise this question concerns the distribution pa b c d e f papbapcbpdcpedpfa e draw the belief network for this distribution. draw the moralised graph. draw the triangulated graph. your triangulated graph should contain cliques of the smallest size possible. draw a junction tree for the above graph and verify that it satisfies the running intersection property. describe a suitable initialisation of clique potentials. describe the absorption procedure and an appropriate message updating schedule. show that the distribution can be expressed in the form pafpba cpca dpda epea fpf draft march exercises exercise for the undirected graph on the square lattice as shown draw a triangulated graph with the smallest clique sizes possible. exercise consider a binary variable markov random field px z ij xj defined ixixj for i a neighbour of j on the lattice on the n n lattice with xj e and i j. a naive way to perform inference is to first stack all the variables in the tth column and call this cluster variable xt as shown. the resulting graph is then singly-connected. what is the complexity of computing the normalisation constant based on this cluster representation? compute log z for n exercise given a consistent junction tree on which a full round of message passing has occurred explain how to form a belief network from the junction tree. exercise the file diseasenet.mat contains the potentials for a disease bi-partite belief network with diseases and symptoms each disease and symptom is a binary variable and each symptom connects to parent diseases. using the brmltoolbox construct a junction tree for this distribution and use it to compute all the marginals of the symptoms psi on most standard computers computing the marginal psi by raw summation of the joint distribution is computationally infeasible. explain how to compute the marginals psi in a tractable way without using the junction tree formalism. by implementing this method compare it with the results from the junction tree algorithm. consider the scenario in which all the symptom variables are instantiated. using the junction tree estimate an upper bound on the number of seconds that computing a marginal takes assuming that for a two clique table containing s states absorption takes o seconds for an unspecified compare this estimate with the time required to compute the marginal by raw summation of the instantiated belief network. draft march exercises draft march chapter making decisions expected utility this chapter concerns situations in which decisions need to be taken under uncertainty. consider the following scenario you are asked if you wish to take a bet on the outcome of tossing a fair coin. if you bet and win you gain if you bet and lose you lose if you don t bet the cost to you is zero. we can set this up using a two state variable x with domx lose a decision variable d with domd no bet and utilities as follows uwin bet ulose bet uwin no bet ulose no bet since we don t know the state of x in order to make a decision about whether or not to bet arguably the best we can do is work out our expected winningslosses under the situations of betting and not if we bet we would expect to gain ubet pwin uwin bet plose ulose bet if we don t bet the expected gain is zero uno bet based on taking the decision which maximises expected utility we would therefore be advised not to bet. definition expected utility. the utility of a decision is ud where px is the distribution of the outcome x and d represents the decision. utility of money you are a wealthy individual with in your bank account. you are asked if you would like to participate in a fair coin tossing bet in which if you win your bank account will become however if you lose your bank account will contain only assuming the coin is fair should you take the bet? if we take the bet our expected bank balance would be ubet if we don t bet our bank balance will remain at based on expected utility we are therefore advised to take the bet. that if one considers instead the amount one will win or lose one may show decision trees that the difference in expected utility between betting and not betting is the same whilst the above is a correct mathematical derivation few people who are millionaires are likely to be willing to risk losing almost everything in order to become a billionaire. this means that the subjective utility of money is not simply the quantity of money. in order to better reflect the situation the utility of money would need to be a non-linear function of money growing slowly for large quantities of money and decreasing rapidly for small quantities of money decision trees decision trees are a way to graphically organise a sequential decision process. a decision tree contains decision nodes each with branches for each of the alternative decisions. chance nodes variables also appear in the tree with the utility of each branch computed at the leaf of each branch. the expected utility of any decision can then be computed on the basis of the weighted summation of all branches from the decision to all leaves from that branch. example consider the decision problem as to whether or not to go ahead with a fund-raising garden party. if we go ahead with the party and it subsequently rains then we will lose money very few people will show up on the other hand if we don t go ahead with the party and it doesn t rain we re free to go and do something else fun. to characterise this numerically we use prain rain prain no rain the utility is defined as u rain u no rain u party rain u party no rain we represent this situation in the question is should we go ahead with the party? since we don t know what will actually happen to the weather we compute the expected utility of each decision uparty rainprain uno party rainprain u u party rain rain rain max p arty based on expected utility we are therefore advised to go ahead with the party. the maximal expected utility is given by demodecparty.m prainup arty rain example an extension of the party problem is that if we decide not to go ahead with the party we have the opportunity to visit a friend. however we re not sure if this friend will be in. the question is should we still go ahead with the party? we need to quantify all the uncertainties and utilities. if we go ahead with the party the utilities are as before uparty rain uparty no rain draft march decision trees y e s rain yes p arty n o y e s rain with figure a decision tree containing chance nodes with ovals decision nodes with squares and utility nodes with diamonds. note that a decision tree is not a graphical representation of a belief network with additional nodes. rather a decision tree is an explicit enumeration of the possible choices that can be made beginning with the leftmost decision node with probabilities on the links out of chance nodes. prain rain prain no rain if we decide not to go ahead with the party we will consider going to visit a friend. in making the decision not to go ahead with the party we have utilities uparty party rain uparty party no rain the probability that the friend is in depends on the weather according to pf riend inrain pf riend inno rain the other probabilities are determined by normalisation. we additionally have uvisit in visit uvisit out visit with the remaining utilities zero. the two sets of utilities add up so that the overall utility of any decision sequence is uparty uvisit. the decision tree for the party-friend problem is shown is for each decision sequence the utility of that sequence is given at the corresponding leaf of the dt. note that the leaves contain the total utility uparty uvisit. solving the dt corresponds to finding for each decision node the maximal expected utility possible optimising over future decisions. at any point in the tree choosing that action which leads to the child with highest expected utility will lead to the optimal strategy. using this we find that the optimal expected utility has value and is given by going ahead with the party see demodecpartyfriend.m. in dts the same nodes are often repeated throughout the tree. for a longer sequence of decisions the number of branches in the tree can grow exponentially with the number of decisions making this representation impractical. in this example the dt is asymmetric since if we decide to go ahead with the party we will not visit the friend curtailing the further decisions present in the lower half of the tree. rain f riend mathematically we can express the optimal expected utility u for the party-visit example by summing over un-revealed variables and optimising over future decisions max p arty prain max v isit pf riendrain arty rain uvisitv isit f riendi arty no where the term i arty no has the effect of curtailing the dt if the party goes ahead. to answer the question as to whether or not to go ahead with the party we take that state of p arty that corresponds to draft march s e y p arty s e y rain n o yes v isit n o s e y rain f riend o u t n o f riend o u t n o f riend o u t yes v isit n o f riend o u t s e y s e y n o in p f o u t yes v n o n o s e y in f o u t in n o f riend o u t yes v n o in f o u t decision trees figure solving a decision tree. decision tree for the party-visit solving the dt responds to making the decision with the highest expected future utility. this can be achieved by starting at the leaves for a chance parent node x the utility of the parent is the expected utility of that variable. for example at the top of the dt we have the rain variable with the children and hence the expected utility of the rain node is for a decision node the value of the node is the optimum of its child values. one recurses thus backwards from the leaves to the root. for example the value of the rain chance node in the lower branch is given by the optimal decision sequence is then given at each decision node by finding which child node has the maximal value. hence the overall best decision is to decide to go ahead with the party. if we decided not to do so and it does not rain then the best decision we could take would be to not visit the friend has an expected utility of a more compact description of this problem is given by the influence diagram see also demodecpartyfriend.m. the maximal expected utility above. the way to read equation is to start from the last decision that needs to be taken in this case v isit. when we are at the v isit stage we assume that we will have previously made a decision about p arty and also will have observed whether or not is is raining. however we don t know whether or not our friend will be in so we compute the expected utility by averaging over this unknown. we then take the optimal decision by maximising over v isit. subsequently we move to the next-to-last decision assuming that what we will do in the future is optimal. since in the future we will have taken a decision under the uncertain f riend variable the current decision can then be taken under uncertainty about rain and maximising this expected optimal utility over p arty. note that the sequence of maximisations and summations matters changing the order will in general result in a different problem with a different expected one only had a sequence of summations the order of the summations is irrelevant likewise for the case of all maximisations. however summation and maximisation operators do not in general commute. draft march extending bayesian networks for decisions p arty rain u tility figure an influence diagram which contains random variables with ovalscircles decision nodes with squares and utility nodes with diamonds. contrasted with this is a more compact representation of the structure of the problem. the diagram represents the expression prainuparty rain. in addition the diagram denotes an ordering of the variables with party rain to the convention given by equation extending bayesian networks for decisions an influence diagram is a bayesian network with additional decision nodes and utility nodes the decision nodes have no associated distribution the utility nodes are deterministic functions of their parents. the utility and decision nodes can be either continuous or discrete for simplicity in the examples here the decisions will be discrete. a benefit of decision trees is that they are general and explicitly encode the utilities and probabilities associated with each decision and event. in addition we can readily solve small decision problems using decision trees. however when the sequence of decisions increases the number of leaves in the decision tree grows and representing the tree can become an exponentially complex problem. in such cases it can be useful to use an influence diagram an id states which information is required in order to make each decision and the order in which these decisions are to be made. the details of the probabilities and rewards are not specified in the id and this can enable a more compact description of the decision problem. syntax of influence diagrams information links an information link from a random variable into a decision node x d indicates that the state of the variable x will be known before decision d is taken. information links from another decision node d in to d similarly indicate that decision d is known before decision d is taken. we use a dashed link to denote that decision d is not functionally related to its parents. random variables random variables may depend on the states of parental random variables in belief networks but also decision node states d x y as decisions are taken the states of some random variables will be revealed. to emphasise this we typically shade a node to denote that its state will be revealed during the sequential decision process. utilities a utility node is a deterministic function of its parents. the parents can be either random variables or decision nodes. in the party example the bn trivially consists of a single node and the influence diagram is given in the more complex party-friend problem is depicted in the id generally provides a more compact representation of the structure of problem than a dt although details about the specific probabilities and utilities are not present in the id. draft march extending bayesian networks for decisions p arty v isit rain f riend uparty uvisit figure an influence diagram for the party-visit problem the partial ordering is p arty rain v isit f riend. the dashed-link from party to visit is not strictly necessary but retained in order to satisfy the convention that there is a directed path connecting all decision nodes. partial ordering an id defines a partial ordering of the nodes. we begin by writing those variables whose states are known variables before the first decision we then find that set of variables whose states are revealed before the second decision subsequently the set of variables xt is revealed before decision the remaining fully-unobserved variables are placed at the end of the ordering xn dn xn with xk being the variables revealed between decision dk and the term partial refers to the fact that there is no order implied amongst the variables within the set xn. for notational clarity at points below we will indicate decision variables with to reinforce that we maximise over these variables and sum over the non-starred variables. where the sets are empty we omit writing them. for example in the ordering is t est oil. the optimal first decision is determined by computing seismic drill uj xn xn i i p j j max dn max for each state of the decision given in equation above i denotes the set of indices for the random variables and j the indices for the utility nodes. for each state of the conditioning variables the optimal decision is found using argmax remark off the partial ordering. sometimes it can be tricky to read the partial ordering from the id. a method is to identify the first decision and then any variables that need to be observed to make that decision. then identify the next decision and the variables that are revealed after decision is taken and before decision is taken etc. this gives the partial ordering place any unrevealed variables at the end of the ordering. implicit and explicit information links the information links are a potential source of confusion. an information link specifies explicitly which quantities are known before that decision is we also implicitly assume the no forgetting principle that all past decisions and revealed variables are available at the current decision revealed variables are necessarily the parents of all past decision nodes. if we were to include all such information links ids would get potentially rather messy. in both explicit and implicit information links are demonstrated. we call an information link fundamental if its removal would alter the partial ordering. authors prefer to write all information links where possible and others prefer to leave them implicit. here we largely take the implicit approach. for the purposes of computation all that is required is a partial ordering one can therefore view this as basic and the information links as superficial draft march extending bayesian networks for decisions t est oil t est oil seismic drill seismic drill figure the partial ordering is t est oil. the explicit information links from t est to seismic and from seismic to drill are both fundamental in the sense that removing either results in a different partial ordering. the shaded node emphasises that the state of this variable will be revealed during the sequential decision process. conversely the non-shaded node will never be observed. based on the id in there is an implicit link from t est to drill since the decision about t est is taken before seismic is revealed. seismic drill causal consistency for an influence diagram to be consistent a current decision cannot affect the past. this means that any random variable descendants of a decision d in the id must come later in the partial ordering. assuming the no-forgetting principle this means that for any valid id there must be a directed path connecting all decisions. this can be a useful check on the consistency of an id. asymmetry ids are most convenient when the corresponding dt is symmetric. however some forms of asymmetry are relatively straightforward to deal with in the id framework. for our party-visit example the dt is asymmetric. however this is easily dealt with in the id by using a link from p arty to uvisit which removes the contribution from uvisit when the party goes ahead. more complex issues arise when the set of variables than can be observed depends on the decision sequence taken. in this case the dt is asymmetric. in general influence diagrams are not well suited to modelling such asymmetries although some effects can be mediated either by careful use of additional variables or extending the id notation. see and for further details of these issues and possible resolutions. example i do a phd?. consider a decision whether or not to do phd as part of our education taking a phd incurs costs uc both in terms of fees but also in terms of lost income. however if we have a phd we are more likely to win a nobel prize which would certainly be likely to boost our income subsequently benefitting our finances this setup is depicted in the ordering is empty sets e p and dome phd no phd domi average high domp no prize the probabilities are pwin nobel prizeno phd pwin nobel prizedo phd plowdo phd no prize paveragedo phd no prize phighdo phd no prize plowno phd no prize paverageno phd no prize phighno phd no prize plowdo phd prize phighdo phd prize plowno phd prize phighno phd prize paveragedo phd prize paverageno phd prize draft march s us p i ub e uc e uc p i ub extending bayesian networks for decisions figure education e incurs some cost but also gives a chance to win a prestigious science prize. both of these affect our likely incomes with corresponding long term financial benefits. the start-up scenario. the utilities are uc phd uc phd ub ub ub the expected utility of education is ue ip pie p ubi so that udo phd whilst not taking a phd is uno phd making it on average beneficial to do a phd. see demodecphd.m. example and start-up companies. influence diagrams are particularly useful when a sequence of decisions is taken. for example in we model a new situation in which someone has first decided whether or not to take a phd. ten years later in their career they decide whether or not to make a start-up company. this decision is based on whether or not they won the nobel prize. the start-up decision is modelled by s with doms fa. if we make a start-up this will cost some money in terms of investment. however the potential benefit in terms of our income could be high. we model this with other required table entries being taken from plowstart up no prize plowno start up no prize paverageno start up no prize phighno start up no prize plowstart up prize plowno start up prize paveragestart up no prize paveragestart up prize paverageno start up prize phighstart up no prize phighstart up prize phighno start up prize and us up us start up our interest is to advise whether or not it is desirable terms of expected utility to take a phd now bearing in mind that later one may or may not win the nobel prize and may or may not make a start-up company. the ordering is empty sets e p s i draft march solving influence diagrams figure markov decision process. these can be used to model planning problems of the form how do i get to where i want to be incurring the lowest total cost? they are readily solvable using a message passing algorithm. the expected optimal utility for any state of e is ue max s p i pis p uce ubi where we assume that the optimal decisions are taken in the future. computing the above we find udo phd uno phd hence we are better off not doing a phd. see demodecphd.m. solving influence diagrams solving an influence diagram means computing the optimal decision or sequence of decisions. here we focus on finding the optimal first decision. the direct approach is to take equation and perform the required sequence of summations and maximisations explicitly. however we may be able to exploit the structure of the problem to for computational efficiency. to develop this we first derive an efficient algorithm for a highly structured id the markov decision process which we will discuss further in efficient inference consider the following function from the id of where the represent conditional probabilities and the u are utilities. we write this in terms of potentials since this will facilitate the generalisation to other cases. our task is to take the optimal first decision based on the expected optimal utility max max whilst we could carry out the sequence of maximisations and summations naively our interest is to derive a computationally efficient approach. let s see how to distribute these operations by hand since only depends on explicitly we can write max max max max max max draft march solving influence diagrams starting with the first line and carrying out the summation over and max over this gives a new function of max in addition we define the message in our particular example will be unity max using this we can write max max now we carry out the sum over and max over for the first row above and define a utility message max and probability max the optimal decision for can be obtained from since the probability message represents information about the distribution passed to via it is more intuitive to write which has the interpretation of the average of a utility with respect to a distribution. it is intuitively clear that we can continue along this line for richer structures than chains. indeed provided we have formed an appropriate junction tree we can pass potential and utility messages from clique to neighbouring clique as described in the following section. using a junction tree in complex ids computational efficiency in carrying out the series of summations and maximisations may be an issue and one therefore seeks to exploit structure in the id. it is intuitive that some form of junction tree style algorithm is applicable. we can first represent an id using decision potentials which consist of two parts as defined below. definition potential. a decision potential on a clique c contains two potentials a probability potential c and a utility potential c. the joint potentials for the junction tree are defined as c c c c c c with the junction tree representing the term our mdp example all these probability messages are unity. draft march solving influence diagrams in this case there are constraints on the triangulation imposed by the partial ordering which restricts the variables elimination sequence. this results in a so-called strong junction tree. the treatment here is inspired by a related approach which deals with more general chain graphs is given in the sequence of steps required to construct a jt for an id is as follows remove information edges parental links of decision nodes are moralization marry all parents of the remaining nodes. remove utility nodes remove the utility nodes and their parental links. strong triangulation form a triangulation based on an elimination order which obeys the partial or dering of the variables. strong junction tree from the strongly triangulated graph form a junction tree and orient the edges towards the strong root clique that appears last in the elimination sequence. the cliques are ordered according to the sequence in which they are eliminated. the separator probability cliques are initialised to the identity with the separator utilities initialised to zero. the probability cliques are then initialised by placing conditional probability factors into the lowest available clique to the elimination order that can contain them and similarly for the utilities. remaining probability cliques are set to the identity and utility cliques to zero. example tree. an example of a junction tree for an id is given in the moralisation and triangulation links are given in the orientation of the edges follows the partial ordering with the leaf cliques being the first to disappear under the sequence of summations and maximisations. a by-product of the above steps is that the cliques describe the fundamental dependencies on previous decisions and observations. in for example the information link from f to is not present in the moralised-triangulated graph nor in the associated cliques of this is because once e is revealed the utility is independent of f giving rise to the two-branch structure in nevertheless the information link from f to is fundamental since it specifies that f will be revealed removing this link would therefore change the partial ordering. absorption by analogy with the definition of messages in for two neighbouring cliques and where is closer to the strong root of the jt last clique defined through the elimination order we define s s new s in the above s new s c is a generalised marginalisation operation it sums over those elements of clique c which are random variables and maximises over the decision variables in the clique. the order of this sequence of sums and maximisations follows the partial ordering defined by absorption is then computed from the leaves inwards to the root of the strong junction tree. the optimal setting of a decision can then be computed from the root clique. subsequently backtracking may be that for the case in which the domain is dependent on the parental variables such links must remain. draft march l a b c d i h j k g a b c d e f b c a b c solving influence diagrams i h l j k g e f b e d c e g g g i i i l b e d b e f d e f f h h h k h k h k j figure influence diagram adapted from causal consistency is satisfied since there is a directed path linking the all decisions in sequence. the partial ordering is b f g c d h i j k l. moralised and strongly triangulated graph. moralisation links are in green strong triangulation links are in red. strong junction tree. absorption passes information from the leaves of the tree towards the root. applied to infer the optimal decision trajectory. the optimal decision for d can be obtained by working with the clique containing d which is closest to the strong root and setting any previously taken decisions and revealed observations into their evidential states. see demodecasia.m for an example. example on a chain. for the id of the moralisation and triangulation steps are trivial and give the jt where the cliques are indexed according the elimination order. the probability and utility cliques are initialised to draft march updating the separator we have the new probability potential solving influence diagrams with the separator cliques initialised to max and utility potential max max max at the next step we update the probability potential and utility potential max the next separator decision potential is max max max max finally we end up with the root decision potential and max max from the final decision potential we have the expression which is equivalent to that which would be obtained by simply distributing the summations and maximisations over the original id. at least for this special case we therefore have verified that the jt approach yields the correct root clique potentials. draft march markov decision processes markov decision processes consider a markov chain with transition probabilities jxt i. at each time t we consider an action which affects the state at time t we describe this by ixt j dt k associated with each state xt i is a utility uxt i and is schematically depicted in one use of such an environment model would be to help plan a sequence of actions required to reach a goal state in minimal total summed cost. more generally one could consider utilities that depend on transitions and decisions i xt j dt k and also time dependent versions of all of these ixt j dt k i xt j dt k. we ll stick with the time-independent case here since the generalisations are conceptually straightforward at the expense of notational complexity. mdps can be used to solve planning tasks such as how can one get to a desired goal state as quickly as possible. by defining the utility of being in the goal state as high and being in the non-goal state as a low value at each time t we have a utility uxt of being in state xt. for positive utilities the total utility of any state-decision path is defined as we know the initial state and the probability with which this happens is given by uxt t max max dt max dt xt at time t we want to make that decision that will lead to maximal expected total utility our task is to compute for each state of and then choose that state with maximal expected total utility. to carry out the summations and maximisations efficiently we could use the junction tree approach as described in the previous section. however in this case the id is sufficiently simple that a direct message passing approach can be used to compute the expected utility. maximising expected utility by message passing consider the mdp t dt uxt for the specific example in the joint model of the bn and utility is to decide on how to take the first optimal decision we need to compute max max since only depends on explicitly we can write max max max draft march max we now start with the first line and carry out the summation over and maximisation over this gives a new function of markov decision processes for each line we distribute the operations max max max which we can incorporate in the next line max similarly we can now carry out the sum over and max over to define a new function max to give given above we can then find the optimal decision by bear in mind that when we come to make decision we will have observed and in general the optimal decision is given by argmax d what about d we can then find d by argmax subsequently we can backtrack further to find d t argmax d pxtxt dt ut dt xt bellman s equation xt ut txt max dt pxtxt dt ut it is more common to define the value of being in state xt as vtxt uxt ut vt uxt and write then the equivalent recursion vt uxt max dt draft march pxtxt dt xt in a markov decision process as above we can define utility messages recursively as temporally unbounded mdps the optimal decision d t is then given by t argmax d dt is called bellman s temporally unbounded mdps in the previous discussion about mdps we assumed a given end time t from which one can propagate messages back from the end of the chain. the infinite t case would appear to be ill-defined since the sum of utilities uxt will in general be unbounded. there is a simple way to avoid this difficulty. if we let u maxs us be the largest value of the utility and consider the sum of modified utilities for a chosen discount factor tuxt u t u t where we used the result for a geometric series. in the limit t this means that the summed modified utility tuxt is finite. the only modification required to our previous discussion is to include a factor in the message definition. assuming that we are at convergence we define a value vxt s dependent only on the state s and not the time. this means we replace the time-dependent bellman s value recursion equation with the stationary equation vs us max d pxt s dt we then need to solve equation for the value vs for all states s. the optimal decision policy when one is in state xt s is then given by d argmax s dt d for a deterministic transition p for each decision d only one state is available this means that the best decision is the one that takes us to the accessible state with highest value. seems straightforward to solve. however the max operation means that the equations are non-linear in the value v and no closed form solution is available. two popular techniques for solving equation are value and policy iteration which we describe below. when the number of states s is very large approximate solutions are required. sampling and state-dimension reduction techniques are described in value iteration a naive procedure is to iterate equation until convergence assuming some initial guess for the values uniform. one can show that this value iteration procedure is guaranteed to converge to a unique the convergence rate depends somewhat on the discount the smaller is the faster is the convergence. an example of value iteration is given in continuous-time analog has a long history in physics and is called the hamilton-jacobi equation and enables one to solve mdps by message passing this being a special case of the more general junction tree approach described earlier in draft march temporally unbounded mdps figure states defined on a two dimensional grid. in each square the top left value is the state number and the bottom right is the utility of being in that state. an agent can move from a state to a neighbouring state as indicated. the task is to solve this problem such that for any position one knows how to move optimally to maximise the expected utility. this means that we need to move towards the goal states with nonzero utility. see demomdp. figure value iteration on a set of states corresponding to a two dimensional grid. deterministic transitions are allowed to neighbours on the grid left right up down. there are three goal states each with utility all other states have utility plotted is the value vs for after updates of value iteration where the states index a point on the x y grid. the optimal decision for any state on the grid is to go to the neighbouring state with highest value. see demomdp. policy iteration in policy iteration we first assume we know the optimal decision d for any state s. we may use this in equation to give vs us pxt s d the maximisation over d has disappeared since we have assumed we already know the optimal decision for each state s. for fixed d equation is now linear in the value. defining the value v and utility u vectors and transition matrix p us vs ps d in matrix notation equation becomes i i v u ptv v u v u these linear equations are readily solved with gaussian elimination. using this the optimal policy is recomputed using equation the two steps of solving for the value and recomputing the policy are iterated until convergence. in policy iteration we guess an initial d then solve the linear equations for the value and then recompute the optimal decision. see demomdp.m for a comparison of value and policy iteration and also an em style approach which we discuss in the next section. example grid-world mdp. a set of states defined on a grid utilities for being in a grid state is given in for which the agent deterministically moves to a neighbouring grid state at each time step. after initialising the value of each grid state to unity the converged value for each state is given in the optimal policy is then given by moving to the neighbouring grid state with highest value. draft march probabilistic inference and planning figure a markov decision prob the corresponding probabiliscess. tic inference planner. a curse of dimensionality consider the following tower of hanoi problem. there are pegs a b c d and disks numbered from to you may move a single disk from one peg to another however you are not allowed to put a bigger numbered disk on top of a smaller numbered disk. starting with all disks on peg a how can you move them all to peg d in the minimal number of moves? this would appear to be a straightforward markov decision process in which the transitions are allowed disk moves. if we use x to represent the state of the disks on the pegs this has states are equivalent up to permutation of the pegs which reduces this by a factor of this large number of states renders this naive approach computationally problematic. many interesting real-world problems suffer from this large number of states issue so that a naive approach based as we ve described is computationally infeasible. finding efficient exact and also approximate state representations is a key aspect to solving large scale mdps see for example probabilistic inference and planning an alternative to the classical mdp solution methods is to make use of the standard methods for training probabilistic models such as the expectation-maximisation algorithm. in order to do so we first need to write the problem of maximising expected utility in a form that is suitable. to do this we first discuss how a mdp can be expressed as the maximisation of a form of belief network in which the parameters to be found relate to the policy. non-stationary markov decision process consider the mdp in in which for simplicity we assume we know the initial state our task is then to find the decisions that maximise the expected utility based on a sequential decision process. the first decision is given by maximising the expected utility max more generally this utility can be computed efficiently using a standard message passing routine ut max dt where ut t ut draft march probabilistic inference and planning non-stationary probabilistic inference planner as an alternative to the above mdp description consider the belief network in which we have a utility associated with the last then the expected utility is given by u here the terms pdtxt t are the policy distributions that we wish to learn and t are the parameters of the tth policy distribution. let s assume that we have one per time so that t is a function that maps a state x to a probability distribution over decisions. our interest is to find the policy distributions that maximise the expected utility. since each time-step has its own t and for each state we have a separate unconstrained distribution to optimise over and we can write max u max max this shows that provided there are no constraints on the policy distributions is a separate one for each timepoint we are allowed to distribute the maximisations over the individual policies inside the summation. more generally for a finite time t one can define messages to solve for the optimal policy distributions ut max t dt pdtxt with ut t ut deterministic policy for a deterministic policy only a single state is allowed so that where d function for each time t equation reduces to pdtxt t d t t is a policy function that maps a state x to a single decision d. since we have a separate policy ut max d t t d which is equivalent to equation this shows that solving the mdp is equivalent to maximising a standard expected utility defined in terms of a belief network under the assumption that each time point has its own policy distribution and that this is deterministic. stationary planner if we reconsider our simple example but now constrain the policy distributions to be the same for all time pdtxt t pdtxt more succinctly t then equation becomes u in this case we cannot distribute the maximisation over the policy over the individual terms of the product. however computing the expected utility for any given policy is straightforward using message passing. one may thus optimise the expected utility using standard numerical optimisation procedures or alternatively an em style approach as we discuss below. draft march a variational training approach probabilistic inference and planning without loss of generality we assume that the utility is positive and define a distribution then for any variational distribution using the definition of and the fact that the denominator in equation is equal to u we obtain the bound log u this then gives a two-stage em style procedure m-step isolating the dependencies on for a given variational distribution qold maximising the bound equation is equivalent to maximising e one then finds a policy new which maximises e new argmax e e-step for fixed the best q is given by the update qnew from this joint distribution in order to determine the m-step updates we only require the marginals and both of which are straightforward to obtain since q is simply a first order markov chain in the joint variables xt dt. for example one may write the q-distribution as a simple chain factor graph for which marginal inference can be performed readily using the sum-product algorithm. this procedure is analogous to the standard em procedure the usual guarantees therefore carry over so that finding a policy that increases e is guaranteed to improve the expected utility. in complex situations in which for reasons of storage the optimal q cannot be used a structured constrained variational approximation may be applied. in this case as in generalised em only a guaranteed improvement on the lower bound of the expected utility is achieved. nevertheless this may be of considerable use in practical situations for which general techniques of approximate inference may be applied. the deterministic case for the special case that the policy is deterministic simply maps each state x to single decision d. writing this policy map as d equation reduces to ud d d we now define a variational distribution only over d d draft march probabilistic inference and planning and the energy term becomes ed d eds t and d argmax eds ds d for a more general problem in which the utility is at the last time point t and no starting state is given we have a stationary transition dt qxt s log px s ds d this shows how to train a stationary mdp using em in which there is a utility defined only at the last time-point. below we generalise this to the case of utilities at each time for both the stationary and non-stationary cases. utilities at each timestep consider a generalisation in which we have an additive utility associated with each time-point. non-stationary policy to help develop the approach let s look at simply including utilities at times t for the previous example. the expected utility is given by u v v defining value messages and u v for a more general case defined over t timesteps we have analogously an expected utility u and our interest is to maximise this expected utility with respect to all the policies max u as before since each timestep has its own policy distribution for each state we may distribute the maximisation using the recursion utxt max t with vtt uxt draft march pdtxt stationary deterministic policy probabilistic inference and planning for an mdp the optimal policy is so that methods which explicitly seek for deterministic policies are of interest. for a stationary deterministic policy we have the expected utility utxt xt u px dx with the convention viewed as a factor graph this is simply a chain so that for any policy d the expected utility can be computed easily. in principle one could then attempt to optimise u with respect to the decisions directly. an alternative is to use an em style to do this we need to define a distribution t utxt zd px dx the normalisation constant zd of this distribution is utxt px dx utxt px dx u if we now define a variational distribution t and consider t t this gives the lower bound log u t log utxt px dx in terms of an em algorithm the m-step requires the dependency on d alone which is ed px dx qx x s t log px s dx d for each given state s we now attempt to find the optimal decision d which corresponds to maximising qx x s t eds defining qx x s t log d we see that for given s up to a constant eds is the kullback-leibler divergence between aligned with d so that the optimal decision d is given by the index of the distribution d argmin and d most closely d draft march further topics figure an example partially observable markov decision process the hidden variables h are never observed. in solving the influence diagram we are required to first sum over variables that are never observed doing so will couple together all past observed variables and decisions that means any decision at time t will depend on all previous decisions. note that the no-forgetting principle means that we do not need to explicitly write that each decision depends on all previous observations this is implicitly assumed. the e-step concerns the computation of the marginal distributions required in the m-step. the optimal q distribution is proportional to p evaluated at the previous decision function d t utxt px dx for a constant discount factor at each time-step and an otherwise stationary utxt tuxt using this t tuxt px dx for each t this is a simple markov chain for which the pairwise transition marginals required for the m-step equation are straightforward. this requires inference in a series of markov models of different lengths. this can be done efficiently using a single forward and backward see mdpemdeterministicpolicy.m which also deals with the more general case of utilities dependent on the decisionaction as well as the state. note that this em algorithm formally fails in the case of a deterministic environment transition pxtxt dt is deterministic see for an explanation and for a possible resolution. further topics partially observable mdps in a pomdp there are states that are not observed. this seemingly innocuous extension of the mdp case can lead however to computational difficulties. let s consider the situation in and attempt to compute the optimal expected utility based on the sequence of summations and maximisations u max max max the sum over the hidden variables couples all the decisions and observations meaning that we no longer have a simple chain structure for the remaining maximisations. for a pomdp of length t this leads to intractable problem with complexity exponential in t. an alternative view is to recognise that all past decisions and observations can be summarised in terms of a belief in the current latent state this suggests that instead of having an actual state as in the mdp case we need the standard mdp framework it is more common to define utxt t so that for comparison with the standard policyvalue routines one needs to divide the expected utility by draft march further topics to use a distribution over states to represent our current knowledge. one can therefore write down an effective mdp albeit over belief distributions as opposed to finite states. approximate techniques are required to solve the resulting infinite state mdps and the reader is referred to more specialised texts for a study of approximation procedures. see for example restricted utility functions an alternative to solving mdps is to consider restricted utilities such that the policy can be found easily. recently efficient solutions have been developed for classes of mdps with utilities restricted to kullbackleibler divergences reinforcement learning reinforcement learning deals mainly with stationary markov decision processes. the added twist is that the transition d possibly the utility is unknown. initially an agent begins to explore the set of states and utilities associated with taking decisions. the set of accessible states and their rewards populates as the agent traverses its environment. consider for example a maze problem with a given start and goal state though with an unknown maze structure. the task is to get from the start to the goal in the minimum number of moves on the maze. clearly there is a balance required between curiosity and acting to maximise the expected reward. if we are too curious t take optimal decisions given the currently available information about the maze structure and continue exploring the possible maze routes this may be bad. on the other hand if we don t explore the possible maze states we might never realise that there is a much more optimal short-cut to follow than that based on our current knowledge. this exploration-exploitation tradeoff is central to the difficulties of rl. see for an extensive discussion of reinforcement learning. for a given set of environment data x transitions and utilities one aspect of rl problem can be considered as finding the policy that maximises expected reward given only a prior belief about the environment and observed decisions and states. if we assume we know the utility function but not the transition we may write u where represents the environment state transition dt given a set of observed states and decisions p px where p is a prior on the transition. similar techniques to the em style training can be carried through in this case as rather than the policy being a function of the state and the environment optimally one needs to consider a policy pdtxt b as a function of the state and the belief in the environment. this means that for example if the belief in the environment has high entropy the agent can recognise this and explicitly carry out decisionsactions to explore the environment. a further complication in rl is that the data collected x depends on the policy if we write t for an episode in which policy t is followed and data xt collected then the utility of the policy given all the historical information is u depending on the priors on the environment and also on how long each episode is we will have different posteriors for the environment parameters. if we then set argmax u this affects the data we collect at the next episode in this way the trajectory of policies can be very different depending on these episode lengths and priors. draft march code code summax under a partial order maxsumpot.m generalised elimination operation according to a partial ordering sumpotid.m summax an id with probability and decision potentials demodecparty.m demo of summingmaxing an id junction trees for influence diagrams there is no need to specify the information links provided that a partial ordering is given. in the code jtreeid.m no check is made that the partial ordering is consistent with the influence diagram. in this case the first step of the junction tree formulation in is not required. also the moralisation and removal of utility nodes is easily dealt with by defining utility potentials and including them in the moralisation process. the strong triangulation is found by a simple variable elimination scheme which seeks to eliminate a variable with the least number of neighbours provided that the variable may be eliminated according to the specified partial ordering. the junction tree is constructed based only on the elimination clique sequence obtained from the triangulation routine. the junction tree is then obtained by connecting a clique ci to the first clique j i that is connected to this clique. clique ci is then eliminated from the graph. in this manner a junction tree of connected cliques is formed. we do not require the separators for the influence diagram absorption since these can be computed and discarded on the fly. note that the code only computes messages from the leaves to the root of the junction tree which is sufficient for taking decisions at the root. if one desires an optimal decision at a non-root one would need to absorb probabilities into a clique which contains the decision required. these extra forward probability absorptions are required because information about any unobserved variables can be affected by decisions and observations in the past. this extra forward probability schedule is not given in the code and left as an exercise for the interested reader. jtreeid.m junction tree for an influence diagram absorptionid.m absorption on an influence diagram triangulateporder.m triangulation based on a partial ordering demodecphd.m demo for utility of doing phd and startup party-friend example the code below implements the party-friend example in the text. to deal with the asymmetry the v isit utility is zero if p arty is in state yes. demodecpartyfriend.m demo for party-friend chest clinic with decisions the table for the chest clinic decision network is taken from see there if an x-ray is taken then information about x is is a slight modification however to the pxe table. available. however if the decision is not to take an x-ray no information about x is available. this is a form of asymmetry. a straightforward approach in this case is to make dx a parent of the x variable and draft march a t x dx ux l e s d b dh uh code s smoking x positive x-ray d dyspnea of breath e either tuberculosis or lung cancer t tuberculosis l lung cancer b bronchitis a visited asia dh hospitalise? dx take x-ray? figure influence diagram for the chest clinic decision example. set the distribution of x to be uninformative if dx fa. pa tr ps tr pt tra tr pt tra fa pl trs tr pl trs fa pb trs tr pb trs fa px tre tr dx tr px tre fa dx tr px tre fa dx fa px tre tr dx fa pd tre tr b tr pd tre tr b fa pd tre fa b tr pd tre fa b fa the two utilities are designed to reflect the costs and benefits of taking an x-ray and hospitalising a patient l tr t tr dh tr l fa t tr dh tr t fa l tr dh tr t fa l fa dh tr l tr dh fa t tr l fa dh fa t tr dh fa t fa l tr dh fa t fa l fa t tr dx tr t fa dx tr dx fa t tr dx fa t fa we assume that we know whether or not the patient has been to asia before deciding on taking an x-ray. the partial ordering is then a dx x dh e l s t the demo demodecasia.m produces the results utility table asia yes takexray yes takexray yes asia no asia yes takexray no asia no takexray no which shows that optimally one should take an x-ray only if the patient has been to asia. demodecasia.m junction tree influence diagram demo draft march exercises markov decision processes in demomdp.m we consider a simple two dimensional grid in which an agent can move to a grid square either above below left right of the current square or stay in the current square. we defined goal states squares that have high utility with others having zero utility. demomdpclean.m demo of value and policy iteration for a simple mdp mdpsolve.m mdp solver using value or policy iteration mdp solver using em and assuming a deterministic policy the following is not fully documented in the text although the method is reasonably straightforward and follows that described in the inference is carried out using a simple style recursion. this could also be implemented using the general factor graph code but was coded explicitly for reasons of speed. the code also handles the more general case of utilities as a function of both the state and the action uxt dt. mdpemdeterministicpolicy.m mdp solver using em and assuming a deterministic policy emqtranmarginal.m marginal information required for the transition term of the energy emqutilmarginal.m marginal information required for the utility term of the energy emtotalbetamessage.m backward information required for inference in the mdp emminimizekl.m find the optimal decision emvaluetable.m return the expected value of the policy exercises exercise you play a game in which you have a probability p of winning. if you win the game you gain an amount s and if you lose the game you lose an amount s. show that the expected gain from playing the game is exercise it is suggested that the utility of money is based not on the amount but rather how much we have relative to other peoples. assume a distribution pi i of incomes using a histogram with bins each bin representing an income range. use a histogram to roughly reflect the distribution of incomes in society namely that most incomes are around the average with few very wealthy and few extremely poor people. now define the utility of an income x as the chance that income x will be higher than a randomly chosen income y the distribution you defined and relate this to the cumulative distribution of p. write a program to compute this probability and plot the resulting utility as a function of income. now repeat the coin tossing bet of so that if one wins the bet one s new income will be placed in the top histogram bin whilst if one loses one s new income is in the lowest bin. compare the optimal expect utility decisions under the situations in which one s original income is average and much higher than average. exercise derive a partial ordering for the id on the right and explain how this id differs from that of t est oil seismic drill exercise this question follows closely demomdp.m and represents a problem in which a pilot wishes to land an airplane. the matrix ux y in the file airplane.mat contains the utilities of being in position x y and is a very crude model of a runway and taxiing area. to tom furmston for coding this. draft march exercises the airspace is represented by an grid gy in the notation employed in demomdp.m. the matrix represents that position is the desired parking bay of the airplane vertical height of the airplane is not taken in to account. the positive values in u represent runway and areas where the airplane is allowed. zero utilities represent neutral positions. the negative values represent unfavourable positions for the airplane. by examining the matrix u you will see that the airplane should preferably not veer off the runway and also should avoid two small villages close to the airport. at each timestep the plane can perform one of the following actions stay up down left right for stay the airplane stays in the same x y position. for up the airplane moves to the x y position. for down the airplane moves to the x y position. for left the airplane moves to the x y position. for right the airplane moves to the x y position. a move that takes the airplane out of the airspace is not allowed. the airplane begins in at point x y assuming that an action deterministically results in the intended grid move find the optimal xt yt sequence for times t for the position of the aircraft. the pilot tells you that there is a fault with the airplane. when the pilot instructs the plane to go right with probability it actually goes up this remains in the airspace. assuming again that the airplane begins at point x y return the optimal xt yt sequence for times t for the position of the aircraft. exercise the influence diagram depicted describes the first stage of a game. the decision variable not play indicates the decision to either play the first stage or not. if you decide to play there is a cost but no cost otherwise play the variable describes if you win or lose the game lose with probabilities play the utility of winninglosing is no play win lose show that the expected utility gain of playing this game is play if you win the exercise above describes the first stage of a new two-stage game. first stage win you have to make a decision as to whether or not play in the second stage not play. if you do not win the first stage you cannot enter the second stage. if you decide to play the second stage you win with probability win play if you decide not to play the second stage there is no chance to win win not play the cost of playing the second stage is play no play draft march exercises and the utility of winninglosing the second stage is win lose draw an influence diagram that describes this two-stage game. a gambler needs to decide if he should even enter the first stage of this two-stage game. show that based on taking the optimal future decision the expected utility based on the first decision is play if if exercise you have b in your bank account. you are asked if you would like to participate in a bet in which if you win your bank account will become w however if you lose your bank account will contain only l. you win the bet with probability pw. assuming that the utility is given by the number of pounds in your bank account write down a formula for the expected utility of taking the bet ubet and also the expected utility of not taking the bet uno bet. the above situation can be formulated differently. if you win the bet you gain b. if you lose the bet you lose l. compute the expected amount of money you gain if you bet ugainbet and if you don t bet ugainno bet. show that ubet uno bet ugainbet ugainno bet. exercise consider the party-friend scenario an alternative is to replace the link from p arty to uvisit by an information link from p arty to v isit with the constraint that v isit can be in state yes only if p arty is in state no. explain how this constraint can be achieved by including an additional additive term to the utilities and modify demodecpartyfriend.m accordingly to demonstrate this. for the case in which utilities are all positive explain how the same constraint can be achieved using a multiplicative factor. exercise consider an objective f x uxpx for a positive function ux and that our task is to maximise f with respect to an expectationmaximisation style bounding approach can be derived by defining the auxiliary distribution px uxpx f so that by considering klqx px for some variational distribution qx we obtain the bound log f px the m-step states that the optimal q distribution is given by qx px old at the e-step of the algorithm the new parameters new are given by maximising the energy term new argmax px px old show that for a deterministic distribution px f the e-step fails giving new old. draft march exercise consider an objective f uxpx x for a positive function ux and px f exercises and an arbitrary distribution nx. our task is to maximise f with respect to as the previous exercise showed if we attempt an em algorithm in the limit of a deterministic model then no-updating occurs and the em algorithm fails to find that optimises show that f and hence x nxux f new f old new old show that if for we can find a new such that f new f old then necessarily new old. using this result derive an em-style algorithm that guarantees to increase f we are already at the optimum for and therefore guarantees to increase hint use px uxpx f and consider klqx px for some variational distribution qx. exercise the file idjensen.mat contains probability and utility tables for the influence diagram of using brmltoolbox write a program that returns the maximal expected utility for this id using a strong junction tree approach and check the result by explicit summation and maximisation. similarly your program should output the maximal expected utility for both states of and check that the computation using the strong junction tree agrees with the result from explicit elimination summation and maximisation. exercise for a pomdp explain the structure of the strong junction tree and relate this to the complexity of inference in the pomdp. exercise define a partial order for the id diagram depicted. draw a junction tree for this id. i b f d g a e c h draft march part ii learning in probabilistic models chapter statistics for machine learning distributions definition distribution function. for a univariate distribution px the cdf is defined as cdfy px y for an unbounded domain cdf and cdf summarising distributions definition the mode x of a distribution px is the state of x at which the distribution takes its highest value x argmax px. a distribution could have more than one node multi-modal. a widespread abuse of terminology is to refer to any isolated local maximum of px to be a mode. x definition and expectation. denotes the average or expectation of fx with respect to the distribution px. a common alternative notation is ex when the context is clear one may drop the notational dependency on px. the notation is shorthand for the average of fx conditioned on knowing the state of variable y i.e. the average of fx with respect to the distribution pxy. an advantage of the expectation notations is that they hold whether the distribution is over continuous or discrete variables. in the discrete case summarising distributions x fx xpx x and for continuous variables fxpxdx the reader might wonder what means when x is discrete. if domx orange pear with associated probabilities px for each of the states what does refer to? clearly makes sense if fx x maps the state x to a numerical value. for example fx apple fx orange fx pear for which is meaningful. unless the states of the discrete variable are associated with a numerical value then has no meaning. for example definition the kth moment of a distribution is given by the average of xk under the distribution for k we have the mean typically denoted by definition and correlation. px px the square root of the variance is called the standard deviation. the notation varx is also used to emphasise for which variable the variance is computed. the reader may show that an equivalent expression is for a multivariate distribution the matrix with elements ij i where i is called the covariance matrix. the diagonal entries of the covariance matrix contain the variance of each variable. an equivalent expression is the correlation matrix has elements ij i i ij j j where i is the deviation of variable xi. the correlation is a normalised form of the covariance so that each element is bounded ij draft march summarising distributions for independent variables xi and xj xi xj the covariance ij is zero. similarly independent variables have zero correlation they are uncorrelated note however that the converse is not generally true two variables can be uncorrelated but dependent. a special case is for when xi and xj are gaussian distributed then independence is equivalent to being uncorrelated see definition and kurtosis. the skewness is a measure of the asymmetry of a distribution px px where is the variance of x with respect to px. a positive skewness means the distribution has a heavy tail to the right. similarly a negative skewness means the distribution has a heavy tail to the left. the kurtosis is a measure of how peaked around the mean a distribution is a distribution with positive kurtosis has more mass around its mean than would a gaussian with the same mean and variance. these are also called super gaussian. similarly a negative kurtosis gaussian distribution has less mass around its mean than the corresponding gaussian. the kurtosis is defined such that a gaussian has zero kurtosis accounts for the term in the definition. definition distribution. for a set of datapoints xn which are states of a random variable x the empirical distribution has probability mass distributed evenly over the datapoints and zero elsewhere. for a discrete variable x the empirical distribution is px n i xn where n is the number of datapoints. for a continuous distribution we have px n xn where is the dirac delta function. the sample mean of the datapoints is given by the and the sample variance is given by the n xn n draft march summarising distributions for vectors the sample mean vector has elements xn i i n and sample covariance matrix has elements i j j ij n definition function. for continuous x we define the dirac delta function which is zero everywhere expect at where there is a spike. and one can view the dirac delta function as an infinitely narrow gaussian lim n the kronecker delta is similarly zero everywhere except for the kronecker delta is equivalent to i we use the expression to denote either the dirac or kronecker delta depending on the context. estimator bias definition estimator. given data x xn from a distribution px we can use the data x to estimate the parameter that was used to generate the data. the estimator is a function of the data which we write for an unbiased estimator px px more generally one can consider any estimating function of data. this is an unbiased estimator of a quantity if figure empirical distribution over a discrete variable with states. the empirical samples consist of n samples at each of states and samples at state where n on normalising this gives a distribution with values over the states. draft march discrete distributions a classical example for estimator bias are those of the mean and variance. let xn n this is an unbiased estimator of the mean since n n on the other hand consider the estimator of the variance n n px n this is biased since a few lines of algebra n n discrete distributions definition distribution. the bernoulli distribution concerns a discrete binary variable x with domx the states are not merely symbolic but real values and px from normalisation it follows that px from this px px the variance is given by varx definition distribution. the categorical distribution generalises the bernoulli distribution to more than two states. for a discrete variable x with symbolic states domx c px c c c the dirichlet is conjugate to the categorical distribution. c definition distribution. the binomial describes the distribution of a discrete two-state variable x with domx where the states are symbolic. the probability that in n bernoulli trials samples k success states will be observed is k k k py k draft march is the binomial coefficient. the mean and variance are k n varx n the beta distribution is the conjugate prior for the binomial distribution. continuous distributions definition distribution. consider a multi-state variable x with domx k with corresponding state probabilities k. we then draw n samples from this distribution. the probability of observing the state times state times state k yk times in the n samples is yi i n! yk! py where n yi. n i varyi n i i n i j j the dirichlet distribution is the conjugate prior for the multinomial distribution. definition distribution. the poisson distribution can be used to model situations in which the expected number of events scales with the length of the interval within which the events can occur. if is the expected number of events per unit interval then the distribution of the number of events x within an interval t is px k t tk k! e k for a unit length interval varx the poisson distribution can be derived as a limiting case of a binomial distribution in which the success probability scales as in the limit n continuous distributions bounded distributions definition distribution. for a variable x the distribution is uniform if px const. over the domain of the variable. definition distribution. for x x px e one can show that for rate varx draft march continuous distributions exponential figure laplace distribution. exponential distribution. the alternative parameterisation b is called the scale. definition distribution. x gam ta tdt x e x is called the shape parameter is the scale parameter and the parameters are related to the mean and variance through s where is the mean of the distribution and s is the standard deviation. the mode is given by for an alternative parameterisation uses the inverse scale gamis b gam x bx definition gamma distribution. invgam x e this has mean for and variance for definition distribution. px b b x x x where the beta function is defined as b draft march and is the gamma function. note that the distribution can be flipped by interchanging x for x which is equivalent to interchanging and continuous distributions the mean is given by varx unbounded distributions definition exponential distribution. px e b for scale b varx univariate gaussian distribution the gaussian distribution is an important distribution in science. it s technical description is given in definition gaussian distribution. px n e where is the mean of the distribution and the variance. this is also called the normal distribution. one can show that the parameters indeed correspond to n for and the gaussian is called the standard normal distribution. definition s t-distribution. px draft march figure gamma distribution with varying for fixed gamma distribution with varying for fixed multivariate distributions figure top datapoints drawn from a gaussian distribution. each vertical line denotes a datapoint at the corresponding x value on the horizontal axis. middle histogram using equally spaced bins of the datapoints. bottom gaussian distribution n from which the in the limit of an infinite amount of datapoints were drawn. data and limitingly small bin size the normalised histogram tends to the gaussian probability density function. where is the mean the degrees of freedom and scales the distribution. the variance is given by varx for for the distribution tends to a gaussian with mean and variance as decreases the tails of the distribution become fatter. the t-distribution can be derived from a scaled mixture pxa b n ba e gamis b d b bae b a d it is conventional to reparameterise using and ab. multivariate distributions definition distribution. the dirichlet distribution is a distribution on probability distributions uq q p where zu zu uq i it is conventional to denote the distribution as dirichlet draft march multivariate gaussian figure dirichlet distribution with parameter displayed on the simplex black denotes low probability and white high probability. the parameter u controls how strongly the mass of the distribution is pushed to the corners of the simplex. setting uq for all q corresponds to a uniform distribution in the binary case q this is equivalent to a beta distribution. the product of two dirichlet distributions is marginal of a dirichlet dirichlet dirichlet dirichlet j dirichlet iui uj p i b the marginal of a single component i is a beta distribution multivariate gaussian the multivariate gaussian plays a central role throughout this book and as such we discuss its properties in some detail. definition gaussian distribution. px n e where is the mean vector of the distribution and the covariance matrix. the inverse covariance is called the precision. one may show n draft march multivariate gaussian figure bivariate gaussian with mean and covariance plotted on the probability density contours for the same vertical axis is the probability density value px. bivariate gaussian. plotted are the unit eigenvectors scaled by the square root of their eigenvalues i. the multivariate gaussian is given in note that det m ddet where m is a d d matrix which explains the dimension independent notation in the normalisation constant of the moment representation uses and to parameterise the gaussian. the alternative canonical representation pxb m c ce xtmxxtb is related to the moment representation via m m btm ce the multivariate gaussian is widely used and it is instructive to understand the geometric picture. this can be obtained by view the distribution in a different co-ordinate system. first we use that every real symmetric matrix d d has an eigen-decomposition e et where ete i and diag d. in the case of a covariance matrix all the eigenvalues i are positive. this means that one can use the transformation y et so that e et yty under this transformation the multivariate gaussian reduces to a product of d univariate zero-mean unit variance gaussians the jacobian of the transformation is a constant. this means that we can view a multivariate gaussian as a shifted scaled and rotated version of an isotropic gaussian in which the centre is given by the mean the rotation by the eigenvectors and the scaling by the square root of the eigenvalues as depicted in isotropic means same under rotation for any isotropic distribution contours of equal probability are spherical around the origin. some useful properties of the gaussian are as follows draft march definition gaussian. for a distribution n defined jointly over two vectors x and y of potentially differing dimensions multivariate gaussian x x y y z with corresponding mean and partitioned covariance xx xy yx yy where yx t xy. the marginal distribution is given by px n x xx and conditional pxy n x xy yy y xx xy yy yx definition of two gaussians. the product of two gaussians is another gaussian with a multiplicative factor n n exp s s where s and the mean and covariance are given by definition transform of a gaussian. for the linear transformation y ax b where x n y a b a y n definition of a gaussian. the differential entropy of a multivariate gaussian px n is hx log det d where d dim x. note that the entropy is independent of the mean draft march multivariate gaussian figure beta distribution. the parameters and can also be witting in terms of the mean and variance leading to an alternative parameterisation see conditioning as system reversal for a joint distribution px y consider the conditional pxy. the statistics of pxy can be obtained using a linear system of the form where x ay n and this reversed noise is uncorrelated with y. to show this we need to make the statistics of x under this linear system match those given by the conditioning operation the mean of the linear system is given by x a y and the covariances by that covariance of y remains unaffected by the system reversal xx a yy at xy a yy from equation we have a xy yy which using in equation gives using equation we similarly obtain xx a yy at xx xy x a y x xy yy yx this means that we can write an explicit linear system of the form equation where the parameters are given in terms of the statistics of the original system. these results are just a restatement of the conditioning results but shows how it may be interpreted as a linear system. this is useful in deriving results in inference with linear dynamical systems. yy y completing the square a useful technique in manipulating gaussians is completing the square. for example the expression e xtaxbtx can be transformed as follows. first we complete the square hence xtax btx xtax btx n e from this one can derive e xtaxbtxdx a a a a a a bta bta bta draft march gaussian propagation let y be linearly related to x through y mx where n and x n x x. then the marginal py x pyxpx is a gaussian py n y m x m xmt multivariate gaussian whitening and centering for a set of data xn with dim xn d we can transform this data to yn with zero mean using centering where the mean m of the data is given by yn xn m m n xn furthermore we can transform to a values zn that have zero mean and unit covariance using whitening where the covariance s of the data is given by zn s m s n m mt y usvt y then an equivalent approach is to compute the svd decomposition of the matrix of centered datapoints z ndiag uty has zero mean and unit covariance see maximum likelihood training given a set of training data x drawn from a gaussian n with unknown mean and covariance how can we find these parameters? assuming the data are drawn i.i.d. the log likelihood is log px l n log det draft march taking the partial derivative with respect to the vector we obtain the vector derivative equating to zero gives that at the optimum of the log likelihood the derivative of l with respect to the matrix requires more work. dependence on the covariance and also parameterise using the inverse covariance it is convenient to isolate the n log m multivariate gaussian optimal l n and therefore optimally xn n optimal using m mt we obtain m n l trace l n equating the derivative to the zero matrix and solving for gives equations and define the maximum likelihood solution mean and covariance for training data x consistent with our previous results in fact these equations simply set the parameters to their sample statistics of the empirical distribution. that is the mean is set to the sample mean of the data and the covariance to the sample covariance. bayesian inference of the mean and variance for simplicity here we deal with the univariate case. assuming i.i.d. data the likelihood is px e for a bayesian treatment we require the posterior of the parameters p px px our aim is to find conjugate priors for the mean and variance. a convenient choice for a prior on the mean is that it is a gaussian centred on p draft march e the posterior is then p e nxn p it is convenient to write this in the form p p multivariate gaussian since equation has quadratic contributions in in the exponent the conditional posterior p is gaussian. to identify this gaussian we multiply out the terms in the exponent to arrive at we encounter a difficulty in attempting to find a conjugate prior for because the term is not a simple expression of for this reason we constrain if we therefore use an inverse gamma distribution we will have a conjugate prior for for a gaussinverse-gamma prior a c draft march b n n xn b a with exp a using the identity we can write a c a p ae c a c n a b p e a c a p p for some fixed hyperparameter defining the constants n xn c n a n b we have a c a c using this expression in equation we obtain e c a p n invgam a b a p p n p n the posterior is also gauss-inverse-gamma with exponential family gauss-gamma distribution it is common to to use a prior on the precision defined as the inverse variance if we then use a gamma prior p gam the posterior will be p gam c a where the gauss-gamma prior distribution p n p n is the conjugate prior for a gaussian with unknown mean and precision the posterior for this prior is a gauss-gamma distribution with parameters gam gam b a a the marginal p is a student s t-distribution. an example of a gauss-gamma priorposterior is given in the maximum likelihood solution is recovered in the limit of a flat prior see the unbiased estimators for the mean and variance are given using the proper prior for the multivariate case the extension of these techniques uses a multivariate gaussian distribution for the conjugate prior on the mean and an inverse wishart distribution for the conjugate prior on the exponential family a theoretically convenient class of distributions are the exponential family which contains many standard distributions including the gaussian gamma poisson dirichlet wishart multinomial markov random field. definition family. for a distribution on a multidimensional variable x or discrete an exponential family model is of the form i i are the parameters tix the test statistics and is the log partition function that ensure normalisation px hxe log hxe x draft march i i exponential family prior posterior figure bayesian approach to inferring the mean and precision variance of a gaussian based a gauss-gamma prior with on n randomly drawn datapoints. gauss-gamma posterior conditional on the data. for comparison the sample mean of the data is and maximum likelihood optimal variance is using the n normalisation. the datapoints were drawn from a gaussian with mean and variance see demogaussbayes.m. one can always transform the parameters to the form in which case the distribution is in canonical form px hxe ttx for example the univariate gaussian can be written e e x log defining x and hx then log note that the parameterisation is not necessarily unique we can for example rescale the functions tix and inversely scale i by the same amount to arrive at an equivalent representation. conjugate priors in principle bayesian learning for the exponential family is straightforward. in canonical form px hxe ttx for a prior with hyperparameters p e t the posterior is p px hxe ttx t e ttx so that the prior equation is conjugate for the exponential family likelihood equation whilst the likelihood is in the exponential family the conjugate prior is not necessarily in the exponential family. draft march the kullback-leibler divergence klqp the kullback-leibler divergence klqp the kullback-leibler divergence klqp measures the difference between distributions q and definition kl divergence for two distributions qx and px klqp qx log where denotes average of the function fx with respect to the distribution rx. the kl divergence is the kl divergence is widely used and it is therefore important to understand why the divergence is positive. to see this consider the following linear bound on the function logx logx x as plotted in the figure on the right. replacing x by pxqx in the above bound px qx log px qx since probabilities are non-negative we can multiply both sides by qx to obtain we now integrate sum in the case of discrete variables both sides. pxdx qxdx px qx qx log px qx log qx px log rearranging gives qx log klqp the kl divergence is zero if and only if the two distributions are exactly the same. entropy for both discrete and continuous variables the entropy is defined as hp for continuous variables this is also called the differential entropy see also the entropy is a measure of the uncertainty in a distribution. one way to see this is that hp klpu const. where u is a uniform distribution. since the klpu the less like a uniform distribution p is the smaller will be the entropy. or vice versa the more similar p is to a uniform distribution the greater will be the entropy. since the uniform distribution contains the least information a prior about which state px is in the entropy is therefore a measure of the a priori uncertainty in the state occupancy. for a discrete distribution we can permute the state labels without changing the entropy. for a discrete distribution the entropy is positive whereas the differential entropy can be negative. draft march code demogaussbayes.m bayesian fitting of a univariate gaussian loggaussgamma.m plotting routine for a gauss-gamma distribution exercises exercises exercise in a public lecture the following phrase was uttered by a professor of experimental psychology in a recent data survey of people claim to have above average intelligence which is clearly nonsense! laughs. is it theoretically possible for of people to have above average intelligence? if so give an example otherwise explain why not. what about above median intelligence? exercise consider the distribution defined on real variables x y px y domx domy show that furthermore show that x and y are uncorrelated whilst x and y are uncorrelated show that they are nevertheless dependent. exercise for a variable x with domx and px show that in n independent draws xn from this distribution the probability of observing k states is the binomial distribution k i k k e dx e i dx by considering exercise constant of a gaussian. the normalisation constant of a gaussian distribution is related to the integral e dy e dxdy and transforming to polar coordinates show that i e dx exercise for a univariate gaussian distribution show that n dirichlet b k b j exercise for a beta distribution show that exercise show that the marginal of a dirichlet distribution is another dirichlet distribution and using x derive an explicit expression for the kth moment of a beta distribution. draft march exercises exercise define the moment generating function as px gt show that px lim t dk dtk gt exercise of variables. consider a one dimensional continuous random variable x with corresponding px. for a variable y fx where fx is a monotonic function show that the distribution of y is df dx py px x f exercise of a multivariate gaussian. consider i e by using the transformation show that z i a b m c d exercise consider the partitioned matrix for which we wish to find the inverse m we assume that a is m m and invertible and d is n n and invertible. by definition the partitioned inverse im in where in the above im is the m m identity matrix of the same dimension as a and the zero matrix of the same dimension as d. using the above derive the results p q p q r s m a b c d r s must satisfy p bd q a ca r d bd s ca the skewness and kurtosis are both exercise show that for gaussian distribution px n zero. exercise consider a small interval of time t and let the probability of an event occurring in this small interval be t. derive a distribution that expresses the probability of at least one event in an interval from to t. draft march exercise consider a vector variable x xn and set of functions defined on each component of x ixi. for example for x we might have exercises consider the distribution z e t px e i ixidxi where is a vector function with ith component ixi and is a parameter vector. each component is tractably integrable in the sense that can be computed either analytically or to an acceptable numerical accuracy. show that xi xj the normalisation constant z can be tractably computed. consider the transformation x my for an invertible matrix m. show that the distribution pym is tractable normalisation constant is known and that in general explain the significance of this is deriving tractable multivariate distributions. exercise show that we may reparameterise the beta distribution by writing the parameters and as functions of the mean m and variance s using m s exercise consider the function f show that lim f log and hence that log d lim using this result show therefore that f log b where b is the beta function. show additionally that log b using the fact that b f where is the gamma function relate the above averages to the digamma function defined as d dx log draft march exercises exercise using a similar generating function approach as in explain how to compute exercise consider the function ui i fx i x e sxfxdx is show that the laplace transform of fx fs d d n i s i ui e i d i fs i ui s by taking the inverse laplace transform show that i ui i ui x fx i ui hence show that the normalisation constant of a dirichlet distribution with parameters u is given by exercise by using the laplace transform as in show that the marginal of a dirichlet distribution is a dirichlet distribution. exercise derive the formula for the differential entropy of a multi-variate gaussian. exercise show that for a gamma distribution gam the mode is given by x provided that exercise consider a distribution px and a distribution with changed by a small amount take the taylor expansion of klpx for small and show that this is equal to log px p more generally for a distribution parameterised by a vector i i show that a small change in the parameter results in ij i j fij where the fisher information matrix is defined as fij i j log px p log px show that the fisher information matrix is positive definite by expressing it equivalently as fij log px j i draft march p exercise consider the joint prior distribution p n n n xn n gam n show that for then the prior distribution becomes flat of and for show that for these settings the mean and variance that jointly maximise the posterior equation are given by the standard maximum likelihood settings exercises exercise show that in the limit the jointly optimal mean and variance obtained from argmax is given by n p xn n n n where note that these correspond to the standard unbiased estimators of the mean and variance. exercise for the gauss-gamma posterior p given in equation compute the marginal posterior p what is the mean of this distribution? exercise derive equation exercise consider the multivariate gaussian distribution px n on the vector x with components xn px e and pyix n calculate xi xn. exercise observations yn are noisy i.i.d. measurements of an underlying variable x with px n with mean x for i n show that yn is gaussian n n y where y yn and variance n such that n n exercise consider a set of data xn drawn from a gaussian with know mean and unknown variance assume a gamma distribution prior on p gamis b show that the posterior distribution is n b p gamis draft march exercises show that the distribution for x is pxx px student x a b exercise the poisson distribution is a discrete distribution on the non-negative integers with p e x x! x you are given a sample of n observations xn drawn from this distribution. determine the maximum likelihood estimator of the poisson parameter exercise for a gaussian mixture model pin i i pi i show that px has mean i px and i pi i pi i i t i i i j pi i pj t j pi exercise show that for the whitened data matrix given in equation zzt ni. exercise consider a uniform distribution pi defined on states i n. show that the entropy of this distribution is h pi log pi log n and that there for as the number of states n increases to infinity the entropy diverges to infinity. exercise consider a continuous distribution px x we can form a discrete approximation with probabilities pi to this continuous distribution by identifying a continuous value in for each state i n. with this pi pin i pin show that the entropy h i pin pin log pin i pi log pi is given by i i pin since for a continuous distribution pxdx h n pin draft march a discrete approximation of this integral into bins of size gives hence show that for large n h px log pxdx const. exercises where the constant tends to infinity as n note that this result says that as a continuous distribution has essentially an infinite number of states the amount of uncertainty in the distribution is infinite we would need an infinite number of bits to specify a continuous value. this motivates the definition of the differential entropy which neglects the infinite constant of the limiting case of the discrete entropy. exercise consider two multivariate gaussians n and n show that the log product of the two gaussians is given by t t log det det a a defining a and b log det det writing a and a show that the product of gaussians is a gaussian with covariance t bta we can write the above as t mean and log prefactor bta t t show that this can be written as n n exp exercise show that px log det det s s log det draft march chapter learning as inference learning as inference in previous chapters we largely assumed that all distributions are fully specified for the inference tasks. in machine learning and related fields however the distributions need to be learned on the basis of data. learning is then the problem of integrating data with domain knowledge of the model environment. definition and posteriors. priors and posteriors typically refer to the parameter distributions before to and after to seeing the data. formally bayes rule relates these via p pv pv where is the parameter of interest and v represents the observed data. learning the bias of a coin consider data expressing the results of tossing a coin. we write vn if on toss n the coin comes up heads and vn if it is tails. our aim is to estimate the probability that the coin will be a head pvn called the bias of the coin. for a fair coin the variables in this environment are vn and and we require a model of the probabilistic interaction of the variables vn assuming there is no dependence between the observed tosses except through we have the belief network vn p pvn which is depicted in the assumption that each observation is identically and independently distributed is called the i.i.d. assumption. learning refers to using the observations vn to infer in this context our interest is p vn vn vn vn vn we still need to fully specify the prior p to avoid complexities resulting from continuous variables we ll consider a discrete with only three possible states specifically we assume p p p vn vn n learning as inference figure belief network for coin tossing model. plate notation equivalent of a plate replicates the quantities inside the plate a number of times as specified in the plate. as shown in this prior expresses that we have belief that the coin is fair belief the coin is biased to land heads and belief the coin is biased to land tails the distribution of given the data and our beliefs is pvn p p vn p p i is the number of occurrences of heads which we more conveniently denote in the as nh. i is the number of tails nt hence p vn p nh for an experiment with nh nt the posterior distribution is p k k p k k p k k where v is shorthand for vn from the normalisation requirement we have so that p p p as shown in these are the posterior parameter beliefs. in this case if we were asked to choose a single a posteriori most likely value for it would be although our confidence in this is low since the posterior belief that is also appreciable. this result is intuitive since even though we observed more tails than heads our prior belief was that it was more likely the coin is fair. repeating the above with nh nt the posterior changes to p p so that the posterior belief in dominates. this is reasonable since in this situation there are so many more tails than heads that this is unlikely to occur from a fair coin. even though we a priori thought that the coin was fair a posteriori we have enough evidence to change our minds. p figure prior encoding our beliefs about the amount the coin is biased posterior having seen to heads. posterior having heads and tails. seen heads and tails. draft march learning as inference making decisions in itself the bayesian posterior merely represents our beliefs and says nothing about how best to summarise these beliefs. in situations in which decisions need to be taken under uncertainty we need to additionally specify what the utility of any decision is as in in the coin tossing scenario where is assumed to be either or we setup a decision problem as follows if we correctly state the bias of the coin we gain points being incorrect however loses points. we can write this using u where is the true value for the bias. the expected utility of the decision that the coin is is u u u u plugging in the numbers from equation we obtain u similarly u and u so that the best decision is to say that the coin is unbiased repeating the above calculations for nh nt we arrive at u u u so that the best decision in this case is to choose as more information about the distribution pv becomes available the posterior p becomes increasingly peaked aiding our decision making process. a continuum of parameters in we considered only three possible values for here we discuss a continuum of parameters. using a flat prior we first examine the case of a flat or uniform prior p k for some constant k. for continuous variables normalisation requires p since p k draft march learning as inference figure posterior p assuming a flat prior on nh nt and nh nt in both cases the most probable state of the posterior is which makes intuitive sense since the fraction of heads to tails in both cases is where there is more data the posterior is more certain and sharpens around the most probable value. the maximum a posteriori setting is in both cases this being the value of for which the posterior attains its highest value. repeating the previous calculations with this flat continuous prior we have c nh p c where c is a constant to be determined by normalisation nh d bnh nt where b is the beta function. definition if the posterior is of the same parametric form as the prior then we call the prior the conjugate distribution for the likelihood distribution. using a conjugate prior determining the normalisation constant of a continuous distribution requires that the integral of the unnormalised posterior can be carried out. for the coin tossing case it is clear that if the prior is of the form of a beta distribution then the posterior will be of the same parametric form p b the posterior is p nh so that p b nh nt b nh nt the prior and posterior are of the same form beta distributions but simply with different parameters. hence the beta distribution is conjugate to the binomial distribution. decisions based on continuous intervals the result of a coin tossing experiment is nh heads and nt tails. you now need to make a decision you win dollars if your guess that the coin is more likely to come up heads than tails is correct. if your guess is incorrect you lose a million dollars. what is your decision? an uninformative prior. we need two quantities for our guess and for the truth. then the utility of saying heads is u u draft march maximum a posteriori and maximum likelihood in the above p p b nh nt nh nt d where ixa b is the regularised incomplete beta function. for the former case of nh nt under a flat prior p nt since the events are exclusive p hence the expected utility of saying heads is more likely is similarly the utility of saying tails is more likely is so we are better off taking the decision that the coin is more likely to come up tails. if we modify the above so that we lose million dollars if we guess tails when in fact it as heads the expected utility of saying tails would be in which case we would be better of saying heads. in this case even though we are more confident that the coin is likely to come up tails we would pay such a penalty of making a mistake in saying tails that it is fact better to say heads. maximum a posteriori and maximum likelihood summarising the posterior definition likelihood and maximum a posteriori. maximum likelihood sets parameter given data v using m l argmax pv maximum a posteriori uses that setting that maximises the posterior distribution of the parameter m ap argmax pv where p is the prior distribution. m in making such an approximation potentially useful information concerning the reliability a crude summary of the posterior is given by a distribution with all its mass in a single most likely state of the parameter estimate is lost. possibilities and their associated credibilities. in contrast the full posterior reflects our beliefs about the range of one can motivate map from a decision theoretic perspective. if we assume a utility that is zero for all but the correct u true i true draft march a an s sn cn n n c a s c maximum a posteriori and maximum likelihood figure a model for the relationship between lung cancer asbestos exposure and smoking. plate notation replicating the observed n datapoints and placing priors over the cpts tied across all datapoints. then the expected utility of is u true i true p truev p this means that the maximum utility decision is to return that with the highest posterior value. when a flat prior p const. is used the map parameter assignment is equivalent to the maximum likelihood setting m l argmax pv the term maximum likelihood refers to the parameter for which the observed data is most likely to be generated by the model. since the logarithm is a strictly increasing function then for a positive function f opt argmax f opt argmax log f so that the map parameters can be found either by optimising the map objective or equivalently its logarithm log p log pv log p log pv where the normalisation constant pv is not a function of the log likelihood is convenient since under the i.i.d. assumption it is a summation of data terms log p n log pvn log p log pv so that quantities such as derivatives of the log-likelihood w.r.t. are straightforward to compute. example in the coin-tossing experiment of the ml setting is in both nh nt and nh nt maximum likelihood and the empirical distribution given a dataset of discrete variables x we define the empirical distribution as i xn draft march qx n maximum a posteriori and maximum likelihood a s c figure a database containing information about the asbestos exposure signifies exposure being a smoker signifies the individual is a smoker and lung cancer signifies the individual has lung cancer. each row contains the information for an individual so that there are individuals in the database. in the case that x is a vector of variables i xn i xn i i the kullback-leibler divergence between the empirical distribution qx and a distribution px is klqp our interest is the functional dependence of klqp on p. since the entropic term is independent of px we may consider this constant and focus on the second term alone. hence n log pxn const. klqp const. we log pxn as the log likelihood under the model px assuming that the data is i.i.d. this means that setting parameters by maximum likelihood is equivalent to setting parameters by minimising the kullback-leibler divergence between the empirical distribution and the parameterised distribution. in the case that px is unconstrained the optimal choice is to set px qx namely the maximum likelihood optimal distribution corresponds to the empirical distribution. maximum likelihood training of belief networks consider the following model of the relationship between exposure to asbestos being a smoker and the incidence of lung cancer pa s c pca spaps which is depicted in each variable is binary doma doms domc we assume that there is no direct relationship between smoking and exposure to asbestos. this is the kind of assumption that we may be able to elicit from medical experts. furthermore we assume that we have a list of patient records where each row represents a patient s data. to learn the table entries pca s we can do so by counting the number of c is in state for each of the parental states of a and s pc s pc s pc s pc s similarly based on counting pa and ps these three cpts then complete the full distribution specification. setting the cpt entries in this way by counting the relative number of occurrences corresponds mathematically to maximum likelihood learning under the i.i.d. assumption as we show below. maximum likelihood corresponds to counting for a bn there is a constraint on the form of px namely px pxipa draft march maximum a posteriori and maximum likelihood to compute the maximum likelihood setting of each term pxipa as shown in we can equivalently minimise the kullback-leibler divergence between the empirical distribution qx and px. for the bn px and empirical distribution qx we have log p const. qx klqp p const. this follows using the general result which says that if the function f only depends on a subset of the variables we only need to know the marginal distribution of this subset of variables in order to carry out the average. since qx is fixed we can add on entropic terms in q and equivalently mimimize klqp qxipa p the final line is a positive weighted sum of individual kullback-leibler divergences. the minimal kullbackleibler setting and that which corresponds to maximum likelihood is therefore in terms of the original data this is pxipa qxipa pxi spa t i s i xj paxi j this expression corresponds to the intuition that the table entry pxipa can be set by counting the number of times the state s pa t occurs in the dataset t is a vector of parental states. the table is then given by the relative number of counts of being in state s compared to the other states for fixed joint parental state t. an alternative method to derive this intuitive result is to use lagrange multipliers see for reader less comfortable with the above kullback-leibler derivation a more direct example is given below which makes use of the notation to denote the number of times that states occur together in the training data. example we wish to learn the table entries of the distribution we address here how to find the cpt entry using maximum likelihood. for i.i.d. data the contribution from to the log likelihood is n log pxn xn the number of times occurs in the log likelihood is the number of such occurrences in the training set. since the normalisation constraint draft march maximum a posteriori and maximum likelihood xn xn y figure a variable y with a large number of parents xn requires the specification of an exponentially large number of entries in the conditional probability xn. one solution to this difficulty is to parameterise the conditional xn the total contribution of to the log likelihood is log log using we have log log differentiating the above expression w.r.t. and equating to zero gives the solution for optimal is then corresponding to the intuitive counting procedure. conditional probability functions consider a binary variable y with n binary parental variables x xn. there are entries in the cpt of pyx so that it is infeasible to explicitly store these entries for even moderate values of n. to reduce the complexity of this cpt we may constrain the form of the table. for example one could use a function py w e wtx where we only need to specify the n-dimensional parameter vector w. in this case rather than using maximum likelihood to learn the entries of the cpts directly we instead learn the value of the parameter w. since the number of parameters in w is small compared with in the unconstrained case we also have some hope that with a small number of training examples we can learn a reliable value for w. example consider the following variable model where xi i we assume that the cpt is parameterised using e one may verify that the above probability is always positive and lies between and due to normalisation we must have draft march bayesian belief network training for unrestricted and the maximum likelihood setting is and the contribution to the log likelihood from the term assuming i.i.d. data is i i xn log e xn this objective function needs to be optimised numerically to find the best and the gradient is l dl d dl d xn xn xn e xn e xn xn the gradient can be used as part of a standard optimisation procedure as conjugate gradients see appendix to aid finding the maximum likelihood parameters bayesian belief network training an alternative to maximum likelihood training of a bn is to use a bayesian approach in which we maintain a distribution over parameters. we continue with the asbestos smoking cancer scenario pa c s pca spaps which can be represented as a belief network so far we ve only specified the independence structure but not the entries of the tables pca s pa ps. given a set of visible observations v sn cn n n we would like to learn appropriate distributions for the table entries. to begin we need a notation for the table entries. with all variables binary we have parameters such as pa a a pc s c c and similarly for the remaining parameters c c c a s c c c c c for our example the parameters are global and local parameter independence in bayesian learning of bns we need to specify a prior on the joint table entries. since in general dealing with multi-dimensional continuous distributions is computationally problematic it is useful to specify only uni-variate distributions in the prior. as we show below this has a pleasing consequence that for i.i.d. data the posterior also factorises into uni-variate distributions. global parameter independence a convenient assumption is that the prior factorises over parameters. for our asbestos smoking cancer example we assume assuming the data is i.i.d. we then have the joint model p a s c p ap sp c p a s cv p ap sp n pan apsn spcnsn an c draft march bayesian belief network training a an s sn cn n n as c s p figure a bayesian parameter model for the relationship between lung cancer asbestos exposure and smoking with factorised parameter priors. the global parameter independence assumption means that the prior over tables factorises into priors over each conditional probability table. the local independence assumption which in this case comes into efc where fect only for pca s means that p c factorises as p p as p the belief network for which is given in learning then corresponds to inference of a convenience of the factorised prior for a bn is that the posterior also factorises since pv a s cp ap sp c pv p a s cv pv a s cp a s c p p a s cv p a s cv p pan a p avap svsp cv n pv p psn s n n pcnsn an c so that one can consider each parameter posterior separately. in this case learning involves computing the posterior distributions p ivi where vi is the set of training data restricted to the family of variable i. the global independence assumption conveniently results in a posterior distribution that factorises over the conditional tables. however the parameter c is itself dimensional. to simplify this we need to make a further assumption as to the structure of each local table. local parameter independence if we further assume that the prior for the table factorises over all states a c p c p c c c c then the posterior p cv pv cp c c c c p c c c p c p c c p c p c c p c c p p c so that the posterior also factorises over the parental states of the local conditional table. posterior marginal table a marginal probability table is given by for example pc s draft march c pc s c cv bayesian belief network training the integral over all the other tables in equation is unity and we are left with pc s pc s c c c learning binary variable tables using a beta prior we continue the example of where all variables are binary but using a continuous valued table prior. the simplest case is to start with pa a since this requires only a univariate prior distribution p a. the likelihood depends on the table variable via pa a a so that the total likelihood term is a the posterior is therefore p ava p a a a then conjugacy will hold and the mathematics this means that if the prior is also of the form of integration will be straightforward. this suggests that the most convenient choice is a beta distribution a p a b a a a b a a a a a a for which the posterior is also a beta distribution p ava b a a a the marginal table is given by pa a p ava a a a a using the result for the mean of a beta distribution the situation for the table pca s is slightly more complex since we need to specify a prior for each of the parental tables. as above this is most convenient if we specify a beta prior one for each of the parental states. let s look at a specific table pc s assuming the local independence property we have p ca s a s ca s a s given by c c as before the marginal probability table is then given by pc s ca s a s ca s ca s s since s a s a s the prior parameters ca s are called hyperparameters. if one had no preference one could set all of the ca s to be equal to the same value and similarly for a complete ignorance prior would correspond to setting see draft march bayesian belief network training no data limit n in the limit of no data the marginal probability table corresponds to the prior which is given in this case by pc s ca s ca s ca s for a flat prior for all states a c this would give a prior probability of pc s infinite data limit n in this limit the marginal probability tables are dominated by the data counts since these will typically grow in proportion to the size of the dataset. this means that in the infinite very large data limit pc s a s a s a s which corresponds to the maximum likelihood solution. this effect that the large data limit of a bayesian procedure corresponds to the maximum likelihood solution is general unless the prior has a pathologically strong effect. example consider the binary variable network pc a s pca spaps the data v is given in using a flat beta prior for all conditional probability tables the marginal posterior tables are given by pa n by comparison the maximum likelihood setting is the bayesian result is a little more cautious than the maximum likelihood which squares with our prior belief that any setting of the probability is equally likely pulling the posterior towards similarly ps n and pc s a s a s a s pc s pc s pc s a s a s a s a s a s a s a s a s a s draft march bayesian belief network training learning multivariate discrete tables using a dirichlet prior the natural generalisation to more than two-state variables is given by using a dirichlet prior again assuming i.i.d. data and the local and global parameter prior independencies. since under the global parameter independence assumption the posterior factorises over variables in equation we can concentrate on the posterior of a single variable. no parents let s consider the contribution of a variable v with domv i. the contribution to the posterior from a datapoint vn is ivni i i pvn p so that the posterior is proportional to i ivni ivni i p for a dirichlet prior distribution with hyperparameters u p ui i using this prior the posterior becomes i ui i ivni ui ivni i p which means that the posterior is given by p dirichlet c where c is a count vector with components ci i i being the number of times state i was observed in the training data. the marginal table is given by integrating pv iv pv i ip iv i pv iv ui ci j uj cj since the single-variable marginal distribution of a dirichlet is a beta distribution the marginal table is the mean of a beta distribution. given that the marginal p is beta distribution with parameters ui ci uj cj the marginal table is given by which generalises the binary state formula equation draft march bayesian belief network training parents to deal with the general case of a variable v with parents pa we denote the probability of v being in state i conditioned on the parents being in state j as pv ipa j iv j the number of states j will be exponential in k. i iv j this forms the components of a vector j. note that if v has k parents then local independence means p j p p j p and global independence means v where v v represents the combined table of all the variables. we drop the explicit sans-serif font on the states from here on in. parameter posterior thanks to the global parameter independence the posterior distribution over the tables factorises with one posterior table per variable. each posterior table for a variable v depends only on the information local to the family of each variable dv. assuming a dirichlet distribution prior p j dirichlet juv j the posterior is proportional to the joint distribution p p zuv j i zuv j i j j iv juivj iv jivnipavnj n j i iv juivj where zu is the normalisation constant of a dirichlet distribution. hence the posterior is p j where the hyperparameter prior term is updated by the observed counts iv j uiv j i pa j u by analogy with the no-parents case the marginal table is given by the states explicitly iv j pv ipa jdv u draft march bayesian belief network training a s c figure a database of patient records about the asbestos exposure signifies exposure being a smoker signifies the individual is a smoker and lung cancer signifies no cancer signifies early stage cancer signifies late state cancer. each row contains the information for an individual so that there are individuals in the database. example consider the pca spspa asbestos example with doma doms except now with the variable c taking three states domc accounting for different kinds of cancer. the marginal table under a dirichlet prior is then given by pc s s a s i uia s i a s assuming a flat dirichlet prior which corresponds to setting all components of u to this gives pc s pc s pc s and similarly for the other three tables pca s pca s pca s model likelihood for a belief network m the joint probability of all variables factorises into the local probabilities of each variable conditioned on its parents pvm pdm v for i.i.d. data d the likelihood under the network m is pvpa m pvnpa m v n v j j zuv j where u are the dirichlet hyperparameters and is given by equation expression can be written explicitly in terms of gamma functions see in the above expression in general the number of parental states differs for each variable v so that implicit in the above formula is that the state product over j goes from to the number of parental states of variable v. due to the local and global parameter independence assumptions the logarithm of the model likelihood splits into terms one for each variable v and parental configuration. this is called the likelihood decomposable property. structure learning up to this point we have assumed that we are given both the structure of the distribution and a dataset d. a more complex task is when we need to learn the structure of the network as well. we ll consider the case in which the data is complete there are no missing observations. since for d variables there is an exponentially large number d of bn structures it s clear that we cannot search over all possible structures. for this reason structure learning is a computationally challenging problem and we must rely on constraints and heuristics to help guide the search. furthermore for all but the sparsest draft march bayesian belief network training algorithm pc algorithm for skeleton learning. start with a complete undirected graph g on the set v of all vertices. i repeat for x v do for y adj do determine if there a subset s of size i of the neighbours of x including y for which x ys. if this set exists remove the x y link from the graph g and set sxy s. end for end for i i until all nodes have i neighbours. networks estimating the dependencies to any accuracy requires a large amount of data making testing of dependencies difficult. indeed for a finite amount of data two variables will always have non-zero mutual information so that a threshold needs to be set to decide if the measured dependence is significant under the finite sample see other complexities arise from the concern that a belief or markov network on the visible variables alone may not be a parsimonious way to represent the observed data if for example there may be latent variables which are driving the observed dependencies. for these reasons we will not discuss this topic in detail here and limit the discussion to two central approaches. a special case that is computationally tractable is when the network is constrained to have at most one parent. we defer discussion of this to pc algorithm the pc first learns the skeleton of a graph after which edges may be oriented to form a oriented dag. the pc algorithm begins at the first round with a complete skeleton g and attempts to remove as many links as possible. at the first step we test all pairs x y if an x and y pair are deemed independent then the link x y is removed from the complete graph. one repeats this for all the pairwise links. in the second round for the remaining graph one examines each x y link and conditions on a single neighbour z of x. if x y z then remove the link x y. one repeats in this way through all the variables. at each round the number of neighbours in the conditioning set is increased by one. see and demopcoracle.m. a refinement of this algorithm known as npc for necessary path attempts to limit the number of independence checks which may otherwise result in inconsistencies due to the empirical estimates of conditional mutual information. given a learned skeleton a partial dag can be constructed using note that this is necessary since the undirected graph g is a skeleton not a belief network of the independence assumptions discovered. for example we may have a graph g with x z y in which the x y link was removed on the basis x y sxy as a mn the graph x z y implies although this is inconsistent with the discovery in the first round x y. this is the reason for the orientation part for consistency we must have x z y for which x y and z. note that in we have for the unmarried collider test z which in this case is true resulting in a collider forming. see also example orienting. z x y x y z x y if x is independent of y it must be that z is a collider since otherwise marginalising over z would introduce a dependence between x and y. example appears in and thanks also to seraf n moral for his online notes. draft march bayesian belief network training x z x z t t y x y x y x y x y x y x y x y x w z w z w z w z w z w z w z w z t t t t t t t y x y x y x y x y x y x y x y x w z w z w z w z w z w z w z w z t t t t t t t y w y w t t the bn from which data is assumed generated and against which figure pc algorithm. conditional independence tests will be performed. the initial skeleton is fully connected. in the first round all the pairwise mutual informations x y are checked and the link between x i we now look at connected subsets on and y removed if deemed independent line. the three variables x y z of the remaining graph removing the link x y if x y z is true. not all steps i we now examine all x ya b. the algorithm terminates after this round are shown. final skeleton. i gets incremented to since there are no nodes with or more neighbours. during this process the sets sxy sxw szw y sxt w syt w were found. see also demopcoracle.m z x y x y z z x y if x is independent of y conditioned on z z must not be a collider. any other orientation is appropriate. empirical independence given a data set d containing variables x y z our interest is to measure if x y z. one approach is to use the conditional mutual information which is the average of conditional kullback-leibler divergences. definition information. mix yz where this expression is equally valid for sets of variables. if x y z is true then mix yz is zero and vice versa. when z the average over pz is absent and one writes mix y. given data we can obtain an estimate of the conditional mutual information by using the empirical distribution px y z estimated by simply counting occurrences in the data. in practice however we only have a finite amount of data to estimate the empirical distribution so that for data sampled from distribution for which the variables truly are independent the empirical mutual information will typically be greater than zero. an issue therefore is what threshold to use for the empirical conditional mutual information to decide if this is sufficiently far from zero to be caused by dependence. a frequentist approach is to compute the distribution of the conditional mutual information and then see where the sample value is compared to the distribution. according to yz is chi-square distributed draft march bayesian belief network training algorithm skeleton orientation algorithm a dag. unmarried collider examine all undirected links x z y. if z sxy set x z y. repeat until no more edges can be oriented. the remaining edges can be arbitrarily oriented provided that the graph remains a dag and no additional x z y x z y for x y if there is a directed path from x to y orient x y if for x z y there is a w such that x w y w z w then orient z w colliders are introduced. with degrees of freedom although this test does not work well in the case of small amounts of data. an alternative pragmatic approach is to estimate the threshold based on empirical samples of the mi under controlled independentdependent conditions see democondindepemp.m for a comparison of these approaches. bayesian conditional independence test a bayesian approach to testing for independence can be made by comparing the likelihood of the data under the independence hypothesis versus the likelihood under the dependent hypothesis. for the independence hypothesis we have a joint distribution over variables and parameters px y z pxz xzpyz yzpz zp xzp yzp z for categorical distributions it is convenient to use a prior dirichlet on the parameters assuming also local as well as global parameter independence. for a set of assumed i.i.d. data yn zn n n the likelihood is then given by integratingover the parameters px zuz zuz zuxz z zuyz z zuxz z zuyz where uxz is a hyperparameter matrix of pseudo counts for each state of x given each state of z. zv is the normalisation constant of a dirichlet distribution with vector parameter v. for the dependent hypothesis we have px y z px y z xyzp xyz the likelihood is then px zuxyz y z zuxyz x z y x y x y x w z w z w z y w t t t t figure skeleton orientation algorithm. the skeleton along with sxy sxw szw z sxy y sxt w syt w. t szw so form collider. so form collider. final partially oriented dag. the remaining edge may be oriented as desired without violating the dag condition. see also demopcoracle.m. draft march px pxn yn zn thanks to conjugacy this is straightforward and gives the expression n bayesian belief network training z zn xn yn n xz yz xyz xn yn zn n figure bayesian conditional independence test a using dirichlet priors on the tables. model hindep for conditional independence x y a model hdep for conditional depenz. dence z. by computing the likelihood of the data under each model a numerical score for the whether the data is more consistent with the conditional independence assumption can be formed. see democondindepemp.m. figure conditional independence test of x y with x y having states respectively. from the oracle belief network shown in each experiment the tables are drawn at random and examples are sampled to form a dataset. for each dataset a test is carried out to determine if x and y are independent conditioned on correct answer being that they are independent. over experiments the bayesian conditional independence test correctly states that the variables are conditionally independent of the time compared with only accuracy using the chi-square mutual information test. see democondindepemp.m. assuming each hypothesis is equally likely for a bayes factor px px greater than we assume that conditional independence holds otherwise we assume the variables are conditionally dependent. democondindepemp.m suggests that the bayesisan hypothesis test tends to outperform the conditional mutual information approach particularly in the small sample size case see network scoring an alternative to local methods such as the pc algorithm is to evaluate the whole network structure. in a probabilistic context given a model structure m we wish to compute pmd pdmpm. some care is needed here since we have to first fit each model with parameters pv m to the data d. if we do this using maximum likelihood alone with no constraints on we will always end up favouring that model m with the most complex structure pm const.. this can be remedied by using the bayesian technique pdm pd mp in the case of directed networks however as we saw in the assumptions of local and global parameter independence make the integrals tractable. for a discrete state network and dirichlet priors we have pdm given explicitly by the bayesian dirichlet score equation first we specify the hyperparameters uv j and then search over structures m to find the one with the best score pdm. the simplest setting for the hyperparameters is set them all to another setting is the uninformative prior uiv j dim v dim pa draft march maximum likelihood for undirected models the correct structure in which all figure learning the structure of a bayesian network. variables are binary. the ancestral order is the dataset is formed from samples the learned structure based on the pc algorithm using the bayesian empirical from this network. conditional independence test. undirected edges may be oriented arbitrarily. the learned structure based on the bayes dirichlet network scoring method. see demopcdata.m and demobdscore.m. where dim x is the number of states of the variables x giving rise to the bdeu score for an equivalent sample size parameter a discussion of these settings is given in under the concept of likelihood equivalence namely that two networks which are markov equivalent should have the same score. how dense the resulting network is can be sensitive to including an explicit prior pm on the networks to favour those with sparse connections is also a sensible idea for which one modified the score to pdmpm. searching over structures is a computationally demanding task. however since the log-score decomposes into terms involving each family of v we can compare two networks differing in a single arc efficiently. search heuristics based on local additionremovalreversal of links that increase the score are in learnbayesnet.m we simplify the problem for demonstration purposes in which we assume we know the ancestral order of the variables and also the maximal number of parents of each variable. example algorithm versus network scoring. in we compare the pc algorithm with bd network scoring based dirichlet hyperparameters set to unity on samples from a known belief network. the pc algorithm conditional independence test is based on the bayesian factor in which dirichlet priors with were used throughout. in the network scoring technique outperforms the pc algorithm. this is partly explained by the network scoring technique being provided with the correct ancestral order and the constraint that each variable has maximally two parents. maximum likelihood for undirected models consider a markov network distribution px defined on necessarily maximal cliques xc x px z cxc c c where z x c cxc c draft march maximum likelihood for undirected models ensures normalisation. given a set of data x n n n and assuming i.i.d. data the log likelihood is log cx n c c n log z l n c in general learning the optimal parameters c c c is awkward since they are coupled via z unlike the bn the objective function does not split into a set of isolated parameter terms and in general we need to resort to numerical methods. in special cases however exact results still apply in particular when the mn is decomposable and no constraints are placed on the form of the clique potentials as we discuss in more generally however gradient based techniques may be used and also give insight into properties of the maximum likelihood solution. the likelihood gradient l c c log z where we used the result c c n cxc c c n log cx n c z x log cxc c pxc log cxc c c pxc the gradient can then be used as part of a standard numerical optimisation package. exponential form potentials a common form of parameterisation is to use an exponential form cxc e t cxc where are the parameters and cxc is a fixed feature function defined on the variables of clique c. differentiating with respect to and equating to zero we obtain that the maximum likelihood solution satisfies that the empirical average of a feature function matches the average of the feature function with respect to the model n where is the number of times the clique state xc is observed in the dataset. an intuitive interpretation is to sample states x from the trained model px and use these to compute the average of each feature function. in the limit of an infinite number of samples for a maximum likelihood optimal model these sample averages will match those based on the empirical average. unconstrained potentials for unconstrained potentials we have a separate table for each of the states defined on the clique. this means we may write n c c yc cx n where the product is over all states of potential c. this expression follows since the indicator is zero for all but the single observed state x n i x n c log cyc n log z l c the log likelihood is then c yc n draft march maximum likelihood for undirected models figure a decomposable markov network. set chain for formed by choosing clique as root and orienting edges consistently away from the root. each separator is absorbed into its child clique to form the set chain. a junction tree for where c cyc z y i x n c n pyc cyc cyc n n n pyc i x n c differentiating the log likelihood with respect to a specific table entry we obtain equating to zero the maximum likelihood solution is obtained when that is the unconstrained optimal maximum likelihood solution is given by setting the clique potentials such that the marginal distribution on each clique pyc matches the empirical distribution on each clique decomposable markov networks in the case that there is no constraint placed on the form of the factors c and if the mn corresponding to these potentials is decomposable then we know the junction tree representation that we can express the distribution in the form of a product of local marginals divided by the separator distributions px c pxc s pxs px c pxcxs by reabsorbing the separators into the numerator terms we can form a set chain distribution since this is directed the maximum likelihood solution to learning the tables is trivial since we assign each set chain factor pxcxs by counting the instances in the see learnmarkovdecom.m. the procedure is perhaps best explained by an example as given below. see for a general description. example given a dataset v n n n we wish to fit by maximum likelihood a mn of the form z draft march maximum likelihood for undirected models algorithm learning of an unconstrained decomposable markov network using maximum likelihood. we have a triangulated markov network on cliques cxc c c and the empirical marginal distributions on all cliques and separators form a junction tree from the cliques. initialise each clique cxc to and each separator sxs to choose a root clique on the junction tree and orient edges consistently away from this root. for this oriented junction tree divide each clique by its parent separator. return the new potentials on each clique as the maximum likelihood solution. where the potentials are unconstrained tables see since the graph is decomposable we know it admits a factorisation of clique potentials divided by the separators we can convert this to a set chain by reabsorbing the denominators into numerator terms see for example by choosing the clique as rootwe can write where we identified the factors with clique potentials and the normalisation constant z is unity see the advantage is that in this representation the clique potentials are independent since the distribution is a bn on cluster variables. the log likelihood for an i.i.d. dataset x n n is log pxn log pxn xn xn log pxn xn log pxn l n where each of the terms is an independent parameter of the model. the maximum likelihood solution then corresponds for the bn case to simply setting each factor to the datacounts. for example n non-decomposable markov networks in the non-decomposable or constrained case no closed form maximum likelihood solution generally exists and one needs to resort to numerical methods. according to equation the maximum likelihood solution is such that the clique marginals match the empirical marginals. assuming that we can absorb the normalisation constant into an arbitrarily chosen clique we can drop explicitly representing the normalisation constant. for a clique c the requirement that the marginal of p matches the empirical marginal on the variables in the clique is xc given an initial setting for the potentials we can then update to satisfy the above marginal requirement newxc draft march maximum likelihood for undirected models which is required for each of the states of xc. by multiplying and dividing the right hand side by this is equivalent to newxc pxc one can view this ipf update as coordinate-wise optimisation of the log likelihood in which the coordinate corresponds to cxc with all other parameters fixed. in this case this conditional optimum is analytically given by the above setting. one proceeds by selecting another potential to update. note that in general with each update the marginal pxc need to be recomputed. computing these marginals may be expensive unless the width of the junction tree formed from the graph is suitably limited. example machine learning. we define the bm as vtwv pvw zw e for symmetric w and binary variables domvi given a set of training data d e v vtwv the log likelihood is zw lw wvn n log zw differentiating w.r.t. wij i j we have the gradient l wij vn i vn j a simple algorithm to optimise the weight matrix w is to use gradient ascent wnew ij wold ij vn i vn j the second order statistics of the model match those of the empirical for a learning rate the intuitive interpretation is that learning will stop gradient is zero when j bm learning however is difficult since is typically computationally intractable for an arbitrary interaction matrix w and therefore needs to be approximated. indeed one cannot compute the likelihood lw exactly so that monitoring performance is also difficult. n vn i vn constrained decomposable markov networks if there are no constraints on the forms of the maximal clique potentials of the markov network as we ve seen learning is straightforward. here our interest is when the functional form of the maximal clique is constrained to be a product of potentials on smaller cx i i c cxc i with no constraint being placed on the non-maximal clique potentials i in general in this case one cannot write down directly the maximum likelihood solution for the non-maximal clique potentials c. cx i i c. cx i boltzmann machine is of this form since any unconstrained binary pairwise potentials can be converted into a bm. for other cases in which the i c are constrained then iterative scaling may be used in place of ipf. draft march maximum likelihood for undirected models interpreted as the distrifigure represents bution a junction tree for the pair wise mn in we have a choice were to place the pairwise cliques and this is one valid choice using the shorthand ab abxa xb and xab xb. graph represents graph a markov network pairwise mn the the as a consider the graph in in the constrained case in which we interpret the graph as a pairwise mn ipf may be used to learn the pairwise tables. since the graph is decomposable there are however computational savings that can be made in this for an empirical distribution maximum likelihood requires that all the pairwise marginals of the mn match the corresponding marginals obtained from as explained in we have a choice as to which junction tree clique each potential is assigned to with one valid choice being given in keeping the potentials of the cliques and fixed we can update the potentials of clique using a bar to denote fixed potentials we the marginal requirement that the mn matches the empirical marginal can be written in shorthand as which can be expressed as the messages and are the boundary separator tables when we choose the central clique as root and carry out absorption towards the root. given these fixed messages we can then perform updates of the root clique using after making this update we can subsequently update similarly using the constraint new so that new given converged updates for this clique we can choose another clique as root propagate towards the root and compute the separator cliques on the boundary of the root. given these fixed boundary clique potentials we perform ipf within the clique. draft march maximum likelihood for undirected models algorithm efficient iterative proportional fitting. given a set of i i i and a corresponding set of reference marginal distributions on the variables of each potential we aim to set all such that all marginals of the markov network match the given empirical marginals. given a markov network on potentials i i i triangulate the graph and form the cliques choose a clique c as root. propagate messages towards the root and compute the separators on the boundary of the root. repeat assign potentials to cliques. thus each clique has a set of associated potentials fc initialise all potentials example to unity. repeat until all markov network marginals converge to the reference marginals. choose a potential i in clique c i fc. perform an ipf update for i given fixed boundary separators and other potentials in c. until potentials in clique c converge. this efficient ipf procedure is described more generally in for an empirical distribution more generally ipf minimises the kullback-leibler divergence between a given reference distribution and the markov network. see demoipfeff.m and ipf.m. example in examples of binary pixel handwritten twos are presented forming the training set from which we wish to fit a markov network. first all pairwise empirical entropies hxi xj i j were computed and used to rank edges with highest entropy edges ranked first. edges were included in a graph g highest ranked first provided the triangulated g had all cliques less than size this resulted in unique cliques and an adjacency matrix for the triangulated g as presented in in the number of times that a pixel appears in the cliques is shown and indicates the degree of importance of each pixel in distinguishing between the examples. two models were then trained and used to compute the most likely reconstruction based on missing data pxmissingxvisible. the first model was a markov network on the maximal cliques of the graph for which essentially no training is required and the settings for each clique potential can be obtained as explained in the model makes errors in reconstruction of the missing pixels. note that the unfortunate effect of reconstructing a white pixel surrounded by black pixels is an effect of the limited training data. with larger amounts of data the model would recognise that such effects do not occur. in the second model the same maximal cliques were used but the maximal clique potentials restricted to be the product of all pairwise two-cliques within the maximal clique. this is equivalent to using a boltzmann machine and was trained using the efficient ipf approach of the corresponding reconstruction error is this performance is worse than the unconstrained network since the boltzmann machine is a highly constrained markov network. see demolearndecmn.m. figure based on the pairwise empirical entropies hxi xj edges are ordered high entropy edges first. shown is the adjacency matrix of the resulting markov network whose junction tree has cliques in size represents an indicated are the number of edge. cliques that each pixel is a member of indicating a degree of importance. note that the lowest clique membership value is so that each pixel is a member of at least one clique. draft march maximum likelihood for undirected models figure learning digits simon lucas algoval system using a markov network. top row the training examples. each example is a binary image on pixels. second row the training data with missing pixels represents a missing pixel. third row reconstructions from the missing data using a thin-junction-tree mn with maximum clique size bottom row reconstructions using a thin-junction-tree boltzmann machine with maximum clique size trained using efficient ipf. iterative scaling we consider markov networks of the exponential form e cfcvc where fcvc and c ranges of the non-maximal cliques vc v. the normalisation requirement is c e cfcvc pv z z v c a maximum likelihood training algorithm for a markov network somewhat analogous to the em approach of can be derived as consider the bound for positive x log x x log x x hence z z old log z log z old z z old log z z old n l n cn then we can write a bound on the log likelihood cfcv n c log z old z z old as it stands the bound is in general not straightforward to optimise since the parameters of each potential are coupled through the z term. for convenience it is useful to first reparmameterise and write then c c c old c c old z c fcvc c c cfcvc e c pc c v e v e d fdvd e c fcvc old c e c fcvc c one can decouple this using an additional bound derived by first considering draft march c pc we may apply jensen s inequality to give maximum likelihood for undirected models e where pc since pc d fdvd c cfcvc z l hence n v c n d fdvd c pce c c fcv n c c c n plugging this bound into we have c fcvc old e f fdvc pce c d pv old log z old pce c lb c the term in curly brackets contains the potential parameters c in an uncoupled fashion. differentiating with respect to c the gradient of each lower bound is given by lb c c n fcv n c n c fcvce c old d pv old this can be used as part of a gradient based optimisation procedure to learn the parameters c. a potential advantage over ipf is that all the parameters may be updated simultaneously whereas in ipf they must be updated sequentially. intuitively the parameters converge when the empirical average of the functions f match the average of the functions with respect to samples drawn from the distribution in line with our general condition for maximum likelihood optimal solution. c fcvc the zero of the gradient can be found in the special case that the functions sum to analytically giving the update c old c log n n fcv n c log old the constraint that the features fc need to be non-negative can be relaxed at the expense of additional variational parameters see in cases where the zero of the gradient cannot be computed analytically there may be little advantage in general in using is over standard gradient based procedures on the log likelihood directly if the junction tree formed from this exponential form markov network has limited tree width computational savings can be made by performing ipf over the cliques of the junction tree and updating the parameters within each clique using this is a modified version of the constrained decomposable case. see also for a unified treatment of propagation and scaling on junction trees. conditional random fields for an input x and output y a crf is defined by a conditional distribution ky x for potentials ky x.to make learning more straightforward the potentials are usually defined as e kfkyx for fixed functions fy x and parameters k. in this case the distribution of the output conditioned on the input is pyx zx k pyx zx k draft march e kfkyx maximum likelihood for undirected models for an i.i.d. dataset of input-outputs d yn n n training based on conditional maximum likelihood requires the maximisation of k l log pynxn kfkyn xn log zxn in general no closed form solution for the optimal exists and this needs to be determined numerically. first we note that equation is equivalent to equation where the parameters are here denoted by and the variables v are here denoted by y. in the crf case the inputs simply have the effect of determining the feature fky x. in this sense iterative scaling or any related method for maximum likelihood training of constrained markov networks may be readily adapted taking advantage also of any computational savings from limited width junction trees. as an alternative here we briefly describe gradient based training. the gradient has components l n i fiyn xn the terms can be problematic and their tractability depends on the structure of the potentials. for a multivariate y provided the structure of the cliques defined on subsets of y is singlyconnected then computing the average is generally tractable. more generally provided the cliques of the resulting junction tree have limited width then exact marginals are avaiable. an example of this is given for a linear-chain crf in see also below. another quantity often useful for numerical optimisation is the hessian which has components l n i j xnfjy where the averages above are with respect to pyxn this expression is a sum of covariance elements and is therefore negative definite. hence the function l is concave and has only a single global optimum. whilst no closed form solution for the optimal exists the optimal solutions can be found easily using a numerical technique such as conjugate gradients. in practice regularisation terms are often added to prevent overfitting for a discussion of regularisation using a term k k k for positive regularisation constants k discourages the weights from being too large. this term is also negative definite and hence the overall objective function remains concave. iterative scaling may also be used to train a crf though in practice gradient based techniques are to be once trained a crf can be used for predicting the output distribution for a novel input x the most likely output y is equivalently given by argmax argmax kfky x y log zx since the normalisation term is independent of y finding the most likely output is equivalent to y y k log pyx k kfky x y argmax y draft march maximum likelihood for undirected models figure training results for a linear chain crf. there are training sequences one per subpanel. in each the top row corresponds to the input sequence xt state represented by a different colour the middle row the correct output sequence yt state represented by a different colour. together the input and output sequences make the training data d. the bottom row contains the most likely output sequence given the trained crf arg five additional test sequences along with the correct output and predicted output sequence. natural language processing in a natural language processing application xt might represent a word and yt a corresponding linguistic tag noun verb etc. a more suitable form in this case is to constrain the crf to be of the form exp kgkyt yt lhlyt xt k l for binary functions gk and hl and parameters k and l. the grammatical structure of tag-tag transitions is encoded in gkyt yt and linguistic tag information in hkyt xt with the importance of these being determined by the corresponding in this case inference of the marginals is straightforward since the factor graph corresponding to the inference problem is a linear chain. variants of the linear chain crf are used heavily in natural language processing including part-of-speech tagging and machine translation which the input sequence x represents a sentence say in english and the output sequence y the corresponding translation into french. see for example example chain crf. we consider a crf with x input states and y output states of the form k kgkytyt e l lhlytxt here the binary functions gkyt yt i ak i bk k model the transitions between two consecutive outputs. the binary functions hlyt xt i al i cl l model the translation of the input to the output. there are therefore parameters in total. in we plot the training and test results based on a small set of data. the training of the crf is obtained using iterations of gradient ascent with a learning rate of see demolinearcrf.m. draft march properties of maximum likelihood pseudo likelihood consider a mn on variables x with dim x d of the form cxc c for all but specially constrained c the partition function z will be intractable and the likelihood of a set of i.i.d. data intractable as well. a surrogate is to use the pseudo likelihood of the probability of each variable conditioned on all other variables is equivalent to conditioning on only the variable s neighbours for a mn c px z l log pxn i i are usually straightforward to work out since they require finding the normalisation the terms pxn of a univariate distribution only. in this case the gradient can be computed exactly and learning of the parameters carried out. at least for the case of the boltzmann machine this forms a consistent learning the structure learning the structure of a markov network can also be based on independence tests as for belief networks. a criterion for finding a mn on a set of nodes v is to use the fact that no edge exits between x and y if conditioned on all other nodes x and y are deemed independent. this is the pairwise markov property described in by checking x yvx y for every pair of variables x and y this edge deletion approach in principle reveals the structure of the for learning the structure from an oracle this method is sound. however a practical difficulty in the case where the independencies are determined from data is that checking if x y vx y requires in principle enormous amounts of data. the reason for this is that the conditioning selects only those parts of the dataset consistent with the conditioning. in practice this will result in very small numbers of remaining datapoints and estimating independencies on this basis is unreliable. the markov boundary uses the local markov property namely that conditioned on its neighbours a variable is independent of all other variables in the graph. by starting with a variable x and an empty neighbourhood set one can progressively include neighbours testing if their inclusion renders the remaining non-neighbours independent of x. a difficultly with this is that if one doesn t have the correct markov boundary then including a variable in the neighbourhood set may be deemed necessary. to see this consider a network which corresponds to a linear chain and that x is at the edge of the chain. in this case only the nearest neighbour of x is in the markov boundary of x. however if this nearest neighbour were not currently in the set then any other non-nearest neighbour would be included even though this is not strictly required. to counter this the neighbourhood variables included in the neighbourhood of x may be later removed if they are deemed superfluous to the in cases where specific constraints are imposed such as learning structures whose resulting triangulation has a bounded tree-width whilst still formally difficult approximate procedures are in terms of network scoring methods for undirected networks computing a score is hampered by the fact that the parameters of each clique become coupled in the normalisation constant of the distribution. this issue can be addressed using hyper markov properties of maximum likelihood training assuming the correct model class consider a dataset x n n generated from an underlying parametric model px our interest is to fit a model px of the same form as the correct underlying model px and examine draft march code whether if in the limit of a large amount of data the parameter learned by maximum likelihood matches the correct parameter our derivation below is non-rigorous but highlights the essence of the argument. assuming the data is i.i.d. the log likelihood l log px is l n log pxn in the limit n the sample average can be replaced by an average with respect to the distribution generating the data px px l px up to a negligible constant this is the kullback-leibler divergence between two distributions in x just with different parameter settings. the that maximises l is that which minimises the kullbackleibler divergence namely in the limit of a large amount of data we can in principle learn the correct parameters we know the correct model class. the property of an estimator such that the parameter converges to the true model parameter as the sequence of data increase is termed a consistency. training when the assumed model is incorrect we write qx for the assumed model and px for the correct generating model. repeating the above calculations in the case of the assumed model being correct we have that in the limit of a large amount of data the likelihood is l qx klpx px since q and p are not of the same form setting to does not necessarily minimise klpx and therefore does not necessarily optimize l code condindepemp.m bayes test and mutual information for empirical conditional independence condmi.m conditional mutual information condmiemp.m conditional mutual information of empirical distribution miemp.m mutual information of empirical distribution pc algorithm using an oracle this demo uses an oracle to determine x y z rather than using data to determine the empirical dependence. the oracle is itself a belief network. for the partial orientation only the first unmarried collider rule is implemented. demopcoracle.m demo of pc algorithm with an oracle pcskeletonoracle.m pc algorithm using an oracle pcorient.m orient a skeleton demo of empirical conditional independence for half of the experiments the data is drawn from a distribution for which x y z is true. for the other half of the experiments the data is drawn from a random distribution for which x y z is false. we then measure the fraction of experiments for which the bayes test correctly decides x y z. we also measure the fraction of experiments for which the mutual information test correctly decides x y z based on draft march fuse drum toner paper roller burning quality wrinkled mult. pages paper jam exercises figure printer nightmare belief network. all variables are binary. the upper variables without parents are possible problems and the lower variables consequences of problems setting the threshold equal to the median of all the empirical conditional mutual information values. a similar empirical threshold can also be obtained for the bayes factor this is not strictly kosher in the pure bayesian spirit since one should in principle set the threshold to zero. the test based on the assumed chi-squared distributed mi is included for comparison although it seems to be impractical in these small data cases. democondindepemp.m demo of empirical conditional independence based on data bayes dirichlet structure learning it is interesting to compare the result of demopcdata.m with demobdscore.m. pcskeletondata.m pc algorithm using empirical conditional independence demopcdata.m demo of pc algorithm with data bdscore.m bayes dirichlet score for a node given parents learnbayesnet.m given an ancestral order and maximal parents learn the network demobdscore.m demo of structure learning exercises exercise nightmare. cheapco is quite honestly a pain in the neck. not only did they buy a dodgy old laser printer from stoppress and use it mercilessly but try to get away with using substandard components and materials. unfortunately for stoppress they have a contract to maintain cheapco s old warhorse and end up frequently sending the mechanic out to repair the printer. after the visit they decide to make a statistical model of cheapco s printer so that they will have a reasonable idea of the fault based only on the information that cheapco s secretary tells them on the phone. in that way stoppress hopes to be able to send out to cheapco only a junior repair mechanic having most likely diagnosed the fault over the phone. based on the manufacturer s information stoppress has a good idea of the dependencies in the printer and what is likely to directly affect other printer components. the belief network in represents these assumptions. however the specific way that cheapco abuse their printer is a mystery so that the exact probabilistic relationships between the faults and problems is idiosyncratic to cheapco. stoppress has the following table of faults for each of the visits. each column represents a visit. fuse assembly malfunction drum unit toner out poor paper quality worn roller burning smell poor print quality wrinkled pages multiple pages fed paper jam draft march exercises the above table is contained in printer.mat. learn all table entries on the basis of maximum likelihood. program the belief network using the tables maximum likelihood tables and brmltoolbox. compute the probability that there is a fuse assembly malfunction given that the secretary complains there is a burning smell and that the paper is jammed and that there are no other problems. repeat the above calculation using a bayesian method in which a flat beta prior is used on all tables. given the above information from the secretary what is the most likely joint diagnosis over the diagnostic variables that is the joint most likely pf use drum t oner p aper rollerevidence? use the max-absorption method on the associated junction tree. compute the joint most likely state of the distribution pf use drum t oner p aper rollerburning smell paper jammed explain how to compute this efficiently using the max-absorption method. exercise explain how to use a factorised beta prior in the case of learning table entries in belief networks in which each variable has maximally a single parent. consider the issues around bayesian learning of binary table entries when the number of parental variables is not restricted. exercise consider data xn n n. show that for a gaussian distribution the maximum likelihood estimator of the mean is m n xn and variance is n exercise a training set consists of one dimensional examples from two classes. the training examples from class are and from class are fit a dimensional gaussian using maximum likelihood to each of these two classes. also estimate the class probabilities and using maximum likelihood. what is the probability that the test point x belongs to class exercise for a set of n observations data x xn and independently gathered observations the log likelihood for a belief network to generate x is log px log p i i we define the notation meaning variable xi is in state s and the parents of variable xi are in the vector of states t. using a lagrangian i st pxi spa t l log p i i s j s xn pa j s xn pa s j xn j ti i ti xn j show that the maximum likelihood setting of i st is s i s draft march exercises cl n exercise likelihood training. consider a situation in which we partition observable variables into disjoint sets x and y and that we want to find the parameters that maximize the conditional likelihood pynxn for a set of training data yn n n. all data is assumed generated from the same distribution px y pyx for some unknown parameter in the limit of a large amount of i.i.d. training data does cl have an optimum at exercise matching. one way to set parameters of a distribution is to match the moments of the distribution to the empirical moments. this sometimes corresponds to maximum likelihood the gaussian distribution for example though generally this is not consistent with maximum likelihood. for data with mean m and variance s show that to fit a beta distribution by moment matching we use m s m m exercise for data xn n n generated from a beta distribution b b show that the log likelihood is given by m la b log xn xn n log ba b where ba b is the beta function. show that the derivatives are a l log xn b b l xn b where d log is the digamma function and suggest a method to learn the parameters ab. exercise consider the boltzmann machine as defined in derive the gradient with respect to the biases wii. write down the pseudo likelihood for a set of i.i.d. data vn and derive the gradient of this exercise show that the model likelihood equation can be written explicitly as with respect to wij i j. pdm v j i uiv j i iv j i iv j j exercise define the set n as consisting of node belief networks in which each node has at most parents. for a given ancestral order a the restricted set is written na how many belief networks are in na? what is the computational time to find the optimal member of na using the bayesian dirichlet score assuming that computing the bd score of any member of na takes second and bearing in mind the decomposability of the bd score. what is the time to find the optimal member of n exercise for the markov network px y z z y z derive an iterative scaling algorithm to learn the unconstrained tables y and y based on a set of i.i.d. data x draft march exercise given training data xn derive an iterative scaling algorithm for maximum likelihood training of crfs of the form px e cfcx z where z cannot all be zero for any given x. exercise for data x n consider maximum likelihood learning of a markov network px c cxc with potentials of the form c e cfcx and non-negative features fcx may assume that the features x with fcxc being general real valued functions and c real valued parameters. by considering exercises c cxc e cfcxc cfcxc c cfcxc for auxiliary variables pc such pc pc c algorithm in which each parameter c can be learned separately. c pc explain how to derive a form of iterative scaling training draft march exercises draft march chapter naive bayes naive bayes and conditional independence naive bayes is a popular classification method and aids our discussion of conditional independence overfitting and bayesian methods. in nb we form a joint model of observations x and the corresponding class label c using a belief network of the form px c pc pxic whose belief network is depicted in coupled with a suitable choice for each conditional distri bution pxic we can then use bayes rule to form a classifier for a novel attribute vector x pcx px px px c px in practice it is common to consider only two classes domc the theory we describe below is valid for any number of classes c though our examples are restricted to the binary class case. also the attributes xi are often taken to be binary as we shall do initially below as well. the extension to more than two attribute states or continuous attributes is straightforward. example ezsurvey.org considers radio station listeners conveniently fall into two groups the young and old they assume that given the knowledge that a customer is either young or old this is sufficient to determine whether or not a customer will like a particular radio station independent of their likes or dislikes for any other stations where each of the variables can take the states either like or dislike and the age variable can take the value either young or old. thus the information about the age of the customer determines the individual product preferences without needing to know anything else. to complete the specification given that a customer is young she has a chance to like a chance to like a chance to like and a chance to like similarly an old listener has a chance to like an chance to like a chance to like and a chance to like they know that of the listeners are old. estimation using maximum likelihood c cn xn i c ic i d n n figure naive bayes classifer. the central assumption is that given the class c the attributes xi assuming the data is i.i.d. are independent. maximum likelihood learns the optimal parameters of the distribution pc and the class-dependent attribute distributions pxic. given this model and a new customer that likes and but dislikes and what is the probability that they are young? this is given by page like dislike like dislike like dislike like dislikeage youngpage young age like dislike like dislikeagepage using the naive bayes structure the numerator above is given by likeage dislikeage young likeage dislikeage youngpage young plugging in the values we obtain the denominator is given by this value plus the corresponding term evaluated under assuming the customer is old which gives page like dislike like dislike estimation using maximum likelihood learning the table entries for nb is a straightforward application of the more general bn learning discussed in for a fully observed dataset maximum likelihood learning of the table entries corresponds to counting the number of occurrences in the training data as we show below. binary attributes consider a dataset n n of binary attributes xn i i d. each datapoint xn has an associated class label cn. the number of datapoints from class c is and the number from class c denoted is for each attribute of the two classes we need to estimate the values pxi c i the other probability pxi is given by the normalisation requirement pxi pxi c i draft march based on the nb conditional independence assumption the probability of observing a vector x can be compactly written estimation using maximum likelihood pxc pxic c i c i xi in the above expression xi is either or and hence each i term contributes a factor c i if xi or c i if xi together with the assumption that the training data is i.i.d. generated the log likelihood of the attributes and class labels is log pxn cn log n i pxn i i log cn xn i xn i cn i log pc log pc this can be written more explicitly in terms of the parameters as n l l in in i cn log i i i cn i i i cn log i i log pc log pc we can find the maximum likelihood optimal c i by differentiating w.r.t. c i and equating to zero giving i i cn i cn c i n c i pxi i i cn c i n i cn c number of times xi for class c number of datapoints in class c similarly optimising equation with respect to pc gives pc number of times class c occurs total number of data points classification boundary we classify a novel input x as class if pc pc using bayes rule and writing the log of the above expression this is equivalent to i log px log pc log px log px log pc log px from the definition of the classifier this is equivalent to normalisation constant log px can be dropped from both sides ic log pc log px log px i ic log pc i log using the binary encoding xi we classify x as class if i log this decision rule can be expressed in the form classify x as class i pc i i x i i i x i wix i a for some suitable choice of weights wi and constant a see the interpretation is that w specifies a hyperplane in the attribute space and x is classified as if it lies on the positive side of the hyperplane. i pc i draft march estimation using maximum likelihood figure english tastes over attributes lager whiskey porridge f ootball. each column represents the tastes of an individual. scottish tastes. example they scottish?. consider the following vector of attributes shortbread likes lager drinks whiskey eats porridge watched england play football a vector x would describe that a person likes shortbread does not like lager drinks whiskey eats porridge and has not watched england play football. together with each vector x there is a label nat describing the nationality of the person domnat english see we wish to classify the vector x as either scottish or english. we can use bayes rule to calculate the probability that x is scottish or english pscottishx pxscottishpscottish px pxscottishpscottish pxscottishpscottish pxenglishpenglish by maximum likelihood the prior class probability pscottish is given by the fraction of people in the database that are scottish and similarly penglish is given as the fraction of people in the database that are english. this gives pscottish and penglish for pxnat under the naive bayes assumption pxnat so that knowing whether not someone is scottish we don t need to know anything else to calculate the probability of their likes and dislikes. based on the table in and using maximum likelihood we have for x we get pscottishx since this is greater than we would classify this person as being scottish. small data counts in consider trying to classify the vector x in the training data all scottish people say they like shortbread. this means that for this particular x px scottish and therefore that we make the extremely confident classification pscottishx this demonstrates a difficulty using maximum likelihood with sparse data. one way to ameliorate this is to smooth the probabilities for example by adding a certain small number to the frequency counts of each attribute. this ensures that draft march estimation using maximum likelihood there are no zero probabilities in the model. an alternative is to use a bayesian approach that discourages extreme probabilities as discussed in potential pitfalls with encoding in many off-the-shelf packages implementing naive bayes binary attributes are assumed. in practice however the case of non-binary attributes often occurs. consider the following attribute age. in a survey a person s age is marked down using the variable a a means the person is between and years old a means the person is between and years old a means the person is older than one way to transform the variable a into a binary representation would be to use three binary variables with representing a a a respectively. this is called of m coding since only of the binary variables is active in encoding the m states. by construction means that the variables are dependent for example if we know that we know that and regardless of any class conditioning these variables will always be dependent contrary to the assumption of naive bayes. a correct approach is to use variables with more than two states as explained in multi-state variables for a variable xi with more than two states domxi s the likelihood of observing a state xi s is denoted sc pxi sc i s pxi sc for a set of data vectors xnn n belonging to class c under the i.i.d. assumption the likelihood of the nb model generating data from class c is ixn i i sc which gives the class conditional log-likelihood l i i s i c log i sc pxncn we can optimize with respect to the parameters using a lagrange multiplier for each of the attributes i and classes c to ensure normalisation l i i s i c log i sc i sc to find the optimum of this function we may differentiate with respect to i the resulting equation we obtain sc and equate to zero. solving c i i i s i c sc i c i hence by normalisation i sc pxi sc i n i s i c i i c the maximum likelihood setting for the parameter pxi sc equals the relative number of times that attribute i is in state s for class c. draft march bayesian naive bayes figure bayesian naive bayes with a factorised prior on the class conditional attribute probabilities pxi sc. for simplicity we assume that the class probability c pc is learned with maximum likelihood so that no distribution is placed over this parameter. c n n cn xn i ic c c i d text classification consider a set of documents about politics and another set about sport. our interest is to make a method that can automatically classify a new document as pertaining to either sport or politics. we search through both sets of documents to find the most commonly occurring words. each document is then represented by a dimensional vector representing the number of times that each of the words occurs in that document the so called bag of words representation is a crude representation of the document since it discards word order. a naive bayes model specifies a distribution of these number of occurrences pxic where xi is the count of the number of times word i appears in documents of type c. one can achieve this using either a multistate representation discussed in or using a continuous xi to represent the frequency of word i in the document. in this case pxic could be conveniently modelled using for example a beta distribution. intuitively a despite the simplicity of naive bayes it can classify documents surprisingly potential justification for the conditional independence assumption is that if we know a document is about politics this is a good indication of the kinds of other words we will find in the document. because naive bayes is a reasonable classifier in this sense and has minimal storage and fast training it has been applied to time-storage critical applications such as automatically classifying webpages into and spam bayesian naive bayes to predict the class c of an input x we use pcxd pxd cpcd pxd cpcd for convenience we will simply set pcd using maximum likelihood n pcd n i c however as we ve seen setting the parameters of pxd c using maximum likelihood training can yield over-confident predictions in the case of sparse data. a bayesian approach that addresses this difficulty sc that discourage extreme values. the model is is to use priors on the probabilities pxi sc i depicted in the prior p ic we will use a prior on the table entries and make the global factorisation assumption p ic draft march bayesian naive bayes we consider discrete xi each of which take states from s. in this case pxi sc corresponds to a multinomial distribution for which the conjugate prior is a dirichlet distribution. under the factorised prior assumption we define a prior for each attribute i and class c p ic where uic is the hyperparameter vector of the dirichlet distribution for table pxic. the posterior first let s see how the bayesian approach is used to classify a novel point x let d denote the training data cn n n. from equation the term px is computed using the following decomposition px px px c px hence in order to make a prediction we require the parameter posterior. consistent with our general bayesian bn training result in the parameter posterior factorises for dirichlet hyperparameters uic the above equation updates the hyperparameter by the number of times variable i is in state s for class c data. a common default setting is to take all components of u to be px id c px i s i sc i p where p ic p ic p ic ic uic uic where the vector uic has components ncnc p ic i i s s ui sc pxn i ic ncnc by conjugacy the posterior for class c is a dirichlet distribution classification the class distribution is given by pc to compute px c pc c we use i s px px i sd c using the general identity i pc c sdirichlet d zu zu us s u s us s draft march where zu is the normalisation constant of the dirichlet distribution dirichlet and figure a chow-liu tree in which each variable xi has at most one parent. the variables may be indexed such that i d. tree augmented naive bayes we obtain pc pc i zu ic z uic where i s u ui sc i i s example naive bayes. repeating the previous analysis for the are they scottish? data from the probability under a uniform dirichlet prior for all the tables gives a value of for the probability that is scottish compared with a value of under the standard naive bayes assumption. tree augmented naive bayes a natural extension of naive bayes is to relax the assumption that the attributes are independent given the class pxc pxic the question then arises which structure should we choose for pxc? as we saw in learning a structure is computationally infeasible for all but very small numbers of attributes. a practical algorithm requires a specific form of constraint on the structure. to do this we first make a digression into the maximum likelihood learning of trees constrained to have at most a single parent. chow-liu trees consider a multivariate distribution px that we wish to approximate with a distribution qx. furthermore we constrain the approximation qx to be a belief network in which each node has at most one parent. first we assume that we have chosen a particular labelling of the variables i d for which the dag single parent constraint means qx qxixpai pai i or pai where pai is the single parent index of node i. to find the best approximating distribution q in this constrained class we may minimise the kullback-leibler divergence klpq since px is fixed the first term is constant. by adding a pxixpai on px alone we can write pxixpai that depends pxixpai pxixpai pxpai klpq const. draft march tree augmented naive bayes algorithm chow-liu trees end for for j to d do compute the mutual information for the pair of variables xi xj wij mixi xj for i to d do end for for the undirected graph g with edge weights w find a maximum weight undirected spanning tree t choose an arbitrary variable as the root node of the tree t form a directed tree by orienting all edges away from the root node. this enables us to recognise that up to a negligible constant the overall kullback-leibler divergence is a positive sum of individual kullback-leibler divergences so that the optimal setting is therefore plugging this solution into equation and using log pxixpai log pxi xpai log pxpai we obtain qxixpai pxixpai klpq const. pxi pxixpai pxpai we still need to find the optimal parental structure pai that minimises the above expression. if we add and subtract an entropy term we can write pxi klpq pxixpai pxpai const. for two variables xi and xj and distribution pxi xj the mutual information can be written as mixi xj log pxi xj pxipxj pxixj which can be seen as the kullback-leibler divergence klpxi xjpxipxj and is therefore nonnegative. using this equation is xpai xpai klpq since our task is to find the parental indices pai and the entropic const. i is independent of this mapping finding the optimal mapping is equivalent to maximising the summed mutual informations under the constraint that pai i. since we also need to choose the optimal initial labelling of the variables as well the problem is equivalent to computing all the pairwise mutual informations wij mixi xj and then finding a maximal spanning tree for the graph with edge weights w spantree.m. this can be thought of as a form of once found we need to identify a directed tree with at most one parent. this is achieved by choosing an arbitrary node and then orienting edges consistently away from this node. draft march tree augmented naive bayes c figure tree augmented naive bayes. each variable xi has at most one parent. the maximum likelihood optimal tan structure is computed using a modified chow-liu algorithm in which the conditional mutual information mixi xjc is computed for all i j. a maximum weight spanning tree is then found and turned into a directed graph by orienting the edges outwards from a chosen root node. the table entries can then be read off using the usual maximum likelihood counting argument. maximum likelihood chow-liu trees if px is the empirical distribution px n then xn klpq const. n n log qxn hence the approximation q that minimises the kullback-leibler divergence between the empirical distribution and p is equivalent to that which maximises the likelihood of the data. this means that if we use the mutual information found from the empirical distribution with pxi a xj b a xj b then the chow-liu tree produced corresponds to the maximum likelihood solution amongst all singleparent trees. an outline of the procedure is given in an efficient algorithm for sparse data is also remark tree structured belief networks. the chow-liu algorithm pertains to the discussion in on learning the structure of belief networks from data. under the special constraint that each variable has at most one parent the chow-liu algorithm returns the maximum likelihood structure to fit the data. learning tree augmented naive bayes networks for a distribution pxc of the form of a tree structure with a single-parent constraint we can readily find the class conditional maximum likelihood solution by computing the chow-liu tree for each class. one then adds links from the class node c to each variable and learns the class conditional probabilities from c to x which can be read off for maximum likelihood using the usual counting argument. note that this would generally result in a different chow-liu tree for each class. practitioners typically constrain the network to have the same structure for all classes. the maximum likelihood objective under the tan constraint then corresponds to maximising the conditional mutual mixi xjc see once the structure is learned one subsequently sets parameters by maximum likelihood counting. techniques to prevent overfitting are discussed in and can be addressed using dirichlet priors as for the simpler naive bayes structure. one can readily consider less restrictive structures than single-parent belief networks. however the complexity of finding optimal bn structures is generally computationally infeasible and heuristics are required to limit the search space. draft march exercises code naivebayestrain.m naive bayes trained with maximum likelihood naivebayestest.m naive bayes test naivebayesdirichlettrain.m naive bayes trained with bayesian dirichlet naivebayesdirichlettest.m naive bayes testing with bayesian dirichlet demonaivebayes.m demo of naive bayes exercises exercise a local supermarket specializing in breakfast cereals decides to analyze the buying patterns of its customers. they make a small survey asking randomly chosen people their age or younger than years and which of the breakfast cereals frosties sugar puffs branflakes they like. each respondent provides a vector with entries or corresponding to whether they like or dislike the cereal. thus a respondent with would like cornflakes frosties and branflakes but not sugar puffs. the older than years respondents provide the following data the younger than years old respondents responded a novel customer comes into the supermarket and says she only likes frosties and sugar puffs. using naive bayes trained with maximum likelihood what is the probability that she is younger than exercise a psychologist does a small survey on happiness each respondent provides a vector with entries or corresponding to whether they answer yes to a question or no respectively. the question vector has attributes x married healthy thus a response would indicate that the respondent was rich unmarried healthy in addition each respondent gives a value c if they are content with their lifestyle and c if they are not. the following responses were obtained from people who claimed also to be content and for not content using naive bayes what is the probability that a person who is not rich married and healthy is content what is the probability that a person who is not rich and married is content is we do not know whether or not they are healthy consider the following vector of attributes if customer is younger than otherwise if customer is between and years old otherwise if customer is older than otherwise if customer walks to work otherwise each vector of attributes has an associated class label rich or poor point out any potential difficulties with using your previously described approach to training using naive bayes. hence describe how to extend your previous naive bayes method to deal with this dataset. exercise whizzco decide to make a text classifier. to begin with they attempt to classify documents as either sport or politics. they decide to represent each document as a vector of attributes describing the presence or absence of words. x football golf defence offence wicket office strategy draft march exercises training data from sport documents and from politics documents is represented below in matlab using a matrix in which each row represents the attributes. politics sport using a naive bayes classifier what is the probability that the document x is about politics? exercise a naive bayes classifier for binary attributes xi is parameterised by i pxi i pxi and pclass and pclass show that the decision boundary to classify a datapoint x can be written as wtx b and state explicitly w and b as a function of exercise this question concerns spam filtering. each email is represented by a vector x xd where xi each entry of the vector indicates if a particular symbol or word appears in the email. the symbolswords are money cash viagra etc. so that if the word cash appears in the email. the training dataset consists of a set of vectors along with the class label c where c indicates the email is spam and c not spam. hence the training set consists of a set of pairs cn n n. the naive bayes model is given by pc x pc pxic draw a belief network for this distribution. derive expressions for the parameters of this model in terms of the training data using maximum likelihood. assume that the data is independent and identically distributed cn xn pcn xn explicitly the parameters are pc pxi pxi i d given a trained model px c explain how to form a classifier pcx. if viagra never appears in the spam training data discuss what effect this will have on the classifi cation for a new email that contains the word viagra write down an expression for the decision boundary and show that it can be written in the form pc pc udxd b for suitably defined u and b. draft march exercises exercise for a distribution px c and an approximation qx c show that when px c corresponds to the empirical distribution finding qx c that minimises the kullback-leibler divergence klpx cqx c corresponds to maximum likelihood training of qx c. exercise consider a distribution px c and a tree augmented approximation qx c i qxixpai c pai i or pai show that for the optimal qx c constrained as above the solution qx c that minimises klpx cqx c when plugged back into the kullback-leibler expression gives as a function of the parental structure klpx cqx c i log pxi xpaic pxpaicpxic const. pxixpaic this shows that under the single-parent constraint and that each tree qxc has the same structure minimising the kullback-leibler divergence is equivalent to maximising the sum of conditional mutual information terms. exercise write a matlab routine a chowliux where x is a d n data matrix containing a multivariate datapoint on each column that returns a chow-liu maximum likelihood tree for x. the tree structure is to be returned in the sparse matrix a. you may find the routine spantree.m useful. the file chowliudata.mat contains a data matrix for variables. use your routine to find the maximum likelihood chow liu tree and draw a picture of the resulting dag with edges oriented away from variable draft march exercises draft march chapter learning with hidden variables hidden variables and missing data in practice data entries are often missing resulting in incomplete information to specify a likelihood. observational variables may be split into visible for which we actually know the state and missing whose states would nominally be known but are missing for a particular datapoint. another scenario in which not all variables in the model are observed are the so-called hidden or latent variable models. in this case there are variables which are essential for the model description but never observed. for example the underlying physics of a model may contain latent processes which are essential to describe the model but cannot be directly measured. why hiddenmissing variables can complicate proceedings in learning the parameters of models as previously described in we assumed we have complete information to define all variables of the joint model of the data pv consider the asbestos-smokingcancer network of in the case of complete data the likelihood is pvn pan sn cn pcnan sn cpan apsn s which is factorised in terms of the table entry parameters. we exploited this property to show that table entries can be learned by considering only local information both in the maximum likelihood and bayesian frameworks. now consider the case that for some of the patients only partial information is available. for example for patient n with record vn s it is known that the patient has cancer and is a smoker but whether or not they had exposure to asbestos is unknown. since we can only use the visible available information is it would seem reasonable to assess parameters using the marginal likelihood pcna sn cpa apsn s pvn pa sn cn a a we will discuss when this approach is valid in using the marginal likelihood may result in computational difficulties since equation is not factorised over the tables. this means that the likelihood function cannot be written as a product of functions one for each separate parameter. in this case the maximisation of the likelihood is more complex since the parameters of different tables are coupled. a similar complication holds for bayesian learning. as we saw in under a prior factorised over each cpt the posterior is also factorised. however in the case of unknown asbestos exposure a term hidden variables and missing data xvis xinv xvis xinv minv minv figure missing at random assumption. missing completely at random assumption. is introduced of the form pvn pcna sn cpa apsn s psn a pcna sn cpa a a which cannot be written as a product of a functions of fs sfa afc c. the missing variable therefore introduces dependencies in the posterior parameter distribution making the posterior more complex. in both the maximum likelihood and bayesian cases one has a well defined likelihood function of the table parametersposterior. the difficulty is therefore not conceptual but rather computational how are we to find the optimum of the likelihoodsummarise the posterior? note that missing data does not always make the parameter posterior non-factorised. for example if the cancer state is unobserved above because cancer is a collider with no descendants the conditional distribution simply sums to and one is left with a factor dependent on a and another on s. the missing at random assumption under what circumstances is it valid to use the marginal likelihood to assess parameters? we partition the variables x into those that are visible xvis and invisible xinv so that the set of all variables can be written x xinv. for the visible variables we have an observed state xvis v whereas the state of the invisible variables is unknown. we use an indicator minv to denote that the state of the invisible variables is unknown. then for a datapoint which contains both visible and invisible information pxvis v minv xinv xinv pxvis v xinv minv pminv v xinv v xinv if we assume that the mechanism which generates invisible data has the form then pminv v xinv pminv v pxvis v minv pminv pxvis v xinv xinv pminv vpxvis v only the term pxvis v conveys information about the model. therefore provided the mechanism by which the data is missing depends only on the visible states we may simply use the marginal likelihood to assess parameters. this is called the missing at random assumption. example missing at random. ezsurvey.org stop men on the street and ask them their favourite colour. all men whose favourite colour is pink decline to respond to the question for any other colour all men respond to the question. based on the data ezsurvey.org produce a histogram of men s favourite colour based on the likelihood of the visible data alone confidently stating that none of them likes pink. draft march hidden variables and missing data for simplicity assume there are only three colours blue green and pink. ezsurvey.org attempts to find the histogram with probabilities b g p with b g p each respondent produces a visible response xc with domxc green pink otherwise mc if there is no response. three men are asked their favourite colour giving data c c c missing green based on the likelihood of the visible data alone we have the log likelihood for i.i.d. data l b g p log b log g b g p where the last lagrange term ensures normalisation. maximising the expression we arrive at b g p the unreasonable result that ezsurvey.org produce is due to not accounting correctly for the mechanism which produces the data. the correct mechanism that generates the data the missing data is blue c green b p g b b g g where we used probability that the favourite colour is pink. maximising the likelihood we arrive at c p since the probability that a datapoint is missing is the same as the b g p as we would expect. on the other hand if there is another visible variable t denoting the time of day and the probability that men respond to the question depends only on the time t alone example the missing probability is high during rush hour then we may indeed treat the missing data as missing at random. a stronger assumption than mar is pminv v xinv pminv which is called missing completely at random. this applies for example to latent variable models in which the variable state is always missing independent of anything else. maximum likelihood throughout the remaining discussion we will assume any missing data is mar or missing completely at random. this means that we can treat any unobserved variables by summing integrating over their states. for maximum likelihood we learn model parameters by optimising the marginal likelihood pv h pv h with respect to identifiability issues the marginal likelihood objective function depends on the parameters only through pv so that equivalent parameter solutions may exist. for example consider a latent variable problem with distribution draft march expectation maximisation in which variable is never observed. this means that the marginal likelihood only depends on the entry given a maximum likelihood solution we can then always find an equivalent maximum likelihood solution provided in other cases there is an inherent symmetry in the parameter space of the marginal likelihood. for example consider the network over binary variables pc a s pca spaps our aim is to learn the table pa and the four tables pc s pc s pc s pc s where we used a to denote that these are parameter estimates. we assume that we have missing data such that the states of variable a are never observed. in this case an equivalent solution the sense that it has the same marginal likelihood is given by interchanging the states of a pa p and the four tables p p s pc s s pc s p p s pc s s pc s a similar situation occurs in a more general setting in which the state of a variable is consistently unobserved models are a case in point yielding an inherent symmetry in the solution space. a well known characteristic of maximum likelihood algorithms is that jostling occurs in the initial stages of training in which these symmetric solutions compete. expectation maximisation the em algorithm is a convenient and general purpose iterative approach to maximising the likelihood under missing datahidden it is generally straightforward to implement and can achieve large jumps in parameter space particularly in the initial iterations. variational em the key feature of the em algorithm is to form an alternative objective function for which the parameter coupling effect discussed in is removed meaning that individual parameter updates can be achieved akin to the case of fully observed data. the way this works is to replace the marginal likelihood with a lower bound it is this lower bound that has the decoupled form. we first consider a single variable pair h where v stands for visible and h for hidden to derive the bound on the marginal likelihood consider the kullback-leibler divergence between a variational distribution qhv and the parametric model phv klqhvphv qhv log phv draft march expectation maximisation the term variational refers to the fact that this distribution will be a parameter of an optimisation problem. using bayes rule phv ph v and the fact that pv does not depend on h ph v log pv klqhvphv rearranging we obtain a bound on the marginal log pv ph v v is the sum of the individual log likelihoods entropy energy the bound is potentially useful since it is similar in form to the fully observed case except that terms with missing data have their log likelihood weighted by a prefactor. is a marginal likelihood bound for a single training example. under the i.i.d. assumption the log likelihood of all training data log pv log pvn summing over the training data we obtain a bound on the log likelihood log pv phn vn note that the bound is exact is the right hand side is equal to the log likelihood when we set qhnvn phnvn n n. the bound suggests an iterative procedure to optimise e-step for fixed find the distributions qhnvn that maximise equation m-step for fixed n n find the parameters that maximise equation classical em in the variational e-step above the fully optimal setting is qhnvn phnvn since q does not depend on new the m-step is equivalent to maximising the energy term alone see example one-parameter one-state example. we consider a model small enough that we can plot fully the evolution of the em algorithm. the model is on a single visible variable v and single two-state hidden variable h we define a model pv h pvhph with pvh e and ph ph for an observation v and our interest is to find the parameter that optimises the likelihood e pv is analogous to a standard partition function bound in statistical physics from where the terminology energy and entropy hails. draft march expectation maximisation algorithm expectation maximisation. compute maximum likelihood value for data with hidden variables. input a distribution px and dataset v. returns ml setting of t choose an initial setting for the parameters while not converged likelihood not converged do end while return t phn vn t phnvn t run over all datapoints e step t t for n to n do qn end for t arg max the max likelihood parameter estimate. t m step iteration counter initialisation figure the log likelihood for the model described in contours of the lower bound lbqh for an initial choice qh and successive updates of the e starting at the em algorithm converges and m steps are plotted. to a local optimum. the log likelihood is plotted in with optimum at the em procedure iteratively optimises the lower bound log pv lbqh qh log qh qh log qh qh log where qh qh from an initial starting the em algorithm finds the q distribution that optimises lq and then updates depending on the initial the solution found is either a global or local optimum of the likelihood see the m-step is easy to work out analytically in this case with new v e-step sets qnewh phv so that qnewh pv pv where we used e e e pv pv pv qh. similarly the draft march logpv expectation maximisation example consider a simple model where assuming an unconstrained distribution our aim is to learn from the data the energy term for the classical em is log old old writing out fully each of the above terms on a separate line gives the energy log old log old log old log old log this expression resembles the standard log likelihood of fully observed data except that terms with missing data have their weighted log parameters. the parameters are conveniently decoupled in this bound from the trivial normalisation constraint so that finding the optimal parameters is straightforward. this is achieved by the m-step update which gives old old old old where old old etc. the e and m-steps are iterated till convergence. the em algorithm increases the likelihood whilst by construction the em algorithm cannot decrease the bound on the likelihood an important question is whether or not the log likelihood itself is necessarily increased by this procedure. we use for the new parameters and for the previous parameters in two consecutive iterations. using qhnvn phnvn we see that as a function of the parameters the lower bound for a single variable pair h depends on and phv ph v phv lb and that is the kullback-leibler divergence is the difference between the lower bound and the true likelihood. we may write lb log pv klphv log pv lb klphv hence log pv log pv lb draft march lb klphv expectation maximisation s c figure a database containing information about being a smoker signifies the individual is a smoker and lung cancer signifies the individual has lung cancer. each row contains the information for an individual so that there are individuals in the database. the first assertion is true since by definition of the m-step we search for a which has a higher value for the bound than our starting value the second assertion is true by the property of the kullback-leibler divergence. for more than a single datapoint we simply sum each individual bound for log pvn hence we reach the important conclusion that the em algorithm increases not only the lower bound on the marginal likelihood but the marginal likelihood itself correctly the em cannot decrease these quantities. shared parameters and tables the case of tables sharing parameters is essentially straightforward. according to the energy term we need to identify all those terms in which the shared parameter occurs. the objective for the shared parameter is then the sum over all energy terms containing the shared parameter. application to belief networks conceptually the application of em to training belief networks with missing data is straightforward. the battle is more notational than conceptual. we begin the development with an example from which intuition about the general case can be gleaned. example consider the network. pa c s pca spaps for which we have a set of data but that the states of variable a are never observed see our goal is to learn the cpts pca s and pa and ps. to apply em to this case we first assume initial parameters a s c the first e-step for iteration t then defines a set of distributions on the hidden variables the hidden variable is a pac s pac s and so on for the training examples n for notational convenience we write qn of qn t in place t we now move to the first m-step. the energy term for any iteration t is e pcnan sn log pan log pcnan t t t log psn the final term is the log likelihood of the variable s and ps appears explicitly only in this term. hence the usual maximum likelihood rule applies and ps is simply given by the relative number of times draft march expectation maximisation that s occurs in the database giving ps ps n log pa the parameter pa occurs in the terms t log pa qn t log pa which using the normalisation constraint is qn t pa t t n qn t n qn n n n n qn n qn t pa qn t differentiating with respect to pa and solving for the zero derivative we get that is whereas in the standard maximum likelihood estimate we would have the real counts of the t data in the above formula here they have been replaced with our guessed values qn t and qn a similar story holds for pc s the contribution of this term to the energy is t log pc s qn qn t log pc s which is log pc s pc s pc s n t pc s qn qn t n i i qn t i i i n n i i qn t i i qn t n i i i i i i n optimising with respect to pc s gives for comparison the setting in the complete data case is there is an intuitive relationship between these updates in the missing data case we replace the indicators by the assumed distributions q. iterating the e and m steps these equations will converge to a local likelihood optimum. to minimise the notational burden we assume that the structure of the missing variables is fixed throughout this being equivalent therefore to a latent variable model. the form of the energy term for belief networks n n i it is useful to define the following notation t qthvn vn qn pxn i i t sets the visible where x h represents all the variables in the distribution. this means that qn variables in the observed state and defines a conditional distribution on the unobserved variables. we draft march expectation maximisation algorithm em for belief networks. tables and dataset on the visible variables v. returns the maximum likelihood setting of tables. t set pt to initial values. while p not converged likelihood not converged do input a bn structure pxipa i k with empty iteration counter initialisation t pt vn qn n qn t t t for n to n do end for for i to k do end for end while return ptxipa qtx qn t n then define the mixture distribution run over all datapoints e step run over all variables m step the max likelihood parameter estimate. the energy term in equation can be written more compactly as n n n n to see this consider the right hand side of the above qthvn vn x n px pxipa n using the structure of the belief network we have qtxipa pxipa this means that maximising the energy is equivalent to minimising i i i qtpaxi n pxipa qtpaxi where we added the constant first term to make this into the form of a kullback-leibler divergence. since this is a sum of independent kullback-leibler divergences optimally the m-step is given by setting pxipa qtxipa in practice storing the qtx over the states of all variables x is prohibitively expense. fortunately since the m-step only requires the distribution on the family of each variable xi one only requires the local distributions qn oldxipa we may therefore dispense with the global qoldx and equivalently use pnewxipa oldxi pa n qn oldpa using the em algorithm the optimal setting for the e-step is to use qthnvn poldhnvn. with this notation the em algorithm can be compactly stated as in see also embeliefnet.m. draft march expectation maximisation example general belief networks. consider a five variable distribution with discrete variables in which the variables and are consistently hidden in the training data and training data for are always present. the distribution can be represented as a belief network in this case the contributions to the energy have the form which may be written as n pxn n pxn n pxn n n n log pxn a useful property can now be exploited namely that each term depends on only those hidden variables in the family that that term represents. thus we may write n pxn n pxn n n n log pxn the final term can be set using maximum likelihood. let us consider therefore a more difficult table when will the table entry j occur in the energy? this happens whenever xn is in state i. since there is a summation over all the states of variables to the average there is also a term with variable in state j. hence the contribution to the energy from terms of the form j is i i log j where the indicator function i isation of the table we add a lagrange term i equals if xn is in state i and is zero otherwise. to ensure normal k j n n i i log j differentiating with respect to j we get n or hence i i j i i i n n nk j j i i k draft march expectation maximisation figure evolution of the log-likelihood versus iterations under the em training procedure solving the printer nightmare with missing data note how rapid progress is made at the beginning but convergence can be slow. using the em algorithm we have jxn xn xn this optimal distribution is easy to compute since this is the marginal on the family given some evidential variables. hence the m-step update for the table is what about the table j? to ensure normalisation of the table we add a lagrange term i i n nk i jxn k jxn xn xn xn xn k j j i n j log j as before differentiating and using the em settings we have j i i j ixn j kxn xn xn xn xn n nk j i i i j and equation would be j i j i i n n there is a simple intuitive pattern to equation and equation if there were no hidden data equation would read all that we do therefore in the general em case is to replace those deterministic functions such as i this is merely a restatement of the general update given in equation under the definition i by their missing variable equivalents ixn xn xn application to markov networks for a mn defined over visible and hidden variables pv h is z log pv hqh c ch v log z c ch v the em variational bound whilst the bound decouples the parameters in the second term the parameters are nevertheless coupled in the normalisation z because of this we cannot optimise the above bound on a parameter by parameter basis. one approach is to use an additional bound log z from above as for iterative scaling. draft march likelihood extensions of em convergence convergence of em can be slow particularly when the number of missing observations is greater than the number of visible observations. in practice one often combines the em with gradient based procedures to improve convergence see note also that the log likelihood is typically a non-convex function of the parameters. this means that there may be multiple local optima and the solution found often depends on the initialisation. extensions of em partial m step it is not necessary to find the full optimum of the energy term at each iteration. as long as one finds a parameter which has a higher energy than that of the current parameter then the conditions required in still hold and the likelihood cannot decrease at each iteration. partial e step phn vn the e-step requires us to find the optimum of log pv with respect to qhnvn. the fully optimal setting is qhnvn phnvn for a guaranteed increase in likelihood at each iteration from we required that the fully optimal setting of q is used. unfortunately therefore one cannot in general guarantee that such a partial e step would always increase the likelihood. of course it is guaranteed to increase the lower bound on the likelihood though not the likelihood itself. intractable energy the em algorithm assumes that we can calculate ph v of distributions for example factorised distributions qhv simpler class of distributions q e.g. q factorised qhv however in general it may be that we can only carry out the average over q for a very restricted class j qhjv. in such cases one may use a i qhiv for which the averaging required for the energy may be simpler. we can find the best distribution in class q by minimising the kl divergence between qhv q and phv numerically using a non-linear optimisation routine qopt argmin q q klqhphv alternatively one can assume a certain structured form for the q distribution and learn the optimal factors of the distribution by free form functional calculus. viterbi training an extreme case is to restrict qhnvn to a delta-function. in this case the entropic term is constant so that the optimal delta function q is to set qhnvn hn draft march where hn argmax h ph vn a failure case for em this is called viterbi training and is common in training hmms see em training with this restricted class of q distribution is therefore guaranteed to increase the lower bound on the log likelihood though not the likelihood itself. a practical advantage of viterbi training is that the energy is always tractable to compute becoming simply log phn vn which is amenable to optimisation. provided there is sufficient data one might hope that the likelihood as a function of the parameter will be sharply peaked around the optimum value. this means that at convergence the approximation of the posterior phv opt by a delta function will be reasonable and an update of em using viterbi training will produce a new approximately the same as opt. for any highly suboptimal however phv will be far from a delta function and therefore a viterbi update is less reliable in terms of leading to an increase in the likelihood itself. this suggests that the initialisation of for viterbi training is more critical than for the standard em. stochastic training another approximate qhnvn distribution would be to use an empirical distribution formed by samples from the fully optimal distribution phnvn that is one draws samples for a discussion on sampling hn hn l from phnvn and forms a q distribution l hn l the energy becomes then proportional to qhnvn log phn l vn so that as in viterbi training the energy is always computationally tractable for this restricted q class. provided that the samples from phnvn are reliable stochastic training will produce an energy function with average the same characteristics as the true energy under the classical em algorithm. this means that the solution obtained from stochastic training should tend to that from the classical em as the number of samples increases. pv h fh ph if we attempt an em approach for this this will fail also for a more general model of the form a failure case for em consider a likelihood of the form pv the e-step is p ph h qh old p old ph draft march variational bayes and the m-step sets new argmax pv h old argmax pvh old where we used the fact that for this model ph is independent of in the case that p fh then ph old fh ph so that optimising the energy requires new argmax fh old since ph old is zero everywhere expect that h for which v fh then the energy is effectively negative infinity if old. however when old the energy is this is therefore the optimum of the energy and represents therefore a failure in updating for em. this situation occurs in practice and has been noted in particular in the context of independent component one can attempt to heal this behaviour by deriving an em algorithm based on the distribution pvh fh where nh is an arbitrary distribution on the hidden variable h. the original deterministic model corresponds to defining pvh pv h we have pv an em algorithm for pv satisfies pv new pv old new old which implies new old this means that the em algorithm for the non-deterministic case is guaranteed to increase the likelihood under the deterministic model at each iteration we are at convergence. see for an application of this antifreeze technique to learning markov decision processes with em. variational bayes as discussed in maximum likelihood corresponds to a form of bayesian approach in which the parameter posterior distribution a flat prior is approximated with a delta function p opt. variational bayes is analogous to em in that it attempts to deal with hidden variables but using a distribution that better represents the posterior distribution than given by maximum likelihood. to keep the notation simple we ll initially assume only a single datapoint with observation v. our interest is then the parameter posterior p pv pv h h the vb approach assumes a factorised approximation of the joint hidden and parameter posterior ph qhq discrete variables and the kronecker delta the energy attains the maximal value of zero when old. in the case of continuous variables however the log of the dirac delta function is not well defined. considering the delta function as the limit of a narrow width gaussian for any small but finite width the energy is largest when old. draft march a bound on the marginal likelihood by minimising the kl divergence klqhq q ph we arrive at the bound log pv q pv h for fixed q if we minimize the kullback-leibler divergence we get the tightest lower bound on log pv. if then for fixed qh we minimise the kullback-leibler divergence w.r.t. q we are maximising the term q pv h and hence pushing up the bound on the marginal likelihood. this simple co-ordinate wise procedure in which we first fix the q and solve for qh and then vice versa is analogous to the e and m step of the em algorithm e-step qhqold qnewh argmin qh m-step kl qnew argmin q klqnewhq variational bayes in full generality for a set of observations v and hidden variables h for distributions qh and q which are parametersisedconstrained the best distributions in the minimal kl sense are returned. in general each iteration of vb is guaranteed to increase the bound on the marginal likelihood but not the marginal likelihood itself. like the em algorithm vb can often does suffer from local maxima issues. this means that the converged solution can be dependent on the initialisation. unconstrained approximations for fixed q the contribution to the kl divergence is pv h klqh ph const. where ph z pvh e where z is a normalising constant. hence for fixed q the optimal qh is given by p pvh qh e similarly for fixed qh optimally pvh q e log pv hn i.i.d. data under the i.i.d. assumption we obtain a bound on the marginal likelihood for the whole dataset n q pvn hn the bound holds for any qhn and q but is tightest for the converged estimates from the vb procedure. for an i.i.d. dataset it is straightforward to show that without loss of generality we may assume qhn under this we arrive at n draft march variational bayes algorithm variational bayes. t choose an initial distribution while not converged likelihood bound not converged do end while return qn t t t arg minqh klqhqt qn t arg minq klqn qn t t iteration counter initialisation e step m step the posterior parameter approximation. hn vn n hn n figure generic form of a model a facwith hidden variables. torised posterior approximation uses in variational bayes. em is a special case of variational bayes if we wish to find a summary of the parameter distribution corresponding to only the most likely point then q where is the single optimal value of the parameter. if we plug this assumption into equation we obtain the bound the m-step is then given by log pv pv h const. pvh log p argmax for a flat prior p const. this is therefore equivalent to energy maximisation in the em algorithm. using this single optimal value in the vb update for qhn we have qn t pv h phv which is the standard e-step of em. hence em is a special case of vb under a flat prior p const. and a delta function approximation of the parameter posterior. factorising the parameter posterior let s reconsider bayesian learning in the binary variable network pa c s pca spaps in which we use a factorised parameter prior p c ap s when all the data is observed the parameter posterior factorises. as we discussed in if the state of a is not observed the parameter posterior no longer factorises p a s cv p ap sp cpv a s c p ap sp p ap sp n n pvn a s c psn an pcnsn an cpan a draft march algorithm variational bayes data. t choose an initial distribution while not converged likelihood bound not converged do t t for n to n do pvnhn t e qn end for qt p pvnhn t end while return qn t s s n a a c c s a a n c variational bayes iteration counter initialisation run over all datapoints e step m step the posterior parameter approximation. figure a model for the relationship between lung cancer asbestos exposure and smoking with factorised parameter priors. variables c and s are observed but variable a is consistently missb a factorised parameter posteing. rior approximation. where the summation over a prevents the factorisation into a product of the individual table parameters. since it is convenient in terms of representations to work with factorised posteriors we can apply vb but with a factorised constraint on the form of the q. in vb we define a distribution over the visible and hidden variables. in this case the hidden variables are the an and the visible are sn cn. the joint posterior over all unobserved variables and missing observations is p a s c anv p ap sp p a s c anv q aq cq n to make a factorised posterior approximation we use n pcnsn an cpsn span a qan and minimise the kullback-leibler divergence between the left and right of the above. m-step hence q a p n pan e pan qan log a qan log a hence e pan it is convenient to use a beta distribution prior a p a a a draft march variational bayes a s n q a b n a similar calculation gives q s b q ca s b qan i n qan i c n n i qan n i qan since the posterior approximation is then also a beta distribution and four tables one for each of the parental states of c. for example these are reminiscent of the standard bayesian equations equation except that the counts have been replaced by q s. e-step we still need to determine qan. the optimal value is given by minimising the kullback-leibler divergence with respect to qan. this gives the solution that optimally pcnsnan pan a qan e for example if assume that for datapoint n s is in state and c in state then a qan e and a qan e to compute such quantities explicitly we need the values and for a beta distribution these are straightforward to compute see the complete vb procedure is then given by iterating equations and until convergence. given a converged factorised approximation computing a marginal table pa is then straightforward under the approximation pa qa aq av a a aq av since q av is a beta distribution b a the mean is straightforward. using this for both states of a leads to n qan n qan n qan pa the application of vb to learning the tables in arbitrarily structured bns is a straightforward extension of the technique outlined here. under the factorised approximation qh qhq one will always obtain a simple updating equation analogous to the full data case but with the missing data replaced by variational approximations. nevertheless if a variable has many missing parents the number of states in the average with respect to the q distribution can become intractable and further constraints on the form of the approximation or additional bounds are required. one may readily extend the above to the case of dirichlet distributions on multinomial variables see indeed the extension to the exponential family is straightforward. draft march optimising the likelihood by gradient methods figure standard ml learning. the best parameter is found by maximising the probability that the model generates the observed data ml-ii learning. in cases where we have a opt arg max pv prior preference for the parameters but with unspecified hyperparameter we can find by opt arg max pv arg max v v bayesian methods and ml-ii consider a parameterised distribution pv for which we wish to the learn the optimal parameters given some data. the model pv is depicted in where a dot indicates that no distribution is present on that variable. for a single observed datapoint v setting by maximum likelihood corresponds to finding the parameter that maximises pv in some cases we may have an idea about which parameters are more appropriate and can express this prior preference using a distribution p if the prior were fully specified then there is nothing to learn since p is now fully known. however in many cases in practice we are unsure of the exact parameter settings of the prior and hence specify a parametersised prior using a distribution p with hyperparameter this is depicted in the learning corresponds to finding the optimal pv this is known as an ml-ii procedure since it corresponds to maximum likelihood but at the higher hyperparameter this is a form of approximate bayesian analysis since although is set using maximum likelihood after training we have a distribution over parameters p optimising the likelihood by gradient methods that maximises the likelihood pv directed models the em algorithm typically works well when the amount of missing information is small compared to the complete information. in this case em exhibits approximately the same convergence as newton based gradient however if the fraction of missing information approaches unity em can converge very slowly. in the case of continuous parameters an alternative is to compute the gradient of the likelihood directly and use this as part of a standard continuous variable optimisation routine. the gradient is straightforward to compute using the following identity. consider the log likelihood l log pv the derivative can be written l pv h at this point we take the derivative inside the integral pv pv h pv l pv h pv h h phv log pv h log pv h where we used log fx fx. the right hand side is the average of the derivative of the log complete likelihood. this is closely related to the derivative of the energy term in the em algorithm though note that the average here is performed with respect the current distribution parameters and not old as in the em case. used in this way computing the derivatives of latent variable models is relatively straightforward. these derivatives may then be used as part of a standard optimisation routine such as conjugate draft march exercises undirected models consider an undirected model which contains both hidden and visible variables pv h z e for i.i.d. data the log likelihood on the visible variables is discrete v and h which has gradient l n l n h e hv e h clamped average phvn h free average phv for a markov network that is intractable partition function z cannot be computed efficiently the gradient is particularly difficult to estimate since it is the difference of two quantities each of which needs to be estimated. even getting the sign of the gradient correct can therefore be computationally difficult. for this reason learning in models such as the boltzmann machine with hidden units is particularly difficult. code in the demo code we take the original chest clinic network and draw data samples from this network. our interest is then to see if we can use the em algorithm to estimate the tables based on the data some parts of the data missing at random. we assume that we know the correct bn structure only that the cpts are unknown. we assume the logic gate table is known so we do not need to learn this. demoemchestclinic.m demo of em in learning the chest clinic tables the following code implements maximum likelihood learning of bn tables based on data with possibly missing values. embeliefnet.m em training of a belief network exercises exercise nightmare continued. continuing with the bn given in the following table represents data gathered on the printer where indicates that the entry is missing. each column represents a datapoint. use the em algorithm to learn all cpts of the network. fuse assembly malfunction drum unit toner out poor paper quality worn roller burning smell poor print quality wrinkled pages multiple pages fed paper jam draft march the table is contained in emprinter.mat using states nan in place of brmltoolbox requires states to be numbered given no wrinkled pages no burning smell and poor print quality what is the probability there is a drum unit problem? exercise consider the following distribution over discrete variables exercises in which the variables and are consistently hidden in the training data and training data for are always present. show that the em update for the table is given by j i i n nk i jxn k jxn xn xn xn xn exercise consider a simple two variable bn py x pyxpx where both y and x are binary variables domx domy you have a set of training data xn n n in which for some cases xn may be missing. we are specifically interested in learning the table px from this data. a colleague suggests that one can set px by simply looking at datapoints where x is observed and then setting px to be the fraction of observed x that is in state explain how this suggested procedure relates to maximum likelihood and em. exercise assume that a sequence is generated by a markov chain. for a single chain of length t we have t vt for simplicity we denote the sequence of visible variables as v vt for a single markov chain labelled by h t pvh h in total there are a set of h such markov chains h. the distribution on the visible variables is therefore pv pvhph there are a set of training sequences vn n n. assuming that each sequence vn is independently and identically drawn from a markov chain mixture model with h components derive the expectation maximisation algorithm for training this model. write a general matlab function in the form function to perform em learning for any set of same length sequences of integers vn t v t t v is a cell array of the training data is the time element of the second training sequence. each element say must be an integer from to v v is the number of states of the visible variables the bio-sequence case below this will be h is the draft march exercises number of mixture components. num_em_loops is the number of em iterations. a is the transition matrix pv is the prior state of the first visible variable ph is a vector of prior probabilities for the mixture state phhph. q is the cell array of posterior probabilities qmuhphvmu. your routine must also display for each em iteration the value of the log likelihood. as a check on your routine the log likelihood must increase at each iteration. the file sequences.mat contains a set of fictitious bio-sequence in a cell array sequencesmut. thus is the third sequence gtctcctgccctctctgaac which consists of timesteps. there are such sequences in total. your task is to cluster these sequences into two clusters assuming that each cluster is modelled by a markov chain. state which of the sequences belong together by assigning a sequence vn to that state for which phvn is highest. exercise write a general purpose routine vbbeliefnetpotxpars along the lines of embeliefnet.m that performs variational bayes under a dirichlet prior using a factorised parameter approximation. assume both global and local parameter independence for the prior and the approximation q exercise consider a layered boltzmann machine which has the form pv z where dim v dim dim dim v y e all variables are binary with states and the parameters for each layer l are l al wij xiyj xixj yiyj in terms of fitting the model to visible data vn is the layered model above any more powerful than fitting a two-layered model factor is not present in the two-layer case? if we use a restricted potential y e ij wij xiyj is the three layered model more powerful in being able to fit the visible data than the two-layered model? exercise the sigmoid belief network is defined by the layered network pxl pxl where vector variables have binary components xl and the width of layer l is given by wl. in addition pxl pxl i and pxl i wt e x for a weight vector wil describing the interaction from the parental layer. the top layer pxl describes a factorised distribution pxl pxl wl. draw the belief network structure of this distribution. for the layer what is the computational complexity of computing the likelihood assuming that all layers have equal width w? draft march assuming a fully factorised approximation for an equal width network qxl i exercises write down the energy term of the variational em procedure for a single data observation and discuss the tractability of computing the energy. exercise a probability table i j ij with ij exercise show how to find the components b g p that maximise equation ij is learned using maximal marginal likelihood in which is never observed. show that if is given as a maximal marginal likelihood solution then has the same marginal likelihood score. draft march chapter bayesian model selection comparing models the bayesian way given two models and with parameters and associated parameter priors px px px px how can we compare the performance of the models in fitting a set of data d xn? the application of bayes rule to models gives a framework for answering questions like this a form of bayesian hypothesis testing applied at the model level. more generally given an indexed set of models mm and associated prior beliefs in the appropriateness of each model pmi our interest is the model posterior probability pmid pdmipmi pd where pd pdmipmi model mi is parameterised by i and the model likelihood is given by pdmi pd i mip imid i in discrete parameter spaces the integral is replaced with summation. note that the number of parameters dim i need not be the same for each model. a point of caution here is that pmid only refers to the probability relative to the set of models specified mm. this is not the absolute probability that model m fits well to compute such a quantity would require one to specify all possible models. whilst interpreting the posterior pmid requires some care comparing two competing model hypotheses mi and mj is straightforward and only requires the bayes factor pmid pmjd pdmi pdmj bayes factor pmi pmj which does not require integrationsummation over all possible models. illustrations coin tossing figure discrete prior model of a fair coin. prior for a biased unfair coin. in both cases we are making explicit choices here about what we consider to be a fair and and unfair illustrations coin tossing we ll consider two illustrations. the first uses a discrete parameter space to keep the mathematics simple. in the second we use a continuous parameter space. a discrete parameter space a simple choice would be to consider two competing models one corresponding to a fair coin and the other a biased coin. the bias of the coin namely the probability that the coin will land heads is specified by so that a truly fair coin has for simplicity we assume dom for the fair coin we use the distribution p air in and for the biased coin the distribution p in for each model m the likelihood is given by pdm pd mp nh p p p assuming that pmf air pmbiased the bayes factor is given by the ratio of the two model likelihoods. example parameter space. heads and tails here pdmf air and pdmbiased the bayes factor is pmf aird pmbiasedd indicating that there is little to choose between the two models. heads and tails here pdmf air and pdmbiased the bayes factor is pmf aird pmbiasedd indicating that have around times the belief in the biased model as opposed to the fair model. a continuous parameter space here we repeat the above calculation but for continuous parameter spaces. draft march illustrations coin tossing figure probability density priors on the probability of a head p for a fair coin p air for an biased coin p b note the different b vertical scales in the two cases. fair coin for the fair coin a uni-modal prior is appropriate. we use beta distribution p b b b b ba b a for convenience since as this is conjugate to the binomial distribution the required integrations are trivial. a reasonable choice for a fair coin is a b as shown in ba b a nh p nh ba b nh bnh a nt b ba b in general pdmf air biased coin for the biased coin we use a bimodal distribution formed for convenience as a mixture of two beta distributions b as shown in the model likelihood pdmbiased is given by p p nh bnh nt nh nh bnh nt assuming no prior preference for either a fair or biased coin pm const. and repeating the above scenario in the discrete parameter case example parameter space. draft march occam s razor and bayesian complexity penalisation figure the likelihood of the total dice score ptn for n to n die. plotted along the horizontal axis is the total score t. the vertical line marks the comparison for pt for the different number of die. the more complex models which can reach more states have lower likelihood due to normalisation over t. heads and tails here pdmf air and pdmbiased the bayes factor is pmf aird pmbiasedd indicating that there is little to choose between the two models. heads and tails here pdmf air and pdmbiased the bayes factor is pmf aird pmbiasedd indicating that have around times the belief in the biased model as opposed to the fair model. occam s razor and bayesian complexity penalisation we return to the dice scenario of there we assumed there are two die whose scores and are not known. only the sum of the two scores t is known. we then computed the posterior joint score distribution for the two die. we repeat the calculation but now for multiple dice and with the twist that we don t know how many dice there only that the sum of the scores is si and are given the value t however we are not told the number of die that is we know t involved n. assuming that any number n is equally likely what is the posterior distribution over n? from bayes rule we need to compute the posterior distribution over models in the above pt pnt ptnpn ptn pt snn i psi i t i si psi description of occam s razor is due to taylan cemgil. draft march a continuous example curve fitting n figure the posterior distribution pnt of the number of die given the observed summed score of where psi for all scores si. by enumerating all states we can explicitly compute ptn as displayed in the important observation is that as the models explaining the data become more complex increases more states become accessible and the probability mass typically reduces. we see this effect at pt where apart from n the value of pt decreases with increasing n since the higher n have mass in more states becoming more spread out. assuming pn const. the posterior pnt is plotted in a posteriori there are only plausible models namely n since the rest are either too complex or impossible. this demonstrates the occam s razor effect which penalises models which are over complex. a continuous example curve fitting consider an additive set of periodic functions cosx wk coskx this can be conveniently written in vector form wt where is a k dimensional vector with elements cosx coskxt and the vector w contains the weights of the additive function. we are given a set of data d yn n n drawn from this distribution where y is the clean corrupted with additive zero mean gaussian noise with variance yn n see assuming i.i.d. data we are interested in the posterior probability of the number of coefficients given the observed data pd we will assume pk const. the likelihood term above is given by the integral pkd pdkpk pd n pxn xn k pwk w xn k pynxn w k for pwk n ik the integrand is a gaussian in w for which it is straightforward to evaluate the integral and log yn k n bta log det a k log k w xn yn yn n for regression under the i.i.d. figure belief network representation of a hierarchical bayesian model data assumption. note that are included to highlight the role of the the intermediate nodes on yn x clean underlying model. n with the intermediate node and place directly arrows from w and xn to yn. since pyw x wtx we can if desired do away n draft march approximating the model likelihood figure the data generated with additive gaussian noise from a k component model. the posterior pkd. the reconstruction of the data using where is the mean posterior vector of the optimal dimensional model pwd k plotted in the continuous line is the reconstruction. plotted in dots is the true underlying clean data. where a i txn b yn assuming and we sampled some data from a model with k components we assume that we know the correct noise level the posterior pkd plotted in is sharply peaked at k which is the correct value used to generate the data. the clean reconstructions for k are plotted in approximating the model likelihood for a model with continuous parameter vector dim k and data d the model likelihood is pdm pd mp for a generic expression pd mp e f unless f is of a particularly simple form in for example one cannot compute the integral in and approximations are required. laplace s method a simple approximation of is given by laplace s method log h log pdm f where is the map solution argmax pd mp and h is the hessian of f at draft march exercises for data d that is i.i.d. generated the above specialises to pdm p pxn md f log p log pxn m in this case laplace s method computes the optimum of the function bayes information criterion for i.i.d. data the hessian scales with the number of training examples n and a crude approximation is to set h nik where k dim in this case one may take as a model comparison procedure the function log pdm log pd m log p k log k log n for a simple prior that penalises the length of the parameter vector p n i the above reduces to log pdm log pd m k log n the bayes information approximates by ignoring the penalty term giving bic log pd m k log n the bic criterion may be used as an approximate way to compare models where the term k log n penalises model complexity. in general the laplace approximation equation is to be preferred to the bic criterion since it more correctly accounts for the uncertainty in the posterior parameter estimate. other techniques that aim to improve on the laplace method are discussed in and exercises exercise write a program to implement the fairbiased coin tossing model selection example of using a discrete domain for explain how to overcome potential numerical issues in dealing with large nh and nt the order of exercise you work at dodder s hedge fund and the manager wants to model next day returns based on current day information xt. the vector of factors each day xt captures essential aspects of the market. he argues that a simple linear model wkxkt should be reasonable and asks you to find the weight vector w based on historical information d t t in addition he also gives you a measure of the volatility t for each day. under the assumption that the returns are i.i.d. gaussian distributed n w pytxt w yt wtxt t explain how to set the weight vector w by maximum likelihood. draft march exercises your hedge fund manager is however convinced that some of the factors are useless for prediction and wishes to remove as many as possible. to do this you decide to use a bayesian model selection method in which you use a prior pwm n i where m indexes the model. each model uses only a subset of the factors. by translating the integer m into a binary vector representation the model describes which factors are to be used. for example if k there would be models where the first model is yt with weight prior n similarly model would be yt with n you decide to use a flat prior pm const. draw the hierarchical bayesian network for this model and explain how to find the best model for the data using bayesian model selection by suitably adapting equation using the data dodder.mat perform bayesian model selection as above for k and find which of the factors are most likely to explain the data. exercise here we will derive the expression and also an alternative form. starting from pynw xn k n i pw yn wt nyn wt e n n wtw e show that this can be expressed as e where e n a i txn wtawbtw n b yn by completing the square derive since each yn n n is linearly related through w and w is gaussian distributed the joint vector yn is gaussian distributed. using the gaussian propagation results derive an alternative expression for log xn draft march part iii machine learning chapter machine learning concepts styles of learning broadly speaking the main two subfields of machine learning are supervised learning and unsupervised learning. in supervised learning the focus is on accurate prediction whereas in unsupervised learning the aim is to find accurate compact descriptions of the data. particularly in supervised learning one is interested in methods that perform well on previously unseen data. that is the method generalises to unseen data. in this sense one distinguishes between data that is used to train a model and data that is used to test the performance of the trained model see supervised learning consider a database of face images each represented by a x. along with each image x is an output class y female that states if the image is of a male or female. a database of such image-class pairs is available d yn n the task is to make an accurate predictor yx of the sex of a novel image x this is an example application that would be hard to program in a traditional programming manner since formally specifying how male faces differ from female faces is difficult. an alternative is to give examples faces and their gender labels and let a machine automatically learn a rule to differentiate male from female faces. definition learning. given a set of data d yn n n the task is to learn the relationship between the input x and output y such that when given a new input x the predicted output y is accurate. to specify explicitly what accuracy means one defines a loss function lypred ytrue or conversely a utility function u l. in supervised learning our interest is describing y conditioned on knowing x. from a probabilistic modelling perspective we are therefore concerned primarily with the conditional distribution pyxd. the term supervised indicates that there is a supervisor specifying the output y for each input x in the available data d. the output is also called a label particularly when discussing classification. predicting tomorrow s stock price yt based on past observations yt is a form of supervised learning. we have a collection of times and prices d yt t t where time t is the input and the price yt is the output. an m n face image with elements fmn we can form a vector by stacking the entries of the matrix. in matlab one may achieve this using xf. train test styles of learning figure in training and evaluating a model conceptually there are two sources of data. the parameters of the model are set on the basis of the train data only. if the test data is generated from the same underlying process that generated the train data an unbiased estimate of the generalisation performance can be obtained by measuring the test data performance of the trained model. importantly the test performance should not be used to adjust the model parameters since we would then no longer have an independent measure of the performance of the model. example a father decides to teach his young son what a sports car is. finding it difficult to explain in words he decides to give some examples. they stand on a motorway bridge and as each car passes underneath the father cries out that s a sports car! when a sports car passes by. after ten minutes the father asks his son if he s understood what a sports car is. the son says sure it s easy an old red vw beetle passes by and the son shouts that s a sports car! dejected the father asks why do you say that? because all sports cars are red! replies the son. this is an example scenario for supervised learning. here the father plays the role of the supervisor and his son is the student learner it s indicative of the kinds of problems encountered in machine learning in that it is not really clear anyway what a sports car is if we knew that then we wouldn t need to go through the process of learning. this example also highlights the issue that there is a difference between performing well on training data and performing well on novel test data. the main interest in supervised learning is to discover an underlying rule that will generalise well leading to accurate prediction on new inputs. for an input x if the output is one of a discrete number of possible classes this is called a classification problem. in classification problems we will generally use c for the output. for an input x if the output is continuous this is called a regression problem. for example based on historical information of demand for sun-cream in your supermarket you are asked to predict the demand for the next month. in some cases it is possible to discretise a continuous output and then consider a corresponding classification problem. however in other cases it is impractical or unnatural to do this for example if the output y is a high dimensional continuous valued vector or if the ordering of states of the variable is meaningful. unsupervised learning definition learning. given a set of data d n n in unsupervised learning we aim to to learn a plausible compact description of the data. an objective is used to quantify the accuracy of the description. in unsupervised learning there is no special prediction variable. from a probabilistic perspective we are interested in modelling the distribution px. the likelihood of the data under the i.i.d. assumption for example would be one objective measure of the accuracy of the description. draft march styles of learning example a supermarket chain wishes to discover how many different basic consumer buying behaviours there are based on a large database of supermarket checkout data. items brought by a customer on a visit to a checkout are represented by a sparse dimensional vector x which contains a in the ith element if the customer bought product i and otherwise. based on million such checkout vectors from stores across the country d n the supermarket chain wishes to discover patterns of buying behaviour. in the table each column represents the buying patterns of a customer customer records and just the first of the products are shown. a indicates that the customer bought that item. we wish to find common patterns in the data such as if someone buys coffee they are also likely to buy milk. coffee tea milk beer diapers aspirin example the table on the right represents a collection of unlabelled two-dimensional points. we can visualise this data by plotting it in dimensions. by simply eye-balling the data we can see that there are two apparent clusters here one centred around and the other around a reasonable model to describe this data might therefore be to describe it as two clusters centred at and each with a standard deviation of around anomaly detection a baby processes a mass of initially confusing sensory data. after a while the baby begins to understand her environment in the sense that novel sensory data from the same environment is familiar or expected. when a strange face presents itself the baby recognises that this is not familiar and may be upset. the baby has learned a representation of the familiar and can distinguish the expected from the unexpected this is an example of unsupervised learning. models that can detect irregular events are used in plant monitoring and require a model of normality which will in most cases be based on unlabelled data. online learning in the above situations we assumed that the data d was given beforehand. in online learning data arrives sequentially and we want to continually update our model as new data becomes available. online learning may occur in either a supervised or unsupervised context. interacting with the environment in many real-world situations an agent is able to interact in some manner with its environment. query learning here the agent has the ability to request data from the environment. for example a predictor might recognise that it is less confidently able to predict in certain regions of the space x and therefore requests more training data in this region. active learning can also be considered in an unsupervised context in which the agent might request information in regions where px looks uninformative or flat draft march supervised learning reinforcement learning one might term this also survival learning one has in mind scenarios such as encountered in real-life where an organism needs to learn the best actions to take in its environment in order to survive as long as possible. in each situation in which the agent finds itself it needs to take an action. some actions may eventually be beneficial to food for example whilst others may be disastrous to being eaten for example. based on accumulated experience the agent needs to learn which action to take in a given situation in order to obtain a desired long term goal. essentially actions that lead to long term rewards need to reinforced. reinforcement learning has connections with control theory markov decision processes and game theory. whilst we discussed mdps and briefly mentioned how an environment can be learned based on delayed rewards in we will not discuss this topic further in this book. semi-supervised learning in machine learning a common scenario is to have a small amount of labelled and a large amount of unlabelled data. for example it may be that we have access to many images of faces however only a small number of them may have been labelled as instances of known faces. in semi-supervised learning one tries to use the unlabelled data to make a better classifier than that based on the labelled data alone. supervised learning supervised and unsupervised learning are mature fields with a wide range of practical tools and associated theoretical analyses. our aim here is to give a brief introduction to the issues and philosophies behind the approaches. we focus here mainly on supervised learning and classification in particular. utility and loss to more fully specify a supervised problem we need to be clear what cost is involved in making a correct or incorrect prediction. in a two class problem domc we assume here that everything we know about the environment is contained in a model px c. given a new input x the optimal prediction also depends on how costly making an error is. this can be quantified using a loss function conversely a utility. in forming a decision function cx that will produce a class label for the new input x we don t know the true class only our presumed distribution pcx the expected utility for the decision function is ctrue ucx uctrue cx and the optimal decision is that which maximises the expected utility. zero-one loss if c uctrue c ctrue for the two class case we then have a count the correct predictions measure of prediction performance is based on the zero-one utility conversely the zero-one loss if c ctrue pctrue for cx pctrue for cx hence in order to have the highest expected utility the decision function cx should correspond to if pc selecting the highest class probability pcx if pc cx ucx in the case of a tie either class is selected at random with equal probability. draft march supervised learning general loss functions in general for a two-class problem we have uctrue c uctrue c for cx uctrue c uctrue c for cx ucx and the optimal decision function cx chooses that class with highest expected utility. one can readily generalise this to multiple-class situations using a utility matrix with elements uij uctrue i cpred j where the i j element of the matrix contains the utility of predicting class j when the true class is i. conversely one could think of a loss-matrix with entries lij uij. the expected loss with respect to pcx is then termed the risk. in some applications the utility matrix is highly non-symmetric. consider a medical scenario in which we are asked to predict whether or not the patient has cancer domc benign. if the true class is cancer yet we predict benign this could have terrible consequences for the patient. on the other hand if the class is benign yet we predict cancer this may be less disastrous for the patient. such asymmetric utilities can bias the predictions in favour of conservative decisions in the cancer case we would be more inclined to decide the sample is cancerous than benign even if the predictive probability of the two classes is equal. what s the catch? in solving for the optimal decision function cx above we are assuming that the model pcx is correct the catch is therefore that in practice we typically don t know the correct model underlying the data all we have is a dataset of examples d yn n n and our domain knowledge. we want our method to perform well not just on a specifically chosen x but any new input that could come along that is we want it to generalise to novel inputs. this means we also need a model for px in order to measure what the expected performance of our decision function would be. hence we require knowledge of the joint distribution pc x pcxpx. we therefore need to form a distribution px cd which should ideally be close to the true but unknown joint data distribution. communities of researchers in machine learning form around different strategies to address the lack of knowledge about the true pc x. using the empirical distribution a direct approach to not knowing the correct model ptruec x is to replace it with the empirical distribution px cd n xn cn n that is we assume that the underlying distribution is approximated by placing equal mass on each of the points cn in the dataset. using this gives the empirical utility n or conversely the empirical risk n r n lcn cxn draft march ucn cxn supervised learning train validate test figure models can be trained using the train data based on different regularisation parameters. the optimal regularisation parameter is determined by the empirical performance on the validation data. an independent measure of the generalisation performance is obtained by using a separate test set. assuming the loss is minimal when the correct class is predicted the optimal decision cx for any input in the train set is trivially given by cxn cn. however for any new x not contained in d then cx is undefined. in order to define the class of a novel input one may use a parametric function for example for a two class problem domc a linear decision function is given by cx fx if tx if tx fx if the vector input x is on the positive side of a hyperplane defined by the vector and bias we assign it to class otherwise to class return to the geometric interpretation of this in the empirical risk then becomes a function of the parameters n r n lcn fxn the optimal parameters are given by minimising the empirical risk with respect to opt argmin r the decision for a new datapoint x is then given by fx opt. in this empirical risk minimisation approach as we make the decision function fx more complex the empirical risk goes down. if we make fx too complex we will have no confidence fx will perform well on a novel input x to constrain the complexity of fx we may minimise the penalised empirical risk r r p for the linear decision function above it is reasonable to penalise wildly changing classifications in the sense that if we change the input x by only a small amount we expect average minimal change in the class label. the squared difference in tx for two inputs and t where x by constraining the length of to be small we would then limit the ability of the classifier to change class for only a small change in input this motivates a penalised risk of the form r r t where is a regularising constant. we subsequently minimise this penalised empirical risk with respect to we discuss how to find an appropriate setting for the regularisation constant below. validation in penalised empirical risk minimisation we need to set the regularisation parameter this can be achieved by evaluating the performance of the learned classifier fx on validation data dvalidate for several different values and choosing the one with the best performance. it s important that the validation data is not the data on which the model was trained since we know that the optimal setting for in that case is zero and again we will have no confidence in the generalisation ability. the distance between two datapoints is distributed according to an isotropic multivariate gaussian with zero t motivating the choice of the euclidean squared mean and covariance the average squared change is length of the parameter as the penalty term. t draft march supervised learning algorithm setting regularisation parameters using cross-validation. choose a set of training and validation set choose a set of regularisation parameters a di traindi i i validate for a to a do for i to i do a argmin i train ap l a end for l a i end for opt argmin a r i validate adi train and validation di given an original dataset d we split this into disjoint parts dtraindvalidate where the size of the validation set is usually chosen to be smaller than the train set. for each parameter a one then finds the minimal empirical risk parameter a. this splitting procedure is repeated each time producing a separate training di validation set along with an optimal penalised empirical risk parameter i a and associated validation performance r i validate. the performance of regularisation parameter a is taken as the average of the validation performances over i. the best regularisation parameter is then given as that with the minimal average validation error see and using the optimal regularisation parameter many practitioners retrain on the basis of the whole dataset d. in cross-validation a dataset is partitioned into training and validation sets multiple times with validation results obtained for each partition. more specifically in k-fold cross validation the data d is split into k validate. this gives a total of equal sized disjoint parts then di k different training-validation sets over which performance is averaged see in practice cross validation is popular as is leave-one-out cross validation in which the validation sets consist of only a single example. validate di and di train ddi adi benefits of the empirical risk approach for a utility uctrue cpred and penalty p the empirical risk approach is summarised in in the limit of a large amount of training data the empirical distribution will tend towards the correct distribution. the discriminant function is chosen on the basis of minimal risk which is the quantity we are ultimately interested in. the procedure is conceptually straightforward. train validate train validate train validate train test draft march figure in cross-validation the original dataset is split into several train-validation sets. depicted is cross-validation. for a range of regularisation parameters the optimal regularisation parameter is found based on the empirical validation performance averaged across the different splits. x px c x c fx uc fx p supervised learning figure empirical risk approach. given the dataset x a model of the data px c is made usually using the empirical distribution. for a classifier fx the parameter is learned by maximising the penalised empirical utility minimising empirical risk with respect to the penalty parameter is set by validation. a novel input x is then assigned to class fx given this optimal figure the unregularised fit to training given by whilst the training data is well the regularised fit whilst the fitted the error on the validation examples is high. train error is high the validation error is all important is low. the true function which generated this noisy data is the dashed line the function learned from the data is given by the solid line. drawbacks of the empirical risk approach it seems extreme to assume that the data follows the empirical distribution particularly for small amounts of training data. to generalise well we need to make sensible assumptions as to px that is the distribution for all x that could arise. if the utility loss function changes the discriminant function needs to be retrained. some problems require an estimate of the confidence of the prediction. whilst there may be heuristic ways to evaluating confidence in the prediction this is not inherent in the framework. when there are multiple penalty parameters performing cross validation in a discretised grid of the parameters becomes infeasible. it seems a shame to discard all those trained models in cross-validation can t they be combined in some manner and used to make a better predictor? example a good regularisation parameter. in we fit the function a sinwx to data learning the parameters a and w. the unregularised solution badly overfits the data and has a high validation error. to encourage a smoother solution a regularisation term ereg is used. the validation error based on several different values of the regularisation parameter was computed finding that gave a low validation error. the resulting fit to novel data is reasonable. bayesian decision approach an alternative to using the empirical distribution is to fit a model pc x to the train data d. given this model the decision function cx is automatically determined from the maximal expected utility draft march supervised learning x px c x pcx uc c c figure bayesian decision approach. a model px c is fitted to the data. after leaning this model is used to compute pcx for a novel x we then find the distribution of the assumed truth pcx the prediction is then given by that c which maximises the expected utility c minimal risk with respect to this model as in equation in which the unknown pctruex is replaced with pcx this approach therefore divorces learning the parameters of pc x from the utility loss. benefits of the bayesian decision approach this is a conceptually clean approach in which one tries ones best to model the environment independent of the subsequent decision process. in this case learning the environment is separated from the ultimate effect this will have on the expected utility. the ultimate decision c for a novel input x can be a highly complex function of x due to the maximisation operation. drawbacks of the bayesian decision approach if the environment model pc x is poor the prediction c could be highly inaccurate since mod elling the environment is divorced from prediction. to avoid fully divorcing the learning of the model pc x from its effect on decisions in practice one often includes regularisation terms in the environment model pc x which are set by validation based on an empirical utility. there are two main approaches to fitting pc x to data d. we could parameterise the joint distribution using pc x pcx cxpx x discriminative approach or pc x pxc xcpc c generative approach we ll consider these two approaches below in the context of trying to make a system that can distinguish between a male and female face. we have a database of face images in which each image is represented as a real-valued vector xn n n along with a label cn stating if the image is male or female. generative approach px c pxc xcpc c for simplicity we use maximum likelihood training for the parameters assuming the data d is i.i.d. we have a log likelihood log pd log pxncn xc n n log pcn c as we see the dependence on xc occurs only in the first term and c only occurs in the second. this means that learning the optimal parameters is equivalent to isolating the data for the male-class and draft march supervised learning c cn xc cx x xn n cn xn n figure two generic strategies for probabilisa class dependent generative tic classification. model of x. after learning parameters classification is obtained by making x evidential and inferring a discriminative classification method pcx. pcx. fitting a model pxc male xmale. we similarly isolate the female data and fit a separate model pxc female xfemale. the class distribution pc c can be easily set according to the ratio of malesfemales in the set of training data. to make a classification of a new image x as either male or female we may use pc malex px c male xmale px c male xmale px c female xfemale based on zero-one loss if this probability is greater than we classify x as male otherwise female. more generally we may use this probability as part of a decision process as in equation advantages prior information about the structure of the data is often most naturally specified through a generative model pxc. for example for male faces we would expect to see heavier eyebrows a squarer jaw etc. disadvantages the generative approach does not directly target the classification model pcx since the goal of generative training is rather to model pxc. if the data x is complex finding a suitable generative data model pxc is a difficult task. on the other hand it might be that making a model of pcx is simpler particularly if the decision boundary between the classes has a simple form even if the data distribution of each class is complex see furthermore since each generative model is separately trained for each class there is no competition amongst the models to explain the data. discriminative approach pc x pcx cxpx x assuming i.i.d. data the log likelihood is log pd log pcnxn cx n n n log pxn x the parameters are isolated in the two terms so that maximum likelihood training is equivalent to finding the parameters of cx that will best predict the class c for a given training input x. the parameters x for modelling the data occur separately in the second term above and setting them can therefore be treated as a separate unsupervised learning problem. this approach therefore isolates modelling the decision boundary from modelling the input distribution see classification of a new point x is based on pcx opt cx as for the generative case this approach still learns a joint distribution pc x pcxpx which can be used as part of a decision process if required. advantages the discriminative approach directly addresses making an accurate classifier based on pcx modelling the decision boundary as opposed to the class conditional data distribution in the generative approach. whilst the data from each class may be distributed in a complex way it could be that the decision boundary between them is relatively easy to model. draft march supervised learning figure each point represents a high dimensional vector with an associated class label either male or female. the point x is a new point for which we would like to predict whether this should be male or female. in the generative approach a male model pxmale generates data similar to the m points. similarly the female model pxfemale generates points that are similar to the f points above. we then use bayes rule to calculate the probability pmalex using the two we directly make a model of pmalex which cares less about how the points m or f are distributed but more about describing the boundary which can separate the two classes as given by the line. fitted models as given in the text. in the discriminative approach ch h xh cn hn xn n figure a strategy for semi-supervised learning. when cn is missing the term pcnhn is absent. the large amount of training data helps the model learn a good lower dimensioncompressed representation h of the data x. fitting then a classification model pch using this lower dimensional representation may be much easier than fitting a model directly from the complex data to the class pcx. disadvantages discriminative approaches are usually trained as black-box classifiers with little prior knowledge built in as to how data for a given class might look. domain knowledge is often more easily expressed using the generative framework. hybrid generative-discriminative approaches one could use a generative description pxc building in prior information and use this to form a joint distribution px c from which a discriminative model pcx may be formed using bayes rule. specifically we can use pxc xcpc c c pxc xcpc c pcx and use a separate model for px x. subsequently the parameters xc c for this hybrid model can be found by maximising the probability of being in the correct class. this approach would appear to leverage the advantages of both the discriminative and generative frameworks since we can more readily incorporate domain knowledge in the generative model pxc xc yet train this in a discriminative way. this approach is rarely taken in practice since the resulting functional form of the likelihood depends in a complex manner on the parameters. in this case no separation occurs was previously the case for the generative and discriminative approaches. learning lower-dimensional representations in semi-supervised learning one way to exploit a large amount of unlabelled training data to improve classification modelling is to try to find a lower dimensional representation h of the data x. based on this the mapping from h to c may be rather simpler to learn than a mapping from x to c directly. to do so we can form the likelihood using see pcx n opt argmax pcx h draft march and then set any parameters for example by using maximum likelihood pxnhn xhph h bayes versus empirical decisions features and preprocessing it is often the case that when attempting to make a predictive model transforming the raw input x into a form that more directly captures the relevant label information can greatly improve performance. for example in the male-female classification case it might be that building a classifier directly in terms of the elements of the face vector x is difficult. however using features which contain geometric information such as the distance between eyes width of mouth etc. may make finding a classifier easier. in practice data is often preprocessed to remove noise centre an image etc. bayes versus empirical decisions the empirical risk and bayesian approaches are at the extremes of the philosophical spectrum. in the empirical risk approach one makes a seemingly over-simplistic data generating assumption. however decision function parameters are set based on the task of making decisions. on the other hand the bayesian approach attempts to learn pc x without regard to its ultimate use as part of a larger decision process. what objective criterion can we use to learn pc x particularly if we are only interested in classification with a low test-risk? the following example is intended to recapitulate the two generic bayes and empirical risk approaches we ve been considering. example two generic decision strategies. consider a situation in which based on patient information x we need to take a decision d as whether or not to operate. the utility of operating ud c depends on whether or not the patient has cancer. for example uoperate cancer udon t operate cancer udon t operate benign uoperate benign we have independent true assessments of whether or not a patient had cancer giving rise to a set of historical records d cn n n. faced with a new patient with information x we need to make a decision whether or not to operate. in the bayesian decision approach one would first make a model pcxd example logistic regression. using this model the decision is given by that which maximises the expected utility d argmax d cancer pbenignxdud benign in this approach learning the model pcxd is divorced from the ultimate use of the model in the decision making process. an advantage of this approach is that from the viewpoint of expected utility it is optimal provided the model pcxd is correct unfortunately this is rarely the case. given the limited model resources it might make sense to focus on ensuring the prediction of cancer is correct since this has a more significant effect on the utility. however formally this is not possible in this framework. the alternative empirical utility approach recognises that the task can be stated as to translate patient information x into an operation decision d. to do so one could parameterise this as dx fx and then learn under maximising the empirical utility ufxn cn for example if x is a vector representing the patient information and the parameter we might use a linear decision function such as tx d operate tx d don t operate u n fx the advantage of this approach is that the parameters of the decision are directly related to the utility of making the decision. a disadvantage is that we cannot easily incorporate domain knowledge into the decision function. it may be that we have a good model of pcx and would wish to make use of this. draft march bayesian hypothesis testing for outcome analysis both approaches are heavily used in practice and which is to be preferred depends very much on the problem. whilst the bayesian approach appears formally optimal it is prone to model mis-specification. a pragmatic alternative bayesian approach is to fit a parameterised distribution pc x to the data d where penalises complexity of the fitted distribution setting using validation on the risk. this has the potential advantage of allowing one to incorporate sensible prior information about pc x whilst assessing competing models in the light of their actual predictive risk. similarly for the empirical risk approach one can modify the extreme empirical distribution assumption by using a more plausible model px c of the data. representing data the numeric encoding of data can have a significant effect on performance and an understanding of the options for representing data is therefore of considerable importance. categorical for categorical nominal data the observed value belongs to one of a number of classes with no intrinsic ordering of the classes. an example of a categorical variable would be the description of the type of job that someone does e.g. healthcare education financial services transport homeworker unemployed engineering etc. one way to transform this data into numerical values would be to use encoding. here s an example there are kinds of jobs soldier sailor tinker spy. a soldier is represented as a sailer as a tinker as and a spy as in this encoding the distance between the vectors representing two different professions is constant. it is clear that encoding induces dependencies in the profession attributes since if one of the profession attributes is the others must be zero. ordinal an ordinal variable consists of categories with an ordering or ranking of the categories e.g. cold cool warm hot. in this case to preserve the ordering we could perhaps use for cold for cool for warm and for hot. this choice is somewhat arbitrary and one should bear in mind that results will generally be dependent on the numerical coding used. numerical numerical data takes on values that are real numbers e.g. a temperature measured by a thermometer or the salary that someone earns. bayesian hypothesis testing for outcome analysis how can we assess whether two classifiers are performing differently? for techniques which are based on bayesian classifiers pc m there will always be in principle a direct way to estimate the suitability of the model m by computing pmd. we consider here the less fortunate situation where the only information presumed available is the test performance of the two classifiers. to outline the basic issue let s consider two classifiers a and b which predict the class of test examples. classifier a makes errors and correct classifications whereas classifier b makes errors and correct classifications. is classifier a better than classifier b? our lack of confidence in pronouncing that a is better than b results from the small number of test examples. on the other hand if classifier a makes errors and correct classifications whilst classifier b makes errors and correct classifications intuitively we would be more confident that classifier a is better than classifier b. perhaps the most practically relevant question from a machine learning perspective is the probability that classifier a outperforms classifier b given the available test information. whilst this question can draft march bayesian hypothesis testing for outcome analysis be addressed using a bayesian procedure we first focus on a simpler question namely whether classifier a and b are the outcome analysis the treatment in this section refers to outcomes and quantifies if data is likely to come from the same multinomial distribution. in the main we will apply this to assessing if two classifiers are essentially performing the same although one should bear in mind that the method applies more generally to assessing if outcomes are likely to have been generated from the same or different underlying processes. consider a situation where two classifiers a and b have been tested on some data so that we have for each example in the test set an outcome pair obn n n where n is the number of test data points and oa q similarly for ob. that is there are q possible types of outcomes that can occur. for example for binary classification we will typically have the four cases domo falsepositive truenegative falsenegative if the classifier predicts class c false and the truth is class t false these are defined as truepositive falsepositive truenegative falsenegative c true c true c false c false t true t false t false t true we call oa n n the outcomes for classifier a and similarly for ob n n for classifier b. to be specific we have two hypotheses we wish to test hdiff oa and ob are from different categorical distributions. hsame oa and ob are from the same categorical distribution. in both cases we will use categorical models poc q h c q with unknown parameters will correspond to using the same parameters a b for both classifiers and hypothesis to using different parameters as we will discuss below. in the bayesian framework we want to find how likely it is that a modelhypothesis is responsible for generating the data. for any hypothesis h calculate phoa ob poa obhph poa ob where ph is our prior belief that h is the correct hypothesis. note that the normalising constant poa ob does not depend on the hypothesis. under all hypotheses we will make the independence of trials assumption poa obh poan obnh. to make further progress we need to state what the specific hypotheses mean. draft march bayesian hypothesis testing for outcome analysis oa ob oa ob p oa ob figure hdiff corresponds to the outcomes for the two classifiers being independently genb hsame both outcomes are generated erated. hdep the outfrom the same distribution. comes are dependent correlated hdiff model likelihood we now use the above assumptions to compute the hypothesis likelihood phdiffoa ob poa obhdiffphdiff poa ob the outcome model for classifier a is specified using parameters giving poa hdiff and similarly we use for classifier b. the finite amount of data means that we are uncertain as to these parameter values and therefore the joint term in the numerator above is poa obphdiffoa ob poa ob hdiffp d phdiff where we assumed poa hdiffp pob hdiffp p p and poa ob hdiff poa hdiffpob hdiff note that one might expect there to be a specific constraint that the two models a and b are different. however since the models are assumed independent and each has parameters sampled from an effectively infinite set and are continuous the probability that sampled parameters and of the two models are the same is zero. since we are dealing with categorical distributions it is convenient to use the dirichlet prior which is conjugate to the categorical distribution zu q p uq q zu uq the prior hyperparameter u controls how strongly the mass of the distribution is pushed to the corners of the simplex see setting uq for all q corresponds to a uniform prior. the likelihood of oa is given poa hdiffp q q zu q q uq q d zu zu where is a vector with components hence q being the number of times that variable a is is state q in the data. phdiffoa ob phdiff zu zu zu zu where zu is given by equation hsame model likelihood in hsame the hypothesis is that the outcomes for the two classifiers are generated from the same categorical distribution. hence poa obphsameoa ob phsame poa hsamepob hsamep phsame zu zu draft march bayesian hypothesis testing for outcome analysis bayes factor if we assume that we have no prior preference for either hypothesis phsame then phdiffoa ob phsameoa ob zu zuzu this is the evidence to suggest that the data were generated by two different categorical distributions. example two people classify the expression of each image into happy sad or normal using states respectively. each column of the data below represents an image classed by the two people is the top row and person the second row. are the two people essentially in agreement? to help answer this question we perform a hdiff versus hsame test. from this data the count vector for person is and for person based on a flat prior for the categorical distribution and assuming no prior preference for either hypothesis we have the bayes factor ppersons and classify differently ppersons and classify the same where the z function is given in equation this is strong evidence the two people are classifying the images differently. below we discuss some further examples for the hdiff versus hsame test. as above the only quantities we need for this test are the vector counts from the data. let s assume that there are three kinds of outcomes q for example domo bad ugly are our set of outcomes and we want to test if two classifiers are essentially producing the same outcome distributions or different. example versus hsame. we have the two outcome counts and then the bayes factor equation is strong evidence in favour of the two classifiers being different. alternatively consider the two outcome counts and then the bayes factor equation is weak evidence against the two classifiers being different. as a final example consider counts and this gives a bayes factor equation of strong evidence that the two classifiers are statistically the same. in all cases the results are consistent with the model in fact used to generate the count data the two outcomes for a and b were indeed from different categorical distributions. the more test data we have the more confident we are in our statements. dependent outcome analysis here we consider the more common case that outcomes are dependent. for example it is often the case that if classifier a works well then classifier b will also work well. thus we want to evaluate the draft march bayesian hypothesis testing for outcome analysis hypothesis hdep the outcomes that the two classifiers make are dependent to do so we assume a categorical distribution over the joint states poan obnp hdep poa i ob j here p is a q q matrix of probabilities so is the probability that a makes outcome i and b makes outcome j. then pohdep po phdepdp pop hdeppphdepdp where for convenience we write o ob. assuming a dirichlet prior on p with hyperparameters u we have pophdepo phdep zvec zvecu where vecd is a vector formed from concatenating the rows of the matrix d. here is the count matrix with equal to the number of times that joint outcome i ob j occurred in the n datapoints. as before we can then use this in a bayes factor calculation. for the uniform prior i j. testing for dependencies in the outcomes hdep versus hdiff to test whether or not the outcomes of the classifiers are dependent hdep against the hypothesis that they are independent hdiff we may use assuming phdiff phdep phdiffo phdepo zu zu zu zvecu zu zvec example versus hdiff. consider the outcome count matrix so that and then phdiffo phdepo strong evidence that the classifiers perform independently. consider the outcome count matrix so that and then phdiffo phdepo strong evidence that the classifiers perform dependently. these results are in fact consistent with the way the data was generated in each case. draft march bayesian hypothesis testing for outcome analysis testing for dependencies in the outcomes hdep versus hsame in practice it is reasonable to believe that dependencies are quite likely in the outcomes that classifiers. for example two classifiers will often do well on easy test examples and badly on difficult examples. are these dependencies strong enough to make us believe that the outcomes are coming from the same process? in this sense we want to test phsameo phdepo zu zu zvecu zvec example versus hsame. consider an experiment which gives the test outcome count matrix so that and then phsameo phdepo strong evidence that the classifiers are performing differently. consider an experiment which gives the test outcome count matrix so that and then phsameo phdepo strong evidence that the classifiers are performing the same. these results are in fact consistent with the way the data was generated. is classifier a better than b? we return to the question with which we began this outcome analysis. given the common scenario of observing a number of errors for classifier a on a test set and a number for b can we say which classifier is better? this corresponds to the special case of binary classes q with dome incorrect. under the hdiff for this special case it makes sense to use a beta distribution corresponds to the dirichlet when q then for a being the probability that classifier a generates a correct label we have poa a similarly correct a incorrect pob b correct b incorrect we assume independent identical beta distribution priors p a b p b b draft march code figure two classifiers a and b and their posterior distributions of the probability that they for a with correct and incorrect labels classify correctly a uniform beta prior. b curve b with correct incorrect b curve. px y for a with correct and incorrect labels curve b b with correct incorrect b curve px y as the amount of data increases the overlap between the distributions decreases and the certainty that one classifier is better than the other correspondingly increases. where a flat prior corresponds to using the hyperparameter setting the posterior distributions for a and b are independent p aoa b correct correct incorrect the question of whether a is better than b can then be addressed by computing incorrect p bob b xa xb ba bbc d yc yd dydx x p a boa ob where a correct b incorrect c correct d incorrect example classifier a makes errors and correct classifications whereas classifier b makes errors and correct classifications. using a flat prior this gives p a boa ob on the other hand if classifier a makes errors and correct classifications whilst classifier b makes errors and correct classifications we have p a boa ob this demonstrates the intuitive effect that even though the proportion of correctincorrect classifications doesn t change for the two scenarios as we have more data our confidence in determining the better classifier increases. code demobayeserroranalysis.m demo for bayesian error analysis betaxbiggery.m px y for x b b y b d draft march notes a general introduction to machine learning is given in an excellent reference for bayesian decision theory is approaches based on empirical risk are discussed in exercises with exercises exercise given the distributions n corresponding prior occurrence of classes and calculate the decision boundary explicitly as a function of how many solutions are there to the decision boundary and are they all reasonable? exercise suppose that instead of using the bayes decision rule to choose class k if pckx pcjx for all j k we use a randomized decision rule choosing class j with probability qcjx. calculate the error for this decision rule and show that the error is minimized by using bayes decision rule. and n and class has the distribution pxc n exercise consider datapoints generated from two different classes. class has the distribution pxc n probabilities of each class are pc pc show that the posterior probability pc is of the form the prior pc exp b and determine a and b in terms of and exercise wowco.com is a new startup prediction company. after years of failures they eventually find a neural network with a trillion hidden units that achieves zero test error on every learning problem posted on the internet up last week. each learning problem included a training and test set. proud of their achievement they market their product aggressively with the claim that it predicts perfectly on all known problems would you buy this product? justify your answer. exercise three people classify images into of three categories. each column in the table below represents the classifications of each image with the top row being the class from person the middle from person and the bottom from person assuming no prior preference amongst hypotheses and a uniform prior on counts compute ppersons and classify differently ppersons and classify the same exercise than random guessing?. consider a classifier that makes r correct classifications and w wrong classifications. is the classifier better than random guessing? let d be the fact that there are r right and w wrong answers. assume also that the classifications are i.i.d. show that under the hypothesis the data is generated purely at random pdhrandom define to be the probability that the classifier makes an error. then pd r draft march exercises then consider pdhnon random pd show that for a beta prior p b b pdhnon random br a w b ba b where ba b is the beta-function. considering the random and non-random hypotheses as a priori equally likely show that phrandomd braw bab for a flat prior a b compute the probability that for correct and incorrect classifications the data is from a purely random distribution to equation repeat this for correct and incorrect classifications. show that the standard deviation in the number of errors of a random classifier is r w and relate this to the above computation. exercise for a prediction model pyx and true data generating distribution px y we define the accuracy as a px y pyx xy you are given a set of training data d yn n n. by taking qx y to be the empirical this shows that the prediction accuracy is lower bounded by the training accuracy and the gap between the empirical distribution and the unknown true data generating mechanism. in theories such as pac bayes one may bound this gap resulting in a bound on the predictive accuracy. according to this naive bound the best thing to do to increase the prediction accuracy is to increase the training accuracy the first kullback-leibler term is independent of the predictor. as n increases the first term kullback-leibler term becomes small and minimising the training error is justifiable. draft march by defining px y pyx a px y and considering klqx y px y show that for any distribution qx y log a klqx ypx y distribution qx y n show that xn yn log a klqx ypx y n log pynxn assuming that the training data are drawn from a distribution pyx which is deterministic show that exercises log a klqxpx n log pynxn and hence that provided the training data is correctly predicted pynxn the accuracy can be related to the empirical input distribution and true input distribution by klqxpx a e draft march chapter nearest neighbour classification do as your neighbour does successful prediction typically relies on smoothness in the data if the class label can change arbitrarily as we move a small amount in the input space the problem is essentially random and no algorithm will generalise well. machine learning researchers therefore construct appropriate measures of smoothness for the problem they have at hand. nearest neighbour methods are a good starting point since they readily encode basic smoothness intuitions and are easy to program forming a useful baseline method. in a classification problem each input vector x has a corresponding class label cn c. given a dataset of n such training examples d cn n n and a novel x we aim to return the correct class cx. a simple but often effective strategy for this supervised learning problem can be stated as for novel x find the nearest input in the training set and use the class of this nearest input for vectors x and representing two different datapoints we measure nearness using a dissimilarity function dx a common dissimilarity is the squared euclidean distance dx which can be more conveniently written based on the squared euclidean distance the decision boundary is determined by the lines which are the perpendicular bisectors of the closest training points with different training labels see this is called a voronoi tessellation. the nearest neighbour algorithm is simple and intuitive. there are however some issues how should we measure the distance between points? whilst the euclidean square distance is popular this may not always be appropriate. a fundamental limitation of the euclidean distance is that it does not take into account how the data is distributed. for example if the length scales of x vary greatly the largest length scale will dominate the squared distance with potentially useful class-specific information in other components of x lost. the mahalanobis distance dx where is the covariance matrix of the inputs all classes can overcome some of these problems since it rescales all length scales to be essentially equal. the whole dataset needs to be stored to make a classification. this can be addressed by a method called data editing in which datapoints which have little or no effect on the decision boundary are removed from the training dataset. k-nearest neighbours figure in nearest neighbour classification a new vector is assigned the label of the nearest vector in the training set. here there are three classes with training points given by the circles along with their class. the dots indicate the class of the nearest training vector. the decision boundary is piecewise linear with each segment corresponding to the perpendicular bisector between two datapoints belonging to different classes giving rise to a voronoi tessellation of the input space. algorithm nearest neighbour algorithm to classify a new vector x given a set of training data d cn n n calculate the dissimilarity of the test point x to each of the stored points dn d xn n n find the training point xn which is nearest to x n argmin n d xn assign the class label cx cn in the case that there are two or more equidistant neighbours with different class labels the most numerous if there is no one single most numerous class we use the k-nearest-neighbours case class is chosen. described in the next section. each distance calculation can be expensive if the datapoints are high dimensional. principal components analysis see is one way to address this and replaces xn with a low dimensional projection p. the euclidean distance of two is then approximately given by see this is both faster to compute and can also improve classification accuracy since only the large scale characteristics of the data are retained in the pca projections. it is not clear how to deal with missing data or incorporate prior beliefs and domain knowledge. for large databases computing the nearest neighbour of a novel point x can be very time-consuming since x needs to be compared to each of the training points. depending on the geometry of the training points finding the nearest neighbour can accelerated by examining the values of each of the components xi of x in turn. such an axis-aligned space-split is called a and can reduce the possible set of candidate nearest neighbours in the training set to the novel x particularly in low dimensions. k-nearest neighbours basing the classification on only the single nearest neighbour can lead to inaccuracies. if your neighbour is simply mistaken an incorrect training class label or is not a particularly representative example of his class then these situations will typically result in an incorrect classification. by including more than the single nearest neighbour we hope to make a more robust classifier with a smoother decision boundary swayed by single neighbour opinions. for datapoints which are somewhat anomalous compared with draft march a probabilistic interpretation of nearest neighbours figure in k-nearest neighbours we centre a hypersphere around the point we wish to classify the central dot. the inner circle corresponds to the nearest neighbour. however using the nearest neighbours we find that there are two blue classes and one red and we would therefore class the point as blue class. in the case of a tie one can extend k until the tie is broken. neighbours from the same class their influence will be outvoted. if we assume the euclidean distance as the dissimilarity measure the k-nearest neighbour algorithm considers a hypersphere centred on the test point x. we increase the radius r until the hypersphere contains exactly k points in the training data. the class label cx is then given by the most numerous class within the hypersphere. choosing k whilst there is some sense in making k there is certainly little sense in making k n being the number of training points. for k very large all classifications will become the same simply assign each novel x to the most numerous class in the training data. this suggests that there is an optimal intermediate setting of k which gives the best generalisation performance. this can be determined using cross-validation as described in example digit example. consider two classes of handwritten digits zeros and ones. each digit contains pixels. the training data consists of zeros and ones a subset of which are plotted in to test the performance of the nearest neighbour method on euclidean distance we use an independent test set containing a further digits. the nearest neighbour method applied to this data correctly predicts the class label of all test points. the reason for the high success rate is that examples of zeros and ones are sufficiently different that they can be easily distinguished. a more difficult task is to distinguish between ones and sevens. we repeat the above experiment now using training examples of ones and training examples of sevens again new test examples ones and sevens were used to assess the performance. this time errors are found using nearest neighbour classification a error rate for this two class problem. the test points on which the nearest neighbour method makes errors are plotted in if we use k nearest neighbours the classification error reduces to a slight improvement. as an aside the best machine learning methods classify real world digits all classes to an error of less than better than the performance of an average human. a probabilistic interpretation of nearest neighbours consider the situation where we have simplicity data from two classes class and class we make the following mixture model for data from class e xn pxc n n class n class where d is the dimension of a datapoint x and are the number of training datapoints of class and is the variance. this is a parzen estimator which models the data distribution as a uniform weighted sum of distributions centred on the training points draft march a probabilistic interpretation of nearest neighbours figure some of the training examples of the digit zero and one and seven there are training examples of each of these three digit classes. figure versus classification using the nn method. the out of test examples that are incorrectly classified the nearest neighbours in the training set corresponding to each testpoint above. n class e similarly for data from class xn pxc to classify a new datapoint x we use bayes rule n class px n pc px px the maximum likelihood setting of pc is and pc an analogous expression to equation holds for pc to see which class is most likely we may use the ratio pc pc px px if this ratio is greater than one we classify x as otherwise is a complicated function of x however if is very small the numerator which is a sum of exponential terms will be dominated by that term for which datapoint in class is closest to the point x similarly the denominator will be dominated by that datapoint in class which is closest to x in this case therefore pc pc e e e e taking the limit with certainty we classify x as class if x has a point in the class data which is closer than the closest point in the class data. the nearest neighbour method is therefore recovered as the limiting case of a probabilistic generative model see the motivation of using k nearest neighbours is to produce a result that is robust against unrepresentative nearest neighbours. to ensure a similar kind of robustness in the probabilistic interpretation we may use a finite value this smoothes the extreme probabilities of classification and means that more points just the nearest will have an effective contribution in equation the extension to more than draft march exercises figure a probabilistic interpretation of nearest neighbours. for each class we use a mixture of gaussians to model the data from that class pxc placing at each training point an isotropic gaussian of width in the limit the width of each gaussian is represented by the circle. a novel point is assigned the class of its nearest neighbour. for finite the influence of non-nearest neighbours has an effect resulting in a soft version of nearest neighbours. two classes is straightforward requiring a class conditional generative model for each class. to go beyond nearest neighbour methods we can relax the assumption of using a parzen estimator and use a richer generative model. we will examine such cases in some detail in later chapters in particular when your nearest neighbour is far away for a novel input x that is far from all training points nearest neighbours and its soft probabilistic variant will confidently classify x as belonging to the class of the nearest training point. this is arguably opposite to what we would like namely that the classification should tend to the prior probabilities of the class based on the number of training data per class. a way to avoid this problem is for each class to include a fictitious mixture component at the mean of all the data with large variance equal for each class. for novel inputs close to the training data this extra fictitious datapoint will have no appreciable effect. however as we move away from the high density regions of the training data this additional fictitious component will dominate. since the distance from x to each fictitious class point is the same in the limit that x is far from the training data the effect is that no class information from the position of x occurs. see for an example. code nearneigh.m k nearest neighbour utility routines majority.m find the majority entry in each column of a matrix demonstration demonearneigh.m k nearest neighbour demo exercises exercise the file nndata.mat contains training and test data for the handwritten digits and using leave one out cross-validation find the optimal k in k-nearest neighours and use this to compute the classification accuracy of the method on the test data. exercise write a routine softnearneighxtrainxtesttrainlabelssigma to implement soft nearest neighbours analogous to nearneigh.m. here sigma is the variance in equation as above the file nndata.mat contains training and test data for the handwritten digits and using leave one out cross-validation find the optimal and use this to compute the classification accuracy of the method on the test data. hint you may have numerical difficulty with this method. to avoid this consider using the logarithm and how to numerically compute for large a and b. see also logsumexp.m. draft march exercise in the text we suggested the use of the mahalanobis distance dx y yt y exercises as a way to improve on the euclidean distance with the covariance matrix of the combined data from both classes. consider a modification based on using a mixture model n class n class and pxc pxc n xn n xn explain how the soft nearest neighbours algorithm can deal with the issue that the distribution of data from the different classes can be very different. for the case pc pc log and and small derive a simple expression that approximates exercise the editor at yoman! mens magazine has just had a great idea. based on the success of a recent national poll to test iq she decides to make a beauty quotient test. she collects as many images of male faces as she can taking care to make sure that all the images are scaled to roughly the same size and under the same lighting conditions. she then gives each male face a bq score from severely aesthetically challenged to generously aesthetically gifted thus for each image x there is an associated value b in the range to in total she collects n images and associated scores bn n n where each image is represented by a d-dimensional real-valued vector x. one morning she bounces into your office and tells you the good news it is your task to make a test for the male nation to determine their beauty quotient. the idea she explains is that a man can send online an image of their face x to yoman! and will instantly receive an automatic bq response b as a first step you decide to use the k nearest neighbour method to assign a bq score b to a novel test image x describe how to determine the optimal number of neighbours k to use. your line manager is pleased with your algorithm but is disappointed that it does not provide any simple explanation of beauty that she can present in a future version of yoman! magazine. to address this you decide to make a model based on linear regression. that is b wtx where w is a parameter vector chosen to minimise ew n bn after training a suitable w how can yoman! explain to its readership in a simple way what facial features are important for determining one s bq? describe fully and mathematically a method to train this linear regression model. your expla nation must be detailed enough so that a programmer can directly implement it. discuss any implications of the situation d n. discuss any advantagesdisadvantages of using the linear regression model compared with using the knn approach. draft march chapter unsupervised linear dimension reduction high-dimensional spaces low dimensional manifolds in machine learning problems data is often high dimensional images bag-of-word descriptions geneexpresssions etc. in such cases we cannot expect the training data to densely populate the space meaning that there will be large parts in which little is known about the data. for the hand-written digits from the data is dimensional. for binary valued pixels the possible number of images that could ever exist is nevertheless we would expect that only a handful of examples of a digit should be sufficient a human to understand how to recognise a digit-like images must therefore occupy a highly constrained subspace of the dimensions and we expect only a small number of directions to be relevant for describing the data to a reasonable accuracy. whilst the data vectors may be very high dimensional they will therefore typically lie close to a much lower dimensional manifold a two-dimensional manifold corresponds to a warped sheet of paper embedded in a high dimensional space meaning that the distribution of the data is heavily constrained. here we concentrate on linear dimension reduction techniques for which there exist computationally efficient approaches. in this approach a high dimensional datapoint x is projected down to a lower dimensional vector y by y fx const. where the non-square matrix f has dimensions dim dim with dim dim the methods in this chapter are largely non-probabilistic although many have natural probabilistic interpretations. for example pca is closely related to factor analysis described in principal components analysis if data lies close to a hyperplane as in we can accurately approximate each data point by using vectors that span the hyperplane alone. effectively we are trying to discover a low dimensional co-ordinate system in which we can approximately represent the data. we express the approximation for datapoint xn as xn c i bi xn yn here the vector c is a constant and defines a point in the hyperplane and the bi define vectors in the hyperplane known as principal component coefficients or loadings the yn i are the low dimensional co-ordinates of the data. expresses how to find the reconstruction xn given the lower dimensional representation yn has components yn i i m. for a data space of dimension principal components analysis figure in linear dimension reduction a hyperplane is fitted such that the average distance between datapoints rings and their projections onto the plane dots is minimal. dim d we hope to accurately describe the data using only a small number m d of co-ordinates y. to determine the best lower dimensional representation it is convenient to use the square distance error between x and its reconstruction x eb y c i i xn it is straightforward to show that the optimal bias c is given by the mean of the data therefore assume that the data has been centred zero n xnn. we n xn so that we can set c to zero and concentrate on finding the optimal basis b below. eb y deriving the optimal linear reconstruction to find the best basis vectors b bj coordinates y we may minimize the sum of squared differences between each vector x i and corresponding low dimensional and its reconstruction x xn where x consider a transformation q of the basis b so that b bq is an orthonormal matrix bt b i. since q is invertible we may write by bq b y which is of then same form as by albeit with an orthonormality constraint on b. hence without loss of generality we may consider equation under the orthonormality constraint btb i namely that the basis vectors are mutually orthogonal and of unit length. byt by trace j bj yn i i by differentiating equation with respect to yn k we obtain the orthonormality constraint eb y xn i bk i j bj yn i i j i yn k j i bj i bk i i jk xn i bk i yn j the squared error eb y therefore has zero derivative when k yn bk i xn i i i bk xn i yn k draft march we now substitute this solution into equation to write the squared error only as a function of b. bijbkjxn k principal components analysis j j bj yn i eb jk the objective eb becomes jk kxn bj i bj k i i n eb i bbt btb i hence the objective becomes xn trace n n eb where s is the sample covariance matrix of the trace trace m n xn s mxn mt n l trace trace btb i i to minimise equation under the constraint btb i we use a set of lagrange multipliers l so that the objective is to minimize the constant prefactor n since the constraint is symmetric we can assume that l is also symmetric. differentiating with respect to b and equating to zero we obtain that at the optimum sb bl whose columns are the corresponding eigenvectors of s. in this case trace which is this is a form of eigen-equation so that a solution is given by taking l to be diagonal and b as the matrix the sum of the eigenvalues corresponding to the eigenvectors forming b. since we wish to minimise eb we take the eigenvectors with largest corresponding eigenvalues. if we order the eigenvalues the squared error is given by from equation n eb trace trace i im i i whilst the solution to this eigen-problem is unique this only serves to define the solution subspace since one may rotate and scale b and y such that the value of the squared loss is exactly the same. the justification for choosing the non-rotated eigen solution is given by the additional requirement that the principal components corresponds to directions of maximal variance as explained in we use the unbiased sample covariance simply because this is standard in the literature. if we were to replace this with the sample covariance as defined in the only change required is to replace n by n throughout which has no effect on the form of the solutions found by pca. draft march yn i principal components analysis figure projection of two dimensional data using one dimensional pca. plotted are the original datapoints x rings and their reconstructions x dots using dimensional pca. the lines represent the orthogonal projection of the original datapoint onto the first eigenvector. the arrows are the two eigenvectors scaled by the square root of their corresponding eigenvalues. the data has been centred to have zero mean. for each high dimensional datapoint x the low dimensional representation y is given in this case by the distance negative from the origin along the first eigenvector direction to the corresponding orthogonal projection point. maximum variance criterion we search first for the single direction b such that when the data is projected onto this direction the variance of this projection is maximal amongst all possible such projections. using equation for a single vector b we have bixn i the projection of a datapoint onto a direction b is btxn for a unit length vector b. hence the sum of squared projections is bt xn b n n which ignoring constants is simply the negative of equation for a single retained eigenvector b sb b. hence the optimal single b which maximises the projection variance is given by the eigenvector corresponding to the largest eigenvalue of s. under the criterion that the next optimal direction should be orthonormal to the first one can readily show that is given by the second largest eigenvector and so on. this explains why despite the squared loss equation being invariant with respect to arbitrary rotation scaling of the basis vectors the ones given by the eigen-decomposition have the additional property that they correspond to directions of maximal variance. these maximal variance directions found by pca are called the principal directions. pca algorithm the routine for pca is presented in in the notation of y fx the projection matrix f corresponds to et. similarly for the reconstruction equation the coordinate yn corresponds to etxn and bi corresponds to ei. the pca reconstructions are orthogonal projections of the data onto the subspace spanned by the eigenvectors corresponding to the m largest eigenvalues of the covariance matrix see figure top row a selection of the digit taken from the database of examples. plotted beneath each digit is the reconstruction using and eigenvectors top to bottom. note how the reconstructions for fewer eigenvectors express less variability from each other and resemble more a mean digit. draft march principal components analysis algorithm principal components analysis to form an m-dimensional approximation of a dataset n n with dim xn d. find the d sample mean vector and d d covariance matrix m n xn s n mxn mt find the eigenvectors em of the covariance matrix s sorted so that the eigenvalue of ei is larger than ej for i j. form the matrix e em the lower dimensional representation of each data point xn is given by yn etxn m the approximate reconstruction of the original datapoint xn is the total squared error over all the training data made by the approximation is xn m eyn j jm where m n are the eigenvalues discarded in the projection. example the dimension of digits. we have examples of handwritten s where each image consists of real-values pixels see each image matrix is stacked to form a dimensional vector giving a dimensional data matrix x. the covariance matrix of this data has eigenvalue spectrum as plotted in where we plot only the largest eigenvalues. note how after around components the mean squared reconstruction error is small indicating that the data indeed lie close to a dimensional hyperplane. the eigenvalues are computed using pca.m. the reconstructions using different numbers of eigenvectors and are plotted in note how using only a small number of eigenvectors the reconstruction more closely resembles the mean image. example in we present example images for which we wish to find a lower dimensional representation. using pca the first eigenfaces are presented along with reconstructions of the original data using these eigenfaces see figure for the digits data consisting of examples of the digit each image being represented by a dimensional vector. plotted as the largest eigenvalues so that the largest eigenvalue is of the sample covariance matrix. draft march numbereigenvalue principal components analysis figure of the training images people with images of each person. each image consists of greyscale pixels. the training data is scaled so that represented as an image the components of each image sum to the average value of each pixel across all images is this is a subset of the images in the full olivetti research face database. figure svd reconstruction of the images in using a combination of the eigen-images. the eigen-images are found using svd of the and taking the eigenvectors with largest eigenvalue. the images corresponding to the largest eigenvalues are contained in the first row and the next in the row below etc. the root mean square reconstruction error is a small improvement over plsa pca and nearest neighbours for high-dimensional data computing the squared euclidean distance between vectors can be expensive and also sensitive to noise. it is therefore often useful to project the data to form a lower dimensional representation first. for example in making a classifier to distinguish between the digit and the digit we can form a lower dimensional representation first by ignoring the class label make a dataset of training points. each of the training points xn is then projected to a lower dimensional pca representation yn. subsequently any distance calculations are replaced by to justify this consider xbtxa xb m eyb mteya m eyb m ybteteya yb ybtya yb where the last equality is due to the orthonormality of eigenvectors ete i. using principal components why this number was chosen and the nearest neighbour rule to classify ones and sevens gave a test-set error of in examples compared to from the standard method on the non-projected data. how can it be that the classification performance has improved? a plausible explanation is that the new pca representation of the data is more robust since only the large scale change directions in the space are retained with low variance directions discarded. draft march high dimensional data figure finding the optimal pca dimension to use for classifying hand-written digits using nearest neighbours. training examples are used and the validation error plotted on further examples. based on the validation error we see that a dimension of is reasonable. example the best pca dimension. there are examples of the digit and examples of the digit we will use half the data for training and the other half for testing. the training examples were further split into a training set of examples and a separate validation set of examples. pca was used to reduce the dimensionality of the inputs and then nearest neighbours used to classify the validation examples. different reduced dimensions were in vestigated and based on the validation results was selected as the optimal number of pca components retained see the independent test error on independent examples using dimensions is comments on pca the intrinsic dimension of data how many dimensions should the linear subspace have? from equation the reconstruction error is proportional to the sum of the discarded eigenvalues. if we plot the eigenvalue spectrum set of eigenvalues ordered by decreasing value we might hope to see a few large values and many small values. if the data does lie close to an m dimensional hyperplane we would see m large eigenvalues with the rest being very small. this would give an indication of the number of degrees of freedom in the data or the intrinsic dimensionality. directions corresponding to the small eigenvalues are then interpreted as noise non-linear dimension reduction in pca we are presupposing that the data lies close to a hyperplane. is this really a good description? more generally we would expect data to lie on low dimensional curved manifolds. also data is often clustered examples of handwritten s look similar to each other and form a cluster separate from the s cluster. nevertheless since linear dimension reduction is computationally relatively straightforward this is one of the most common dimensionality reduction techniques. high dimensional data the computational complexity of computing an eigen-decomposition of a d d matrix is you exceed one can exploit this fact to bound the complexity by as described below. might be wondering therefore how it is possible to perform pca on high dimensional data. for example if we have images each of pixels the covariance matrix will be dimensional. it would appear a significant computational challenge to compute the eigen-decomposition of this matrix. in this case however since there are only such vectors the number of non-zero eigenvalues cannot draft march of eigenvaluesnumber of errors eigen-decomposition for n d high dimensional data first note that for zero mean data the sample covariance matrix can be expressed as xn i xn j n in matrix notation this can be written s n xxt where the d n matrix x contains all the data vectors x since the eigenvectors of a matrix m are equal to those of m for scalar one can consider more simply the eigenvectors of xxt. writing the d n matrix of eigenvectors as e is a non-square thin matrix since there will be fewer eigenvalues than data dimensions and the eigenvalues as an n n diagonal matrix the eigen-decomposition of the covariance s is xxte e xtxxte xte xtx e e where we defined e xte. the final expression above represents the eigenvector equation for xtx. this is a matrix of dimensions n n so that calculating the eigen-decomposition takes operations compared with operations in the original high-dimensional space we then can therefore calculate the eigenvectors e and eigenvalues of this matrix more easily. once found we use the fact that the eigenvalues of s are given by the diagonal entries of and the eigenvectors by e x e pca via singular value decomposition an alternative to using an eigen-decomposition routine to find the pca solution is to make use of the singular value decomposition of an d n dimensional matrix x. this is given by x udvt where utu id and vtv in and d is a diagonal matrix of the singular values. we assume that the decomposition has ordered the singular values so that the upper left diagonal element of d contains the largest singular value. the matrix xxt can then be written as xxt udvtvdut since is in the form of an eigen-decomposition the pca solution is equivalently given by performing the svd decomposition of x for which the eigenvectors are then given by u and corresponding eigenvalues by the square of the singular values. shows that pca is a form of matrix decomposition method x udvt um dm vt m where um dm vm correspond to taking only the first m singular values of the full matrices. draft march latent semantic analysis figure document data for a dictionary containing words and documents. black indicates that a word was present in a document. the data consists of two similar topics only in their usage of the first two words and a random background topic. latent semantic analysis in the document analysis literature pca is also called latent semantic analysis and is concerned with analysing a set of n documents where each document dn n n is represented by a vector xn xn d might count how many times the word cat appears of word occurrences. for example the first element xn the number of occurrences of dog etc. this bag of words is formed by first choosing in document n xn is the normalised number of occurrences of the a dictionary of d words. the vector element xn i word i in the document n. typically d will be large of the order and x will be very sparse since any document contains only a small fraction of the available words in the dictionary. given a set of documents d the aim in lsa is to form a lower dimensional representation of each document. the whole document database is represented by the so-called term-document matrix x which has dimension d n see for example in this small example the term-document matrix is short and fat whereas in practice most often the matrix will be tall and thin example topic. we have a small dictionary containing the words influenza flu headache nose temperature bed cat tree car foot. the database contains a large number of articles that discuss ailments and articles which seem to talk about the effects of influenza in addition to some background documents that are not specific to ailments. some of the more formal documents exclusively use the term influenza whereas the other more tabloid documents use the informal term flu. each document is represented by a simple bag-of-words style description namely a vector in which element i of that vector is set to if word i occurs in the document and otherwise. the data is represented in the data is generated using the artificial mechanism described in demolsi.m. the result of using pca on this data is represented in where we plot the eigenvectors scaled by their eigenvalue. the first eigenvector groups all the influenza words together and the second deals with the different usage of the terms influenza and flu. rescaling in lsa it is common to scale the transformation so that the projected vectors have approximately unit covariance centred data. using the covariance of the projections is obtained from y n m ut m x n n n yn d m ut m um d m d m ut m d m i xn xxt generally one can consider term-counts in which terms can can be single words or sets of words or even sub-words. draft march latent semantic analysis figure hinton diagram of the eigenvector matrix e where each eigenvector column is scaled by the corresponding eigenvalue. red indicates positive and green negative area of each square corresponds to the magnitude showing that there are only a few large eigenvalues. note that the overall sign of any eigenvector is irrelevant. the first eigenvector corresponds to a topic in which the words influenza flu headache nose temperature bed are prevalent. the second eigenvector denotes that there is negative correlation between the occurrence of influenza and flu. given y the approximate reconstruction x is the euclidean distance between two points xa and xb is then approximately ya m ya ya n x um dm y n xa d ya n dm ut m um dm it is common to ignore the the projected space just to be the euclidean distance between the y vectors. m term factor and consider a measure of dissimilarity in lsa for information retrieval consider a large collection of documents from the web creating a database d. our interest it to find the most similar document to a specified query document. using a bag-of-words style representation for document n xn and similarly for the query document x we address this task by first defining a measure of dissimilarity between documents for example dxn xm xmt xm one then searches for the document that minimises this dissimilarity nopt argmin n dxn x and returns document xnopt as the result of the search query. a difficulty with this approach is that the bag-of-words representation will have mostly zeros be very sparse. hence differences may be due to noise rather than any real similarity between the query and database document. lsa alleviates this problem by using a lower dimensional representation y of the high-dimensional x. the y capture the main variations in the data and are less sensitive to random uncorrelated noise. using the dissimilarity defined in terms of the lower dimensional y is therefore more robust and likely to retrieve more useful documents. the squared difference between two documents can also be written xtx if as is commonly done the bag-of-words representations are scaled to have unit length x x xtx so that xt x the distance is x x xt draft march pca with missing data figure two bag-of-word vectors. the euclidean distance between the two is large. normalised vectors. the euclidean distance is now related directly to the angle between the vectors. in this case two documents which have the same relative frequency of words will both have the same dissimilarly even though the number of occurrences of the words is different. figure top original data matrix x. black is missing white present. the data is constructed from a set of only basis vectors. middle x with missing data sparsity. bottom reconstruction found using svdm.m svd for missing data. this problem is essentially easy since despite there being many missing elements the data is indeed constructed from a model for which svd is appropriate. such techniques have application in collaborative filtering and recommender systems where one wishes to fill in missing values in a matrix. and one may equivalently consider the cosine similarity s x xt cos where is the angle between the unit vectors x and pca is arguably suboptimal for document analysis since we would expect the presence of a latent topic to contribute only positive counts to the data. a related version of pca in which the decomposition is constrained to have positive elements is called plsa and discussed in example continuing the influenza example someone who uploads a query document which uses the term flu might also be interested in documents about influenza however the search query term flu does not contain the word influenza so how can one retrieve such documents? since the first component using pca groups all influenza terms together if we use only the first component of the representation y to compare documents this will retrieve documents independent of whether the term flu or influenza is used. pca with missing data when values of the data matrix x are missing the standard pca algorithm as described cannot be implemented. unfortunately there is no quick fix pca solution when some of the xn i are missing and more complex numerical procedures need to invoked. a naive approach in this case is to require the draft march squared reconstruction error to be small only for the existing elements of x. that is xn j eb y n i i j bj yn i pca with missing data where n we find that the optimal weights satisfy btb i i if the ith entry of the nth vector is available and is zero otherwise. differentiating as before n i xn i bk i n yk i one then substitutes this expression into the squared error and minimises the error with respect to b under the orthonormality constraint. an alternative iterative optimisation procedure is as follows first select a random d m matrix b. then iterate until convergence the following two steps optimize y for fixed b for fixed b the above e b y is a quadratic function of the matrix y which can be optimised directly. by differentiating and equating to zero one obtains the fixed point condition j i yn j bj i l n i xn bl i xn i yn l bk i e b y n i i yl n l i kl bl i bk i n i i n i xn i bk i in matrix notation we then have a set of linear systems n n cn mnyn one may solve each linear system using gaussian elimination can avoid explicit matrix inversion by using the operator in matlab. it can be that one or more of the above linear systems is underdetermined this can occur when there are less observed values in the nth data column of x than there are components m. in this case one may use the pseudo-inverse to provide a minimal length solution. optimize b for fixed y one now freezes y and considers the function xn j n i i j bj yn i eb y for fixed y the above expression is quadratic in the matrix b which can again be optimised using linear algebra. this corresponds to solving a set of linear systems for the ith row of b mi fibi mathematically this is bi fi n i xn i yn k n k kj n n i yn j yn k in this manner one is guaranteed to iteratively decrease the value of the squared error loss until a minimum is reached. this technique is implemented in svdm.m. note that efficient techniques based on updating the solution as a new column of x arrives one at a time online updating are available see for example draft march pca with missing data finding the principal directions for the missing data case the basis b found using the above technique is based only on minimising the squared reconstruction error and therefore does not necessarily satisfy the maximal variance principal directions criterion namely that the columns of b point along the eigen-directions. for a given b y with approximate decomposition x by we can return a new orthonormal basis u by performing svd on the completed data by usvt to return an orthonormal basis b u. in general however this is potentially computationally expensive. if only the principal directions are required an alternative is to explicitly transform the solution b using an invertible matrix q x bqq calling the new basis b bq for a solution to be aligned with the principal directions we need bt b i in other words qtbtbq i forming the svd of b b udvt and substituting in equation we have the requirement qtvdutudvtq i since utu i and vtv vvt i we can use q vtd hence given a solution b we can find the principal directions from the svd of b using b utd if the d m matrix b is non-square m d then the matrix d will be non-square and non-invertible. to make the above well defined one may append d with the columns of the identity im id where ik is the kth column of the identity matrix and use in place of d above. collaborative filtering using pca with missing data entry in the vector x specifies the rating the user gives to the ith film. the matrix x for all a database contains a set of vectors each describing the film ratings for a user in the database. the ith the n users has many missing values since any single user will only have given a rating for a small selection of the possible d films. in a practical example one might have d films and n users. for any user n the task is to predict reasonable values for the missing entries of their rating vector xn thereby providing a suggestion as to which films they might like to view. viewed as a missing data problem one can fit b and y using svdm.m as above. given b and y we can form a reconstruction on all the entries of x by using x by giving therefore a prediction for the missing values. draft march matrix decomposition methods figure under-complete representation. there are too few basis over-complete representation. vectors to represent the datapoints. there are too many basis vectors to form a unique representation of a datapoint in terms of a linear combination of the basis vectors. z x y x y z figure joint plsa. conditional plsa. whilst written as a graphical model some care is required in the interpretation see text. matrix decomposition methods given a data matrix x for which each column represents a datapoint an approximate matrix decomposition is of the form x by into a basis matrix b and weight coordinate matrix y. symbolically matrix decompositions are of the form x data d n b basis d m y weightscomponents m n in this section we will consider some common matrix decomposition methods. under-complete decompositions when m d there are fewer basis vectors than dimensions the matrix b is then called tall or thin in this case the matrix y forms a lower dimensional approximate representation of the data x pca being a classic example. over-complete decompositions for m d the basis is over-complete there being more basis vectors than dimensions in such cases additional constraints are placed on either the basis or components. for example one might require that only a small number of the large number of available basis vectors is used to form the representation for any given x. such sparse-representations are common in theoretical neurobiology where issues of energy efficiency rapidity of processing and robustness are of probabilistic latent semantic analysis consider two objects x and y where domx i and domy j. we have a count matrix with elements cij which describes the number of times that x i y j was observed. we can transform this count matrix into a frequency matrix p with elements px i y j px i y j ij cij xij our interest is to find a decomposition of this frequency matrix of the form in k px iz k py jz k pz k bik ykj px i y j draft march matrix decomposition methods which is a form of matrix decomposition into basis b and coordinates y. this has the interpretation of discovering latent topics z that describe the joint behaviour of x and y. an em style training algorithm in order to find the approximate decomposition we first need a measure of difference between the matrix with elements pij and the approximation with elements pij. since all elements are bounded between and and sum to we may interpret p as a joint probability and p as an approximation to this. for probabilities a useful measure of discrepancy is the kullback-leibler divergence since p is fixed minimising the kullback-leibler divergence with respect to the approximation p is equivalent to maximising the likelihood term this is px y log px y klp p klqzx y pzx y xy z it s convenient to derive an em style algorithm to learn pxz pyz and pz. to do this consider qzx y log qzx y qzx y log pzx y z z implies summation over all states of the variable z. rearranging this gives the bound qzx y log pz x y log px y plugging this into the likelihood term above we have the bound px y log px y xy qzx y log qzx y z qzx y log qzx y px px xy z z xy z m-step for fixed pxz pyz the contribution to the bound from pz is qzx y pxz log pyz log pz similarly for fixed pyz pz it is straightforward to see that the optimal setting of pz is xy qzx ypx y pz qzx y log pz qzx ypx y xy px pz xy z xy z px y since equation is up to a constant kl the contribution to the bound from pxz is qzx y log pyz therefore optimally pxz px yqzx y and similarly pyz px yqzx y x draft march matrix decomposition methods the images figure conditional plsa reconstruction of in using a positive combination of the positive base images in the root mean square reconstruction error is the base images tend to be more localised than the corresponding eigen-images here one sees local structure such as foreheads chins etc. e-step the optimal setting for the q distribution at each iteration is qzx y pzx y which is fixed throughout the m-step. the procedure is given in and a demonstration is in demoplsa.m. the likelihood equation is guaranteed to increase the kullback-leibler divergence equation decrease under iterating between the e and m-steps since the method is analogous to an em procedure generalisations such as using simpler q distributions to generalised em procedures are immediate based on modifying the above derivation. a related probabilistic model for variables x y z with z hidden and domx i domy j domz k consider a distribution px y z pxz pyz pz and data d yn n n. assuming the data are i.i.d. draws from equation the log likelihood is log pd pxn yn where log pxn yn pxnzn pynzn pzn zn if xn yn are sampled from a distribution px y then in the limit of an infinite number of samples n equation becomes log pd px which is equation from this viewpoint even though we started out with a set of samples in the limit only the distribution of the observed data px y is relevant. the generic code for the finite sample case trained with em is given in demomultinomialpxygz.m. see also a fully probabilistic interpretation of plsa can be made via poisson a related probabilistic model is latent dirichlet allocation which is described in draft march matrix decomposition methods algorithm plsa given a frequency matrix px i y j return a k py jz k pz k. see plsa.m initialise pz pxz pyz. while not converged do set qzx y pzx y set pxz set pyz end while y px yqzx y x px yqzx y xy px yqzx y k px iz e-step m-steps set pz algorithm conditional plsa given a frequency matrix px iy j return a decomposition k px iz k pz ky j. see plsacond.m initialise pxz pzy. while not converged do y pxyqzx y x pxyqzx y end while set qzx y pzx y set pxz set pzy e-step m-steps conditional plsa in some cases it is more natural to consider a conditional frequency matrix px iy j px iy j and seek an approximate decomposition px iz k k xij bik pz ky j ykj as depicted in deriving an em style algorithm for this is straightforward and is presented in being equivalent to the non-negative matrix factorisation algorithm of is unity example the basis. a set of images is give in these were created by first defining base images each base image is positive and scaled so that the sum of the pixels i px iz k where k and x indexes the pixels see we then sum each of these images using a randomly chosen positive set of weights the constraint that the weights sum to to generate a training image with elements px iy j and j indexes the training image. this is repeated times to form the full training set the task is given only the training set images to reconstruct the basis from which the images were formed. we assume that we know the correct number of base images namely the results of using conditional plsa on this task are presented in and using svd in in this case plsa finds the correct natural basis corresponding to the way the images were generated. the eigenbasis is just as good in terms of being able to represent any of the training images but in this case does not correspond to the constraints under which the data was generated extensions and variations non-negative matrix factorisation non-negative matrix factorisation considers a decomposition in which both the basis and weight matrices have non-negative entries. an early example of this work is as a form of constrained factor draft march matrix decomposition methods figure training data consisting of a positive combination of the base images. basis learned using conditional the chosen base images from which the training data is derived. eigenbasis plsa on the training data. this is virtually indistinguishable from the true basis. called eigenfaces closely related works are which is a generalisation of plsa there is no requirement that the basis or components sum to unity. in all cases em-style training algorithms exist although their convergence can be slow. a natural relaxation is when only one of the factors in the decomposition is constrained to be non-negative. we will encounter similar models in the discussion on independent component analysis gradient based training em style algorithms are easy to derive and implement but can exhibit poor convergence. gradient based methods to simultaneously optimize with respect to the basis and the components have been developed but require a parameterisation that ensures positivity of the array decompositions it is straightforward to extend the method to the decomposition of multidimensional arrays based also on more than one basis. for example ps tu v puw pv pw vw ps t u ps t uv w pv w vw such extensions require only additional bookkeeping. applications of plsanmf physical models non-negative decompositions can arise naturally in certain physical situations. for example in acoustics positive amounts of energy combine linearly from different signal sources to form the observed signal. let s imagine that two kinds of signals are present in an acoustic signal say a piano and a singer. using nmf one can learn two separate bases for these cases and then reconstruct a given signal using only one of the bases. this means that one could potentially remove the singer from a recording leaving only the piano. see also for a more standard probabilistic model in acoustics. this would be analogous to reconstructing the images in using say only one of the learned basis images see modelling citations we have a collection of research documents which cite other documents. for example document might cite documents etc. given only the list of citations for each document can we identify key research papers and the communities that cite them? note that this is not the same question as finding the most cited documents rather we want to identify documents with communities and find their relevance for a draft march matrix decomposition methods learning learning to predict by the methods of temporal differences. sutton. neuronlike adaptive elements that can solve difficult learning control problems. barto et al. practical issues in temporal difference learning. tesauro. learning explanation-based generalization a unifying view. mitchell et al. learning internal representations by error propagation. rumelhart et al. explanation-based learning an alternative view. dejong et al. networks learning internal representations by error propagation. rumelhart et al. neural networks and the bias-variance dilemma. geman et al. the cascade-correlation learning architecture. fahlman et al. classification and regression trees. breiman et al. learnability and the vapnik-chervonenkis dimension. blumer et al. learning quickly when irrelevant attributes abound. littlestone. reasoning probabilistic reasoning in intelligent systems networks of plausible inference. pearl. factor factor factor factor factor maximum likelihood from incomplete data via the em algorithm. dempster et al. factor factor local computations with probabilities on graphical structures. lauritzen et al. algorithms genetic algorithms in search optimization and machine learning. goldberg. adaptation in natural and artificial systems. holland. genetic programming on the programming of computers by means of natural selection. koza. efficient induction of logic programs. muggleton et al. learning logical definitions from relations. quinlan. inductive logic programming techniques and applications. lavrac et al. table highest ranked documents according to pcz. the factor topic labels are manual assignments based on similarities to the cora topics. reproduced from community. we use the variable d d to index documents and c d to index citations d and c have the same domain namely the index of a research article. if document d i cites article c j then we set the entry of the matrix cij if there is no citation cij is set to zero. we can form a distribution over documents and citations using pd i c j ij cij example citations. the cora corpus mccallum contains an archive of around computer science research papers. from this archive the authors in extracted the papers in the machine learning category consisting of documents and citations. using these the distribution equation was formed. the documents have additionally been categorised by hand into topics case-based reasoning genetic algorithms neural networks probabilistic methods reinforcement learning rule learning and theory. in the joint plsa method is fitted to the data using z topics. from the trained model the expression pc jz k defines how authoritative paper j is according to community z k. the results are presented in and show how the method discovers intuitively meaningful topics. modelling the web consider a collection of websites indexed by i. if website j points to website i one sets cij giving a directed graph of website-to-website links. since a website will discuss usually only of a small number of draft march topics we might be able to explain why there is a link between two websites using a plsa decomposition. these algorithms have proved useful for internet search for example to determine the latent topics of websites and identify the most authoritative websites. see for a discussion. kernel pca kernel pca kernel pca is a non-linear extension of pca designed to discover non-linear manifolds. here we only briefly describe the approach and refer the reader to for details. in kernel pca we replace each x by a feature vector x note that the use of x here does not have the interpretation we used before as the approximate reconstruction. the feature map takes a vector x and produces a higher dimensional vector x. for example we could map a two dimensional vector x using the idea is then to perform pca on these higher dimensional feature vectors subsequently mapping back the eigenvectors to the original space x. the main challenge is to write this without explicitly computing pca in the potentially very high dimensional feature vector space. as a reminder in standard pca for zero mean data one forms an eigen-decomposition of the sample s n x xt for simplicity we concentrate here on finding the first principal component e which satisfies x xt e e for corresponding eigenvalue n the dual representation is obtained by pre-multiplying by xt so that in terms of f xt e the standard pca eigen-problem reduces to solving xt x f f e x f xt x xt x yi n i the feature eigenvector e is then recovered using we note that matrix xt x has elements mn and recognise this as the scalar product between vectors. this means that the matrix is positive definite and we may equivalently use a positive definite kernel see kxm xn kmn mn then equation can be written as k f f one then solves this eigen-equation to find the n dimensional principal dual feature vector f. the projection of the feature x is given by y xt e xt x f more generally for a larger number of components the ith kernel pca projection yi can be expressed in terms of the kernel directly as kx xn f i n use the normalisation n as opposed to n just for notational convenience in practice there is little difference. draft march kernel pca figure canonical correlation analysis. training data. the top panel contains the x matrix of dimensional points and the bottom the corresponding dimensional y matrix. the data in was produced using x ah y bh where a is a matrix and b is a matrix. matrices a and b learned by cca. note that they are close to the true a and b up to rescaling and sign changes. see democca.m. where i is the eigenvalue label. the above derivation implicitly assumed zero mean features x. even if the original data x is zero mean due to the non-linear mapping the features may not be zero mean. to correct for this one may show that the only modification required is to replace the matrix k in equation above with mn kxm xn k n kxd xn n kxm xd n xd finding the reconstructions the above gives a procedure for finding the kpca projection y. however in many cases we would also like to have an approximate reconstruction using the lower dimensional y. this is not straightforward since the mapping from y to x is in general highly non-linear. here we outline a procedure for achieving this. first we find the reconstruction x of the feature space x. now f n i x yi ei i i yi i n given x we try to find that point in the original data space that maps to x this can be found by minimising x yi i i n up to negligable constants this is i kxn f n one then finds by minimising numerically. draft march training canonical correlation analysis canonical correlation analysis consider x and y which have dimensions dim and dim respectively. for example x might represent a segment of video and y the corresponding audio. given then a collection yn n n an interesting challenge is to identify which parts of the audio and video files are strongly correlated. one might expect for example that the mouth region of the video is strongly correlated with the audio. one way to achieve this is to project each x and y to one dimension using atx and bty such that the correlation between the projections is maximal. the unnormalised correlation between the projections atx and bty atxnbtyn at xnynt b n n and the normalised correlation is atsxyb where sxy is the sample x y cross correlation matrix. when the joint covariance of the stacked vectors zn yn is considered sxx sxy syx syy are the blocks of the joint covariance matrix. since equation is invariant with respect to length scaling of a and also b we can consider the equivalent objective ea b atsxyb subject to atsxxa and btsyyb to find the optimal projections a b under the constraints we use the lagrangian l b a b atsxyb a atsxxa from which we obtain the zero derivative criteria btsyyb b sxyb asxxa syxa bsyyb hence atsxyb aatsxxa a btsyxa bbtsyyb b since atsxyb btsyxa we must have a b at the optimum. if we assume that syy is invertible b s yy syxa using this to eliminate b in equation we have sxys yy syxa which is a generalised eigen-problem. assuming that sxx is invertible we can equivalently write xx sxys s yy syxa which is a standard eigen-problem with as the eigenvalue. once this is solved we can find b using equation draft march exercises svd formulation it is straightforward to show that we can find a by first computing the svd of xx sxys yy s in the form udvt and extracting the maximal singular vector of u first column on u. then a yy where is the first column of v. in this way is optimally s the extension to finding m multiple directions a and b is clear one xx and similarly b is optimally s takes the corresponding first m singular values accordingly. doing so maximises the criterion this approach is taken in cca.m see for a demonstration. one can also show that cca corresponds to the probabilistic factor analysis model under a block restriction on the form of the factor loadings see cca and related kernel extensions have been applied in machine learning contexts for example to model the correlation between images and text in order to improve image retrieval from text queries see notes pca is also known as the karhunen-loeve decomposition particularly in the engineering literature. code pca.m principal components analysis demolsi.m demo of latent semantic indexinganalysis svdm.m singular value decomposition with missing data demosvdmissing.m demo svd with missing data plsa.m probabilistic latent semantic analysis plsacond.m conditional probabilistic latent semantic analysis demoplsa.m demo of plsa demomultnomialpxygz.m demo of finite sample plsa cca.m canonical correlation analysis democca.m demo of canonical correlation analysis exercises exercise consider a dataset in two dimensions where the data lies on the circumference of a circle of unit radius. what would be the effect of using pca on this dataset in which we attempt to reduce the dimensionality to suggest an alternative one dimensional representation of the data. exercise consider two vectors xa and xb and their corresponding pca approximations and c aiei biei where the eigenvectors ei i m are mutually orthogonal and have unit length. the eigenvector ei has corresponding eigenvalue i. approximate by using the pca representations of the data and show that this is equal to exercise show how the solution for a to the cca problem in equation can be transformed into the form expressed by equation as claimed in the text. draft march exercises xa s xa exercise let s be the covariance matrix of the data. the mahalanobis distance between xa and xb is defined as explain how to approximate this distance using m-dimensional pca approximations. exercise with external inputs. in some applications one may suspect that certain external variables v have a strong influence on how the data x is distributed. for example if x represents an image it might be that we know the lighting condition v under which the image was made this will have a large effect on the image. it would make sense therefore to include the known lighting condition in forming a lower dimensional representation of the image. note that we don t want to form a lower dimensional representation of the joint x v rather we want to form a lower dimensional representation of x alone bearing in mind that some of the variability observed may be due to v. we therefore assume an approximation k ck vn j k where the coefficients yn i i n n n and basis vectors bj j j and ck k k are to be determined. the external inputs vn are given. the sum squared error loss between the xn and their linear reconstruction equation is j bj yn xn xn e ni j k i j bj yn i k ck vn i find the parameters that minimise e. exercise consider the following datapoints perform principal components analysis by calculating the mean c of the data. calculating the covariance matrix s finding the eigenvalues and eigenvectors ei of the covariance matrix. xnxnt cct of the data. you should find that only two eigenvalues are large and therefore that the data can be well represented using two components only. let and be the two eigenvectors with largest eigenvalues. calculate the two dimensional representation of each datapoint c c n calculate the reconstruction of each datapoint c n exercise consider a conditional frequency matrix px iy j k show how to derive an em style algorithm for an approximate decomposition of this matrix in the form px iy j px iz k pz ky j where k z i x j y exercise for the multinomial model px y z described in equation derive explicitly the em algorithm and implement this in matlab. for randomly chosen values for the conditional probabilities draw samples from this model for x y z and compute from this the matrix with elements pij i y j i y j now run plsa plsa.m with the settings x y z to learn and compare your results with those obtained from the finite sample model equation draft march chapter supervised linear dimension reduction supervised linear projections in we discussed dimension reduction using an unsupervised procedure. in cases where class information is available and our ultimate interest is to reduce dimensionality for improved classification it makes sense to use the available class information in forming the projections. exploiting the class label information to improve the projection is a form of supervised dimension reduction. let s consider data from two different classes. for class we have a set of data datapoints and similarly for class we have a set of datapoints our interest is then to find a linear projection y wtx where dim w d l l d such that for datapoints xi xj in the same class the distance between their projections yi yj should be small. conversely for datapoints in different classes the distance between their projections should be large. this may be useful for classification purposes since for a novel point x if its projection y wtx is close to class projected data we would expect x to belong to class in forming the supervised projection only the class discriminative parts of the data are retained so that the procedure can be considered a form of supervised feature extraction. fisher s linear discriminant we restrict attention to binary class data. also for simplicity we project the data down to one dimension. the canonical variates algorithm of deals with the generalisations. gaussian assumption we model the data from each class with a gaussian. that is n n fisher s linear discriminant figure the large crosses represent data from class and the large circles from class their projections onto dimension are represented by their small counterparts. fisher s linear discriminant unsupervised dimension reduction analysis. here there is little class overlap in the projections. using principal components analysis for comparison. there is considerable class overlap in the projection. in both and the one dimensional projection is the distance along the line measured from an arbitrary chosen fixed point on the line. where is the sample mean of class data and the sample covariance similarly for class the projections of the points from the two classes are then given by because the projections are linear the projected distributions are also gaussian wtxn yn wtxn yn n n we search for a projection w such that the projected distributions have minimal overlap. this can be achieved if the projected gaussian means are maximally separated is large. however if the variances are also large there could be a large overlap still in the classes. a useful objective function therefore is where i represents the fraction of the dataset in class i. equation is in terms of the projection w the objective f wt w wt w wtaw wtbw where the optimal w can be found by differentiating equation with respect to w. this gives a wtaw wtbw w aw and therefore the zero derivative requirement is wtbw wtbw aw wtaw bw b wtaw bw draft march canonical variates multiplying by the inverse of b we have b w wtaw wtbw w this means that the optimal projection is explicitly given by w b although the proportionality factor depends on w we may take it to be constant since the objective function f of equation is invariant to rescaling of w. we may therefore take w kb it is common to rescale w to have unit length wtw such that k b an illustration of the method is given in which demonstrates how supervised dimension reduction can produce lower dimensional representations more suitable for subsequent classification than an unsupervised method such as pca. one can also arrive at the equation from a different starting objective. by treating the projection as a regression problem y wtxb in which the outputs y are defined as and for classes and class respectively one may show that for suitably chosen and the solution using a least squares criterion is given by equation this also suggests a way to regularise lda see kernel extensions of lda are possible see for example when the naive method breaks down the above derivation relied on the existence of the inverse of b. in practice however b may not be invertible and the above procedure requires modification. a case where b is not invertible is when there are fewer datapoints than dimensions d. another case is when there are elements of the input vectors that never vary. for example in the hand-written digits case the pixels at the corner edges are actually always zero. let s call this corner pixel z. the matrix b will then have a zero entry for the whole zth row and column will be zero so that for any vector w wz wtbw this shows that the denominator of fisher s objective can become zero and the objective ill defined. we will deal with these issues canonical variates canonical variates generalises fisher s method to projections in more than one dimension and more than two classes. the projection of any point is given by y wtx where w is a d l matrix. assuming that the data x from class c is gaussian distributed px n mc sc the projections y are also gaussian py n y wtmc wtscw to extend to more than two classes we define the following matrices draft march between class scatter find m the mean of the whole dataset and mc the mean of the each class c. canonical variates where nc is the number of datapoints in class c c c. within class scatter for each class c form a covariance matrix sc and mean mc. define form a nc m mt b ncsc this naturally gives rise to a raleigh quotient objective trace f bt b b then defining the objective can be written in terms of w trace wt b ta b w w bw w b w wt w wtc w f w trace f w trace subject to wt w i assuming b is invertible otherwise we can define the cholesky factor b with if we assume an orthonormality constraint on w then we equivalently require the maximisation of where c b ta b since c is symmetric and positive semi-definite it has a real eigen-decomposition c e et where diag d is diagonal with non-negative entries containing the eigenvalues sorted by decreasing order and ete i. hence wte et w f w trace by setting w el where el is the lth eigenvector the objective becomes the sum of the first l eigenvalues. this setting maximises the objective function since forming w from any other columns of e would give a lower sum. we then return w b w as the projection matrix. the procedure is outlined in note that since a has rank c there can be no more than c non-zero eigenvalues and corresponding directions. draft march canonical variates algorithm canonical variates compute the between and within class scatter matrices a equation and b equation compute the cholesky factor b of b. compute the l principal eigenvectors el of b ta b w el return w b w as the projection matrix. dealing with the nullspace the above derivation of canonical variates also fisher s lda requires the invertibility of the matrix b. however as we discussed in one may encounter situations where b is not invertible. a solution is to require that w lies only in the subspace spanned by the data is there can be no contribution from the nullspace. to do this we first concatenate the training data from all classes into one large matrix x. a basis for x can be found using for example the thin-svd technique which returns an orthonormal basis q. we then require the solution w to be expressed in this basis w for some matrix substituting this in the canonical variates objective equation we obtain trace f this is of the same form as the standard quotient equation on replacing the between-scatter a with qtaq and the within-scatter b with qtbq in this case is guaranteed invertible and one may carry out canonical variates as in above. this will return a matrix we then return w see also canonvar.m. example canonical variates on the digits data. we apply canonical variates to project the digit data onto two dimensions see there are examples of a three examples of a five and examples of a seven. thus overall there are examples lying in a pixels dimensional space. note how the canonical variates projected data onto two dimensions has very little class overlap see in comparison the projections formed from pca which discards the class information displays a high degree of class overlap. the different scales of the canonical variates and pca projections figure each three dimensional datapoint lies in a two-dimensional plane meaning that the matrix b is not full rank and therefore not invertible. a solution is given by finding vectors that span the plane and expressing the canonical variates solution in terms of these vectors alone. draft march exercises figure canonical variates projection of examples of handwritten digits o and there are examples from each digit class. plotted are the projections down to dimensions. pca projections for comparison. in pca w is unitary in canonical is due to the different constraints on the projection matrices w. variates wtbw i meaning that w will scale with the inverse square root of the largest eigenvalues of the within class scatter matrix. since the canonical variates objective is independent of linear scaling w can be rescaled with an arbitrary scalar prefactor w as desired. using non-gaussian data distributions the applicability of canonical variates depends on our assumption that a gaussian is a good description of the data. clearly if the data is multimodal using a single gaussian to model the data in each class is a poor assumption. this may result in projections with a large class overlap. in principle there is no conceptual difficulty in using more complex distributions with say more general criteria such as kullbackleibler divergence between projected distributions used as the objective. however such criteria typically result in difficult optimisation problems. canonical variates is popular due to its simplicity and lack of local optima issues in constructing the projection. code canonvar.m canonical variates democanonvardigits.m demo for canonical variates exercises exercise what happens to fisher s linear discriminant if there are less datapoints than dimensions? exercise modify democanonvardigits.m to project and visualise the digits data in dimensions. exercise consider class datapoints and class datapoints we will make a linear predictor for the data y wtx b draft march exercises with the aim to predict value for data from class and for data from class two. a measure of the fit is given by b b show that by setting and the w which minimises e corresponds to fisher s lda solution. hint first show that the two zero derivative conditions are ew and b b xt b b xt which can be reduced to the single equation n where b is as defined for lda in the text equation n nb w note that this suggests a way to regularise lda namely by adding on a term wtw to ew this can be absorbed into redefining equation as b i in other words one can increase the covariance b by an additive amount i. the optimal regularising constant may be set by cross-validation. more generally one can consider the use of a regularising matrix r where r is positive definite. exercise consider the digit data of fives and sevens make a training set which consists of the first examples from each digit class. use canonical variates to first project the data down to dimensions and compute the nearest neighbour performance on the remaining digits. compare the classification accuracy to using nearest neighbours the projections from pca using components. exercise consider an objective function of the form f aw bw where aw and bw are positive functions and our task is to maximise f with respect to w. it may be that this objective does not have a simple algebraic solution even though aw and bw are simple functions. we can consider an alternative objective namely jw aw bw where is a constant scalar. choose an initial point wold at random and set old awoldbwold in that case jwold old now choose a w such that jw old aw oldbw this is certainly possible since jwold old if we can find a w such that jw old then aw oldbw show that for such a w f f and suggest an iterative optimisation procedure for objective functions of the form f draft march exercises draft march chapter linear models introduction fitting a straight line given training data yn n n for scalar input xn and scalar output yn a linear regression fit is yx a bx to determine the best parameters a b we use a measure of the discrepancy between the observed outputs and the linear regression fit such as the sum squared training error. this is also called ordinary least squares and minimises the average vertical projection of the points y to fitted line ea b a our task is to find the parameters a and b that minimise ea b. differentiating with respect to a and b we obtain a ea b a bxn b ea b a bxnxn dividing by n and equating to zero the optimal parameters are given from the solution to the two linear equations a where we used the notation to denote to determine a and b a b n hence b and a is found by substituting this value for b into equation fxn yn. we can readily solve the in contrast to ordinary least squares regression pca from minimises the orthogonal projection of y to the line and is known as orthogonal least squares see linear parameter models for regression figure data from crickets the number of chirps per second versus the temperature in fahrenheit. example consider the data in in which we plot the number of chirps c per second for crickets versus the temperature t in degrees fahrenheit. a biologist believes that there is a simple relation between the number of chirps and the temperature of the form c a bt where she needs to determine the parameters a and b. for the cricket data the fit is plotted in for comparison we plot the fit from the pca which minimises the sum of the squared orthogonal projections from the data to the line. in this case there is little numerical difference between the two fits. linear parameter models for regression we can generalise on the idea of fitting straight lines to fitting linear functions of vector inputs. for a dataset yn n n a linear parameter regression model is defined yx wt where is a vector valued function of the input vector x. for example in the case of a straight line fit with a scalar input and output we have xt w bt we define the train error as the sum of squared differences between the observed outputs and the predictions under the linear model ew wt where n we now wish to determine the parameter vector w that minimises ew. writing out the error in terms of the components of w ew wi n i wj n j differentiating with respect to wk and equating to zero gives j i n yn n wi i n n k or in matrix notation k i yn n n ntw that the model is linear in the parameter w not necessarily linear in x. draft march per sectemperature linear parameter models for regression figure straight line regression fit to the cricket data. pca fit to the data. in regression we minimize the residuals the vertical distances from datapoints to the line. in pca the fit minimizes the orthogonal projections to the line. in this case there is little difference in the fitted lines. both go through the mean of the data the linear regression fit has slope and the pca fit has slope these are called the normal equations for which the solution is w n nt yn n although we write the solution using matrix inversion in practice one finds the numerical solution using gaussian since this is faster and more numerically stable. example a cubic polynomial fit a cubic polynomial is given by yx as a lpm this can be expressed using x the ordinary least squares solution has the form given in equation the fitted cubic polynomial is plotted in see also democubicpoly.m. example return. in we present fitting an lpm with vector inputs x to a scalar output y. the vector x represents factors that are believed to affect the stock price of a company with the stock price return given by the scalar y. a hedge fund manager believes that the returns may be linearly related to the factors yt wixit and wishes to fit the parameters w in order to use the model to predict future stock returns. this is straightforward using ordinary least squares this being simply an lpm with a linear function. see figure cubic polynomial fit to the cricket data. draft march per sectemperature per sectemperature per sectemperature linear parameter models for regression on yt figure predicting stock return using a linear lpm. the top five panels present the inputs for train days and test days the corresponding train output return y for each day is given in the bottom panel. the predictions are the predictions based i wixit with w trained using ordinary least squares. with a regularisation term the ols learned w is despite the simplicity of these models their application in the finance industry is widespread with significant investment made on collating factors x that may be indicative of future return. see demolpmhedge.m. for an example. such models also form the basis for more complex models in finance see for example vector outputs it is straightforward to generalise the above framework to vector outputs y. using a separate weight vector wi for each output component yi we have the mathematics follows similarly to before and we may define a training error per output as yix wt i ew ewi i i n i i wt yn since the training error decomposes into individual terms one for each output the weights for each output can be trained separately. in other words the problem decomposes into a set of independent scalar output problems. in case the parameters w are tied or shared amongst the outputs the training is still straightforward since the objective function remains linear in the parameters and this is left as an exercise for the interested reader. regularisation for most purposes our interest is not just to find the function that best fits the training data but one that that will generalise well. to control the complexity of the fitted function we may add an extra regularising penalty term to the training error to penalise rapid changes in the output. for example a regularising term that can be added to equation is yxn e xn yxn xn the factor factor e regularising term. penalises large differences in the outputs corresponding to two inputs. the has the effect of weighting more heavily terms for which two input vectors xn and are close together is a fixed length-scale parameter and determines the overall strength of the draft march linear parameter models for regression figure a set of fixed-width radial basis functions with the centres mi evenly spaced. by taking a linear combie nation of these functions we can form a flexible function class. since y wt expression can be written as wtrw where r xn n n e the regularised train error is then e wt wtrw n by differentiating the regularised training error and equating to zero we find the optimal w is given by w n nt r yn n in practice it is common to use a regulariser that penalises the sum square length of the weights wtw i i which corresponds to setting r i. regularising pararameters such as may be determined using a validation set a popular lpm is given by the non-linear function with components radial basis functions ix exp these basis functions are bump shaped with the center of the bump i being given by mi and the width by an example is given in in which several rbfs are plotted with different centres. in lpm regression we can then use a linear combination of these bumps to fit the data. one can apply the same approach using vector inputs. for vector x and centre m the radial basis function depends on the distance between x and the centre m giving a bump in input space example consider fitting the data in using radial basis functions uniformly spread over the input space with width parameter and regularising term wtw. the generalisation performance on the test data depends heavily on the width and regularising parameter in order to find reasonable values for these parameters we may use a validation set. for simplicity we set the regularisation parameter to and use the validation set to determine a suitable in we plot the validation error as a function of based on this graph we can find the best value of that which minimises the validation error. the predictions are also given in draft march the dual representation and kernels figure the are the training points and the are the validation points. the solid line is the correct underlying function which is corrupted with a small amount of additive noise to form the train data. the dashed line is the best predictor based on the validation set. a curse of dimensionality if the data has non-trivial behaviour over some region in x then we need to cover the region of x space fairly densely with bump type functions. in the above case we used basis functions for this one dimensional space. in dimensions if we wish to cover each dimension to the same discretisation level we would need basis functions. similarly for dimensions we would need functions. to fit such an lpm would require solving a linear system in more than variables. this explosion in the number of basis functions with the input dimension is a curse of dimensionality a possible remedy is to make the basis functions very broad so that each covers more of the high dimensional space. however this will mean a lack of flexibility of the fitted function since it is constrained to be smooth. another approach is to place basis functions centred on the training input points and add some more basis functions randomly placed close to the training inputs. the rational behind this is that when we come to do prediction we will most likely see novel x that are close to the training points we do not need to make accurate predictions over all the space. a further approach is to make the positions of the basis functions adaptive allowing them to be moved around in the space to minimise the error. this approach motivates the neural network an alternative is to reexpress the problem of fitting an lpm by reparameterising the problem as discussed below. the dual representation and kernels consider a set of training data with inputs x n n and corresponding outputs yn n n. for an lpm of the form fx wtx our interest is to find the best fit parameters w. we assume that we have found an optimal parameter w the nullspace of x are those x which are orthogonal to all the inputs in x that is xn x w x for all n. if we then consider the vector w with an additional component in the direction orthogonal to the space spanned by x xn wt xn figure the validation error as a function of the basis function width for the validation data in and rbfs in based on the validation error the optimal setting of the basis function width parameter is draft march error the dual representation and kernels the output of an rbf figure here function exp the combined and output for two rbfs with as above and this means that adding a contribution to w outside of the space spanned by x has no effect on the predictions on the train data. if the training criterion depends only on how well the lpm predicts the train data there is therefore no need to consider contributions to w from outside of x that is without loss of generality we may consider the representation w anxn the parameters a an are called the dual parameters. we can then write the output of the lpm directly in terms of the dual parameters wtxn am xn more generally for a vector function the solution will lie in the space spanned by w an and we may write wt am amk xn where we have defined a kernel function k xn in matrix form the output of the lpm on a training input x is then wt atkn where kn is the nth column of the gram matrix k. regression in the dual-space for ordinary least squares regression using equation we have a train error yn ea is analogous to the standard regression equation on interchanging a for w and kn for similarly the regularisation term can be expressed as draft march wtw anam atka the dual representation and kernels by direct analogy the optimal solution for a is therefore a kn k ynkn we can express the above solution more conveniently by first writing a k i ynk since kn is the nth column of k then k is the nth column of the identity matrix. with a little thought we can rewrite equation more simply as a i y where y is the vector with components formed from the training inputs yn using this the prediction for a new input x is given by yx kt i y where the vector k has components k xm this dual space solution shows that predictions can be expressed purely in terms of the kernel k this means that we may dispense with defining the vector functions and define a kernel function directly. this approach is also used in gaussian processes and enables us to use effectively very large infinite dimensional vectors without ever explicitly needing to compute them. note that the gram matrix k has dimension n n which means that the computational complexity of performing the be prohibitively expensive and numerical approximations are required. this is in contrast to the computational complexity of solving the normal equations in the original weight space viewpoint is o matrix inversion in equation is for moderate to large n than this will dim to an extent the dual parameterisation helps us with the curse of dimensionality since the complexity of learning in the dual parameterisation scales cubically with the number of training points not cubically with the dimension of the vector. positive definite kernels functions the kernel k in was defined as the scalar product between two vectors and for any set of points xm the resulting matrix zmkmnzn ztkz i m i n i is positive semi-definite since for any z zn n i zm m i zm m i mn i m n i m instead of specifying high-dimensional vectors we may instead specify a function kx that produces a positive definite matrix k for any inputs x such a function is called a covariance function or a positive kernel. for example a popular choice is e for this is commonly called the squared exponential kernel. for this is known as the ornstein-uhlenbeck kernel. covariance functions are discussed in more detail in draft march linear parameter models for classification figure the logistic sigmoid function x. the parameter determines the steepness of the sigmoid. the full line is for and the dashed for as the logistic sigmoid tends to a heaviside step function. the dotted curve is the error function erf x for which closely matches the standard logistic sigmoid with linear parameter models for classification in a binary classification problem we are given some training data d cn n n where the targets c inspired by the lpm regression model we can assign the probability that a novel input x belongs to class using pc fxtw where fx in the statistics literature fx is termed a mean function the inverse function f is the link function. two popular choices for the function fx are the and probit functions. the logit is given by fx ex ex e x which is also called the logistic sigmoid and written the scaled version is defined as x a closely related model is probit regression which uses in place of the logistic sigmoid the error function the cumulative distribution of the standard normal distribution fx e dt erfx this can also be written in terms of the standard error function x x erfx e dt the shape of the probit and logistic functions are similar under rescaling see we focus below on the logit function. similar derivations carry over in a straightforward manner to any monotonic mean function. logistic regression logistic regression corresponds to the model pc xtw where b is a scalar and w is a vector. as the argument b xtw of the sigmoid function increases the probability x belongs to class increases. the decision boundary the decision boundary is defined as that set of x for which pc pc this is given by the hyperplane b xtw draft march linear parameter models for classification w figure the decision boundary pc line. for two dimensional data the decision boundary is a line. if all the training data for class circles lie on one side of the line and for class circles on the other the data is said to be linearly separable. on the side of the hyperplane for which b xtw inputs x are classified as s and on the other side they are classified as s. the bias parameter b simply shifts the decision boundary by a constant amount. the orientation of the decision boundary is determined by w the normal to the hyperplane see to clarify the geometric interpretation let x be a point on the decision boundary and consider a new point x x w where w is a vector perpendicular to w so that wtw then b wtx b x w b wtx wtw b wtx thus if x is on the decision boundary so is x plus any vector perpendicular to w. in d dimensions the space of vectors that are perpendicular to w occupy a d dimensional hyperplane. for example if the data is two dimensional the decision boundary is a one dimensional hyperplane a line as depicted in linear separability and linear independence definition separability. if all the training data for class lies on one side of a hyperplane and for class on the other the data is said to be linearly separable. for d dimensional data provided there are no more than d training points then these are linearly separable provided they are linearly independent. to see this let cn if xn is in class and cn if xn is in class for the data to be linearly separable we require wtxn b n n where is an arbitrarily small positive constant. the above equations state that each input is just the correct side of the decision boundary. if there are n d datapoints the above can be written in matrix form as xw b c where x is a square matrix whose nth column contains xn. provided that x is invertible the solution is w x b the bias b can be set arbitrarily. this shows that provided the xn are linearly independent we can always find a hyperplane that linearly separates the data. provided the data are not-collinear occupying the same d dimensional subspace the bias can be used to improve this to enabling d arbitrarily labelled points to be linearly separated in d dimensions. a dataset that is not linearly separable is given by the following four training points and class labels draft march linear parameter models for classification figure the xor problem. this is not linearly separable. this data represents the xor function and is plotted in this function is not linearly separable since no straight line has all inputs from one class on one side and the other class on the other. classifying data which is not linearly separable can only be achieved using a non-linear decision boundary. it might be that data is non-linearly separable in the original data space. however by mapping to a higher dimension using a non-linear vector function we generate a set of non-linearly dependent highdimensional vectors which can then be separated using a high-dimensional hyperplane. we will discuss this in the perceptron the perceptron assigns x to class if b wtx and to class otherwise. that is pc xtw where the step function is defined as if we consider the logistic regression model pc b xtw x x and take the limit we have the perceptron like classifier pc b xtw b xtw b xtw the only difference between this probabilistic perceptron and the standard perceptron is in the technical definition of the value of the step function at the perceptron may therefore essentially be viewed as a limiting case of logistic regression. maximum likelihood training given a data set d how can we learn the weights to obtain good classification? probabilistically if we assume that each data point has been drawn independently from the same distribution that generates the data standard i.i.d. assumption the likelihood of the observed data is explicitly the conditional dependence on the parameters b w pdb w pcnxn b wpxn pc b wcn pc b cn pxn where we have used the fact that cn for this discriminative model we do not model the input distribution px so that we may equivalently consider the log likelihood of the output class variables conditioned on the training inputs for logistic regression this gives lw b cn log wtxn cn log draft march wtxn gradient ascent linear parameter models for classification there is no closed form solution to the maximisation of lw b which needs to be carried out numerically. one of the simplest methods is gradient ascent for which the gradient is given by wl bxn here we made use of the derivative relation d dl db b for the logistic sigmoid. the derivative with respect to the bias is the gradient ascent procedure then corresponds to updating the weights and bias using wnew w wl bnew b dl db where the learning rate is a scalar chosen small enough to ensure convergence. the application of the above rule will lead to a gradual increase in the log likelihood. batch training writing the updates explicitly gives wnew w bxn bnew b b this is called a batch update since the parameters w and b are updated only after passing through the whole of training data. this batch version converges in all cases since the error surface is bowl shaped below. for linearly separable data however the optimal setting is for the weights to become infinitely large since then the logistic sigmoids will saturate to or below. a stopping criterion based on either minimal changes to the log likelihood or the parameters is therefore required to halt the optimisation routine. for non-linearly separable data the likelihood has a maximum at finite w so the algorithm converges. however the predictions will be less certain reflected in a broad confidence interval see in batch training the zero gradient criterion is bxn ment that for a zero gradient cn meaning that the weights must tend to infinity for this in the case that the inputs xn n n are linearly independent we immediately have the require condition to hold. for linearly separable data we can also show that the weights must become infinite at convergence. taking the scalar product of equation with w we have the zero gradient requirement n wtxn where n for simplicity we assume b for linearly separable data we have c c wtxn draft march linear parameter models for classification then using the fact that n we have n wtxn c c figure the decision boundary pc line and confidence boundaries pc and pc and iteraa tions of batch gradient ascent with non-linearly separalinearly separable data. ble data. note how the confidence interval remains broad see demologreg.m. each term n wtxn is non-negative and the zero gradient condition requires the sum of these terms to be zero. this can only happen if all the terms are zero implying that cn n requiring the sigmoid to saturate for which the weights must be infinite. online training in practice it is common to update the parameters after each training example has been considered wnew w n bxn bnew b n b an advantage of online training is that the dataset does not need to be stored since only the performance on the current input is required. provided that the data is linearly separable the above online procedure converges is not too large. however if the data is not linearly separable the online version will not converge since the opposing class labels will continually pull the weights one way and then the other as each conflicting example is used to form an update. for the limiting case of the perceptron with and linearly separable data online updating converges in a finite number of but does not converge for non-linearly separable data. geometry of the error surface the hessian of the log likelihood lw is the matrix with n ijn ij hij wiwj xn i xn j n this is negative definite since for any z zihijzj zixn i zjxn j n zixn i in this means that the error surface is concave upside down bowl and gradient ascent is guaranteed to converge to the optimal solution provided the learning rate is small enough. example handwritten digits. we apply logistic regression to the handwritten digits of in which there are ones and sevens in the train data. using gradient ascent training with a suitably chosen stopping criterion the number of errors made on the test points is compared with errors using nearest neighbour methods. see for a visualisation of the learned w. simplicity we ignore the bias b. this can readily be dealt with by extending x to a d dimensional vector x with a in the d component. then for a d dimensional w we have wt x wtx draft march the kernel trick for classification figure logistic regression for classifying hand written digits and displayed is a hinton diagram of the learned weight vector w plotted as a image for visual interpretation. green are positive and an input x with a value in this component will tend to increase the probability that the input is classed as a similarly inputs with positive contributions in the red regions tend to increase the probability as being classed as a digit. note that the elements of each input x are either positive or zero. beyond first order gradient ascent since the surface has a single optimum a newton update wnew wold h where h is the hessian matrix as above and will typically converge much faster than gradient ascent. for large scale problems the inversion of the hessian is computationally demanding and limited memory bfgs or conjugate gradient methods may be considered as more practical alternatives see avoiding overconfident classification provided the data is linearly separable the weights will continue to increase and the classifications will become extreme. this is undesirable since the classifications will be over-confident. this can be prevented by adding a penalty term to the objective function l b lw b wtw. the scalar constant encourages smaller values of w that we wish to maximise the log likelihood. an appropriate value for can be determined using validation data. multiple classes for more than two classes one may use the softmax function pc ix ewt i xbi ewt j xbj where c is the number of classes. when c this reduced to the logistic sigmoid. one can show that the likelihood for this case is also concave see and the kernel trick for classification a drawback of logistic regression as described above is the simplicity of the decision surface a hyperplane. one way to extend the method to more complex non-linear decision boundaries is to consider mapping the inputs x in a non-linear way to pc wt b for example the one-dimensional input x could get mapped to a two dimensional vector sinx. mapping into a higher dimensional space makes it easier to find a separating hyperplane since any set of points that are linearly independent can be linearly separated provided we have as many dimensions as datapoints. draft march support vector machines figure logistic regression pc using a quadratic function iterations of gradient ascent training were performed with a learning rate plotted are the datapoints for the two classes cross and circle and the equal probability contours. the decision boundary is the contour. see demologregnonlinear.m. for the maximum likelihood criterion we can use exactly the same algorithm as before on replacing x with see for a demonstration using a quadratic function. since only the scalar product between the vectors plays a role the dual representation may be used in which we assume the weight can be expressed in the form n n w n we then subsequently find a solution in terms of the dual parameters n. this is potentially advantageous since there may be less training points than dimensions of the classifier depends only on scalar products which can be written in terms of a positive definite kernel n pc ankx xn for convenience we can write the above as pc atkx where the n dimensional vector kx has elements kx xn. then the above is of exactly the same form as the original specification of logistic regression namely as a function of a linear combination of vectors. hence the same training algorithm to maximise the likelihood can be employed simply on replacing xn with kxn. the details are left to the interested reader and follow closely the treatment of gaussian processes for classification support vector machines like kernel logistic regression svms are a form of kernel linear classifier. however the svm uses an objective which more explicitly encourages good generalisation performance. svms do not fit comfortably within a probabilistic framework and as such we describe them here only briefly referring the reader to the wealth of excellent literature on this the description here is inspired largely by maximum margin linear classifier in the svm literature it is common to use and to denote the two classes. for a hyperplane defined by weight w and bias b a linear discriminant is given by wtx b wtx b class class draft march support vector machines figure svm classification of data from two classes circles and filled circles. the decision boundary wtx b line. for linearly separable data the maximum margin hyperplane is equidistant from the closest opposite class points. these support vectors are highlighted in blue and the margin in red. the distance of the decision boundary from the origin is b wtw and the distance of a general point x from the origin along the direction w is xtw wtw. w origin for a point x that is close to the decision boundary at wtx b a small change in x can lead to a change in classification. to make the classifier more robust we therefore impose that for the training data at least the decision boundary should be separated from the data by some finite margin in the first instance that the data is linearly separable wtx b wtx b class class since w b and can all be rescaled arbitrary we need to fix the scale of the above to break this invariance. it is convenient to set so that a point x from class that is closest to the decision boundary satisfies wtx b and a point x from class that is closest to the decision boundary satisfies wtx b from vector algebra the distance from the origin along the direction w to a point x is given by wtx wtw the margin between the hyperplanes for the two classes is then the difference between the two distances along the direction w which is wt wtw x wtw to set the distance between the two hyperplanes to be maximal we need to minimise the length wtw. given that for each xn we have a corresponding label yn in order to classify the training labels correctly and maximise the margin the optimisation problem is therefore equivalent to subject to minimise wtw this is a quadratic programming problem. note that the factor is just for convenience. wtxn b n n to account for potentially mislabelled training points for data that is not linearly separable we relax the exact classification constraint and use instead wtxn b n where n here each n measures how far xn is from the correct margin. for n datapoint xn is on the correct side of the decision boundary. however for n the datapoint is assigned the opposite class to its training label. ideally we want to limit the size of these violations n. here we briefly describe two standard approaches. draft march support vector machines soft-margin n w origin figure slack margin. the term n measures how far a variable is from the wrong side of the margin for its class. if n then the point will be misclassified treated as an outlier. the soft-margin objective is minimise wtw c n subject to wtxn b n n n where c controls the number of mislabellings of the training data. the constant c needs to be determined empirically using a validation set. the optimisation problem expressed by can be formulated using the lagrangian lw b for points xn on the correct side of the decision boundary which is to be minimised with respect to x b and maximised with respect to wtw c wtxn b n n n so that maximising l with respect to requires the corresponding n to be set to zero. only training points that are support vectors lying on the decision boundary will have non-zero n. n n differentiating the lagrangian and equating to zero we have the conditions lw b wi lw b n n wi b nynxn i nyn n lw b c n n from this we see that the solution for w is given by w n nynxn since only the support vectors have non-zero n the solution for w will typically depend on only a small number of the training data. using these conditions and substituting back into the original problem the objective is equivalent to minimising n l subject to n nm n ynym n m xm n yn n n if we define kxn xm xm draft march support vector machines figure svm training. the solid red and solid blue circles represent training data from different classes. the support vectors are highlighted in green. for the unfilled test points the class assigned to them by the svm is given by the colour. see demosvm.m the optimisation problem is maximize subject to n n nm yn n n kxn xm c nm ynym n m n optimising this objective is discussed in soft-margin constraint in the soft-margin version one uses a penalty n c n to give the optimisation problem minimise wtwc subject to wtxn b n n n n where c is an empirically determined penalty factor that controls the number of mislabellings of the training data. to reformulate the optimisation problem we use the lagrangian lw b wtwc n n n wtxn b n n n rn n n n rn the variables rn are introduced in order to give a non-trivial solution n c. following a similar argument as for the case by differentiating the lagrangian and equating to zero we arrive at the optimisation problem maximize subject to n n nm yn n n ynym n mkxn xm n c which is closely related to the problem except that we now have the box-constraint n c. using kernels the final objectives and depend on the inputs xn only via the scalar product xn. if we map x to a vector function of x then we can write kxn xm this means that we can use any positive definite kernel k and make a non-linear classifier. see draft march soft zero-one loss for outlier robustness performing the optimisation both of the above soft-margin svm optimisations problems and are quadratic programs for which the exact computational cost scales as whilst these can be solved with general purpose mal optimisation whose practical performance is typically or better. a variant of routines specifically tailored routines that exploit the structure of the problem are preferred in practice. of particular practical interest are chunking techniques that optimise over a subset of the in the limit of updating only two components of this can be achieved analytically resulting in the sequential mini this algorithm is provided in svmtrain.m. once the optimal solution is found the decision function for a new point x is n ynkxn x b assign to class assign to class the optimal b is determined using the maximum margin condition n b min wt xn max yn wt xn probabilistic interpretation kernelised logistic-regression has some of the characteristics of the svm but does not express the large margin requirement. also the sparse data usage of the svm is similar to that of the relevance vector machine we discuss in however a probabilistic model whose map assignment matches exactly the svm is hampered by the normalisation requirement for a probability distribution. whilst arguably no fully satisfactory direct match between the svm and a related probabilistic model has been achieved approximate matches have been soft zero-one loss for outlier robustness both the support vector machine and logistic regression are potentially mislead by outliers. for the svm a mislabelled datapoint that is far from the correct side of the decision boundary would require a large slack however since exactly such large are discouraged it is unlikely that the svm would admit such a solution. for logistic regression the probability of generating a mislabelled point far from the correct side of the decision boundary is so exponentially small that this will never happen in practice. this means that the model trained with maximum likelihood will never present such a solution. in both cases therefore mislabelled points outliers have a significant impact on the location of the decision boundary. a robust technique to deal with outliers is to use the zero-one loss in which a mislabeled point contributes only a relatively small loss. soft variants of this are obtained by using the objective wtxn wtw which is to be minimised with respect to w and b. for the first term above tends to the zero-one loss. the second term represents a penalty on the length of w and prevents overfitting. kernel extensions of this soft zero-one loss are straightforward. unfortunately the objective is highly non-convex and finding the optimal w b is computationally difficult. a simple-minded scheme is to fix all components of w except one wi and then perform a numerical one-dimensional optimisation over this single parameter wi. at the next step another parameter wj is chosen and the procedure repeated until convergence. as usual can be set using validation. the practical difficulties of minimising non-convex high-dimensional objective functions means that these approaches are rarely used in practice. a discussion of practical attempts in this area is given in draft march exercises figure soft zero-one loss decision boundary line versus logistic regression line. the number of mis-classified training points using the soft zero-one loss is compared to for logistic regression. the penalty was used for the soft-loss with for logistic regression no penalty term was used. the outliers have a significant impact on the decision boundary for logistic regression whilst the soft zero-one loss essentially gives up on the outliers and fits a large margin classifier between the remaining points. see demosoftloss.m. an illustration of the difference between logistic regression and this soft zero-one loss is given in which demonstrates how logistic regression is influenced by the mass of the data points whereas the zero-one loss attempts to mimimise the number of mis-classifications whilst maintaining a large margin. notes the perceptron has a long history in artificial intelligence and machine learning. rosenblatt discussed the perceptron as a model for human learning arguing that its distributive nature input-output patterns are stored in the weight vector is closely related to the kind of information storage believed to be present in biological to deal with non-linear decision boundaries the main thrust of research in the ensuing neural network community was on the use of multilayered structures in which the outputs of perceptrons are used as the inputs to other perceptrons resulting in potentially highly non-linear discriminant functions. this line of research was largely inspired by analogies to biological information processing in which layered structures are prevalent. such multilayered artificial neural networks are fascinating and once trained are extremely fast in forming their decisions. however reliably training these systems is a highly complex task and probabilistic generalisations in which priors are placed on the parameters lead to computational difficulties. whilst perhaps less inspiring from a biological viewpoint the alternative route of using the kernel trick to boost the power of a linear classifier has the advantage of ease of training and generalisation to probabilistic variants. more recently however there has been a resurgence of interest in the multilayer systems with new heuristics aimed at improving the difficulties in training see for example code democubicpoly.m demo of fitting a cubic polynomial demologreg.m demo logistic regression logreg.m logistic regression gradient ascent training demologregnonlinear.m demo of logistic regression with a non-linear svmtrain.m svm training using the smo algorithm demosvm.m svm demo demosoftloss.m softloss demo softloss.m softloss function exercises exercise give an example of a two-dimensional dataset for which the data are linearly separable but not linearly independent. can you find a dataset which is linearly independent but not linearly separable? draft march exercises the fitted lines go through the ynn. exercise show that for both ordinary and orthogonal least squares regression fits to data yn n n exercise consider the softmax function for classifying an input vector x into one of c c classes using c ewt ewt x pcx a set of input-class examples is given by d cn n n. write down the log-likelihood l of the classes conditional on the inputs assuming that the data is i.i.d. compute the hessian with elements hij wiwj where w is is the stacked vector w wt wt c and show that the hessian is positive semi-definite that is zthz for any z. exercise derive from equation the dual optimisation problem equation exercise a datapoint x is projected to a lower dimensional vector x using x mx where m is a fat matrix. for a set of data xn n n and corresponding binary class labels yn using logistic regression on the projected datapoints xn corresponds to a form of constrained logistic regression in the original higher dimensional space x. explain if it is reasonable to use an algorithm such as pca to first reduce the data dimensionality before using logistic regression. exercise the logistic sigmoid function is defined as what is the inverse function exercise given a dataset d cn n n where cn logistic regression uses the model pc b. assuming the data is drawn independently and identically show that the derivative of the log likelihood l with respect to w is wl cn wtxn b xn exercise consider a dataset d cn n n where cn and x is a d dimensional vector. show that if the training data is linearly separable with the hyperplane wtx b the data is also separable with the hyperplane wtx b where w w b b for any scalar what consequence does the above result have for maximum likelihood training of linearly separable data? exercise consider a dataset d cn n n where cn and x is a n dimensional vector. we have n datapoints in a n dimensional space. in the text we showed that we can find a hyperplane by b that linearly separates this data we need for each datapoint xn wtxn b where for cn and for cn comment on the relation between maximum likelihood training and the algorithm suggested above. draft march exercise given training data d yn n n you decide to fit a regression model y mx c to this data. derive an expression for m and c in terms of d using the minimum sum squared error criterion. exercise given training data d cn n n cn where x are vector inputs a discriminative model is exercises pc where gx exp and ex is a neural with a single hidden layer and two hidden units. x x write down the log likelihood for the class conditioned on the inputs based on the usual i.i.d. as sumption. calculate the derivatives of the log likelihood as a function of the network parameters v comment on the relationship between this model and logistic regression. comment on the decision boundary of this model. draft march chapter bayesian linear models regression with additive gaussian noise the linear models in were trained under maximum likelihood and do not deal with the issue that from a probabilistic perspective parameter estimates are inherently uncertain due to the limited available training data. regression refers to inferring a mapping on the basis of observed data d yn n n where yn represents an input-output pair. we discuss here the scalar output case vector inputs x with the extension to the vector output case y is straightforward. we assume that each output is generated from a model f w where the parameters w of the function f are unknown. an output y is generated by the addition of noise to the clean model output the model generates an output y for input x with y f w if the noise is gaussian distributed n probability fx w pyw x n e f if we assume that each data input-output pair is generated identically and independently the likelihood the model generates the data is pdw pynw xnpxn we may use a prior weight distribution pw to quantify our a priori belief in the suitability each parameter setting. writing d the posterior weight distribution is then given by pwd pdwpw pdywdxpw using the gaussian noise assumption and for convenience defining this gives log pwd fxn log p n log const. note the similarity between equation and the regularised training error equation in the probabilistic framework we identify the choice of a sum square error with the assumption of additive gaussian noise. similarly the regularising term is identified with log pw. regression with additive gaussian noise w xn yn n figure belief network representation of a bayesian model for regression under the i.i.d. data assumption. the hyperparameter acts as a form of regulariser controlling the flexibility of the prior on the weights w. the hyperparameter controls the level of noise on the observations. bayesian linear parameter models linear parameter models as discussed in have the form fx w wi ix wt where the parameters wi are also called weights and dim w b. such models have a linear parameter dependence but may represent a non-linear input-output mapping if the basis functions ix are nonlinear in x. since the output scales linearly with w we can discourage extreme output values by penalising large weight values. a natural weight prior is thus pw n b wtw e yn wt where the precision is the inverse variance. if is large the total squared length of the weight vector w is encouraged to be small. under the gaussian noise assumption the posterior distribution is log pw wtw const. where represents the hyperparameter set. parameters that determine the functions may also be included in the hyperparameter set. using the lpm in equation with a gaussian prior equation and completing the square the weight posterior is a gaussian distribution pw n m s where the covariance and mean are given by s i t m s yn the mean prediction for an input x is then given by fx fx wpwd mt similarly the variance of the underlying estimated clean function is varfx wt txs the output variance varfx depends only on the input variables and not on the training outputs y. since the additive noise is uncorrelated with the model outputs the predictive variance is varyx varfx and represents the variance of the noisy output for an input x. draft march regression with additive gaussian noise figure along the horizontal axis we plot the input x and along the vertical axis the output t. prediction using regularised training and fixed hyperparamethe raw input-output training data. prediction using ml-ii optimised hyperparameters. also plotted are standard error bars on ters. the clean underlying example in we show the mean prediction on the data in using gaussian basis functions ix with width and centres ci spread out evenly over the input space from to we set the other hyperparameters by hand to and the prediction severely overfits the data a result of a poor choice of hyperparameter settings. this is resolved in using the ml-ii parameters as described below. determining hyperparameters ml-ii the hyperparameter posterior distribution is p pd a simple summarisation of the posterior is given by the map assignment which takes the single optimal setting argmax p if the prior belief about the hyperparameters is weak const. this is equivalent to using the that maximises the marginal likelihood pd pd wpw this approach to setting hyperparameters is called ml-ii or the evidence in the case of bayesian linear parameter models under gaussian additive noise computing the marginal likelihood equation involves only gaussian integration. a direct approach to deriving an expression for the marginal likelihood is to consider yn wt wtw pd wpw exp draft march regression with additive gaussian noise where d by collating terms in w the square the above represents a gaussian in w with additional factors. after integrating over this gaussian we have log pd dts log det b log n log n log n see for an alternative expression. example using the hyperparameters that optimise expression gives the results in where we plot both the mean predictions and standard predictive error bars. this demonstrates that an acceptable setting for the hyperparameters can be obtained by maximising the marginal likelihood. learning the hyperparameters using em we can set hyperparameters such as and by maximising the marginal likelihood equation a convenient computational procedure to achieve this is to interpret the w as latent variables and apply the em algorithm in this case the energy term is according to the general em procedure we need to maximise the energy term. for a hyperparameter the derivative of the energy is given by for the bayesian lpm with gaussian weight and noise distributions we obtain pwd old e e pdw old log pdw yn mt yn mt new yn wt n e n trace n s s solving for the zero derivatives gives the m-step update pw oldd trace s txn where s is the empirical covariance of the basis-function vectors n n. similarly for e b wtw b trace mtm pw oldd which on equating to zero gives the update new b trace mtm where s and m are given in equation an alternative fixed point procedure that is often more rapidly convergent than em is given in equation closed form updates for other hyperparameters such as the width of the basis functions are generally not available and the corresponding energy term needs to be optimised numerically. draft march regression with additive gaussian noise figure predictions for an rbf for different widths for each the ml-ii optimal are obtained by running the em procedure to convergence and subsequently used to form the predictions. in each panel the dots represent the training points with x along the horizontal axis and y along the vertical axis. mean predictions are plotted along with predictive error bars of one standard deviation. according to ml-ii the best model corresponds to see the smaller values of overfit the data giving rise to too rough functions. the largest values of underfit giving too smooth functions. see demobayeslinreg.m. hyperparameter optimisation using the gradient hyperparameters such as can be set by maximising the marginal likelihood pd w pdw to find the optimal we search for the zero derivative of log pd from equation we can use the general derivative identity to arrive at log pw pw log pd since log pw wtw b log const. figure the log marginal likelihood log pd having found the optimal values of the hyperparameters and using ml-ii. these optimal values are dependent on according to ml-ii the best model corresponds to draft march marginal likelihoodlambda wtw b pw we obtain log pd wtw pw b setting the derivative to zero the optimal satisfies one may now form a fixed point equation regression with additive gaussian noise new b trace which is in fact a re-derivation of the em procedure for this model. for a gaussian posterior pw n m s wtw trace mtm new b trace mtm gull-mackay fixed point iteration from equation we have wtw pw b s mtwt b so that an alternative fixed point is new b s mtm in practice this update converges more rapidly than equation example the basis function widths. in we plot the training data for a regression problem using a bayesian lpm. a set of radial basis functions are used ix with ci i spread out evenly between and the hyperparameters and are learned by ml-ii under em updating. for a fixed width we then present the predictions each time finding the optimal and for this width. the optimal joint hyperparameter setting is obtained as described in which shows the marginal log likelihood for a range of widths. validation likelihood the hyperparameters found by ml-ii are those which are best at explaining the training data. in principle this is different from those that are best for prediction and in practice therefore it is reasonable to set hyperparameters also by validation techniques. one such method is to set hyperparameters by minimal prediction error on a validation set. another common technique is to set hyperparameters by their likelihood on a validation set val ym w val m m pyvalw pyval draft march regression with additive gaussian noise from which we obtain log pyval where yval val cval vals t and the design matrix t val ym val val val log det cval valmt c val valm the optimal hyperparameters can then be found by maximising with respect to prediction the mean function predictor based on hyperparameters and weights w is given by fx fx wpw fx wpw p the term in curly brackets is the mean predictor for fixed hyperparameters. then weights each mean predictor by the posterior probability of the hyperparameter p this is a general recipe for combining model predictions where each model is weighted by its posterior probability. however computing the integral over the hyperparameter posterior is numerically challenging and approximations are usually required. provided the hyperparameters are well determined by the data we may instead approximate the above hyperparameter integral by finding the map hyperparameters and use fx fx wpw the relevance vector machine the relevance vector machine assumes that only a small number of components of the basis function vector are relevant in determining the solution for w. for a predictor fx w wi ix wt it is often the case that some basis functions will be redundant in the sense that a linear combination of the other basis functions can reproduce the training outputs with insignificant loss in accuracy. to exploit this effect and seek a parsimonious solution we may use a more refined prior that encourages each wi itself to be small the modifications required to the description of are to replace s with s diag t the marginal likelihood is then given by log pd dts log det log i n log n log the em update for is unchanged and the em update for each i is new i i draft march pw i pwi i n where the prior on each individual weight is given by i i i i e pwi i algorithm evidence procedure for bayesian logistic regression initialise w and while not converged do end while find optimal w by iterating equation equation to convergence. update according to equation classification for the logistic regression model i pc x wi ix classification e-step m-step the maximum likelihood method returns only a single optimal w. to deal with the inevitable uncertainty in estimating w we need to determine the posterior distribution of the weights w. to do so we first define a prior on the weights pw for which a convenient choice is a gaussian e pw n where is the inverse variance called the precision. given a dataset of input-class labels d cn n n the parameter posterior is pd pw pw pdw pcnxn w pd unfortunately this distribution is not of any standard form and exactly inferring statistics such as the mean or the most probable value are formally computationally intractable. hyperparameter optimisation hyperparameters such as can be set by maximising the marginal likelihood pd pdwpw w pcnxn w w e wtw there are several approaches one could take to approximate this and below we discuss the laplace and a variational technique. common to all approaches however is the form of the gradient differing only in the statistics under an approximation to the posterior. for this reason we derive first generic hyperparameter update formulae that apply under all approximations. to find the optimal we search for the zero derivative of log pd this is equivalent to the linear regression case and we immediately obtain setting the derivative to zero an exact equation is that the optimal satisfies wtw b pw log pd wtw pw b one may now form a fixed point equation new b draft march classification the averages in the above expression cannot be computed exactly and are replaced by averages with respect an approximation of the posterior qw note that since we only have an approximation to the posterior and therefore the mean and covariance statistics we cannot guarantee that the likelihood will always increase. for a gaussian approximation of the posterior qw n m s wtw trace trace mtm new b trace mtm in this case the gull-mackay alternative fixed point is new b s mtm the hyperparameter updates and have the same form as for the regression model. the mean m and covariance s of the posterior in the regression and classification cases are however different. in the classification case we need to approximate the mean and covariance as discussed below. laplace approximation the weight posterior is given by ew pw e where ew wtw log hn n by approximating ew by a quadratic function in w we obtain a gaussian approximation qwd to pwd to do so we first find the minimum of ew. differentiating we obtain it is convenient to use a newton method to find the optimum. the hessian matrix with elements e w nhn n wi wj ew hij is given by h i n n nt j note that the hessian is positive semidefinite so that the function ew is convex shaped and finding a minimum of ew is numerically unproblematic. a newton update then is wnew w h e given a converged w the posterior approximation is given by qwd n m s where m w is the converged estimate of the minimum point of ew and h is the hessian of ew at this point. s h draft march classification wtw e e w ew approximating the marginal likelihood the marginal likelihood is given by pd pdwpw w w pcnxn w log n for an optimum value m w we approximate the marginal likelihood using log pd l log det i j b log given this approximation l to the marginal likelihood an alternative strategy for hyperparameter optimisation is to optimises l with respect to by differentiating l directly the reader may show that the resulting updates are in fact equivalent to using the general condition equation under a laplace approximation to the posterior statistics. making predictions ultimately our interest is to classify in novel situations averaging over posterior weight uncertainty pc w pc wpw the b dimensional integrals over w cannot be computed analytically and numerical approximation is required. in this particular case the relative benign nature of the posterior log posterior is concave see below suggests that a simple laplace approximation may suffice for a variational approximation. to make a class prediction for a novel input x we use pc pc wpwd xtw pwd however since the term depends on w via the scalar product xtw we only require the integral to compute the predictions it would appear that we need to carry out an integral in b dimensions. over the one-dimensional projection h xtw so that pc phxd h xtm xt x phxd n under the laplace approximation w is gaussian pwd n m s. since h is a projection of w h is also gaussian distributed predictions may then be made by numerically evaluating the one-dimensional integral over the gaussian distribution in h equation approximating the gaussian average of a logistic sigmoid predictions under a gaussian posterior approximation require the computation of i draft march classification figure bayesian logistic regression with the rbf e placing basis functions centred on a subset of the training points. the green points are training data from class and the red points are training data from class the contours represent the probability of being in class the optimal value of found by the evidence procedure in this case is is set by hand to see demobayeslogregression.m where xt xt x. gaussian quadrature is an obvious numerical an alternative is to replace the logistic sigmoid by a suitably transformed erf the reason being that the gaussian average of an erf function is another erf function. using a single erf an approximation erf x these two functions agree at a reasonable criterion is that the derivatives of these two should agree at x since then they have locally the same slope around the origin and have globally similar shape. using and that the derivative is this requires a more accurate approximation can be found by considering ui i erf ix i ui suitable values for ui and i are given in logsigapp.m which uses a linear combination of erf functions to approximate the logistic sigmoid. to compute the approximate average of over a gaussian one may then make use of the result erf e d dx erf d since e we have erf x dx e erf x dx i uierf i i further approximate statistics can be obtained using the results derived in x that the definition of the erf function used here is taken to be consistent with matlab namely that erfx e dt. other authors define it to be the cumulative density function of a standard gaussian x e dt. draft march classification figure classification using the rvm with rbf e placing a basis function on a subset of the training data points. the green points are training data from class and the red points are training data from class the contours represent the probability of being in class training points. the training points weighted by their relevance value n. nearly all the points have a value so small that they effectively vanish. see demobayeslogregrvm.m relevance vector machine for classification in adopting the rvm prior to classification we encourage individual weights to be small pw i where pwi i n pwi i wi i n the only alterations in the previous evidence procedure are ei iwi nhn i h diag j these are used in the newton update formula as before. the em update equation for the s is given by new i i sii where h similarly the gull-mackay update is given by new i isii i running this procedure one typically finds that many of the s tend to infinity and the corresponding weights are pruned from the system. the remaining weights tend correspond to basis functions the rbf case in the centres of mass of clusters of datapoints of the same class see contrast this with the situation in svms where the retained datapoints tend to be on the decision boundaries. the number of training points retained by the rvm tends to be very small smaller indeed that the number retained in the svm framework. whilst the rvm does not support large margins and hence may be a less robust classifier it does retain the advantages of a probabilistic a potential critique of the rvm coupled with an ml-ii procedure for learning the i is that it is overly aggressive in terms of pruning. indeed as one may verify running demobayeslogregrvm.m it is common to find an instance of a problem for which there exists a set of i such that the training data can be classified perfectly however after using ml-ii so many of the i are set to zero that the training data can no longer be classified perfectly. draft march exercises multi-class case we briefly note that the multi-class case can be treated by using the softmax function under a one-of-m class coding scheme. the class probabilities are pc my eym which automatically enforces the constraint classes the cost of the laplace approximation scales as however one may show by careful implementation that the cost may be reduced to only analogous to the cost savings possible in m pc m naively it would appear that for c the gaussian process classification model code demobayeslinreg.m demo of bayesian linear regression bayeslinreg.m bayesian linear regression demobayeslogregrvm.m demo of bayesian logistic regression bayeslogregressionrvm.m bayesian logistic regression avsigmagauss.m approximation of the gaussian average of a logistic sigmoid logsigapp.m approximation of the logistic sigmoid using mixture of erfs exercises exercise the exercise concerns bayesian regression. show that for f wtx and pw n that pfx is gaussian distributed. furthermore find the mean and covariance of this gaussian. what is pft x? consider a target point t f where n exercise a bayesian linear parameter regression model is given by yn wt n in vector notation y this can be written with t and is a zero mean gaussian distributed vector with covariance y w an expression for the marginal likelihood of a dataset is given in equation a more compact expression can be obtained by considering xn since yn is linearly related to xn through w. then y is gaussian distributed with mean and covariance matrix for pw n draft march w w show that the covariance matrix can be expressed as c i t hence show that the log marginal likelihood can we written as log xn log det c ytc exercises exercise using as a basis derive expression for the log likelihood on a validation set. exercise consider the function ew as defined in equation compute the hessian matrix which has elements hij wi wj ew ij n n n nt show that the hessian is positive semidefinite exercise show that for any function f fxtwpwdw fhphdh where ph is the distribution of the scalar xtw. the significance of this result is that any high-dimensional integral of the above form can be reduced to a one-dimensional integral over the distribution of the field exercise this exercise concerns bayesian logistic regression. our interest to derive a formula for the optimal regularisation parameter based on the laplace approximation to the marginal log-likelihood given by log pd l log n log det i j b log n log as in equation the laplace procedure finds first an optimal w that minimises which will depend on the setting of formally therefore in finding the that optimises l we should make use of the total derivative formula dl d l l wi wi however when evaluated at w w l w this means that in order to compute the derivative with respect to we only need consider the terms with an explicit dependence. equating the derivative to zero and using log det i show that the optimal satisfies the fixed point equation new n trace i j draft march chapter gaussian processes non-parametric prediction gaussian processes are flexible bayesian models that fit well within the probabilistic modelling framework. in developing gps it is useful to first step back and see what information we need to form a predictor. given a set of training data d yn n n x y where xn is the input for datapoint n and yn the corresponding output continuous variable in the regression case and a discrete variable in the classification case our aim is to make a prediction y for a new input x in the discriminative framework no model of the inputs x is assumed and only the outputs are modelled conditioned on the inputs. given a joint model yn y xn x py y x we may subsequently use conditioning to form a predictor py much use of the i.i.d. assumption that each datapoint is independently sampled from the same generating distribution. in this context this might appear to suggest the assumption in previous chapters we ve made yn y xn x py x pynx x n however this is clearly of little use since the predictive conditional is simply py meaning the predictions make no use of the training outputs. for a non-trivial predictor we therefore need to specify a joint non-factorised distribution over outputs. x py x from parametric to non-parametric if we revisit our i.i.d. assumptions for parametric models we used a parameter to make a model of the input-output distribution pyx for a parametric model predictions are formed using x x py x x under the assumption that given the data is i.i.d. we obtain py py py p p n where py py pyn xn n py pyn xn yn xn y x y x yn xn non-parametric prediction figure a parametric model for predicb the form of the tion assuming i.i.d. data. model after integrating out the parameters our non-parametric model will have this structure. after integrating over the parameters the joint data distribution is given by py py pyn xn n which does not in general factorise into individual datapoint terms see the idea of a nonparametric approach is to specify the form of these dependencies without reference to an explicit parametric model. one route towards a non-parametric model is to start with a parametric model and integrate out the parameters. in order to make this tractable we use a simple linear parameter predictor with a gaussian parameter prior. for regression this leads to closed form expressions although the classification case will require numerical approximation. from bayesian linear models to gaussian processes to develop the gp we briefly revisit the bayesian linear parameter model of for parameters w and basis functions ix the output is given by zero output noise y wi ix i if we stack all the yn into a vector y then y w where is the design matrix. assuming a gaussian weight prior pw n w and since y is linear in w the joint output yn is gaussian distributed. a gaussian prior on w induces a gaussian on the joint y with mean and covariance pw w t w t w from this we see that the w can be absorbed into using its cholesky decomposition. in other words without loss of generality we may assume w i. after integrating out the weights the bayesian linear regression model induces a gaussian distribution on any set of outputs y as xn n k where the covariance matrix k depends on the training inputs alone via n n n since the matrix k is formed as the scalar product of vectors it is by construction positive semidefinite as we saw in after integrating out the weights the only thing the model directly depends draft march non-parametric prediction on is the covariance matrix k. in a gaussian process we directly specify the joint output covariance k as a function of two inputs. specifically we need to define the n element of the covariance matrix for any two inputs xn and kxn this is achieved using a covariance function kxn the required form of the function kxn is very special when applied to create the elements of the matrix k it must produce a positive definite matrix. we discuss how to create such covariance functions in one explicit straightforward construction is to form the covariance function from the scalar product of the basis vector and for finite-dimensional this is known as a finite dimensional gaussian process. given any covariance function we can always find a corresponding basis vector representation that is for any gp we can always relate this back to a parametric bayesian lpm. however for many commonly used covariance functions the basis functions corresponds to infinite dimensional vectors. it is in such cases that the advantages of using the gp framework are particularly evident since we would not be able to compute efficiently with the corresponding infinite dimensional parametric model. a prior on functions the nature of many machine learning applications is such that the knowledge about the true underlying mechanism behind the data generation process is limited. instead one relies on generic smoothness assumptions of the form that for two inputs x and that are close the corresponding outputs y and should be similar. many generic techniques in machine learning can be viewed as different characterisations of smoothness. an advantage of the gp framework in this respect is that the mathematical smoothness properties of the functions are well understood giving confidence in the procedure. for a given covariance matrix k equation specifies a distribution on in the following sense we specify a set of input points x xn and a n n covariance matrix k. then we draw a vector y from the gaussian defined by equation we can then plot the sampled function at the finite set of points yn n n. what kind of functions does a gp correspond to? consider two scalar inputs xi and xj separated by a distance xj. the corresponding sampled outputs yi and yj fluctuate as different functions are drawn. for a covariance function that has a high value for xj small we expect yi and yj to be very similar since they are highly correlated. conversely for a covariance function that has low value for a given small separation xj we expect yi and yj to be effectively in general we would expect the correlation between yi and yj to decrease the further apart xi and xj are. in we show three sample functions drawn from a squared exponential covariance function defined over points uniformly spaced from to each sampled function looks reasonably smooth. conversely for the ornstein uhlenbeck covariance function the sampled functions look locally rough. these smoothness properties are related to the form of the covariance function as discussed in the zero mean assumption implies that if we were to draw a large number of such functions the mean across these functions at a given point x tends to zero. similarly for any two points x and if we compute the sample covariance between the corresponding y and for all such sampled functions this will tend to the covariance function value kx the zero-mean assumption can be easily relaxed by defining a mean function mx to give pyx n m k. in many practical situations one typically deals with detrended data in which such mean trends have been already removed. for this reason much of the development of gps in the machine learning literature is for the zero mean case. term function is potentially confusing since we do not have an explicit functional form for the input output-mapping. for any finite set of inputs xn the values for the function are given by the outputs at those points yn periodic functions however we would expect high correlation at separating distances corresponding to the period of the function. draft march gaussian process prediction gaussian process prediction for a dataset d and novel input x a zero mean gp makes a gaussian model of the joint outputs yn y given the joint inputs xn x for convenience we write this as y py y x n where is a n dimensional zero-vector. the covariance matrix k is a block matrix with elements k kxx kxx kx where kxx is the covariance matrix of the training inputs x that is kx kxn n n n the n vector kxx has elements kxn x n n kx is the transpose of the above vector. the scalar covariance is given by kx kx x the predictive distribution py giving a gaussian distribution is obtained by gaussian conditioning using the results in xxkxx py n kx xxy kx kx for fixed hyperparameters gp regression is an exact method and there are no issues with local minima. furthermore gps are attractive since they automatically model uncertainty in the predictions. however the computational complexity for making a prediction is due to the requirement of performing the matrix inversion solving the corresponding linear system by gaussian elimination. this can be prohibitively expensive for large datasets and a large body of research on efficient approximations exists. a discussion of these techniques is beyond the scope of this book and the reader is referred to regression with noisy training outputs to prevent overfitting to noisy data it is useful to assume that a training output yn is the result of some clean process f n corrupted by additive gaussian noise yn f n where in this case our interest is to predict the clean signal f for a novel input x then the distribution py f kxx kxx x is a zero mean gaussian with block covariance matrix n kx kx so that kxx is replaced by kxx in forming the prediction equation draft march covariance functions example training data from a one-dimensional input x and one dimensional output y are plotted in along with the mean regression function fit based on two different covariance functions. note how the smoothness of the prior translates into smoothness of the prediction. the smoothness of the function space prior is a consequence of the choice of covariance function. naively we can partially understand this by the behaviour of the covariance function at the origin see demogpreg.m the marginal likelihood and hyperparameter learning for a set of n one-dimensional training inputs represented by the n dimensional vector y and a covariance matrix k defined on the inputs xn the log marginal likelihood is one can learn any free parameters of the covariance function by maximising the marginal likelihood. for example a squared exponential covariance function may have parameters log pyx log det k ytk x kx x exp the parameter in equation specifies the appropriate length-scale of the inputs and the variance of the function. the dependence of the marginal likelihood on the parameters is typically complex and no closed form expression for the maximum likelihood optimum exists in this case on resorts to numerical optimisation techniques such as conjugate gradients. vector inputs for regression with vector inputs and scalar outputs we need to define a covariance as a function of the two vectors kx using the multiplicative property of covariance functions a simple way to do this is to define i kxi x kx i for example for the squared exponential covariance function this gives kx e though correlated forms are possible as well see we can generalise the above using parameters kx exp x l l where xl is the lth component of x and d are parameters. the l in equation allow a different length scale on each input dimension and can be learned by numerically maximising the marginal likelihood. for irrelevant inputs the corresponding l will become small and the model will ignore the lth input dimension. this is closely related to automatic relevance determination covariance functions covariance functions kx are special in that they define elements of a positive definite matrix. these functions are also referred to as kernels particulary in the machine learning literature. draft march covariance functions figure the input space from to is split evenly into points three samples from a gp prior with squared exponential covariance function the covariance matrix k is defined using the se kernel from which the samples are drawn using prediction based on training points. plotted is the posterior predicted function based on the se covariance. the central line is the mean prediction with standard three samples from the ornsteinerrors bars on either side. the log marginal likelihood is posterior prediction for the ou covariance. the log marginal uhlenbeck gp prior with likelihood is meaning that the se covariance is much more heavily supported by the data than the rougher ou covariance. definition function. given any collection of points xm a covariance function kxi xj defines the elements of a m m matrix kxi xj such that c is positive semidefinite. making new covariance functions from old the following rules can all be proved directly generate new covariance functions from existing covariance functions definition kx draft march covariance functions definition kx x y definition spaces. for z kz and kz definition rescaling. kx for any function ax. definition and embedding. kx for any mapping x ux where the mapping ux has arbitrary dimension. a small collection of covariance functions commonly used in machine learning is given below. we refer the reader to and for further popular covariance functions. stationary covariance functions definition kernel. a kernel kx is stationary if the kernel depends only on the separation x that is kx kx following the notation in for a stationary covariance function we may write kd where d x this means that for functions drawn from the gp on average the functions depend only on the distance between inputs and not on the absolute position of an input. in other words the functions are on average translation invariant. for isotropic covariance functions the covariance is defined as a function of the distance kd. draft march definition exponential. kd e covariance functions the squared exponential is one of the most common covariance functions. there are many ways to show that this is a covariance function. an elementary technique is to consider e e xn xn e the first two factors form a kernel of the form is the linear kernel. taking the exponential and writing the power series expansion of the exponential we have in the final term i! ki this can be expressed as a series of integer powers of with positive coefficients. by the product itself and sum rules above this is therefore a kernel as well. we then use the fact that equation is the product of two kernels and hence also a kernel. definition kd e when we have the squared exponential covariance function. when this is the ornsteinuhlenbeck covariance function. definition ern. kd k where k is a modified bessel function definition quadratic. kd definition for x and a stationary isotropic covariance function can be obtained by first mapping x to the two dimensional vector ux sinx and then using the se covariance e e kx x see and draft march covariance functions figure plots of the gamma-exponential covariance e versus x. the case corresponds to the se covariance function. the drop in the covariance is much more rapid as a function of the separation x for small suggesting that the functions corresponding to smaller will be locally rough as for but zoomed in towards the possess relatively higher long range correlation. origin. for the se case the derivative of the covariance function is zero whereas the ou covariance has a first order contribution to the drop in the covariance suggesting that locally ou sampled functions will be much rougher than se functions. non-stationary covariance functions definition kx definition network. kx arcsin the functions defined by this covariance always go through the origin. to shift this one may use the embedding x x where the has the effect of a bias from the origin. to change the scale of the bias and and non-bias contributions one may use additional parameters x x. the nn covariance function can be derived as a limiting case of a neural network with infinite hidden and making use of exact integral results in definition kx i i i for functions rix draft march e i i analysis of covariance functions figure samples from a gp prior for x points uniformly placed from to from the periodic covariance function with bias b and neural network covariance function sampled analysis of covariance functions smoothness of the functions we examine local smoothness for a translation invariant kernel kx kx for two onedimensional points x and separated by a small amount x the covariance between the outputs y and is by taylor expansion kx x dk so that the change in the covariance at the local level is dominated by the first derivative of the covariance function. for the se covariance kx e dk dx is zero at x this means that for the se covariance function the first order change in the covariance is zero and only higher order terms contribute. for the ornstein-uhlenbeck covariance kx e the right derivative at the origin is lim k lim e where this result is obtained using l h opital s rule. hence for the ou covariance function there is a first order negative change in the covariance at the local level this decrease in the covariance is therefore much more rapid than for the se covariance see since low covariance implies low dependence gaussian distributions locally the functions generated from the ou process are rough whereas they are smooth in the se case. a more formal treatment for the stationary case can be obtained by examining the eigenvalue-frequency plot of the covariance function density for rough functions the density of eigenvalues for high frequency components is higher than for smooth functions. mercer kernels consider the function kx x sx sx where is a vector with component functions bx. then for a set of points xp we construct the matrix k with elements kxi xj sxi sxj draft march analysis of covariance functions we claim that the matrix k so constructed is positive semidefinite and hence a valid covariance matrix. recalling that a matrix is positive semidefinite if for any non zero vector z ztkz using the definition of k above we have ztkz zikijzj zi sxi s sxjzj s s hence any function of the form equation is a covariance function. we can generalise the mercer kernel to complex functions using kx x where represents the complex conjugate. then the matrix k formed from inputs xi i p is positive semidefinite since for any real vector z ztkz zi sxi s sxjzj s where we made use of the general result for a complex variable xx a further generalisation is to write kx x fs s sds for fs and scalar complex functions s. then replacing summations with integration assuming we can interchange the sum over the components of z with the integral over s we obtain ztkz fs zi s ds fs szj spectral decomposition is a generalisation of the spectral decomposition of a kernel kx since if we write fs as a sum of dirac delta functions fs k k and using k kx for an eigenfunction kx indexed by k with eigenvalue k we obtain the spectral decomposition kx x k kx kx if all the eigenvalues of a kernel are non-negative the kernel is a covariance function. consider for example the following function e kx x we claim that this is a covariance function. this is indeed a valid covariance function in the sense that for any set of points xd the d d matrix formed with elements kxd is positive definite as discussed after the solution given to shows that there do indeed exist real-valued vectors such that one can represent kx x where the vectors are infinite dimensional. this demonstrates the generalisation of the finite-dimensional weight space viewpoint of a gp to the potentially implicit infinite dimensional representation. draft march gaussian processes for classification cn yn xn c y x figure gp classification. the gp induces a gaussian distribution on the latent activations yn y given the observed values of cn the classification of the new input x is then given via the correlation induced by the training points on the latent activation y fourier analysis for stationary kernels gx for a function gx with fourier transform gs we may use the inverse fourier transform to write gse ixsds where i for a stationary kernel kx with fourier transform ks we can therefore write kx x kse ix kse which is of the same form as equation where the fourier transform ks is identified with fs and s e isx. hence provided the fourier transform ks is positive the translation invariant kernel kx is a covariance function. bochner s asserts the converse that any translation invariant covariance function must have such a fourier representation. application to the squared exponential kernel for the translation invariant squared exponential kernel kx e dx e e e ks e its fourier transform is hence the fourier transform of the se kernel is a gaussian. since this is positive the se kernel is a covariance function. gaussian processes for classification adapting the gp framework to classification requires replacing the gaussian regression term pyx with a corresponding classification term pcx for a discrete label c. to do so we will use the gp to define a latent continuous space which will then be mapped to a class probability using pcx given training data inputs x corresponding class labels c and a novel pcyxpyxdy pcypyxdy input x then pc where py pc py py pcypy x pcnyn x class mapping yn y xn x gaussian process dyn draft march gaussian processes for classification the graphical structure of this model is depicted in the posterior latent y is then formed from the standard regression term from the gaussian process multiplied by a set of non-gaussian maps from the latent activations to the class probabilities. we can reformulate the prediction problem more conveniently as follows py py py x where pycx xn pcnyn in equation the term py x does not contain any class label information and is simply a conditional gaussian. the advantage of the above description is that we can therefore form an approximation to pycx and then reuse this approximation in the prediction for many different x without needing to rerun the binary classification for the binary class case we will use the convention that c we therefore need to specify pc for a real valued activation y. a convenient choice is the logistic transfer e x then pcy y is a valid distribution since x ensuring that the sum over the class states is a difficulty is that the non-linear class mapping term makes the computation of the posterior distribution equation difficult since the integrals over yn cannot be carried out analytically. there are many approximate techniques one could apply in this case including variational methods analogous to that described in section??. below we describe the straightforward laplace method leaving the more sophisticated methods for further laplace s approximation in the laplace method we approximate the non-gaussian distribution by a qycx pycx qycx predictions can be formed from the joint gaussian for compactness we define the class label vector and outputs x y py py c and notationally drop the present conditioning on the inputs x. also for convenience we define will also refer to this as the sigmoid function more strictly a sigmoid function refers to any s-shaped function the greek for s authors use the term laplace approximation solely for approximating an integral. here we use the term to refer to a gaussian approximation of a non-gaussian distribution. draft march finding the mode the laplace approximation corresponds to a second order expansion around the mode of the distribution. our task is therefore to find the maximum of gaussian processes for classification pyc py c e where cty eyn ytk xxy log det n log the maximum needs to be found numerically and it is convenient to use the newton method ynew y differentiating equation with respect to y we obtain the gradient and hessian k xxy k xx d where the noise matrix is given by d diag n n using these expressions in the newton update gives to avoid unnecessary inversions one may rewrite this in the form ynew y xx k ynew kxx dkxx c xx and qyx x in equation predictions are then made y qyx x given a converged solution y we have found a gaussian approximation we now have gaussians for py using making predictions n py py yqyx x where by conditioning kx we can also write this as a linear system n x py y xxy kx kx kx kx where n aging over y and the noise we obtain xxkxx xxy kx kx xxkxx using equation and equation and aver similarly the variance of the latent prediction is xx y kx y xx k d kxx kx xx kx kx vary xxkxx kx kx xxkxx draft march gaussian processes for classification figure gaussian process classification. the x-axis are the inputs and the class is the y-axis. green points are training points from class and red from class the dots are the predictions pc for a rand of points x ou covariance see square exponential covariance where the last line is obtained using the matrix inversion lemma the class prediction for a new input x is then given by pc in order to calculate the gaussian integral over the logistic sigmoid function we use an approximation of the sigmoid function based on the error function erfx see and avsigmagauss.m. example an example of binary classification is given in in which one-dimensional input training data with binary class labels is plotted along with the class probability predictions on a range of input points. in both cases the covariance function is of the form xj ij. the square exponential covariance produces a smoother class prediction than the ornstein-uhlenbeck covariance function. see and demogpclass.m. marginal likelihood the marginal likelihood is given by under the laplace approximation the marginal likelihood is approximated by pcx pcypyx y pcx y e ye ytay y where a integrating over y gives log pcx log qcx where log det a log log qcx y y ct y e yn xx n log ytk xx y log det kxxd where y is the converged iterate of equation one can also simplify the above using that at convergence k xx y c draft march hyperparameter optimisation code the approximate marginal likelihood can be used to assess hyperparameters of the kernel. a little care is required in computing derivatives of the approximate marginal likelihood since the optimum y depends on we use the total derivative formula log qcx ytk i d d log qcx log qcx yi log qcx d d yi xxy log det kxxd which can be evaluated using the standard results for the derivative of a matrix determinant and inverse. since the derivative of is zero at y and noting that d depends explicitly on y yi log qcx yi log det kxxd the implicit derivative is obtained from using the fact that at convergence y kxx to give d d y kxxd kxx these results are substituted into equation to find an explicit expression for the derivative. multiple classes the extension of the preceding framework to multiple classes is essentially straightforward and may be achieved using the softmax function pc my eym which automatically enforces the constraint classes the cost of implementing the laplace approximation for the multiclass case scales as however one may show by careful implementation that the cost is only and we refer the reader m pc m naively it would appear that the for c to for details. further reading gaussian processes have been heavily developed within the machine learning community over recent years for which efficient approximations for both regression and classification remains an active research topic. we direct the interested reader to and for further discussion. code gpreg.m gaussian process regression demogpreg.m demo gp regression covfnge.m gamma-exponential covariance function gpclass.m gaussian process classification demogpclass.m demo gaussian process classification draft march exercises exercises exercise show that the sample covariance matrix with elements sij xi i is positive semidefinite. xn xn i xn j xi xj where exercise show that e sinx kx x is a covariance function. exercise consider the function fxi xj e xj for one dimensional inputs xi. show that fxi xj e i exixj e j xj is a kernel and find an explicit representation by taylor expanding the central term show that e for the kernel fxi xj as the scalar product of two infinite dimensional vectors. exercise show that for a covariance function then kx is also a covariance function for any polynomial fx with positive coefficients. show therefore that and tan are covariance functions. exercise for a covariance function show that is also a valid covariance function for a positive definite symmetric matrix a. exercise kernel. let x and be two strings of characters and sx be the number of times that substring s appears in string x. then ws sx sx kx x s is a kernel covariance function provided the weight of each substring ws is positive. given a collection of strings about politics and another collection about sport explain how to form a gp classifier using a string kernel. explain how the weights ws can be adjusted to improve the fit of the classifier to the data and give an explicit formula for the derivative with respect to ws of the log marginal likelihood under the laplace approximation. exercise regression. consider predicting a vector output y given training data x y yn n n. to make a gp predictor py we need a gaussian model yn y xn x a gp requires then a specification of the covariance cym two different input vectors. show that under the dimension independence assumption i yn j xm of the components of the outputs for cym j xm ciym i xm is a covariance function for the ith dimension that separate gp predictors can be where ciym constructed independently one for each output dimension i. i xm ij i yn i yn i yn draft march exercise consider the markov update of a linear dynamical system exercises xt axt t where a is a given matrix and t is zero mean gaussian noise with it ij also t show that xt is gaussian distributed. show that the covariance matrix of xt has elements n t t at t t and explain why a linear dynamical system is a gaussian process. consider yt bxt where is zero mean gaussian noise with covariance ij the vectors are uncorrelated with the vectors show that the sequence of vectors yt is a gaussian process with a suitably defined covariance function. exercise a form of independent components analysis of a one-dimensional signal yt is obtained from the joint model t t t w n with t t w n t t n where is a given noise variance. the signal can be viewed as the linear combination of two independent gaussian processes. the covariance matrices of the two processes have elements from a stationary kernel e write down an em algorithm for learning the mixing parameters given an observation se quence consider an extension of the above model to the case of two outputs pyi t t wi n t with t t t t t w n t w n t t t t t t n show that for t the likelihood is not invariant with respect to an orthogonal rotation wr with rrt i and explain the significance of this with respect to identifying independent components. draft march chapter mixture models density estimation using mixtures a mixture model is one in which a set of component models is combined to produce a richer model pv pvhph the variable v is visible or observable and h h indexes each component model pvh along with its weight ph. mixture models have natural application in clustering data where h indexes the cluster. this interpretation can be gained from considering how to generate a sample datapoint v from the model equation first we sample a cluster h from ph and then draw a visible state v from pvh. for a set of i.i.d. data vn a mixture model is of the form hn vn pvnhnphn clustering is achieved by inference of argmax vn which thanks to the factorised form of the distribution is equivalent to computing arg maxhn phnvn for each datapoint. in this way we can cluster many kinds of data for which a distance measure in the sense of the classical k-means algorithm is not directly apparent. explicitly writing the dependence on the parameters the model is pv h pvh vhph h the optimal parameters vh h of a mixture model are then most commonly set by maximum likelihood opt argmax vn numerically this can be achieved using an optimisation procedure such as gradient based approaches. alternatively by treating the component indices as latent variables one may also apply the em algorithm as described in the following section which in many classical models produces simple update formulae. expectation maximisation for mixture models h vh hn vn n figure a mixture model has a trivial graphical representation as a dag with a single hidden node which can be in and one of h states i h. the parameters are assumed common across all datapoints. example the data in naturally has two clusters and can be modelled with a mixture of two two-dimensional gaussians each gaussian describing one of the clusters. here there is a clear visual interpretation of the meaning of cluster with the mixture model placing two datapoints in the same cluster if they are both likely to be generated by the same model component. expectation maximisation for mixture models by treating the index h as a missing variable mixture models can be trained using the em algorithm there are two sets of parameters vh for each component model pvh vh and h for the mixture weights ph h. according to the general approach for i.i.d. data of we need to consider the energy term where e pvn h pvnh poldhvn ph poldhvn pvnh old vhph old h and maximise with respect to the parameters vh h h h. unconstrained discrete tables here we consider training a simple belief networkpvh vhph h v v h h in which the tables are unconstrained. this is a special case of the more general framework discussed in figure two dimensional data which displays clusters. in this case a gaussian mixture model would fit the data well for suitable means and covariances draft march expectation maximisation for mixture models h ph isolating the dependence of equation on ph we obtain m-step ph if no constraint is placed on ph h we may write the parameters as simply ph with the understanding that ph we now wish to maximise equation with respect to ph under the constraint j ph it is standard to treat this maximisation problem using lagrange multipliers see here we take an alternative approach based on recognising the similarity of the above to a form of kullback-leibler divergence. first we define the distribution poldhvn log ph h poldhvn poldhvn h ph then maximising equation is equivalent to maximising ph since the two expressions are related by the constant factor poldhvn. by subtracting the independent term ph from equation we obtain the negative kullback-leibler divergence kl pp. this means that the optimal ph is that distribution which minimises the kullback-leibler divergence. optimally therefore ph ph so that h pvh jpv ih j v l differentiating with respect to pv ih j and equating to zero i v poldhvn log p pv ih j i i poldh jv i i i poldh jv i i i poldh jv i i i poldh jv i l pv ih j hence pnewv ih j pnewv ih j draft march which using the normalisation requirement gives pnewh poldhvn poldhvn m-step pvh the dependence of equation on pvh is n h poldhvn vh if the distributions vh poldhvn are not constrained we can apply a similar kullback-leibler method as we did in for compatibility with other texts here we demonstrate the more standard lagrange procedure. we need to ensure that pvh is a distribution for each of the mixture states h h. this can be achieved using a set of lagrange multipliers giving the lagrangian expectation maximisation for mixture models h vih hn vn i i d n n figure mixture of a product of bernoulli distributions. in a bayesian treatment a parameter prior is used. in the text we simply set the parameters using maximum likelihood. e-step according to the general em procedure optimally we set pnewhvn phvn pnewhvn pvnhph h pvnhph equations are repeated until convergence. the initialisation of the tables and mixture probabilities can severely affect the quality of the solution found since the likelihood often has local optima. if random initialisations are used it is recommended to record the converged value of the likelihood itself to see which parameters have the higher likelihood. the solution with the highest likelihood is to be preferred. mixture of product of bernoulli distributions we describe a simple mixture model that can be used to clustering binary vectors v vdt vi the mixture of bernoulli model is given by pv ph pvih where each term pvih is a bernoulli distribution. the model is depicted in and has parameters ph and pvi h h. em training to train the model under maximum likelihood it is convenient to use the em algorithm which as usual may be derived by writing down the energy n pvn n i i n and then performing the maximisation over the table entries. from our general results we may immediately jump to the updates. the m-step is given by i poldh jvn i i poldh jvn i n i poldh jvn pvn n n i n poldh jvn n pvn i j pnewvi j pnewh j and the e-step by poldh jvn ph j is similar to the naive bayes classifier in which the class labels are always hidden. draft march expectation maximisation for mixture models figure data from questionnaire responses. people were each asked questions with yes and no answers. black denotes that the absence of a response data. this training data was generated by two component binomial mixture. missing data was simulated by randomly removing values from the dataset. equations are iterated until convergence. if an attribute i is missing for datapoint n one needs to sum over the states of the corresponding vn effect of performing the summation for this model is simply to remove the corresponding factor pvn from the algorithm see i the i initialisation the em algorithm can be very sensitive to initial conditions. consider the following initialisation pvi j with ph set arbitrarily. this means that at the first iteration poldh jvn ph j. the subsequent m-step updates are pnewh ph pnewvih j pnewvih j for any j this means that the parameters pvh immediately become independent of h and the model is numerically trapped in a symmetric solution. it makes sense therefore to initialise the parameters in a non-symmetric fashion. example a company sends out a questionnaire containing a set of d yesno questions to a set of customers. the binary responses of a customer are stored in a vector v vdt. in total n customers send back their questionnaires vn and the company wishes to perform an analysis to find what kinds of customers it has. the company assumes there are h essential types of customer for which the profile of responses is defined by only the customer type. data from a questionnaire containing questions with respondents is presented in the data has a large number of missing values. we assume there are h kinds of respondents and attempt to assign each respondent into one of the two clusters. running the em algorithm on this data with random initial values for the tables produces the results in based on assigning each datapoint vn to the cluster with maximal posterior probability hn arg maxh phvn given a trained model pvhph the model assigns of the data to the correct cluster is known in this simulated case. see and mixprodbern.m. draft march figure em learning of a mixture true ph of bernoulli products. and learned ph for h true pvh and learned pvh for v each column pair corresponds to pvih and pvih with i the learned probabilities are reasonably close to the true values. the gaussian mixture model figure top a selection of of the handwritten digits in the training set. bottom the trained cluster outputs pvi for h mixtures. see demomixbernoullidigits.m. example digits. we have a collection of handwritten digits which we wish to cluster into groups each digit is a dimensional binary vector. using a mixture of bernoulli products trained with iterations of em a random perturbation of the mean of the data used as initialisation the clusters are presented in as we see the method captures natural clusters in the data for example there are two kinds of one slightly more slanted than the other two kinds of etc. the gaussian mixture model gaussians are particularly convenient continuous mixture components since they constitute bumps of probability mass aiding an intuitive interpretation of the model. as a reminder a d dimensional gaussian distribution for a continuous variable x is s exp mt s m where m is the mean and s is the covariance matrix. a mixture of gaussians is then p s p where pi is the mixture weight for component i. for a set of data x and under the usual exp mit s i mi pxmi sipi i.i.d. assumption the log likelihood is log px si matrices in addition to pi pi log where the parameters are si pi i h. the optimal parameters can be set using maximum likelihood bearing in mind the constraint that the si must be symmetric positive definite i pi gradient based optimisation approaches are feasible under a parameterisation of the si cholesky decomposition and pi softmax that enforce the constraints. an alternative is the em approach which in this case is particularly convenient since it automatically provides parameter updates that ensure these constraints. em algorithm the energy term is pxn draft march the gaussian mixture model plugging in the definition of the gaussian components we have poldixn mit s i mi log det si log pi the m-step requires the maximisation of the above with respect to mi si pi. m-step optimal mi maximising equation with respect to mi is equivalent to minimising poldixn mit s i mi differentiating with respect to mi and equating to zero we have mi hence optimally mi i poldixns poldixnxn poldixn poldixn poldixn poldni mi poldnixn by defining the membership distribution which quantifies the membership of datapoints to cluster i we can write equation more compactly as m-step optimal si optimising equation with respect to si is equivalent to minimising i log i poldixn n i n i s where n i xn mi. to aid the matrix calculus we isolate the dependency on si to give log i poldixn i trace poldixn n differentiating with respect to s i i n i and equating to zero we obtain i n i si poldixn poldixn n using the membership poldni the optimal si is given by si poldni mi mit draft march the gaussian mixture model figure training a mixture of isotropic gaussians if we start with large variances for the gaussians even after one iteration the gaussians are centred close to the mean of the the gaussians begin to separate data. one by one the gaussians move towards appropriate parts of the data the final converged solution. the gaussians are constrained to have variances greater than a set amount. see demogmmem.m. iteration iterations iterations iterations this ensures that si is symmetric positive semi-definite. a special case is to constrain the covariances si to be diagonal for which the update is see si poldnidiag mi where above diag means forming a new matrix from the matrix m with zero entries except for the diagonal entries of m. a more extreme case is that of isotropic gaussians si i i. the reader may show that the optimal update for in this case is given by taking the average of the diagonal entries of the i diagonally constrained covariance update poldnixn if no constraint is placed on the weights the update follows the general formula given in equation i dim x m-step optimal mixture coefficients poldixn pi n e-step explicitly this is given by the responsibility poldixn poldxnipi mit s s pi exp exp poldixn i mi the above equations are iterated until convergence. the performance of em for gaussian mixtures can be strongly dependent on the initialisation which we discuss below. in addition constraints on the covariance matrix are required in order to find sensible solutions. draft march the gaussian mixture model practical issues infinite troubles a difficulty arises with using maximum likelihood to fit a gaussian mixture model. consider placing a component pxmi si with mean mi set to one of the datapoints mi xn. the contribution from that gaussian will be pxnmi si si xnts e i xn si in the limit that the width of the covariance goes to zero eigenvalues of si tend to zero this probability density becomes infinite. this means that one can obtain a maximum likelihood solution by placing zero-width gaussians on a selection of the datapoints resulting in an infinite likelihood. this is clearly undesirable and arises because in this case the maximum likelihood solution does not constrain the parameters in a sensible way. note that this is not related to the em algorithm but a property of the maximum likelihood method itself. all computational methods which aim to fit unconstrained mixtures of gaussians using maximum likelihood therefore succeed in finding reasonable solutions merely by getting trapped in favourable local maxima. a remedy is to include an additional constraint on the width of the gaussians ensuring that they cannot become too small. one approach is to monitor the eigenvalues of each covariance matrix and if an update would result in a new eigenvalue smaller than a desired threshold the update is rejected. in gmmem.m we use a similar approach in which we constrain the determinant product of the eigenvalues of the covariances to be greater than a desired specified minimum value. one can view the formal failure of maximum likelihood in the case of gaussian mixtures as a result of an inappropriate prior. maximum likelihood is equivalent to map in which a flat prior is placed on each matrix si. this is unreasonable since the matrices are required to be positive definite and of non-vanishing width. a bayesian solution to this problem is possible placing a prior on covariance matrices. the natural prior in this case is the wishart distribution or a gamma distribution in the case of a diagonal covariance. initialisation a useful intialisation strategy is to set the covariances to be diagonal with large variances. this gives the components a chance to sense where data lies. an illustration of the performance of the algorithm is given in symmetry breaking if the covariances are initialised to large values the em algorithm appears to make little progress in the first iterations as each component jostles with the others to try to explain the data. eventually one gaussian component breaks away and takes responsibility for explaining the data in its vicinity see the origin of this jostling is an inherent symmetry in the solution it makes no difference to the likelihood if we relabel what the components are called. this permutation symmetry causes initial confusion as to which model should explain which parts of the data. eventually this symmetry is broken and a local solution is found. the symmetries can severely handicap em in fitting a large number of models in the mixture since the number of permutations increases dramatically with the number of components. a heuristic is to begin with a small number of components say two for which symmetry breaking is less problematic. once a local broken solution has been found more models are included into the mixture initialised close to the currently found solutions. in this way a hierarchical breaking scheme is envisaged. another popular method for initialisation is to center the means to those found by the k-means algorithm however this itself requires a heuristic initialisation. classification using gaussian mixture models consider data drawn from two classes c we can fit a gmm pxc to the data from class and another gmm pxc to the data from class this gives rise to two class-conditional gmms pxcxc picn mc i sc i draft march the gaussian mixture model figure a gaussian mixture model with h components. there is a component with large variance and small weight that has little effect on the distribution close to where the other three components have appreciable mass. as we move further away this additional component gains in influence. the gmm probability density function from plotted on a log scale the influence of each gaussian far from the origin becomes clearer. for a novel point x the posterior class probability is pcx px where pc is the prior class probability. the maximum likelihood setting is that pc is proportional to the number of training points in class c. consider a testpoint x a long way from the training data for both classes. for such a point the probability that either of the two class models generated the data is very low. however one will be much lower than the other gaussians drop exponentially quickly meaning that the posterior probability will be confidently close to for that class which has a component closest to x this is an unfortunate property since we would end up confidently predicting the class of novel data that is not similar to anything we ve seen before. we would prefer the opposite effect that for novel data far from the training data the classification confidence drops and all classes become equally likely. a remedy for this situation is to include an additional component in the gaussian mixture for each class that is very broad. we first collect the input data from all classes into a dataset x and let m be the mean of all this data and s the covariance. then for the model of each class c data we include an additional gaussian the notational dependency on x pxc in mc pc i sc i pc m s pc i where pc i i h i h where is a small positive value and inflates the covariance take and in demogmmclass.m. the effect of the additional component on the training likelihood is negligible since it has small weight and large variance compared to the other components see however as we move away from the region where the first h components have appreciable mass the additional component gains in influence since it has a higher variance. if we include the same additional component in the gmm for each class c then the influence of this additional component will be the same for each class dominating as we move far from the influence of the other components. for a point far from the training data the likelihood will be roughly equal for each class since in this region the additional broad component dominates with equal measure. the posterior distribution will then tend to the prior class probability pc mitigating the deleterious effect of a single gmm dominating when a testpoint is far from the training data. draft march the gaussian mixture model figure class conditional gmm training and classification. data from two different classes. we fit a gmm with two components to the data from each class. the diamond is a test point far from the training data we will clasb upper subpanel are the class sify. probabilities pc for the training points and the point being the test point. the lower subpanel are the class probabilities but including the additional large variance gaussian term. see demogmmclass.m. example the data in has a cluster structure for each class. based on fitting a gmm to each of the two classes a test point far from the training data is confidently classified as belonging to class this is an undesired effect since we would prefer that points far from the training data are not classified with any certainty. by including an additional large variance gaussian component for each class this has little effect on the class probabilities of the training data yet has the desired effect of making the class probability for the test point maximally uncertain the parzen estimator the parzen density estimator is formed by placing a bump of mass on each datapoint a popular choice is a d dimensional x n px xn n px n giving the mixture of gaussians e there is no training required for a parzen estimator only the positions of the n datapoints need storing. whilst the parzen technique is a reasonable and cheap way to form a density estimator it does not enable us to form any simpler description of the data. in particular we cannot perform clustering since there is no lower number of clusters assumed to underly the data generating process. this is in contrast to gmms trained using maximum likelihood on a fixed number h n of components. k-means consider a mixture of k isotropic gaussians in which each covariance is constrained to be equal to px pin mi whilst the em algorithm breaks down if a gaussian component is allowed to set mi equal to a datapoint with by constraining all components to have the same variance the algorithm has a well draft march algorithm k-means the gaussian mixture model initialise the centres mi i k. while not converged do for each centre i find all the xn for which i is the nearest euclidean sense centre. call this set of points ni. let ni be the number of datapoints in set ni. update the means n ni xn mnew i ni end while figure datapoints clustered using k-means with components. the means are given by the evolution of the red crosses. mean square distance to nearest centre with iterations of the algorithm. the means were initialised to close to the overall mean of the data. see demokmeans.m. defined limit as the reader may show that in this case the membership distribution equation becomes deterministic poldni if mi is closest to xn otherwise in this limit the em update for the mean mi is given by taking the average of the points closest to mi. this limiting and constrained gmm then reduces to the so-called k-means algorithm despite its simplicity the k-means algorithm converges quickly and often gives a reasonable clustering provided the centres are initialised reasonably. see k-means is often used as a simple form of data compression. rather than sending the datapoint xn one sends instead the index of the centre to which it is associated. this is called vector quantisation and is a form of lossy compression. to improve the quality more information can be transmitted such as an approximation of the difference between x and the corresponding mean m which can be used to improve the reconstruction of the compressed datapoint. bayesian mixture models bayesian extensions include placing priors on the parameters of each model in the mixture and also on the component distribution. in most cases this will give rise to the marginal likelihood being an intractable integral. methods that approximate the integral include sampling techniques see also for an approximate variational treatment focussed on bayesian gaussian mixture models. semi-supervised learning in some cases we may know to which mixture component certain datapoints belong. given this information we want to fit a mixture model with a specified number of components h and parameters we write hm m m for the m known datapoints and corresponding components and hn n n for the remaining datapoints whose components hn are unknown. we aim then to maximise the draft march mixture of experts n n xn yn hn w u figure mixture of experts model. the prediction of the output yn or continuous given the input xn averages over individual experts pynxn whn. the expert hn is selected by the gating mechanism with probability phnxn u so that some experts will be more able to predict the output for xn in their part of the space. the parameters w u can be learned by maximum likelihood after marginalising over the hidden expert variables. likelihood m pvm n hn pvnhn if we were to lump all the datapoints together this is essentially equivalent to the standard unsupervised case expect that some of the h are fixed into known states. the only effect on the em algorithm is therefore in the terms poldhv which are delta functions in the known state resulting in a minor modification of the standard algorithm mixture of experts the mixture of experts is related to discriminative training of an output y distribution conditioned on an input x. this can be used in either the regression of classification contexts and has the general form see pyx w u pyx whphx u here h indexes the mixture component. each expert has parameters w wh and corresponding gating parameters u uh. unlike a standard mixture model the component distribution phx u is dependent on the input x. this so-called gating distribution is conventionally taken to be of the softmax form phx u eut h eut hx the idea is that we have a set of h predictive models pyx wh each with a different parameter wh h h. how suitable model h is for predicting with the current input x is determined by the alignment of input x with the weight vector uh. in this way the input x is softly assigned to the appropriate experts. maximum likelihood training can be achieved using a form of em. we will not derive the em algorithm for the mixture of experts model in full merely pointing the direction along which the derivation would continue. for a single datapoint x the em energy term is pyx whphx for regression a simple choice is y xtwh pyx wh n and for classification py wh draft march in both cases computing the derivatives of the energy with respect to the parameters w is straightforward so that an em algorithm is readily available. an alternative to em is to compute the gradient of the likelihood directly using the standard approach discussed in indicator models a bayesian treatment is to consider pyx where it is conventional to assume pw pyx whphx upwpu h pwh pu wu h h puh. the integrals are generally intractable and approximations are required. see for a variational treatment for regression and for a variational treatment of classification. an extension to bayesian model selection in which the number of experts is estimated is considered in indicator models in the indicator approach we specify a distribution over the cluster assignments. for consistency with the literature we use an indicator z as opposed to a hidden variable h although they play the same role. a clustering model with parameters on the component models and joint indicator prior takes the form pvnzn since the zn indicate cluster membership below we discuss the role of different indicator priors in clustering. joint indicator approach factorised prior assuming prior independence of indicators pzn we obtain from equation zn k pvnzn pvnzn zn which recovers the standard mixture model equation as we discuss below more sophisticated joint indicator priors can be used to explicitly control the complexity of the indicator assignments and open the path to essentially infinite dimensional models. joint indicator approach polya prior for a large number of available clusters components k using a factorised joint indicator distribution could potentially lead to overfitting resulting in little or no meaningful clustering. one way to control the effective number of components that are used is via a parameter that regulates the complexity n pzn p draft march indicator models zn vn zn vn zn vn n figure a generic mixture model for data each zn indicates the cluster of each datapoint. is a set of parameters and zn k selects parameter k for datapoint vn. for a potentially large number of clusters one way to control complexity is to constrain the joint plate notation of indicator distribution. figure the number of unique clusters u when indicators are sampled from a polya distribution k k k equation with and n datapoints. even though the number of available clusters k is larger than the number of datapoints the effective number of used clusters remains constrained. see demopolya.m. where pz is a categorical distribution pzn k k a convenient choice for p is the dirichlet distribution this is conjugate to the categorical distribution k p dirichlet the number of unique clusters used is then given by u nk n the integral over in equation can be performed analytically to give a polya distribution i k i the distribution over likely cluster numbers is controlled by the parameter the scaling in equation ensures a sensible limit as k see in which limit the models are known as dirichlet process mixture models. this approach means that we do not need to explicitly constrain the number of possible components k since the number of active components u remains limited even for very large k. k clustering is achieved by considering argmax in practice it is common to consider argmax zn unfortunately posterior inference of for this class of models is formally computationally intractable and approximate inference techniques are required. a detailed discussion of these techniques is beyond the scope of this book and we refer the reader to for a deterministic approach and for a discussion of sampling approaches. draft march mixed membership models figure latent dirichlet allocation. for document n we first sample a distribution of topics n. then for each word position w in the document we sample a topic zn w from the topic distribution. given the topic we then sample a word from the word distribution of that topic. the parameters of the model are the word distributions for each topic and the parameters of the topic distribution n zn w vn w wn n mixed membership models unlike standard mixture models in which each object is assumed to have been generated from a single cluster in mixed membership models an object may be a member of more than one group. latent dirichlet allocation discussed below is an example of such a mixed membership model and is one of a number of models developed in recent years latent dirichlet allocation so far we ve considered clustering in the sense that each observation is assumed to have been generated from a single cluster. in contrast latent dirichlet and related methods are generative mixed membership models in which each datapoint may belong to more than a single cluster. a typical application is to identify topic clusters in a collection of documents. a single document contains a sequence of words for example v cat sat on the cat mat vn if each word in the available dictionary is assigned to a unique state dog tree cat we can represent then the nth document as a vector vn wn vn i d where wn is the number of words in the nth document. the number of words wn in each document can vary although the overall dictionary from which they came is fixed. the aim is to find common topics in documents assuming that any document could potentially contain more than one topic. it is useful to think first of an underlying generative model of words including latent topics we will later integrate out. we first sample a probability distribution that represents the topics likely to occur for this document. then for each word-position in the document sample a topic and subsequently a word from the distribution of words for that topic. mathematically for document n and the wth word-position in the document vn w k to indicate which of the k possible topics that word belongs. for each topic k one then has a categorical distribution over all the words i d in the dictionary w we use zn pvn w izn w k ik distribution of topics n the animal topic has high probability to emit animal-like words etc. for each document n we have a k which gives a latent description of the document in terms of its topic membership. for example document n discusses issues related to wildlife conservation might have a topic distribution with high mass on the latent animals and environment topics. note that the topics are indeed latent the name animal would be given post-hoc based on the kinds of words n draft march mixed membership models that the latent topic would generate ik. as in to control complexity one may use a dirichlet prior to limit the number of topics active in any particular document p n dirichlet n where is a vector of length the number of topics. a generative model for sampling a document vn with wn word positions is choose n dirichlet n for each of word position vn w w wn choose a topic zn choose a word vn w p w w n w w training the lda model corresponds to learning the parameters which relates to the number of topics and which describes the distribution of words within each topic. unfortunately finding the requisite marginals for learning from the posterior is formally computationally intractable. efficient approximate inference for this class of models is a topic of research interest and both variational and sampling approaches have recently been there are close similarities between lda and both of which describe a document in terms of a distribution over latent topics. lda is a probabilistic model for which issues such as setting hyperparameters can be addressed using maximum likelihood. plsa on the other hand is essentially a matrix decomposition technique as pca. issues such as hyperparameters setting for plsa are therefore addressed using validation data. whilst plsa is a description only of the training data lda is a generative data model and can in principle be used to synthesise new documents. example an illustration of the use of lda is given in the documents are taken from the trec associated press corpus containing newswire articles with unique terms. after removing a standard list of stop words words such as the a etc. that would otherwise dominate the statistics the em algorithm variational approximate inference was used to find the dirichlet and conditional categorical parameters for a lda model. the top words from four resulting categorical distributions ik are illustrated these distributions capture some of the underlying topics in the corpus. an example document from the corpus is presented along with the words coloured by the most probable latent topic they correspond to. graph based representations of data mixed membership models are used in a variety of contexts and are distinguished also by the form of data available. here we focus on analysing a representation of the interactions amongst a collection of objects in particular the data has been processed such that all the information of interest is characterised by an interaction matrix. for graph based representations of data two objects are similar if they are neighbours on a graph representing the data objects. in the field of social-networks for example each individual is represented as a node in a graph with a link between two nodes if the individuals are friends. given a graph one might wish to identify communities of closely linked friends. interpreted as a social network in individual is a member of his work group and also the poker group these two groups of individuals are otherwise disjoint. discovering such groupings contrasts with graph partitioning in which each node is assigned to only one of a set of subgraphs for which a typical criterion is that each subgraph should be roughly of the same size and that there are few connections between the draft march mixed membership models arts new film show music movie play musical best actor first york opera theater actress love budgets million tax program budget billion federal year spending new state plan money programs government congress children education children women people child years families work parents says family welfare men percent care life school students schools education teachers high public teacher bennett manigat namphy state president elementary haiti the william randolph hearst foundation will give million to lincoln center metropolitan opera co. new york philharmonic and juilliard school. our board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health medical research education and the social services hearst foundation president randolph a. hearst said monday in announcing the grants. lincoln centers share will be for its new building which will house young artists and provide new public facilities. the metropolitan opera co. and new york philharmonic will receive each. the juilliard school where music and the performing arts are taught will get the hearst foundation a leading supporter of the lincoln center consolidated corporate fund will make its usual annual donation too. figure a subset of the latent topics discovered by lda and the high probability words associated with each topic. each column represents a topic with the topic name such as art assigned by hand after viewing the most likely words corresponding to the topic. a document from the training data in which the words are coloured according to the most likely latent topic. this demonstrates the mixed-membership nature of the model assigning the datapoint in this case to several clusters reproduced from figure the social network of a set of individuals represented as an undirected graph. here individual belongs to the group and also by contrast in graph partitioning one breaks the graph into roughly equally sized disjoint partitions such that each node is a member of only a single partition with a minimal number of edges between partitions. another example is that nodes in the graph represent products and a link between nodes i and j indicates that customers who by product i frequently also buy product j. the aim is to decompose the graph into groups each corresponding to products that are commonly co-bought by a growing area of application of graph based representations is in bioinformatics in which nodes represent genes and a link between them representing that the two genes have similar activity profiles. the task is then to identify groups of similarly behaving dyadic data consider two kinds of objects for example films and customers. each film is indexed by f f and each user by u u. the interaction of user u with film f can be described by the element of a matrix muf representing the rating a user gives to a film. a dyadic dataset consists of such a matrix and the aim is to decompose this matrix to explain the ratings by finding types of films and types of user. another example is to consider a collection of documents summarised by an interaction matrix in which mwd is if word w appears in document d and zero otherwise. this matrix can be represented as a bipartite graph as in the upper nodes represent documents and the lower nodes words with a link between them if that word occurs in that document. one the seeks assignments of documents to groups or latent topics to succinctly explain the link structure of the bipartite graph via a small number of latent nodes as schematically depicted in one may view this as a form of matrix t mwd uwtv t td where t indexes the topics and the feature matrices u and v control the word-to-topic mapping and the topic-to-document mapping. this differs from latent dirichlet allocation which has a probabilistic interpretation of first generating a topic and then a word conditional on the chosen topic. here the interaction between document-topic matrix v and word-topic matrix u is non-probabilistic. more generally we can draft march there are figure graphical representation of dyadic data. documents and words. a link represents that a particular worddocument pair occurs in the dataset. a latent decomposition of using topics a topic corresponds to a collection of words and each document a collection of topics. the open nodes indicate latent variables. where u and v are feature matrices. in real-valued data is modelled using pmu w v n m uwvt where u and v are assumed binary and the real-valued w is a topic-interaction matrix. in this viewpoint learning then consists of inferring uwv given the dyadic observation matrix m. assuming factorised priors the posterior over the matrices is pu w vm pmu w vpupwpv a convenient choice is a gaussian prior distribution for w with the feature matrices u and v sampled from beta-bernoulli priors. the resulting posterior distribution is formally computationally intractable and in this is addressed using a sampling approximation. monadic data in monadic data there is only one type of object and the interaction between the objects is represented by a square interaction matrix. for example one might have a matrix with elements aij if proteins i and j can bind to each other and otherwise. a depiction of the interaction matrix is given by a graph in which an edge represents an interaction for example in the following section we discuss a particular mixed membership model and highlight potential applications. the method is based on clique decompositions of graphs and as such we require a short digression into clique-based graph representations. cliques and adjacency matrices for monadic binary data a symmetric adjacency matrix has elements aij with a indicating a link between nodes i and j. for the graph in the adjacency matrix is mixed membership models write a distribution pmu v a where we include self connections on the diagonal. given a our aim is to find a simpler description that reveals the underlying cluster structure such as and in given the undirected graph in the incidence matrix finc is an alternative description of the adjacency draft march figure the minimal clique cover is mixed membership models figure bipartite representations of the decompositions of shaded nodes represent observed variables and open nodes latent variables. incidence matrix representation. minimal clique decomposition. given the v nodes in the graph we construct finc as follows for each link i j in the graph form a column of the matrix finc with zero entries except for a in the ith and jth row. the column ordering is arbitrary. for example for the graph in an incidence matrix is the incidence matrix has the property that the adjacency structure of the original graph is given by the outer product of the incidence matrix with itself. the diagonal entries contain the degree of links of each node. for our example this gives finc fincft inc fincft inc so that a h here h is the element-wise heaviside step function if mij and is otherwise. a useful viewpoint of the incidence matrix is that it identifies two-cliques in the graph we are using the term clique in the non-maximal sense. there are five in and each column of finc specifies which elements are in each graphically we can depict this incidence decomposition as a bipartite graph as in where the open nodes represent the five the incidence matrix can be generalised to describe larger cliques. consider the following matrix as a decomposition for and its outer-product f fft the interpretation is that f represents a decomposition into two as in the incidence matrix each column represents a clique and the rows containing a express which elements are in the clique defined by that column. this decomposition can be represented as the bipartite graph of for the graph of both finc and f satisfy a h h fincft inc one can view equation as a form of binary matrix factoristation of the binary square matrix a into non-square binary matrices. for our clustering purposes the decomposition using f is to be preferred to the incidence decomposition since f decomposes the graph into a smaller number of larger cliques. a formal specification of the problem of finding a minimum number of maximal fully-connected subsets is the computational problem min clique indeed f solves min clique cover for draft march mixed membership models figure the function increases this sigmoid function tends to a step function. e for as ii express the number of cliquescolumns that node i occurs in. off-diagonal elements definition matrix. given an adjacency matrix i j v a clique matrix f has elements fic i v c c such that a hfft. diagonal elements contain the number of cliquescolumns that nodes i and j jointly inhabit ij whilst finding a clique decomposition f is easy the incidence matrix for example finding a clique with the minimal number of columns i.e. solving min clique cover is a generative model of adjacency matrices solving min clique cover is a computationally hard problem and approximations are in general unavoidable. below we relax the strict clique requirement and assume that provided only a small number of links in an almost clique are missing this may be considered a sufficiently well-connected group of nodes to form a cluster. given an adjacency matrix a and a prior on clique matrices f our interest is the posterior pfa pafpf we first concentrate on the generative term paf. to find well-connected clusters we relax the constraint that the decomposition is in the form of cliques in the original graph and view the absence of links as statistical fluctuations away from a perfect clique. given a v c matrix f we desire that the higher the overlap between fi and fj is the greater the probability of a link between i and j. this may be achieved using for example paij e fif t j with where controls the steepness of the function see the shift in equation ensures that approximates the step-function since the argument of is an integer. under equation if j and paij is high. absent links fi and fj have at least one in the same position fif t contribute paij paij the parameter controls how strictly matches a for large very little flexibility is allowed and only cliques will be identified. for small subsets that would be cliques if it were not for a small number of missing links are clustered together. the setting of is user and problem dependent. assuming each element of the adjacency matrix is sampled independently from the generating process the joint probability of observing a is its diagonal elements paf i j j fif t j fif t j the ultimate quantity of interest is the posterior distribution of clique structure equation for which we now specify a prior pf over clique matrices. use lower indices fi to denote the ith row of f. draft march mixed membership models figure adjacency matrix of political books clique matrix non-zero entries. adjacency reconstruction using an approximate clique matrix with cliques see also and democliquedecomp.m. figure political books. dimensional clique matrix broken into groups by a politically astute reader. a black square indicates qfic liberal books conservative books neutral booksyellow. by inspection cliques largely correspond to conservative books. clique matrix prior pf since we are interested in clustering ideally we want to place as many nodes in the graph as possible in a cluster. this means that we wish to bias the contributions to the adjacency matrix a to occur from a small number of columns of f. to achieve this we first reparameterise f as f cmaxf where c play the role of indicators and f c is column c of f. cmax is an assumed maximal number of clusters. ideally we would like to find an f with a low number of indicators cmax in state to achieve this we define a prior distribution on the binary hypercube cmax c c to encourage a small number of the ensure that is less than this gives rise to a beta-bernoulli distribution cs to be we use a beta prior p with suitable parameters to p c p p ba n b cmax n where ba b is the beta function and n ba b c is the number of indicators in state to encourage that only a small number of components should be active we set a b corresponds to a mean of and variance the distribution is on the vertices of the binary hypercube with a bias towards vertices close to the origin through equation the prior on induces a prior on f. the resulting distribution pf pf is formally intractable and in this is addressed using a variational technique. see cliquedecomp.c and cliquedecomp.m. clique matrices also play a natural role in the parameterisation of positive definite matrices see draft march years for revenge bush vs. the beltway charlie wilson s war losing bin laden sleeping with the devil the man who warned america why america slept ghost wars a national party no more bush country dereliction of duty legacy off with their heads persecution rumsfeld s war breakdown betrayal shut up and sing meant to be the right man ten minutes from normal hillary s scheme the french betrayal of america tales from the left coast hating america the third terrorist endgame spin sisters all the shah s men dangerous dimplomacy the price of loyalty house of bush house of saud the death of right and wrong useful idiots the o reilly factor let freedom ring those who trespass bias slander the savage nation deliver us from evil give me a break the enemy within the real america who s looking out for you? the official handbook vast right wing conspiracy power plays arrogance the perfect wife the bushes things worth fighting for surprise security the american experience allies why courage matters hollywood interrupted fighting back we will prevail the faith of george w bush rise of the vulcans downsize this! stupid white men rush limbaugh is a big fat idiot the best democracy money can buy the culture of fear america unbound the choice the great unraveling rogue nation soft power colossus the sorrows of empire against all enemies american dynasty big lies the lies of george w. bush worse than watergate plan of attack bush at war the new pearl harbor bushwomen the bubble of american supremacy living history the politics of truth fanatics and fools bushwhacked disarming iraq lies and the lying liars who tell them moveon s ways to love your country the buying of the president perfectly legal hegemony or survival the exception to the rulers freethinkers had enough? it s still the economy stupid! we re right they re wrong what liberal media? the clinton wars weapons of mass deception dude where s my country? thieves in high places shrub buck up suck up the future of freedom empire exercises example books clustering. the data consists of books on us politics sold by the online bookseller amazon. the adjacency matrix with element aij represents frequent co-purchasing of books i and j krebs www.orgnet.com. additionally books are labelled liberal neutral or conservative according to the judgement of a politically astute reader mejnnetdata. the interest is to assign books to clusters using a alone and then see if these clusters correspond in some way to the ascribed political leanings of each book. note that the information here is minimal all that is known to the clustering algorithm is which books were co-bought a no other information on the content or title of the books are exploited by the algorithm. with an initial cmax cliques beta parameters a b and steepness the most probably posterior marginal solution contains cliques giving a perfect reconstruction of the adjacency a. for comparison the incidence matrix has however this clique matrix is too large to provide a compact interpretation of the data indeed there are more clusters than books. to cluster the data more aggressively we fix cmax and re-run the algorithm. this results only in an approximate clique decomposition a hfft as plotted in the resulting approximate clique matrix is plotted in and demonstrates how individual books are present in more than one cluster. interestingly the clusters found only on the basis of the adjacency matrix have some correspondence with the ascribed political leanings of each book cliques correspond to largely conservative books. most books belong to more than a single cliquecluster suggesting that they are not single topic books consistent with the assumption of a mixed membership model. further reading the literature on mixture modelling is extensive and a good overview and entrance to the literature is contained in code mixprodbern.m em training of a mixture of product bernoulli distributions demomixbernoulli.m demo of a mixture of product bernoulli distributions gmmem.m em training of a mixture of gaussians gmmloglik.m gmm log likelihood demogmmem.m demo of a em for mixture of gaussians demogmmclass.m demo gmm for classification kmeans.m k-means demokmeans.m demo of k-means demopolya.m demo of the number of active clusters from a polya distribution dirrnd.m dirichlet random distribution generator cliquedecomp.m clique matrix decomposition cliquedecomp.c clique matrix decomposition democliquedecomp.m demo clique matrix decomposition exercises pv exercise consider a mixture of factorised models h i draft march pvih show that optimally ph n for assumed i.i.d. data vn n n some observation components may be missing so that for example the third component of the fifth datapoint is unknown. show that maximum likelihood training on the observed data corresponds to ignoring components vn i that are missing. exercise derive the optimal em update for fitting a mixture of gaussians under the constraint that the covariances are diagonal. exercises exercise consider a mixture of k isotropic gaussians each with the same covariance si in the limit show that the em algorithm tends to the k-means clustering algorithm. exercise consider the term we wish to optimise the above with respect to the distribution ph. this can be achieved by defining the lagrangian l by differentiating the lagrangian with respect to ph and using the normalisation ph h h ph poldhvn exercise we showed that fitting an unconstrained mixture of gaussians using maximum likelihood is problematic since by placing one of the gaussians over a datapoints and letting the covariance determinant go to zero we obtain an infinite likelihood. in contrast when fitting a single gaussian n to i.i.d. data xn show that the maximum likelihood optimum for has non-zero determinant and that the optimal likelihood remains finite. exercise modify gmmem.m suitably so that it can deal with the semi-supervised scenario in which the mixture component h of some of the observations v is known. exercise you wish to parameterise covariance matrices s under the constraint that specified elements are zero. the constraints are specified using a matrix a with elements aij if sij and aij otherwise. consider a clique matrix z for which a hzzt and matrix s z zt with ij if zij if zij for parameters show that for any s is positive semidefinite and parameterises covariance matrices under the zero constraints specified by a. draft march chapter latent linear models factor analysis in we discussed principal components analysis which forms lower dimensional representations of data based on assuming that the data lies close to a hyperplane. here we describe a related probabilistic model for which extensions to bayesian methods can be envisaged. any probabilistic model may also be used as a component of a larger more complex model such as a mixture model enabling natural generalisations. we use v to describe a real data vector to emphasise that this is a visible quantity. the dataset is then given by a set of vectors v where dim d. our interest is to find a lower dimensional probabilistic description of this data. if data lies close to a h-dimensional hyperplane we may accurately approximate each datapoint by a low h-dimensional coordinate system. in general datapoints will not lie exactly on the hyperplane and we model this discrepancy with gaussian noise. mathematically the fa model generates an observation v according to v fh c where the noise is gaussian distributed n the constant bias c sets the origin of the coordinate system. the factor loading matrix f plays a similar role as the basis matrix in pca see similarly the hidden coordinates h plays the role of the components we used in the difference between pca and factor analysis is in the choice of probabilistic pca factor analysis diag d factor analysis figure factor analysis. the visible vector variable v is related to the vector hidden variable h by a linear mapping with independent additive gaussian noise on each visible variable. the prior on the hidden variable may be taken to be an isotropic gaussian thus being independent across its components. a probabilistic description from equation and equation given h the data is gaussian distributed with mean fh c and covariance p n fh c e fh ct fh c to complete the model we need to specify the hidden distribution ph. a convenient choice is a gaussian p n i e under this prior the coordinates h will be preferentially concentrated around values close to if we sample a h from ph and then draw a value for v using pvh the sampled v vectors would produce a saucer or pancake of points in the v space. using a correlated gaussian prior ph n h has no effect on the complexity of the model since h can be absorbed into f since v is linearly related to h through equation and both and h are gaussian then v is gaussian distributed. the mean and covariance can be computed using the propagation results in p p p dh n v c fft invariance of the likelihood under factor rotation since the matrix f only appears in the final model pv through fft the likelihood is unchanged if we rotate f using fr with rrt i frfrt frrtft fft the solution space for f is therefore not unique we can arbitrarily rotate the matrix f and produce an equally likely model of the data. some care is therefore required when interpreting the entries of f. varimax provides a more interpretable f by using a suitable rotation matrix r. the aim is to produce a rotated f for which each column has only a small number of large values. finding a suitable rotation results in a non-linear optimisation problem and needs to be solved numerically. see for details. finding the optimal bias for a set of data v and using the usual i.i.d. assumption the log likelihood is log pvn ct d c n log det d log pvf where d fft c n differentiating equation with respect to c and equating to zero we arrive at the maximum likelihood optimal setting that the bias c is the mean of the data vn v draft march factor analysis maximum likelihood latent two-dimensional figure factor analysis points generated from the model. points hn sampled from n i. these are transformed to a point on the three-dimensional plane by xn c fhn. the covariance of is degenerate with covariance matrix fft. for each point xn on the plane a random noise vector is drawn from n and added to the in-plane vector to form a sample xn plotted in red. the distribution of points forms a pancake in space. points underneath the plane are not shown. we will use this setting throughout. with this setting the log likelihood equation can be written log pvf n d log det where s is the sample covariance matrix s n v vt factor analysis maximum likelihood we now specialise to the assumption that diag d. we consider two methods for learning the factor loadings f a direct and an em approach direct likelihood optimisation optimal f for fixed to find the maximum likelihood setting of f we differentiate the log likelihood equation with respect to f and equate to zero. this gives d f d d d f using ffft f fft ffft a stationary point is given when d f d s d f since d is invertible the optimal f satisfies f s d f using the definition of d equation one can rewrite d f as d f i ft presentation here follows closely that of draft march factor analysis maximum likelihood plugging this into the zero derivative condition equation becomes f i ft s using the reparameterisations equation can be written in the isotropic form s s f f i ft f f s f we assume that the transformed factor matrix f has a thin svd decomposition f uhlvt where dim uh d h dim l h h dim v h h and ut huh ih and l diag lh are the singular values of f. plugging this assumption into equation we obtain vtv ih ih suh uh which gives suhlvt h is then an eigen-equation for uh. intuitively it s clear that we need to find then the eigendecomposition of s and then set the columns of uh to those eigenvectors corresponding to the largest eigenvalues. this is derived more formally below. determining the appropriate eigenvalues we can relate the form of the solution to the eigen-decomposition of s s u ut u where ur are arbitrary additional columns chosen to complete uh to form an orthogonal u utu i i or li i given uut i. using diag d equation stipulates the solution for f the solution for f is found from equation to determine the optimal i we write the log likelihood in terms of the i as follows. using the new parameterisation log det draft march d f ft i d trace we have and s s n log pvf trace the log likelihood equation in this new parameterisation is f ft id s id f s id f log det using i i and equation we can write id f ft id h udiag h ut i i i h i h i i i log i using this we can write the log likelihood as a function of the eigenvalues fixed as to maximise the likelihood we need to minimise the right hand side of the above. since log we i log i term. a solution for fixed is therefore i log det factor analysis maximum likelihood so that trace similarly log det id f s id f should place the largest h eigenvalues in log pvf log i h n f uh h ih r where h diag h are the h largest eigenvalues of r is an arbitrary orthogonal matrix. s with uh being the matrix of the corresponding eigenvectors. svd based approach rather than finding the eigen-decomposition of considering the thin svd decomposition of s we can avoid forming the covariance matrix by where the data matrix is x n x x given a thin decomposition x uh vt we obtain the eigenvalues i using this svd method is o svd methods are finding the optimal s s diag new diag min ii. for d n this is convenient since the computational complexity when the matrix x is too large to store in memory online the zero derivative of the log likelihood occurs when where f is given by equation there is no closed form solution to a simple iterative scheme is to first guess values for the diagonal entries of and then find the optimal f using equation subsequently is updated using we update f using equation and using equation until convergence. alternative schemes for updating the noise matrix can improve convergence considerably. for example updating only a single component of with the rest fixed can be achieved using a closed form draft march expectation maximisation a popular way to train factor analysis in machine learning is to use em. we assume that the bias c has been optimally set to the data mean v. factor analysis maximum likelihood m-step as usual we need to consider the energy which neglecting constants is e fht fh qhvn n log det where dn vn v. the optimal variational distribution qhvn is determined by the e-step below. maximising e with respect to f gives fnew ah where a n n finally new n n h fh n qhvn qhvn dn diag n qhvn diag n n dndnt fhft e-step the above recursions depend on the statistics for the e-step we have qhvn. using the em optimal choice with qhvn pvnhph n mn i ft ft i ft using these results we can express the statistics in equation as mn h n n mnmnt equations are iterated till convergence. as for any em algorithm the likelihood equation the diagonal constraint on increases at each iteration. convergence using this em technique can be slower than that of the direct eigen-approach of and commercial implementations usually avoid em for this reason. provided however that a reasonable initialisation is used the performance of the two training algorithms can be similar. a useful initialisation is to use pca and then set f to the principal directions. mixtures of fa an advantage of probabilistic models is that they may be used as components in more complex models such as mixtures of training can then be achieved using em or gradient based approaches. bayesian extensions are clearly of interest whilst formally intractable they can be addressed using approximate methods for example draft march interlude modelling faces g g origin f figure latent identity model. the mean represents the mean of the faces. the subspace f represents the directions of variation of different faces so that is a mean face for individual and similarly for the subspace g denotes the directions of variability for any individual face caused by pose lighting etc. this variability is assumed the same for each person. a particular mean face is then given by the mean face of the person plus poseillumination variation for example a sample face is then given by a mean face xij plus gaussian noise from n wij hi xij j i figure the jth image of the ith person xij is modelled using a linear latent model with parameters interlude modelling faces factor analysis has widespread application in statistics and machine learning. as an inventive application of fa highlighting the probabilistic nature of the model we describe a face modelling technique that has as its heart a latent linear consider a gallery of face images x i i j j so that the vector xij represents the jth image of the ith person. as a latent linear model of faces we consider xij fhi gwij here f f d f is used to model variability between people and g g d g models variability related to pose illumination etc. within the different images of each person. the contribution fi fhi accounts for variability between different people being constant for individual i. for fixed i the contribution gwij accounts for the variability over the images of person i explaining why two images of the same person do not look identical. see for a graphical representation. as a probabilistic linear latent variable model we have for an image xij the parameters are g for the collection of images assuming i.i.d. data pxijhi wij n fhi gwij phi n i pwij n i pxijhi wij phi for which the graphical model is depicted in the task of learning is then to maximise the likelihood px w h px px w h wh draft march interlude modelling faces mean variance figure latent identity model of face images. each image is represented by a vector comes from the rgb colour coding. there are i individuals in the database and j per pixel standard deviation black is low white is images per person. three samples from high. the model with h fixed and drawing randomly from w in the within-individual subspace g. reproduced from three directions from the between-individual subspace f. mean of the data. this model can be seem as a constrained version of factor analysis by using stacked vectors for only a single individual i f g f g g f the generalisation to multiple individuals i is straightforward. the model can be trained using either a constrained form of the direct method or em as described in example images from the trained model are presented in recognition in closed set face recognition a new probe face x is to be matched to a person n in the gallery of training faces. in model mn the nth gallery face is forced to share its latent identity variable hn with the test face indicating that these faces belong to the same assuming a single exemplar per person xi x pxn x pxi bayes rule then gives the posterior class assignment xi x xi x for a uniform prior the term pmn is constant and can be neglected. the quantities we require above are given by pxn pxn hn wn px px h w hnwn h w is analogous to bayesian outcome analysis in in which the hypotheses assume that either the errors were generated from the same or a different model. draft march probabilistic principal components analysis x w hnwnw pxn x x w figure face recognition model only for a single examplar per person j in model the test image probe x is assumed to be from person albeit with a different for model poseillumination. the test image is assumed to be from person one calculates x and x and then uses bayes rule to infer which person the test image x most likely belongs. pxn x hn wn w where px h w is obtained from equation we assume the parameters are fixed having been learned using maximum likelihood. note that in equation we do not introduce h since both xn and x are assumed to be generated from the same person with the latent identity hn. these marginal probabilities are straightforward to derive since they are marginals of gaussians. in practice the best results are obtained using a between-individual subspace dimension f and withinindividual subspace dimension g both equal to this model then has performance competitive with the a benefit of the probabilistic model is that the extension to mixtures of this model is essentially straightforward which boosts performance further. related models can also be used for the open set face recognition problem in which the probe face may or may not belong to one of the individuals in the probabilistic principal components analysis ppca corresponds to factor analysis under the restriction plugging this assumption into the direct optimisation solution equation gives f uh h ih r where the eigenvalues entries of h and corresponding eigenvectors of uh are the largest eigenvalues of since the eigenvalues of are those of s simply scaled by the eigenvectors are unchanged we can equivalently write h r f uh where r is an arbitrary orthogonal matrix with rtr i and uh h are the eigenvectors and corresponding eigenvalues of the sample covariance s. classical pca is recovered in the limit note that for a full correspondence with pca one needs to set r i which points f along the principal directions. optimal a particular convenience of ppca is that the optimal noise can be found immediately. we order the eigenvalues of s so that d. in equation an expression for the log likelihood is given in which the eigenvalues are those on replacing i with i we can therefore write an explicit expression for the log likelihood in terms of and the eigenvalues of a l n d draft march i h log h log i canonical correlation analysis and factor analysis figure for a hidden unit model here are plotted the results of training ppca and fa on examples of the handwritten digit seven. the top row contains the factor analysis factors and the bottom row the largest eigenvectors from ppca are plotted. by differentiating l and equating to zero the maximum likelihood optimal setting for is d h j in summary ppca is obtained by taking the principal eigenvalues and corresponding eigenvectors of the sample covariance matrix s and setting the variance by equation the single-shot training nature of ppca makes it an attractive algorithm and also gives a useful initialisation for factor analysis. example comparison of fa and ppca. we trained both ppca and fa to model handwritten digits of the number from a database of such images we fitted both ppca and fa iterations of em in each case from the same random initialisation using hidden units. the learned factors for these models are in to get a feeling for how well each of these models the data we drew samples from each model as given in compared with ppca in fa the individual noise on each observation pixel enables a cleaner representation of the regions of zero sample variance. canonical correlation analysis and factor analysis we outline how cca as discussed in is related to a constrained form of fa. as a brief reminder cca considers two spaces x and y where for example x might represent an audio sequence of a person speaking and y the corresponding video sequence of the face of the person speaking. the two streams of data are dependent since we would expect the parts around the mouth region to be correlated with the speech signal. the aim in cca is to find a low dimensional representation that explains the correlation between the x and y spaces. a model that achieves a similar effect to cca is to use a latent factor h to underlie the data in both the x and y spaces. that is px y where pxhpyhphdh pxh n ha x pyh n hb y ph n figure samples from the learned fa model. note how the noise variance depends on the pixel being zero for pixels on the boundary of samples the image. from the learned ppca model. factor analysis ppca draft march fafafafafapcapcapcapcapca independent components analysis h x y canonical correlation analysis. cca corresponds to the latent variable model in which a common latent variable generates both the observed x and y variables. this is therefore a formed of constrained factor analysis. we can express equation as a form of factor analysis by writing n x n y x y z x y a b h a b f by using the stacked vectors and integrating out the latent variable h we obtain ff t x y from the fa results equation the optimal f is given by pz n f t f s f s so that optimally f is given by the principal eigenvector of s by imposing x above equation can be expressed as the coupled equations xi y yi the x x sxxa syxa y y sxyb syyb a b i eliminating b we have for an arbitrary proportionality constant x sxx a x y sxy y syy syxa i x in the limit y this tends to the zero derivative condition equation so that cca can be seen as in fact a form limiting of fa for a more thorough correspondence. by viewing cca in this manner extensions to using more than a single latent dimension h become clear see in addition to the benefits of a probabilistic interpretation. as we ve indicated cca corresponds to training a form of fa by maximising the joint likelihood px yw u. alternatively training based on the maximising the conditional pyx w u corresponds to a special case of a technique called partial least squares see for example this correspondence is left as an exercise for the interested reader. extending fa to kernel variants is not straightforward since under replacing x with a non-linear mapping normalising the expression e is in general intractable. independent components analysis independent components analysis is a linear decomposition of the data v in which the components of underlying latent vector variable h are in other words we seek a linear coordinate system in which the coordinates are independent. such independent coordinate systems arguably form draft march independent components analysis exp figure latent data is sampled from the prior pxi with the mixing matrix a shown in green to create the observed two dimensional vectors y ax. the red lines are the mixing matrix estimated by ica.m based on the observations. for comparison pca produces the blue components. note that the components have been scaled to improve visualisation. as expected pca finds the orthogonal directions of maximal variation. ica however correctly estimates the directions in which the components were independently generated. see demoica.m. a natural representation of the data and can give rise to very different representations than pca see from a probabilistic viewpoint the model is described by phi pv ha pvh i in ica it is common to assume that the observations are linearly related to the latent variables h. for technical reasons the most convenient practical choice is to where a is a square mixing matrix so that the likelihood of an observation v is v ah pvh i pv phidh i phidh i i the underlying independence assumptions are then the same as for ppca the limit of zero output noise. below however we will choose a non-gaussian prior phi. for a given set of data v and prior ph our aim is to find a. for i.i.d. data the log likelihood is conveniently written in terms of b a lb n log det log pbvni n i note that for a gaussian prior ph e the log likelihood becomes lb n log det n btbvn const. lb n aba bab where d dx log px b n px d dx px which is invariant with respect to an orthogonal rotation b rb with rtr i. this means that for a gaussian prior ph we cannot estimate uniquely the mixing matrix. to break this rotational invariance we therefore need to use a non-gaussian prior. assuming we have a non-gaussian prior ph taking the derivative w.r.t. bab we obtain treatment follows that presented in draft march exercises a simple gradient ascent learning rule for b is then n b t n n n bnew b an alternative natural gradient that approximates a newton update is given by multiplying the gradient by btb on the right to give the update bnew b i b here is a learning rate which in the code ica.m we nominally set to a natural extension is to consider noise on the outputs for which an em algorithm is readily available. however in the limit of low output noise the em formally fails an effect which is related to the general discussion in a popular alternative estimation method is and can be related to an iterative maximum likelihood optimisation procedure. ica can also be motivated from several alternative directions including information we refer the reader to for an in-depth discussion of ica and related extensions. code fa.m factor analysis demofa.m demo of factor analysis ica.m independent components analysis demoica.m demo ica exercises exercise factor analysis and scaling. assume that a h-factor model holds for x. now consider the the transformation y cx where c is a non-singular square diagonal matrix. show that factor analysis is scale invariant i.e. that the h-factor model also holds for y with the factor loadings appropriately scaled. how must the specific factors be scaled? exercise for the constrained factor analysis model h n diag n h n i derive a maximum likelihood em algorithm for the matrices a and b assuming the datapoints xn are i.i.d. exercise an apparent extension of fa analysis is to consider a correlated prior ph n h show that provided no constraints are placed on the factor loading matrix f using a correlated prior ph is an equivalent model to the original uncorrelated fa model. exercise using the woodbury identity and the definition of d in equation show that one can rewrite d a b x as d i ft www.cis.hut.fiprojectsicafastica draft march exercise consider an ica model show l is maximal for j d h pyjx py xw j x pyjx w n yj wt j i with pxi w wj exercises exercise for the log likelihood function l n d log i i h log h for the above model derive an em algorithm for a set of i.i.d. data yn and show that the required statistics for the m-step are pxynw. show that for a non-gaussian prior pxi the posterior pxy w is non-factorised non-gaussian and generally intractable normalisation constant cannot be computed efficiently. show that in the limit the em algorithm fails. draft march chapter latent ability models the rasch model consider an exam in which student s answers question q either correctly xqs or incorrectly xqs for a set of n students and q questions the performance of all students is given in the q n binary matrix x. based on this data alone we wish to evaluate the ability of each student. one approach is to define the ability as as the fraction of questions student s answered correctly. a more subtle analysis is to accept that some questions are more difficult than others so that a student who answered difficult questions should be awarded more highly than a student who answered the same number of easy questions. a priori we do not know which are the difficult questions and this needs to be estimated based on x. to account for inherent differences in question difficulty we may model the probability that a student s gets a question q correct based on the student s latent ability s and the latent difficulty of the question q. a simple generative model of the response is pxqs s q where e x. under this model the higher the latent ability is above the latent difficulty of the question the more likely it is that the student will answer the question correctly. maximum likelihood training making the i.i.d. assumption the likelihood of the data x under this model is px l log px the log likelihood is s qxqs s xqs xqs log s q xqs log s q qs q q xqs s s figure the rasch model for analysing questions. each element of the binary matrix x with xqs if student s gets question q correct is generated using the latent ability of the student s and the latent difficulty of the question q. with derivatives competition models s q l q s q l s a simple way to learn the parameters is to use gradient ascent see demorasch.m with extensions to newton methods being straightforward. the generalisation to more than two responses xqs can be achieved using a softmax-style function. more generally the rasch model is an example of an item response theory a subject dealing with the analysis of missing data assuming the data is missing at random missing data can be treated by computing the likelihood of only the observed elements of x. in rasch.m missing data is assumed to be coded as nan so that the likelihood and gradients are straightforward to compute based on summing only over terms containing non nan entries. example we display an example of the use of the rasch model in estimating the latent abilities of students based on a set of questions. based on using the number of questions each student answered correctly the best students are from first alternatively ranking students according to the latent ability gives this differs slightly in this case from the number-correct ranking since the rasch model takes into account the fact that some students answered difficult questions correctly. for example student answered some difficult questions correctly. bayesian rasch models the rasch model will potentially overfit the data especially when there is only a small amount of data. for this case a natural extension is to use a bayesian technique placing independent priors on the ability and question difficulty so that the posterior ability and question difficulty is given by p px natural priors are p s s n p q n q where and are hyperparameters that can be learned by maximising px even using gaussian priors the posterior distribution p is not of a standard form and approximations are required. in this case however the posterior is log concave so that approximation methods based on variational or laplace techniques are potentially adequate or alternatively one may use sampling approximations. competition models bradly-terry-luce model the bradly-terry-luce model assesses the ability of players based on one-on-one matches. here we describe games in which only winlose outcomes arise leaving aside the complicating possibility of draws. for this draft march competition models figure rasch model. the data of correct answers and incorrect answers the fraction of questions each student answered the estimated latent difficulty of each question. correctly. the estimated latent ability. winlose scenario the btl model is a straightforward modification of the rasch model so that for latent ability i of player i and latent ability j of player j the probability that i beats j is given by where i j stands for player i beats player j. based on a matrix of games data x with pi j i j if i j in game n otherwise xn ij px where mij the likelihood of the model is given by i jxn n ij ij ij i jmij or a bayesian technique can then proceed as for the rasch model. n xn ij is the number of times player i beat player j. training using maximum likelihood for the case of only two objects interacting these models are called pairwise comparison models. thurstone in the s applied such models to a wide range of data and the bradley-terry-luce model is in fact a special case of his example an example application of the btl model is given in in which a matrix x containing the number of times that competitor i beat competitor j is given. the matrix entries x were drawn from a btl model based on true abilities using x alone the maximum likelihood estimate of these latent abilities is in close agreement with the true abilities. draft march of questions ability competition models the data x with xij being the number of times that competitor i beat figure btl model. competitor j. the true versus estimated ability. even though the data is quite sparse a reasonable estimate of the latent ability of each competitor is found. elo ranking model the elo system used in chess ranking is closely related to the btl model above though there is the added complication of the possibility of draws. in addition the elo system takes into account a measure of the variability in performance. for a given ability i the actual performance i of player i in a game is given by i i where n variability in the performance. more formally the elo model modifies the btl model to give the variance is fixed across all players and thus takes into account intrinsic px px p n where px is given by equation on replacing with glicko and trueskill and are essentially bayesian versions of the elo model with the refinement that the latent ability is modelled not by a single number but by a gaussian distribution this can capture the fact that a player may be consistently reasonable high i and low erratic genius i but with large i the parameters of the model are then i or an for a set of s players. the interaction model px is as for the winlose elo model equation the likelihood for the model given the parameters is i p i i n i i i i i px px this integral is formally intractable and numerical approximations are required. in this context expectation propagation has proven to be a useful the trueskill system is used for example to assess the abilities of players in online gaming also taking into account the abilities of teams of individuals in tournaments. a temporal extension has recently been used to reevaluate the change in ability of chess players with draft march competitorcompetitor abilityestimated ability exercises code rasch.m rasch model training demorasch.m demo for the rasch model exercises exercise bronco. bronco.mat contains information about a bucking bronco competition. there are competitors and bucking broncos. a competitor j attempts to stay on a bucking bronco i for a minute. if the competitor succeeds the entry xij is otherwise each competitor gets to ride three bucking broncos only missing data is coded as nan. having viewed all the amateurs desperate dan enters the competition and bribes the organisers into letting him avoid having to ride the difficult broncos. based on using a rasch model which are the top most difficult broncos in order of the most difficult first? exercise training. show that the log likelihood for the bradly-terry-luce model is given by xij log i j l ij where xij is the number of times that player i beats player j in a set of games. compute the gradient of l compute the hessian of the btl model and verify that it is negative semidefinite. exercise reine. program a simple gradient ascent routine to learn the latent abilities of competitors based on a series of winlose outcomes. in a modified form of swiss cow fighting a set of cows compete by pushing each other until submission. at the end of the competition one cow is deemed to be la reine based on the data in btl.mat which xij contains the number of times cow i beat cow j fit a btl model and return a ranked list of the top ten best fighting cows la reine first. exercise an extension of the btl model is to consider additional factors that describe the state of the competitors when they play. for example we have a set of s football teams and a set of matrices ij if team i beat team j in match n. in addition we have for each match and team xn with x n i that describes the team. for example for the team i a vector of binary factors f n united the factor if bozo is playing if not. it is suggested that the ability of team i in game n is measured by n i di whif n hi hi if factor h is present in team i in game n. di is a default latent ability of the team which where f n is assumed constant across all games. we have such a set of factors for each match giving f n hi. using the above definition of the latent ability in the btl model our interest is to find the weights w and abilities d that best predict the ability of the team given that we have a set of historical plays fn n n. write down the likelihood for the btl model as a function of the set of all team weights w d. draft march exercises compute the gradient of the log likelihood of this model. explain how this model can be used to assess the importance of bozo s contribution to madchester united s ability. given learned w d and the knowledge that madchester united will play chelski tomorrow explain how given the list of factors f for chelski includes issues such as who will be playing in the team one can select the best madchester united team to maximise the probability of winning the game. draft march part iv dynamical models chapter discrete-state markov models markov models time-series are datasets for which the constituent datapoints can be naturally ordered. this order often corresponds to an underlying single physical dimension typically time though any other single dimension may be used. the time-series models we consider are probability models over a collection of random variables vt with individual variables vt indexed by a time index t. these indices are elements of the index set t for nonnegative indices t n the model is a discrete-time process. continuoustime processes t r are natural in particular application domains yet require additional notation and concepts. we therefore focus exclusively on discrete-time models. a probabilistic time-series model requires a specification of the joint distribution vt for the case in which the observed data vt is discrete the joint probability table for vt has exponentially many entries. we cannot expect to independently specify all the exponentially many entries and are therefore forced to make simplified models under which these entries can be parameterised in a lower dimensional manner. such simplifications are at the heart of time-series modelling and we will discuss some classical models in the following sections. definition notation. xab xa xb with xab xa for b a for timeseries data vt we need a model for consistency with the causal nature of time it is meaningful to consider the decomposition with the convention for t it is often natural to assume that the influence of the immediate past is more relevant than the remote past and in markov models only a limited number of previous observations are required to predict the future. definition chain. a markov chain defined on either discrete or continuous variables is one in which the following conditional independence assumption holds vt pvtvt l vt markov models figure first order markov chain. second order markov chain. where l is the order of the markov chain and vt for t for a first order markov chain pvtvt for a stationary markov chain the transitions pvt wise the chain is non-stationary pvt s s t. s s are time-independent. other equilibrium and stationary distribution of a markov chain the stationary distribution p of a markov chain with transition matrix m is defined by the condition p pxt ixt j mij j p in matrix notation this can be written as the vector equation p mp so that the stationary distribution is proportional to the eigenvector with unit eigenvalue of the transition matrix. note that there may be more than one stationary distribution. see and given a state we can iteratively draw samples xt from the markov chain drawing a sample from and then from etc. as we repeatedly sample a new state from the chain the distribution at time t for an initial distribution is pt if for t p is independent of the initial distribution then p is called the equilibrium distribution of the chain. see for an example of a markov chain which does not have an equilibrium distribution. example despite their apparent simplicity markov chains have been put to interesting use in information retrieval and search-engines. define the matrix aij if website j has a hyperlink to website i otherwise from this we can define a markov transition matrix with elements mij i aij the equilibrium distribution of this markov chain has the interpretation if follow links at random jumping from website to website the equilibrium distribution component p is the relative number of times we will visit website i. this has a natural interpretation as the importance of website i if a website draft march markov models is isolated in the web it will be visited infrequently by random hopping if a website is linked by many others it will be visited more frequently. a crude search engine works then as follows. for each website i a list of words associated with that website is collected. after doing this for all websites one can make an inverse list of which websites contain word w. when a user searches for word w the list of websites that that word is then returned ranked according to the importance of the site defined by the equilibrium distribution. this is a crude summary as how early search engines worked infolab.stanford.edu backrubgoogle.html. figure a state transition diagram for a three state markov chain. note that a state transition diagram is not a graphical model it simply displays the non-zero entries of the transition matrix pij. the absence of link from j to i indicates that pij fitting markov models given a sequence fitting a stationary markov chain by maximum likelihood corresponds to setting the transitions by counting the number of observed transitions in the sequence pv iv j i i vt j to show this for convenience we write pv iv j ij so that the likelihood is is known ivtivt ij vtvt ij ij l taking logs and adding the lagrange constraint for the normalisation i i vt j log ij j ij j i ij differentiating with respect to ij and equating to zero we immediately arrive at the intuitive setting equation for a set of timeseries vn n n the transition is given by counting all transitions across time and datapoints. the maximum likelihood setting for the initial first timestep distribution is i bayesian fitting i. i n for simplicity we assume a factorised prior on the transition p a convenient choice for each conditional transition is a dirichlet distribution with hyperparameters uj p j p where uj p draft march since this is conjugate to the categorical transition and uj ivtivt ij uij ij t ij j i i vt j being the number of j i transitions in the dataset. markov models h figure mixture of first order markov chains. the discrete hidden variable domh h indexes the markov chain pvtvt h. such models can be useful as simple sequence clustering tools. mixture of markov models given a set of sequences v n n how might we cluster them? to keep the notation less cluttered we assume that all sequences are of the same length t with the extension to differing lengths being straightforward. one simple approach is to fit a mixture of markov models. assuming the data we define a mixture model for a single datapoint here we assume each n pvn component model is first order markov is i.i.d. pv ph pvtvt h the graphical model is depicted in clustering can then be achieved by finding the maximum likelihood parameters ph pvtvt h and subsequently assigning the clusters according to phvn below we discuss the application of the em algorithm to this model to learn the maximum likelihood parameters. em algorithm under the i.i.d. data assumption the log likelihood is log ph pvn t h for the m-step our task is to maximise the energy pvn the contribution to the energy from the parameter ph is t pvtvt log pv e by defining poldh kl poldhvn poldhph pnewh poldhvn one can view maximising as equivalent to minimising so that the optimal choice from the m-step is to set pnew pold namely for those less comfortable with this argument a direct maximisation including a lagrange term to ensure normalisation of ph can be used to derive the same result. draft march markov models similarly the m-step for pvtvt h is pnewvt ivt j h k poldh kvn t i t i the initial term is updated using ih k the e-step sets i poldh kvn poldhvn phpvn ph pvn t t h given an initialisation the em algorithm then iterates and until convergence. for long sequences explicitly computing the product of many terms may lead to numerical underflow issues. in practice it is therefore best to work with logs log poldhvn log ph log pvn t t h const. in this way any large constants common to all h can be removed and the distribution may be computed accurately. see mixmarkov.m. example clustering. consider the fictitious gene sequences below presented in an arbitrarily chosen order. each sequence consists of symbols from the set c g t. the task is to try to cluster these sequences into two groups based on the biologically unrealistic assumption that gene sequences in the same cluster follow a stationary markov chain. cataggcattctatgtgctg gtgcctggacctgaaaagcc gttggtcagcacacggactg taagtgtcctctgctcctaa gccaagcagggtctcaactt ccagttacggacgccgaaag cggccgcgcctccgggaacg cctcccctcccctttcctgc caccatcacccttgctaagg catggactgctccacaaagg tggaaccttaaaaaaaaaaa aaagtgctctgaaaactcac cactacggctacctgggcaa aaagaactcccctccctgcc aaaaaaacgaaaaacctaag gtctcctgccctctctgaac acatgaactacatagtataa cggtccgtccgaggcactc caaatgcctcacgcgtctca gcgtaaaaaaagtcctgggt a simple approach is to assume that the sequences are generated from a two-component h mixture of markov models and train the model using maximum likelihood. the likelihood has local optima so that the procedure needs to be run several times and the solution with the highest likelihood chosen. one can if this posterior probability is greater than then assign each of the sequences by examining ph we assign it to class otherwise to class using this procedure we find the following clusters cataggcattctatgtgctg ccagttacggacgccgaaag cggccgcgcctccgggaacg acatgaactacatagtataa gttggtcagcacacggactg cactacggctacctgggcaa cggtccgtccgaggcactcg caccatcacccttgctaagg caaatgcctcacgcgtctca gccaagcagggtctcaactt catggactgctccacaaagg tggaaccttaaaaaaaaaaa gtctcctgccctctctgaac gtgcctggacctgaaaagcc aaagtgctctgaaaactcac cctcccctcccctttcctgc taagtgtcctctgctcctaa aaagaactcccctccctgcc aaaaaaacgaaaaacctaag gcgtaaaaaaagtcctgggt where sequences in the first column are assigned to cluster and sequences in the second column to cluster in this case the data in was in fact generated by a two-component markov mixture and the posterior assignment is in agreement with the known clusters. see demomixmarkov.m draft march hidden markov models figure a first order hidden markov model with hidden variables domht h t t the visible variables vt can be either discrete or continuous. hidden markov models the hidden markov model defines a markov chain on hidden latent variables the observed visible variables are dependent on the hidden variables through an emission pvtht. this defines a joint distribution pvthtphtht for which the graphical model is depicted in for a stationary hmm the transition phtht and emission pvtht distributions are constant through time. the use of the hmm is widespread and a subset of the many applications of hmms is given in definition distribution. for a stationary hmm the transition distribution is defined by the h h transition matrix i i and an initial distribution ai i. definition distribution. for a stationary hmm and emission distribution pvtht with discrete states vt v we define a v h emission matrix bij pvt iht j for continuous outputs ht selects one of h possible output distributions pvtht ht h. in the engineering and machine learning communities the term hmm typically refers to the case of discrete variables ht a convention that we adopt here. in statistics the term hmm often refers to any model with the independence structure in equation regardless of the form of the variables ht for example the classical inference problems filtering prediction smoothing likelihood most likely hidden path alignment the present the future the past argmax t s t u the most likely hidden path problem is termed viterbi alignment in the engineering literature. all these classical inference problems are straightforward since the distribution is singly-connected so that any standard inference method can be adopted for these problems. the factor graph and junction trees for draft march hidden markov models figure factor graph for the first order hmm of junction tree for the first order hmm are given in in both cases after suitable setting of the factors and clique potentials filtering corresponds to passing messages from left to right and upwards smoothing corresponds to a valid schedule of message passingabsorption both forwards and backwards along all edges. it is also straightforward to derive appropriate recursions directly. this is instructive and also useful in constructing compact and numerically stable algorithms. filtering we first compute the joint marginal pht from which the conditional marginal can subsequently be obtained by normalisation. a recursion for pht is obtained by considering pht ht ht ht pht ht vt ht ht ht ht pvthtphtht the cancellations follow from the conditional independence assumptions of the model. hence if we define pht pvtht corrector ht equation above gives the phtht predictor with t this recursion has the interpretation that the filtered distribution is propagated forwards by the dynamics for one timestep to reveal a new prior distribution at time t. this distribution is then modulated by the observation vt incorporating the new evidence into the filtered distribution is also referred to as a predictor-corrector method. since each is smaller than and the recursion involves multiplication by terms less than the s can become very small. to avoid numerical problems it is therefore advisable to work with log see hmmforward.m. normalisation gives the filtered posterior if we only require the filtered posterior we are free to rescale the s as we wish. in this case an alternative to working with log messages is to work with normalised messages so that the sum of the components is always draft march hidden markov models we can write equation above directly as a recursion for the filtered distribution pvthtphtht t ht intuitively the term pht has the effect of removing all nodes in the graph before time t and replacing their influence by a modified prior distribution on ht. one may interpret pvthtphtht as a likelihood giving rise to the joint posterior pht ht under bayesian updating. at the next timestep the previous posterior becomes the new prior. parallel smoothing there are two main approaches to computing perhaps the most common in the hmm literature is the parallel method which is equivalent to message passing on factor graphs. in this one separates the smoothed posterior into contributions from the past and future pht pht pht past future the term is obtained from the forward recursion the term may be obtained using a backward recursion as we show below. the forward and backward recursions are independent and may therefore be run in parallel with their results combined to obtain the smoothed posterior. this approach is also sometimes termed the two-filter smoother. the recursion pvttht ht ht ht defining pvt htht ht ht htht ht ht equation above gives the pvthtphtht t t with as for the forward pass working in log space is recommended to avoid numerical difficulties. if one only desires posterior distributions one can also perform local normalisation at each stage since only the relative magnitude of the components of are of importance. the smoothed posterior is then given by ht together the recursions are called the forward-backward algorithm. correction smoothing an alternative to the parallel method is to form a recursion directly for the smoothed posterior. this can be achieved by recognising that conditioning on the present makes the future redundant pht draft march hidden markov models this gives a recursion for with the term may be computed using the filtered results where the proportionality constant is found by normalisation. this is a form of dynamics reversal as if we are reversing the direction of the hidden to hidden arrow in the hmm. this procedure also termed the rauch-tung-striebel is sequential since we need to first complete the recursions after which the recursion may begin. this is a so-called correction smoother since it corrects the filtered result. interestingly once filtering has been carried out the evidential states are not needed during the subsequent recursion. the and recursions are related through computing the pairwise marginal pht ht to implement the em algorithm for learning we require terms such as pht ht these can be obtained by message passing on either a factor graph or junction tree which the pairwise marginals are contained in the cliques see alternatively an explicit recursion is as follows pht ht ht ht ht ht ht rearranging we therefore have pht see hmmsmooth.m. the likelihood the likelihood of a sequence of observations can be computed from pht ht an alternative computation can be found by making use of the decomposition ht each factor can be computed using pvt ht ht ht ht phtht is most common to use this terminology for the continuous variable case though we adopt it here also for the discrete variable case. draft march hidden markov models figure localising the burglar. the latent variable ht denotes the positions defined over the grid of the ground floor of the house. a representation of the probability that the floor will creak at each of the positions pvcreakh. light squares represent probability and dark square a representation of the probability pvbumph that the burglar will bump into something in each of the positions. creaks bumps where the final term pht is the filtered result. in both approaches the likelihood of an output sequence requires only a forward computation if required one can also compute the likelihood using ht which is valid for any t t most likely joint state the most likely path of is the same as the most likely state fixed of pvthtphtht t the most likely path can be found using the max-product version of the factor graph or max-absorption on the junction tree. alternatively an explicit derivation can be obtained by considering max ht pvthtphtht pvthtphtht max ht pvtht the message conveys information from the end of the chain to the penultimate timestep. we can continue in this manner defining the recursion max ht pvthtphtht t t with this means that the effect of maximising over ht is compressed into a message so that the most likely state h argmax h is given by once computed backtracking gives t argmax h ht pvthtphth t this special case of the max-product algorithm is called the viterbi algorithm. similarly one may use the n-max-product algorithm to obtain the n-most likely hidden paths. example localisation example. you re asleep upstairs in your house and awoken by noises from downstairs. you realise that a burglar is on the ground floor and attempt to understand where he his from listening to his movements. you mentally partition the ground floor into a grid. for each grid position you know the probability that if someone is in that position the floorboard will creak draft march hidden markov models creaks and bumps filtering smoothing viterbi true burglar position t t vcreak t vbump where vcreak t t otherwise and vbump each panel represents the means that there was a creak in the floorboard meaning bumped into something is in state otherwise. t and the right t the lighter colour represents the occurrence of a creak or bump the darker colour the absence. the smoothed the most likely figure localising the burglar through time for time steps. visible information vt there are panels one for each time t the left half of the panel represents half the filtered distribution representing where we think the burglar is. distribution so that we can figure out where we think the burglar went. burglar path arg the actual path of the burglar. similarly you know for each position the probability that someone will bump into something in the dark the floorboard creaking and bumping into objects can occur independently. in addition you assume that the burglar will move only one grid square forwards backwards left or right in a single timestep. based on a series of bumpno bump and creakno creak information you try to figure out based on your knowledge of the ground floor where the burglar might be. we can represent the scenario using a hmm where h denotes the grid square. the visible variable has a factorised form v vcreak vbump and to use our standard code we form a new visible variable with states using pvh pvcreakhpvbumph based on the past information our belief as to where the burglar might be is represented by the filtered distribution after the burglar has left at t the police arrive and try to piece together where the burglar went based on the sequence of creaks and bumps you provide. at any time t the information as to where the burglar could have been is represented by the smoothed distribution the police s single best-guess for the sequence of burglar positions is provided by the most likely joint hidden state arg self localisation and kidnapped robots a robot has an internal grid-based map of its environment and for each location h h knows the likely sensor readings he would expect in that location. the robot is kidnapped and placed somewhere in the environment. the robot then starts to move gathering sensor information. based on these readings and intended movements the robot attempts to figure out his location. due to wheel slippage on the floor an intended action by the robot such as move forwards might not be successful. given all the information the robot has he would like to infer this problem differs from the burglar scenario in that the robot now has knowledge of the intended movements he makes. this should give more draft march learning hmms figure a model for robot self-localisation. at each time the robot makes an intended movement mt. as a generative model knowing the intended movement mt and the current grid position ht the robot has an idea of where he should be at the next time-step and what sensor reading he would expect there. based on only the sensor information and the intended movements the task is to infer a distribution over robot locations information as to where he could be. one can view this as extra visible information though it is more natural to think of this as additional input information. a model of this scenario is see pvthtphtht mt the visible variables are known as are the intended movements the model expresses that the movements selected by the robot are random no decision making in terms of where to go next. we assume that the robot has full knowledge of the conditional distributions defining the model knows the map of his environment and all state transition and emission probabilities. if our interest is only in localising the robot since the inputs m are known this model is in fact a form of time-dependent hmm pvthtphtht t for a time-dependent transition phtht t defined by the intended movement mt any inference task required then follows the standard stationary hmm algorithms albeit on replacing the time-independent transitions phtht with the known time-dependent transitions. in self localisation and mapping the robot does not know the map of his environment. this corresponds to having to learn the transition and emission distributions on-the-fly as he explores the environment. natural language models a simple generative model of language can be obtained from the letter-to-letter transitions so-called bigram. in the example below we use this in a hmm to clean up the mis-typings. example fingers. a stubby fingers typist has the tendency to hit either the correct key or a neighbouring key. for simplicity we assume that there are keys lower case a to lower case z and the space bar. to model this we use an emission distribution bij pv ih j where i j as depicted in a database of letter-to-next-letter frequencies yields the transition matrix aij ih j in english. for simplicity we assume that is uniform. also we assume that each intended key press results in a single press. given a typed sequence kezrninh what is the most likely word that this corresponds to? by listing the most likely hidden sequences the n-max-product algorithm and discarding those that are not in a standard english dictionary the most likely word that was intended is learning. see demohmmbigram.m. learning hmms given a set of data v of n sequences where sequence vn vn is of length tn we seek the hmm transition matrix a emission matrix b and initial vector a most likely to have have generated draft march learning hmms figure the letter-to-letter transition matrix for english ih j. the letter emission matrix for a typist with stubby fingers in which the key or its neighbours on the keyboard are likely to be hit. v. we make the i.i.d. assumption so that each sequence is independently generated and assume that we know the number of hidden states h. for simplicity we concentrate here on the case of discrete visible variables assuming also we know the number of states v em algorithm the application of em to the hmm model is called the baum-welch algorithm and follows the general strategy outlined in m-step assuming i.i.d. data the m-step is given by maximising the energy t n hn vn vn pvn tn hn hn t with respect to the parameters a b a hn denotes using the form of the hmm we obtain pvn t where for compactness we drop the sequence index from the h variables. to avoid potential confusion we write i to denote the table entry for the probability that the initial hidden variable is in state i. optimising equation with respect to enforcing to be a distribution we obtain anew i i n ivn tn which is the average number of times that the first hidden variable is in state i. similarly which is the number of times that a transition from hidden state i to hidden state occurs averaged over all times we assumed stationarity and training sequences. normalising we obtain poldht i i i anew i poldht i poldht i anew draft march abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz finally bnew ji pnewvt jht i i t j poldht ivn learning hmms which is the expected number of times that for the observation being in state j the hidden state is i. the proportionality constant is determined by the normalisation requirement. e-step in computing the m-step above the quantities ivn poldht i are obtained by inference using the techniques described in and poldht ivn equations are repeated until convergence. see hmmem.m and demohmmlearn.m. parameter initialisation the em algorithm converges to a local maxima of the likelihood and in general there is no guarantee that the algorithm will find the global maximum. how best to initialise the parameters is a thorny issue with a suitable initialisation of the emission distribution often being critical for a practical strategy is to initialise the emission pvh based on first fitting a simpler non-temporal mixture model h pvhph to the data. continuous observations for a continuous vector observation vt with dim vt d we require a model pvtht mapping the discrete state ht to a distribution over outputs. using a continuous output does not change any of the standard inference message passing equations so that inference can be carried out for essentially arbitrarily complex emission distributions. indeed filtering smoothing and viterbi inference the normalisation z of the emission pvh hz is not required. for learning however the emission normalisation constant is required since this is a dependent on the parameters of the model. mixture emission to make a richer emission model for continuous observations one approach is use a mixture pvtht kt n pvtkt htpktht where kt is a discrete summation variable. for learning it is useful to consider the kt as additional latent variables so that updates for each component of the emission model can be derived. to achieve this consider the contribution to the energy from the emission equal length sequences ev pvn t as it stands the parameters of each component pvtkt ht are coupled in the above expression. one approach is to consider klqkthtpktht vt from which we immediately obtain the bound log pvt ht pvt kt and log pvn t t qkthn t t pvn t hn t t pkthn t t draft march learning hmms n n using this in the energy contribution we have the bound on the energy contribution ev qkthn t t pvn t hn t t pkthn t t qhn t we may now maximise this lower bound on the energy of the energy itself. the contribution from each emission component pv vh h k k is qkt khn t hqhn t hvn log pvn t h k k the above can then be optimised for fixed qkt khn using t h with these distributions updated the contribution to the energy bound from the mixture weights is given by t ktpktht qnewkthn t pvnhn log pk kh n pk kh h n qkt khn t hqhn t hvn qkt khn t hqhn t hvn so that the m-step update for the mixture weights is is fixed during which the emissions pvh k are learned along with updating qkt khn t in this case the em algorithm is composed of an emission em loop in which the transitions and qhn t h. hvn the transition em loop fixes the emission distribution pvh and learns the best transition phtht an alternative to the above derivation is to consider the k as hidden variables and then use standard em algorithm on the joint latent variables kt. the reader may show that the two approaches are equivalent. the hmm-gmm ktht ktht a common continuous observation mixture emission model component is a gaussian pvtkt ht n so that kt ht indexes the k h mean vectors and covariance matrices. em updates for these means and covariances are straightforward to derive from equation see these models are common in tracking applications in particular in speech recognition under the constraint that the covariances are diagonal. discriminative training hmms can be used for supervised learning of sequences. that is for each sequence vn we have a corresponding class label cn. for example we might associated a particular composer c with a sequence and wish to make a model that will predict the composer for a novel music sequence. a generative approach to using hmms for classification is to train a separate hmm for each class and subsequently use bayes rule to form the classification for a novel sequence v using pc pv pv if the data is noisy and difficult to model however this generative approach may not work well since much of the expressive power of each model is used to model the complex data rather than focussing on draft march related models figure an explicit duration hmm. the counter variables ct deterministically count down to zero. when they reach one a h transition is allowed and the new value for ct is sampled. the decision boundary. in applications such as speech recognition improvements in performance are often reported when the models are trained in a discriminative way. in discriminative training see for example one defines a new single discriminative model formed from the c hmms using log pcnvn log pvn log pc log generative likelihood and then maximises the likelihood of a set of observed classes and corresponding observations for a single data pair vn the log likelihood is pvn t the first term above represents the generative likelihood term with the last term accounting for the discrimination. whilst deriving em style updates is hampered by the discriminative terms computing the gradient is straightforward using the technique described in in some applications a class label ct is available at each timestep together with an observation vt. given a training sequence more generally a set of sequences the aim is to find the optimal class sequence c for a novel observation sequence v one approach is to train a generative model pvtctpctct and subsequently use viterbi to form the class c arg however this approach may not be optimal in terms of class discrimination. a cheap surrogate is to train a discriminative classification model pctvt separately. with this one can form the emission written for continuous vt pctvt pvt pctvt pvt vt pvtct where pvt is user defined. whilst computing the local if the only use of pvtct is to find the optimal class sequence for a novel observation sequence v pctvt pvt may be problematic argmax c vt then the local normalisations play no role since they are independent of c. hence during viterbi decoding we may replace the term pvtht with pctvt without affecting the optimal sequence. using a model in this way is a special case of the general hybrid procedure described in the approach is suboptimal since learning the classifier is divorced from learning the transition model. nevertheless this heuristic historically has some support in the speech recognition community. related models explicit duration model for a hmm with self-transition pht iht i i the probability that the latent dynamics stays i which decays exponentially with time. in practice however we would in state i for timesteps is draft march related models figure a first order input-output hidden markov model. the input x and output v nodes are shaded to emphasise that their states are known during training. during testing the inputs are known and the outputs are predicted. often like to constrain the dynamics to remain in the same state for a minimum number of timesteps or to have a specified duration distribution. a way to enforce this is to use a latent counter variable ct which at the beginning is initialised to a duration sampled from the duration distribution pdurct with maximal duration dmax. then at each timestep the counter decrements by until it reaches after which a new duration is sampled pctct the state ht can transition only when ct ct ht pdurct ct ct ct ct phtht ct ptranhtht including the counter variable c defines a joint latent variable distribution that ensures h remains in a desired minimal number of timesteps see since dim ct ht dmaxh naively the the forward and backward recursions the deterministic nature of the transitions means that this can be computational complexity of inference in this model scales as h reduced to h see also however when one runs max the hidden semi-markov model generalises the explicit duration model in that once a new duration ct is sampled the model emits a distribution pvttct defined on a segment of the next ct input-output hmm the is a hmm with additional input variables see each input can be continuous or discrete and modulates the transitions pvtht xtphtht xt t the iohmm may be used as a conditional predictor where the outputs vt represent the prediction at time t. in the case of continuous inputs and discrete outputs the tables pvtht xt and phtht xt are usually parameterised using a non-linear function for example pvt yht h xt x w ewt hyx inference then follows in a similar manner as for the standard hmm. defining pht the forward pass is given by pvtxt ht ht ht draft march pht ht ht ht ht ht phtht xt x figure linear chain crf. since the input x is observed the distribution is just a linear chain factor graph. the inference of pairwise marginals pyt yt is therefore straightforward using message passing. related models the backward pass is pht for which we need the likelihood can be found ht ht direction bias consider predicting the output distribution given both past and future input information because the hidden states are unobserved we have thus the prediction uses only past information and discards any future contextual information. this direction bias is sometimes considered problematic in natural language modelling and motivates the use of undirected models such as conditional random fields. linear chain crfs linear chain conditional random fields are an extension of the unstructured crfs we briefly discussed in and have application to modelling the distribution of a set of outputs given an input vector x. for example x might represent a sentence in english and should represent the translation into french. note that the vector x does not have to have dimension t a first order linear chain crf has the form zx tyt yt x where are the free parameters of the potentials. in practice it is common to use potentials of the form exp kfktyt yt x where fktyt yt x are features see also given a set of input-output sequence pairs xn yn n n all sequenced have equal length t for simplicity we can learn the parameters by maximum likelihood. under the standard i.i.d. data assumption the log likelihood is kfkyn t yn t xn log zxn n the reader may readily check that the log likelihood is concave so that the objective function has no local optima. the gradient is given by fiyn t yn t xn yt learning therefore requires inference of the marginal terms pyt yt since equation corresponds to a linear chain factor graph see inference of pairwise marginals is straightforward draft march l tn k l nt i related models figure using a linear chain crf to learn the sequences in log likelihood under gradient ascent. the learned parameter vector at the end of training. the evolution of the using message passing. this can be achieved using either the standard factor graph message passing or by deriving an explicit algorithm see finding the most likely output sequence for a novel input x is straightforward since t argmax y tyt yt x corresponds again to a simple linear chain for which max-product inference yields the required result see also in some applications particularly in natural language processing the dimension k of the vector of features fk may be many hundreds of thousands. this means that the storage of the hessian is not feasible for newton based training and either limited memory methods or conjugate gradient techniques are typically table a subset of the training input-output sequences. each row contains an input xt entry and output yt entry. there are input states and output states. potentials yt xt exp example chain crf. as a model for the data in a linear crf model has i ifiyt yt xt where we set the binary feature functions by first mapping each of the dim dim states to a unique integer ia b c from to dim dim fiabcyt yt xt i a i b i c that is each joint configuration of yt yt xt is mapped to an index and in this case the feature vector f will trivially have only a single non-zero entry. the evolution of the gradient ascent training algorithm is plotted in in practice one would use richer feature functions defined to seek features of the input sequence x and also to produce a feature vector with more than one non-zero entry. see demolinearcrf.m. draft march applications figure a dynamic bayesian network. possible transitions between variables at the same timeslice have not been shown. figure a coupled hmm. for example the upper hmm might model speech and the lower the corresponding video sequence. the upper hidden units then correspond to phonemes and the lower to mouth positions this model therefore captures the expected coupling between mouth positions and phonemes. dynamic bayesian networks a dbn is defined as a belief networkreplicated through time. for a multivariate xt with dim xt d the dbn defines a joint model xt pxitxit xt where xit denotes the set of variables at time t except for xit. the form of each pxitxit xt is chosen such that the overall distribution remains acyclic. at each time-step t there is a set of variables xit i x some of which may be observed. in a first order dbn each variable xit has parental variables taken from the set of variables in the previous time-slice xt or from the present time-slice. in most applications the model is temporally homogeneous so that one may fully describe the distribution in terms of a two-time-slice model the generalisation to higher-order models is straightforward. a coupled hmm is a special dbn that may be used to model coupled streams of information for example video and audio see applications object tracking hmms are used to track moving objects based on an understanding of the dynamics of the object in the transition distribution and an understanding of how an object with a known position would be observed in the emission distribution. given an observed sequence the hidden position can then be inferred. the burglar is a case in point. hmms have been applied in a many tracking contexts including tracking people in videos musical pitch and many automatic speech recognition many speech recognition systems make use of roughly speaking a continuous output vector vt at time t represents which frequencies are present in the speech signal in a small window around time t. these acoustic vectors are typically formed from taking a discrete fourier transform of the speech signal over a small window around time t with additional transformations to mimic human auditory processing. alternatively related forms of linear coding of the observed acoustic waveform may be the corresponding discrete latent state ht represents a phoneme a basic unit of human speech which there are in standard english. training data is painstakingly constructed by a human linguist who draft march applications determines the phoneme ht for each time t and many different observed sequences vt. given then each acoustic vector vt and an associated phoneme ht one may use maximum likelihood to fit a mixture of isotropic gaussians pvtht to vt. this forms the emission distribution for a hmm. using the database of labelled phonemes the phoneme transition phtht can be learned simple counting and forms the transition distribution for a hmm. note that in this case since the hidden variable h and observation v are known during training training the hmm is straightforward and boils down to training the emission and transition distributions independently. for a new sequence of acoustic vectors we can then use the hmm to infer the most likely phoneme sequence through time arg which takes into account both the way that phonemes appear as acoustic vectors and also the prior language constraints of likely phoneme to phoneme transitions. the fact that people speak at different speeds can be addressed using time-warping in which the latent phoneme remains in the same state for a number of timesteps. hmm models are typically trained on the assumption of clean underlying speech. in practice noise corrupts the speech signal in a complex way so that the resulting model is inappropriate and performance degrades significantly. to account for this it is traditional to attempt to denoise the signal before sending this to a standard hmm recogniser. if the hmm is used to model a single word it is natural to constrain the hidden state sequence to go forwards through time visiting a set of states in sequence the phoneme order for the word is known. in this case the structure of the transition matrices is upper triangular lower depending on your definition or even a banded triangular matrix. such forward constraints describe a so-called left-to-right transition matrix. bioinformatics in the field of bioinformatics hmms have been widely applied to modelling genetic sequences. multiple sequence alignment using forms of constrained hmms have been particularly successful. other applications involve gene finding and protein family part-of-speech tagging consider the sentence below in which each word has been linguistically tagged hospitality_nn is_bez an_at excellent_jj virtue_nn but_cc not_xnot when_wrb the_ati guests_nns have_hv to_to sleep_vb in_in rows_nns in_in the_ati cellar_nn the subscripts denote a linguistic tag for example nn is the singular common noun tag ati is the article tag etc. given a training set of such tagged sequences the task is to tag a novel word sequence. one approach is to use ht to be a tag and vt to be a word and fit a hmm to this data. for the training data both the tags and words are observed so that maximum likelihood training of the transition and emission distribution can be achieved by simple counting. given a new sequence of words the most likely tag sequence can be inferred using the viterbi algorithm. more recent part-of-speech taggers tend to use conditional random fields in which the input sequence is the sentence and the output sequence is the tag sequence. one possible parameterisation of for a linear chain crf is to use a potential of the form yt x in which the first factor encodes the grammatical structure of the language and the second the a priori likely tag draft march exercises code demomixmarkov.m demo for mixture of markov models mixmarkov.m mixture of markov models demohmminference.m demo of hmm inference hmmforward.m forward recursion hmmbackward.m forward recursion hmmgamma.m rts correction recursion hmmsmooth.m single and pairwise smoothing hmmviterbi.m most likely state algorithm demohmmburglar.m demo of burglar localisation demohmmbigram.m demo of stubby fingers typing hmmem.m em algorithm for hmm demohmmlearn.m demo of em algorithm for hmm demolinearcrf.m demo of learning a linear chain crf the following linear chain crf potential is particularly simple and in practice one would use a more complex one. linearcrfpotential.m linear crf potential the following likelihood and gradient routines are valid for any linear crf potential yt x. linearcrfgrad.m linear crf gradient linearcrfloglik.m linear crf log likelihood exercises exercise a stochastic matrix mij as non-negative entries and eigenvector e j mijej ei. by summing over i show that i mij consider an eigenvalue i ei then must exercise consider the markov chain with transition matrix be equal to m show that this markov chain does not have an equilibrium distribution and state a stationary distribution for this chain. exercise consider a hmm with states and output symbols with a left-to-right state transition matrix a b where aij pht iht j emission matrix bij pvt iht j and initial state probability vector a given the observed symbol sequence is compute compute find the most probable hidden state sequence arg draft march exercises exercise this exercise follows from given the long character string rgenmonleunosbpnntje vrancg typed with stubby fingers what is the most likely correct english sentence intended? in the list of decoded sequences what value is log for this sequence? you will need to modify demohmmbigram.m suitably. exercise show that if a transition probability aij pht iht j in a hmm is initialised to zero for em training then it will remain at zero throughout training. exercise consider the problem find the most likely joint output sequence for a hmm. that is v argmax where pvthtphtht explain why a local message passing algorithm cannot in general be found for this problem and discuss the computational complexity of finding an exact solution. explain how to adapt the expectation-maximisation algorithm to form a recursive algorithm for explain which it guarantees an improved solution at each iteration. finding an approximate v additionally explain how the algorithm can be implemented using local message passing. exercise explain how to train a hmm using em but with a constrained transition matrix. particular explain how to learn a transition matrix with an upper triangular structure. in exercise write a program to fit a mixture of lth order markov models. exercise using the correspondence a c g t define a transition matrix p that produces sequences of the form a c g t a c g t a c g t a c g t now define a new transition matrix pnew define a transition matrix q that produces sequences of the form t g c a t g c a t g c a t g c a now define a new transition matrix qnew assume that the probability of being in the initial state of the markov chain is constant for all four states a c g t what is the probability that the markov chain pnew generated the sequence s given by s a a g t a c t t a c c t a c g c similarly what is the probability that s was generated by qnew? does it make sense that s has a higher likelihood under pnew compared with qnew? draft march using the function randgen.m generate sequences of length from the markov chain defined by pnew. similarly generate sequences each of length from the markov chain defined by qnew. concatenate all these sequences into a cell array v so that contains the first sequence and the last sequence. use mixmarkov.m to learn the optimum maximum likelihood parameters that generated these sequences. assume that there are h kinds of markov chain. the result returned in phgvn indicate the posterior probability of sequence assignment. do you agree with the solution found? exercises take the sequence s as defined in equation define an emission distribution that has output states such that pv ih j i j i j using this emission distribution and the transition given by pnew defined in equation adapt demohmminferencesimple.m suitably to find the most likely hidden sequence hp that generated the observed sequence s. repeat this computation but for the transition qnew to give hq which hidden sequence hp is to be preferred? justify your answer. or hq exercise derive an algorithm that will find the most likely joint state argmax tht ht for arbitrarily defined potentials tht ht. first consider max tht ht t t derive the recursion t tht max ht tht ht t explain how the above recursion enables the computation of argmax tht ht show that how the maximisation over ht may be pushed inside the product and that the result of the maximisation can be interpreted as a message explain how once the most likely state for is computed one may efficiently compute the remaining optimal states ht exercise derive an algorithm that will compute pairwise marginals pht ht from the joint distribution tht ht for arbitrarily defined potentials tht ht. draft march exercises first tht ht show that how the summation over may be pushed inside the product and that the result of the maximisation can be interpreted as a message derive the recursion tht ht t t similarly show that one can push the summation of ht inside the product to define ht t tht t t t ht pht ht t ht and that by pushing in ht etc. one can define messages show that t t ht t exercise a second order hmm is defined as phm m phtht ht following a similar approach to the first order hmm derive explicitly a message passing algorithm to compute the most likely joint state argmax phm m exercise since the likelihood of the hmm can be computed using filtering only in principle we do not need smoothing to maximise the likelihood to the em approach. explain how to compute the likelihood gradient by the use of filtered information alone using only a forward pass. exercise derive the em updates for fitting a hmm with an emission distribution given by a mixture of multi-variate gaussians. exercise consider the hmm defined on hidden variables h ht and observations v vt pvh phtht show that the posterior phv is a markov chain phv phtht where phtht and are suitably defined distributions. draft march exercise for training a hmm with a gaussian mixture emission hmm-gmm model in derive the following em update formulae for the means and covariances new kh new kh kht n n and where kht nvn t kht qkt khn t qkt khn t kh t kh t hqhn t hqhn t hvn t hvn exercises exercise consider the hmm duration model defined by equation and equation with emission distribution pvtht. our interest is to derive a recursion for the filtered distribution tht ct pht ct show that tht ct pvtht ht phtht ctpctct t ct using this derive ht tht ct pvtht phtht cpctct t ct phtht c ht ct pcct t ct show that the right hand side of the above can be written as ht phtht ct cpct cct t i ht phtht c t c show that the recursion for is then given by th pvtht ht ptranhht t i pvtht ht ptranhtht t and for c th c pvtht hpdurc t i dmax t c explain why the computational complexity of filtered inference in the duration model is h derive an efficient smoothing algorithm for this duration model. draft march chapter continuous-state markov models observed linear dynamical systems in many practical timeseries applications the data is naturally continuous particularly for models of the physical environment. in contrast to discrete-state markov models continuous state distributions are not automatically closed under operations such as products and marginalisation. to make practical algorithms for which inference and learning can be carried efficiently we therefore are heavily restricted in the form of the continuous transition pvtvt a simple yet powerful class of such transitions are the linear dynamical systems. a deterministic observed linear dynamical defines the temporal evolution of a vector vt according to the discrete-time update equation vt atvt where at is the transition matrix at time t. for the case that at is invariant with t the process is called time-invariant which we assume throughout unless explicitly stated otherwise. a motivation for studying oldss is that many equations that describe the physical world can be written as an olds. oldss are interesting since they may be used as simple prediction models if vt describes the state of the environment at time t then avt predicts the environment at time as such these models have widespread application in many branches of science from engineering and physics to economics. the olds equation is deterministic so that if we specify all future values are defined. for a dim v v dimensional vector its evolution is described by vt at p t where diag v is the diagonal eigenvalue matrix and p is the corresponding eigenvector matrix of a. if i then for large t vt will explode. on the other hand if i then t i will tend to zero. for stable systems we require therefore no eigenvalues of magnitude greater than and only unit eigenvalues will contribute in long term. note that the eigenvalues may be complex which corresponds to rotational behaviour see more generally we may consider additive noise on v and define a stochastic olds. definition linear dynamical system. vt atvt t use the terminology observed lds to differentiate from the more general lds state-space model. in some texts however the term lds is applied to the models under discussion in this chapter. where t is a noise vector sampled from a gaussian distribution n t t t this is equivalent to a first order markov model pvtvt n atvt t t auto-regressive models at t we have an initial distribution n for t if the parameters are timeindependent t at a t the process is called time-invariant. stationary distribution with noise consider the one-dimensional linear system t v vt avt t t n if we start at some state and then for t recursively sample according to vt avt t does the distribution of the vt t tend to a steady fixed distribution? assuming that we can represent the distribution of vt as a gaussian with mean t and variance using we have t vt n t then t t t t a t a t t t v t v t vt n so that assuming there is a fixed variance for the infinite time case the stationary distribution satisfies v v similarly the mean is given by a if a the variance mean increases indefinitely with t. for a the mean tends to zero yet the variance remains finite. even though the magnitude of vt is decreased by a factor of a at each iteration the additive noise on average boosts the magnitude so that it remains steady in the long run. more generally for a system updating a vector vt according to vt avt t for the existence of a steady state we require that all eigenvalues of a must be auto-regressive models a scalar time-invariant auto-regressive model is defined by vt alvt l t t n t where a alt are called the ar coefficients and is called the innovation noise. the model predicts the future based on a linear combination of the previous l observations. as a belief network the ar model can be written as an lth order markov model pvtvt vt l with vi for i draft march auto-regressive models figure fitting an order ar model to the training points. the x axis represents time and the y axis the value of the timeseries. the solid line is the mean prediction and the dashed lines one standard deviation. see demoartrain.m with pvtvt vt l n vt alvt l introducing the vector of the l previous observations vt vt vt lt we can write more compactly pvtvt vt l n vt at vt ar models are heavily used in financial time-series prediction for example being able to capture simple trends in the data. another common application area is in speech processing whereby for a onedimensional speech signal partitioned into windows of length t the ar coefficients best able to describe the signal in each window are these ar coefficients then form a compressed representation of the signal and subsequently transmitted for each window rather than the original signal itself. the signal can then be approximately reconstructed based on the ar coefficients. such a representation is used for example in telephones and known as a linear predictive training an ar model maximum likelihood training of the ar coefficients is straightforward based on log vt vt differentiating w.r.t. a and equating to zero we arrive at log pvt vt t t t t t vt vt vt so that optimally t a vt vt t vt vt t vt vt t these equations can be solved by gaussian elimination. similarly optimally above we assume that negative timepoints are available in order to keep the notation simple. if times before the window over which we learn the coefficients are not available a minor adjustment is required to start the summations from t l given a trained a future predictions can be made using vt capturing the trend in the data. t a. as we see the model is capable of draft march auto-regressive models figure a time-varying ar model as a latent lds. since the observations are known this model is a time-varying latent lds for which smoothed inference determines the time-varying ar coefficients. ar model as an olds we can write equation as an olds using vt vt vt vt vt vt l t al we can write equation as the olds t n t vt a vt t where we define the block matrices al i a in this representation the first component of the vector is updated according to the standard ar model with the remaining components being copies of the previous values. time-varying ar model an alternative to maximum likelihood is to view learning the ar coefficients as a problem in inference in a latent lds a model which is discussed in detail in if at are the latent ar coefficients the term vt vt t t t n can be viewed as the emission distribution of a latent lds in which the hidden variable is at and the time-dependent emission matrix is given by vt t by placing a simple latent transition t a t we encourage the ar coefficients to change slowly with time. this defines a model a t n at at a t pvtat vt t our interest is then in the conditional from which we can compute the a-posteriori most likely sequence of ar coefficients. standard smoothing algorithms can then be applied to yield the time-varying ar coefficients see demoarlds.m. n definition fourier transform. for a sequence the dft is defined as fk i n kn xne k n fk is a representation as to how much frequency k is present in the sequence the power of component k is defined as the absolute length of the complex fk. draft march auto-regressive models spectrogram of up to hz. figure the raw recording of seconds of a nightingale song additional background clustering of the results in panel using birdsong. an component gaussian mixture model. the index to of the component most probably responsible for the observation is indicated vertically in black. the ar coefficients learned using clustering the results in panel using a gaussian mixture v model with components. the ar components group roughly according to the different song regimes. h see arlds.m. definition given a timeseries the spectrogram at time t is a representation of the frequencies present in a window localised around t. for each window one computes the discrete fourier transform from which we obtain a vector of log power in each frequency. the window is then moved one step forward and the dft recomputed. note that by taking the logarithm small values in the original signal can translate to visibly appreciable values in the spectrogram. example in we plot the raw acoustic recording for a second fragment of a nightingale song the spectrogram is also plotted and gives an indication of which frequencies are present in the signal as a function of time. the nightingale song is very complicated but at least locally can be very repetitive. a crude way to find which segments repeat is to form a cluster analysis of the spectrogram. in we show the results of fitting a gaussian mixture model with components from which we see there is some repetition of components locally in time. an alternative representation of the signal is given by the time-varying ar coefficients as plotted in a gmm clustering with components draft march latent linear dynamical systems figure a lds. both hidden and visible variables are gaussian distributed. in this case produces a somewhat clearer depiction of the different phases of the nightingale singing than that afforded by the spectrogram. latent linear dynamical systems the latent lds defines a stochastic linear dynamical system in a latent hidden space on a sequence of vectors each observation vt is as linear function of the latent vector ht. this model is also called a linear gaussian state space model the model can also be considered a form of lds on the joint variables xt ht with parts of the vector xt missing. for this reason we will also refer to this model as a linear dynamical system the latent prefix. t are noise vectors. at is called the transition matrix and bt the emission matrix. the where h terms ht and vt are the hidden and output bias respectively. the transition and emission models define a first order markov model definition linear dynamical system. h v ht h t t t vt v t transition model emission model ht atht h vt btht v t t and v t h t n v t n atht ht h btht vt v t t phtht with the transitions and emissions given by gaussian distributions phtht n pvtht n n this lds can be represented as a belief networkin with the extension to higher orders being intuitive. one may also include an external input ot at each time which will add cot to the mean of the hidden variable and dot to the mean of the observation. h explicit expressions for the transition and emission distributions are given below for the time-invariant case with vt ht each hidden variable is a multidimensional gaussian distributed vector ht with phtht exp aht h aht which states that has a mean equal to aht with gaussian fluctuations described by the covariance matrix h. similarly pvtht v exp bhtt v bht describes an output vt with mean bht and covariance v models are also often called kalman filters. we avoid this terminology here since the word filter refers to a specific kind of inference and runs the risk of confusing a filtering algorithm with the model itself. draft march inference figure a single phasor plotted as a damped two dimensional rotation r ht with a damping factor by taking a projection onto the y axis the phasor generates a damped sinusoid. example consider a dynamical system defined on two dimensional vectors ht cos sin cos sin r ht with r r rotates the vector ht through angle in one timestep. under this lds h will trace out points on a circle through time. by taking a scalar projection of ht for example vt the elements vt t t describe a sinusoid through time see by using a block diagonal r blkdiag r m and taking a scalar projection of the extended m dimensional h vector one can construct a representation of a signal in terms of m sinusoidal components. inference given an observation sequence we wish to consider filtering and smoothing as we did for the hmm for the hmm in deriving the various message passing recursions we used only the independence structure encoded by the belief network. since the lds has the same independence structure as the hmm we can use the same independence assumptions in deriving the updates for the lds. however in implementing them we need to deal with the issue that we now have continuous hidden variables rather than discrete states. the fact that the distributions are gaussian means that we can deal with continuous messages exactly. in translating the hmm message passing equations we first replace summation with integration. for example the filtering recursion becomes ht pvthtphtht t since the product of two gaussians is another gaussian and the integral of a gaussian is another gaussian the resulting is also gaussian. this closure property of gaussians means that we may represent pht n ft ft with mean ft and covariance ft the effect of equation is equivalent to updating the mean ft and covariance ft into a mean ft and covariance ft for our task below is to find explicit algebraic formulae for these updates. numerical stability translating the message passing inference techniques we developed for the hmm into the lds is largely straightforward. indeed one could simply run a standard sum-product algorithm for continuous variables see demosumprodgausscanonlds.m. in long timeseries numerical instabilities can build up and may result in grossly inaccurate results depending on the transition and emission distribution parameters and the method of implementing the message updates. for this reason specialised routines have been developed that are reasonably numerically stable under certain parameter for the hmm in we discussed two alternative methods for smoothing the parallel approach and the sequential approach. the recursion is suitable when the emission and transition covariance entries are small and the recursion usually preferable in the more standard case of small covariance values. draft march inference analytical shortcuts in deriving the inference recursions we need to frequently multiply and integrate gaussians. whilst in principle straightforward this can be algebraically tedious and wherever possible it is useful to appeal to known shortcuts. for example one can exploit the general result that the linear transform of a gaussian random variable is another gaussian random variable. similarly it is convenient to make use of the conditioning formulae as well as the dynamics reversal intuition. these results are stated in and below we derive the most useful for our purposes here. consider a linear transformation of a gaussian random variable x n x x n y mx where x and are assumed to be generated from independent processes. to find the distribution py one approach would be to write this formally as py n mx x x dx and carry out the integral completing the square. however since a gaussian variable under linear transformation is another gaussian we can take a shortcut and just find the mean and covariance of the transformed variable. its mean is given by m x to find the covariance consider the displacement of a variable h from its mean which we write as the covariance is by h for y the displacement is h h y m x y so that the covariance is x x x x since the noises and x are assumed we have mt m mt m y m xmt filtering we represent the filtered distribution as a gaussian with mean ft and covariance ft n ft ft this is called the moment representation. our task is then to find a recursion for ft ft in terms of ft ft a convenient approach is to first find the joint distribution pht and then condition on vt to find the distribution the term pht is a gaussian whose statistics can be found from the relations vt bht v t ht aht h t using the above and assuming time-invariance and zero biases we readily find ht ht t a ht ht t at h aft h draft march inference parameters t b h v h algorithm lds forward pass. compute the filtered posteriors n ft for a lds with t. the log-likelihood l log is also returned. t l log for t t do ft pt ldsforwardft ft vt l l log pt end for function ldsforwardf f v mean of pht covariance of pht find by conditioning compute bt v aft h vv vh vh vv v h af h v b h v hh afat h vv b hhbt v vh b hh vh vv v h t vt exp return end function vv hh t in the above using our moment representation of the forward messages vt vt ht ht t t t t aft h bt v b ht ht vt ht b ft then using phtvt will have mean t t ft t and covariance ht ht ht vt vt vt ft b b ht vt vt vt t vt ht t writing out the above explicitly we have for the mean bpbt v t baft bpbt v bp ft ht ht t ft aft ft p h and covariance where p aft h the filtering procedure is presented in with a single update in ldsforwardupdate.m. one can write the covariance update as where we define the kalman gain matrix ft kb p k v we present in algorithm?? the recursion in standard engineering notation. see also ldssmooth.m. the iteration is expected to be numerically stable when the noise covariances are small. y and covariance xx xy yy yx. is a gaussian with mean x xy yy draft march symmetrising the updates inference a potential numerical issue with the covariance update is that it is the difference of two positive definite matrices. if there are numerical errors the ft may not be positive definite nor symmetric. using the woodbury identity equation can be written more compactly as ft p bt v b whilst this is positive semidefinite this is numerically expensive since it involves two matrix inversions. an alternative is to use the definition of k from which we can write k v kt kb pbtkt hence we arrive at joseph s symmetrized p kbt kb p kbt k v kt kb kb p the left hand side is the addition of two positive definite matrices so that the resulting update for the covariance is more numerically stable. a similar method can be used in the backward pass below. an alternative is to avoid using covariance matrices directly and use their square root as the parameter deriving updates for these smoothing rauch-tung-striebel correction method the smoothed posterior is necessarily gaussian since it is the conditional marginal of a larger gaussian. by representing the posterior as a gaussian with mean gt and covariance gt n gt gt we can form a recursion for gt and gt as follows pht the term can be found by conditioning the joint distribution pht which is obtained in the usual manner by finding its mean and covariance the term is a known gaussian from filtering with mean ft and covariance ft. hence the joint distribution pht has means aft and covariance elements ft ht ht t ftat ft ht ht ht aftat h to find we may use the conditioned gaussian results it is useful to use the system reversal result which interprets as an equivalent linear system going backwards in time ht mt t where at ht ht ht draft march inference algorithm lds backward pass. compute the smoothed posteriors this requires the filtered results from gt ft ft gt ft gt ft for t t do end for function ldsbackwardg g f f afat h a t ag at h af h f t ag m return mt t t end function ht ht with and t n t ht af m f a h statistics of pht dynamics reversal backward propagation ht ht t ht ht ht ht t using dynamics reversal equation and assuming that is gaussian distributed it is then straightforward to work out the statistics of the mean is given by and covariance gt at mt mt at t t at ht ht ht t gt at t t this procedure is the rauch-tung-striebel kalman this is called a correction method since it takes the filtered estimate and corrects it to form a smoothed estimate the procedure is outlined in and is detailed in ldsbackwardupdate.m. see also ldssmooth.m. the cross moment an advantage of the dynamics reversal interpretation given above is that the cross moment is required for learning is immediately obtained from gtgt htht ht the likelihood we can compute the likelihood using the decomposition in which each conditional is a gaussian in vt. it is straightforward to show that the term has mean and covariance b t baft t h tt b bt v log t the log likelihood is then given by bt v t t t log det t draft march most likely state since the mode of a gaussian is equal to its mean there is no difference between the most probable joint posterior state inference argmax and the set of most probable marginal states ht argmax htt t t hence the most likely hidden state sequence is equivalent to the smoothed mean sequence. time independence and riccati equations both the filtered ft and smoothed gt covariance recursions are independent of the observations depending only on the parameters of the model. this is a general characteristic of linear gaussian systems. typically the covariance recursions converge quickly to values that are reasonably constant throughout the dynamics with only appreciable differences at the boundaries t and t t in practice one often drops the time-dependence of the covariances and approximates them with a single time-independent covariance. this approximation dramatically reduces storage requirements. the converged filtered f satisfies the recursion afat h b afat h bt v b afat h f afat h which can be related to a form of algebraic riccati equation. a technique to solve these equations is to being with setting the covariance to with this a new f is found using the right hand side of and subsequently recursively updated. alternatively using the woodbury identity the converged covariance satisfies f afat h bt v b although this form is less numerically convenient in forming an iterative solver for f since it requires two matrix inversions. example trajectory analysis. a toy rocket with unknown mass and initial velocity is launched in the air. in addition the constant accelerations from the rocket s propulsion system are unknown. it is known is that newton s laws apply and an instrument can measure the vertical height and horizontal distance of the rocket at each time xt yt from the origin. based on noisy measurements of xt and yt our task is to infer the position of the rocket at each time. although this is perhaps most appropriately considered from the using continuous time dynamics we will translate this into a discrete time approximation. newton s law states that x fxt m y fyt m where m is the mass of the object and fxt fyt are the horizontal and vertical forces respectively. hence as they stand these equations are not in a form directly usable in the lds framework. a naive approach is to reparameterise time to use the variable t such that t t where t is integer and is a unit of time. the dynamics is then x t x t x y t y t y t t draft march learning linear dynamical systems figure estimate of the trajectory of a newtonian ballistic object based on noisy observations circles. all time labels are known but omitted in the plot. the x points are the true positions of the object and the crosses are the estimated smoothed mean positions of the object plotted every several time steps. see demoldstracking.m where dy t x x dt we can write an update equation for the and as t fy t fx t y y these are discrete time difference equations indexed by t. the instrument which measures xt and yt is not completely accurate. for simplicity we relabel axt fxtmt ayt fytmt these accelerations will be assumed to be roughly constant but unknown ax t ax t x ay t ay t y where x and y are small noise terms. the initial distributions for the accelerations are assumed vague using a zero mean gaussian with large variance. we describe the above model by considering xt yt axt ayt as hidden variables giving rise to a h dimensional lds with transition and emission matrices as below b a we place a large variance on their initial values and attempt to infer the unknown trajectory. a demonstration is given in despite the significant observation noise the object trajectory can be accurately inferred. learning linear dynamical systems whilst in many applications particularly of underlying known physical processes the paremeters of the lds are known in many machine learning tasks we need to learn the parameters of the lds based on for simplicity we assume that we know the dimensionality h of the lds. identifiability issues an interesting question is whether we can uniquely identify the parameters of an lds. there are always trivial redundancies in the solution obtained by permuting the hidden variables arbitrarily and flipping their signs. to show that there are potentially many more equivalent solutions consider the following lds vt bht v t ht aht h t draft march we now attempt to transform this original system to a new form which will produce exactly the same outputs for an invertible matrix r we consider learning linear dynamical systems rht rar r h t which is representable as a new latent dynamics ht a ht h t where a rar ht rht h the transformed h t r h t in addition we can reexpress the outputs to be a function of vt br v t b ht v t hence provided we place no constraints on a b and h there exists an infinite space of equivalent solutions a ra r b br h r hrt all with the same likelihood value. this means that directly interpreting the learned parameters needs to be done with some care. this redundancy can be mitigated by imposing constraints on the parameters. em algorithm for simplicity we assume we have a single sequence to which we wish to fit a lds using maximum likelihood. since the lds contains latent variables one approach is to use the em algorithm. as usual the m-step of the em algorithm requires us to maximise the energy with respect to the parameters a b a v h. thanks to the form of the lds the energy decomposes as phtht it is straightforward to derive that the m-step for the parameters is given by brackets denote expectation with respect to the smoothed posterior htht t t vt bt vt at a new v t b htht vtvt t t t htht t a t t new h anew t new t new vt bnew new v t t vtvt t t new h t htht t htht t t vt a htht if b is updated according to the above the first equation can be simplified to similarly if a is updated according to em algorithm then the second equation can be simplified to draft march learning linear dynamical systems the statistics required therefore include smoothed means covariances and cross moments. the extension to learning multiple timeseries is straightforward since the energy is simply summed over the individual sequences. the performance of the em algorithm for the lds often depends heavily on a the initialisation. if we remove the hidden to hidden links the model is closely related to factor analysis lds can be considered a temporal extension of factor analysis. one initialisation technique is therefore to learn the b matrix using factor analysis by treating the observations as temporally independent. subspace methods an alternative to em and maximum likelihood training of an lds is to use a subspace the chief benefit of these techniques is that they avoid the convergence difficulties of em. to motivate subspace techniques consider a deterministic lds vt bht ht aht under this assumption vt bht baht and more generally vt this means that a low dimensional system underlies all visible information since all points lie in a h-dimensional subspace which is then projected to form the observation. this suggests that some form of subspace identification technique will enable us to learn a and b. given a set of observation vectors vt consider the block hankel matrix formed from stacking the vectors. for an order l matrix this is a v l t l matrix. for example for t and l this is b ba m m w we now find the svd of m m u s vt if the v are generated from a free lds we can write where w is termed the extended observability matrix. the matrix s will contain the singular values up to the dimension of the hidden variables h with the remaining singular values from equation this means that the emission matrix b is contained in the estimated hidden variables are then contained in the submatrix based on the relation ht aht one can then find the best least squares estimate for a by minimising aht for which the optimal solution is ht a ht where denotes the pseudo inverse see ldssubspace.m. estimates for the covariance matrices can also be obtained from the residual errors in fitting the block hankel matrix v and extended observability matrix h. whilst this derivation formally holds only for the noise free case one can nevertheless apply this in the case of non-zero noise and hope to gain an estimate for a and b that is correct in the mean. in addition to forming a solution in its own right the subspace method forms a potentially useful way to initialise the em algorithm. draft march switching auto-regressive models figure a first order switching ar model. in terms of inference conditioned on this is a hmm. structured ldss many physical equations are local both in time and space. for example in weather models the atmosphere is partitioned into cells hit each containing the pressure at that location. the equations describing how the pressure updates only depend on the pressure at the current cell and small number of neighbouring cells at the previous time t if we use a linear model and measure some aspects of the cells at each time then the weather is describable by a lds with a highly structured sparse transition matrix a. in practice the weather models are non-linear but local linear approximations are often a similar situation arises in brain imaging in which voxels cubes of activity depend only on their neighbours from the previous another application of structured ldss is in temporal independent component analysis. this is defined as the discovery of a set of independent latent dynamical processes from which the data is a projected observation. if each independent dynamical process can itself be described by a lds this gives rise to a structured lds with a block diagonal transition matrix a. such models can be used to extract independent components under prior knowledge of the likely underlying frequencies in each of the temporal see also bayesian ldss the extension to placing priors on the transition and emission parameters of the lds leads in general to computational difficulties in computing the likelihood. for example for a prior on a the likelihood is a which is difficult to evaluate since the dependence of the likelihood on the matrix a is a complicated function. approximate treatments of this case are beyond the scope of this book although we briefly note that sampling are popular in this context in addition to deterministic variational switching auto-regressive models for a time-series of scalar values an lth order switching ar model can be written as vt vt t t t where we now have a set of ar coefficients s s themselves have a markov transition pvtvt vt l st t n t t pstst so that the full model is the discrete switch variables given an observed sequence and parameters inference is straightforward since this is a form of hmm. to make this more apparent we may write inference t where pvtstpstst pvtst pvtvt vt l st n vt vt t draft march switching auto-regressive models figure learning a switching ar model. the upper plot shows the training data. the colour indicates which of the two ar models is active at that time. whilst this information is plotted here this is assumed unknown to the learning algorithm as are the coefficients as. we assume that the order l and number of switches s however is known. in the bottom plot we show the time series again after training in which we colour the points according to the most likely smoothed ar model at each timestep. see demosarlearn.m. note that the emission distribution pvtst is time-dependent. the filtering recursion is then pvtstpstst st smoothing can be achieved using the standard recursions modified to use the time-dependent emissions see demosarinference.m. maximum likelihood learning using em to fit the set of ar coefficients and innovation variances as s s using maximum likelihood training for a set of data we may make use of the em algorithm. which we need to maximise with respect to the parameters using the definition of the emission and isolating the dependency on a we have t pstst m-step up to negligible constants the energy is given by t pvt vt vt vt t e t log const. on differentiating with respect to as and equating to zero the optimal as satisfies the linear equation which may be solved using gaussian elimination. similarly one may show that updates that maximise the energy with respect to are poldst vt vt t t t vt vt t poldst vt vt poldst t t as the update for pstst follows the standard em for hmm rule equation see sarlearn.m. here we don t include an update for the prior since there is insufficient information at the start of the sequence and assume is flat. with high frequency data it is unlikely that a change in the switch variable is reasonable at each time t. a simple constraint to account for this is to use a modified transition pstst pstst st otherwise mod tskip draft march switches code figure a latent switching order ar model. here the st indicates which of a set of available ar models is active at time t. the square nodes emphasise that these are discrete variables. the clean ar signal vt which is not observed is corrupted by additive noise to form the noisy observations vt. in terms of inference conditioned on this can be expressed as a switching lds signal reconstruction using the latent switching ar model in top noisy signal bottom reconstructed clean signal the dashed lines and the numbers show the most-likely state segmentation arg e-step the m-step requires the smoothed statistics poldst and poldst s st obtained from hmm inference. which can be example a switching ar model. in the training data is generated by an switching ar model so that we know the ground truth as to which model generated which parts of the data. based on the training data the labels st are unknown a switching ar model is fitted using em. in this case the problem is straightforward so that a good estimate is obtained of both the sets of ar parameters and which switches were used at which time. example parts of speech. in a segment of a speech signal is shown described by a switching ar model. each of the available ar models is responsible for modelling the dynamics of a basic subunit of the model was trained on many example sequences using s states with a left-to-right transition matrix. the interest is to determine when each subunit is most likely to be active. this corresponds to the computation of the most-likely switch path given the observed signal code in the linear dynamical system code below only the simplest form of the recursions is given. no attempt has been made to ensure numerical stability. ldsforwardupdate.m lds forward ldsbackwardupdate.m lds backward ldssmooth.m linear dynamical system filtering and smoothing ldsforward.m alternative lds forward algorithm slds chapter ldsbackward.m alternative lds backward algorithm slds chapter demosumprodgausscanonlds.m sum-product algorithm for smoothed inference demoldstracking.m demo of tracking in a newtonian system draft march exercises ldssubspace.m subspace learning matrix method demoldssubspace.m demo of subspace learning method autoregressive models note that in the code the autoregressive vector a has as its last entry the first ar coefficient reverse order to that presented in the text. artrain.m learn ar coefficients elimination demoartrain.m demo of fitting an ar model to data arlds.m learn ar coefficients using a lds demoarlds.m demo of learning ar coefficients using an lds demosarinference.m demo for inference in a switching autoregressive model in in sarlearn.m a slight fudge is used since we do not deal fully with the case at the start where there is insufficient information to define the ar model. for long timeseries this will have a negligible effect although it might lead to small decreases in the log likelihood under the em algorithm. sarlearn.m learning of a sar using em demosarlearn.m demo of sar learning hmmforwardsar.m switching autoregressive hmm forward pass hmmbackwardsar.m switching autoregressive hmm backward pass exercises exercise consider the two-dimension linear model ht r ht where r r is rotation matrix which rotates the vector ht through angle in one timestep. sin cos sin cos by xt xt yt yt eliminate yt to write an equation for in terms of xt and xt explain why the eigenvalues of a rotation matrix are general imaginary. explain how to model a sinusoid rotating with angular velocity using a two-dimensional lds. explain how to model a sinusoid using an ar model. explain the relationship between the second order differential equation x x which describes a harmonic oscillator and the second order difference equation which approximates this differential equation. is it possible to find a difference equation which exactly matches the solution of the differential equation at chosen points? exercise show that for any anti-symmetric matrix m m mt the matrix exponential matlab this is expm a em draft march is orthogonal namely ata i exercises explain how one may then construct random orthogonal matrices with some control over the angles of the complex eigenvalues. discuss how this relates to the frequencies encountered in a lds where a is the transition matrix. exercise run the demo demoldstracking.m which tracks a ballistic object using a linear dynamical system see modify demoldstracking.m so that in addition to the x and y positions the x speed is also observed. compare and contrast the accuracy of the tracking with and without this extra information. exercise nightsong.mat contains a small stereo segment nightingale song sampled at hertz. plot the original waveform using download the program myspecgram.m from labrosa.ee.columbia.edumatlabsgrammyspecgram.m and plot the spectrogram imagesclogabsy the routine demogmmem.m demonstrates fitting a mixture of gaussians to data. the mixture assignment probabilities are contained in phgn. write a routine to cluster the data vlogabsy using gaussian components and explain how one might segment the series x into different regions. examine the routine demoarlds.m which fits autoregressive coefficients using an interpretation as a linear dynamical system. adapt the routine demoarlds.m to learn the ar coefficients of the data x. you will almost certainly need to subsample the data x for example by taking every datapoint. with the learned ar coefficients the smoothed results fit a gaussian mixture with components. compare and contrast your results with those obtained from the gaussian mixture model fit to the spectrogram. exercise consider a supervised learning problem in which we make a linear model of the scaler output yt based on vector input xt t where y t xt y yt wt t is zero mean gaussian noise. training data d yt t t is available. for a time-invariant weight vector wt w explain how to find the single weight vector w and the noise variance by maximum likelihood. extend the above model to include a transition wt wt w t where w is zero mean gaussian noise with a given covariance has zero mean. explain t how to cast finding as smoothing in a related linear dynamical system. write a routine w linpredarxysigmawsigmay that takes an input data matrix x xt where each column contains an input and vector y yt sigmaw is the additive weight noise and sigmay is an assumed known time-invariant output noise. the returned w contains the smoothed mean weights. draft march chapter switching linear dynamical systems introduction complex timeseries which are not well described globally by a single linear dynamical system may be divided into segments each modelled by a potentially different lds. such models can handle situations in which the underlying model jumps from one parameter setting to another. for example a single lds might well represent the normal flows in a chemical plant. when a break in a pipeline occurs the dynamics of the system changes from one set of linear flow equations to another. this scenario can be modelled suing a sets of two linear systems each with different parameters. the discrete latent variable at each time st pipe broken indicates which of the ldss is most appropriate at the current time. this is called a switching lds and used in many disciplines from econometrics to machine learning the switching lds at each time t a switch variable st s describes which of a set of ldss is to be used. the observation visible variable vt rv is linearly related to the hidden state ht rh by vt bstht vst vst n vst vst vst here st describes which of the set of emission matrices bs is active at time t. the observation noise vst is drawn from a gaussian with mean vst and covariance vst. the transition dynamics of the continuous hidden state ht is linear ht astht hst hst n hst hst hst and the switch variable st selects a single transition matrix from the available set as. the gaussian transition noise hst also depends on the switch variable. the dynamics of st itself is markovian with transition pstst for the more general augmented aslds model the switch st is dependent on both the previous st and ht the model defines a joint distribution pvtht stphtht stpstht st with pvtht st n vst bstht vst phtht st n ht hst astht hst gaussian sum filtering figure the independence structure of the aslds. square nodes st denote discrete switch variables ht are continuous latenthidden variables and vt continuous observedvisible variables. the discrete state st determines which linear dynamical system from a finite set of linear dynamical systems is operational at time t. in the slds links from h to s are not normally considered. at time t denotes the prior and denotes the slds can be thought of as a marriage between a hidden markov model and a linear dynamical system. the slds is also called a jump markov modelprocess switching kalman filter switching linear gaussian state space model conditional linear gaussian model. exact inference is computationally intractable both exact filtered and smoothed inference in the slds is intractable scaling exponentially with time. as an informal explanation consider filtered posterior inference for which by analogy with equation the forward pass is ht st ht at timestep is an indexed set of gaussians. at timestep due to the summation over the states will be an indexed set of s gaussians similarly at timestep it will be and in general gives rise to st gaussians at time t. even for small t the number of components required to exactly represent the filtered distribution is therefore computationally intractable. analogously smoothing is also intractable. the origin of the intractability of the slds differs from structural intractability that we ve previously encountered. in the slds in terms of the cluster variables with xt ht and visible variables the graph of the distribution is singly-connected. from a purely graph theoretic viewpoint one would therefore envisage little difficulty in carrying out inference. indeed as we saw above the derivation of the filtering algorithm is straightforward since the graph is singly-connected. however the numerical implementation of the algorithm is intractable since the description of the messages requires an exponentially increasing number of terms. in order to deal with this intractability several approximation schemes have been introduced here we focus on techniques which approximate the switch conditional posteriors using a limited mixture of gaussians. since the exact posterior distributions are mixtures of gaussians albeit with an exponentially large number of components the aim is to drop low weight components such that the resulting approximation accurately represents the posterior. gaussian sum filtering describes the exact filtering recursion with an exponentially increasing number of components with time. in general the influence of ancient observations will be much less relevant than that of recent observations. this suggests that the effective time is limited and that therefore a corresponding limited number of components in the gaussian mixture should suffice to accurately represent the filtered our aim is to form a recursion for pst based on a gaussian mixture approximation of phtst given an approximation of the filtered distribution pst qst the exact recursion equation is approximated by ht st ht draft march gaussian sum filtering this approximation to the filtered posterior at the next timestep will contain s times more components than in the previous timestep and to prevent an exponential explosion in mixture components we will need to subsequently collapse the mixture in a suitable way. we will deal with this issue once has been computed. to derive the updates it is useful to break the new filtered approximation from equation into continuous and discrete parts qht qhtst and derive separate filtered update formulae as described below. continuous filtering the exact representation of phtst is a mixture with components. to retain computational feasibility we therefore approximate this with a limited i-component mixture qhtst qhtit st where qhtit st is a gaussian parameterised with mean fit st and covariance fit st. strictly speaking we should use the notation ftit st since for each time t we have a set of means indexed by it st although we drop these dependencies in the notation used here. urally this gives rise to a mixture of gaussians for an important remark is that many techniques approximate phtst using a single gaussian. natst phtst however in making a single gaussian approximation to phtst the representation of the posterior may be poor. our aim here is to maintain an accurate approximation to phtst by using a mixture of gaussians. to find a recursion for the approximating distribution we first assume that we know the filtered approximation qht and then propagate this forwards using the exact dynamics. to do so consider first the relation stit stit st it wherever possible we now substitute the exact dynamics and evaluate each of the two factors above. the usefulness of decomposing the update in this way is that the new filtered approximation is of the form of a gaussian mixture where it is gaussian and qst are the weights or mixing proportions of the components. we describe below how to compute these terms explicitly. produces a new gaussian mixture with i s components which we will collapse back to i components at the end of the computation. evaluating it we aim to find a filtering recursion for it since this is conditional on switch states and components this corresponds to a single lds forward step which can be evaluated by considering first the joint distribution it ht st it and subsequently conditioning on in the above we used the exact dynamics where possible. states that we know the filtered information up to time t in addition to knowing the switch states st draft march gaussian sum filtering and the mixture component index it. to ease the burden on notation we derive this for ht vt for all t. the exact forward dynamics is then given by given the mixture component index it it st n fit st fit st we propagate this gaussian with the exact dynamics equation then it is a gaussian with covariance and mean elements hh vv vh hh t these results are obtained from integrating the forward dynamics equations over ht using the results in hv v st h st to find it we condition it on using the standard gaussian conditioning formulae to obtain it n hv hv with hv h hv vv v hv hh hv vv vh where the quantities required are defined in equation evaluating the mixture weights qst up to a normalisation constant the mixture weight in equation can be found from qst st st the first factor in equation st is gaussian with mean v and covariance vv as given in equation the last two factors qitst and are given from the previous filtered iteration. finally st is found from st augmented slds standard slds where the result above for the standard slds follows from the independence assumptions present in the standard slds. in the aslds the term in equation will generally need to be computed numerically. a simple approximation is to evaluate equation at the mean value of the distribution qhtit st to take covariance information into account an alternative would be to draw samples from the gaussian qhtit st and thus approximate the average of st by sampling. note that this does not equate gaussian sum filtering with a sequential sampling procedure such as particle filtering the sampling here is exact for which no convergence issues arise. closing the recursion we are now in a position to calculate equation for each setting of the variable we have a mixture of i s gaussians. to prevent the number of components increasing exponentially with time we numerically collapse back to i gaussians to form any method of choice may be supplied to collapse a mixture to a smaller mixture. a straightforward approach is to repeatedly merge low-weight components as explained in in this way the new mixture coefficients i are defined. this completes the description of how to form a recursion for the continuous filtered posterior approximation in equation draft march gaussian sum filtering figure gaussian sum filtering. the leftmost column depicts the previous gaussian mixture approximation qht for two states s and blue and three mixture components i with the mixture weight represented by the area of each oval. there are s different linear systems which take each of the components of the mixture into a new filtered state the colour of the arrow indicating which dynamic system is used. after one time-step each mixture component branches into a further s components so that the joint approximation contains components column. to keep the representation computationally tractable the mixture of gaussians for each state is collapsed back to i components. this means that each coloured state needs to be approximated by a smaller i component mixture of gaussians. there are many ways to achieve this. a naive but computationally efficient approach is to simply ignore the lowest weight components as depicted on the right column see discrete filtering a recursion for the switch variable distribution in equation is it st itst stit the r.h.s. of the above equation is proportional to it st st for which all terms have been computed during the recursion for we now have all the quantities required to compute the gaussian sum approximation of the filtering forward pass. a schematic representation of gaussian sum filtering is given in and the pseudo code is presented in see also sldsforward.m. the likelihood the likelihood may be found from t where st st in the above expression all terms have been computed in forming the recursion for the filtered posterior collapsing gaussians given a mixture of n gaussians px pin i i we wish to collapse this to a smaller k n mixture of gaussians. we describe a simple method which has the advantage of computational efficiency but the disadvantage that no spatial information about draft march gaussian sum smoothing algorithm aslds forward pass. approximate the filtered posterior t phtst it wtit stn ftit st ftit st. also return the approximate log-likelihood l log it are the number of components in each gaussian mixture approximation. we require s it s it as bs hs vs hs vs. for to s do p p end for for t to t do for st to s do for i to it and s to s do xyi s xyi s p ldsforwardft s ft s vt p s st i s wt sp s t p end for collapse the it s mixture of gaussians defined by xy xy and weights pi sst i s phtst pitst it this defines the new means ftit st covariances ftit st and mixture weights wtit st pitst compute tst l l is i s end for normalise t to a gaussian with it stis i s components end for the mixture is first we describe how to collapse a mixture to a single gaussian. this can be achieved by finding the mean and covariance of the mixture distribution these are pi i i i pi i i t i t to collapse a mixture then to a k-component mixture we may first retain the k gaussians with the largest mixture weights. the remaining n k gaussians are simply merged to a single gaussian using the above method. alternative heuristics such as recursively merging the two gaussians with the lowest mixture weights are also reasonable. more sophisticated methods which retain some spatial information would clearly be potentially useful. the method presented in is a suitable approach which considers removing gaussians which are spatially similar not just low-weight components thereby retaining a sense of diversity over the possible solutions. in applications with many thousands of timesteps speed can be a factor in determining which method of collapsing gaussians is to be preferred. relation to other methods gaussian sum filtering can be considered a form of analytical particle filtering in which instead of point distributions functions being propagated gaussians are propagated. the collapse operation to a smaller number of gaussians is analogous to resampling in particle filtering. since a gaussian is more expressive than a delta-function the gaussian sum filter is generally an improved approximation technique over using point particles. see for a numerical comparison. gaussian sum smoothing approximating the smoothed posterior pht is more involved than filtering and requires additional approximations. for this reason smoothing is more prone to failure since there are more assumptions that need to be satisfied for the approximations to hold. the route we take here is to assume that a gaussian draft march gaussian sum smoothing pht pst pst sum filtered approximation has been carried out and then approximate the backward pass analogous to that of by analogy with the rts smoothing recursion equation the exact backward pass for the slds reads pht where is composed of the discrete and continuous components of the smoothed posterior at the next time step. the recursion runs backwards in time beginning with the initialisation pht set by the filtered result time t t the filtered and smoothed posteriors coincide. apart from the fact that the number of mixture components will increase at each step computing the integral over in equation is problematic since the conditional distribution term is non-gaussian in for this reason it is more useful derive an approximate recursion by beginning with the exact relation which can be expressed more directly in terms of the slds dynamics as st in forming the recursion we assume access to the distribution from the future timestep. however we also require the distribution which is not directly known and needs to be inferred in itself a computationally challenging task. in the expectation correction approach one assumes the approximation resulting in an approximate recursion for the smoothed posterior pst st where represents averaging with respect to the distribution in carrying out the approximate recursion we will end up with a mixture of gaussians that grows at each timestep. to avoid the exponential explosion problem we use a finite mixture approximation and plug this into the approximate recursion above. from equation a recursion for the approximation is given by st qht as for filtering wherever possible we replace approximate terms by their exact counterparts and parameterise the posterior using to reduce the notational burden here we outline the method only for the case of using a single component approximation in both the forward and backward passes. the extension to using a mixture to approximate each is conceptually straightforward and deferred to in the single gaussian case we assume we have a gaussian approximation available for n draft march gaussian sum smoothing figure the ec backpass approximates st by the motivation for this is that st influences only indirectly through ht. however ht will most likely be heavily influenced by so that not knowing the state of st is likely to be of secondary importance. the green shaded node is the variable we wish to find the posterior for. the values of the blue shaded nodes are known and the red shaded node indicates a known variable which is assumed unknown in forming the approximation. st ht vt st ht vt continuous smoothing for given st an rts style recursion for the smoothed continuous is obtained from equation giving qhtst st to compute equation we then perform a single update of the lds backward recursion discrete smoothing the second average in equation corresponds to a recursion for the discrete variable and is given by the average of with respect to cannot be achieved in closed form. a simple approach is to approximate the average by evaluation at the where is the mean of with respect to replacing by its mean gives the approximation where e z zt and z ensures normalisation over st. is the filtered covariance of given st and the observations which may be taken from hh in equation approximations which take covariance information into account can also be considered although the above simple fast method may suffice in practice collapsing the mixture from and we now have all the terms in equation to compute the approximation to equation due to the summatino over in equation the number of mixture components is multiplied by s at each iteration. to prevent an exponential explosion of components the mixture equation is then collapsed to a single gaussian qht qhtst the collapse to a mixture is discussed in general this approximation has the form f draft march gaussian sum smoothing routine needs the results from algorithm aslds ec backward pass. approximates and phtst utjt stn st gtjt st using a mixture of gaussians. jt it jt s it this gt ft gt ft ut wt for t t to do for s to s s fti s fti s pit s it pi s to s i to it to do end for for st to s do collapse the mixture defined by weights pit i pi st means st and covariances st to a ture with jt components. this defines the new means gtjt st covariances gtjt st and mixture weights utjt st. pit st end for end for using mixtures in smoothing the extension to the mixture case is straightforward based on the representation pit st this mixture can then be collapsed to a smaller mixture using any method of choice to give phtst phtst jt qjtst the resulting procedure is sketched in including using mixtures in both the forward and backward passes. draft march phtst qht qjtst analogously to the case with a single component it st the average in the last line of the above equation can be tackled using the same techniques as outlined in the single gaussian case. to approximate it st we consider this as the marginal of the joint distribution qht st it st st as in the case of a single mixture the problematic term is st analogously to equation we make the assumption st meaning that information about the current switch state st it is ignored. we can then form relation to other methods gaussian sum smoothing a classical smoothing approximation for the slds is generalised pseudo bayes in gpb one starts from the exact recursion pst the quantity is difficult to obtain and gpb makes the approximation plugging this into equation we have st the recursion is initialised with the approximate filtered computing the smoothed recursion for the switch states in gpb is then equivalent to running the rts backward pass on a hidden markov model independently of the backward recursion for the continuous variables. the only information the gpb method uses to form the smoothed distribution from the filtered distribution is the markov switch transition this approximation drops information from the future since information passed via the continuous variables is not taken into account. in contrast to gpb the ec gaussian smoothing technique preserves future information passing through the continuous variables. as for ec gpb forms an approximation for phtst by using the recursion where is given by in sldsbackward.m one may choose to use either ec or gbp. example flow. a illustration of modelling and inference with a slds is to consider a simple network of traffic flow here there are junctions a b c d and traffic flows along the roads in the direction indicated. traffic flows into the junction at a and then goes via different routes to d. flow out of a junction must match the flow in to a junction to noise. there are traffic light switches at junctions a and b which depending on their state route traffic differently along the roads. using to denote the clean free flow we model the flows using the switching linear system at at i i i at i i i a bt i i a bt i i b ct by identifying the flows at time t with a dimensional vector hidden variable ht we can write the above flow equations as at a dt a bt b dt b ct c dt ht asht h t sb which takes for a set of suitably defined matrices as indexed by the switch variable s sa states. we additionally include noise terms to model cars parking or de-parking during a single timestep. the covariance h is diagonal with a larger variance at the inflow point a to model that the total volume of traffic entering the system can vary. noisy measurements of the flow into the network are taken at a at v draft march gaussian sum smoothing a d b c figure a representation of the traffic flow between junctions at abcd with traffic lights at a and b. if sa a d and a b carry and of the flow out of a respectively. if sa all the flow from a goes through a d for sa all the flow goes through a b. for sb the flow out of b is split equally between b d and b c. for sb all flow out of b goes along b c. figure time evolution of the traffic flow measured at two points in the network. sensors measure the total flow into the network at and the total flow out of the network dt a dt b dt c dt. the total inflow at a undergoes a random walk. note that the flow measured at d can momentarily drop to zero if all traffic is routed through a b c in two consecutive time steps. along with a noisy measurement of the total flow out of the system at d dt a dt b dt c dt v the observation model can be represented by vt bht v t using a constant projection matrix b. the switch variables follow a simple markov transition pstst which biases the switches to remain in the same state in preference to jumping to another state. see demosldstraffic.m for details. given the above system and a prior which initialises all flow at a we draw samples from the model using forward sampling which form the observations using only the observations and the known model structure we then attempt to infer the latent switch variables and traffic flows using gaussian sum filtering and smoothing method with mixture components per switch state we note that a naive hmm approximation based on discretising each continuous flow into bins would contain or million states. even for modest size problems a naive approximation based on discretisation is therefore impractical. example the price trend. the following is a simple model of the price trend of a stock which assumes that the price tends to continue going up down for a while before it reverses direction h i h vt vst here represents the price and the direction. there is only a single observation variable at each time which is the price plus a small amount of noise. there are two switch states domst when st the model functions normally with the direction being equal to the previous direction plus a small amount of noise h when st however the direction is sampled from a gaussian with a large variance. the transition pstst is set so that normal dynamics is more likely and when st it is likely to go back to normal dynamics the next timestep. full details are in sldspricemodel.mat. in we plot some samples from the model and also smoothed inference of the switch distribution showing how we can a posteriori infer the likely changes in the stock price direction. see also draft march reset models figure given the observations from we infer the flows and switch states of all the latent the correct latent flows through time along with the switch variable state used to variables. generate the data. the colours corresponds to the flows at the corresponding coloured edgesnodes in filtered flows based on a i gaussian sum forward pass approximation. plotted are the components of the vector with the posterior distribution of the sa and sb traffic light states smoothed flows and corresponding smoothed switch psa states using a gaussian sum smoothing approximation with j plotted below. t figure the top panel is a time series of prices the prices tend to keep going up or down with infrequent changes in the direction. based on fitting a simple slds model to capture this kind of behaviour the probability of a significant change in the price direction is given in the panel below based on the smoothed distribution pst reset models reset models are special switching models in which the switch state isolates the present from the past resetting the position of the latent dynamics are also known as changepoint models. whilst these models are rather general it can be helpful to consider a specific model and here we consider the slds changepoint model with two states. we use the state st to denote that the lds continues with the standard dynamics. with st however the continuous dynamics is reset to a prior n st st aht st st phtht st where n similarly we write pvtht st for simplicity we assume the switch dynamics are first order markov with transition pstst under this model the dynamics follows a standard lds but when st ht is reset to a value drawn from draft march reset models figure the independence structure of a reset model. square nodes ct denote the binary reset variables and st the state dynamics. the ht are continuous variables and vt continuous observations. if the dynamics resets the dependence of the continuous ht on the past is cut. a gaussian distribution independent of the past. such models might be of interest in prediction where the time-series is following a trend but suddenly changes and the past is forgotten. whilst this may not seem like a big change to the model this model is computationally more tractable scaling with compared to in the general two-state slds. to see this consider the filtering recursion st ht st pvtht stphtht stpstst st st we now consider the two cases st st pst st pst st ht ht st st shows that pht st is not a mixture model in ht but contains only a single component proportional to if we use this information in equation we have st ht st ht st assuming st is a mixture distribution with k components then st will be a in general therefore st will contain t components and mixture with k components. st a single component. as opposed to the full slds case the number of components therefore grows only linearly with time as opposed to exponentially. this means that the computational effort to perform exact filtering scales as run-length formalism one may also describe reset models using a run-length formalism using at each time t a latent variable rt which describes the length of the current if there is a change the run-length variable is reset to zero otherwise it is increased by rt rt rt pcp prtrt pcp t draft march where pcp is the probability of a reset changepoint the joint distribution is given by prtrt rt rt pvtvt rtt with the understanding that if rt then pvtvt rtt pvt. the graphical model of this distribution is awkward to draw since the number of links depends on the run-length rt. predictions can be made using reset models rt prt rt rt where the filtered run-length is given by the forward recursion prt rt vt pvtrtrt prtrt rtt rt prt vtrt which shows that filtered inference scales with rt a poisson reset model the changepoint structure is not limited to conditionally gaussian cases only. to illustrate this we consider the following at each time t we observe a count yt which we assume is poisson distributed with an unknown positive intensity h. the intensity is constant but at certain unknown times t it jumps to a new value. the indicator variable ct denotes whether time t is such a changepoint or not. mathematically the model is pct bect pvtht povt ht phtht ct i ht i b the symbols g be and po denote the gamma bernoulli and the poisson distributions respectively gh a b exp log h bh log a log b bec exp log c pov h exp log h h log given observed counts the task is to find the posterior probability of a change and the associated intensity levels for each region between two consecutive changepoints. plugging the above definitions in the generic updates equation and equation we see that ct is a gamma potential and that ct is a mixture of gamma potentials where a gamma potential is defined as elgh a b via the triple b l. for the corrector update step we need to calculate the product of a poisson term with the observation model pvtht povt ht. a useful property of the poisson distribution is that given the observation the latent variable is gamma distributed pov h v log h h log log h h log gh v hence the update equation requires multiplication of two gamma potentials. a nice property of the gamma density is that the product of two gamma densities is also a gamma potential example is due to taylan cemgil. draft march reset models where log the recursions for this reset model are therefore closed in the space of a mixture of gamma potentials with an additional gamma potential in the mixture at each timestep. a similar approach can be used to form the smoothing recursions. example mining disasters. we illustrate the algorithm on the coal mining disaster dataset the data set consists of the number of deadly coal-mining disasters in england per year over a time span of years from to it is widely agreed in the statistical literature that a change in the intensity expected value of the number of disasters occurs around the year after new health and safety regulations were introduced. in we show the marginals along with the filtering density. note that we are not constraining the number of changepoints and in principle allow any number. the smoothed density suggests a sharp decrease around t figure estimation of change points. coal mining disaster dataset. filtered estimate of the marginal intensity and smoothed estimate we evaluate these mixture of gamma distributions on a fixed grid of h and show the density as a function of t. here darker color means higher probability. hmm-reset the reset model defined by equations above is useful in many applications but is limited since only a single dynamical model is considered. an important extension is to consider a set of available dynamical models indexed by st s with a reset that cuts dependency of the continuous variable on the st phtht st ct ct ct with the reset recursions on replacing ht by st. to see this we consider the filtering recursion for the two cases the computational complexity of filtering for this model is which can be understood by analogy the states st follow a markovian dynamics pstst ct see a reset occurs if the state st changes otherwise no reset occurs pct st i st stpstst ct st ct st ct ht st st ct ctpct st st ct st ht st draft march pct st ct ct of accidentsfiltered exercises figure the independence structure of a hmm-reset model. square nodes ct denote discrete switch variables ht are continuous latent variables and vt continuous observations. the discrete state ct determines which linear dynamical system from a finite set of linear dynamical systems is operational at time t. from equation we see that st ct contains only a single component proportional to this is therefore exactly analogous to the standard reset model except that we need now to index a set of messages with st therefore each message taking o steps to compute. the computational effort to perform exact filtering scales as code sldsforward.m slds forward sldsbackward.m slds backward correction collapse a mixture of gaussians to a smaller mixture of gaussians sldsmarggauss.m marginalise an slds gaussian mixture logeps.m logarithm with offset to deal with demosldstraffic.m demo of traffic flow using a switching linear dynamical system exercises exercise consider the setup described in for which the full slds model is given in sldspricemodel.m following the notation used in demosldstraffic.m. given the data in the vector v your task is to fit a prediction model to the data. to do so approximate the filtered distribution pht using a mixture of i components. the prediction of the price at the next day is then vpredt where st pht compute the mean prediction error compute the mean naive prediction error which corresponds to saying that tomorrow s price will be the same as today s. you might find sldsmarggauss.m of interest. exercise the data in are observed prices from an intermittent mean-reverting process contained in meanrev.mat. there are two states s there is a true price pt and an observed price vt is plotted. when s the true underlying price reverts back towards the mean m with rate r otherwise the true price follows a random walk rpt m m p t pt p t pt st st draft march exercises figure data from an intermittent mean-reverting process. see where p t n p n p t st st t the observed price vt is related to the unknown price pt by vt n pt it is known that of the time is in the same state as at time t and that at time t either state of s is equally likely. also at t n m based on this information and using gaussian sum filtering with i components sldsforward.m what is the probability at time t that the dynamics is following a random walk repeat this computation for smoothing based on using expectation correction with i j components. draft march exercises draft march chapter distributed computation introduction how natural organisms process information is a fascinating subject and one of the grand challenges of science. whilst this subject is still in its early stages loosely speaking there are some generic properties that most such systems are believed to possess patterns are stored in a set of neurons recall of patterns is robust to noise transmission between neurons is of a binary nature and is stochastic information processing is distributed and highly modular. in this chapter we discuss some of the classical toy models that have been developed as a test bed for analysing such in particular we discuss some classical models from a probabilistic viewpoint. stochastic hopfield networks hopfield networks are models of biological memory in which a pattern is represented by the activity of a set of v interconnected neurons. the term network here refers to the set of neurons see and not the belief network representation of distribution of neural states unrolled through time at time t neuron i fires vit or is quiescent vit firing depending on the states of the neurons at the preceding time t explicitly neuron i fires depending on the potential ait i wijvjt where wij characterizes the efficacy with which neuron j transmits a binary signal to neuron i. the threshold i relates to the neuron s predisposition to firing. writing the state of the network at time t as vt vv the probability that neuron i fires at time t is modelled as pvit where e x and controls the level of stochastic behaviour of the neuron. the probability of being in the quiescent state is given by normalization pvit pvit these two rules can be compactly written as pvit which follows directly from x. learning sequences figure a depiction of a hopfield network neurons. the connectivity of the neurons is described by a weight matrix with elements wij. the graph represents a snapshot of the state of all neurons at time t which simultaneously update as function of the network at the previous time t in the limit the neuron updates deterministically vit sgn in a synchronous hopfield network all neurons update independently and simultaneously so that we can represent the temporal evolution of the neurons as a dynamic bayes network pvt pvit given this toy description of how neurons update how can we use the network to do interesting things for example to store a set of patterns and recall them under some cue. the patterns will be stored in the weights and in the following section we address how to learn suitable parameters wij and i to learn temporal sequences based on a simple local learning rule. learning sequences a single sequence given a sequence of network states v vt we would like the network to store this sequence such that it can be recalled under some cue. that is if the network is initialized in the correct starting state of the training sequence vt the remainder of the training sequence for t should be reproduced under the deterministic dynamics equation without error. two classical approaches to learning a temporal sequence are the and pseudo inverse in both the standard hebb and pi cases the thresholds i are usually set to zero. standard hebb rule t wij v vit hebb a neurobiologist actually let us assume that the persistence or repetition of a reverberatory activity trace tends to induce lasting cellular changes that add to its stability. when an axon of cell a is near enough to excite a cell b and repeatedly or persistently takes part in firing it some growth process or metabolic change takes place in one or both cells such that a s efficiency as one of the cells firing b is increased. this statement is sometimes interpreted to mean that weights are exclusively of the correlation form equation for a discussion. this can severely limit the performance and introduce adverse storage artifacts including local draft march learning sequences figure a dynamic bayesian network representation of a hopfield network. the network operates by simultaneously generating a new set of neuron states according to equation defines a markov transition matrix modelling the transition probability vt and furthermore imposes the constraint that the neurons are conditionally independent given the previous state of the network. the hebb rule can be motivated mathematically by considering j wijvjt v v vi t vit t j j v vit j vj t vi v j vi j vj if the patterns are uncorrelated then the interference term t vi j vj vj will be relatively small. to see this we first note that for randomly drawn patterns the mean of is zero since t and the patterns are randomly the variance is therefore given by for j k all the terms are independent and contribute zero on average. therefore j v jk v t t meaning that the sign i j v j j t v j when all the terms are independent zero mean and contribute zero. hence provided that the number of neurons v is significantly larger than the length of the sequence t then the average size of the interference will be small. in this case the term in equation dominates j wijvjt will be that of vit and the correct pattern sequence recalled. the hebb rule is capable of storing a random temporal sequence of length time however the hebb rule performs poorly for the case of correlated patterns since interference from the other patterns becomes pseudo inverse rule the pi rule finds a matrix wij that solves the linear equations wijvjt vit t t j draft march under this condition sgn j wijvjt recalled. in matrix notation we require wv v where vit t t w v vtv vt sgn vit so that patterns will be correctly learning sequences v it vit t t for t v the problem is under-determined. one solution is given by the pseudo inverse the pseudo inverse rule can store any sequence of v linearly independent patterns. whilst attractive compared to the standard hebb in terms of its ability to store longer correlated sequences this rule suffers from very small basins of attraction for temporally correlated patterns see the maximum likelihood hebb rule an alternative to the above classical algorithms is to view this as a problem of pattern storage in the dbn equation first we need to clarify what we mean by store given that we initialize the network in a state vt we wish that the remaining sequence will be generated with high probability. that is we wish to adjust the network parameters such that the probability pvt vt is furthermore we might hope that the sequence will be recalled with high probability not just when initialized in the correct state but also for states close hamming distance to the correct initial state due to the markov nature of the dynamics the conditional likelihood is pvt vt pvt this is a product of transitions from given states to given states. since these transition probabilities are known the conditional likelihood can be easily evaluated. the sequence log likelihood is t pvt t lw log t log pvt log our task is then to find weights w and thresholds that maximisise lw there is no closed form solution and the weights therefore need to be determined numerically. this corresponds to a straightforward computational problem since the log likelihood is a convex function. to show this we compute the hessian for expositional clarity this does not affect the conclusions t t dwijdwkl where we defined itvkt ik it patterns can also be considered in this framework as a set of patterns that map to each other. draft march learning sequences figure leftmost panel the temporally highlycorrelated training sequence we desire to store. the other panels show the temporal evolution of the network after initialization in the correct starting state but corrupted with noise. during recall deterministic updates were used. the maximum likelihood rule was trained using batch epochs with see also demohopfield.m it is straightforward to show that the hessian is negative definite and hence the likelihood has a single global maximum. to increase the likelihood of the sequence we can use a simple method such as gradient wnew ij wij dl dwij new i i dl d i where t dl dwij itvit t dl d i itvit the learning rate is chosen empirically to be sufficiently small to ensure convergence. the learning rule equation can be seen as a modified hebb learning rule the basic hebb rule being given when it as learning progresses the it will typically tend to values close to either or and hence the learning rule can be seen as asymptotically equivalent to making an update only in the case of disagreement and vit are of different signs. this batch training procedure can be readily converted to an online in which an update occurs immediately after the presentation of two consecutive patterns. storage capacity of the ml hebb rule the ml hebb rule is capable of storing a sequence of v linearly independent patterns. to see this we can form an input-output training set for each neuron i vit t t each neuron has an associated weight vector wi wij j v which forms a logistic regressor or in the limit a for perfect recall of the patterns we therefore need only that the patterns on the sequence be linearly separable. this will be the case if the patterns are linearly independent regardless of the outputs vit t t relation to the perceptron rule in the limit that the activation is large vit i vit provided the activation and desired next output are the same sign no update is made for neuron i. in this limit equation is called the perceptron for an activation a that is close to the one can use more sophisticated methods such as the newton method or conjugate gradients. in theoretical neurobiology the emphasis is small gradient style updates since these are deemed to be biologically more plausible. draft march training sequencetimeneuron learning sequences figure the fraction of neurons correct for the final state of the network t for a neuron hopfield network trained to store a length sequence patterns. after initialization in the correct initial state at t the hopfield network is updated deterministically with a randomly chosen percentage of the neurons flipped post updating. the correlated sequence of length t was produced by flipping with probability of the previous state of the network. a fraction correct value of indicates perfect recall of the final state and a value of indicates a performance no better than random guessing of the final state. for maximum likelihood epochs of training were used with during recall deterministic updates were used. the results presented are averages over simulations resulting in standard errors of the order of the symbol sizes. decision boundary a small change can lead to a different sign of the neural firing. to guard against this it is common to include a stability criterion vit m vit m i where m is an empirically chosen positive threshold. example a correlated sequence. in we consider storage of a highly-correlated temporal sequence of length t of neurons using the three learning rules hebb maximum likelihood and pseudo inverse. the sequence is chosen to be highly correlated which constitutes a difficult learning task. the thresholds i are set to zero throughout to facilitate comparison. the initial state of the training sequence corrupted by noise is presented to the trained networks and we desire that the training sequence will be generated from this initial noisy state. whilst the hebb rule is operating in a feasible limit for uncorrelated patterns the strong correlations in this training sequence entails poor results. the pi rule is capable of storing a sequence of length yet is not robust to perturbations from the correct initial state. the maximum likelihood rule performs well after a small amount of training. stochastic interpretation by straightforward manipulations the weight update rule in equation can be written as t dl dwij vit a stochastic online learning rule is therefore wijt vit vjt vjt where vit is with probability and otherwise. provided that the learning rate is small this stochastic updating will approximate the learning rule example sequences under perpetual noise. we compare the performance of the maximum likelihood learning rule zero thresholds with the standard hebb pseudo inverse and perceptron rule for learning a single temporal sequence. the network is initialized to a noise corrupted draft march probabilityfraction correctsequence likelihoodnoise trained max likelihoodperceptron inverse learning sequences the figure original t binary video sequence on a set of neurons. reconstructions beginning from a noise perturbed initial state. every odd time reconstruction is also randomly perturbed. despite the high level of noise the basis of attraction of the pattern sequence is very broad and the patterns immediately fall back close to the pattern sequence even after a single timestep. version of the correct initial state vt from the training sequence. the dynamics is then run for the same number of steps as the length of the training sequence and the fraction of bits of the recalled final state which are the same as the training sequence final state vt is measured at each stage in the dynamics the last the state of the network was corrupted with noise by flipping each neuron state with the specified flip probability. the training sequences are produced by starting from a random initial state and then choosing at random percent of the neurons to flip each of the chosen neurons being flipped with probability giving a random training sequence with a high degree of temporal correlation. the standard hebb rule performs relatively poorly particularly for small flip rates whilst the other methods perform relatively well being robust at small flip rates. as the flip rate increases the pseudo inverse rule becomes unstable especially for the longer temporal sequence which places more demands on the network. the perceptron rule can perform as well as the maximum likelihood rule although its performance is critically dependent on an appropriate choice of the threshold m. the results for m perceptron training are poor for small flip rates. an advantage of the maximum likelihood rule is that it performs well without the need for fine tuning of parameters. in all cases batch training was used. an example for a larger network is given in which consists of highly correlated sequences. for such short sequences the basin of attraction is very large and the video sequence can be stored robustly. multiple sequences the previous section detailed how to train a hopfield network for a single temporal sequence. we now address the learning a set of sequences n n n. if we assume that the sequences are independent the log likelihood of a set of sequences is the sum of the individual sequences. the gradient is given by n i i j n i i t dl dwij where t dl d i i i an wijvn j j n i i i the log likelihood remains convex since it is the sum of convex functions so that the standard gradient based learning algorithms can be used here as well. draft march tractable continuous latent variable models boolean networks the hopfield network is one particular parameterisation of the table pvit however less constrained parameters may be considered indeed one could consider the fully unconstrained case in which each neuron i would have an associated parental states. this exponentially large number of states is impractical and an interesting restriction is to consider that each neuron has only k parents so that each table contains entries. learning the table parameters by maximum likelihood is straightforward since the log likelihood is a convex function of the table entries. hence for given any sequence set of sequences one may readily find parameters that maximise the sequence reconstruction probability. the maximum likelihood method also produces large basins of attraction for the associated stochastic dynamical system. such models are of potential interest in artificial life and random boolean networks in which emergent macroscopic behaviour appears from local update sequence disambiguation a limitation of first order networks defined on visible variables alone as the hopfield network is that the observation transition v is the same every time the joint state v is encountered. this means that if the sequence contains a subsequence such as a b a c this cannot be recalled with high probability since a transitions to different states depending on time. whilst one could attempt to resolve this sequence disambiguation problem using a higher order markov model to account for a longer temporal context we would lose biological plausibility. using latent variables is an alternative way to sequence disambiguation. in the hopfield model the recall capacity can be increased using latent variables by make a sequencing in the joint latent-visible space that is linearly independent even if the visible variable sequence alone is not. in we discuss a general method that extends dynamic bayes networks defined on visible variables alone such as the hopfield network to include continuous non-linearly updating latent variables without requiring additional approximations. tractable continuous latent variable models a dynamic bayes network with latent variables takes the form t t t ht as we saw in provided all hidden variables are discrete inference in these models is straightforward. however in many physical systems it is more natural to assume continuous ht. in we saw that one such tractable continuous ht model is given by linear gaussian transitions and emissions the lds. whilst this is useful we cannot represent non-linear changes in the latent process using an lds alone. the switching lds of is able to model non-linear continuous dynamics switching although we saw that this leads to computational difficulties. for computational reasons we therefore seem limited to either purely discrete h no limitation on the discrete transitions or purely continuous h be forced to use simple linear dynamics. is there a way to have a continuous state with non-linear dynamics for which posterior inference remains tractable? the answer is yes provided that we assume the hidden transitions are when conditioned on the visible variables this renders the hidden unit distribution trivial. this allows the consideration of rich non-linear dynamics in the hidden space if required. deterministic latent variables consider a belief network defined on a sequence of visible variables to enrich the model we include additional continuous latent variables that will follow a non-linear markov transition. to retain tractability of inference we constrain the latent dynamics to be deterministic described by pht vt ht f vt ht h here represents the dirac delta function for continuous hidden variables. the non-linear function f parameterises the cpt. whilst the restriction to deterministic cpts appears severe the model draft march tractable continuous latent variable models ht vt ht vt figure a first order dynamic bayesian network with deterministic hidden cpts by diamonds that is the hidden node is certainly in a single state determined by its parents. conditioning on the visible variables forms a directed chain in the hidden space which is deterministic. hidden unit inference can be achieved by forward propagation alone. integrating out hidden variables gives a cascade style directed visible graph which so that each vt depends on all t retains some attractive features the marginal t is non-markovian coupling all the variables in the sequence see whilst hidden unit inference t t is deterministic as illustrated in the adjustable parameters of the hidden and visible cpts are represented by h and v respectively. for learning the log likelihood of a single training sequence v is to maximise the log likelihood using gradient techniques we need to the derivatives with respect to the model parameters. these can be calculated as follows ft fvt vt ht h hence the derivatives can be calculated by deterministic forward propagation of errors alone. the case of training multiple independently generated sequences v n n n is a straightforward extension. an augmented hopfield network to make the deterministic latent variable model more explicit we consider the case of continuous hidden units and discrete binary visible units vit in particular we restrict attention to the hopfield model augmented with latent variables that have a simple linear dynamics for a nonlinear extension ht bvt deterministic latent transition it cht dvt pvt ht draft march t t l v hv log v log pvt ht v where the hidden unit values are calculated recursively using ht f vt ht h log v v log pvt ht v v t dl d v dl d h dht d h where ht ft h log pvt ht v dht ft ht dht d h d h this model generalises a recurrent stochastic heteroassociative hopfield to include deterministic hidden units dependent on past network states. the parameters of the model are a b c d. for gradient based training we require the derivatives with respect to each of these parameters. the derivative of the log likelihood for a generic parameter is this gives all indices are summed over the dimensions of the quantities they relate to neural models vit it d d where it i it it d d l it it da cij cij d d j d d j j hjt hjt db da db d dc d dd d da d it i h it i v hit it it j hit db it j aij d da aij d db hjt i h hjt i v if we assume that is a given fixed value we can compute the derivatives recursively by forward propagation. gradient based training for this augmented hopfield network is therefore straightforward to implement. this model extends the power of the original hopfield model being capable of resolving ambiguous transitions in sequences such as a b a c see and demohopfieldlatent.m. in terms of a dynamic system the learned network is an attractor with the training sequence as a stable point and demonstrates that such models are capable of learning attractor recurrent networks more powerful than those without hidden units. example disambiguation. the sequence in contains repeated patterns and therefore cannot be reliably recalled with a first order model containing visible variables alone. to deal with this we consider a hopfield network with visible units and additional hidden units with deterministic latent dynamics. the model was trained with gradient ascent to maximise the likelihood of the binary sequence in as shown in the learned network is capable of recalling the sequence correctly even when initialised in an incorrect state having no difficulty with the fact that the sequence transitions are ambiguous. neural models the tractable deterministic latent variable model introduced in presents an opportunity to extend models such as the hopfield network to include more biologically realistic processes without losing computational tractability. first we discuss a general framework for learning in a class of neural this being a special case of the deterministic latent variable and a generalisation of the spike-response model of theoretical draft march neural models figure the training sequence consists of a random set of vectors the reconstruction using h hidden units. the over t time steps. initial state vt for the recalled sequence was set to the correct initial training value albeit with one of the values flipped. note that the method is capable of sequence disambiguation in the sense that the transitions of the form a b a c can be recalled. stochastically spiking neurons we assume that neuron i fires depending on the membrane potential ait through pvit ht pvit to be specific we take throughout pvit here we to define the quiescent state as vit so that pvit the choice of the sigmoid function is not fundamental and is chosen merely for analytical convenience. the log-likelihood of a sequence of visible states v is l log and the gradient of the log-likelihood is then dl dwij dait dwij t t where we used the fact that vi here wij are parameters of the membrane potential below. we take equation as common in the following models in which the membrane potential ait is described with increasing sophistication. hopfield membrane potential as a first step we show how the hopfield network training as described in can be recovered as a special case of the above framework. the hopfield membrane potential is ait wijvjt bi where wij characterizes the efficacy of information transmission from neuron j to neuron i and bi is a threshold. applying the maximum likelihood framework to this model to learn a temporal sequence v by adjustment of the parameters wij bi are fixed for simplicity we obtain the learning rule daidwij vjt in equation t wnew ij wij dl dwij dl dwij vjt where the learning rate is chosen empirically to be sufficiently small to ensure convergence. matches equation uses the encoding. draft march neural models figure learning with depression u t despite the apparent complexity of the dynamics learning appropriate neural connection weights is straightforward using maximum likelihood. the reconstruction using the standard hebb rule by contrast is dynamic synapses in more realistic synaptic models neurotransmitter generation depends on a finite rate of cell subcomponent production and the quantity of vesicles released is affected by the history of loosely speaking when a neuron fires it releases a chemical substance from a local reservoir this reservoir being refilled at a lower rate than the neuron can fire. if the neuron fires continually its ability to continue firing weakens since the reservoir of release chemical is depleted. this can be accounted for by using a depression mechanism that affects the membrane potential for depression factors xjt a simple dynamics for these depression factors ait wijxjtvjt xjt xjt t xjt u xjtvjt where t and u represent time scales recovery times and spiking effect parameters respectively. note that these depression factor dynamics are exactly of the form of deterministic hidden variables. it is straightforward to include these dynamic synapses in a principled way using the maximum likelihood learning framework. for the hopfield potential the learning dynamics is simply given by equations with dait dwij xjtvjt example with depression. in we demonstrate learning a random temporal sequence of time steps for an assembly of neurons with dynamic depressive synapses. after learning wij the trained network is initialised in the first state of the training sequence. the remaining states of the sequence were then correctly recalled by iteration of the learned model. the corresponding generated factors xit are also plotted. for comparison we plot the results of using the dynamics having set the wij using the temporal hebb rule equation the poor performance of the correlation based hebb rule demonstrates the necessity in general to couple a dynamical system with an appropriate learning mechanism. leaky integrate and fire models leaky integrate and fire models move a step further towards biological realism in which the membrane potential increments if it receives an excitatory stimulus and decrements if it receives an inhibitory stimulus after firing the membrane potential is reset to a low value below the firing threshold and thereafter steadily increases to a resting level for example a model that incorporates such effects is ait j vit vit f ired wijvjt rest ait draft march neuron exercises since vi if neuron i fires at time t the potential is reset to f ired at time t. similarly with no synaptic input the potential equilibrates to rest with time constant log despite the increase in complexity of the membrane potential over the hopfield case deriving appropriate learning dynamics for this new system is straightforward since as before the hidden variables the membrane potentials update in a deterministic fashion. the potential derivatives are dait dwij vit dait dwij vjt by initialising the derivative equations define a first order recursion for the gradient which can be used to adapt wij in the usual manner wij wij dldwij. we could also apply synaptic dynamics to this case by replacing the term vjt in equation by xjtvjt. dwij although a detailed discussion of the properties of the neuronal responses for networks trained in this way is beyond the scope of these notes an interesting consequence of the learning rule equation is a spike time dependent learning window in qualitative agreement with experimental in summary provided one deals with deterministic latent dynamics essentially arbitrarily complex spatiotemporal patterns may potentially be learned and generated under cued retrieval for very complex neural dynamics. the spike-response model can be seen as a special case of the deterministic latent variable model in which the latent variables have been explicitly integrated out. code demohopfield.m demo of hopfield sequence learning hebbml.m gradient ascent training of a set of sequences using max likelihood hopfieldhiddennl.m hopfield network with additional non-linear latent variables demohopfieldlatent.m demo of hopfield net with deterministic latent variables hopfieldhiddenliknl.m hopfield net with hidden variables sequence likelihood t exercises exercise consider a very large hopfield network v used to store a single temporal sequence of length t t v in this case the weight matrix w may be difficult to store. explain how to justify the assumption wij uitvit where uit are the dual parameters and derive an update rule for the dual parameters u. exercise a hopfield network is used to store a raw uncompressed binary video sequence. each image in the sequence contains binary pixels. at a rate of frames per second how many hours of video can neurons store? exercise derive the update equation exercise show that the hessian equation is negative definite. that is ijkl xijxkl dwijdwkl for any x draft march exercise for the augmented hopfield network of latent dynamics hit aijhjt bijvjt j exercises derive the derivative recursions described in draft march part v approximate inference chapter sampling introduction sampling concerns drawing realisations xl of a variable x from a distribution px. for a discrete variable x in the limit of a large number of samples the fraction of samples in state x tends to px x. that is xl x px x in the continuous case one can consider a small region such that the probability that the samples occupy tends to the integral of px over in other words the relative frequency x tends to x px. given a finite set of samples one can then approximate expectations using lim l l l f f l f fxl f this approximation holds for both discrete and continuous variables. provided the samples are indeed from px then the average of the approximation is the variance of the approximation is fxl pxlxl l px px hence the mean of the approximation is the exact mean of f and the variance of the approximation scales inversely with the number of samples. in principle therefore provided the samples are independently drawn from px only a small number of samples is required to accurately estimate this mean. importantly this result is independent of the dimension of x. however the critical difficulty is in actually generating independent samples from px. drawing samples from high-dimensional distributions is generally difficult and few guarantees exist to ensure that in a practical timeframe the samples produced are representative enough such that expectations can be approximated accurately. there are many different sampling algorithms all of which work in principle but each working in practice only when the distribution satisfies particular before we develop schemes for multi-variate distributions we consider the univariate case. introduction figure a representation of the discrete distribution equation the unit interval from to is partitioned in parts whose lengths are equal to and univariate sampling in the following we assume that a random number generator exists which is able to produce a value uniformly at random from the unit interval we will make use of this uniform random number generator to draw samples from non-uniform distributions. discrete case consider the one dimensional discrete distribution px where domx with x x x px this represents a partitioning of the unit interval in which the interval has been labelled as state as state and as state if we were to drop a point anywhere at random uniformly in the interval the chance that would land in interval is and the chance that it would be in interval is and similarly for interval this therefore defines for us a valid sampling procedure for discrete one-dimensional distributions as described in in our example we have we then draw a sample uniformly from say u then the sampled state would be state since this is in the interval sampling from a discrete univariate distribution is straightforward since computing the cumulant takes only o steps for a k state discrete variable. continuous case in the following we assume that a method exists to generate samples from the uniform distribution intuitively the generalisation of the discrete case to the continuous case is clear. first u we calculate the cumulant density function cy pxdx y then we sample u uniformly from and obtain the corresponding sample x by solving cx u x c formally therefore sampling of a continuous univariate variable is straightforward provided we can compute the integral of the corresponding probability density function. algorithm sampling from a univariate discrete distribution p with k states. label the k states as i k with associated probabilities pi. calculate the cumulant ci j i pj and set draw a value u uniformly at random from the unit interval find that i for which ci u ci. return state i as a sample from p. draft march introduction figure histograms of the samples from the three state distribution px samples. samples. as the number of samples increases the relative frequency of the samples tends to the distribution px. for special distributions such as gaussians numerically efficient alternative procedures exist usually based on co-ordinate transformations see multi-variate sampling one way to generalise the one dimensional discrete case to a higher dimensional distribution xn is to translate this into an equivalent one-dimensional distribution. this can be achieved by enumerating all the possible joint states xn giving each a unique integer i from to the total number of states and constructing a univariate distribution with probability pi px for i corresponding to the multivariate state x. this then transforms the multi-dimensional distribution into an equivalent one-dimensional distribution and sampling can be achieved as before. in general of course this procedure is impractical since the number of states will grow exponentially with the number of variables xn. an alternative exact approach would be to capitalise on the relation we can sample from the joint distribution by first sampling a state for from the one-dimensional and then with clamped to this state sampling a state for from the one-dimensional it is clear how to generalise this to more variables by using a cascade decomposition xn pxnxn however in order to apply this technique we need to know the conditionals pxixi unless these are explicitly given we need to compute these from the joint distribution xn. such conditionals will in general require the summation over an exponential number of states and except for small n generally also be impractical. for belief networks however by construction the conditionals are specified so that this technique becomes practical as we discuss in drawing samples from a multi-variate distribution is in general therefore a complex task and one seeks to exploit any structural properties of the distribution to make this computationally more feasible. a common approach is to seek to transform the distribution into a product of lower dimensional distributions. a classic example of this is sampling from a multi-variate gaussian which can be reduced to sampling from a set of univariate gaussians by a suitable coordinate transformation as discussed in example from a multi-variate gaussian. our interest is to draw a sample from the multi-variate gaussian px n m s. for a general covariance matrix s px does not factorise into a product of univariate distributions. however consider the transformation where c is chosen so that cct s. since this is a linear transformation y is also gaussian distributed with mean y c m c px m px c m since the mean of y is zero the covariance is given by c m c t c t c t i px draft march ancestral sampling figure an ancestral belief network without any evidential variables. to sample from this distribution we draw a sample from variable and then variables in order. hence py n i i n hence a sample from y can be obtained by independently drawing a sample from each of the univariate zero mean unit variance gaussians. given a sample for y a sample for x is obtained using x cy m drawing samples from a univariate gaussian is a well-studied topic with a popular method being the box-muller technique ancestral sampling belief networks take the general form px i pxipa where each of the conditional distributions pxipa is known. provided that no variables are evidential we can sample from this distribution in a straightforward manner. for convenience we first rename the variable indices so that parent variables always come before their children ordering for example one can sample first from those nodes that do not have any parents and given these values one can then sample and then and and finally despite the presence of loops in the graph such a forward sampling procedure is straightforward. this procedure holds for both discrete and continuous variables. if one attempted to carry out an exact inference scheme using moralisation and triangulation in more complex multiply connected graphs cliques can become very large. however regardless of the loop structure ancestral sampling is straightforward. ancestral or forward sampling is a case of perfect sampling termed exact sampling since each sample is indeed drawn from the required distribution. this is in contrast to markov chain monte carlo methods for which the samples are from px only in the limit of a large number of iterations. dealing with evidence how can we sample from a distribution in which certain variables xe are clamped to evidential states? formally we need to sample from pxexe pxe xe pxe draft march gibbs sampling figure the markov blanket of to draw a sample from we clamp into their evidential states and draw a sample from where z is a normalisation constant. if an evidential variable xi has no parents then one can simply set the variable into this state and continue forward sampling as before. for example to compute a sample from defined in equation one simply clamps the into its evidential state and continues forward sampling. the reason this is straightforward is that conditioning on merely defines a new distribution on a subset of the variables for which the distribution is immediately known. on the other hand consider sampling from using bayes rule we have the conditioning on means that the structure of the distribution on the non-evidential variables changes for example and become coupled. one could attempt to work out an equivalent new forward sampling structure although generally this will be as complex as running an exact inference approach. probability that a sample from px will be consistent with the evidence is roughly o an alternative is to proceed with forward sampling from the non-evidential distribution and then discard any samples which do not match the evidential states. this is generally not recommended since the i where i is the number of states of evidential variable i. in principle one can ease this effect by discarding dim xe the sample as soon as any variable state is inconsistent with the evidence. nevertheless the number of re-starts required to obtain a valid sample would on average be very large. i dim xe perfect sampling for a markov network for a markov network we can draw exact samples by forming an equivalent directed representation of the graph see and subsequently using ancestral sampling on this directed graph. this is achieved by first choosing a root clique and then consistently orienting edges away from this clique. an exact sample can then be drawn from the markov network by first sampling from the root clique and then recursively from the children of this clique. see potsample.m jtsample.m and demojtreesample.m. gibbs sampling the inefficiency of methods such as ancestral sampling under evidence motivates alternative techniques. an important and widespread technique is gibbs sampling which is generally straightforward to implement. no evidence assume we have a joint sample state from the multivariate distribution px. we then consider a particular variable xi. using bayes rule we may write px xi xi xn given a joint initial state from which we can read off the parental state can then draw a sample i from i n we i n pxixi draft march gibbs sampling figure a toy intractable distribution. gibbs sampling by conditioning on all variables except one leads to a simple univariate conditional disb conditioning on yields a new tribution. distribution that is singly-connected for which exact sampling is straightforward. one then selects another variable xj which only xi has been updated i i n we assume this distribution is easy to sample from since it is univariate. we call this new joint sample to sample and by continuing this procedure generates a set xl of samples in which each differs from xl in only a single component. the reason this is valid sampling scheme is outlined in for a belief network the conditional pxixi is defined by the markov blanket of xi pxjpa pxixi z pxipa j chi z pxipa xi j chi see for example this means that only the parent and parents of children states are required in forming the sample update. the normalisation constant for this univariate distribution is straightforward to work out from the requirement pxjpa in the case of a continuous variable xi the summation above is replaced with integration. evidence evidence is readily dealt with by clamping for all samples the evidential variables into their evidential states. there is also no need to sample for these variables since their states are known. gibbs sampling as a markov chain in gibbs sampling we have a sample of the joint variables xl at stage l. based on this we produce a new joint sample this means that we can write gibbs sampling as a procedure that draws from for some distribution if we choose the variable to update xi at random from a distribution qi then gibbs sampling corresponds to drawing samples using the markov transition iqi i i j xl j with qi our interest is to show that the stationary distribution of out assuming x is continuous the discrete case is analogous qi qi qi xi qipx qx x x j xj px x px ix ix ixipxi xi qipx px is px. we carry this i qx x i i i i i draft march gibbs sampling figure a two dimensional distribution for which gibbs sampling fails. the distribution has mass only in the shaded quadrants. gibbs sampling proceeds from the lth sample state and then sampling from which we write one then continues with a etc. if we start in the lower left quadrant sample from and proceed this way the upper right region is never explored. xl where xl hence as long as we continue to draw samples according to the distribution in the limit of a large number of samples we will ultimately tend to draw samples from px. any distribution qi suffices so visiting all variables equally often is also a valid choice. technically we also require that has px as its equilibrium distribution so that no matter in which state we start we always converge to px see for a discussion of this issue. structured gibbs sampling one can extend gibbs sampling by using conditioning to reveal a tractable distribution on the remaining variables. for example consider the simple distribution in single-site gibbs sampling we would condition on three of the four variables and sample from the remaining variable. for example however we may use more limited conditioning as long as the conditioned distribution is easy to sample from. in the case of equation we can condition on alone to give this can be written as a modified distribution as a distribution on this is a singly-connected linear chain from which samples can be drawn exactly. a simple approach is compute the normalisation constant by any of the standard techniques for example using the factor graph method. one may then convert this undirected linear chain to a directed graph and use ancestral sampling. these operations are linear in the number of variables in the conditioned distribution. alternatively one may form a junction tree from a set of potentials choose a root and then form a set chain by reabsorption on the junction tree. ancestral sampling can then be performed on the resulting oriented clique tree. this is the approach taken in gibbssample.m. in the above example one can also reveal a tractable distribution by conditioning on and then draw a sample of from this distribution. a valid sampling procedure is then to draw a sample from equation and then a sample from equation these two steps are then iterated. note that and are not constrained to be equal to their values in the previous sample. this procedure is generally to be preferred to the single-site gibbs updating since the samples are less correlated from one sample to the next. see demogibbssample.m for a comparison of unstructured and structured sampling from a set of potentials. draft march gibbs sampling figure two hundred gibbs samples for a two dimensional gaussian. at each stage only a single component is updated. for a gaussian with low correlation gibbs sampling can move through the for a strongly correlated gaussian gibbs sampling is less effective and likely regions effectively. does not rapidly explore the likely regions see demogibbsgauss.m. remarks if the initial sample is in a part of the state space that is very unlikely then it may take some time for the samples to become representative as only a single component of x is updated at each iteration. this motivates a so-called burn in stage in which the initial samples are discarded. in single site gibbs sampling there will be a high degree of correlation in any two successive samples since only one variable the single-site updating version is updated at each stage. an ideal perfect sampling scheme would draw each x at random from px clearly in general two such perfect samples will not possess the same degree of correlation as those from gibbs sampling. this motivates subsampling in which say every sample xk is taken and the rest discarded. due to its simplicity gibbs sampling is one of the most popular sampling methods and is particularly convenient when applied to belief networks due to the markov blanket gibbs sampling is a special case of the mcmc framework and as with all mcmc methods one should bear in mind that convergence can be a major issue that is answering questions such as how many samples are needed to be reasonably sure that my sample estimate is accurate? is difficult. despite mathematical results for special cases general rules of thumb and awareness on behalf of the user are required to monitor the efficiency of the sampling. gibbs sampling assumes that we can move throughout the space effectively by only single co-ordinate updates. we also require that every state can be visited infinitely often. in we show a case in which the two dimensional continuous distribution has mass only in the lower left and upper right regions. in that case if we start in the lower left region we will always remain there and never explore the upper right region. this problem occurs when two regions which are not connected by a probable gibbs path. gibbs sampling becomes a perfect sampler when the distribution is factorised that is the variables are independent. this suggests that in general gibbs sampling will be less effective when variables are strongly correlated. for example if we consider gibbs sampling from a strongly correlated two variable gaussian distribution then updates will move very slowly in space bugs package www.mrc-bsu.cam.ac.ukbugs is general purpose software for sampling from belief networks. draft march markov chain monte carlo markov chain monte carlo we assume we have a multivariate distribution in the form px p z assume we are able to evaluate p x for any state x but not z since z where z is the normalisation constant of the distribution and p is the unnormalised distribution. we x p is an intractable high dimensional summationintegration. the idea in mcmc sampling is to sample not directly from px but from a different distribution such that in the limit of a large number of samples effectively the samples will be from px. to achieve this we forward sample from a markov transition whose stationary distribution is equal to px. markov chains consider the conditional distribution if we are given an initial sample then we can recursively generate samples xl. after a long time l provided the markov chain is irreducible meaning that we can eventually get from any state to any other state the samples are from the stationary distribution q which is defined as a continuous variable x q qx the condition for a discrete variable is analogous on replacing integration with summation. the idea in mcmc is for a given distribution px to find a transition which has px as its stationary distribution. if we can do so then we can draw samples from the markov chain by forward sampling and take these as samples from px. note that for every distribution px there will be more than one transition with px as its stationary distribution. this is why there are very many different mcmc sampling methods each with different characteristics and varying suitability for the particular distribution at hand. detailed balance how do we construct a transition with given px as its stationary distribution? this problem can be simplified if we consider special transitions that satisfy the detailed balance condition. if we are given the marginal distribution px the detailed balance condition for a transition q is under this we see qx x px x x px qxx x so that px is the stationary distribution of the detailed balance requirement can make the process of constructing a suitable transition easier since only the relative value of to px is required in equation and not the absolute value of px or metropolis-hastings sampling consider the following transition qx qx x x qx x draft march algorithm metropolis-hastings mcmc sampling. markov chain monte carlo choose a starting point for i to l do draw a candidate sample xcand from the proposal let a qxl qxcandxl if a then xl xcand else draw a random value u uniformly from the unit interval if u a then xl xcand else end for xl xl end if end if accept the candidate accept the candidate reject the candidate where valid distribution is a so-called proposal distribution and x a positive function. this defines a since it is non-negative and qx qx x x our interest is to set fx such that the stationary distribution of proposal is equal to px for any qx px that is qx x qx x qx x xpx px in order that this holds we require the integral variable from to x now consider the metropolis-hastings acceptance function x qx x fx x min fx x qx xpx qxx qx qxx x min qxx qx fx x which is defined for all x and has the detailed balance property qxx hence the function x as defined above ensures equation holds and that its stationary distribution. how do we sample from can be interpreted as a mixture of two distributions x. one proportional to to draw a sample from this we draw a sample from and accept this with probability x. since drawing from and accepting are performed independently the probability of accepting the drawn candidate is the product of these probabilities namely x. otherwise the candidate is rejected and we take the sample x. using the properties of the acceptance function equation the following is equivalent to deciding on acceptingrejecting the candidate. if x and the other x with mixture coefficient has px as qxx qx draft march auxiliary variable methods figure metropolis-hastings samples from a bivariate distribution using a proposal n x i. we also plot the iso-probability contours of p. although px is multimodal the dimensionality is low enough and the modes sufficiently close such that a simple gaussian proposal distribution is able to bridge the two modes. in higher dimensions such multi-modality is more problematic. see demometropolis.m if we reject the candidate we take x. otherwise we accept the sample from we accept the sample from note that if the candidate is rejected we take the original x as the new sample. hence at each iteration of the algorithm produces a sample either a copy of the current sample or the candidate sample. a rough rule of thumb is to choose a proposal distribution for which the acceptance rate is between and with probability gaussian proposal distribution a common proposal distribution for multivariate x explicitly as a vector is n for which x and the acceptance criterion becomes e x min p p if the unnormalised probability of the candidate state is higher than the current state we therefore accept the candidate. otherwise if the unnormalised probability of the candidate state is lower than the current state we accept the candidate only with probability p if the candidate is rejected the new sample is taken to be a copy of the previous sample x. see for a demonstration. in high dimensions it is unlikely that a random candidate sampled from a gaussian will result in a candidate probability higher than the current value because of this only very small jumps small are likely to be accepted. this limits the speed at which we explore the space x. this acceptance function above highlights that sampling is different from finding the optimum. provided has a higher probability than x we accept however we also accept a specified acceptance probability candidates that have also a lower probability than the current sample. auxiliary variable methods a practical concern in mcmc methods is ensuring that one moves effectively through the significant probability regions of the distribution. for methods such as metropolis-hastings with local proposal distributions in the sense they are unlikely to propose a candidate far from the current sample if the target distribution has isolated islands of high density then the likelihood that we would be able to move from one island to the other is very small. if we attempt to make the proposal less local by using one with a high variance the chance then of landing at random on a high density island is remote. auxiliary variable methods use additional dimensions to exploration and in certain cases to provide a bridge between draft march auxiliary variable methods figure hybrid monte carlo. multi-modal distribution px for which we desire samples. hmc forms the joint distibution pxpy where py is gaussian. starting from the point x we first draw a y from the gaussian py giving a point y green line. then we use hamiltonian dynamics line to traverse the distribution at roughly constant energy for a fixed number of steps giving we accept this point if hx and make the new sample line. otherwise this candidate is accepted with probability hx if rejected the new sample is taken as a copy of x. isolated high density islands. consider drawing samples from px where x is a high-dimensional vector. for an auxiliary variable y we introduce a distribution pyx to form the joint distribution px y pyxpx if we draw samples yl from this joint distribution then a valid set of samples from px is given by taking the xl alone. if we sampled x directly from px and then y from pyx introducing y is pointless since there is no effect on the x sampling procedure. in order for this to be useful therefore the auxiliary variable must influence how we sample x. below we discuss some of the common auxiliary variable schemes. hybrid monte carlo hybrid mc is a method for continuous variables that aims to make non-local jumps in the samples and in so doing to jump potentially from one mode to another. we define the distribution from which we wish to sample as px zx ehxx for some given hamiltonian hxx is just a potential. we then define another easy distribution from which we can readily generate samples py zy ehyy so that the joint distribution is given by px y pxpy z ehxxhyy z ehxy in the standard form of the algorithm a multi-dimensional gaussian is chosen for the auxiliary distribution with dim y dim x so that hyy yty the hmc algorithm first draws from py and subsequently from px y. for a gaussian py sampling from is straightforward. in the next dynamic step a sample is drawn from px y using a metropolis draft march auxiliary variable methods mcmc sampler. the idea is to go from one point of the space x y to a new point that is a non-trivial distance from x y and which will be accepted with a high probability. the candidate will have a good chance to be accepted if is close to hx y this can be achieved by following a contour of equal energy h as described in the next section. hamiltonian dynamics we wish to make an update x x y y for small x and y such that the hamiltonian hx y hxx hyy is conserved we can satisfy this to first order by considering the taylor expansion hx y hx x y y hx xt xhx y hy yt yhx y conservation up to first order therefore requires xt xhx y yt yhx y this is a single scalar requirement and there are therefore many different solutions for x and y that satisfy this single condition. it is customary to use hamiltonian dynamics which correspond to the setting x yhx y y xhx y where is a small value to ensure that the taylor expansion is accurate. hence xt xt yhyy yt yt xhxx for the hmc method hx y hxx hyy so that xhx y xhxx and yhx y yhyy for the gaussian case yhyy y so that yt yt xhx xt xt there are specific ways to implement the hamiltonian dynamics called leapfrog discretisation that are more accurate than the simple time-discretisation used above and we refer the reader to for details. in order to make a symmetric proposal distribution at the start of the dynamic step we choose or uniformly. this means that there is the same chance that we go back to the point x y starting from as vice versa. we then follow the hamiltonian dynamics for many time steps of the order of several hundred to reach a candidate point if the hamiltonian dynamics is numerically accurate will have roughly the same value as hx y. we then do a metropolis step and accept the point if hx y and otherwise accept it with probability hx y. if rejected we take the initial point x y as the sample. combined with the py sample step we then have the general procedure as described in in hmc we use not just the potential hxx to define candidate samples but the gradient of hxx as well. an intuitive explanation for the success of the algorithm is that it is less myopic than straightforward metropolis since the gradient enables the algorithm to feel its way to other regions of high probability by contouring paths in the augmented space. one can also view the auxiliary variables as momentum variables it is as if the sample has now a momentum which can carry it through the low-density xregions. provided this momentum is high enough we can escape local regions of significant probability see draft march auxiliary variable methods algorithm hybrid monte carlo sampling start from x for i to l do end for draw a new sample y from py. choose a random or backwards trajectory direction. starting from x y follow hamiltonian dynamics for a fixed number of steps giving a candidate accept if hx y otherwise accept it with probability hx y. if rejected we take the sample as x y. current sample of states figure swendson-wang updating for px on a nearest neighbour lattice. like coloured neighbours are bonded together with probability e forming clusters of variables. each cluster is given a random colour forming the new sample. i j exp i xj. swendson-wang originally the sw method was introduced to alleviate the problems encountered in sampling from ising models close to their critical at this point large islands of same-state variables form so that strong correlations appear in the distribution the scenario under which for example gibbs sampling is not well suited. the method has been generalised to other although here we outline the procedure for the ising model only referring the reader to more specialised text for the extensions see also for the use of auxiliary variables in perfect sampling. the ising model with no external fields is defined on variables x xn xi and takes the form e ixixj i j px z which means that this is a pairwise markov network with a potential contribution e if neighbouring nodes i and j on a square lattice are in the same state and a contribution otherwise. we assume that which encourages neighbours to be in the same state. the lattice based neighbourhood structure makes this difficult to sample from and especially when which encourages large scale islands of same-state variables to form. the aim is to remove the problematic terms e ixixj by the use of the auxiliary bond variables yij one for each edge on the lattice making the conditional pxy easy to sample from. this is given by pxy pyxpx yij e ixixj using pyx we can cancel the terms e ixixj by setting pyijxi xj pyx where yij e ixixj denotes a uniform distribution between and e ixixj zij is the normalisa e ixixj i j i j zij i j draft march auxiliary variable methods with figure ten successive samples from a ising model px exp close to the critical temperature. the swendson-wang procedure is used. starting in a random initial configuration the samples quickly move away from this initial state with the characteristic longrange correlations of the variables seen close to the critical temperature. i j i xj tion constant zij e ixixj hence yij e ixixj pxy pyxpx e ixixj yij e ixixj i j i j e ixixj let s assume that we have a sample if yij then to draw a sample from pxy we must have e ixixj which means that xi and xj are constrained to be in the same state. otherwise if yij then this introduces no extra constraint on xi and xj. hence wherever yij we bond xi and xj to be in the same state. same state. then pyijxi xj to sample from the bond variables pyijxi xj consider first the situation that xi and xj are in the pyijxi xj u a bond will occur if yij which occurs with probability e similarly when xi and xj are in different states zij yij yij e pyij xj e e e hence if xi xj we bind xi and xj to be in the same state with probability e on the other hand if xi and xj are in different states yij is uniformly distributed between and after doing this for all the xi and xj pairs we will end up with a graph in which we have clusters of like-state bonded variables. the algorithm simply chooses a random state for each cluster that is with probability all variables in the cluster are in state the algorithm is described below see algorithm swendson-wang sampling n. for i j in the edge set do start from a random configuration of all for l to l do end for if xi xj we bond variables xi and xj with probability e end for for each cluster formed from the above set the state of the cluster uniformly at random. this gives a new joint configuration xl xl n. this technique has found application in spatial statistics particularly image slice sampling slice is an auxiliary variable technique that aims to overcome some of the difficulties in choosing an appropriate length scale in methods such as metropolis sampling. the brief discussion draft march importance sampling figure the full slice for a given y. ideally slice sampling would draw an x sample from anywhere on the full slice in general this is intractable for a complex distribution and a local approximate slice is formed instead see z p where the here follows that presented in and we want to draw samples from px normalisation constant z is unknown. by introducing the auxiliary variable y and defining the distribution for y p p otherwise px y we px ydy z dy z px p which shows that the marginal of px y over y is equal to the distribution we wish to draw samples from. hence if we draw samples from px y we can ignore the y samples and we will have a valid sampling scheme for px. to draw from px y we use gibbs sampling first drawing from pyx and then from pxy. drawing a sample from pyx means that we draw a value y from the uniform distribution u p given a sample y one then draws a sample x from pxy. using pxy px y we see that pxy is the distribution over x such that p y pxy i y for a given y we call the x that satisfy this a slice computing the normalisation of this distribution is in general non-trivial since we would in principle need to search over all x to find those for which p y. ideally we would like to get as much of the slice as feasible since this will improve the mixing of the chain. if we concentrate on the part of the slice only very local to the current x then the samples move through the space very slowly. if we attempt to guess at random a point a long way from x and check if is in the slice this will be mostly unsuccessful. the happy compromise presented in and described in determines an appropriate local slice by adjusting the left and right regions. the technique is to start from the current x and attempt to find the largest local slice by incrementally widening the candidate slice. once we have the largest potential slice we attempt to sample from this. if the sample point within the local slice is in fact not in the slice this is rejected and the slice is shrunk. this describes a procedure for sampling from a univariate distribution p to sample from a multivariate distribution px single variable gibbs sampling can be used to sample from pxjxj repeatedly choosing a new variable xj to sample. importance sampling z where p can be evaluated but z importance sampling is a technique to approximate averages with respect to an intractable distribution px. the term sampling is arguably a misnomer since the method does not attempt to draw samples from px. rather the method draws samples from a simpler importance distribution qx and then reweights them such that averages with respect to px can be approximated using the samples from qx. consider px p x p is an intractable normalisation constant. the average of fx with respect to px is given by x fx p p qx qx x fxp x p fxpx qx qx x x draft march importance sampling figure for the current sample x a point y is sampled between and p giving a point y circle. then an interval of width w is placed around x the blue bar. the ends of the bar the interval is increased until it denote if the point is in the slice or out of the slice given an interval a sample is taken uniformly in the interval. if the hits a point out of the slice. candidate is not in the slice y the candidate is rejected and the interval is shrunk. the sampling from the interval is repeated until a candidate is in the slice and is subsequently accepted. let xl be samples from qx then we can approximate the average by fxpx x fxl p qxl p qxl fxlwl where we define the normalised importance weights p p wl with wl in principle reweighing the samples from q will give the correct result for the average with respect to p. since the weight is a measure of how well q matches p there will typically be only a single dominant weight. this is particularly evident in high dimensions there will typically only be one dominant weight with value close to and the rest will be zero particularly if the sampling distribution q is not well matched to p. as an indication of this effect consider a d-dimensional multivariate x with two samples and for simplicity we assume that both p and q are factorised over their variables. the associated unnormalised importance weights are then p d d p d d if we assume the the match between q and p better is worse by a factor in each of the dimensions at than then so that importance weight at will exponentially dominate a method that can help address this weight dominance is resampling. given the weight distribution wl one draws a set of l sample indices. this new set of indices will almost certainly contain repeats since any of the original low-weight samples will most likely not be included. the weight of each of these new samples is set uniformly to this procedure helps select only the fittest of the samples and is known as sampling importance draft march draw a vertical coordinate y uniformly from the p create a horizontal interval t xright that contains xi as follows draw r u xlef t xi rw xright xi rw while p t y do xlef t xlef t w algorithm slice sampling end while while p y do xright xright w choose a starting point and step size w. for i to l do end for end if end while end if else end while accept false while accept false do draw a random value uniformly from the unit interval t xright. if p y then accept true modify the interval t xright as follows if xi then xright xlef t else importance sampling create an initial interval step out left step out right found a valid sample shrinking sequential importance sampling one can apply importance sampling to temporal distributions for which the importance distribution samples from are paths. in many applications such as tracking one wishes to update ones beliefs as time increases and as such is required to resample and then reweight the whole path. for distributions with a markov structure one would expect that a local update is possible without needing to deal with the previous path. to show this consider the unnormalised importance weights for a sample path xl t p qxl wl p qxl p txl p p qxl wl we can recursively define the un-normalised weights using wl t wl t l t t where l t p txl p this means that in sis we need only define the conditional importance distribution the ideal setting of the sequential importance distribution is q p and although this choice is impractical in most cases. draft march importance sampling figure a dynamic bayesian network. in many applications of interest the emission distribution pvtht is non-gaussian leading to the formal intractability of filteringsmoothing. for dynamic bayes networks equation will simplify considerably. for example consider distributions with a hidden markov independence structure pvthtphtht where are observations and are the random variables. a cancelation of terms in the numerator and denominator occurs leaving simply l t t pvthl qhl tphl thl thl sequential importance sampling is also known as particle filtering. particularly in cases where the transition is easy to sample from a common sequential importance distribution is phtht in which case from equation l by t pvtht and the unnormalised weights are recursively defined wl t wl t t a drawback of this procedure is that after a small number of iterations only very few particle weights will be significantly non-zero due to the mismatch between the importance distribution q and the target distribution p. this can be addressed using resampling as described in particle filtering as an approximate forward pass particle filtering can be viewed as an approximation to the exact filtering recursion. using to represent the filtered distribution the exact filtering recursion is pvtht phtht ht a pf can be viewed as an approximation of equation in which the message is approximated by a sum of wl t ht hl t t are the normalised importance where wl t are the particles in other words the message is represented as a weighted mixture of delta-spikes where the weight and position of the spikes are the parameters of the distribution. using equation in equation we have t and hl wl z pvtht draft march phthl t t importance sampling the constant z is used to normalise the distribution although was a simple sum of delta peaks in general will not be the delta-peaks get broadened by the hidden-to-hidden and hiddento-observation factors. our task is then to approximate as a new sum of delta-peaks. below we discuss a method to achieve this for which explicit knowledge of the normalisation z is not required. this is useful since in many tracking applications the normalisation of the emission pvtht is unknown. a monte-carlo sampling approximation a simple approach to forming an approximate mixture-of-delta functions representation of equation is to generate a set of sample points using importance sampling. that is we generate a set from some importance distribution qht which gives the unnormalised importance of samples weights t hl t pvthl wl t t t phl qhl t defining the normalised weights we obtain an approximation wl t t wl wl t ht hl t ideally one would use the importance distribution that makes the importance weights unity namely qht pvtht phthl t t however this is often difficult to sample from directly due to the unknown normalisation of the emission pvtht. a simpler alternative is to sample from the transition mixture qht phthl t t to do so one first samples a component l from the histogram with weights from this sample index say l one then draws a sample from phthl t given t in this case the un-normalised weights t wl become simply t pvthl wl t this forward-sampling-resampling procedure is used in demoparticlefilter.m and in the following toy example. example toy face-tracking example. at time t a binary face template is in a location ht which describes the upper-left corner of the template using a two-dimensional vector. at time t the position of the face is known see the face template is known. in subsequent times the face moves randomly according to ht ht t where t n t i is a two dimensional zero mean unit covariance noise vector. in addition a fraction of the binary pixels in the whole image are selected at random and their states flipped. the aim draft march importance sampling figure tracking an object with a particle filter containing particles. the small circles are the particles scaled by their weights. the correct corner position of the face is given by the the filtered average by the large circle o and the most likely particle by initial position of the face without noise and corresponding weights of the particles. face with noisy background and the tracked corner position after timesteps. the forwardsampling-resampling pf method is used to maintain a healthy proportion of non-zero weights. see demoparticlefilter.m is to try to track the upper-left corner of the face through time. we need to define the emission distribution pvtht on the binary pixels with vi consider the following compatibility function ht vt t vht where vht is the vector representing the image with a clean face placed at position ht. this measures the overlap between the face template and the noisy image restricted to the template pixels. the compatibility function is maximal when the observed image vt has the face placed at position ht. we can therefore tentatively define pvtht ht a subtlety is that ht is continuous and in the compatibility function we first map ht to the nearest integer pixel representation. we have not specified the normalisation constant of this distribution which fortunately this is not required by the particle filtering algorithm. in particles are used to track the face. the particles are plotted along with their corresponding weights. for each t of the pixels are selected at random in the image and their states flipped. using the forward-samplingresampling method we can successfully track the face despite the presence of the background clutter. real tracking applications involve complex issues including tracking multiple objects transformations of the object rotation morphology changes. nevertheless the principles are largely the same and many tracking applications work by seeking simple compatibility functions often based on the colour histogram in a template. indeed tracking objects in complex environments was one of the original applications of particle filters draft march weights code potsample.m exact sample from a set of potentials ancestralsample.m ancestral sample from a belief net jtsample.m sampling from a consistent junction tree gibbssample.m gibbs sampling from a set of potentials demometropolis.m demo of metropolis sampling for a bimodal distribution metropolis.m metropolis sample logp.m log of a bimodal distribution demoparticlefilter.m demo particle filtering method placeobject.m place an object in a grid compat.m compatibility function demosamplehmm.m naive gibbs sampling for a hmm exercises exercise method. let u u and log sin show that log cos n exercises and suggest an algorithm to sample from a univariate normal distribution. exercise consider the distribution for fixed in a given state write down a distribution on the remaining variables and explain how forward sampling can be carried out for this new distribution. exercise consider an ising model on an m m square lattice with nearest neighbour interactions i xj i j px exp now consider the m m grid as a checkerboard and give each white square a label wi and each black square a label bj so that each square is associated with a particular variable. show that that is conditioned on the white variables the black variables are independent. the converse is also true that conditioned on the black variables the white variables are independent. explain how this can be exploited by a gibbs sampling procedure. this procedure is known as checkerboard or black and white sampling. exercise consider the symmetric gaussian proposal distribution and the target distribution q x n q p n px n log px where dim x n. show that discuss how this result relates to the probability of accepting a metropolis-hastings update under a gaussian proposal distribution in high-dimensions. draft march exercises exercise the file demosamplehmm.m performs naive gibbs sampling of the posterior for a hmm. at each gibbs update a single variable ht is chosen with the remaining h variables clamped. the procedure starts from t and sweeps forwards through time. when the end time t t is reached the joint state is taken as a sample from the posterior. the parameter controls how deterministic the hidden transition matrix phtht will be. adjust demosamplehmm.m to run times each time for the same computing a mean absolute error over these runs. then repeat this for discuss why the performance of this gibbs sampling routine deteriorates with increasing draft march exercises draft march chapter deterministic approximate inference introduction deterministic approximate inference methods are an alternative to the stochastic techniques discussed in whilst stochastic methods are powerful and often generally applicable they nevertheless produce sample estimates of a quantity. even if we are able to perform perfect sampling we would still only obtain an approximate result due to the inherent uncertainty introduced by sampling. furthermore in practice drawing exact samples is typically computationally intractable and assessing the quality of the sample estimates is difficult. in this chapter we discuss some alternative deterministic approximate inference schemes. the first laplace s method is a simple perturbation technique. the second class of methods are those that produce rigorous bounds on quantities of interest. such methods are interesting since they provide certain knowledge it may be sufficient for example to show that a marginal probability is greater than in order to make an informed decision. a further class of methods are the consistency methods such as loopy belief propagation. such methods have revolutionised certain fields including error providing performance unobtainable from sampling based procedures. it is important to bear in mind that no single approximation technique deterministic or stochastic is going to beat all others on all problems given the same computational resources. in this sense insight as to the properties of the approximation method used is useful in matching an approximation method to the problem at hand. the laplace approximation consider a distribution on a continuous variable of the form px ex z e the laplace method makes a gaussian approximation of px based on a local perturbation expansion around a mode x first we find the mode numerically giving x argmin x ex then a taylor expansion up to second order around this mode gives ex ex x ex where h exx is the hessian evaluated at the mode. at the mode ex and an approximation of the distribution is given by the gaussian x h x x h p z e x x n properties of kullback-leibler variational inference figure fitting a mixture of gaussians px with a single gaussian. the green curve minimises klqp corresponding to fitting a local model. the red curve minimises klpq corresponding to moment matching. which has mean x and covariance h with z h similarly we can use the above ex h expansion to estimate the integral e ex x x ex e x x e although the laplace approximation fits a gaussian to a distribution it is not necessarily the best gaussian approximation. as we ll see below other criteria such as based on minimal kl divergence between px and a gaussian approximation may be more appropriate depending on the context. a benefit of laplace s method is its relative speed and simplicity compared with other approximate inference techniques. properties of kullback-leibler variational inference variational methods can be used to approximate a complex distribution px by a simpler distribution qx. given a definition of discrepancy between an approximation qx to px any free parameters of qx are then set by minimising the discrepancy. a particularly popular measure of the discrepancy between an approximation qx and the intractable distribution px is the kullback-leibler divergence klqp it is straightforward to show that klqp and is zero if and only if the distributions p and q are identical see note that whilst the kl divergence cannot be negative there is no upper bound on the value it can potentially take so that the discrepancy can be infinitely bad. bounding the normalisation constant for a distribution of the form px z e we have klqp log z since klqp this immediately gives the bound log z entropy energy which is called the free energy bound in the physics using the notation hq for the entropy of q we can write the bound more compactly as log z hq the klqp method provides therefore a lower bound on the normalisation constant. for some models it is possible alternative methods see for example and to also form an upper draft march properties of kullback-leibler variational inference bound on the normalisation constant. with both an upper and lower bound on the normalisation terms we are able to bracket marginals l pxi u see the tightness of the resulting bracket gives an indication as to how tight the bounding procedures are. even in cases where the resulting bracket is weak for example it might be that the result is that pcancer true this may be sufficient for decision making purposes since the probability of cancer is sufficiently large to merit action. bounding the marginal likelihood in bayesian modelling the likelihood of the model m with parameters generating data d is given by pdm pd likelihood p prior this quantity is fundamental to model comparison. however in cases where is high-dimensional the integral over is difficult to perform. using bayes rule p pd pdm and considering klq q p q pd log pdm the non-negativity of the kullback-leibler divergence gives the bound log pdm q pd this bound holds for any distribution q and saturates when q p since using the optimal setting is assumed computationally intractable the idea in variational bounding is to choose a distribution family for q for which the bound is computationally tractable example factorised or gaussian and then maximise the bound with respect to any free parameters of q the resulting bound then can be used as a surrogate for the exact marginal likelihood in model comparison. gaussian approximations using kl divergence minimising klqp using a simple approximation qx of a more complex distribution px by minimising klqp tends to give a solution for qx that focuses on a local mode of px thereby underestimating the variance of px. to show this consider approximating a mixture of two gaussians with equal variance n n m px qx n see with a single gaussian we wish to find the optimal m that minimise klqp if we consider the case that the two gaussian components of px are well separated then setting qx to be centred on the left mode at the gaussian qx only has appreciable mass close to so that the second mode at has negligible contribution to the kullback-leibler divergence. in this sense one can approximate px qx so that klqp log draft march properties of kullback-leibler variational inference representing a distribution of the form figure a planar pairwise markov random field on a set of variables in statistical physics such lattice models include the ising model on binary spin variables xi with xj ewij xixj i j xj. on the other hand setting m which is the correct mean of the distribution px very little of the mass of the mixture is captured unless is large giving a poor fit and large kl divergence. another way to view this is to consider klqp provided q is close to p around where q has significant mass the ratio qxpx will be order and the kl divergence small. setting m means that qxpx is large where q has significant mass and is therefore a poor fit. the optimal solution in this case is to place the gaussian close to a single mode. note however that for two modes that are less well-separated the optimal solution will not necessarily be to place the gaussian around a local mode. in general the optimal gaussian fit needs to be determined numerically that is there is no closed form solution to finding the optimal mean and parameters. minimising klpq for fitting a gaussian q to p based on klpq we have log const. klpq px px minimising this with respect to m and we obtain m so that the optimal gaussian fit matches the first and second moments of px. in the case of the mean of px is a zero and the variance of px is large. this solution is therefore dramatically different from that produced by fitting the gaussian using klqp. the fit found using klqp focusses on making q fit p locally well whereas klpq focusses on making q fit p well to the global statistics of the distribution at the expense of a good local match. for simplicity consider a factorised approximation qx moment matching properties of minimising klpq i qxi. then the first entropic term is independent of qx so that up to a constant independent of qx the above is i klpq klpxiqxi i so that optimally qxi pxi. that is the optimal factorised approximation is to set the factors of qxi to the marginals of pxi for any approximating distribution in the exponential family minimising klpq corresponds to moment matching see in practice one generally cannot compute the moments of px the distribution px is considered intractable so that fitting q to p based only on klpq does not itself lead to a practical algorithm for approximate inference. nevertheless as we will see it is a useful subroutine for local approximations in particular expectation propagation. draft march variational bounding using klqp variational bounding using klqp in this section we discuss how to fit a distribution qx from some assumed family to an intractable distribution px. as we saw above for the case of fitting gaussians the optimal q needs to be found numerically. this itself can be a complex task formally this can be just as difficult as performing inference directly with the intractable p and the reader may wonder why we trade a difficult inference task for a potentially difficult optimisation problem. the general idea is that the optimisation problem has some local smoothness properties that enable one to rapidly find a reasonable optimum based on generic optimisation methods. to make these ideas more concrete we discuss a particular case of fitting q to a formally intractable p in below. pairwise markov random field a canonical intractable distribution is the pairwise markov random field defined on binary variables xi i d px ij wij xixj zw b e ij wij xixj zw b e i bixi i bixi here the partition function zw b ensures normalisation x i the terms i are constant and without loss of generality we may set wii to this since gives an undirected distribution with connection geometry defined by the weights w. in practice the weights often define local interactions on a lattice see a case for which inference in this model is required is given in pyx i px e example image denoising. consider a binary image where x describes the state of the clean pixels encoding. we assume a noisy pixel generating process that takes each clean pixel xi and flips its binary state pyixi pyixi e yixi the probability that yi and xi are in the same state is e e our interest is to the posterior distribution on clean pixels pxy. in order to do this we need to make an assumption as to what clean images look like. we do this using a mrf i j wij xixj for some settings of wij with j indicating that i and j are neighbours. this encodes the assumption that clean images tend to have neighbouring pixels in the same state. an isolated pixel in a different state to its neighbours is unlikely under this prior. we now have the joint distribution px y pyixi i see from which the posterior is given by pxy pyxpx x pyxpx e i j wij xixj i yixi quantities such as the map state a posteriori probable image marginals pxiy and the normalisation constant are of interest. inference with a general mrf is formally computationally intractable exact polynomial time methods are known two celebrated results that we mention in passing are that for the planar mrf model with pure interactions the partition function is computable in polynomial as is the map state for attractive planar ising models w see draft march variational bounding using klqp figure a distribution on pixels. the filled nodes indicate observed noisy pixels the unshaded nodes a markov random field on latent clean pixels. the task is to infer the clean pixels given the noisy pixels. the mrf encourages the posterior distribution on the clean pixels to contain neighbouring pixels in the same state. kullback-leibler based methods for the mrf we have klqp bi log z rewriting this gives a bound on the log-partition function ij wij wij ij i i energy bi log z entropy the bound saturates when q p. however this is of little help since we cannot compute the averages of variables with respect to this intractable distribution the idea of a variational method is to assume a simpler tractable distribution q for which these averages can be computed along with the entropy of q. minimising the kl divergence with respect to any free parameters of qx is then equivalent to maximising the lower bound on the log partition function. factorised approximation a simple naive assumption is the fully factorised distribution qx qixi i the graphical model of this approximation is given in in this case wij ij bi i log z for a factorised distribution and bearing in mind that xi i i j i j for a binary variable one may use the convenient parametrization qixi e i e i e i so that qxi qxi tanh i qx tion. tion. figure naive mean field approximation a spanning tree approximac a decomposable approxima i qixi. draft march variational bounding using klqp this gives the following lower bound on the log partition function wij tanh i tanh j i i h i i e i e i tanh i log z b h i log bi tanh i where h i is the binary entropy of a distribution parameterised according to equation finding the best factorised approximation in the minimal kullback-liebler divergence sense then corresponds to maximising the bound b with respect to the variational parameters the bound b equation is generally non-convex in and riddled with local optima. finding the globally optimal is therefore typically a computationally hard problem. it seems that we have simply replaced a computationally hard problem of computing log z by an equally hard computational problem of maximising b indeed the graphical structure of this optimisation problem matches exactly that of the original mrf. however the hope is that by transforming a difficult discrete summation into a continuous optimisation problem we will be able to bring to the table techniques of continuous variable numerical optimisation to find a good approximation. a particularly simple optimisation technique is to differentiate the bound equation and equate to zero. straightforward algebra leads to requirement that the optimal solution satisfies the equations wij tanh j i i bi one may show that updating any i according to equation increases b this is called asychronous updating and is guaranteed to lead to a minimum of the kl divergence once a converged solution has been identified in addition to a bound on log z we can approximate tanh i validity of the factorised approximation when might one expect such a naive factorised approximation to work well? clearly if wij is very small for i j the distribution p will be effectively factorised. however a more interesting case is when each variable xi has many neighbours. it is useful to write the mrf as px ij wij xixj z i xi d j wij xj z i xizi where the local fields are defined as d zi wijxj j zi each of the terms xj in the summation we now invoke a circular self-consistent argument let s assume that px is factorised. then for j wijxj is independent. provided the wij are not strongly correlated the conditions of validity of the central limit theorem and each zi will be gaussian distributed. assuming that each wij is o the mean of zi is the variance is i wij o ik draft march o e z j d variational bounding using klqp hence the variance of the field zi is much smaller than its mean value. as d increases the fluctuations around the mean diminish and we may write pxi px z i i the assumption that p is approximately factorised is therefore self-consistent in the limit of mrfs with a large number of neighbours. hence the factorised approximation would appear to be reasonable in the extreme limits a very weakly connected system wij or a large densely connected system. the fully factorised approximation is also called the naive mean field theory since for the mrf case it assumes that we can replace the effect of the neighbours by a mean of the field at each site. general mean field equations for a general intractable distribution px on discrete or continuous x the kl divergence between a factorised approximation qx and px is isolating the dependency of the above on a single factor qxi we have i qxi i kl i qxipx qxi exp qxj qxj qxj qxi up to a normalisation constant this is therefore the kl divergence between qxi and a distribution proportional to exp so that the optimal setting for qxi satisfies these are known as the mean-field equations and define a new approximation factor in terms of the previous approximation factors. note that if the normalisation constant of px is unknown this presents no problem since this constant is simply absorbed into the normalisation of the factors qxi. in other words one may replace px with the unnormalised p in equation beginning with an initial randomly chosen set of distributions qxi the mean-field equations are iterated until convergence. asynchronous updating is guaranteed to decrease the kl divergence at each stage asynchronous updating guarantees approximation improvement for a factorised variational approximation equation we claim that each update equation reduces the kullback-leibler approximation error. to show this we write a single updated distribution as the joint distribution under this single update is qnew i zi qold j qnew qnew i qold j klqnewp kl qoldp using klqnewp qnew i i our interest is the change in the approximation error under this single mean-field update log qold j j qold qold j qnew i draft march variational bounding using klqp figure a toy intractable distribution. a structured singly-connected approximation. qold j ziqnew qold i log qold i i i q qold i i q qold i qold j qnew i i i q i i and defining the un-normalised distribution q then i qnew log qold i i i log zi log qold i i q i log zi qold kl i qnewp i kl qoldp hence kl qold j qold i so that updating a single component of q at a time is guaranteed to improve the approximation. note that this result is quite general holding for any distribution px. in the case of a markov network the guaranteed approximation improvement is equivalent to a guaranteed increase speaking a non-decrease in the lower bound on the partition function. intractable energy whilst the fully factorised approximation is rather severe even this may not be enough to render the qxj for field equations tractably implementable. to do so we need to be able to compute p some models of interest this is still not possible and additional approximations are required. example intractable mean-field approximation. consider the posterior distribution from a relevance vector machine classification problem pwd n the terms render pwd non-gaussian. we can find a gaussian approximation qw n n by minimising the kullback-leibler divergence klqwpwd log n the entropic term is straightforward since this is the negative entropy of a gaussian. however we also require the energy which includes a contribution there is no closed form expression for this. one approach is to use additional variational approximations another approach is to recognise that the multi-variate average can be reduced to a uni-variate gaussian average n m m txn xn log draft march mutual information maximisation a kl variational approach and the uni-variate gaussian average can be carried out using quadrature. this approach was used in to approximate the posterior distribution of bayesian neural networks. structured variational approximation one can extend the factorised kl variational approximation by using non-factorised those for which averages of the variables can be computed in linear time include spanning trees and decomposable graphs for example for the distribution z a tractable q distribution would be z in this case we have klqp i j since q is singly-connected computing the marginals and entropy is straightforward the entropy requires only pairwise marginals on graph neighbours. more generally one can exploit any structural approximation with an arbitrary hypertree width w by use of the junction tree algorithm in combination with the kl divergence. however the computational expense increases exponentially with the hypertree mutual information maximisation a kl variational approach here we take a short interlude here to demonstrate an application of the kullback-leibler variational approach in information theory. a fundamental goal is to maximise information transfer measured by the also ix y hx hxy where the and are defined hx hxy here we are interested in the situation in which px is fixed but pyx has adjustable parameters that we wish to set in order to maximise ix y in this case hx is constant and the optimisation problem is equivalent to minimising the conditional entropy hxy unfortunately in many cases of practical interest hxy is computationally intractable. see below for a motivating example. we discuss in a general procedure based on the kullback-leibler divergence to approximately maximise the mutual information. example consider a neural transmission system in which xi denotes an emitting neuron in a non-firing state or firing state and yj a receiving neuron. if each receiving neuron fires independently depending only on the emitting neurons we have pyix draft march pyx i mutual information maximisation a kl variational approach algorithm im algorithm bution px i find the optimal parameters wi that maximise the mutual figure an information transfer problem. for a fixed distrii pxi and parameterised distributions pyjx information between the variables x and y. such considerations are popular in theoretical neuroscience and aim to understand how the receptive fields wi of a neuron relate to the statistics of the environment px. choose a class of approximating distributions q example factorised initialise the parameters repeat for fixed qxy find new argmax ix y for fixed qnewxy argmax qxy q ix y where q is a chosen class of distributions. until converged where for example we could use pyi wt i x px pxi if we make the simple assumption that emitting neurons fire independently i then for pxy all components of the x variable are dependent see this defines a complex high-dimensional px y for which the conditional entropy is typically intractable. the information maximisation algorithm consider this immediately gives a bound klpxyqxy pxy log pxy x x multiplying both sides by py we obtain pxy log qxy xy pypxy log pxy px y log qxy xy from the definition the left of the above is hxy hence ix y hx ix y draft march loopy belief propagation c a e d b f figure classical belief propagation can be derived by considering how to compute the marginal of a variable on a mrf. in this case the marginal pd depends on messages transmitted via the neighbours of d. by defining local messages on the links of the graph a recursive algorithm for computing all marginals can be derived see text. from this lower bound on the mutual information we arrive at the information maximisation given a distribution px and a parameterised distribution pyx we seek to maximise ix y with respect to a co-ordinate wise optimisation procedure is presented in the blahut-arimoto algorithm in information theory for example is a special case in which the optimal decoder qxy pyx is used. in applications where the blahut-arimoto algorithm is intractable to implement the im algorithm can provide an alternative by restricting q to a tractable family of distributions in the sense that the lower bound can be computed. the blahut-arimoto algorithm is analogous to the em algorithm for maximum likelihood and guarantees a non-decrease of the mutual information at each stage of the update. similarly the im procedure is analogous to a generalised em procedure and each step of the procedure cannot decrease the lower bound on the mutual information. linear gaussian decoder a special case of the im framework is to use a linear gaussian decoder qxy n uy log qxy uyt uy plugging this into the mi bound equation and optimising with respect to and u we obtain uy ix y hx log det u where using this in the mi bound we obtain const. up to irrelevant constants this is equivalent to linsker s as-if-gaussian approximation to the mutual information one can therefore view linsker s approach as a special case of the im algorithm restricted to linear-gaussian decoders. in principle one can therefore improve on linsker s method by considering more powerful non-linear-gaussian decoders. applications of this technique to neural systems are discussed in loopy belief propagation belief propagation is a technique for exact inference of marginals pxi for singly-connected distributions px. there are different formulations of bp the most modern treatment being the sum-product algorithm on the corresponding factor graph as described in an important observation is that the algorithm is purely local the updates are unaware of the global structure of the graph depending only on the local neighbourhood structure. this means that even if the graph is multiply-connected is loopy one can still apply the algorithm and see what happens provided the loops in the graph are relatively long one may hope that running loopy bp will converge to a good approximation of the true marginals. in general this cannot be guaranteed however when the method converges the results can be surprisingly accurate. in the following we will show how loopy bp can also be motivated by a variational objective. to do so the most natural connection is with the classical bp algorithm than the factor graph sum-product algorithm. for this reason we briefly describe below the classical bp approach. draft march loopy belief propagation x x i x i xi xi xj xj xi i x i x figure loopy belief propagation. once a node has received incoming messages from all neighbours the one it wants to send a message to it may send an outgoing message to a neighbour xj xi xi xi xi xj xi classical bp on an undirected graph graph. consider calculating the marginal pd work in we denote both a node and its state by the same symbol so that bp can be derived by considering how to calculate a marginal in terms of messages on an undirected abcef pa b c d e f for the pairwise markov netb b denotes summation over the states of the variable b. carrying out such a summation results in a message b d containing information from node b to node d. in order to compute the summation efficiently we distribute summations as follows pd z b a f d d f b dd a dd f dd e c e dd e c ee where we define messages sending information from node to node as a function of the state of node in general a node xi passes a message to node xj via xi xj xj xi k xk xi this algorithm is equivalent to the sum-product algorithm provided the graph is singly-connected. loopy bp as a variational procedure a variational procedure that corresponds to loopy bp can be derived by considering the terms of a standard variational approximation based on the kullback-leibler divergence for a pairwise mrf defined on potentials xj and approximating distribution qx the kullback-leibler bound is i j px z xj log z i j energy since each contribution to the energy depends on qx only via the pairwise marginals qxi xj. this suggests that these marginals should form the natural parameters of any approximation. can we then find an expression for the entropy in terms of these pairwise marginals? consider a case in which the required marginals are draft march loopy belief propagation given these marginals the energy term is straightforward to compute and we are left with requiring only the entropy of q. either by appealing to the junction tree representation or by straightforward algebra one can show that we can uniquely express q in terms of the marginals as qx an intuitive way to arrive at this result is by examining the numerator of equation the variable appears twice as does the variable and since any joint distribution cannot have replicated variables on the left of any conditioning we must compensate for the additional and variables by dividing by these marginals. in this case the entropy of qx can be written as hqx more generally any decomposable graph can be represented as c qxc s qxs qx where the qxc are the marginals defined on cliques of the graph with xc being the variables of the clique and the qxs are defined on the separators of neighbouring cliques. the expression for the entropy of the distribution is then given by a sum of marginal entropies minus the entropy of the marginals defined on the separators. bethe free energy consider now a mrf corresponding to a non-decomposable graph for example the px z the energy requires therefore that we know assuming that these marginals are given can we find an expression for the entropy of the joint distribution in terms of its pairwise marginals qxi xj? in general this is not possible since the graph corresponding to the marginals contains loops that the junction tree representation would result in cliques greater than size however a simple no overcounting approximation is to write qx subject to the constraints qxi xj qxj xi an entropy approximation using this representation is therefore hqx hqxi with this approximation the log partition function is known in statistical physics as the bethe free energy. our interest is then to maximise this expression with respect to the parameters qxi xj xi qxi xj qxj. these may be enforced using lagrange i j qxi xi ij qxi xj draft march multipliers. one can write the bethe free energy as subject to marginal consistency hqxi hqxi xj fq i j i i j loopy belief propagation where i j denotes the unique neighbouring edges on the graph edge is counted only once. this is no longer a bound on the log partition function since the entropy approximation is not a lower bound on the true entropy. the task is now to maximise this approximate bound with respect to the parameters namely all the pairwise marginals qxi xj and the lagrange multipliers a simple scheme to maximise equation is to use a fixed point iteration by equating the derivatives of the bethe free energy with respect to the parameters qxi xj to zero and likewise for the lagrange multipliers. one may show that the resulting set of fixed point equations on eliminating q is equivalent to belief for which in general a node xi passes a message to node xj using xi xj xj xi k xk xi at convergence the marginal pxi is then approximated by qxi i nej xj xi the prefactor being determined by normalisation. for a singly-connected distribution p this message passing scheme converges and the marginal corresponds to the exact result. for multiply connected structures running this loopy belief propagation will generally result in an approximation. naturally we can dispense with the bethe free energy if desired and run the associated loopy belief propagation algorithm directly on the undirected graph. the convergence of loopy belief propagation which can be heavily dependent on the topology of the graph and also the message updating the potential benefit of the bethe free energy viewpoint is that it gives an objective that is required to be optimised opening up the possibility of more general optimisation techniques than bp. the so-called double-loop techniques iteratively isolate convex contributions to the bethe free energy interleaved with concave contributions. at each stage the resulting optimisiations can be carried out validity of loopy belief propagation for a mrf which has a loop computationally this means that a perturbation in a variable on the loop eventually reverberates to the same variable. however if there are a large number of variables in the loop and the individual neighbouring links are not all extremely strong the numerical effect of the loop is small in the sense that influence of the variable on itself is negligible. in such cases one would expect the loopy bp approximation to be accurate. an area of particular success for loopy belief propagation inference is in error correction based on low density parity check codes which are designed to have this longloop in many examples of practical interest example an mrf with nearest neighbour interactions on a lattice however loops can be very short. in such cases a naive implementation of loopy bp will fail. a natural extension is to cluster variables to alleviate some of the issues arising from strong local dependencies this technique is called the kikuchi or cluster variation more elaborate ways of clustering variables can be considered using region example the file demomfbpgibbs.m compares the performance of naive mean field theory belief propagation and unstructured gibbs sampling on marginal inference in a pairwise markov network pw x y z wxw x wyw y wzw z xyx y xzx z yzy z in which all variables take states. in the experiment the tables are selected from a uniform distribution raised to a power for close to zero all the tables are essentially flat and therefore the variables become independent a situation for which mf bp and gibbs sampling are ideally suited. as is increased to draft march expectation propagation the markov network figure that we wish to approximate the marginals pw px py pz for. all tables are drawn from a uniform distribution raised to a power on there are the right is shown the naive mean field approximation factorised structure. states of the distribution. shown is a randomly sampled distribution for which has many isolated peaks suggesting the distribution is far from factorised. in this case the mf and gibbs sampling approxc as is increased to typically only one state of the distribution imations may perform poorly. dominates. see demomfbpgibbs.m. figure multiply-connected factor expectation graph representing px. propagation approximates in terms of a tractable factor graph. the open squares indicate that the factors are parameters of the approximation. the basic ep approximation is to replace all factors in px by product factors. tree structured ep. the dependencies amongst the variables increase and the methods perform worse especially mf and gibbs. as is increased to the distribution becomes sharply peaked around a single state such that the posterior is effectively factorised see this suggests that a mf approximation also gibbs sampling should work well. however finding this state is computationally difficult and both methods often get stuck in local minima. belief propagation seems less susceptible to being trapped in local minima in this regime and tends to outperform both mf and gibbs sampling. expectation propagation the messages in schemes such as belief propagation are not always representable in a compact form. the switching linear dynamical system is such an instance in which the messages require an exponential amount of storage. this limits bp to cases such as discrete networks or more generally exponential family messages. expectation propagation extends the applicability of bp to cases in which the messages are not in the exponential family by projecting the messages back to the exponential family at each stage. this projection is obtained by using a kullback-leibler consider a distribution of the form i px z ix in ep one identifies those factors ix which if replaced by simpler factors ix would render the distribution px tractable. one then sets any free parameters of ix by minimising the draft march wxyzoriginal graphwxyzfactorised distribution expectation propagation leibler divergence klp p. for example consider a pairwise mrf px z with factor graph as depicted in if we replace all terms ijxi xj by approximate factors ijxi ijxj then the resulting joint distribution p would be factorised and hence tractable. since the variable xi appears in more than one term from px we need to index the approximation factors. a convenient way to do this is p z which is represented in the idea in ep is now to determine the optimal approximation term by the self-consistent requirement that on replacing it with its exact form there is no difference to the marginal of p. consider the approximation parameters and to set these we first replace the contribution by this gives p z now consider the kullback-leibler divergence between this distribution and our approximation kl p p p p p since our interest is in updating and we isolate the contribution from these parameters in the kullback-leibler divergence which gives kl p p log z log p const. also since p is factorised up to a constant proportionality factor the dependence of z on and is z differentiating the kullback-leibler divergence equation with respect to and equating to zero we obtain p similarly optimising w.r.t. gives p these equations only determine the approximation factors up to a proportionality constant. we can therefore write the optimal updates as p and p draft march where and are proportionality terms. we can determine the proportionalities by the requirement that the term approximation has the same effect on the normalisation of p as it has expectation propagation on p that which on substituting in the updates equation and equation reduces to z where and z p p any choice of local normalisations that satisfies equation suffices to ensure that the scale of the term approximation matches. for example one may set z once set an approximation for the global normalisation constant of p is z z the above gives a procedure for updating the terms and one then chooses another term and replaces it with its approximation until the parameters of the approximation converge. the generic procedure is outlined in comments on ep for the mrf example above ep corresponds to belief propagation sum-product form on the factor graph. this is intuitively clear since in both ep and bp the product of messages incoming to a variable is proportional to the approximation of the marginal of that variable. a difference however is the schedule in ep all messages corresponding to a term approximation are updated simultaneously the above and whereas in bp they are updated in arbitrary order. ep is a useful extension of bp to cases in which the bp messages cannot be easily represented. in the case that the approximating distribution p is in the exponential family the minimal kullback-leibler criterion equates to matching moments of the approximating distribution to p see for a more detailed discussion. in general there is no need to replace all terms in the joint distribution with factorised approximations. one only needs that the resulting approximating distribution is tractable this results in a structured expectation propagation algorithm see ep and its extensions are closely related to other variational procedures such as and fractional designed to compensate for message overcounting effects. draft march algorithm expectation propagation approximation of px z decide on a set of terms ixi to replace with ixi in order to reveal a tractable distribution i ixi. map for mrfs i px z ixi initialise the all parameters ixi. repeat select a term ixi from p to update. replace the term ixi by the tractable term ixi to form ixi jxj ixi jxj j find the parameters of ixi by ixi argmin ixi px kl p p jxj x p set any proportionality terms of ixi by requiring ixi i where x z i until converged return j jxj z x i px ixi ixi as an approximation to px where z approximates the normalisation constant z. map for mrfs consider a pairwise mrf px eex with i j f xj i ex gxi i where i j denotes neighbouring variables. here the terms f xj represent pairwise interactions. the i represent unary interactions written for convenience in terms of a pairwise interaction with terms gxi a fixed typically the term fxi xj is used to ensure that neighbouring variables xi and xj are in similar states the term gxi i such models have application in areas such as computer vision and image restoration in which an observed noisy image is to be cleaned to do so we seek a clean image x for which each clean pixel value xi is close to the observed noisy pixel value i is used to bias xi to be close to a desired state i whilst being in a similar state to its clean neighbours. map assignment the map assignment of a set of variables xd corresponds to that joint x that maximises ex. for a general graph connectivity we cannot naively exploit dynamic programming intuitions to find an draft march map for mrfs b c f a e d a e b c f figure a graph with bidirectional weights a graph cut partitions the nodes wij wji. into two groups s and t the weight of the cut is the sum of the edge weights from s to t intuitively it is clear that after assigning nodes to state blue and that the weight of the cut corresponds to the summed weights of neighbours in different states. here we highlight those weight contributions. the non-highlighted edges do not contribute to the cut weight. note that only one of the edge directions contributes to the cut. d exact solution since the graph is loopy. as we see below in special cases even though the graph is loopy this is possible. in general however approximate algorithms are required. a simple general approximate solution can be found as follows first initialise all x at random. then select a variable xi and find the state of xi that maximally improves ex keeping all other variables fixed. one then repeats this selection and local maximal state computation until convergence. this is called iterated conditional due to the markov properties its clear that we can improve on this icm method by simultaneously optimising all variables conditioned on their respective markov blankets to the approach used in black-white sampling. another improvement is to update only a subset of the variables where the subset has the form of singly-connected structure. by recursively clamping variables to reveal a singly-connected structure on un-clamped variables one may find an approximate solution by solving a sequence of tractable problems. remarkably in the special case of binary variables and positive w discussed below an efficient exact algorithm exists for finding the map state regardless of the topology. attractive binary mrfs consider finding the map of a mrf with binary variables domxi and positive connections wij in this case our task is to find the assignment x that maximises cixi i j wiji xj i ex where i j denotes neighbouring variables and real ci. note that for binary variables xi for this particular case an efficient map algorithm exists for arbitrary topology of the algorithm first translates the map assignment problem into an equivalent min s-t-cut for which efficient algorithms exist. in min s-t-cut we need a graph with positive weights on the edges. this is clearly satisfied i cixi need to be addressed. draft march to translate the map assignment problem to a min-cut problem we need to deal with the additional linear i cixi. first consider the effect of including a new node x and connecting this to each existing dealing with the bias terms i xj xixj xj if wij although the bias terms cii x i i cixi node i with weight ci. this adds then a term if we set x in state this will then add terms ci xi x i map for mrfs s s a e a e b c b c f f d d t t a graph with bidirectional figure weights wij wji augmented with a source node s and sink node t. each node has a corresponding bias whose sign is indicated. the source node is linked to the nodes corresponding to positive bias and the a graph nodes with negative bias to the sink. cut partitions the nodes into two groups s and t where s is the union of the source node and nodes in state t is the union of the sink node and nodes in state the weight of the cut is the sum of the edge weights from s to t the red lines indicate contributions to the cut and can be considered penalties since we wish to find the minimal cut. for example a being in state does not incur a penalty since ca on the other hand variable f being in state incurs a penalty since cf otherwise if we set x in state we obtain i ci xi i cixi const. since our requirement is that we need the weights to be positive we see that we can achieve this by defining two additional nodes. we define a source node xs set to state and connect it to those xi which have positive ci defining wsi ci. in addition we define a sink node xt and connect all nodes with negative ci to xt using weight wit ci is therefore positive. for the source node clamped to xs and the sink node to xt then including the source and sink we have wiji xj const. ex i j is equal to the energy function equation definition cut. for a graph g with vertices vd and weights wij a cut is a partition of the vertices into two disjoint groups called s and t the weight of a cut is then defined as the sum of the weights that leave s and land in t see the weight of a cut corresponds to the sum of weights between mismatched neighbours see that is wiji xj cutx cutx i j i j since i xj i xj we can define the weight of the cut equivalently as wij i xj wiji xj const. i j draft march map for mrfs clean imfigure age. restored image using icm. see demomrfclean.m. noisy image. so that the minimal cut assignment will correspond to which take operations or less. this means that one can find the exact map assignment of an attractive binary mrf efficiently in operations. in maxflow.m we implement the ford-fulkerson i j wiji xj. in the mrf case our translation into a weighted graph with positive interactions then requires that we identify the source and all other variables assigned to state with s and the sink and all variables in state with t see a fundamental result is that the min s-t-cut solution corresponds to the max-flow solution from the source s to the sink t there are efficient algorithms for max-flow see for example breadth first search see also ex wiji xj example dirty pictures. in we present a noisy binary y image that we wish to clean. to do so we use an objective i yi ij i the variables xi i are defined on a grid and where wij if xi and xj are neighbours on the grid. using i yi xi const. we have a standard binary mrf map problem with bias b once can then find the exact optimal x by the min-cut procedure. however our implementation of this is slow and instead we use the simpler icm algorithm with results as shown in potts model an extension of the previous model is to the case when the variables are non-binary which is termed the potts model ex wiji xj i i j i i are known. this model has immediate application in non-binary image restorawhere wij and the tion and also in clustering based on a similarity score. whilst no efficient exact algorithm is known a useful approach is to approximate the problem as a sequence of binary problems as we describe below. potts to binary mrf translation consider the representation xi si sixold i or where si and for a given n. this restricts xi to be either the state xold depending on the binary variable si. using a new binary variable s we can therefore restrict x to a subpart i draft march map for mrfs figure noisy image. restored image. the method was used with suitable interactions w and bias c to ensure reasonable results. from of the full space and write a new objective function in terms of s alone es i j i sj ij w i isi const. c for ij this new problem is of the form of an attractive binary mrf which can be solved exactly using the graph cuts procedure. the idea is then to choose another value random and then find the optimal s for the new in this way we are guaranteed to iteratively increase e. for a given and xold the transformation of the potts model objective is given by using si and considering i xj i sj sjxold si sixold i xold xold sisjuij aisi bjsj const. j xold i j with uij xold j xold i xold j xold j xold i sisj and similarly defined ai bi. by enumeration it is straightforward to show that uij is either or using the mathematical identity sisj sj si sj we can write i xj uij sj si sj aisi bjsj const. hence terms wiji xj translate to positive interaction terms i sj all the unary terms are easily exactly mapped into corresponding unary terms i defined as the sum of all unary terms in si. this shows that the positive interaction wij in terms of the original variables x maps to a positive interaction in the new variables s. hence we can find the maximal state of s using a graph cut algorithm. a related different procedure is outlined in isi for draft march exercises example model for image reconstruction. an example image restoration problem for nearest neighbour interactions on a pixel lattice and suitably chosen w c is given in the images are non-binary and therefore the optimal map assignment cannot be computed exactly in an efficient way. the alpha-expansion technique was used here combined with an efficient min-cut approach see for details. further reading approximate inference is a highly active research area and increasingly links to convex are being developed. see for a general overview and for recent application of convex optimisation to approximate inference in a practical machine learning application. code loopybp.m loopy belief propagation graph formalism demoloopybp.m demo of loopy belief propagation demomfbpgibbs.m comparison of mean field belief propagation and gibbs sampling demomrfclean.m demo of analysing a dirty picture maxflow.m max-flow min-cut algorithm binarymrfmap.m optimising a binary mrf exercises exercise for the max-flow-min-cut problem under the convention that the source node xs is clamped to state and the sink node xt to state a standard min-cut algorithm returns that joint x which wiji i ij explain how this can be written in the form wiji xj ij exercise using brmltoolbox write a routine kldivqp that returns the kullback-leibler divergence between two discrete distributions q and p defined as potentials q and p. exercise the file p.mat contains a distribution px y z on ternary state variables. using brmltoolbox find the best approximation qx yqz that minimises the kullback-leibler divergence klqp and state the value of the minimal kullback-leibler divergence for the optimal q. exercise consider the pairwise mrf defined on a lattice as given in pmrf.mat. using brmltoolbox find the optimal fully factorised find the optimal fully factorised approximation factor graph formalism. equations. qbp i by loopy belief propagation based on the qm f i by solving the variational mean field by pure enumeration compute the exact marginals pi. draft march exercises averaged over all variables compute the mean expected deviation in the marginals j pix j for both the bp and mf approximations and comment on your results. exercise in loopybp.m the message schedule is chosen at random. modify the routine to choose a schedule using a forward-reverse elimination sequence on a random spanning tree. exercise integration bounds. consider a bound then for fx gx x fx show that a fxdx x a gx gxdx where fx gx for x a fx gx x fx a for all x fxdx gx x a gxdx the significance is that this double integration summation in the case of discrete variables is a general procedure for generating a new bound from an existing bound exercise starting from ex and using the double integration procedure show that ex x a z by replacing x stws for s and a hts derive a bound on the partition function of a boltzmann distribution estws s show that this bound is equivalent to the mean field bound on the partition function. discuss how one can generate tighter bounds on the partition function of a boltzmann distribution by further application of the double integration procedure. exercise consider a pairwise mrf for symmetric w. consider the decomposition extwxbtx z px w qiwi where qi i i i bound on the normalisation z and discuss a naive method to find the tightest upper bound. i qi and the graph of each matrix wi is a tree. explain how to form an upper draft march exercise derive linkser s bound on the mutual information equation exercises exercise consider the average of a positive function fx with respect to a distribution px x j log pxfx j x px log fx where fx the simplest version of jensen s inequality states that by considering a distribution rx pxfx and klqr for some variational distribution qx show that j klqxpx the bound saturates when qx pxfx. this shows that if we wish to approximate the average j the optimal choice for the approximating distribution depends on both the distribution px and integrand fx. furthermore show that j klqxpx klqxfx hqx where hqx is the entropy of qx. the first term encourages q to be close to p. the second encourages q to be close to f and the third encourages q to be sharply peaked. exercise for a markov random field over d binary variables xi i d we define extwx px z show that pxi zi z where zi extwx and explain why a bound on the marginal pxi requires both upper and lower bounds on partition functions. exercise consider a directed graph such that the capacity of an edge x y is cx y the flow on an edge fx y must not exceed the capacity of the edge. the aim is to maximise the flow from a defined source node s to a defined sink node t. in addition flow must be conserved such that for any node other than the source or sink s t fy x fx y x x a cut is defined as a partition of the nodes into two non-overlapping sets s and t such that s is in s and t in t show that the net flow from s to t valf is the same as the net flow from s to t fy x valf x sy t fx y y t s valf x sy t fx y namely that the flow is upper bounded by the capacity of the cut. draft march exercises the max-flow-min-cut theorem further states that the maximal flow is actually equal to the capacity of the cut. exercise to ising translation. consider the function ex defined on a set of multistate variables domxi n ex wiji xj i i j i where wij and observed pixel states maximisation of ex. using the restricted parameteristion i are known as are ci. our interest is to find an approximate xi si sixold i where si and for a given n show how to write ex as a function of the binary variables isi const. c es i j i sj ij w i for using the graph cuts procedure. ij this new problem is of the form of an attractive binary mrf which can be solved exactly exercise consider an approximating distribution in the exponential family qx z e tgx we wish to use qx to approximate a distribution px using the kl divergence klpq show that optimally show that a gaussian can be written in the exponential form n z e tgx where x and suitably chosen hence show that the optimal gaussian fit n sense matches the moments px px to any distribution in the minimal klpq m to a distribution px. exercise we wish to find a gaussian approximation qx n show that klpq const. write the kl divergence explicitly as a function of m and and confirm the general result that the optimal m and that minimise klpq are given by setting the mean and variance of q to those of p. draft march exercise for a pairwise binary markov random field p with partition function zw b e x bi log zw b i bixi ij wij xixj zw b x show that the means can be computed using i j wij xixj xie i bixi and that similarly the covariance is given by bi bj log zw b exercises exercise the naive mean field theory applied to a pairwise mrf px e ij wij xixj domxi gives a factorised approximation qx i bixi this we can approximate i qxi based on minimising klqp. using to produce a better non-factorised approximation to we could fit a non-factorised q. the linearresponse may also be used based on a perturbation expansion of the free energy. alternatively consider the relation i j pxi xj pxixjpxj show that pxi xj pxi explain how to use a modified naive mean field method to find a non-factorised approximation to exercise derive the ep updates equation and equation exercise you are given a set of datapoints labelled to n and a similarity metric wij i j n which denotes the similarity of the points i and j. you want to assign each datapoint to a cluster index cn k. for a subset of the datapoints you have a preference for the cluster index. explain how to use a potts model to formulate an objective function for this semi-supervised clustering problem. draft march appendix a background mathematics linear algebra vector algebra let x denote the n-dimensional column vector with components xn definition product. the scalar product w x is defined as w x wixi wtx and has a natural geometric interpretation as w x cos where is the angle between the two vectors. thus if the lengths of two vectors are fixed their inner product is largest when whereupon one vector is a constant multiple of the other. if the scalar product xty then x and y are orthogonal are a right angles to each other. the length of a vector is denoted the squared length is given by xtx a unit vector x has xtx n definition dependence. a set of vectors xn is linearly dependent if there exists a vector xj that can be expressed as a linear combination of the other vectors. vice-versa if the only solution to ixi linear algebra figure resolving a vector a into components along the orthogonal directions e and e the projection of a onto these two directions are lengths and along the directions e and e is for all i i n the vectors xn are linearly independent. the scalar product as a projection suppose that we wish to resolve the vector a into its components along the orthogonal directions specified by the unit vectors e and e that is and e e this is depicted in we are required to find the scalar values and such that a e e from this we obtain a e e e e e a e e e e e from the orthogonality and unit lengths of the vectors e and e this becomes simply a e a e a set of vectors is orthonormal if they are mutually orthogonal and have unit length. this means that we can write the vector a in terms of the orthonormal components e and e as a e e e e one can see therefore that the scalar product between a and e projects the vector a onto the direction e. the projection of a vector a onto a direction specified by f is therefore a f f lines in space a line in more dimensions can be specified as follows. the vector of any point along the line is given for some s by the equation where u is parallel to the line and the line passes through the point a see this is called the parametric representation of the line. an alternative specification can be given by realising that all vectors along the line are orthogonal to the normal of the line n and n are orthonormal. that is p a su s r. a n p n a n if the vector n is of unit length the right hand side of the above represents the shortest distance from the origin to the line drawn by the dashed line in this is the projection of a onto the normal direction. figure a line can be specified by some position vector on the line a and a unit vector along the direction of the line u. in dimensions there is a unique direction n perpendicular to the line. in three dimensions the vectors perpendicular to the direction of the line lie in a plane whose normal vector is in the direction of the line u. draft march ee aeeaapnu linear algebra figure a plane can be specified by a point in the plane a and two non-parallel directions in the plane u and v. the normal to the plane is unique and in the same direction as the directed line from the origin to the nearest point on the plane. planes and hyperplanes a line is a one dimensional hyperplane. to define a two-dimensional plane arbitrary dimensional space one may specify two vectors u and v that lie in the plane need not be mutually orthogonal and a position vector a in the plane see any vector p in the plane can then be written as p a su tv t r. an alternative definition is given by considering that any vector within the plane must be orthogonal to the normal of the plane n. a n p n a n the right hand side of the above represents the shortest distance from the origin to the plane drawn by the dashed line in the advantage of this representation is that it has the same form as a line. indeed this representation of is independent of the dimension of the space. in addition only two vectors need to be defined a point in the plane a and the normal to the plane n. matrices an m n matrix a is a collection of scalar m n values arranged in a rectangle of m rows and n columns. a vector can be considered a n matrix. if the element of the i-th row and j-th column is aij then at denotes the matrix that has aji there instead the transpose of a. for example a and its transpose are a ij. at the i j element of matrix a can be written aij or in cases where more clarity is required definition the transpose bt of the n by m matrix b is the m by n matrix d with components bjk b b and btat. if the shapes of the matrices ab and c are such that it makes k m j n kj sense to calculate the product abc then ctbtat a square matrix a is symmetric if at a. a square matrix is called hermitian if a at where denotes the complex conjugate operator. for hermitian matrices the eigenvectors form an orthogonal set with real eigenvalues. draft march anuvp definition addition. for two matrix a and b of the same size bij linear algebra definition multiplication. for an l by n matrix a and an n by m matrix b the product ab is the l by m matrix with elements i l k m for example note that even if ba is defined as well that is if l n generally ba is not equal to ab they do we say they commute. the matrix i is the identity matrix necessarily square with s on the diagonal and s everywhere else. for clarity we may also write im for an square m m identity matrix. then for an m n matrix a ima ain a the identity matrix has elements ij given by the kronecker delta i j i j ij i definition trace aii i i where i are the eigenvalues of a. linear transformations rotations if we assume that rotation of a two-dimensional vector x yt can be accomplished by matrix multiplication rx then since matrix multiplication is distributive we only need to work out how the axes unit vectors i and j transform since rx xri yrj the unit vectors i and j under rotation by degrees transform to vectors cos sin rj sin cos ri draft march linear algebra from this one can simply read off the values for the elements cos sin cos sin r determinants definition for a square matrix a the determinant is the volume of the transformation of the matrix a to a sign change. that is we take a hypercube of unit volume and map each vertex under the transformation and the volume of the resulting object is defined as the determinant. writing aij det det the determinant in the case has the form the determinant of the matrix a is given by the sum of terms where ai is the matrix formed from a by removing the ith row and column. this form of the determinant generalises to any dimension. that is we can define the determinant recursively as an expansion along the top row of determinants of reduced matrices. the absolute value of the determinant is the volume of the transformation. det det for square matrices a and b of equal dimensions det det det det for any matrix a which collapses dimensions then the volume of the transformation is zero and so is the determinant. if the determinant is zero the matrix cannot be invertible since given any vector x given a projection y ax we cannot uniquely compute which vector x was projected to y there will in general be an infinite number of solutions. definition matrix. a square matrix a is orthogonal if aat i ata. from the properties of the determinant we see therefore that an orthogonal matrix has determinant and hence corresponds to a volume preserving transformation i.e. a rotation. x definition rank. for an m n matrix x with n columns each written as an m-vector the rank of x is the maximum number of linearly independent columns equivalently rows. a n n square matrix is full rank if the rank is n and the matrix is non-singular. otherwise the matrix is reduced rank and is singular. draft march matrix inversion linear algebra definition inversion. for a square matrix a its inverse satisfies a i aa it is not always possible to find a matrix a such that a i. in that case we call the matrix a singular. geometrically singular matrices correspond to projections if we were to take the transform of each of the vertices v of a binary hypercube av the volume of the transformed hypercube would be zero. if you are given a vector y and a singular transformation a one cannot uniquely identify a vector x for which y ax typically there will be a whole space of possibilities. provided the inverses matrices exist b for a non-square matrix a such that aat is invertible then the pseudo inverse defined as a satisfies aa i. computing the matrix inverse for a matrix it is straightforward to work out for a general matrix the explicit form of the inverse. if the matrix whose inverse we wish to find is a then the condition for the inverse is b c d c d g h f b bg af bh f d c cf dh ce dg ad bc b a g h a multiplying out the left hand side we obtain the four conditions it is readily verified that the solution to this set of four linear equations is given by the quantity ad bc is the determinant of a. there are many ways to compute the inverse of a general matrix and we refer the reader to more specialised texts. note that if one wants to solve only a linear system although the solution can be obtained through matrix inversion this should not be use. often one needs to solve huge dimensional linear systems of equations and speed becomes an issue. these equations can be solved much more accurately and quickly using elimination techniques such as gaussian elimination. eigenvalues and eigenvectors the eigenvectors of a matrix correspond to the natural coordinate system in which the geometric transformation represented by a can be most easily understood. draft march linear algebra definition and eigenvectors. for a square matrix a e is an eigenvector of a with eigenvalue if ae e det trace i i hence a matrix is singular if it has a zero eigenvalue. the trace of a matrix can be expressed as i for an n dimensional matrix there are repetitions n eigenvalues each with a corresponding eigenvector. we can reform equation as i e this is a linear equation for which the eigenvector e and eigenvalue is a solution. we can write equation as be where b a i. if b has an inverse then a solution is e b which trivially satisfies the eigen-equation. for any non-trivial solution to the problem be we therefore need b to be non-invertible. this is equivalent to the condition that b has zero determinant. hence is an eigenvalue of a if det i this is known as the characteristic equation. this determinant equation will be a polynomial of degree n and the resulting equation is known as the characteristic polynomial. once we have found an eigenvalue the corresponding eigenvector can be found by substituting this value for in equation and solving the linear equations for e. it may be that the for an eigenvalue the eigenvector is not unique and there is a space of corresponding vectors. geometrically the eigenvectors are special directions such that the effect of the transformation a along a direction e is simply to scale the vector e. for a rotation matric r in general there will be no direction preserved under the rotation so that the eigenvalues and eigenvectors are complex valued is why the fourier representation which corresponds to representation in a rotated basis is necessarily complex. remark of eigenvectors of symmetric matrices. for a real symmetric matric a at and two of its eigenvectors ei and ej of a are orthogonal if the eigenvalues i and j are different. the above can be shown by considering aei iei iejtei since a is symmetric the left hand side is equivalent to jejtei iejtei jejtei if i j this condition can be satisfied only if namely that the eigenvectors are orthogonal. draft march matrix decompositions the observation that the eigenvectors of a symmetric matrix are orthogonal leads directly to the spectral decomposition formula below. linear algebra definition decomposition. a symmetric matrix a has an eigen-decomposition a ieiet i where i is the eigenvalue of eigenvector ei and the eigenvectors form an orthogonal set ej ij ei in matrix notation a e et where e is the matrix of eigenvectors and the corresponding diagonal eigenvalue matrix. more generally for a square non-symmetric non-singular a we can write a e e definition value decomposition. the svd decomposition of a n p matrix x is x usvt where dim u n n with utu in. also dim v p p with vtv ip. the matrix s has dim s n p with zeros everywhere except on the diagonal entries. the singular values are the diagonal entries and are positive. the singular values are ordered so that the upper left diagonal element of s contains the largest singular value. quadratic forms definition form. xtax xtb definition definite matrix. a symmetric matrix a with the property that xtax for any vector x is called nonnegative definite. a symmetric matrix a with the property that xtax for any vector x is called positive definite. a positive definite matrix has full rank and is thus invertible. using the eigen-decomposition of a xtax iyteieitx i i i which is greater than zero if and only if all the eigenvalues are positive. hence a is positive definite if and only if all its eigenvalues are positive. draft march multivariate calculus matrix identities definition formula. for a positive definite matrix a trace a log det note that the above logarithm of a matrix is not the element-wise logarithm. in matlab the required function is logm. in general for an analytic function fx fm is defined via the power-series expansion of the function. on the right since det is a scalar the logarithm is the standard logarithm of a scalar. definition inversion lemma formula. provided the appropriate inverses exist a a a i vta vta eigenfunctions kx x ax a ax x x by an analogous argument that proves the theorem of linear algebra above the eigenfunctions are orthogonal of a real symmetric kernel x kx are orthogonal ax bx ab where is the complex conjugate of from the previous results we know that a symmetric real matrix k must have a decomposition in terms eigenvectors with positive real eigenvalues. since this is to be true for any dimension of matrix it suggests that we need the symmetric kernel function itself to have a decomposition the eigenvalues are countable kxi xj yikxi xjyj ij ij since yi i i yi zi yi z i which is greater than zero if the eigenvalues are all positive for complex z zz if the eigenvalues are uncountable happens when the domain of the kernel is unbounded the appropriate decomposition is kxi xj s sds multivariate calculus definition of the inner product is useful and particularly natural in the context of translation invariant kernels. we are free to define the inner product but this conjugate form is often the most useful. draft march multivariate calculus figure interpreting the gradient. the ellipses are contours of constant function value f const. at any point x the gradient vector fx points along the direction of maximal increase of the function. definition derivative. consider a function of n variables xn or fx. the partial derivative of f w.r.t. at x is defined as the following limit it exists f fx h x x n fx h lim h the gradient vector of f will be denoted by f or g f f xn fx gx interpreting the gradient vector consider a function fx that depends on a vector x. we are interested in how the function changes when the vector x changes by a small amount x x where is a vector whose length is very small. according to a taylor expansion the function will change to i f xi f fx f fx ft xi i we can interpret the summation above as the scalar product between the vector f with components fi f and the gradient points along the direction in which the function increases most rapidly. why? consider a direction p unit length vector. then a displacement units along this direction changes the function value to fx p fx fx p the direction p for which the function has the largest change is that which maximises the overlap fx p fx p cos fx cos where is the angle between p and fx. the overlap is maximised when giving p fx fx. hence the direction along which the function changes the most rapidly is along fx. higher derivatives the first derivative of a function of n variables is an n-vector the second-derivative of an n-variable function is defined by the partial derivatives of the n first partial derivatives w.r.t. the n variables i n j n draft march f xj xi multivariate calculus which is usually written xi xj i j xi i j if the partial derivatives f xi f xj and xi xj are continuous then xi xj exists and xi xj xj xi these second partial derivatives are represented by a square symmetric matrix called the hessian matrix of fx. xn hf xn xn chain rule consider xn. now let each xj be parameterized by um i.e. xj um. what is f u f xn xn xn xj u higher order terms f xj xj u u higher order terms xj u so f therefore definition rule. f u f xj xj u or in vector notation u fxu f txu xu u f xj xj higher order terms j vj f xj definition derivative. assume f is differentiable. we define the scalar directional derivative of f in a direction v at a point x let x x hv then d dh fx hv matrix calculus draft march f tv definition of a matrix trace. for matrices a and b a trace bt definition of log det log det trace a so that log det a t a definition of a matrix inverse. for an invertible matrix a a a t aa inequalities convexity inequalities definition function. a function fx is defined as convex if for any x y and f x fx if fx is convex fx is called concave. an intuitive picture of a convex function is to consider first the quantity x as we vary from to this traces points between x and y hence for we start at the point x fx and as increase trace a straight line towards the point y fy at convexity states that the function f always lies below this straight line. geometrically this means that the function fx is always always increasing non-decreasing. hence if the function is convex. as an example the function log x is concave since its second derivative is negative d dx log x x log x jensen s inequality for a convex function fx it follows directly from the definition of convexity that for any distribution px. draft march gradient descent optimisation critical points when all first-order partial derivatives at a point are zero f then the point is said to be a stationary or critical point. can be a minimum maximum or saddle point. necessary first-order condition for a minimum there is a minimum of f at x if fx fx for all x sufficiently close to x let x x hv for small h and some direction v. then by a taylor expansion for small h fx hv fx h f tv and thus for a minimum h f tv choosing v to be f the condition becomes h f t f and is violated for small positive h unless f t f so x can only be a local minimum if fx i.e. if fx necessary second-order condition for a minimum at a stationary point f hence the taylor expansion is given by fx hv fx v thus the minimum condition requires that vthf v i.e. the hessian is non-negative definite. definition for a minimum. sufficient conditions for a minimum at x are fx and hf is positive definite. for a quadratic function fx reads btx c with symmetric a the necessary condition fx ax b if a is invertible this equation has the unique solution x a minimum. if a is positive definite x is a gradient descent almost all of the search techniques that we consider are iterative i.e. we proceed towards the minimum x by a sequence of steps. on the kth step we take a step of length k in the direction pk xk kpk the length of the step can either be chosen using prior knowledge or by carrying out a line search in the direction pk. it is the way that pk is chosen that tends to distinguish the different methods of multivariate optimization that we will discuss. draft march gradient descent we shall assume that we can analytically evaluate the gradient of f and will often use the shorthand notation gk fxk typically we will want to choose pk using only gradient information for large problems it can be very expensive to compute the hessian and this can also require a large amount of storage. consider the change of variables x my. then gy fx fmy and j g yi f xi xi yj so that yg m xf then the change in g is different from the change in f even though the only difference between the two functions is the coordinate system. this unfortunate sensitivity to the parameterisation of the function is partially addressed in first order methods such as gradient descent by the natural gradient which uses a prefactor designed to compensate for some of the lost invariance. we refer the reader to for a description of this method. gradient descent with fixed stepsize locally if we are at point xk we can decrease fx by taking a step in the direction gx. if we make the update equation xk gk then we are doing gradient descent with fixed stepsize if is non-infinitesimal it is always possible that we will step over the true minimum. making very small guards against this but means that the optimization process will take a very long time to reach a minimum. to see why gradient descent works consider the general update xk pk for small we can expand f around xk using taylor s theorem fxk kpk fxk kgt k pk with pk gk and for small positive k we see a guaranteed reduction fxk kpk fxk gradient descent with momentum a simple idea that can improve convergence of gradient descent is to include at each iteration a proportion of the change from the previous iteration. one uses e x xk where is the momentum coefficient. draft march multivariate minimization quadratic functions figure optimisation using line search along steepest descent directions. rushing off following the steepest way downhill from a point continuing for a finite time in that direction doesn t always result in the fastest way to get to the bottom! gradient descent with line searches an extension to the idea of gradient descent is to choose the direction of steepest descent as indicated by the gradient g but to calculate the value of the step to take which most reduces the value of e when moving in that direction. this involves solving the one-dimensional problem of minimizing exk gk with respect to and is known as a line search. that step is then taken and the process repeated again. finding the size of the step takes a little work for example you might find three points along the line such that the error at the intermediate point is less than at the other two so that there is some minimum along the line lies between the first and second or between the second and third and some kind of intervalhalving approach can then be used to find it. minimum found in this way just as with any sort of gradient-descent algorithm may not be a global minimum of course. there are several variants of this theme. notice that if the step size is chosen to reduce e as much as it can in that direction then no further improvement in e can be made by moving in that direction for the moment. thus the next step will have no component in that direction that is the next step will be at right angles to the one just taken. this can lead to zig-zag type behaviour in the optimisation see exact line search condition at the k-th step we chose k to minimize fxk kpk. so setting f fxk pk at this step we solve the one-dimensional minimization problem for f thus our choice of k will satisfy f k now f k d dh fxk kpk f k d dh d dh so f k means the directional derivative in the search direction must vanish at the new point and this gives the exact line search condition f gt for a quadratic function fx calculate k. since axk kapk b fxk kapk we find btxc with symmetric we can use the condition to analytically k pt k gk pt k apk multivariate minimization quadratic functions the goal of this section is to derive efficient algorithms for minimizing multivariate quadratic functions. we shall begin by summarizing some properties of quadratic functions and as byproduct obtain an efficient method for checking whether a symmetric matrix is positive definite. minimising quadratic functions using line search consider minimising the quadratic function fx xtax btx c draft march multivariate minimization quadratic functions ptap where a is positive definite and symmetric. a is not symmetric consider instead the symmetrised matrix which gives the same function f. although we know where the minimum of this function is just using linear algebra we wish to use this function as a toy model for more complex functions which however locally look approximately quadratic. one approach is to search along a particular direction p and find a minimum along this direction. we can then search for a deeper minima by looking in different directions. that is we can search along a line p such that the function attains a minimum. that is the directional derivative is zero along this line this has solution p p ptap now we ve found the minimum along the line through with direction p. but how should we choose the line search direction p? it would seem sensible to choose successive line search directions p according to pnew fx so that each time we minimise the function along the line of steepest descent. however this is far from the optimal choice in the case of minimising quadratic functions. a much better set of search directions are those defined by the vectors conjugate to a. if the matrix a were diagonal then the minimisation is straightforward and can be carried out independently for each dimension. if we could find an invertible matrix p with the property that ptap is diagonal then the solution is easy since for f x xtptap x btp x c with x p x we can compute the minimum for each dimension of x separately and then retransform to find x p x definition vectors. the vectors pi i k are called conjugate to the matrix a if and only if for i j k and i j i api pt i apj and pt the two conditions guarantee that conjugate vectors are linearly independent assume that i jpj jpj ipi jpj now multiplying from the left with pt as we can make this argument for any i k all of the i must be zero. i a yields ipt i api. so i is zero since we know that pt i api gram-schmidt construction of conjugate vectors let p pk where the columns are formed from a-conjugate vectors and note that we start with an n by k matrix k n. the reason for this is that we are aiming at an incremental procedure where columns are successively added to p. since pt i apj the matrix ptap will be diagonal i apj for i j. assume we already have k conjugate vectors pk and let v be a vector if pt which is linearly independent of pk. we then set v pt j av j apj pt pj for which it is clear that the vectors are conjugate if a is positive definite. using the gramschmidt procedure we can construct n conjugate vectors for a positive definite matrix in the following way. we start with n linearly independent vectors un we might chose ui ei the unit vector in draft march multivariate minimization quadratic functions the ith direction. we then set and use to compute from and v next we set v and compute from and v. continuing in this manner we obtain n conjugate vectors. note that at each stage of the procedure the vectors uk span the same subspace as the vectors pk. what is going to happen if a is not positive definite? if we could find n conjugate vectors a would be positive definite and so at some point k the gram-schmidt procedure must break down. this will happen if pt k apk so by trying out the gram-schmidt procedure we can in fact find out whether a matrix is positive definite. the conjugate vectors algorithm let us assume that when minimising fx conjugate to a which we use as our search directions. so btx c we first construct n vectors pn xk kpk at each step we chose k by an exact line search thus k pt k gk k apk pt this conjugate vectors algorithm has the geometrical interpretation that not only is the directional derivative zero at the new point along the direction pk it is zero along all the previous search directions pk. theorem expanding subspace theorem. be a sequence of vectors in rn conjugate to the definite matrix a and let fx btx c. then for any the sequence generated according to and has the property that the directional derivative of f in the direction pi vanishes at the point if i k i.e. proof for i k we can write as jpj since fx ax b we have b b a jpj japj so pt i pt i jpt i apj jpt i apj now since the point was obtained by an exact line search in the direction pi. but all i apj by conjugacy. so of the terms in the sum over j also vanish since j i and pt the subspace theorem shows that because we use conjugate vectors optimizing in the direction pk does not spoil the optimality w.r.t. to the previous search directions. in particular after having carried out n steps of the algorithm we have f for i n. the n equations can be written in a more compact form as f pn draft march multivariate minimization quadratic functions the square matrix p pn is invertible since the pi are conjugate so the point is the minimum x of the quadratic function f. so in contrast to gradient descent for a quadratic function the conjugate vectors algorithm converges in a finite number of steps. the conjugate gradients algorithm the conjugate gradients algorithm is a special case of the conjugate vectors algorithm in which the gramschmidt procedure becomes very simple. we do not use a predetermined set of conjugate vectors but construct these on-the-fly after k-steps of the conjugate vectors algorithm we need to construct a vector which is conjugate to pk. this could be done by applying the gram-schmidt procedure to any vector v which is linearly independent of the vectors pk. in the conjugate gradients algorithm one makes the special choice v by the subspace theorem the gradient at the new point is orthogonal to pi i k. so is linearly independent of pk and a valid choice for v unless in the latter case is our minimum and we are done and from now on we assume that using the notation gk fxk the equation for the new search direction given by the gram-schmidt procedure is pt i pt i api pi since is orthogonal to pi i k by the subspace theorem we have pt can be written as gt so gt pt and in particular we now want to show that because we have been using the conjugate gradients algorithm at the previous steps as well in equation all terms but the last in the sum over i vanish. we shall assume that k since in the first step we just set first note that gi b b xi iapi and since i api gi i. so in equation i gt pt gt gi i gt i since the pi where obtained by applying the gram-schmidt procedure to the gradients gi the subspace theorem gt for i k. this shows that implies also gt pt i gt i gt k if i k if i k hence equation simplifies to gt k pt k apk pk this can be brought into an even simpler form by applying equation to k gt pt k apk we shall write this in the form k apk pt gt k gk pk gt gt k gk pk kpk where k gt gt k gk draft march multivariate minimization quadratic functions algorithm conjugate gradients for minimising a function fx k choose while gk do k argmin xk kpk k gt k gk kpk k k end while fxk kpk k line search the formula for k is due to fletcher and reeves. since the gradients are orthogonal k can also be written as k gt gk k gk gt this is the polak-ribiere formula. the choice between the two expression for k can be of some importance if f is not quadratic. newton s method consider a function fx that we wish to find the minimum of. a taylor expansion up to second order gives fx fx t f th o the matrix h is the hessian. differentiating the right hand side with respect to equivalently completing the square we find that the right hand side has its lowest value when f h h f hence an optimisation routine to minimise e is given by the newton update xk h f a benefit of newton method over gradient descent is that the decrease in the objective function is invariant under a linear change of co-ordinates y mx. quasi-newton methods for large-scale problems the inversion of the hessian is computationally demanding especially if the matrix is close to singular. an alternative is to set up the iteration xk kskgk. this is a very general form if sk a then we have newton s method while if sk i we have steepest descent. in general it would seem to be a good idea to choose sk to be an approximation to the inverse hessian. also note that it is important that sk be positive definite so that for small k we obtain a descent method. the idea behind most quasi-newton methods is to try to construct an approximate inverse hessian hk using information gathered as the descent progresses and to set sk hk. as we have seen for a quadratic optimization problem we have the relationship gk xk draft march algorithm quasi-newton for minimising a function fx constrained optimisation using lagrange multipliers k choose i while gk do pk hkgk k argmin xk kpk sk xk yk gk and update k k fxk kpk k end while defining sk xk and yk gk we see that equation becomes yk ask line search it is reasonable to demand that i k si after n linearly independent steps we would then have a for k n there are an infinity of solutions for satisfying equation a popular choice is the broyden-fletcher-goldfarb-shanno bfgs update given by hk hkyk yt k k sk yt skyt k skst k k yk st hk hkykst k k yk st this is a correction to hk constructed from the vectors sk and hkyk. the direction vectors pk pk hkgk produced by the algorithm obey pt i apj pi i j k i k equation is called the hereditary property. in our notation sk kpk and as the s are non-zero equation can also be written as st i asj i j k since the pk s are a-conjugate and since we successively minimize f in these directions we see that the bfgs algorithm is a conjugate direction method with the choice of i it is in fact the conjugate gradient method. note that the storage requirements for quasi newton methods scale quadratically with the number of variables and hence tends to be used for smaller problems. limited memory bfgs reduces the storage by only using the l latest updates in computing the approximate hessian inverse equation in contrast the memory requirements for pure conjugate gradient methods scale only linearly with the dimension of x. constrained optimisation using lagrange multipliers single constraint consider first the problem of minimising fx subject to a single constraint cx imagine that we have already identified an x that satisfies the constraint that is cx how can we tell if this x minimises draft march constrained optimisation using lagrange multipliers the function f? we are only allowed to search for lower function values around this x in directions which are consistent with the constraint. for a small change the change in the constraint is cx cx cx hence in order that the constraint remains satisfied we can only search in a direction such that cx that is in directions that are orthogonal to cx. so let us explore the change in f along a direction where cx fx fx fx since we are looking for a point x that minimises the function f we require x to be a stationary point fx thus must be orthogonal to both fx and cx. since we wish to constrain as little as possible the most freedom is given by enforcing fx to be parallel to cx so that fx cx for some r. to solve the optimisation problem therefore we look for a point x such that fx cx for some and for which cx an alternative formulation of this dual requirement is to look for x and that jointly minimise the lagrangian lx fx cx differentiating with respect to x we get the requirement fx cx and differentiating with respect to we get that cx multiple constraints consider the problem of optimising fx subject to the constraints cix i r n where n is the dimensionality of the space. denote by s the n r dimensional subspace of x which obeys the constraints. assume that x is such an optimum. as in the unconstrained case we consider perturbations v to x but now such that v lies in s a cix hv cix vt cix i cix thus for the perturbation to stay within s we require that vta let a i for all i r. r. then this condition can be rewritten as a v a let a be the matrix whose columns are a we also require for a local optimum that vt f for all v in s. we see that f must be orthogonal to v and that v must be orthogonal to the a i s. this can be achieved by forcing f to be a linear combination of the a i a geometrically this says that the gradient vector is normal to the tangent plane to s at x these conditions give rise to the method of lagrange multipliers for optimisation problems with equality constraints. the method requires finding x and which solve the equations f i s i.e. i f i aix i cix for i r there are n r equations and n r unknowns so the system is well-determined. however the system is nonlinear x in general and so may not be easy to solve. we can restate these conditions by introducing the lagrangian function lx fx icix. i the partial derivatives of l with respect to x and reproduce equations and hence a necessary condition for a local minimizer is that x is a stationary point of the lagrangian function. note that this stationary point is not a minimum but a saddle point as l depends linearly on we have given first-order necessary and sufficient conditions for a local optimum. to show that this optimum is a local minimum we would need to consider second-order conditions analogous to the positive definiteness of the hessian in the unconstrained case this can be done but will not be considered here. draft march constrained optimisation using lagrange multipliers draft march bibliography l. f. abbott j. a. varela k. sen and s. b. nelson. synaptic depression and cortical gain control. science d. h. ackley g. e. hinton and t. j. sejnowski. a learning algorithm for boltzmann machines. cognitive science r. p. adams and d. j. c. mackay. bayesian online changepoint detection. cavendish laboratory department of physics university of cambridge cambridge uk e. airoldi d. blei e. xing and s. fienberg. a latent mixed membership model for relational data. in linkkdd proceedings of the international workshop on link discovery pages new york ny usa acm. e. m. airoldi d. m. blei s. e. fienberg and e. p. xing. mixed membership stochastic blockmodels. journal of machine learning research d. l. alspach and h. w. sorenson. nonlinear bayesian estimation using gaussian sum approximations. ieee transactions on automatic control s-i. amari. natural gradient works efficiently in learning. neural computation s-i. amari. natural gradient learning for over and under-complete bases in ica. neural computation i. androutsopoulos j. koutsias k. v. chandrinos and c. d. spyropoulos. an experimental comparison in proceedings of of naive bayesian and keyword-based anti-spam filtering with personal e-mail messages. the annual international acm sigir conference on research and development in information retrieval pages new york ny usa acm. s. arora and c. lund. hardness of approximations. in approximation algorithms for np-hard problems pages pws publishing co. boston ma usa f. r. bach and m. i. jordan. thin junction trees. in t. g. dietterich s. becker and z. ghahramani editors advances in neural information processing systems number pages cambridge ma mit press. f. r. bach and m. i. jordan. a probabilistic interpretation of canonical correlation analysis. computer science division and department of statistics university of california berkeley berkeley usa y. bar-shalom and xiao-rong li. estimation and tracking principles techniques and software. artech house norwood ma d. barber. dynamic bayesian networks with deterministic tables. in s. becker s. thrun and k. obermayer editors advances in neural information processing systems number pages cambridge ma mit press. d. barber. learning in spiking neural assemblies. in s. becker s. thrun and k. obermayer editors advances in neural information processing systems number pages cambridge ma mit press. bibliography bibliography d. barber. are two classifiers performing equally? a treatment using bayesian hypothesis testing. idiap rr idiap rue de simplon martigny switerland may idiap-rr d. barber. expectation correction for smoothing in switching linear gaussian state space models. journal of machine learning research d. barber. clique matrices for statistical graph decomposition and parameterising restricted positive in d. a. mcallester and p. myllymaki editors uncertainty in artificial intelligence definite matrices. number pages corvallis oregon usa auai press. d. barber and f. v. agakov. correlated sequence learning in a network of spiking neurons using maximum likelihood. informatics research reports edinburgh university d. barber and f.v. agakov. the im algorithm a variational approach to information maximization. in advances in neural information processing systems number d. barber and c. m. bishop. bayesian model comparison by monte carlo chaining. in m. c. mozer m. i. jordan and t. petsche editors advances in neural information processing systems number pages cambridge ma mit press. d. barber and c. m. bishop. ensemble learning in bayesian neural networks. machine learning pages springer in neural networks and d. barber and s. chiappa. unified inference for variational bayesian linear gaussian state-space models. in b. sch olkopf j. platt and t. hoffman editors advances in neural information processing systems number pages cambridge ma mit press. d. barber and w. wiegerinck. tractable variational structures for approximating graphical models. in m. s. kearns s. a. solla and d. a. cohn editors advances in neural information processing systems number pages cambridge ma mit press. d. barber and c. k. i. williams. gaussian processes for bayesian classification via hybrid monte carlo. in m. c. mozer m. i. jordan and t. petsche editors advances in neural information processing systems nips pages cambridge ma mit press. r. j. baxter. exactly solved models in statistical mechanics. academic press m. j. beal f. falciani z. ghahramani c. rangel and d. l. wild. a bayesian approach to reconstructing genetic regulatory networks with hidden factors. bioinformatics a. becker and d. geiger. a sufficiently fast algorithm for finding close to optimal clique trees. artificial intelligence a. j. bell and t. j. sejnowski. an information-maximization approach to blind separation and blind deconvolution. neural computation r. e. bellman. dynamic programming. princeton university press princeton nj paperback edition by dover publications y. bengio and p. frasconi. input-output hmms for sequence processing. ieee trans. neural networks a. l. berger s. d. della pietra and v. j. d. della pietra. a maximum entropy approach to natural language processing. computational linguistics j. o. berger. statistical decision theory and bayesian analysis. springer second edition d. p. bertsekas. dynamic programming and optimal control. athena scientific second edition j. besag. spatial interactions and the statistical analysis of lattice systems. journal of the royal statistical society series b j. besag. on the statistical analysis of dirty pictures. journal of the royal statistical society series b j. besag and p. green. spatial statistics and bayesian computation. journal of the royal statistical society series b draft march bibliography bibliography g. j. bierman. measurement updating using the u-d factorization. automatica n. l. biggs. discrete mathematics. oxford university press k. binder and a. p. young. spin glasses experimental facts theoretical concepts and open questions. rev. mod. phys. oct c. m. bishop. neural networks for pattern recognition. oxford university press c. m. bishop. pattern recognition and machine learning. springer c. m. bishop and m. svens en. bayesian hierarchical mixtures of experts. in u. kjaerulff and c. meek editors proceedings nineteenth conference on uncertainty in artificial intelligence pages morgan kaufmann d. blei a. ng and m. jordan. latent dirichlet allocation. journal of machine learning research r. r. bouckaert. bayesian belief networks from construction to inference. phd thesis university of utrecht s. boyd and l. vandenberghe. convex optimization. cambridge university press y. boykov and v. kolmogorov. an experimental comparison of min-cutmax-flow algorithms for energy minimization in vision. ieee trans. pattern anal. mach. intell. y. boykov o. veksler and r. zabih. fast approximate energy minimization via graph cuts. ieee trans. pattern anal. mach. intell. m. brand. incremental singular value decomposition of uncertain data with missing values. conference on computer vision pages in european j. breese and d. heckerman. decision-theoretic troubleshooting a framework for repair and experiment. in e. horvitz and f. jensen editors uncertainty in artificial intelligence number pages san francisco ca morgan kaufmann. h. bunke and t. caelli. hidden markov models applications in computer vision. machine perception and artificial intelligence. world scientific publishing co. inc. river edge nj usa w. buntine. theory refinement on bayesian networks. in uncertainty in artificial intelligence number pages san francisco ca morgan kaufmann. a. cano and s. moral. advances in intelligent computing ipmu chapter heuristic algorithms for the triangulation of graphs pages number in lectures notes in computer sciences. springer-verlag o. capp e e. moulines and t. ryden. inference in hidden markov models. springer new york e. castillo j. m. gutierrez and a. s. hadi. expert systems and probabilistic network models. springer a. t. cemgil. bayesian inference in non-negative matrix factorisation models. technical report cuedf university of cambridge july a. t. cemgil b. kappen and d. barber. a generative model for music transcription. ieee transactions on audio speech and language processing h. s. chang m. c. fu j. hu and s. i. marcus. simulation-based algorithms for markov decision processes. springer s. chiappa and d. barber. bayesian linear gaussian state space models for biosignal decomposition. signal processing letters s. chib and m. dueker. non-markovian regime switching with endogenous states and time-varying state strengths. econometric society north american summer meetings econometric society august c. k. chow and c. n. liu. approximating discrete probability distributions with dependence trees. ieee transactions on information theory draft march bibliography bibliography p. s. churchland and t. j. sejnowski. the computational brain. mit press cambridge ma usa d. cohn and h. chang. learning to probabilistically identify authoritative documents. in p. langley editor international conference on machine learning number pages morgan kaufmann d. cohn and t. hofmann. the missing link a probabilistic model of document content and hypertext connectivity. number pages cambridge ma mit press. a. c. c. coolen r. k uhn and p. sollich. theory of neural information processing systems. oxford university press g. f. cooper and e. herskovits. a bayesian method for the induction of probabilistic networks from data. machine learning a. corduneanu and c. m. bishop. variational bayesian model selection for mixture distributions. in t. jaakkola and t. richardson editors artifcial intelligence and statistics pages morgan kaufmann m. t. cover and j. a. thomas. elements of information theory. wiley r. g. cowell a. p. dawid s. l. lauritzen and d. j. spiegelhalter. probabilistic networks and expert systems. springer d. r. cox and n. wermuth. multivariate dependencies. chapman and hall n. cristianini and j. shawe-taylor. an introduction to support vector machines. cambridge university press p. dangauthier r. herbrich t. minka and t. graepel. trueskill through time revisiting the history of chess. in b. sch olkopf j. platt and t. hoffman editors advances in neural information processing systems number pages cambridge ma mit press. h. a. david. the method of paired comparisons. oxford university press new york a. p. dawid. influence diagrams for causal modelling and inference. international statistical review a. p. dawid and s. l. lauritzen. hyper markov laws in the statistical analysis of decomposable graphical models. annals of statistics p. dayan and l.f. abbott. theoretical neuroscience. mit press p. dayan and g. e. hinton. using expectation-maximization for reinforcement learning. neural computa tion t. de bie n. cristianini and r. rosipal. handbook of geometric computing applications in pattern recognition computer vision neuralcomputing and robotics chapter eigenproblems in pattern recognition. springer-verlag r. dechter. bucket elimination a unifying framework for probabilistic inference algorithms. in e. horvitz and f. jensen editors uncertainty in artificial intelligence pages san francisco ca morgan kaufmann. s. diederich and m. opper. learning of correlated patterns in spin-glass networks by local learning rules. physical review letters r. diestel. graph theory. springer a. doucet and a. m. johansen. a tutorial on particle filtering and smoothing fifteen years later. in d. crisan and b. rozovsky editors oxford handbook of nonlinear filtering. oxford university press r. o. duda p. e. hart and d. g. stork. pattern classification. wiley-interscience publication r. durbin s. r. eddy a. krogh and g. mitchison. biological sequence analysis probabilistic models of proteins and nucleic acids. cambridge university press a. d uring a. c. c. coolen and d. sherrington. phase diagram and storage capacity of sequence processing neural networks. journal of physics a draft march bibliography bibliography j. m. gutierrez e. castillo and a. s. hadi. expert systems and probabilistic network models. springer verlag j. edmonds and r. m. karp. theoretical improvements in algorithmic efficiency for network flow problems. journal of the acm r. edwards and a. sokal. generalization of the fortium-kasteleyn-swendson-wang representation and monte carlo algorithm. physical review d a. e. elo. the rating of chess players past and present. arco new york second edition y. ephraim and w. j. j. roberts. revisiting autoregressive hidden markov modeling of speech signals. ieee signal processing letters february e. erosheva s. fienberg and j. lafferty. mixed membership models of scientific publications. in proceedings of the national academy of sciences volume pages r-e. fan p-h. chen and c-j. lin. working set selection using second order information for training support vector machines. journal of machine learning research p. fearnhead. exact and efficient bayesian inference for multiple changepoint problems. technical report deptartment of mathematics and statistics lancaster university g. h. fischer and i. w. molenaar. rasch models foundations recent developments and applications. springer new york m. e. fisher. statistical mechanics of dimers on a plane lattice. physical review b. frey. extending factor graphs as to unify directed and undirected graphical models. in c. meek and u. kj rulff editors uncertainty in artificial intelligence number pages morgan kaufmann n. friedman d. geiger and m. goldszmidt. bayesian network classifiers. machine learning s. fr uhwirth-schnatter. finite mixture and markov switching models. springer m. frydenberg. the chain graph markov property. scandanavian journal of statistics t. furmston and d. barber. solving deterministic policy using expectation-maximisation and antifreeze. in e. suzuki and m. sebag editors european conference on machine learning and principles and practice of knowledge discovery in databases september workshop on learning and data mining for robotics. a. galka o. yamashita t. ozaki r. biscay and p. valdes-sosa. a solution to the dynamical inverse problem of eeg generation using spatiotemporal kalman filtering. neuroimage p. gandhi f. bromberg and d. margaritis. learning markov network structure using few independence tests. in proceedings of the siam international conference on data mining pages m. r. garey and d. s. johnson. computers and intractability a guide to the theory of np-completeness. w.h. freeman and company new york a. gelb. applied optimal estimation. mit press a. gelman g. o. roberts and w. r. gilks. efficient metropolis jumping rules. in j. o. bernardo j. m. berger a. p. dawid and a. f. m. smith editors bayesian statistics volume pages oxford university press s. geman and d. geman. stochastic relaxation gibbs distributions and the bayesian restoration of images. in readings in uncertain reasoning pages san francisco ca usa morgan kaufmann publishers inc. m. g. genton. classes of kernels for machine learning a statistics perspective. journal of machine learning research w. gerstner and w. m. kistler. spiking neuron models. cambridge university press draft march bibliography bibliography z. ghahramani and m. j. beal. variational inference for bayesian mixtures of factor analysers. in s. a. solla t. k. leen and k-r. m uller editors advances in neural information processing systems number pages cambridge ma mit press. z. ghahramani and g. e. hinton. variational learning for switching state-space models. neural computation a. gibbons. algorithmic graph theory. cambridge university press w. r. gilks s. richardson and d. j. spiegelhalter. markov chain monte carlo in practice. chapman hall m. girolami and a. kaban. on an equivalence between plsi and lda. in proceedings of the annual international acm sigir conference on research and development in information retrieval pages new york ny usa acm press. m. e. glickman. parameter estimation in large dynamic paired comparison experiments. applied statistics a. globerson and t. jaakkola. approximate inference using planar graph decomposition. in b. sch olkopf j. platt and t. hoffman editors advances in neural information processing systems number pages cambridge ma mit press. d. goldberg d. nichols b. m. oki and d. terry. using collaborative filtering to weave an information tapestry. communications acm g. h. golub and c. f. van loan. matrix computations. johns hopkins university press edition m. c. golumbic and i. ben-arroyo hartman. graph theory combinatorics and algorithms. springer-verlag c. goutis. a graphical method for solving a decision analysis problem. ieee transactions on systems man and cybernetics p. j. green and b. w. silverman. nonparametric regression and generalized linear models volume of monographs on statistics and applied probability. chapman and hall d. m. greig b. t. porteous and a. h. seheult. exact maximum a posteriori estimation for binary images. journal of the royal statistical society series b g. grimmett and d. stirzaker. probability and random processes. oxford university press second edition s. f. gull. bayesian data analysis straight-line fitting. in j. skilling editor maximum entropy and bayesian methods pages kluwer a. k. gupta and d. k. nagar. matrix variate distributions. chapman and hallcrc boca raton florida usa d. j. hand and k. yu. idiot s bayes not so stupid after all? international statistical review d. r. hardoon s. szedmak and j. shawe-taylor. canonical correlation analysis an overview with appli cation to learning methods. neural computation d. o. hebb. the organization of behavior. wiley new york d. heckerman. a tutorial on learning with bayesian networks. technical report microsoft research redmond wa march revised november d. heckerman d. geiger and d. chickering. learning bayesian networks the combination of knowledge and statistical data. machine learning r. herbrich t. minka and t. graepel. trueskilltm a bayesian skill rating system. in b. sch olkopf j. platt and t. hoffman editors advances in neural information processing systems number pages cambridge ma mit press. h. hermansky. should recognizers have ears? speech communication draft march bibliography bibliography j. hertz a. krogh and r. palmer. introduction to the theory of neural computation. addison-wesley t. heskes. convexity arguments for efficient minimization of the bethe and kikuchi free energies. journal of artificial intelligence research d. m. higdon. auxiliary variable methods for markov chain monte carlo with applications. journal of the american statistical association g. e. hinton and r. r. salakhutdinov. reducing the dimensionality of data with neural networks. science t. hofmann j. puzicha and m. i. jordan. learning from dyadic data. in m. s. kearns s. a. solla and d. a. cohn editors advances in neural information processing systems pages cambridge ma mit press. r. a. howard and j. e. matheson. influence diagrams. decision analysis republished version of the original report. a. hyv arinen j. karhunen and e. oja. independent component analysis. wiley aapo hyv arinen. consistency of pseudolikelihood estimation of fully visible boltzmann machines. neural computation m. isard and a. blake. condensation conditional density propagation for visual tracking. international journal of computer vision t. s. jaakkola and m. i. jordan. variational probabilistic inference and the qmr-dt network. journal of artificial intelligence research t. s. jaakkola and m. i. jordan. bayesian parameter estimation via variational methods. statistics and computing r. a. jacobs f. peng and m. a. tanner. a bayesian approach to model selection in hierarchical mixtures of-experts architectures. neural networks r. g. jarrett. a note on the intervals between coal-mining disasters. biometrika e. t. jaynes. probability theory the logic of science. cambridge university press f. jensen f. v. jensen and d. dittmer. from influence diagrams to junction trees. in proceedings of the annual conference on uncertainty in artificial intelligence pages san francisco ca morgan kaufmann. f. v. jensen and f. jensen. optimal junction trees. in r. lopez de mantaras and d. poole editors uncertainty in artificial intelligence number pages san francisco ca morgan kaufmann. f. v. jensen and t. d. nielson. bayesian networks and decision graphs. springer verlag second edition m. i. jordan and r. a. jacobs. hierarchical mixtures of experts and the em algorithm. neural computation b. h. juang w. chou and c. h. lee. minimum classification error rate methods for speech recognition. ieee transactions on speech and audio processing l. p. kaelbling m. l. littman and a. r. cassandra. planning and acting in partially observable stochastic domains. artificial intelligence h. j. kappen. an introduction to stochastic control theory path integrals and reinforcement learning. in proceedings granada seminar on computational physics computational and mathematical modeling of cooperative behavior in neural systems volume pages american institute of physics h. j. kappen and f. b. rodr guez. efficient learning in boltzmann machines using linear response theory. neural compution h. j. kappen and w. wiegerinck. novel iteration schemes for the cluster variation method. in t. g. dietterich s. becker and z. ghahramani editors advances in neural information processing systems number pages cambridge ma mit press. draft march bibliography bibliography y. karklin and m. s. lewicki. emergence of complex cell properties by learning to generalize in natural scenes. nature november g. karypis and v. kumar. a fast and high quality multilevel scheme for partitioning irregular graphs. siam journal on scientific computing p. w. kasteleyn. dimer statistics and phase transitions. journal of mathematical physics s. a. kauffman. at home in the universe the search for laws of self-organization and complexity. oxford university press oxford uk c-j. kim. dynamic linear models with markov-switching. journal of econometrics c-j. kim and c. r. nelson. state-space models with regime switching. mit press g. kitagawa. the two-filter formula for smoothing and an implementation of the gaussian-sum smoother. annals of the institute of statistical mathematics u. b. kjaerulff and a. l. madsen. bayesian networks and influence diagrams a guide to construction and analysis. springer a. krogh m. brown i. mian k. sjolander and d. haussler. hidden markov models in computational biology applications to protein modeling. journal of molecular biology s. kullback. information theory and statistics. dover k. kurihara m. welling and y. w. teh. collapsed variational dirichlet process mixture models. in proceedings of the international joint conference on artificial intelligence volume pages j. lafferty a. mccallum and f. pereira. conditional random fields probabilistic models for segmenting and labeling sequence data. in c. e. brodley and a. p. danyluk editors international conference on machine learning number pages san francisco ca morgan kaufmann. h. lass. elements of pure and applied mathematics. mcgraw-hill by dover s. l. lauritzen. graphical models. oxford university press s. l. lauritzen a. p. dawid b. n. larsen and h-g. leimer. independence properties of directed markov fields. networks s. l. lauritzen and d. j. spiegelhalter. local computations with probabilities on graphical structures and their application to expert systems. journal of royal statistical society b d. d. lee and h. s. seung. algorithms for non-negative matrix factorization. in t. k. leen t. g. dietterich and v. tresp editors advances in neural information processing systems number pages cambridge ma mit press. m. a. r. leisink and h. j. kappen. a tighter bound for graphical models. in neural computation volume pages mit press v. lepar and p. p. shenoy. a comparison of lauritzen-spiegelhalter hugin and shenoy-shafer architectures for computing marginals of probability distributions. in g. cooper and s. moral editors uncertainty in artificial intelligence number pages san francisco ca morgan kaufmann. u. lerner r. parr d. koller and g. biswas. bayesian fault detection and diagnosis in dynamic systems. in proceedings of the seventeenth national conference on artificial intelligence pages u. n. lerner. hybrid bayesian networks for reasoning about complex systems. computer science department stanford university r. linsker. improved local learning rule for information maximization and related applications. neural networks y. l. loh e. w. carlson and m. y. j. tan. bond-propagation algorithm for thermodynamic functions in general two-dimensional ising models. physical review b h. lopes and m. west. bayesian model assessment in factor analysis. statistica sinica draft march bibliography bibliography t. j. loredo. from laplace to supernova sn bayesian inference in astrophysics. in p.f. fougere editor maximum entropy and bayesian methods pages kluwer d. j. c. mackay. bayesian interpolation. neural computation d. j. c. mackay. probable networks and plausisble predictions a review of practical bayesian methods for supervised neural networks. network computation in neural systems d. j. c. mackay. introduction to gaussian processes. in neural networks and machine learning volume of nato advanced study institute on generalization in neural networks and machine learning pages springer august d. j. c. mackay. information theory inference and learning algorithms. cambridge university press u. madhow. fundamentals of digital communication. cambridge university press k. v. mardia j. t. kent and j. m. bibby. multivariate analysis. academic press h. markram j. lubke m. frotscher and b. sakmann. regulation of synaptic efficacy by coincidence of postsynaptic aps and epsps. science g. mclachlan and t. krishnan. the em algorithm and extensions. john wiley and sons g. mclachlan and d. peel. finite mixture models. wiley series in probability and statistics. wiley interscience e. meeds z. ghahramani r. m. neal and s. t. roweis. modeling dyadic data with binary latent factors. in b. sch olkopf j. platt and t. hoffman editors advances in neural information processing systems volume pages cambridge ma mit press. m. meila. an accelerated chow and liu algorithm fitting tree distributions to high-dimensional sparse data. in i. bratko editor international conference on machine learning pages san francisco ca morgan kaufmann. m. meila and m. i. jordan. triangulation by continuous embedding. in m. c. mozer m. i. jordan and t. petsche editors advances in neural information processing systems number pages cambridge ma mit press. b. mesot and d. barber. switching linear dynamical systems for noise robust speech recognition. ieee transactions of audio speech and language processing n. meuleau m. hauskrecht k-e. kim l. peshkin kaelbling. l. p. t. dean and c. boutilier. solving very large weakly coupled markov decision processes. in proceedings of the fifteenth national conference on artificial intelligence pages t. mills. the econometric modelling of financial time series. cambridge university press t. minka. expectation propagation for approximate bayesian inference. in j. breese and d. koller editors uncertainty in artificial intelligence number pages san francisco ca morgan kaufmann. t. minka. a comparison of numerical optimizers for logistic regression. technical report microsoft research research.microsoft.com minkapaperslogreg. t. minka. divergence measures and message passing. technical report microsoft research ltd. cambridge uk december a. mira j. m ller and g. o. roberts. perfect slice samplers. journal of the royal statistical society series b methodology. c. mitchell m. harper and l. jamieson. on the complexity of explicit duration hmm s. speech and audio processing ieee transactions on may t. mitchell. machine learning. mcgraw-hill j. mooij and h. j. kappen. sufficient conditions for convergence of loopy belief propagation. ieee infor mation theory a. moore. a tutorial httpwww.cs.cmu.edu awmpapers.html. on kd-trees. technical report available from draft march bibliography bibliography j. moussouris. gibbs and markov random systems with constraints. journal of statistical physics r. m. neal. connectionist learning of belief networks. artificial intelligence r. m. neal. probabilistic inference using markov chain monte carlo methods. dept. of computer science university of toronto r. m. neal. markov chain sampling methods for dirichlet process mixture models. journal of computational and graphical statistics r. m. neal. slice sampling. annals of statistics r. e. neapolitan. learning bayesian networks. prentice hall a. v. nefian luhong l. xiaobo p. liu x. c. mao and k. murphy. a coupled hmm for audio-visual speech recognition. in ieee international conference on acoustics speech and signal processing volume pages d. nilsson. an efficient algorithm for finding the m most probable configurations in a probabilistic expert system. statistics and computing d. nilsson and j. goldberger. sequentially finnding the n-best list in hidden markov models. internation joint conference on artificial intelligence a. b. novikoff. on convergence proofs on perceptrons. in symposium on the mathematical theory of automata york volume pages brooklyn n.y. polytechnic press of polytechnic institute of brooklyn. f. j. och and h. ney. discriminative training and maximum entropy models for statistical machine translation. in proceedings of the annual meeting of the association for computational linguistics pages philadelphia july b. a. olshausen and d. j. field. sparse coding with an overcomplete basis set a strategy employed by vision research a. v. oppenheim r. w. shafer m. t. yoder and w. t. padgett. discrete-time signal processing. prentice hall third edition m. ostendorf v. digalakis and o. a. kimball. from hmms to segment models a unified view of stochastic modeling for speech recognition. ieee transactions on speech and audio processing p. paatero and u. tapper. positive matrix factorization a non-negative factor model with optimal utilization of error estimates of data values. environmetrics v. pavlovic j. m. rehg and j. maccormick. learning switching linear models of human motion. in t. k. leen t. g. dietterich and v. tresp editors advances in neural information processing systems number pages cambridge ma mit press. j. pearl. probabilistic reasoning in intelligent systems networks of plausible inference. morgan kaufmann j. pearl. causality models reasoning and inference. cambridge university press b. a. pearlmutter and l. c. parra. maximum likelihood blind source separation a context-sensitive generalization of ica. in m. c. mozer m. i. jordan and t. petsche editors advances in neural information processing systems number pages cambridge ma mit press. k. b. petersen and o. winther. the em algorithm in independent component analysis. in ieee international conference on acoustics speech and signal processing volume pages j-p. pfister t. toyiozumi d. barber and w. gerstner. optimal spike-timing dependent plasticity for precise action potential firing in supervised learning. neural computation j. platt. fast training of support vector machines using sequential minimal optimization. in b. sch olkopf c. j. c. burges and a. j. smola editors advances in kernel methods support vector learning pages mit press draft march bibliography bibliography i. porteous d. newman a. ihler a. asuncion p. smyth and m. welling. fast collapsed gibbs sampling for latent dirichlet allocation. in kdd proceeding of the acm sigkdd international conference on knowledge discovery and data mining pages new york ny usa acm. j. e. potter and r. g. stern. statistical filtering of space navigation measurements. in american institute of aeronautics and astronautics guidance and control conference volume pages cambridge mass. august w. press w. vettering s. teukolsky and b. flannery. numerical recipes in fortran. cambridge university press s. j. d. prince and j. h. elder. probabilistic linear discriminant analysis for inferences about identity. in ieee international conference on computer vision iccv pages l. r. rabiner. a tutorial on hidden markov models and selected applications in speech recognition. proc. of the ieee c. e. rasmussen and c. k. i. williams. gaussian processes for machine learning. mit press h. e. rauch g. tung and c. t. striebel. maximum likelihood estimates of linear dynamic systems. american institute of aeronautics and astronautics journal t. richardson and p. spirtes. ancestral graph markov models. annals of statistics d. rose r. e. tarjan and e. s. lueker. algorithmic aspects of vertex elimination of graphs. siam journal on computing f. rosenblatt. the perceptron a probabilistic model for information storage and organization in the brain. psychological review d. b. rubin. using the sir algorithm to simulate posterior distributions. in m. h. bernardo k. m. degroot d. v. lindley and a. f. m. smith editors bayesian statistics oxford university press d. saad and m. opper. advanced mean field methods theory and practice. mit press r. salakhutdinov s. roweis and z. ghahramani. optimization with em and expectation-conjugategradient. in t. fawcett and n. mishra editors international conference on machine learning number pages menlo park ca aaai press. l. k. saul t. s. jaakkola and m. j. jordan. mean field theory for sigmoid belief networks. journal of artificial intelligence research l. k. saul and m. i. jordan. exploiting tractable substructures in intractable networks. in d. s. touretzky m. mozer and m. e. hasselmo editors advances in neural information processing systems number pages cambridge ma mit press. l. savage. the foundations of statistics. wiley r. d. schachter. bayes-ball the rational pastime determining irrelevance and requisite information in g. cooper and s. moral editors uncertainty in artificial in belief networks and influence diagrams. intelligence number pages san francisco ca morgan kaufmann. b. sch olkopf a. smola and k. r. m uller. nonlinear component analysis as a kernel eigenvalue problem. neural computation n. n. schraudolph and d. kamenetsky. efficient exact inference in planar ising models. in d. koller d. schuurmans y. bengio and l. bottou editors advances in neural information processing systems number pages cambridge ma mit press. e. schwarz. estimating the dimension of a model. annals of statistics m. seeger. gaussian processes for machine learning. international journal of neural systems m. seeger. expectation propagation for exponential families. technical report department of eecs berkeley www.kyb.tuebingen.mpg.debspeopleseeger. draft march bibliography bibliography m. seeger and h. nickisch. large scale variational inference and experimental design for sparse generalized linear models. technical report max planck institute for biological cybernetics september j. shawe-taylor and n. cristianini. kernel methods for pattern analysis. cambridge university press s. siddiqi b. boots and g. gordon. a constraint generation approach to learning stable linear dynamical systems. in j. c. platt d. koller y. singer and s. roweis editors advances in neural information processing systems number pages cambridge ma mit press. t. silander p. kontkanen and p. myllym aki. on sensitivity of the map bayesian network structure to the equivalent sample size parameter. in r. parr and l. van der gaag editors uncertainty in artificial intelligence number pages corvallis oregon usa auai press. s. s. skiena. the algorithm design manual. springer-verlag new york usa e. smith and m. s. lewicki. efficient auditory coding. nature p. smolensky. parallel distributed processing volume foundations chapter information processing in dynamical systems foundations of harmony theory pages mit press cambridge ma g. sneddon. studies in the atmospheric sciences chapter a statistical perspective on data assimilation in numerical models. number in lecture notes in statistics. springer-verlag p. sollich. bayesian methods for support vector machines evidence and predictive class probabilities. machine learning d. x. song d. wagner and x. tian. timing analysis of keystrokes and timing attacks on ssh. proceedings of the conference on usenix security symposium. usenix association in a. s. spanias. speech coding a tutorial review. proceedings of the ieee oct d. j. spiegelhalter a. p. dawid s. l. lauritzen and r. g. cowell. bayesian analysis in expert systems. statistical science p. spirtes c. glymour and r. scheines. causation prediction and search. mit press edition n. srebro. maximum likelihood bounded tree-width markov networks. in j. breese and d. koller editors uncertainty in artificial intelligence number pages san francisco ca morgan kaufmann. h. steck. constraint-based structural learning in bayesian networks using finite data sets. phd thesis technical university munich h. steck. learning the bayesian network structure dirichlet prior vs data. in d. a. mcallester and p. myllymaki editors uncertainty in artificial intelligence number pages corvallis oregon usa auai press. h. steck and t. jaakkola. on the dirichlet prior and bayesian regularization. in s. becker s. thrun and k. obermayer editors nips pages mit press m. studen y. on mathematical description of probabilistic conditional independence structures. phd thesis academy of sciences of the czech republic m. studen y. on non-graphical description of models of conditional independence structure. in hsss workshop on stochastic systems for individual behaviours. louvain la neueve belgium january c. sutton and a. mccallum. an introduction to conditional random fields for relational learning. in l. getoor and b. taskar editors introduction to statistical relational learning. mit press r. s. sutton and a. g. barto. reinforcement learning an introduction. mit press r. j. swendsen and j-s. wang. nonuniversal critical dynamics in monte carlo simulations. physical review letters b. k. sy. a recurrence local computation approach towards ordering composite beliefs in bayesian belief networks. international journal of approximate reasoning t. sejnowski. the book of hebb. neuron draft march bibliography bibliography r. e. tarjan and m. yannakakis. simple linear-time algorithms to test chordality of graphs test acyclicity of hypergraphs and selectively reduce acyclic hypergraphs. siam journal on computing s. j. taylor. modelling financial time series. world scientific second edition y. w. teh d. newman and m. welling. a collapsed variational bayesian inference algorithm for latent dirichlet allocation. in j. c. platt d. koller y. singer and s. roweis editors advances in neural information processing systems number pages cambridge ma mit press. y. w. teh and m. welling. the unified propagation and scaling algorithm. in t. g. dietterich s. becker and z. ghahramani editors advances in neural information processing systems number pages cambridge ma mit press. m. tipping and c. m. bishop. mixtures of probabilistic principal component analysers. neural computation m. e. tipping. sparse bayesian learning and the relevance vector machine. journal of machine learning research d. m. titterington a. f. m. smith and u. e. makov. statistical analysis of finite mixture distributions. wiley e. todorov. efficient computation of optimal actions. proceedings of the national academy of sciences of the united states of america m. toussaint s. harmeling and a. storkey. probabilistic inference for solving research report university of edinburgh school of informatics m. tsodyks k. pawelzik and h. markram. neural networks with dynamic synapses. neural computation p. van overschee and b. de moor. subspace identification for linear systems theory implementations applications. kluwer v. vapnik. the nature of statistical learning theory. springer new york m. verhaegen and p. van dooren. numerical aspects of different kalman filter implementations. ieee transactions of automatic control t. verma and j. pearl. causal networks semantics and expressiveness. in r. d. schacter t. s. levitt l. n. kanal and j.f. lemmer editors uncertainty in artificial intelligence volume pages amsterdam north-holland. t. o. virtanen a. t. cemgil and s. j. godsill. bayesian extensions to nonnegative matrix factorisation for audio signal modelling. in ieee international conference on acoustics speech and signal processing pages g. wahba. support vector machines repreducing kernel hilbert spaces and randomized gacv pages mit press m. j. wainwright and m. i. jordan. graphical models exponential families and variational inference. foun dations and trends in machine learning h. wallach. efficient training of conditional random fields. master s thesis division of informatics university of edinburgh y. wang j. hodges and b. tang. classification of web documents using a naive bayes method. ieee international conference on tools with artificial intelligence pages s. waterhouse d. mackay and t. robinson. bayesian methods for mixtures of experts. in d. s. touretzky m. mozer and m. e. hasselmo editors advances in neural information processing systems number pages cambridge ma mit press. y. weiss and w. t. freeman. correctness of belief propagation in gaussian graphical models of arbitrary topology. neural computation draft march bibliography bibliography m. welling t. p. minka and y. w. teh. structured region graphs morphing ep into gbp. in f. bacchus and t. jaakkola editors uncertainty in artificial intelligence number pages corvallis oregon usa auai press. j. whittaker. graphical models in applied multivariate statistics. john wiley sons w. wiegerinck. variational approximations between mean field theory and the junction tree algorithm. in c. boutilier and m. goldszmidt editors uncertainty in artificial intelligence number pages san francisco ca morgan kaufmann. w. wiegerinck and t. heskes. fractional belief propagation. in s. becker s. thrun and k. obermayer editors advances in neural information processing systems number pages cambridge ma mit press. c. k. i. williams. computing with infinite networks. in m. c. mozer m. i. jordan and t. petsche editors advances in neural information processing systems nips pages cambridge ma mit press. c. k. i. williams and d. barber. bayesian classification with gaussian processes. ieee trans pattern analysis and machine intelligence c. yanover and y. weiss. finding the m most probable configurations using loopy belief propagation. in s. thrun l. saul and b. sch olkopf editors advances in neural information processing systems number pages cambridge ma mit press. j. s. yedidia w. t. freeman and y. weiss. constructing free-energy approximations and generalized belief propagation algorithms. information theory ieee transactions on july s. young d. kershaw j. odell d. ollason v. valtchev and p. woodland. the htk book version cambridge university press a. l. yuille and a. rangarajan. the concave-convex procedure. neural computation j.-h. zhao p. l. h. yu and q. jiang. ml estimation for factor analysis em or non-em? statistics and computing o. zoeter. monitoring non-linear and switching dynamical systems. phd thesis radboud university nijmegen draft march index of m coding n-max-product absorbing state absorption influence diagram acceptance function active learning adjacency matrix algebraic riccati equation ancestor ancestral ordering ancestral sampling antifreeze approximate inference belief propagation bethe free energy double integration bound expectation propagation graph cut laplace approximation switching linear dynamical system variational approach variational inference ar model see auto-regressive model artificial life asychronous updating asymmetry auto-regressive model switching time-varying automatic relevance determination auxiliary variable sampling average backtracking bag of words batch update baum-welch bayes information criterion bayes factor model selection theorem bayes rule see bayes theorem bayesian decision theory hypothesis testing image denoising linear model mixture model model selection occam s razor outcome analysis bayesian dirichlet score bayesian linear model bd score bdeu score bdeu score belief network asbestos-smoking-cancer cascade chest clinic divorcing parents dynamic noisy and gate noisy logic gate noisy or gate sigmoid structure learning training bayesian belief propagation loopy belief revision see max-product bellman s equation bessel function beta distribution function bethe free energy index bias unbiased estimator bigram binary entropy bioinformatics black and white sampling black-box blahut-arimoto algorithm boltzmann machine restricted bond propagation bonferroni inequality boolean network bradly-terry-luce model bucket elimination burn in calculus canonical correlation analysis constrained factor analysis canonical variates causal consistency causality do calculus influence diagrams post intervention distribution cca see canonical correlation analysis centering chain graph chain component chain rule chain structure changepoint model checkerboard chest clinic missing data with decisions children see directed acyclic graph cholesky chord chordal chow-liu tree classification bayesian boundary error analysis linear parameter model multiple classes performance random guessing softmax clique decomposition graph index matrix cliquo cluster variation method clustering collaborative filtering collider see directed acyclic graph commute compatibility function competition model bradly-terry-luce elo trueskill competition models concave function condindep.m conditional entropy conditional likelihood conditional mutual information conditional probability conditional random field conditioning loop cut set conjugate distribution exponential family gaussian prior conjugate gradient conjugate gradients algorithm conjugate vector conjugate vectors algorithm connected components connected graph consistent consistent estimator convex function correction smoother correlation matrix cosine similarity coupled hmm covariance matrix covariance function construction gibbs isotropic mat ern mercer kernel neural network non-stationary ornstein-uhlenbeck periodic rational quadratic smoothness draft march index index squared exponential stationary cpt see conditional probability table crf see conditional random field critical point cross-validation cumulant curse of dimensionality cut set conditioning d-map see dependence map dag see directed acyclic graph data anomaly detection catagorical dyadic handwritten digits labelled monadic numerical ordinal unlabelled data compression vector quantisation decision boundary decision function decision theory decision tree decomposable degree degree of belief delta function see dirac delta function kronecker density estimation parzen estimator dependence map descendant design matrix detailed balance determinant deterministic latent variable model differentiation digamma function digit data dijkstra s algorithm dimension reduction linear supervised dimensionality reduction linear non-linear dirac delta function directed acyclic graph ancestor draft march ancestral order cascade children collider descendant family immorality markov blanket moralisation parents direction bias directional derivative dirichlet distribution dirichlet process mixture models discount factor discriminative approach training discriminative approach discriminative training dissimilarity function distributed computation distribution bernoulli beta binomial categorical change of variables conjugate continuous density dirichlet discrete divergence double exponential empirical average expectation exponential exponential family canonical gamma mode gauss-gamma gauss-inverse-gamma gaussian canonical exponential form conditioning conjugate entropy isotropic mixture multivariate normalisation index index partitioned propagation system reversal univariate inverse gamma inverse wishart joint kurtosis laplace marginal mode multinomial normal poisson polya scaled mixture skewness student s t uniform wishart domain double integration bound dual parameters dual representation dyadic data dynamic bayesian network dynamic synapses dynamical system linear non-linear dynamics reversal edge list efficient ipf eigen decomposition equation function problem spectrum value elo model emission distribution emission matrix empirical independence empirical distribution empirical risk penalised empirical risk minimisation energy entropy differential gaussian ep see expectation propagation error function estimator consistent evidence see marginal likelihood hard likelihood soft uncertain virtual evidence procedure exact sampling expectation see average expectation correction expectation maximisation algorithm antifreeze belief networks e-step energy entropy failure case generalised intractable energy m-step markov decision process mixture model partial e-step partial m-step variational bayes viterbi training expectation propagation exponential family canonical form conjugate extended observability matrix face model factor analysis factor rotation probabilistic pca training em svd factor graph factor loading family see directed acyclic graph feature map filtering finite dimensional gaussian process fisher information fisher s linear discriminant floyd-warshall-roy algorithm forward sampling forward-backward draft march index index forward-sampling-resampling gamma digamma distribution function gaussian canonical representation distribution moment representation sub super gaussian mixture model bayesian collapsing the mixture em algorithm infinite problems k-means parzen estimator symmetry breaking gaussian process classification laplace approximation multiple classes regression smoothness weight space view gaussian sum filtering gaussian sum smoothing generalisation generalised pseudo bayes generative approach model training generative approach gibbs sampling glicko gmm see gaussian mixture model google gradient descent natural gram matrix gram-schmidt procedure gramm matrix graph adjacency matrix chain chain structured chordal clique clique matrix cliquo connected draft march cut decomposable descendant directed disconnected edge list factor loopy multiply-connected neighbour path separation set chain singly-connected skeleton spanning tree tree triangulated undirected vertex degree graph cut algorithm graph partitioning gull-mackay iteration hamilton-jacobi equation hamiltonian dynamics hammersley clifford theorem handwritten digits hankel matrix harmonium see restricted boltzmann machine heaviside step function hebb hebb rule hedge fund hermitian hessian hidden markov model recursion recursion coupled direction bias discriminative training duration model entropy filtering input-output likelihood most likely state pairwise marginal rauch tung striebel smoother smoothing parallel sequential viterbi index viterbi algorithm hidden variables hmm see hidden markov model hopfield network augmented capacity hebb rule heteroassociative maximum likelihood perceptron pseudo inverse rule sequence learning hybrid monte carlo hyper markov hyper tree hyperparameter hyperplane hypothesis testing bayesian error analysis i-map see independence map ica identically and independently distributed identifiability identity matrix iid see identically and independently distributed im algorithm see information-maximisation algo rithm immorality importance distribution sampling particle filter resampling sequential weight incidence matrix independence bayesian conditional empirical map markov equivalent mutual information naive bayes parameter perfect map independent components analysis indicator function indicator model induced representation inference bond propagation bucket elimination causal index cut set conditioning hmm linear dynamical system map marginal markov decision process max-product message passing mixed mpm sum-product algorithm transfer matrix variable elimination influence diagram absorption asymmetry causal consistency chest clinic decision potential fundamental link information link junction tree no forgetting principle partial order probability potential solving utility utility potential information link information maximisation information retrieval information-maximisation algorithm innovation noise input-output hmm inverse modus ponens ipf see iterative proportional fitting efficient ising model see markov network approximate inference isotropic isotropic covariance functions item response theory iterated conditional modes iterative proportional fitting iterative scaling jeffrey s rule jensen s inequality joseph s symmetrized update jump markov model see switching linear dynamical system junction tree absorption algorithm clique graph draft march index index computational complexity conditional marginal consistent hyper tree influence diagram marginal likelihood most likely state normalisation constant potential running intersection property separator strong strong triangulation tree width triangulation k-means kalman filter kalman gain kd-tree kernel see covariance function classifier kidnapped robot kikuchi kl divergence see kullback-leibler divergence knn see nearest neighbour kronecker delta kullback-leibler divergence kurtosis labelled data lagrange multiplier lagrangian laplace approximation latent ability model latent dirichlet allocation latent linear model latent semantic analysis latent topic latent variable deterministic model lattice model lda regularised lds see linear dynamical system leaky integrate and fire model leapfrog discretisation learning active anomaly detection bayesian belief network belief networks em draft march dirichlet prior inference nearest neighbour online query reinforcement semi-supervised sequences sequential structure supervised unsupervised learning rate likelihood bound marginal model approximate pseudo likelihood decomposable line search linear algebra linear dimension reduction canonical correlation analysis latent semantic analysis non-negative matrix factorisation probabilistic latent semantic analysis supervised unsupervised linear discriminant analysis linear discriminant analysis as regression penalised regularised linear dynamical system cross moment dynamics reversal filtering identifiability inference learning likelihood most likely state numerical stability riccati equations smoothing subspace method switching symmetrising updates linear gaussian state space model linear model bayesian classification factor analysis latent index index regression linear parameter model bayesian linear perceptron linear separability linear transformation linearly independent linearly separable linsker s as-if-gaussian approximation localisation logic aristotle logistic regression logistic sigmoid logit loop cut set loopy loss function zero-one loss matrix luenberger expanding subspace theorem mahalanobis distance manifold linear low dimensional map see most probable a posteriori mar see missing at random margin soft marginal generalised marginal likelihood approximate marginalisation markov chain first order stationary distribution equivalent global hyper local model pairwise random field approximation markov blanket see directed acyclic graph markov chain absorbing state detailed balance pagerank markov chain monte carlo auxiliary variable hybrid monte carlo slice sampling swendson-wang gibbs sampling metropolis-hastings proposal distribution structured gibbs sampling markov decision process bellman s equation discount factor non-stationary policy partially observable planning policy iteration reinforcement learning stationary stationary deterministic policy temporally unbounded value iteration variational inference markov equivalence markov network boltzmann machine continuous-state temporal discrete-state temporal gibbs distribution gibbs network hammersley clifford theorem pairwise potential markov random field alpha-expansion attractive binary graph cut map potts model matrix adjacency cholesky clique gramm hankel incidence inversion inversion lemma orthogonal positive definite pseudo inverse rank matrix factorisation max-product n most probable states max-sum maximum cardinality checking maximum likelihood belief network draft march index index chow-liu tree counting empirical distribution factor analysis gaussian gradient optimisation markov network ml-ii naive bayes properties maximum likelihood hopfield network mcmc see markov chain monte carlo mdp see markov decision process mean field theory asynchronous updating mercer kernel message passing schedule message passing metropolis-hastings acceptance function metropolis-hastings sampling minimum clique cover missing at random completely missing data mixed inference mixed membership model mixing matrix mixture gaussian mixture model bernoulli product dirichlet process mixture expectation maximisation factor analysis gaussian indicator approach markov chain pca mixture of experts mn see markov network mode model auto-regressive changepoint deterministic latent variable faces leaky integrate and fire linear mixed membership mixture rasch model selection draft march approximate moment generating function moment representation momentum monadic data money financial prediction loadsa moralisation most probable a posteriori most probable path multiple-source multiple-sink most probable state n most probable mrf see markov random field multiply-connected multiply-connected-distributions multpots.m mutual information approximation conditional maximisation naive bayes bayesian tree augmented naive mean field naive mean field theory natural gradient nearest neighbour probabilistic network flow network modelling neural computation neural network depression dynamic synapses leaky integrate and fire newton update newton s method no forgetting principle node extremal simplical non-negative matrix factorisation normal distribution normal equations normalised importance weights observed linear dynamical system occam s razor one of m encoding online learning optimisation broyden-fletcher-goldfarb-shanno index index conjugate gradients algorithm conjugate vectors algorithm constrained optimisation critical point gradient descent luenberger expanding subspace theorem newton s method quasi newton method ordinary least squares ornstein-uhlenbeck orthogonal orthogonal least squares orthonormal outcome analysis outlier over-complete representation over-complete representations overcounting overfitting pagerank pairwise comparison models pairwise markov network parents see directed acyclic graph part-of-speech tagging partial least squares partial order partially observable mdp particle filter partition function partitioned matrix inversion parzen estimator path blocked pc algorithm pca see principal components analysis perceptron logistic regression perfect elimination order perfect map see independence perfect sampling planning plant monitoring plate poisson distribution policy iteration non-stationary stationary deterministic polya distribution pomdp see partially observable mdp positive definite kernel matrix parameterisation posterior dirichlet potential potts model precision prediction auto-regression financial non-parametric parameteric predictive variance predictor-corrector principal components analysis algorithm high dimensional data kernel latent semantic analysis missing data probabilistic principal directions printer nightmare missing data prior probabilistic latent semantic analysis conditional em algorithm probabilistic pca probability conditional function density frequentist posterior potential prior subjective probit probit regression projection proposal distribution pseudo inverse pseudo inverse hopfield network pseudo likelihood quadratic form quadratic programming query learning questionnaire analysis radial basis functions raleigh quotient random boolean networks rasch model draft march index bayesian rauch-tung-striebel reabsorption region graphs regresion linear parameter model regression logisitic regularisation reinforcement learning relevance vector machine relevance vector machine reparameterisation representation dual over-complete sparse under-complete resampling reset model residuals resolution responsibility restricted boltzmann machine riccati equation risk robust classification rose-tarjan-lueker elimination running intersection property sample mean variance sampling ancestral gibbs importance multi-variate particle filter univariate sampling importance resampling scalar product scaled mixture search engine self localisation and mapping semi-supervised learning lower dimensional representations separator sequential importance sampling sequential minimal optimisation set chain shortest path shortest weighted path sigmoid logistic draft march sigmoid belief network sigmoid function approximate average simple path simplical nodes simpson s paradox singly-connected singular singular value decomposition thin skeleton skewness slice sampling smoothing softmax function spam filtering spanning tree sparse representation spectrogram speech recognition spike response model squared euclidean distance squared exponential standard deviation standard normal distribution stationary distribution stationary markov chain stationary planner stop words strong junction tree strong triangulation structure learning bayesian network scoring pc algorithm undirected structured expectation propagation subsampling subspace method sum-product sum-product algorithm supervised learning classification regression support vector machine chunking training support vectors svd see singular value decomposition svm see support vector machine swendson-wang sampling switching ar model index index index switching kalman filter see switching linear dynam ical system switching linear dynamical system under-complete representation undirected graph undirected model changepoint model expectation correction filtering gaussian sum smoothing generalised pseudo bayes inference computational complexity likelihood smoothing switching linear dynamical systemcollapsing gaus sians symmetry breaking system reversal tagging tall matrix taylor expansion term-document matrix test set text analysis latent semantic analysis latent topic probabilistic latent semantic analysis time-invariant lds tower of hanoi trace-log formula train set training batch discriminative generative generative-discriminative hmm linear dynamical system online transfer matrix transition distribution transition matrix tree chow-liu tree augmented network tree width triangulation check greedy elimination maximum cardinality strong variable elimination trueskill two-filter smoother uncertainty learning hidden variable latent variable uniform distribution unit vector unlabelled data unsupervised learning utility matrix message money potential zero-one loss validation cross value value iteration variable hidden missing visible variable elimination variance variational approximation factorised structured variational bayes expectation maximisation variational inference varimax vector algebra vector quantisation viterbi viterbi algorithm viterbi alignment voronoi tessellation web modelling website analysis whitening woodbury formula xor function zero-one loss draft march