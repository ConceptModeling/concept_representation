a brief introduction to neural networks david kriesel dkriesel.com download locationhttpwww.dkriesel.comenscienceneural_networksnew for the programmers scalable and efficient nn framework written in java httpwww.dkriesel.comentechsnipe dkriesel.com in remembrance of dr. peter kemp notary bonn germany. d. kriesel a brief introduction to neural networks iii a small preface this work has been prepared in the framework of a seminar of the university of bonn in germany but it has been and will be extended being presented and published online under www.dkriesel.com on first and foremost to provide a comprehensive overview of the subject of neural networks and second just to acquire more and more knowledge about latex and who knows maybe one day this summary will become a real preface! abstract of this work end of the above abstract has not yet become a preface but at least a little preface ever since the extended text pages long has turned out to be a download hit. ambition and intention of this manuscript the entire text is written and laid out more effectively and with more illustrations than before. i did all the illustrations myself most of them directly in latex by using xypic. they reflect what i would have liked to see when becoming acquainted with the subject text and illustrations should be memorable and easy to understand to offer as many people as possible access to the field of neural networks. nevertheless the mathematically and formally skilled readers will be able to under stand the definitions without reading the running text while the opposite holds for readers only interested in the subject matter everything is explained in both colloquial and formal language. please let me know if you find out that i have violated this principle. the sections of this text are mostly independent from each other the document itself is divided into different parts which are again divided into chapters. although the chapters contain cross-references they are also individually accessible to readers with little previous knowledge. there are larger and smaller chapters while the larger chapters should provide profound insight into a paradigm of neural networks the classic neural network structure the perceptron and its learning procedures the smaller chapters give a short overview but this is also ex v dkriesel.com the original high-performance simulation design goal. those of you who are up for learning by doing andor have to use a fast and stable neural networks implementation for some reasons should definetely have a look at snipe. however the aspects covered by snipe are not entirely congruent with those covered by this manuscript. some of the kinds of neural networks are not supported by snipe while when it comes to other kinds of neural networks snipe may have lots and lots more capabilities than may ever be covered in the manuscript in the form of practical hints. anyway in my experience almost all of the implementation requirements of my readers are covered well. on the snipe download page look for the section started with snipe you will find an easy step-by-step guide concerning snipe and its documentation as well as some examples. snipe this manuscript frequently incorporates snipe. shaded snipe-paragraphs like this one are scattered among large parts of the manuscript providing information on how to implement their context in snipe. this also implies that those who do not want to use snipe just have to skip the shaded snipeparagraphs! the snipe-paragraphs assume the reader has had a close look at the started with snipe section. often class names are used. as snipe consists of only a few different packages i omitted the package names within the qualified class names for the sake of readability. plained in the introduction of each chapter. in addition to all the definitions and explanations i have included some excursuses to provide interesting information not directly related to the subject. unfortunately i was not able to find free german sources that are multi-faceted in respect of content the paradigms of neural networks and nevertheless written in coherent style. the aim of this work is if it could not be fulfilled at first go to close this gap bit by bit and to provide easy access to the subject. want to learn not only by reading but also by coding? use snipe! is a well-documented java library that implements a framework for neural networks in a speedy feature-rich and usable way. it is available at no cost for non-commercial purposes. it was originally designed for high performance simulations with lots and lots of neural networks large ones being trained simultaneously. recently i decided to give it away as a professional reference implementation that covers network aspects handled within this work while at the same time being faster and more efficient than lots of other implementations due to scalable and generalized neural information processing engine downloadable at httpwww. dkriesel.comtechsnipe online javadoc at httpsnipe.dkriesel.com vi d. kriesel a brief introduction to neural networks dkriesel.com it s easy to print this manuscript this text is completely illustrated in color but it can also be printed as is in monochrome the colors of figures tables and text are well-chosen so that in addition to an appealing design the colors are still easy to distinguish when printed in monochrome. there are many tools directly integrated into the text different aids are directly integrated in the document to make reading more flexible however anyone me who prefers reading words on paper rather than on screen can also enjoy some features. in the table of contents different types of chapters are marked different types of chapters are directly marked within the table of contents. chapters that are marked as are definitely ones to read because almost all subsequent chapters heavily depend on them. other chapters additionally depend on information given in other chapters which then is marked in the table of contents too. speaking headlines throughout the text short ones in the table of contents the whole manuscript is now pervaded by such headlines. speaking headlines are not just title-like learning but centralize the information given in the associated section to a single sentence. in the named instance an appropriate headline would be learning methods provide feedback to the network whether it behaves good or bad. however such long headlines would bloat the table of contents in an unacceptable way. so i used short titles like the first one in the table of contents and speaking ones like the latter throughout the text. marginal notes are a navigational aid the entire document contains marginal notes in colloquial language the exam- hypertext ple in the margin allowing you to on paper the document quickly to find a certain passage in the text the titles. new mathematical symbols are marked by specific marginal notes for easy finding the example for x in the margin. there are several kinds of indexing this document contains different types of indexing if you have found a word in the index and opened the corresponding page you can easily find it by searching d. kriesel a brief introduction to neural networks vii dkriesel.com for highlighted text all indexed words are highlighted like this. mathematical symbols appearing in several chapters of this document for an output neuron i tried to maintain a consistent nomenclature for regularly recurring elements are separately indexed under symbols so they can easily be assigned to the corresponding term. names of persons written in small caps are indexed in the category and ordered by the last names. you must maintain the author s attribution of the document at all times. you may not use the attribution to imply that the author endorses you or your document use. for i m no lawyer the above bullet-point summary is just informational if there is any conflict in interpretation between the summary and the actual license the actual license always takes precedence. note that this license does not extend to the source files used to produce the document. those are still mine. terms of use and license beginning with the epsilon edition the text is licensed under the creative commons attribution-no derivative works unported except for some little portions of the work licensed under more liberal licenses as mentioned some figures from wikimedia commons. a quick license summary you are free to redistribute this document though it is a much better idea to just distribute the url of my homepage for it always contains the most recent version of the text. you may not modify transform or build upon the document except for personal use. httpcreativecommons.orglicenses how to cite this manuscript there s no official publisher so you need to be careful with your citation. please find more information in english and german language on my homepage respectively the subpage concerning the acknowledgement now i would like to express my gratitude to all the people who contributed in whatever manner to the success of this work since a work like this needs many helpers. first of all i want to thank the proofreaders of this text who helped me and my readers very much. in alphabetical order wolfgang apolinarski kathrin gr ve paul imhoff thomas httpwww.dkriesel.comenscience neural_networks viii d. kriesel a brief introduction to neural networks dkriesel.com k hn christoph kunze malte lohmeyer joachim nock daniel plohmann daniel rosenthal christian schulz and tobias wilken. additionally i want to thank the readers dietmar berger igor buchm ller marie christ julia damaschek jochen d ll maximilian ernestus hardy falk anne feldmeier sascha fink andreas friedmann jan gassen markus gerhards sebastian hirsch andreas hochrath nico h ft thomas ihme boris jentsch tim hussein thilo keller mario krenn mirko kunze maikel linke adam maciak benjamin meier david m ller andreas m ller rainer penninger lena reichel alexander schier matthias siegmund mathias tirtasana oliver tischler maximilian voit igor wall achim weber frank weinreis gideon maillette de buij wenniger philipp woock and many others for their feedback suggestions and remarks. additionally i d like to thank sebastian merzbach who examined this work in a very conscientious way finding inconsistencies and errors. in particular he cleared lots and lots of language clumsiness from the english version. especially i would like to thank beate kuhl for translating the entire text from german to english and for her questions which made me think of changing the phrasing of some paragraphs. i would particularly like to thank prof. rolf eckmiller and dr. nils goerke as well as the entire division of neuroinformatics department of computer science of the university of bonn they all made sure that i always learned also had to learn something new about neural networks and related subjects. especially dr. goerke has always been willing to respond to any questions i was not able to answer myself during the writing process. conversations with prof. eckmiller made me step back from the whiteboard to get a better overall view on what i was doing and what i should do next. globally and not only in the context of this work i want to thank my parents who never get tired to buy me specialized and therefore expensive books and who have always supported me in my studies. for many and the very special and cordial atmosphere i want to thank andreas huber and tobias treutler. since our first semester it has rarely been boring with you! now i would like to think back to my school days and cordially thank some teachers who my opinion had imparted some scientific knowledge to me although my class participation had not always been wholehearted mr. wilfried hartmann mr. hubert peters and mr. frank n kel. furthermore i would like to thank the whole team at the notary s office of dr. kemp and dr. kolb in bonn where i have always felt to be in good hands and who have helped me to keep my printing costs low in particular christiane flamme and dr. kemp! d. kriesel a brief introduction to neural networks ix dkriesel.com thanks go also to the wikimedia commons where i took some images and altered them to suit this text. last but not least i want to thank two people who made outstanding contributions to this work who occupy so to speak a place of honor my girlfriend verena thomas who found many mathematical and logical errors in my text and discussed them with me although she has lots of other things to do and christiane schultze who carefully reviewed the text for spelling mistakes and inconsistencies. david kriesel x d. kriesel a brief introduction to neural networks contents a small preface i from biology to formalization motivation philosophy history and realization of neural models v introduction motivation and history why neural networks? the rule simple application examples history of neural networks the beginning golden age long silence and slow reconstruction renaissance exercises biological neural networks the vertebrate nervous system peripheral and central nervous system cerebrum cerebellum diencephalon brainstem the neuron components electrochemical processes in the neuron receptor cells various types information processing within the nervous system light sensing organs the amount of neurons in living organisms xi contents dkriesel.com technical neurons as caricature of biology exercises components of artificial neural networks the concept of time in neural networks components of neural networks connections propagation function and network input activation threshold value activation function common activation functions output function learning strategy network topologies feedforward recurrent networks completely linked networks the bias neuron representing neurons orders of activation synchronous activation asynchronous activation input and output of data exercises supervised learning fundamentals on learning and training samples paradigms of learning unsupervised learning reinforcement learning o ine or online learning? questions in advance training patterns and teaching input using training samples division of the training set order of pattern representation learning curve and error measurement when do we stop learning? xii d. kriesel a brief introduction to neural networks dkriesel.com contents problems of gradient procedures gradient optimization procedures exemplary problems boolean functions the parity function the problem the checkerboard problem the identity function other exemplary problems original rule generalized form hebbian rule exercises ii supervised learning network paradigms the perceptron backpropagation and its variants the singlelayer perceptron perceptron learning algorithm and convergence theorem delta rule linear separability the multilayer perceptron backpropagation of error derivation boiling backpropagation down to the delta rule selecting a learning rate resilient backpropagation adaption of weights dynamic learning rate adjustment rprop in practice further variations and extensions to backpropagation momentum term flat spot elimination second order backpropagation weight decay pruning and optimal brain damage initial configuration of a multilayer perceptron number of layers the number of neurons d. kriesel a brief introduction to neural networks xiii contents dkriesel.com selecting an activation function initializing weights the encoding problem and related problems exercises radial basis functions information processing in rbf neurons components and structure information processing of an rbf network analytical thoughts prior to the training training of rbf networks adding neurons limiting the number of neurons deleting neurons comparing rbf networks and multilayer perceptrons exercises centers and widths of rbf neurons growing rbf networks recurrent perceptron-like networks on chapter jordan networks elman networks training recurrent networks unfolding in time teacher forcing recurrent backpropagation training with evolution hopfield networks inspired by magnetism structure and functionality input and output of a hopfield network significance of weights change in the state of neurons generating the weight matrix autoassociation and traditional application heteroassociation and analogies to neural data storage generating the heteroassociative matrix stabilizing the heteroassociations biological motivation of heterassociation xiv d. kriesel a brief introduction to neural networks dkriesel.com contents continuous hopfield networks exercises learning vector quantization about quantization purpose of lvq using codebook vectors adjusting codebook vectors the procedure of learning connection to neural networks exercises iii unsupervised learning network paradigms self-organizing feature maps examples structure functionality and output interpretation training the topology function monotonically decreasing learning rate and neighborhood topological defects adjustment of resolution and position-dependent learning rate application interaction with rbf networks variations neural gas multi-soms multi-neural gas growing neural gas exercises adaptive resonance theory task and structure of an art network resonance learning process pattern input and top-down learning resonance and bottom-up learning adding an output neuron d. kriesel a brief introduction to neural networks xv contents dkriesel.com extensions iv excursi appendices and registers a excursus cluster analysis and regional and online learnable fields k-means clustering k-nearest neighboring neighboring the silhouette coefficient regional and online learnable fields structure of a rolf training a rolf evaluating a rolf comparison with popular clustering methods initializing radii learning rates and multiplier application examples exercises b excursus neural networks used for prediction about time series one-step-ahead prediction two-step-ahead prediction recursive two-step-ahead prediction direct two-step-ahead prediction additional optimization approaches for prediction changing temporal parameters heterogeneous prediction remarks on the prediction of share prices c excursus reinforcement learning system structure the gridworld agent und environment states situations and actions reward and return the policy learning process rewarding strategies the state-value function xvi d. kriesel a brief introduction to neural networks dkriesel.com contents monte carlo method temporal difference learning the action-value function q learning example applications td gammon the car in the pit the pole balancer reinforcement learning in connection with neural networks exercises bibliography list of figures index d. kriesel a brief introduction to neural networks xvii part i from biology to formalization motivation philosophy history and realization of neural models chapter introduction motivation and history how to teach a computer? you can either write a fixed program or you can enable the computer to learn on its own. living beings do not have any programmer writing a program for developing their skills which then only has to be executed. they learn by themselves without the previous knowledge from external impressions and thus can solve problems better than any computer today. what qualities are needed to achieve such a behavior for devices like computers? can such cognition be adapted from biology? history development decline and resurgence of a wide approach to solve problems. why neural networks? there are problem categories that cannot be formulated as an algorithm. problems that depend on many subtle factors for example the purchase price of a real estate which our brain can calculate. without an algorithm a computer cannot do the same. therefore the question to be asked is how do we learn to explore such problems? exactly we learn a capability computers obviously do not have. humans have a brain that can learn. computers have some processing units and memory. they allow the computer to perform the most complex numerical calculations in a very short time but they are not adaptive. if we compare computer and we will note that theoretically the computer should be more powerful than our brain it comprises transistors with a switching time of seconds. the brain contains neurons but these only have a switching time of about seconds. the largest part of the brain is working continuously while the largest part of the computer is only passive data storage. thus the brain is parallel and therefore performing close to its theoretical of course this comparison is for obvious reasons controversially discussed by biologists and computer scientists since response time and quantity do not tell anything about quality and performance of the processing units as well as neurons and transistors cannot be compared directly. nevertheless the comparison serves its purpose and indicates the advantage of parallelism by means of processing time. parallelism computers cannot learn chapter introduction motivation and history dkriesel.com no. of processing units type of processing units type of calculation data storage switching time possible switching operations actual switching operations brain neurons massively parallel associative s s computer transistors usually serial address-based s s table the comparison between brain and computer at a glance. inspired by mum from which the computer is orders of magnitude away additionally a computer is static the brain as a biological neural network can reorganize itself during its and therefore is able to learn to compensate errors and so forth. within this text i want to outline how we can use the said characteristics of our brain for a computer system. so the study of artificial neural networks is motivated by their similarity to successfully working biological systems which in comparison to the overall system consist of very simple but numerous nerve cells that work massively in parallel and is probably one of the most significant aspects have the capability to learn. there is no need to explicitly program a neural network. for instance it can learn from training samples or by means of encouragement with a carrot and a stick so to speak learning. one result from this learning procedure is the capability of neural networks to gen eralize and associate data after successful training a neural network can find reasonable solutions for similar problems of the same class that were not explicitly trained. this in turn results in a high degree of fault tolerance against noisy input data. fault tolerance is closely related to biological neural networks in which this characteristic is very distinct as previously mentioned a human has about neurons that continuously reorganize themselves or are reorganized by external influences neurons can be destroyed while in a drunken stupor some types of food or environmental influences can also destroy brain cells. nevertheless our cognitive abilities are not significantly affected. thus the brain is tolerant against internal errors and also against external errors for we can often read a really scrawl although the individual letters are nearly impossible to read. our modern technology however is not automatically fault-tolerant. i have never heard that someone forgot to install the d. kriesel a brief introduction to neural networks simple but many processing units n. network capable to learn n. network fault tolerant dkriesel.com why neural networks? i.e. hard disk controller into a computer and therefore the graphics card automatically took over its tasks removed conductors and developed communication so that the system as a whole was affected by the missing component but not completely destroyed. a disadvantage of this distributed faulttolerant storage is certainly the fact that we cannot realize at first sight what a neural neutwork knows and performs or where its faults lie. usually it is easier to perform such analyses for conventional algorithms. most often we can only transfer knowledge into our neural network by means of a learning procedure which can cause several errors and is not always easy to manage. fault tolerance of data on the other hand is already more sophisticated in state-ofthe-art technology let us compare a record and a cd. if there is a scratch on a record the audio information on this spot will be completely lost will hear a pop and then the music goes on. on a cd the audio data are distributedly stored a scratch causes a blurry sound in its vicinity but the data stream remains largely unaffected. the listener won t notice anything. so let us summarize the main characteristics we try to adapt from biology self-organization and learning capa bility generalization capability and fault tolerance. what types of neural networks particularly develop what kinds of abilities and can be used for what problem classes will be discussed in the course of this work. in the introductory chapter i want to neural netclarify the following work does not exist. there are different paradigms for neural networks how they are trained and where they are used. my goal is to introduce some of these paradigms and supplement some remarks for practical application. we have already mentioned that our brain works massively in parallel in contrast to the functioning of a computer i.e. every component is active at any time. if we want to state an argument for massive parallel processing then the rule can be cited. the rule experiments showed that a human can recognize the picture of a familiar object or person in seconds which corresponds to a neuron switching time of seconds in discrete time steps of parallel processing. a computer following the von neumann architecture however can do practically nothing in time steps of sequential processing which are assembler steps or cycle steps. now we want to look at a simple application example for a neural network. d. kriesel a brief introduction to neural networks important! parallel processing chapter introduction motivation and history dkriesel.com put is called h for signal. therefore we need a mapping f that applies the input signals to a robot activity. the classical way there are two ways of realizing this mapping. on the one hand there is the classical way we sit down and think for a while and finally the result is a circuit or a small computer program which realizes the mapping is easily possible since the example is very simple. after that we refer to the technical reference of the sensors study their characteristic curve in order to learn the values for the different obstacle distances and embed these values into the aforementioned set of rules. such procedures are applied in the classic artificial intelligence and if you know the exact rules of a mapping algorithm you are always well advised to follow this scheme. the way of learning on the other hand more interesting and more successful for many mappings and problems that are hard to comprehend straightaway is the way of learning we show different possible situations to the robot on page and the robot shall learn on its own what to do in the course of its robot life. in this example the robot shall simply learn when to stop. we first treat the figure a small robot with eight sensors and two motors. the arrow indicates the driving direction. simple application examples let us assume that we have a small robot as shown in fig. this robot has eight distance sensors from which it extracts input data three sensors are placed on the front right three on the front left and two on the back. each sensor provides a real numeric value at any time that means we are always receiving an input i despite its two motors will be needed later the robot in our simple example is not capable to do much it shall only drive on but stop when it might collide with an obstacle. thus our output is binary h for is okay drive on and h for out d. kriesel a brief introduction to neural networks dkriesel.com why neural networks? our example can be optionally expanded. for the purpose of direction control it would be possible to control the motors of our robot with the sensor layout being the same. in this case we are looking for a mapping f which gradually controls the two motors by means of the sensor inputs and thus cannot only for example stop the robot but also lets it avoid obstacles. here it is more difficult to analytically derive the rules and de facto a neural network would be more appropriate. our goal is not to learn the samples by heart but to realize the principle behind them ideally the robot should apply the neural network in any situation and be able to avoid obstacles. in particular the robot should query the network continuously and repeatedly while driving in order to continously avoid obstacles. the result is a constant cycle the robot queries the network. as a consequence it will drive in one direction which changes the sensors values. again the robot queries the network and changes its position the sensor values are changed once again and so on. it is obvious that this system can also be adapted to dynamic i.e changing environments the moving obstacles in our example. there is a robot called khepera with more or less similar characteristics. it is round-shaped approx. cm in diameter has two motors with wheels and various sensors. for more information i recommend to refer to the internet. figure initially we regard the robot control as a black box whose inner life is unknown. the black box receives eight real sensor values and maps these values to a binary output value. neural network as a kind of black box this means we do not know its structure but just regard its behavior in practice. the situations in form of simply measured sensor values placing the robot in front of an obstacle see illustration which we show to the robot and for which we specify whether to drive on or to stop are called training samples. thus a training sample consists of an exemplary input and a corresponding desired output. now the question is how to transfer this knowledge the information into the neural network. the samples can be taught to a neural network by using a simple learning procedure learning procedure is a simple algorithm or a mathematical formula. if we have done everything right and chosen good samples the neural network will generalize from these samples and find a universal rule when it has to stop. d. kriesel a brief introduction to neural networks chapter introduction motivation and history dkriesel.com figure the robot is positioned in a landscape that provides sensor values for different situations. we add the desired output values h and so receive our learning samples. the directions in which the sensors are oriented are exemplarily applied to two robots. a brief history of neural networks the field of neural networks has like any other field of science a long history of development with many ups and downs as we will see soon. to continue the style of my work i will not represent this history in text form but more compact in form of a timeline. citations and bibliographical references are added mainly for those topics that will not be further discussed in this text. citations for keywords that will be explained later are mentioned in the corresponding chapters. the history of neural networks begins in the early s and thus nearly simulta neously with the history of programmable electronic computers. the youth of this field of research as with the field of computer science itself can be easily recognized due to the fact that many of the cited persons are still with us. the beginning as soon as warren mcculloch and walter pitts introduced models of neurological networks recreated threshold switches based on neurons and showed that even simple networks of this kind are able to calculate nearly any logic or arithmetic function further d. kriesel a brief introduction to neural networks dkriesel.com history of neural networks figure some institutions of the field of neural networks. from left to right john von neumann donald o. hebb marvin minsky bernard widrow seymour papert teuvo kohonen john hopfield the order of appearance as far as possible. more the first computer precursors brainswere developed among others supported by konrad zuse who was tired of calculating ballistic trajectories by hand. walter pitts and warren mcculloch indicated a practical field of application was not mentioned in their work from namely the recognition of spacial patterns by neural networks donald o. hebb formulated the classical hebbian rule which represents in its more generalized form the basis of nearly all neural learning procedures. the rule implies that the connection between two neurons is strengthened when both neurons are active at the same time. this change in strength is proportional to the product of the two activities. hebb could postulate this rule but due to the absence of neurological research he was not able to verify it. neuropsychologist karl lashley defended the thesis that the brain information storage is realized as a distributed system. his thesis was based on experiments on rats where only the extent but not the location of the destroyed nerve tissue influences the rats performance to find their way out of a labyrinth. golden age for his dissertation marvin minsky developed the neurocomputer snark which has already been capable to adjust its automatically. but it has never been practically implemented since it is capable to busily calculate but nobody really knows what it calculates. well-known scientists and ambitious students met at the dartmouth summer research project and discussed to put it crudely how to simulate a brain. differences between top-down and bottom-up research developed. while the early we will learn soon what weights are. d. kriesel a brief introduction to neural networks chapter introduction motivation and history dkriesel.com supporters of artificial intelligence wanted to simulate capabilities by means of software supporters of neural networks wanted to achieve system behavior by imitating the smallest parts of the system the neurons. at the mit frank rosenblatt charles wightman and their coworkers developed the first successful neurocomputer the mark i perceptron which was capable to recognize simple numerics by means of a pixel image sensor and electromechanically worked with motor driven potentiometers each potentiometer representing one variable weight. frank rosenblatt described different versions of the perceptron formulated and verified his perceptron convergence theorem. he described neuron layers mimicking the retina threshold switches and a learning rule adjusting the connecting weights. bernard widrow and marcian e. hoff introduced the adaline linear neuron a fast and precise adaptive learning system being the first widely commercially used neural network it could be found in nearly every analog telephone for realtime adaptive echo filtering and was trained by menas of the widrow-hoff rule or delta rule. at that time hoff later co-founder of intel corporation was a phd student of widrow who himself is known as the inventor of modern microprocessors. one advantage the delta rule had over the original perceptron learning algorithm was its adaptivity if the difference between the actual output and the correct solution was large the connecting weights also changed in larger steps the smaller the steps the closer the target was. disadvantage missapplication led to infinitesimal small steps close to the target. in the following stagnation and out of fear of scientific unpopularity of the neural networks adaline was renamed in adaptive linear element which was undone again later on. karl steinbuch introduced technical realizations of associative memory which can be seen as predecessors of today s neural associative memories additionally he described concepts for neural techniques and analyzed their possibilities and limits. in his book learning machines nils nilsson gave an overview of the progress and works of this period of neural network research. it was assumed that the basic principles of self-learning and therefore generally speaking systems had already been discovered. today this assumption seems to be an exorbitant overestimation but at that time it provided for high popularity and sufficient research funds. marvin minsky and seymour papert published a precise mathe d. kriesel a brief introduction to neural networks development accelerates first spread use backprop developed dkriesel.com history of neural networks research funds were stopped matical analysis of the perceptron to show that the perceptron model was not capable of representing many important problems xor problem and linear separability and so put an end to overestimation popularity and research funds. the implication that more powerful models would show exactly the same problems and the forecast that the entire field would be a research dead end resulted in a nearly complete decline in research funds for the next years no matter how incorrect these forecasts were from today s point of view. long silence and slow reconstruction the research funds were as previouslymentioned extremely short. everywhere research went on but there were neither conferences nor other events and therefore only few publications. this isolation of individual researchers provided for many independently developed neural network paradigms they researched but there was no discourse among them. in spite of the poor appreciation the field received the basic theories for the still continuing renaissance were laid at that time teuvo kohonen introduced a the linear associator model of a model of an associative memory in the same year such a model was presented independently and from a neurophysiologist s point of view by james a. anderson christoph von der malsburg used a neuron model that was nonlinear and biologically more motivated for his dissertation in harvard paul werbos developed a learning procedure called backpropagation of error but it was not until one decade later that this procedure reached today s importance. and thereafter stephen instance grossberg presented many papers in which numerous neural models are analyzed mathematically. furthermore he dedicated himself to the problem of keeping a neural network capable of destroying already learned associations. under cooperation of gail carpenter led to models of adaptive this resonance theory learning without teuvo kohonen described the feature maps self-organizing also known as kohonen maps. he was looking for the mechanisms involving self-organization in the brain knew that the information about the creation of a being is stored in the genome which has however not enough memory for a structure like the brain. as a consequence the brain has to organize and create itself for the most part. d. kriesel a brief introduction to neural networks chapter introduction motivation and history dkriesel.com john hopfield also invented the so-called hopfield networks which are inspired by the laws of magnetism in physics. they were not widely used in technical applications but the field of neural networks slowly regained importance. fukushima miyake and ito introduced the neural model of the neocognitron which could recognize handwritten characters and was an extension of the cognitron network already developed in renaissance through the influence of john hopfield who had personally convinced many researchers of the importance of the field and the wide publication of backpropagation by rumelhart hinton and williams the field of neural networks slowly showed signs of upswing. john hopfield published an article describing a way of finding acceptable solutions for the travelling salesman problem by using hopfield nets. the backpropagation of error learning procedure as a generalization of the delta rule was separately developed and widely published by the parallel distributed processing group non-linearly-separable problems could be solved by multilayer perceptrons and marvin minsky s negative evaluations were disproven at a single blow. at the same time a certain kind of fatigue spread in the field of artificial intelligence caused by a series of failures and unfulfilled hopes. from this time on the development of the field of research has almost been explosive. it can no longer be itemized but some of its results will be seen in the following. exercises exercise give one example for each of the following topics a book on neural networks or neuroin formatics a collaborative group of a university working with neural networks a software tool realizing neural net works a company using neural networks and a product or service being realized by means of neural networks. exercise show at least four applications of technical neural networks two from the field of pattern recognition and two from the field of function approximation. exercise briefly characterize the four development phases of neural networks and give expressive examples for each phase. d. kriesel a brief introduction to neural networks renaissance chapter biological neural networks how do biological systems solve problems? how does a system of neurons work? how can we understand its functionality? what are different quantities of neurons able to do? where in the nervous system does information processing occur? a short biological overview of the complexity of simple elements of neural information processing followed by some thoughts about their simplification in order to technically adapt them. before we begin to describe the technical side of neural networks it would be useful to briefly discuss the biology of neural networks and the cognition of living organisms the reader may skip the following chapter without missing any technical information. on the other hand i recommend to read the said excursus if you want to learn something about the underlying neurophysiology and see that our small approaches the technical neural networks are only caricatures of nature and how powerful their natural counterparts must be when our small approaches are already that effective. now we want to take a brief look at the nervous system of vertebrates we will start with a very rough granularity and then proceed with the brain and up to the neural level. for further reading i want to recommend the books which helped me a lot during this chapter. the vertebrate nervous system the entire information processing system i.e. the vertebrate nervous system consists of the central nervous system and the peripheral nervous system which is only a first and simple subdivision. in reality such a rigid subdivision does not make sense but here it is helpful to outline the information processing in a body. peripheral and central nervous system the peripheral nervous system comprises the nerves that are situated outside of the brain or the spinal cord. these nerves form a branched and very dense network throughout the whole body. the pe chapter biological neural networks dkriesel.com ripheral nervous system includes for example the spinal nerves which pass out of the spinal cord within the level of each vertebra of the spine and supply extremities neck and trunk but also the cranial nerves directly leading to the brain. the central nervous system however is the within the vertebrate. it is the place where information received by the sense organs are stored and managed. furthermore it controls the inner processes in the body and last but not least coordinates the motor functions of the organism. the vertebrate central nervous system consists of the brain and the spinal cord however we want to focus on the brain which can for the purpose of simplification be divided into four areas on the next page to be discussed here. the cerebrum is responsible for abstract thinking processes. the cerebrum is one of the areas of the brain that changed most during evolution. along an axis running from the lateral face to the back of the head this area is divided into two hemispheres which are organized in a folded structure. these cerebral hemispheres are connected by one strong nerve cord and several small ones. a large number of neurons are located in the cerebral cortex which is approx. cm thick and divided into different cortical fields each having a specific task to figure illustration of the central nervous system with spinal cord and brain. d. kriesel a brief introduction to neural networks dkriesel.com the vertebrate nervous system and errors are continually corrected. for this purpose the cerebellum has direct sensory information about muscle lengths as well as acoustic and visual information. furthermore it also receives messages about more abstract motor signals coming from the cerebrum. in the human brain the cerebellum is considerably smaller than the cerebrum but this is rather an exception. in many vertebrates this ratio is less pronounced. if we take a look at vertebrate evolution we will notice that the cerebellum is not small but the cerebum is large least it is the most highly developed structure in the vertebrate brain. the two remaining brain areas should also be briefly discussed the diencephalon and the brainstem. the diencephalon controls fundamental physiological processes the interbrain includes parts of which only the thalamus will be briefly discussed this part of the diencephalon mediates between sensory and motor signals and the cerebrum. particularly the thalamus decides which part of the information is transferred to the cerebrum so that especially less important sensory perceptions can be suppressed at short notice to avoid overloads. another part of the diencephalon is the hypothalamus which controls a number of processes within the body. the diencephalon thalamus filters incoming data figure illustration of the brain. the colored areas of the brain are discussed in the text. the more we turn from abstract information processing to direct reflexive processing the darker the areas of the brain are colored. fulfill. primary cortical fields are responsible for processing qualitative information such as the management of differthe visual cortex ent perceptions is responsible for the management of vision. association cortical fields however perform more abstract association and thinking processes they also contain our memory. the cerebellum controls and coordinates motor functions the cerebellum is located below the cerebrum therefore it is closer to the spinal cord. accordingly it serves less abstract functions with higher priority here large parts of motor coordination are performed i.e. balance and movements are controlled d. kriesel a brief introduction to neural networks chapter biological neural networks dkriesel.com is also heavily involved in the human circadian rhythm clock and the sensation of pain. the brainstem connects the brain with the spinal cord and controls reflexes. in comparison with the diencephalon the brainstem or the cerebri respectively is phylogenetically much older. roughly speaking it is the spinal cord and thus the connection between brain and spinal cord. the brainstem can also be divided into different areas some of which will be exemplarily introduced in this chapter. the functions will be discussed from abstract functions towards more fundamental ones. one important component is the pons a kind of transit station for many nerve signals from brain to body and vice versa. if the pons is damaged by a cerebral infarct then the result could be the locked-in syndrome a condition in which a patient is within his own body. he is conscious and aware with no loss of cognitive function but cannot move or communicate by any means. only his senses of sight hearing smell and taste are generally working perfectly normal. locked-in patients may often be able to communicate with others by blinking or moving their eyes. furthermore the brainstem is responsible for many fundamental reflexes such as the blinking reflex or coughing. all parts of the nervous system have one thing in common information processing. this is accomplished by huge accumulations of billions of very similar cells whose structure is very simple but which communicate continuously. large groups of these cells send coordinated signals and thus reach the enormous information processing capacity we are familiar with from our brain. we will now leave the level of brain areas and continue with the cellular level of the body the level of neurons. neurons are information processing cells before specifying the functions and processes within a neuron we will give a rough description of neuron functions a neuron is nothing more than a switch with information input and output. the switch will be activated if there are enough stimuli of other neurons hitting the information input. then at the information output a pulse is sent to for example other neurons. components of a neuron now we want to take a look at the components of a neuron on the facing page. in doing so we will follow the way the electrical information takes within the neuron. the dendrites of a neuron receive the information by special connections the synapses. d. kriesel a brief introduction to neural networks dkriesel.com the neuron figure illustration of a biological neuron with the components discussed in this text. synapses weight the individual parts of information incoming signals from other neurons or cells are transferred to a neuron by special connections the synapses. such connections can usually be found at the dendrites of a neuron sometimes also directly at the soma. we distinguish between electrical and chemical synapses. the electrical synapse is the simpler variant. an electrical signal received by the synapse i.e. coming from the presynaptic side is directly transferred to the postsynaptic nucleus of the cell. thus there is a direct strong unadjustable connection between the signal transmitter and the signal receiver which is for example relevant to shortening reactions that must be coded within a living organism. the chemical synapse is the more distinctive variant. here the electrical coupling of source and target does not take place the coupling is interrupted by the synaptic cleft. this cleft electrically separates the presynaptic side from the postsynaptic one. you might think that nevertheless the information has to flow so we will discuss how this happens it is not an electrical but a chemical process. on the presynaptic side of the synaptic cleft the electrical signal is converted into a chemical signal a process induced by chemical cues released there so-called neurotransmitters. these neurotransmitters cross the synaptic cleft and transfer the information into the nucleus of the cell is a very simple explanation but later on we will see how this exactly works where it is reconverted into electrical information. the neurotransmitters are degraded very fast so that it is possible to re electrical synapse simple d. kriesel a brief introduction to neural networks cemical synapse is more complex but also more powerful chapter biological neural networks dkriesel.com lease very precise information pulses here too. in spite of the more complex functioning the chemical synapse has compared with the electrical synapse utmost advantages one-way connection a chemical synapse is a one-way connection. due to the fact that there is no direct electrical connection between the pre- and postsynaptic area electrical pulses area cannot flash over to the presynaptic area. in the postsynaptic adjustability there is a large number of different neurotransmitters that can also be released in various quantities in a synaptic cleft. there are neurotransmitters that stimulate the postsynaptic cell nucleus and others that slow down such stimulation. some synapses transfer a strongly stimulating signal some only weakly stimulating ones. the adjustability varies a lot and one of the central points in the examination of the learning ability of the brain is that here the synapses are variable too. that is over time they can form a stronger or weaker connection. dendrites collect all parts of information dendrites branch like trees from the cell nucleus of the neuron is called soma and receive electrical signals from many different sources which are then transferred into the nucleus of the cell. the amount of branching dendrites is also called dendrite tree. in the soma the weighted information is accumulated after the cell nucleus has received a plenty of activating and inhibiting signals by synapses or dendrites the soma accumulates these signals. as soon as the accumulated signal exceeds a certain value threshold value the cell nucleus of the neuron activates an electrical pulse which then is transmitted to the neurons connected to the current one. the axon transfers outgoing pulses the pulse is transferred to other neurons by means of the axon. the axon is a long slender extension of the soma. in an extreme case an axon can stretch up to one meter within the spinal cord. the axon is electrically isolated in order to achieve a better conduction of the electrical signal will return to this point later on and it leads to dendrites which transfer the information to for example other neurons. so now we are back at the beginning of our description of the neuron elements. an axon can however transfer information to other kinds of cells in order to control them. d. kriesel a brief introduction to neural networks dkriesel.com the neuron electrochemical processes in the neuron and its components after having pursued the path of an electrical signal from the dendrites via the synapses to the nucleus of the cell and from there via the axon into other dendrites we now want to take a small step from biology towards technology. in doing so a simplified introduction of the electrochemical information processing should be provided. neurons maintain electrical membrane potential one fundamental aspect is the fact that compared to their environment the neurons show a difference in electrical charge a potential. in the membrane of the neuron the charge is different from the charge on the outside. this difference in charge is a central concept that is important to understand the processes within the neuron. the difference is called membrane potential. the membrane potential i.e. the difference in charge is created by several kinds of charged atoms whose concentration varies within and outside of the neuron. if we penetrate the membrane from the inside outwards we will find certain kinds of ions more often or less often than on the inside. this descent or ascent of concentration is called a concentration gradient. let us first take a look at the membrane potential in the resting state of the neu ron i.e. we assume that no electrical signals are received from the outside. in this case the membrane potential is mv. since we have learned that this potential depends on the concentration gradients of various ions there is of course the central question of how to maintain these concentration gradients normally diffusion predominates and therefore each ion is eager to decrease concentration gradients and to spread out evenly. if this happens the membrane potential will move towards mv so finally there would be no membrane potential anymore. thus the neuron actively maintains its membrane potential to be able to process information. how does this work? the secret is the membrane itself which is permeable to some ions but not for others. to maintain the potential various mechanisms are in progress at the same time concentration gradient as if the described above the ions try to be as uniformly distributed as possible. the concentration of an ion is higher on the inside of the neuron than on it will try to diffuse the outside and vice versa. to outside charged ion k the positively occurs very frequently within the neuron but less frequently outside of the neuron and therefore it slowly diffuses out through the neuron s membrane. but another group of negative ions collectively called a remains within the neuron since the membrane is not permeable to them. thus the inside of the neuron becomes negatively charged. d. kriesel a brief introduction to neural networks chapter biological neural networks dkriesel.com negative a ions remain positive k ions disappear and so the inside of the cell becomes more negative. the result is another gradient. electrical gradient the electrical gradient acts contrary to the concentration gradient. the intracellular charge is now very strong therefore it attracts positive ions k wants to get back into the cell. if these two gradients were now left alone they would eventually balance out reach a steady state and a membrane potential of mv would develop. but we want to achieve a resting membrane potential of mv thus there seem to exist some disturbances which prevent this. furthermore there is another important ion na for which the membrane is not very permeable but which however slowly pours through the membrane into the cell. as a result the sodium is driven into the cell all the more on the one hand there is less sodium within the neuron than outside the neuron. on the other hand sodium is positively charged but the interior of the cell has negative charge which is a second reason for the sodium wanting to get into the cell. due to the low diffusion of sodium into the cell the intracellular sodium concentration increases. but at the same time the inside of the cell becomes less negative so that k pours in more slowly can see that this is a complex mechanism where everything is influenced by everything. the sodium shifts the intracellular equilibrium from negative to less negative compared with its environment. but even with these two ions a standstill with all gradients being balanced out could still be achieved. now the last piece of the puzzle gets into the game a rather the protein atp actively transports ions against the direction they actually want to take! sodium is actively pumped out of the cell although it tries to get into the cell along the concentration gradient and the electrical gradient. potassium however diffuses strongly out of the cell but is actively pumped back into it. for this reason the pump is also called sodium-potassium pump. the pump maintains the concentration gradient for the sodium as well as for the potassium so that some sort of steady state equilibrium is created and finally the resting potential is mv as observed. all in all the membrane potential is maintained by the fact that the membrane is impermeable to some ions and other ions are actively pumped against the concentration and electrical gradients. now that we know that each neuron has a membrane potential we want to observe how a neuron receives and transmits signals. the neuron is activated by changes in the membrane potential above we have learned that sodium and potassium can diffuse through the membrane sodium slowly potassium faster. d. kriesel a brief introduction to neural networks dkriesel.com the neuron they move through channels within the membrane the sodium and potassium channels. in addition to these permanently open channels responsible for diffusion and balanced by the sodiumpotassium pump there also exist channels that are not always open but which only response required. since the opening of these channels changes the concentration of ions within and outside of the membrane it also changes the membrane potential. these controllable channels are opened as soon as the accumulated received stimulus exceeds a certain threshold. for example stimuli can be received from other neurons or have other causes. there exist for example specialized forms of neurons the sensory cells for which a light incidence could be such a stimulus. if the incoming amount of light exceeds the threshold controllable channels are opened. the said threshold threshold potential lies at about mv. as soon as the received stimuli reach this value the neuron is activated and an electrical signal an action potential is initiated. then this signal is transmitted to the cells connected to the observed neuron i.e. the cells to the neuron. now we want to take a closer look at the different stages of the action potential on the next page resting state only permanently open sodium and potassium channels are permeable. the membrane potential is at mv and actively kept there by the neuron. the stimulus up to the threshold a stimulus opens channels so that sodium can pour in. the intracellular charge becomes more positive. as soon as the membrane potential exceeds the threshold of mv the action potential is initiated by the opening of many sodium channels. depolarization sodium is pouring in. remember sodium wants to pour into the cell because there is a lower intracellular than extracellular concentration of sodium. additionally the cell is dominated by a negative environment which attracts the positive sodium ions. this massive influx of sodium drastically increases the membrane potential up to approx. mv which is the electrical pulse i.e. the action potential. repolarization now the sodium channels are closed and the potassium channels are opened. the positively charged ions want to leave the positive interior of the cell. additionally the intracellular concentration is much higher than the extracellular one which increases the e ux of ions even more. the interior of the cell is once again more negatively charged than the exterior. hyperpolarization sodium as well as potassium channels are closed again. at first the membrane potential is slightly more negative than the resting potential. this is due to the fact that the potassium channels close more slowly. as a result d. kriesel a brief introduction to neural networks chapter biological neural networks dkriesel.com figure initiation of action potential over time. d. kriesel a brief introduction to neural networks dkriesel.com the neuron charged potassium effuses because of its lower extracellular concentration. after a refractory period of ms the resting state is re-established so that the neuron can react to newly applied stimuli with an action potential. in simple terms the refractory period is a mandatory break a neuron has to take in order to regenerate. the shorter this break is the more often a neuron can fire per time. then the resulting pulse is transmitted by the axon. in the axon a pulse is conducted in a saltatory way we have already learned that the axon is used to transmit the action potential across long distances you will find an illustration of a neuron including an axon in fig. on page the axon is a long slender extension of the soma. in vertebrates it is normally coated by a myelin sheath that consists of schwann cells the pns or oligodendrocytes the cns which insulate the axon very well from electrical activity. at a distance of there are gaps between these cells the so-called nodes of ranvier. the said gaps appear where one insulate cell ends and the next one begins. it is obvious that at such a node the axon is less insulated. schwann cells as well as oligodendrocytes are varieties of the glial cells. there are about times more glial cells than neurons they surround the neurons glue insulate them from each other provide energy etc. now you may assume that these less insulated nodes are a disadvantage of the axon however they are not. at the nodes mass can be transferred between the intracellular and extracellular area a transfer that is impossible at those parts of the axon which are situated between two nodes and therefore insulated by the myelin sheath. this mass transfer permits the generation of signals similar to the generation of the action potential within the soma. the action potential is transferred as follows it does not continuously travel along the axon but jumps from node to node. thus a series of depolarization travels along the nodes of ranvier. one action potential initiates the next one and mostly even several nodes are active at the same time here. the pulse from node to node is responsible for the name of this pulse conductor saltatory conductor. obviously the pulse will move faster if its jumps are larger. axons with large internodes mm achieve a signal dispersion of approx. meters per second. however the internodes cannot grow indefinitely since the action potential to be transferred would fade too much until it reaches the next node. so the nodes have a task too to constantly amplify the signal. the cells receiving the action potential are attached to the end of the axon often connected by dendrites and synapses. as already indicated above the action potentials are not only generated by information received by the dendrites from other neurons. d. kriesel a brief introduction to neural networks chapter biological neural networks dkriesel.com receptor cells are modified neurons there are different receptor cells for various types of perceptions action potentials can also be generated by sensory information an organism receives from its environment through its sensory cells. specialized receptor cells are able to perceive specific stimulus energies such as light temperature and sound or the existence of certain molecules for example the sense of smell. this is working because of the fact that these sensory cells are actually modified neurons. they do not receive electrical signals via dendrites but the existence of the stimulus being specific for the receptor cell ensures that the ion channels open and an action potential is developed. this process of transforming stimulus energy into changes in the membrane potential is called sensory transduction. usually the stimulus energy itself is too weak to directly cause nerve signals. therefore the signals are amplified either during transduction or by means of the stimulus-conducting apparatus. the resulting action potential can be processed by other neurons and is then transmitted into the thalamus which is as we have already learned a gateway to the cerebral cortex and therefore can reject sensory impressions according to current relevance and thus prevent an abundance of information to be managed. primary receptors transmit their pulses directly to the nervous system. a good example for this is the sense of pain. here the stimulus intensity is proportional to the amplitude of the action potential. technically this is an amplitude modulation. secondary receptors however continuously transmit pulses. these pulses control the amount of the related neurotransmitter which is responsible for transferring the stimulus. the stimulus in turn controls the frequency of the action potential of the receiving neuron. this process is a frequency modulation an encoding of the stimulus which allows to better perceive the increase and decrease of a stimulus. there can be individual receptor cells or cells forming complex sensory organs eyes or ears. they can receive stimuli within the body means of the interoceptors as well as stimuli outside of the body means of the exteroceptors. after having outlined how information is received from the environment it will be interesting to look at how the information is processed. d. kriesel a brief introduction to neural networks dkriesel.com receptor cells information is processed on every level of the nervous system there is no reason to believe that all received information is transmitted to the brain and processed there and that the brain ensures that it is in the form of motor pulses only thing an organism can actually do within its environment is to move. the information processing is entirely decentralized. in order to illustrate this principle we want to take a look at some examples which leads us again from the abstract to the fundamental in our hierarchy of information processing. it is certain that information is processed in the cerebrum which is the most developed natural information processing structure. the midbrain and the thalamus which serves as we have already learned as a gateway to the cerebral cortex are situated much lower in the hierarchy. the filtering of information with respect to the current relevance executed by the midbrain is a very important method of information processing too. but even the thalamus does not receive any preprocessed stimuli from the outside. now let us continue with the lowest level the sensory cells. on the lowest level i.e. at the receptor cells the information is not only received and transferred but directly processed. one of the main aspects of this subject is to prevent the transmission of stimuli to the central nervous system because of sensory adaptation due to continuous stimulation many receptor cells automatically become insensitive to stimuli. thus receptor cells are not a direct mapping of specific stimulus energy onto action potentials but depend on the past. other sensors change their sensitivity according to the situation there are taste receptors which respond more or less to the same stimulus according to the nutritional condition of the organism. even before a stimulus reaches the receptor cells information processing can already be executed by a preceding signal carrying apparatus for example in the form of amplification the external and the internal ear have a specific shape to amplify the sound which also allows in association with the sensory cells of the sense of hearing the sensory stimulus only to increase logarithmically with the intensity of the heard signal. on closer examination this is necessary since the sound pressure of the signals for which the ear is constructed can vary over a wide exponential range. here a logarithmic measurement is an advantage. firstly an overload is prevented and secondly the fact that the intensity measurement of intensive signals will be less precise doesn t matter as well. if a jet fighter is starting next to you small d. kriesel a brief introduction to neural networks chapter biological neural networks dkriesel.com changes in the noise level can be ignored. just to get a feeling for sensory organs and information processing in the organism we will briefly describe light sensing organs i.e. organs often found in nature. for the third light sensing organ described below the single lens eye we will discuss the information processing in the eye. an outline of common light sensing organs for many organisms it turned out to be extremely useful to be able to perceive electromagnetic radiation in certain regions of the spectrum. consequently sensory organs have been developed which can detect such electromagnetic radiation and the wavelength range of the radiation perceivable by the human eye is called visible range or simply light. the different wavelengths of this electromagnetic radiation are perceived by the human eye as different colors. the visible range of the electromagnetic radiation is different for each organism. some organisms cannot see the colors ranges we can see others can even perceive additional wavelength ranges in the uv range. before we begin with the human being in order to get a broader knowledge of the sense of sight we briefly want to look at two organs of sight which from an evolutionary point of view exist much longer than the human. compound eyes and pinhole eyes only provide high temporal or spatial resolution let us first take a look at the so-called compound eye on the next page which is for example common in insects and crustaceans. the compound eye consists of a great number of small individual eyes. if we look at the compound eye from the outside the individual eyes are clearly visible and arranged in a hexagonal pattern. each individual eye has its own nerve fiber which is connected to the insect brain. since the individual eyes can be distinguished it is obvious that the number of pixels i.e. the spatial resolution of compound eyes must be very low and the image is blurred. but compound eyes have advantages too especially for fast-flying insects. certain compound eyes process more than images per second the human eye however movies with images per second appear as a fluent motion. compound eye high temp. low spatial resolution pinhole eyes are for example found in octopus species and work as you can guess similar to a pinhole camera. a pinhole pinhole eye has a very small opening for camera high spat. light entry which projects a sharp image low onto the sensory cells behind. thus the temporal spatial resolution is much higher than in resolution the compound eye. but due to the very small opening for light entry the resulting image is less bright. d. kriesel a brief introduction to neural networks dkriesel.com receptor cells the retina does not only receive information but is also responsible for information processing the light signals falling on the eye are received by the retina and directly preprocessed by several layers of informationprocessing cells. we want to briefly discuss the different steps of this information processing and in doing so we follow the way of the information carried by the light photoreceptors receive the light signal und cause action potentials are different receptors for different color components and light intensities. these receptors are the real light-receiving part of the retina and they are sensitive to such an extent that only one single photon falling on the retina can cause an action potential. then several photoreceptors transmit their signals to one single bipolar cell. this means that here the information has already been summarized. finally the now transformed light signal travels from several bipolar cells into ganglion cells. various bipolar cells can transmit their information to one ganglion cell. the higher the number of photoreceptors that affect the ganglion cell the larger the field of perception the receptive field which covers the ganglions and the less there are different kinds of bipolar cells as well but to discuss all of them would go too far. figure compound eye of a robber fly single lens eyes combine the advantages of the other two eye types but they are more complex the light sensing organ common in vertebrates is the single lense eye. the resulting image is a sharp high-resolution image of the environment at high or variable light intensity. on the other hand it is more complex. similar to the pinhole eye the light enters through an opening and is projected onto a layer of sensory cells in the eye. but in contrast to the pinhole eye the size of the pupil can be adapted to the lighting conditions means of the iris muscle which expands or contracts the pupil. these differences in pupil dilation require to actively focus the image. therefore the single lens eye contains an additional adjustable lens. single lense eye high temp. and spat. resolution d. kriesel a brief introduction to neural networks chapter biological neural networks dkriesel.com sharp is the image in the area of this ganglion cell. so the information is already reduced directly in the retina and the overall image is for example blurred in the peripheral field of vision. so far we have learned about the information processing in the retina only as a top-down structure. now we want to take a look at the horizontal and amacrine cells. these and compressing cells are not connected from the front backwards but laterally. they allow the light signals to influence themselves laterally directly during the information processing in the retina a much more powerful information processing method of than blurring. when the horizontal cells are excited by a photoreceptor they are able to excite other nearby photoreceptors and at the same time inhibit more distant bipolar cells and receptors. this ensures the clear perception of outlines and bright points. amacrine cells can further intensify certain stimuli by distributing information from bipolar cells to several ganglion cells or by inhibiting ganglions. these first steps of transmitting visual information to the brain show that information is processed from the first moment the information is received and on the other hand is processed in parallel within millions of information-processing cells. the system s power and resistance to errors is based upon this massive division of work. the amount of neurons in living organisms at different stages of development an overview of different organisms and their neural capacity large part from neurons are required by the nervous system of a nematode worm which serves as a popular model organism in biology. nematodes live in the soil and feed on bacteria. neurons make an ant simplify matters we neglect the fact that some ant species also can have more or less efficient nervous systems. due to the use of different attractants and odors ants are able to engage in complex social behavior and form huge states with millions of individuals. if you regard such an ant state as an individual it has a cognitive capacity similar to a chimpanzee or even a human. with neurons the nervous system of a fly can be constructed. a fly can evade an object in real-time in threedimensional space it can land upon the ceiling upside down has a considerable sensory system because of compound eyes vibrissae nerves at the end of its legs and much more. thus a fly has considerable differential and integral calculus in high dimensions implemented hardware. we all know that a fly is not easy to catch. of course the bodily functions are d. kriesel a brief introduction to neural networks dkriesel.com the amount of neurons in living organisms also controlled by neurons but these should be ignored here. with neurons we have enough cerebral matter to create a honeybee. honeybees build colonies and have amazing capabilities in the field of aerial reconnaissance and navigation. neurons result in a mouse and here the world of vertebrates already begins. neurons are sufficient for a rat an animal which is denounced as being extremely intelligent and are often used to participate in a variety of intelligence tests representative for the animal world. rats have an extraordinary sense of smell and orientation and they also show social behavior. the brain of a frog can be positioned within the same dimension. the frog has a complex build with many functions it can swim and has evolved complex behavior. a frog can continuously target the said fly by means of his eyes while jumping in three-dimensional space and and catch it with its tongue with considerable probability. neurons make a bat. the bat can navigate in total darkness through a room exact up to several centimeters by only using their sense of hearing. it uses acoustic signals to localize self-camouflaging insects some moths have a certain wing structure that reflects less sound waves and the echo will be small and also eats its prey while flying. neurons are required by the brain of a dog companion of man for ages. now take a look at another popular companion of man neurons can be found in a cat which is about twice as much as in a dog. we know that cats are very elegant patient carnivores that can show a variety of behaviors. by the way an octopus can be positioned within the same magnitude. only very few people know that for example in labyrinth orientation the octopus is vastly superior to the rat. for neurons you already get a chimpanzee one of the animals being very similar to the human. neurons make a human. usually the human has considerable cognitive capabilities is able to speak to abstract to remember and to use tools as well as the knowledge of other humans to develop advanced technologies and manifold social structures. with neurons there are nervous systems having more neurons than the human nervous system. here we should mention elephants and certain whale species. our state-of-the-art computers are not able to keep up with the aforementioned processing power of a fly. recent research results suggest that the processes in nervous systems might be vastly more powerful than people thought until not long ago michaeva et al. describe a separate d. kriesel a brief introduction to neural networks chapter biological neural networks dkriesel.com synapse-integrated information way of information processing posterity will show if they are right. therefore it is a vector. in nature a neuron receives pulses of to other neurons on average. transition to technical neurons neural networks are a caricature of biology how do we change from biological neural networks to the technical ones? through radical simplification. i want to briefly summarize the conclusions relevant for the technical part we have learned that the biological neurons are linked to each other in a weighted way and when stimulated they electrically transmit their signal via the axon. from the axon they are not directly transferred to the succeeding neurons but they first have to cross the synaptic cleft where the signal is changed again by variable chemical processes. in the receiving neuron the various inputs that have been postprocessed in the synaptic cleft are summarized or accumulated to one single pulse. depending on how the neuron is stimulated by the cumulated input the neuron itself emits a pulse or not thus the output is non-linear and not proportional to the cumulated input. our brief summary corresponds exactly with the few elements of biological neural networks we want to take over into the technical approximation scalar output the output of a neuron is a scalar which means that the neuron only consists of one component. several scalar outputs in turn form the vectorial input of another neuron. this particularly means that somewhere in the neuron the various input components have to be summarized in such a way that only one component remains. synapses change input in technical neural networks the inputs are preprocessed too. they are multiplied by a number weight they are weighted. the set of such weights represents the information storage of a neural network in both biological original and technical adaptation. accumulating the inputs in biology the inputs are summarized to a pulse according to the chemical change i.e. they are accumulated on the technical side this is often realized by the weighted sum which we will get to know later on. this means that after accumulation we continue with only one value a scalar instead of a vector. non-linear characteristic the input of our technical neurons is also not proportional to the output. vectorial input the input of technical neurons consists of many components adjustable weights the weights weighting the inputs are variable similar to d. kriesel a brief introduction to neural networks dkriesel.com technical neurons as caricature of biology bits of information. na vely calculated how much storage capacity does the brain have? note the information which neuron is connected to which other neuron is also important. the chemical processes at the synaptic cleft. this adds a great dynamic to the network because a large part of the of a neural network is saved in the weights and in the form and power of the chemical processes in a synaptic cleft. so our current only casually formulated and very simple neuron model receives a vectorial input with components xi. these are multiplied by the appropriate weights wi and accumulated x i wixi. aforementioned term is the weighted sum. mapping f defines the scalar output y called then the nonlinear x y f wixi i after this transition we now want to specify more precisely our neuron model and add some odds and ends. afterwards we will take a look at how the weights can be adjusted. exercises exercise it is estimated that a human brain consists of approx. nerve cells each of which has about to synapses. for this exercise we assume synapses per neuron. let us further assume that a single synapse could save d. kriesel a brief introduction to neural networks chapter components of artificial neural networks formal definitions and colloquial explanations of the components that realize the technical adaptations of biological neural networks. initial descriptions of how to combine these components into a neural network. this chapter contains the formal definitions for most of the neural network components used later in the text. after this chapter you will be able to read the individual chapters of this work without having to know the preceding ones this would be useful. the concept of time in neural networks certain point in time the notation will be for example netjt or oit. from a biological point of view this is of course not very plausible the human brain a neuron does not wait for another one but it significantly simplifies the implementation. in some definitions of this text we use the term time or the number of cycles of the neural network respectively. time is divided into discrete time steps discrete time steps definition concept of time. the current time time is referred to as the next time step as the preceding one as all other time steps are referred to analogously. if in the following chapters several mathematical variables netj or oi refer to a components of neural networks a technical neural network consists of simple processing units the neurons and directed weighted connections between those neurons. here the strength of a connection the connecting weight be chapter components of artificial neural networks dkriesel.com n. network neurons weighted connection tween two neurons i and j is referred to as wij definition network. a neural network is a sorted triple v w with two sets n v and a function w where n is the set of neurons and v a set ji j n whose elements are called connections between neuron i and neuron j. the function w v r defines the weights where wi j the weight of the connection between neuron i and neuron j is shortened to wij depending on the point of view it is either undefined or for connections that do not exist in the network. snipe in snipe an instance of the class neuralnetworkdescriptor is created in the first place. the descriptor object roughly outlines a class of neural networks e.g. it defines the number of neuron layers in a neural network. in a second step the descriptor object is used to instantiate an arbitrary number of neuralnetwork objects. to get started with snipe programming the documentations of exactly these two classes are in that order the right thing to read. the presented layout involving descriptor and dependent neural networks is very reasonable from the implementation point of view because it is enables to create and maintain general parameters of even very large sets of similar not neccessarily equal networks. so the weights can be implemented in a square weight matrix w or optionally in a weight vector w with the row note in some of the cited literature i and j could be interchanged in wij. here a consistent standard does not exist. but in this text i try to use the notation i found more frequently and in the more significant citations. ber of the matrix indicating where the connection begins and the column number of the matrix indicating which neuron is the target. indeed in this case the numeric marks a non-existing connection. this matrix representation is also called hinton the neurons and connections comprise the following components and variables m following the path of the data within a neuron which is according to fig. on the facing page in top-down direction connections carry information that is processed by neurons data are transferred between neurons via connections with the connecting weight being either excitatory or inhibitory. the definition of connections has already been included in the definition of the neural network. be set weights the method connection snipe can neuralnetwork.setsynapse. using the propagation function converts vector inputs to scalar network inputs looking at a neuron j we will usually find a lot of neurons with a connection to j i.e. which transfer their output to j. note that here again in some of the cited literature axes and rows could be interchanged. the published literature is not consistent here as well. d. kriesel a brief introduction to neural networks dkriesel.com components of neural networks inputs for a neuron j the propagation function receives the outputs oin of other neurons in are connected to j and transforms them in con- manages sideration of the connecting weights wij into the network input netj that can be further processed by the activation function. thus the network input is the result of the propagation function. definition function and let i in be the set of neurons such that z n wizj. then the network input of j called netj is calculated by the propagation function fprop as follows network input. netj oin winj here the weighted sum is very popular the multiplication of the output of each neuron i by wij and the summation of the results netj i i wij figure data processing of a neuron. the activation function of a neuron implies the threshold value. snipe the propagation function in snipe was implemented using the weighted sum. the activation is the status of a neuron based on the model of nature every neuron is to a certain extent at all times active excited or whatever you will call it. the d. kriesel a brief introduction to neural networks propagierungsfunktion gewichtete summe verarbeitet eingaben zur netzeingabe ausgabefunktion aus aktivierung die ausgabe ist oft identit t aktivierungsfunktion aus netzeingabe und alter aktivierung die neue aktivierungeingaben anderer neuronen netzeingabeaktivierungausgabe zu anderen neuronen propagation function weighted sum transforms outputs of other neurons to net input output function identity function transforms activation to output for other neurons activation function net input and sometimes old activation to new activationdata input of other neurons network inputactivationdata output to other neurons chapter components of artificial neural networks dkriesel.com how active is a neuron? reactions of the neurons to the input values depend on this activation state. the activation state indicates the extent of a neuron s activation and is often shortly referred to as activation. its formal definition is included in the following definition of the activation function. but generally it can be defined as follows definition state activation in general. let j be a neuron. the activation state aj in short activation is explicitly assigned to j indicates the extent of the neuron s activity and results from the activation function. snipe it is possible to get and set activation states of neurons by using the methods getactivation or setactivation in the class neuralnetwork. the activation function determines the activation of a neuron dependent on network input and treshold value at a certain time as we have already learned the activation aj of a neuron j depends on the activation state of the neuron and the external input. definition function and activation. let j be a neuron. the activation function is defined as ajt factnetjt ajt j. it transforms the network input netj as well as the previous activation state ajt into a new activation state ajt with the threshold value playing an important role as already mentioned. calculates activation neurons get activated if the network input exceeds their treshold value near the threshold value the activation function of a neuron reacts particularly sensitive. from the biological point of view the threshold value represents the threshold at which a neuron starts firing. the threshold value is also mostly included in the definition of the activation function but generally the definition is the following definition value in general. let j be a neuron. the threshold value j is uniquely assigned to j and marks the position of the maximum gradient value of the activation function. unlike the other variables within the neural network unlike the ones defined so far the activation function is often defined globally for all neurons or at least for a set of neurons and only the threshold values are different for each neuron. we should also keep in mind that the threshold values can be changed for example by a learning procedure. so it can in particular become necessary to relate the threshold value to the time and to write for instance j as jt for reasons of clarity i omitted this here. the activation function is also called transfer function. the previous activation is not always relevant for the current we will see examples for both variants. highest point of sensation d. kriesel a brief introduction to neural networks dkriesel.com components of neural networks snipe in snipe activation functions are generalized to neuron behaviors. such behaviors can represent just normal activation functions or even incorporate internal states and dynamics. corresponding parts of snipe can be found in the package neuronbehavior which also contains some of the activation functions introduced in the next section. the interface neuronbehavior allows for implementation of custom behaviors. objects that inherit from this interface can be passed to a neuralnetworkdescriptor instance. it is possible to define individual behaviors per neuron layer. common activation functions the simplest activation function is the binary threshold function on the next page which can only take on two values referred to as heaviside function. if the input is above a certain threshold the function changes from one value to another but otherwise remains constant. this implies that the function is not differentiable at the threshold and for the rest the derivative is due to this fact backpropagation learning for example is impossible we will see later. also very popular is the fermi function or logistic function e x which maps to the range of values of and the hyperbolic tangent which maps to both functions are differentiable. the fermi function can be expanded by a temperature parameter t into the form e x t the smaller this parameter the more does it compress the function on the x axis. thus one can arbitrarily approximate the heaviside function. incidentally there exist activation functions which are not explicitly defined but depend on the input according to a random distribution activation function. a alternative to the hypberbolic tangent that is really worth mentioning was suggested by anguita et al. who have been tired of the slowness of the workstations back in thinking about how to make neural network propagations faster they quickly identified the approximation of the e-function used in the hyperbolic tangent as one of the causes of slowness. consequently they an approximation to the hyperbolic tangent just using two parabola pieces and two half-lines. at the price of delivering a slightly smaller range of values than the hyperbolic tangent instead of dependent on what cpu one uses it can be calculated times faster because it just needs two multiplications and one addition. what s more it has some other advantages that will be mentioned later. snipe the activation functions introduced here are implemented within the classes fermi and tangenshyperbolicus both of which are located in the package neuronbehavior. the fast hyperbolic tangent approximation is located within the class tangenshyperbolicusanguita. d. kriesel a brief introduction to neural networks chapter components of artificial neural networks dkriesel.com an output function may be used to process the activation once again the output function of a neuron j calculates the values which are transferred to the other neurons connected to j. more formally definition function. let j be a neuron. the output function foutaj oj informs other neurons calculates the output value oj of the neu- ron j from its activation state aj. generally the output function is defined globally too. often this function is the identity i.e. the activation aj is directly foutaj aj so oj aj unless explicitly specified differently we will use the identity as output function within this text. learning strategies adjust a network to fit our needs since we will address this subject later in detail and at first want to get to know the principles of neural network structures i will only provide a brief and general definition here other definitions of output functions may be useful if the range of values of the activation function is not sufficient. figure various popular activation functions from top to bottom heaviside or binary threshold function fermi function hyperbolic tangent. the fermi function was expanded by a temperature parameter. the original fermi function is represented by dark colors the temperature parameters of the modified fermi tions are ordered ascending by steepness und d. kriesel a brief introduction to neural networks function function with temperature parameter tangent network of layers dkriesel.com network topologies definition learning rule. the learning strategy is an algorithm that can be used to change and thereby train the neural network so that the network produces a desired output for a given input. network topologies after we have become acquainted with the composition of the elements of a neural network i want to give an overview of the usual topologies designs of neural networks i.e. to construct networks consisting of these elements. every topology described in this text is illustrated by a map and its hinton diagram so that the reader can immediately see the characteristics and apply them to other networks. in the hinton diagram the dotted weights are represented by light grey fields the solid ones by dark grey fields. the input and output arrows which were added for reasons of clarity cannot be found in the hinton diagram. in order to clarify that the connections are between the line neurons and the column neurons i have inserted the small arrow in the upper-left cell. snipe snipe is designed for realization of arbitrary network topologies. in this respect snipe defines different kinds of synapses depending on their source and their target. any kind of synapse can separately be allowed or forbidden for a set of networks using the setallowed methods in a neuralnetworkdescriptor instance. feedforward networks consist of layers and connections towards each following layer feedforward in this text feedforward networks on the following page are the networks we will first explore if we will use different topologies later. the neurons are grouped in the following layers one input layer n hidden processing layers from the outside that s why the neurons are also referred to as hidden neurons and one output layer. in a feedforward network each neuron in one layer has only directed connections to the neurons of the next layer the output layer. in fig. on the next page the connections permitted for a feedforward network are represented by solid lines. we will often be confronted with feedforward networks in which every neuron i is connected to all neurons of the next layer layers are called completely linked. to prevent naming conflicts the output neurons are often referred to as definition network. the neuron layers of a feedforward network on the following page are clearly separated one input layer one output layer and one or more processing layers which are invisible from the outside called hidden layers. connections are only permitted to neurons of the following layer. d. kriesel a brief introduction to neural networks shortcuts skip layers chapter components of artificial neural networks dkriesel.com shortcut connections skip layers gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii aaaaaaaaa aaaaaaaaa gfed gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii aaaaaaaaa aaaaaaaaa gfed gfed some feedforward networks permit the socalled shortcut connections on the next page connections that skip one or more levels. these connections may only be directed towards the output layer too. definition network with shortcut connections. similar to the feedforward network but the connections may not only be directed towards the next layer but also towards any other subsequent layer. recurrent networks have influence on themselves figure a feedforward network with three layers two input neurons three hidden neurons and two output neurons. characteristic for the hinton diagram of completely linked feedforward networks is the formation of blocks above the diagonal. recurrence is defined as the process of a neuron influencing itself by any means or by any connection. recurrent networks do not always have explicitly defined input or output neurons. therefore in the figures i omitted all markings that concern this matter and only numbered the neurons. direct recurrences start and end at the same neuron some networks allow for neurons to be connected to themselves which is called direct recurrence sometimes selfrecurrence on the facing page. as a result neurons inhibit and therefore strengthen themselves in order to reach their activation limits. d. kriesel a brief introduction to neural networks t t dkriesel.com network topologies gfed figure a network similar to a feedforward network with directly recurrent neurons. the direct recurrences are represented by solid lines and exactly correspond to the diagonal in the hinton diagram matrix. gfed gfed gfed gfed gfed gfed figure a feedforward network with shortcut connections which are represented by solid lines. on the right side of the feedforward blocks new connections have been added to the hinton diagram. definition recurrence. now we expand the feedforward network by connecting a neuron j to itself with the weights of these connections being referred to as wjj. in other words the diagonal of the weight matrix w may be different from neurons influence themselves d. kriesel a brief introduction to neural networks s s t t t t v v v v u u v v v v v v u u v v v v chapter components of artificial neural networks dkriesel.com indirect recurrences can influence their starting neuron only by making detours if connections are allowed towards the input layer they will be called indirect recurrences. then a neuron j can use indirect forwards connections to influence itself for example by influencing the neurons of the next layer and the neurons of this next layer influencing j definition recurrence. again our network is based on a feedforward network now with additional connections between neurons and their preceding layer being allowed. therefore below the diagonal of w is different from lateral recurrences connect neurons within one layer connections between neurons within one layer are called lateral recurrences on the facing page. here each neuron often inhibits the other neurons of the layer and strengthens itself. as a result only the strongest neuron becomes active scheme. definition recurrence. a laterally recurrent network permits connections within one layer. completely linked networks allow any possible connection completely linked networks permit connections between all neurons except for direct figure a network similar to a feedforward network with indirectly recurrent neurons. the indirect recurrences are represented by solid lines. as we can see connections to the preceding layers can exist here too. the fields that are symmetric to the feedforward blocks in the hinton diagram are now occupied. d. kriesel a brief introduction to neural networks u u x x x x g g u u x x g g x x dkriesel.com the bias neuron figure a network similar to a feedforward network with laterally recurrent neurons. the direct recurrences are represented by solid lines. here recurrences only exist within the layer. in the hinton diagram filled squares are concentrated around the diagonal in the height of the feedforward blocks but the diagonal is left uncovered. recurrences. furthermore the connections must be symmetric on the next page. a popular example are the selforganizing maps which will be introduced in chapter definition interconnection. in this case every neuron is always allowed to be connected to every other neuron but as a result every neuron can become an input neuron. therefore direct recurrences normally cannot be applied here and clearly defined layers do not longer exist. thus the matrix w may be unequal to everywhere except along its diagonal. the bias neuron is a technical trick to consider threshold values as connection weights by now we know that in many network paradigms neurons have a threshold value that indicates when a neuron becomes active. thus the threshold value is an activation function parameter of a neuron. from the biological point of view this sounds most plausible but it is complicated to access the activation function at runtime in order to train the threshold value. but threshold values jn for neurons jn can also be realized as connecting weight of a continuously firing neuron for this purpose an additional bias neuron whose output value d. kriesel a brief introduction to neural networks k k u u k k j j k k u u k k chapter components of artificial neural networks dkriesel.com i ujjjjjjjjjjjjjjjjjjjjjjj o i ujjjjjjjjjjjjjjjjjjjjjjj figure a completely linked network with symmetric connections and without direct recurrences. in the hinton diagram only the diagonal is left blank. is always is integrated in the network and connected to the neurons jn. these new connections get the weights jn i.e. they get the negative threshold values. definition a bias neuron is a neuron whose output value is always and which is represented by it is used to represent neuron biases as connection weights which enables any weighttraining algorithm to train the biases at the same time. gfed bias then the threshold value of the neurons jn is set to now the threshold values are implemented as connection weights on page and can directly be trained together with the connection weights which considerably facilitates the learning process. in other words instead of including the threshold value in the activation function it is now included in the propagation function. or even shorter the threshold value is subtracted from the network input i.e. it is part of the network input. more formally let jn be neurons with threshold values jn. by inserting a bias neuron whose output value is always generating connections between the said bias neuron and the neurons jn and connections wbiasjnwith jn we can set jn and weighting these bias neuron replaces thresh. value with weights d. kriesel a brief introduction to neural networks i i i o o o o u o i o j j u o o o o dkriesel.com orders of activation receive an equivalent neural network whose threshold values are realized by connection weights. undoubtedly the advantage of the bias neuron is the fact that it is much easier to implement it in the network. one disadvantage is that the representation of the network already becomes quite ugly with only a few neurons let alone with a great number of them. by the way a bias neuron is often referred to as on neuron. from now on the bias neuron is omitted for clarity in the following illustrations but we know that it exists and that the threshold values can simply be treated as weights because of it. snipe in snipe a bias neuron was implemented instead of neuron-individual biases. the neuron index of the bias neuron is gau wvut pqrs wvut pqrs tanh gfed wvut pqrs fermi onml hijk onml hijk fact lh wvut pqrs gfed bias figure different types of neurons that will appear in the following text. take care of the order in which neuron activations are calculated for a neural network it is very important in which order the individual neurons receive and process the input and output the results. here we distinguish two model classes representing neurons synchronous activation we have already seen that we can either write its name or its threshold value into a neuron. another useful representation which we will use several times in the following is to illustrate neurons according to their type of data processing. see fig. for some examples without further explanation the different types of neurons are explained as soon as we need them. all neurons change their values synchronously i.e. they simultaneously calculate network inputs activation and output and pass them on. synchronous activation corresponds closest to its biological counterpart but it is if to be implemented in hardware only useful on certain parallel computers and especially not for feedforward networks. this order of activation is the most generic and can be used with networks of arbitrary topology. d. kriesel a brief introduction to neural networks chapter components of artificial neural networks dkriesel.com gfed bbbbbbbbb gfed gfed bias gfed tttttttttt aaaa aaaa figure two equivalent neural networks one without bias neuron on the left one with bias neuron on the right. the neuron threshold values can be found in the neurons the connecting weights at the connections. furthermore i omitted the weights of the already existing connections by dotted lines on the right side. biologically plausible definition activation. all neurons of a network calculate network inputs at the same time by means of the propagation function activation by means of the activation function and output by means of the output function. after that the activation cycle is complete. snipe when implementing in software one could model this very general activation order by every time step calculating and caching every single network input and after that calculating all activations. this is exactly how it is done in snipe because snipe has to be able to realize arbitrary network topologies. asynchronous activation of time. for this there exist different orders some of which i want to introduce in the following easier to implement random order definition order of activation. with random order of activation a neuron i is randomly chosen and its neti ai and oi are updated. for n neurons a cycle is the n-fold execution of this step. obviously some neurons are repeatedly updated during one cycle and others however not at all. here the neurons do not change their values simultaneously but at different points apparently this order of activation is not always useful. d. kriesel a brief introduction to neural networks dkriesel.com orders of activation random permutation with random permutation each neuron is chosen exactly once but in random order during one cycle. definition permutation. initially a permutation of the neurons is calculated randomly and therefore defines the order of activation. then the neurons are successively processed in this order. this order of activation is as well used rarely because firstly the order is generally useless and secondly it is very timeconsuming to compute a new permutation for every cycle. a hopfield network is a topology nominally having a random or a randomly permuted order of activation. but note that in practice for the previously mentioned reasons a fixed order of activation is preferred. for all orders either the previous neuron activations at time t or if already existing the neuron activations at time t for which we are calculating the activations can be taken as a starting point. topological order often very useful definition activation. with topological order of activation the neurons are updated during one cycle and according to a fixed order. the order is defined by the network topology. since otherwise there is no order of activation. thus in feedforward networks which the procedure is very reasonable the input neurons would be updated first then the inner neurons and finally the output neurons. this may save us a lot of time given a synchronous activation order a feedforward network with n layers of neurons would need n full propagation cycles in order to enable input data to have influence on the output of the network. given the topological activation order we just need one single propagation. however not every network topology allows for finding a special activation order that enables saving time. feature snipe those who want to use snipe for implementing feedforward networks may save some calculation time by using the fastprop within the documentation of the class neuralnetworkdescriptor. once fastprop is enabled it will cause the data propagation to be carried out in a slightly different way. in the standard mode all net inputs are calculated first followed by all activations. in the fastprop mode for every neuron the activation is calculated right after the net input. the neuron values are calculated in ascending neuron index order. the neuron numbers are ascending from input to output layer which provides us with the perfect topological activation order for feedforward networks. fixed orders of activation during implementation this procedure can only be considered for non-cyclic i.e. non-recurrent networks obviously fixed orders of activation can be defined as well. therefore when implementing feedforward for instance d. kriesel a brief introduction to neural networks chapter components of artificial neural networks dkriesel.com networks it is very popular to determine the order of activation once according to the topology and to use this order without further verification at runtime. but this is not necessarily useful for networks that are capable to change their topology. outputs ym. they are regarded as output vector y ym. thus the output dimension is referred to as m. data is output by a neural net- work by the output neurons adopting the components of the output vector in their output values. communication with the outside world input and output of data in and from neural networks finally let us take a look at the fact that of course many types of neural networks permit the input of data. then these data are processed and can produce output. let us for example regard the feedforward network shown in fig. on page it has two input neurons and two output neurons which means that it also has two numerical inputs and outputs as a simplification we summarize the input and output components for n input or output neurons within the vectors x xn and y yn. definition vector. a network with n input neurons needs n inputs xn. they are considered as input vector x xn. as a consequence the input dimension is referred to as n. data is put into a neural network by using the components of the input vector as network inputs of the input neurons. definition vector. a network with m output neurons provides m snipe in order to propagate data through a neuralnetwork-instance the propagate method is used. it receives the input vector as array of doubles and returns the output vector in the same way. now we have defined and closely examined the basic components of neural networks without having seen a network in action. but first we will continue with theoretical explanations and generally describe how a neural network could learn. exercises exercise would it be useful your point of view to insert one bias neuron in each layer of a layer-based network such as a feedforward network? discuss this in relation to the representation and implementation of the network. will the result of the network change? exercise show for the fermi function fx as well as for the hyperbolic tangent tanhx that their derivatives can be expressed by the respective functions themselves so that the two statements fx fx and d. kriesel a brief introduction to neural networks dkriesel.com are true. input and output of data d. kriesel a brief introduction to neural networks chapter fundamentals on learning and training samples approaches and thoughts of how to teach machines. should neural networks be corrected? should they only be encouraged? or should they even learn without any help? thoughts about what we want to change during the learning procedure and how we will change it about the measurement of errors and when we have learned enough. as written above the most interesting characteristic of neural networks is their capability to familiarize with problems by means of training and after sufficient training to be able to solve unknown problems of the same class. this approach is referred to as generalization. before introducing specific learning procedures i want to propose some basic principles about the learning procedure in this chapter. there are different paradigms of learning learning is a comprehensive term. a learning system changes itself in order to adapt to e.g. environmental changes. a neural network could learn from many things but of course there will always be in the question of how to implement it. principle a neural network changes when its components are changing as we have learned above. theoretically a neural network could learn by developing new connections deleting existing connections changing connecting weights changing the threshold values of neu rons varying one or more of the three neuron functions activation function propagation function and output function developing new neurons or deleting existing neurons so of course existing connections. from what do we learn? chapter fundamentals on learning and training samples dkriesel.com as mentioned above we assume the change in weight to be the most common procedure. furthermore deletion of connections can be realized by additionally taking care that a connection is no longer trained when it is set to moreover we can develop further connections by setting a non-existing connection the value in the connection matrix to a value different from as for the modification of threshold values i refer to the possibility of implementing them as weights thus we perform any of the first four of the learning paradigms by just training synaptic weights. the change of neuron functions is difficult to implement not very intuitive and not exactly biologically motivated. therefore it is not very popular and i will omit this topic here. the possibilities to develop or delete neurons do not only provide well adjusted weights during the training of a neural network but also optimize the network topology. thus they attract a growing interest and are often realized by using evolutionary procedures. but since we accept that a large part of learning possibilities can already be covered by changes in weight they are also not the subject matter of this text it is planned to extend the text towards those aspects of training. of allow for the changes class snipe methods in neuralnetwork connection weights and addition and removal of both connections and neurons. methods in neuralnetworkdescriptor enable the change of neuron behaviors respectively per layer. activation functions learning by changes in weight thus we let our neural network learn by modifying the connecting weights according to rules that can be formulated as algorithms. therefore a learning procedure is always an algorithm that can easily be implemented by means of a programming language. later in the text i will assume that the definition of the term desired output which is worth learning is known i will define formally what a training pattern is and that we have a training set of learning samples. let a training set be defined as follows definition set. a train- ing set p is a set of training patterns which we use to train our neural net. i will now introduce the three essential paradigms of learning by presenting the differences between their regarding training sets. unsupervised learning provides input patterns to the network but no learning aides unsupervised learning is the biologically most plausible method but is not suitable for all problems. only the input patterns are given the network tries to identify similar patterns and to classify them into similar categories. definition learning. the training set only consists of input patterns the network tries by itself to detect similarities and to generate pattern classes. d. kriesel a brief introduction to neural networks dkriesel.com paradigms of learning here i want to refer again to the popular example of kohonen s self-organising maps reinforcement learning methods provide feedback to the network whether it behaves well or bad in reinforcement learning the network receives a logical or a real value after completion of a sequence which defines whether the result is right or wrong. intuitively it is clear that this procedure should be more effective than unsupervised learning since the network receives specific critera for problem-solving. definition learning. the training set consists of input patterns after completion of a sequence a value is returned to the network indicating whether the result was right or wrong and possibly how right or wrong it was. supervised learning methods provide training patterns together with appropriate desired outputs in supervised learning the training set consists of input patterns as well as their correct results in the form of the precise activation of all output neurons. thus for each training set that is fed into the network the output for instance can directly be compared with the correct solution and and the network weights can be changed according to their difference. the objective is to change the weights to the effect that the network cannot only associate input and output patterns independently after the training but can provide plausible results to unknown similar input patterns i.e. it generalises. definition learning. the training set consists of input patterns with correct results so that the network can receive a precise error can be returned. this learning procedure is not always biologically plausible but it is extremely effective and therefore very practicable. at first we want to look at the the supervised learning procedures in general which in this text are corresponding to the following steps entering the input pattern of input neurons forward propagation of the input by the network generation of the output comparing the output with the desired output input provides error vector vector corrections of the network are calculated based on the error vector corrections are applied. the term error vector will be defined in section where mathematical formalisation of learning is discussed. d. kriesel a brief introduction to neural networks network receives reward or punishment network receives correct results for samples learning scheme chapter fundamentals on learning and training samples dkriesel.com o ine or online learning? it must be noted that learning can be o ine set of training samples is presented then the weights are changed the total error is calculated by means of a error function operation or simply accumulated see also section or online every sample presented the weights are changed. both procedures have advantages and disadvantages which will be discussed in the learning procedures section if necessary. o ine training procedures are also called batch training procedures since a batch of results is corrected all at once. such a training section of a whole batch of training samples including the related change in weight values is called epoch. definition ine learning. several training patterns are entered into the network at once the errors are accumulated and it learns for all patterns at the same time. definition learning. the network learns directly from the errors of each training sample. how must the weights be modified to allow fast and reliable learning? how can the success of a learning process be measured in an objective way? is it possible to determine the learning procedure? is it possible to predict if a learning procedure terminates i.e. whether it will reach an optimal state after a finite time or if it for example will oscillate between different states? how can the learned patterns be stored in the network? is it possible to avoid that newly learned patterns destroy previously learned associations so-called stabilityplasticity dilemma? we will see that all these questions cannot be generally answered but that they have to be discussed for each learning procedure no easy and each network topology individually. answers! training patterns and teaching input questions you should answer before learning the application of such schemes certainly requires preliminary thoughts about some questions which i want to introduce now as a check list and if possible answer them in the course of this text where does the learning input come from and in what form? before we get to know our first learning rule we need to introduce the teaching input. in case of supervised learning we assume a training set consisting of training patterns and the corresponding correct output values we want to see at the output neurons after the training. while the network has not finished training i.e. as long as it is generating wrong outputs these output values are referred desired output d. kriesel a brief introduction to neural networks dkriesel.com training patterns and teaching input desired output to as teaching input and that for each neuron individually. thus for a neuron j with the incorrect output oj tj is the teaching input which means it is the correct or desired output for a training pattern p. definition patterns. a training pattern is an input vector p with the components pn whose desired output is known. by entering the training pattern into the network we receive an output that can be compared with the teaching input which is the desired output. the set of training patterns is called p. it contains a finite number of ordered pairsp t of training patterns with corresponding desired output. training patterns are often simply called patterns that is why they are referred to as p. in the literature as well as in this text they are called synonymously patterns training samples etc. definition input. let j be an output neuron. the teaching input tj is the desired and correct value j should output after the input of a certain training pattern. analogously to the vector p the teaching inputs tn of the neurons can also be combined into a vector t. t always refers to a specific training pattern p and is as already mentioned contained in the set p of the training patterns. are that relevant snipe classes in for training the package class trainingsamplelesson allows for storage of training patterns and teaching inputs data are training. located the as well as simple preprocessing of the training data. definition vector. for sev- eral output neurons n the difference between output vector and teaching input under a training input p tn yn ep is referred to as error vector sometimes it is also called difference vector. depending on whether you are learning offline or online the difference vector refers to a specific training pattern or to the error of a set of training patterns which is normalized in a certain way. now i want to briefly summarize the vectors we have yet defined. there is the input vector x which can be entered into the neural network. depending on the type of network being used the neural network will output an output vector y. basically the training sample p is nothing more than an input vector. we only use it for training purposes because we know the corresponding teaching input t which is nothing more than the desired output vector to the training sample. the error vector ep is the difference between the teaching input t and the actural output y. d. kriesel a brief introduction to neural networks chapter fundamentals on learning and training samples dkriesel.com important! so what x and y are for the general network operation are p and t for the network training and during training we try to bring y as close to t as possible. one advice concerning notation we referred to the output values of a neuron i as oi. thus the output of an output neuron is called o but the output values of a network are referred to as y certainly these network outputs are only neuron outputs too but they are outputs of output neurons. in this respect y o is true. using training samples we have seen how we can learn in principle and which steps are required to do so. now we should take a look at the selection of training data and the learning curve. after successful learning it is particularly interesting whether the network has only memorized i.e. whether it can use our training samples to quite exactly produce the right output but to provide wrong answers for all other problems of the same class. suppose that we want the network to train a mapping and therefor use the training samples from fig. then there could be a chance that finally the network will exactly mark the colored areas around the training samples with the output top and otherwise will output thus it has sufficient storage capacity to concentrate on the six training figure visualization of training results of the same training set on networks with a capacity being too high correct or too low d. kriesel a brief introduction to neural networks dkriesel.com using training samples samples with the output this implies an oversized network with too much free storage capacity. on the other hand a network could have insufficient capacity bottom this rough presentation of input data does not correspond to the good generalization performance we desire. thus we have to find the balance middle. it is useful to divide the set of training samples an often proposed solution for these problems is to divide the training set into one training set really used to train and one verification set to test our progress provided that there are enough training samples. the usual division relations are for instance for training data and for verification data chosen. we can finish the training when the network provides good results on the training data as well as on the verification data. snipe the method splitlesson within the class trainingsamplelesson allows for splitting a trainingsamplelesson with respect to a given ratio. but note if the verification data provide poor results do not modify the network structure until these data provide good results otherwise you run the risk of tailoring the network to the verification data. this means that these data are included in the training even if they are not used explicitly for the training. the solution is a third set of validation data used only for validation after a supposably successful training. by training less patterns we obviously withhold information from the network and risk to worsen the learning performance. but this text is not about exact reproduction of given samples but about successful generalization and approximation of a whole function for which it can definitely be useful to train less information into the network. order of pattern representation you can find different strategies to choose the order of pattern presentation if patterns are presented in random sequence there is no guarantee that the patterns are learned equally well this is the standard method. always the same sequence of patterns on the other hand provokes that the patterns will be memorized when using recurrent networks we will learn more about this type of networks. a random permutation would solve both problems but it is as already mentioned very time-consuming to calculate such a permutation. snipe the method shufflesamples located in the class trainingsamplelesson permutes a lesson. d. kriesel a brief introduction to neural networks chapter fundamentals on learning and training samples dkriesel.com norm to compare learning curve and error measurement the learning curve indicates the progress of the error which can be determined in various ways. the motivation to create a learning curve is that such a curve can indicate whether the network is progressing or not. for this the error should be normalized i.e. represent a distance measure between the correct and the current output of the network. for example we can take the same pattern-specific squared error with a prefactor which we are also going to use to derive the backpropagation of error be output neurons and o the set of output neurons x o errp y definition error. the specific error errp is based on a single training sample which means it is generated online. additionally the root mean square rms and the euclidean distance are often used. the euclidean distance of the theorem of pythagoras is useful for lower dimensions where we can still visualize its usefulness. definition distance. the euclidean distance between two vectors t and y is defined as sx o errp generally the root mean square is commonly used since it considers extreme outliers to a greater extent. definition mean square. the root mean square of two vectors t and y is defined as sp errp ot y as for o ine learning the total error in the course of one training epoch is interesting and useful too err x p p errp definition error. the total error err is based on all training samples that means it is generated o ine. analogously we can generate a total rms and a total euclidean distance in the course of a whole epoch. of course it is possible to use other types of error measurement. to get used to further error measurement methods i suggest to have a look into the technical report of prechelt in this report both error measurement methods and sample problems are discussed is why there will be a simmilar suggestion during the discussion of exemplary problems. snipe there are several static methods representing different methods of error measurement implemented in the class errormeasurement. y depending on our method of error measurement our learning curve certainly d. kriesel a brief introduction to neural networks objectivity dkriesel.com learning curve and error measurement changes too. a perfect learning curve looks like a negative exponential function that means it is proportional to e t on the following page. thus the representation of the learning curve can be illustrated by means of a logarithmic scale second diagram from the bottom with the said scaling combination a descending line implies an exponential descent of the error. with the network doing a good job the problems being not too difficult and the logarithmic representation of err you can see metaphorically speaking a descending line that often forms at the bottom here we reach the limit of the resolution of our computer and our network has actually learned the optimum of what it is capable of learning. i.e. typical learning curves can show a few flat areas as well they can show some steps which is no sign of a malfunctioning learning process. as we can also see in fig. a well-suited representation can make any slightly decreasing learning curve look good so just be cautious when reading the literature. when do we stop learning? now the big question is when do we stop learning? generally the training is stopped when the user in front of the learning computer the error was small enough. indeed there is no easy answer and thus i can once again only give you something to think about which however depends on a more objective view on the comparison of several learning curves. confidence in the results for example is boosted when the network always reaches nearly the same final error-rate for different random initializations so repeated initialization and training will provide a more objective result. on the other hand it can be possible that a curve descending fast in the beginning can after a longer time of learning be overtaken by another curve this can indicate that either the learning rate of the worse curve was too high or the worse curve itself simply got stuck in a local minimum but was the first to find it. remember larger error values are worse than the small ones. but in any case note many people only generate a learning curve in respect of the training data then they are surprised that only a few things will work but for reasons of objectivity and clarity it should not be forgotten to plot the verification data on a second learning curve which generally provides values that are slightly worse and with stronger oscillation. but with good generalization the curve can decrease too. when the network eventually begins to memorize the samples the shape of the learning curve can provide an indication if the learning curve of the verification samples is suddenly and rapidly rising while the learning curve of the verification d. kriesel a brief introduction to neural networks chapter fundamentals on learning and training samples dkriesel.com figure all four illustrations show the same because very smooth learning curve. note the alternating logarithmic and linear scalings! also note the small spikes visible in the sharp bend of the curve in the first and second diagram from bottom. d. kriesel a brief introduction to neural networks dkriesel.com gradient optimization procedures data is continuously falling this could indicate memorizing and a generalization getting poorer and poorer. at this point it could be decided whether the network has already learned well enough at the next point of the two curves and maybe the final point of learning is to be applied here procedure is called early stopping. once again i want to remind you that they are all acting as indicators and not to draw if-then conclusions. gradient optimization procedures in order to establish the mathematical basis for some of the following learning procedures i want to explain briefly what is meant by gradient descent the backpropagation of error learning procedure for example involves this mathematical basis and thus inherits the advantages and disadvantages of the gradient descent. gradient descent procedures are generally used where we want to maximize or minimize n-dimensional functions. due to clarity the illustration on the next page shows only two dimensions but principally there is no limit to the number of dimensions. the gradient is a vector g that is defined for any differentiable point of a function that points from this point exactly towards the steepest ascent and indicates the gradient in this direction by means gradient is multi-dim. derivative of its norm thus the gradient is a generalization of the derivative for multidimensional functions. accordingly the negative gradient g exactly points towards the steepest descent. the gradient operator is referred to as nabla op- erator the overall notation of the the gradient g of the point y of a twodimensional function f being gx y fx y. definition let g be a gradient. then g is a vector with n components that is defined for any point of a n-dimensional function xn. the gradient operator notation is defined as xn xn. g directs from any point of f towards the steepest ascent from this point with corresponding to the degree of this ascent. gradient descent means to going downhill in small steps from any starting point of our function towards the gradient g means vividly speaking the direction to which a ball would roll from the starting point with the size of the steps being proportional to steeper the descent the longer the steps. therefore we move slowly on a flat plateau and on a steep ascent we run downhill rapidly. if we came into a valley we would depending on the size of our steps jump over it or we would return into the valley across the opposite hillside in order to come closer and closer to the deepest point of the valley by walking back and forth similar to our ball moving within a round bowl. d. kriesel a brief introduction to neural networks chapter fundamentals on learning and training samples dkriesel.com figure visualization of the gradient descent on a two-dimensional error function. we move forward in the opposite direction of g i.e. with the steepest descent towards the lowest point with the step width being proportional to steeper the descent the faster the steps. on the left the area is shown in on the right the steps over the contour lines are shown in here it is obvious how a movement is made in the opposite direction of g towards the minimum of the function and continuously slows down proportionally to source patternclassificationgraddescent.pdf we go towards the gradient definition descent. let f be an n-dimensional function and s sn the given starting point. gradient descent means going from fs against the direction of g i.e. towards g with steps of the size of towards smaller and smaller values of f. gradient descent procedures are not an errorless optimization procedure at all we will see in the following sections however they work still well on many problems which makes them an optimization paradigm that is frequently used. anyway let us have a look on their potential disadvantages so we can keep them in mind a bit. gradient procedures incorporate several problems as already implied in section the gradient descent therefore the backpropagation is promising but not foolproof. one problem is that the result does not always reveal if an error has occurred. often gradient descents converge against suboptimal minima every gradient descent procedure can for example get stuck within a local minimum a of fig. on the facing page. gradient descent with errors d. kriesel a brief introduction to neural networks dkriesel.com gradient optimization procedures figure possible errors during a gradient descent a detecting bad minima b quasi-standstill with small gradient c oscillation in canyons d leaving good minima. this problem is increasing proportionally to the size of the error surface and there is no universal solution. in reality one cannot know if the optimal minimum is reached and considers a training successful if an acceptable minimum is found. flat plataeus on the error surface may cause training slowness when passing a flat plateau for instance the gradient also becomes negligibly small because there is hardly a descent b of fig. which requires many further steps. a hypothetically possible gradient of would completely stop the descent. even if good minima are reached they may be left afterwards on the other hand the gradient is very large at a steep slope so that large steps can be made and a good minimum can possibly be missed d of fig. steep canyons in the error surface may cause oscillations a sudden alternation from one very strong negative gradient to a very strong positive one can even result in oscillation c of fig. in nature such an error does not occur very often so that we can think about the possibilities b and d. d. kriesel a brief introduction to neural networks chapter fundamentals on learning and training samples dkriesel.com exemplary problems allow for testing self-coded learning strategies we looked at learning from the formal point of view not much yet but a little. now it is time to look at a few exemplary problem you can later use to test implemented networks and learning rules. boolean functions table illustration of the parity function with three inputs. a popular example is the one that did not work in the nineteen-sixties the xor function we need a hidden neuron layer which we have discussed in detail. thus we need at least two neurons in the inner layer. let the activation function in all layers in the input layer of course be the hyperbolic tangent. trivially we now expect the outputs or depending on whether the function xor outputs or and exactly here is where the first beginner s mistake occurs. for outputs close to or i.e. close to the limits of the hyperbolic tangent in case of the fermi function or we need very large network inputs. the only chance to reach these network inputs are large weights which have to be learned the learning process is largely extended. therefore it is wiser to enter the teaching inputs or as desired outputs or to be satisfied when the network outputs those values instead of and another favourite example for singlelayer perceptrons are the boolean functions and and or. the parity function the parity function maps a set of bits to or depending on whether an even number of input bits is set to or not. basically this is the function bn it is characterized by easy learnability up to approx. n in table but the learning effort rapidly increases from n the reader may create a score table for the parity function. what is conspicuous? the problem as a training sample for a function let us take two spirals coiled into each other on the facing page with the function certainly representing a mapping d. kriesel a brief introduction to neural networks dkriesel.com exemplary problems figure illustration of the training samples of the problem figure illustration of training samples for the checkerboard problem one of the spirals is assigned to the output value the other spiral to here memorizing does not help. the network has to understand the mapping itself. this example can be solved by means of an mlp too. the checkerboard problem we again create a two-dimensional function of the form and specify checkered training samples with one colored field representing and all the rest of them representing the difficulty increases proportionally to the size of the function while a field is easy to learn the larger fields are more difficult we eventually use methods that are more suitable for this kind of problems than the mlp. the problem is very similar to the checkerboard problem only that mathematically speaking the first problem is using polar coordinates instead of cartesian coordinates. i just want to introduce as an example one last trivial case the identity. the identity function by using linear activation functions the identity mapping from to course only within the parameters of the used activation function is no problem for the network but we put some obstacles in its way by using our sigmoid functions so that d. kriesel a brief introduction to neural networks chapter fundamentals on learning and training samples dkriesel.com it would be difficult for the network to learn the identity. just try it for the fun of it. now it is time to hava a look at our first mathematical learning rule. there are lots of other exemplary problems for lots and lots of further exemplary problems i want to recommend the technical report written by prechelt which also has been named in the sections about error measurement procedures.. the hebbian learning rule is the basis for most other learning rules in donald o. hebb formulated the hebbian rule which is the basis for most of the more complicated learning rules we will discuss in this text. we distinguish between the original form and the more general form which is a kind of principle for other learning rules. original rule definition rule. neuron j receives an input from neuron i and if both neurons are strongly active at the same time then increase the weight wij the strength of the connection between i and j. mathematically speaking the rule is wij oiaj with wij being the change in weight from i to j which is proportional to the wij following factors the output oi of the predecessor neu ron i as well as the activation aj of the successor neu ron j a constant i.e. the learning rate which will be discussed in section the changes in weight wij are simply added to the weight wij. why am i speaking twice about activation but in the formula i am using oi and aj i.e. the output of neuron of neuron i and the activation of neuron j? remember that the identity is often used as output function and therefore ai and oi of a neuron are often the same. besides hebb postulated his rule long before the specification of technical neurons. considering that this learning rule was preferred in binary activations it is clear that with the possible activations the weights will either increase or remain constant. sooner or later they would go ad infinitum since they can only be corrected when an error occurs. this can be compensated by using the activations thus the weights are decreased when the activation of the predecessor neuron dissents from the one of the successor neuron otherwise they are increased. but that is no longer the version of the hebbian rule. weights go ad infinitum d. kriesel a brief introduction to neural networks early form of the rule dkriesel.com hebbian rule generalized form exercises exercise calculate the average value and the standard deviation for the following data points. most of the learning rules discussed before are a specialization of the mathematically more general form of the hebbian rule. definition rule more general. the generalized form of the hebbian rule only specifies the proportionality of the change in weight to the product of two undefined functions but with defined input values. wij hoi wij gaj tj thus the product of the functions gaj tj and hoi wij as well as the constant learning rate results in the change in weight. as you can see h receives the output of the predecessor cell oi as well as the weight from predecessor to successor wij while g expects the actual and desired activation of the successor aj and tj t stands for the aforementioned teaching input. as already mentioned g and h are not specified in this general definition. therefore we will now return to the path of specialization we discussed before equation after we have had a short picture of what a learning rule could look like and of our thoughts about learning itself we will be introduced to our first network paradigm including the learning procedure. d. kriesel a brief introduction to neural networks part ii supervised learning network paradigms chapter the perceptron backpropagation and its variants a classic among the neural networks. if we talk about a neural network then in the majority of cases we speak about a percepton or a variation of it. perceptrons are multilayer networks without recurrence and with fixed input and output layers. description of a perceptron its limits and extensions that should avoid the limitations. derivation of learning procedures and discussion of their problems. as already mentioned in the history of neural networks the perceptron was described by frank rosenblatt in initially rosenblatt defined the already discussed weighted sum and a non-linear activation function as components of the perceptron. there is no established definition for a perceptron but most of the time the term is used to describe a feedforward network with shortcut connections. this network has a layer of scanner neurons with statically weighted connections to the following layer and is called input layer on the next page but the weights of all other layers are allowed to be changed. all neurons subordinate to the retina are pattern detectors. here we initially use a binary perceptron with every output neuron having exactly two possi ble output values or thus a binary threshold function is used as activation function depending on the threshold value of the output neuron. in a way the binary activation function represents an if query which can also be negated by means of negative weights. the perceptron can thus be used to accomplish true logical information processing. whether this method is reasonable is another matter of course this is not the easiest way to achieve boolean logic. i just want to illustrate that perceptrons can be used as simple logical components and that theoretically speaking any boolean function can be realized by means of perceptrons being connected in series or interconnected in a sophisticated way. but chapter the perceptron backpropagation and its variants dkriesel.com figure architecture of a perceptron with one layer of variable connections in different views. the solid-drawn weight layer in the two illustrations on the bottom can be trained. left side example of scanning information in the eye. right side upper part drawing of the same example with indicated fixed-weight layer using the defined designs of the functional descriptions for neurons. right side lower part without indicated fixed-weight layer with the name of each neuron corresponding to our convention. the fixed-weight layer will no longer be taken into account in the course of this work. d. kriesel a brief introduction to neural networks einkleiner uberblick uberneuronalenetzeepsilon-de input neuron only forwards data dkriesel.com we will see that this is not possible without connecting them serially. before providing the definition of the perceptron i want to define some types of neurons used in this chapter. definition neuron. an input neuron is an identity neuron. it exactly forwards the information received. thus it represents the identity function which should be indicated by the symbol therefore the input neuron is repre sented by the symbolgfed definition processing neuron. information processing neurons somehow process the input information i.e. do not represent the identity function. a binary neuron sums up all inputs by using the weighted sum as propagation function which we want to illustrate by the sign then the activation function of the neuron is the binary threshold function which can be illustrated by lh. this leads us to the complete depiction of information processing neurons lh other neurons that use the weighted sum as propagation function but the activation functions hyperbolic tangent or fermi function or with a separately defined activation function fact are similarly represented by namely wvut pqrs these neurons are also referred to as fermi neurons or tanh neuron. wvut pqrs tanh wvut pqrs fermi onml hijk fact now that we know the components of a perceptron we should be able to define it. definition the perceptron on the facing page a feedforward network containing a retina that is used only for data acquisition and which has fixed-weighted connections with the first neuron layer layer. the fixed-weight layer is followed by at least one trainable weight layer. one neuron layer is completely linked with the following layer. the first layer of the perceptron consists of the input neurons defined above. a feedforward network often contains shortcuts which does not exactly correspond to the original description and therefore is not included in the definition. we can see that the retina is not included in the lower part of fig. as a matter of fact the first neuron layer is often understood and sufficient for this method as input layer because this layer only forwards the input values. the retina itself and the static weights behind it are no longer mentioned or displayed since they do not process information in any case. so the depiction of a perceptron starts with the input neurons. it may confuse some readers that i claim that there is no definition of a perceptron but then define the perceptron in the following section. i therefore suggest keeping my definition in the back of your mind and just take it for granted in the course of this work. retina is unconsidered d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com the methods snipe setsettingstopologyfeedforward and the variation in neuralnetworkdescriptor-instance a apply settings to a descriptor which are appropriate for feedforward networks or feedforward networks with shortcuts. the respective kinds of connections are allowed all others are not and fastprop is activated. the singlelayer perceptron provides only one trainable weight layer trainable here connections with trainable weights go from the input layer to an output neuron which returns the information layer whether the pattern entered at the input neurons was recognized or not. thus a singlelayer perception slp has only one level of trainable weights on page definition perceptron. a singlelayer perceptron is a perceptron having only one layer of variable weights and one layer of output neurons the technical view of an slp is shown in fig. important! certainly the existence of several output neurons n does not considerably change the concept of the perceptron a perceptron with several output neurons can also be regarded as several different perceptrons with the same input. bias gfed wbias gfed gfed figure a singlelayer perceptron with two input neurons and one output neuron. the network returns the output by means of the arrow leaving the network. the trainable layer of weights is situated in the center as a reminder the bias neuron is again included here. although the weight wbias is a normal weight and also treated like this i have represented it by a dotted line which significantly increases the clarity of larger networks. in future the bias neuron will no longer be included. gfed gfed gfed gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii vnnnnnnnnnnnnnnnnnn wnnnnnnnnnnnnnnnnn aaaaaaaaa aaaaaaaaa gfed gfed gfed figure singlelayer perceptron with several output neurons d. kriesel a brief introduction to neural networks v t w dkriesel.com the singlelayer perceptron gfed gfed gfed aaaa gfed aaaa aaaa gfed aaaa gfed figure two singlelayer perceptrons for boolean functions. the upper singlelayer perceptron realizes an and the lower one realizes an or. the activation function of the information processing neuron is the binary threshold function. where available the threshold values are written into the neurons. the boolean functions and and or shown in fig. are trivial examples that can easily be composed. now we want to know how to train a singlelayer perceptron. we will therefore at first take a look at the perceptron learning algorithm and then we will look at the delta rule. perceptron learning algorithm and convergence theorem the original perceptron learning algorithm with binary neuron activation function is described in alg. it has been proven that the algorithm converges in finite time so in finite time the perceptron can learn anything it can represent convergence theorem but please do not get your hopes up too soon! what the perceptron is capable to represent will be explored later. during the exploration of linear separability of problems we will cover the fact that at least the singlelayer perceptron unfortunately cannot represent a lot of problems. the delta rule as a gradient based learning strategy for slps in the following we deviate from our binary threshold value as activation function because at least for backpropagation of error we need as you will see a differentiable or even a semi-linear activation function. for the now following delta rule backpropagation derived in it is not always necessary but useful. this fact however will also be pointed out in the appropriate part of this work. compared with the aforementioned perceptron learning algorithm the delta rule has the advantage to be suitable for non-binary activation functions and being far away from fact now differentiable d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com if y t then else output is okay no correction of weights end for end if if y then for all input neurons i do wi wi oi weight towards by oi input p into the network calculate output y set of training patterns for all output neurons do while p p and error too large do end if end for end while algorithm perceptron learning algorithm. the perceptron learning algorithm reduces the weights to output neurons that return instead of and in the inverse case increases weights. wi wi oi weight towards by oi end for end if if y then for all input neurons i do d. kriesel a brief introduction to neural networks dkriesel.com the singlelayer perceptron the learning target to automatically learn faster. suppose that we have a singlelayer perceptron with randomly set weights which we want to teach a function by means of training samples. the set of these training samples is called p. it contains as already defined the pairs t of the training samples p and the associated teaching input t. i also want to remind you that x is the input vector and y is the output vector of a neural net work output neurons are referred to as i is the input and o is the output of a neuron. additionally we defined that the error vector ep represents the difference y under a certain training sample p. furthermore let o be the set of out put neurons and i be the set of input neurons. another naming convention shall be that for example for an output o and a teaching input t an additional index p may be set in order to indicate that these values are pattern-specific. sometimes this will considerably enhance clarity. now our learning target will certainly be that for all training samples the output y of the network is approximately the desired output t formally it is true that i.e. p y t or p ep this means we first have to understand the total error err as a function of the weights the total error increases or decreases depending on how we change the weights. definition function. the error function err w r regards the of weights w as a vector and maps the values onto the normalized output error because otherwise not all errors can be mapped onto one single e r to perform a gradient descent. it is obvious that a specific error function can analogously be generated for a single pattern p. error as function as already shown in section gradient descent procedures calculate the gradient of an arbitrary but finite-dimensional function of the error function errw and move down against the direction of the gradient until a minimum is reached. errw is defined on the set of all weights which we here regard as the vector w. so we try to decrease or to minimize the error by simply tweaking the weights thus one receives information about how to change the weights change in all following the tradition of the literature i previously defined w as a weight matrix. i am aware of this conflict but it should not bother us here. d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com thus we tweak every single weight and observe how the error function changes i.e. we derive the error function according to a weight wi and obtain the value wi of how to change this weight. wi errw wi now the following question arises how is our error function defined exactly? it is not good if many results are far away from the desired ones the error function should then provide large values on the other hand it is similarly bad if many results are close to the desired ones but there exists an extremely far outlying result. the squared distance between the output vector y and the teaching input t appears adequate to our needs. it provides the error errp that is specific for a training sample p over the output of all output neurons errpw x o yp figure exemplary error surface of a neural network with two trainable connections und generally neural networks have more than two connections but this would have made the illustration too complex. and most of the time the error surface is too craggy which complicates the search for the minimum. weights is referred to as w by calculating the gradient errw of the error function errw w errw. due to this relation there is a proportionality constant for which equality holds will soon get another meaning and a real practical use beyond the mere meaning of a proportionality constant. i just ask the reader to be patient for a while. w errw. to simplify further analysis we now rewrite the gradient of the error-function according to all weights as an usual partial derivative according to a single weight wi only variable weights exists between the hidden and the output layer thus we calculate the squared difference of the components of the vectors t and y given the pattern p and sum up these squares. the summation of the specific errors errpw of all patterns p then yields the definition of the error err and there d. kriesel a brief introduction to neural networks dkriesel.com the singlelayer perceptron fore the definition of the error function errw results from the sum of the specific errors errpw sum over all p x o yp sum over all errw x z x p p p p the observant reader will certainly wonder where the factor in equation on the preceding page suddenly came from and why there is no root in the equation as this formula looks very similar to the euclidean distance. both facts result from simple pragmatics our intention is to minimize the error. because the root function decreases with its argument we can simply omit it for reasons of calculation and implementation efforts since we do not need it for minimization. similarly it does not matter if the term to be minimized is divided by therefore i am allowed to multiply by this is just done so that it cancels with a in the course of our calculation. now we want to continue deriving the delta rule for linear activation functions. we have already discussed that we tweak the individual weights wi a bit and see how the error errw is changing which corresponds to the derivative of the error function errw according to the very same weight wi this derivative corresponds to the sum of the derivatives of all specific errors errp according to this weight the total error errw wi x p p errw wi errpw wi once again i want to think about the question of how a neural network processes data. basically the data is only transferred through a function the result of the function is sent through another one and so on. if we ignore the output function the path of the neuron outputs and which the neurons and entered into a neuron initially is the propagation function weighted sum from which the network input is going to be received. this is then sent through the activation function of the neuron so that we receive the output of this neuron which is at the same time a component of the output vector y net fact factnet o y as we can see this output results from many nested functions o factnet it is clear that we could break down the output into the single input neurons is unnecessary here since they do not d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com process information in an slp. thus we want to calculate the derivatives of equation on the preceding page and due to the nested functions we can apply the chain rule to factorize the derivative errpw in equation on the previous wi page. errpw wi errpw op op wi i.e. let us take a look at the first multiplicative factor of the above equation which represents the derivative of the specific error errpw according to the output the change of the error errp with an output op the examination of errp on page clearly shows that this change is exactly the difference between teaching input and output op since is an output neuron op yp the closer the output is to the teaching input the smaller is the specific error. thus we can replace one by the other. this difference is also called p is the reason for the name delta rule errpw wi op op wi p op wi the second multiplicative factor of equation and of the following one is the derivative of the output specific to the pattern p of the neuron according to the weight wi so how does op change when the weight from i to is changed? errpw wi i iopiwi wi due to the requirement at the beginning of the derivation we only have a linear activation function fact therefore we can just as well look at the change of the network input when wi is changing p p the resulting derivative p p p i iopiwi wi can now be simplified the function i iopiwi to be derived consists of many summands and only the summand opiwi contains the variable wi according to which we derive. thus i iopiwi wi opi and therefore errpw wi p opi opi p we insert this in equation on the previous page which results in our modification rule for a weight wi opi p wi x p p however from the very beginning the derivation has been intended as an o ine rule by means of the question of how to add the errors of all patterns and how to learn them after all patterns have been represented. although this approach is mathematically correct the implementation is far more time-consuming and as we will see later in this chapter partially d. kriesel a brief introduction to neural networks dkriesel.com linear separability needs a lot of compuational effort during training. the version of the delta rule simply omits the summation and learning is realized immediately after the presentation of each pattern this also simplifies the notation is no longer necessarily related to a pattern p wi oi this version of the delta rule shall be used for the following definition definition rule. if we determine analogously to the aforementioned derivation that the function h of the hebbian theory on page only provides the output oi of the predecessor neuron i and if the function g is the difference between the desired activation t and the actual activation a we will receive the delta rule also known as widrowhoff rule wi oi a oi if we use the desired output of the activation as teaching input and therefore the output function of the output neurons does not represent an identity we obtain wi oi o oi and then corresponds to the difference between t and o in the case of the delta rule the change of all weights to an output neuron is proportional in. in. output table definition of the logical xor. the input values are shown of the left the output values on the right. to the difference between the current activation or output a or o and the corresponding teaching input t we want to refer to this factor as which is also referred to as apparently the delta rule only applies for slps since the formula is always related to the teaching input and there is no teaching input for the inner processing layers of neurons. delta rule only for slp a slp is only capable of representing linearly separable data let f be the xor function which expects two binary inputs and generates a binary output the precise definition see table let us try to represent the xor function by means of an slp with two input neurons and one output neuron on the following page. d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com gfed gfed bbbb bbbb xor? figure sketch of a singlelayer perceptron that shall represent the xor function which is impossible. here we use the weighted sum as propagation function a binary activation function with the threshold value and the identity as output function. depending on and has to output the value if the following holds net we assume a positive weight the inequality is then equivalent to with a constant threshold value the right part of inequation is a straight line through a coordinate system defined by the possible outputs und of the input neurons and for a required for inequation positive the output neuron fires for figure linear separation of n inputs of the input neurons and by a straight line. a and b show the corners belonging to the sets of the xor function that are to be separated. d. kriesel a brief introduction to neural networks dkriesel.com linear separability n number of share lin. separable ones binary functions table number of functions concerning n binary inputs and number and proportion of the functions thereof which can be linearly separated. in accordance with input combinations lying above the generated straight line. for a negative it would fire for all input combinations lying below the straight line. note that only the four corners of the unit square are possible inputs because the xor function only knows binary inputs. in order to solve the xor problem we have to turn and move the straight line so that input set a is separated from input set b this is obviously impossible. generally the input parameters of n many input neurons can be represented in an ndimensional cube which is separated by an slp through an hyperplane only sets that can be separated by such a hyperplane i.e. which are linearly separable can be classified by an slp. slp cannot do everything figure linear separation of n inputs from input neurons and by plane. unfortunately it seems that the percentage of the linearly separable problems rapidly decreases with increasing n table which limits the functionality of the slp. additionally tests for linear separability are difficult. thus for more difficult tasks with more inputs we need something more powerful than slp. the xor problem itself is one of these tasks since a perceptron that is supposed to represent the xor function already needs a hidden layer on the next page. few tasks are linearly separable d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com gfed aaaa gfed gfed aaaa gfed xor figure neural network realizing the xor function. threshold values far as they are existing are located within the neurons. a multilayer perceptron contains more trainable weight layers a perceptron with two or more trainable weight layers multilayer perceptron or mlp is more powerful than an slp. as we know a singlelayer perceptron can divide the input space by means of a hyperplane a two-dimensional input space by means of a straight line. a twostage perceptron trainable weight layers three neuron layers can classify convex polygons by further processing these straight lines e.g. in the form patterns lying above straight line below straight line and below straight line thus we metaphorically speaking took an slp with several output neurons and another slp more planes it part of fig. on the facing page. a multilayer perceptron represents an universal function approximator which is proven by the theorem of cybenko another trainable weight layer proceeds analogously now with the convex polygons. those can be added subtracted or somehow processed with other operations part of fig. on the next page. generally can be mathematically proven that even a multilayer perceptron with one layer of hidden neurons can arbitrarily precisely approximate functions with only finitely many discontinuities as well as their first derivatives. unfortunately this proof is not constructive and therefore it is left to us to find the correct number of neurons and weights. in the following we want to use a widespread abbreviated form for different multilayer perceptrons we denote a twostage perceptron with neurons in the input layer neurons in the hidden layer and neurons in the output layer as a definition perceptron. perceptrons with more than one layer of variably weighted connections are referred to as multilayer perceptrons an n-layer or n-stage perceptron has thereby exactly n variable weight layers and n neuron layers retina is disregarded here with neuron layer being the input layer. since three-stage perceptrons can classify sets of any form by combining and sepa mlp is sufficient d. kriesel a brief introduction to neural networks dkriesel.com the multilayer perceptron gfed gfed tjjjjjjjjjjjjjjjjjjjjjjjjj gfed gfed gfed wooooooooooooooooo gfed gfed gfed gfed gfed gfed gfed gfed wnnnnnnnnnnnnnnnnn gfed gfed figure we know that an slp represents a straight line. with trainable weight layers several straight lines can be combined to form convex polygons by using trainable weight layers several polygons can be formed into arbitrary sets d. kriesel a brief introduction to neural networks t w t t u u w w t t r r w q q chapter the perceptron backpropagation and its variants dkriesel.com n classifiable sets hyperplane convex polygon any set any set as well i.e. no advantage table representation of which perceptron can classify which types of sets with n being the number of trainable weight layers. rating arbitrarily many convex polygons another step will not be advantageous with respect to function representations. be cautious when reading the literature there are many different definitions of what is counted as a layer. some sources count the neuron layers some count the weight layers. some sources include the retina some the trainable weight layers. some exclude some reason the output neuron layer. in this work i chose the definition that provides in my opinion the most information about the learning capabilities and i will use it cosistently. remember an n-stage perceptron has exactly n trainable weight layers. you can find a summary of which perceptrons can classify which types of sets in table we now want to face the challenge of training perceptrons with more than one weight layer. backpropagation of error generalizes the delta rule to allow for mlp training next i want to derive and explain the backpropagation of error learning rule backpropagation backprop or bp which can be used to train multistage perceptrons with activation functions. binary threshold functions and other non-differentiable functions are no longer supported but that doesn t matter we have seen that the fermi function or the hyperbolic tangent can arbitrarily approximate the binary threshold function by means of a temperature parameter t. to a large extent i will follow the derivation according to and once again i want to point out that this procedure had previously been published by paul werbos in but had consideraby less readers than in backpropagation is a gradient descent procedure all strengths and weaknesses of the gradient descent with the error function errw receiving all n weights as arguments on page and assigning them to the output error i.e. being n-dimensional. on errw a point of small error or even a point of the smallest error is sought by means of the gradient descent. thus in analogy to the delta rule backpropagation trains the weights of the neural network. and it is exactly semilinear functions are monotonous and differen tiable but generally they are not linear. d. kriesel a brief introduction to neural networks dkriesel.com backpropagation of error the delta rule or its variable i for a neuron i which is expanded from one trainable weight layer to several ones by backpropagation. the derivation is similar to the one of the delta rule but with a generalized delta let us define in advance that the network input of the individual neurons i results from the weighted sum. furthermore as with the derivation of the delta rule let opi netpi etc. be defined as the already familiar oi neti etc. under the input pattern p we used for the training. let the output function be the identity again thus oi factnetpi holds for any neuron i. since this is a generalization of the delta rule we use the same formula framework as with the delta rule on page as already indicated we have to generalize the variable for every neuron. first of all where is the neuron for which we want to calculate it is obvious to select an arbitrary inner neuron h having a set k of predecessor neurons k as well as a set of l successor neurons l which are also inner neurons fig. it is therefore irrelevant whether the predecessor neurons are already the input neurons. now we perform the same derivation as for the delta rule and split functions by means the chain rule. i will not discuss this derivation in great detail but the principal is similar to that of the delta rule generalization of onml hijk xrrrrrrrrrrrrrrr fact wppppppp nnnnnnn pppppppp wkh h whl k h l figure illustration of the position of our neuron h within the neural network. it is lying in layer h the preceding layer is k the subsequent layer is l. differences are as already mentioned in the generalized we initially derive the error function err according to a weight wkh. errwkh wkh neth wkh err neth h the first factor of equation is h which we will deal with later in this text. the numerator of the second factor of the equation includes the network input i.e. the weighted sum is included in the numerator so that we can immediately derive it. again all summands of the sum drop out apart from the summand containing wkh. d. kriesel a brief introduction to neural networks w x chapter the perceptron backpropagation and its variants dkriesel.com this summand is referred to as wkh ok. if we calculate the derivative the output of neuron k becomes according to the definition of the multidimensional chain rule we immediately obtain equation neth wkh k k wkhok wkh p ok l l err oh err netl netl oh as promised we will now discuss the h of equation on the previous page which is split up again according of the chain rule the sum in equation contains two factors. now we want to discuss these factors being added over the subsequent layer l. we simply calculate the second factor in the following equation h err neth err oh oh neth netl oh p whl h h whl oh oh the derivation of the output according to the network input second factor in equation clearly equals the derivation of the activation function according to the network input the same applies for the first factor according to the definition of our err netl l oh neth factneth neth fact now we insert err oh l l lwhl consider this an important passage! we now analogously derive the first factor in equation therefore we have to point out that the derivation of the error function according to the output of an inner neuron layer depends on the vector of all network inputs of the next following layer. this is reflected in equation netll err oh oh you can find a graphic version of the generalization including all splittings in fig. on the facing page. the reader might already have noticed that some intermediate results were shown in frames. exactly those intermediate results were highlighted in that way which are a factor in the change in weight of wkh. if the aforementioned equations are d. kriesel a brief introduction to neural networks dkriesel.com backpropagation of error h err neth oh neth err oh actneth err netl l l l netl oh p p whl oh h h oh whl figure graphical representation of the equations equal signs and chain rule splittings arrows in the framework of the backpropagation derivation. the leaves of the tree reflect the final results from the generalization of which are framed in the derivation. d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com combined with the highlighted intermediate results the outcome of this will be the wanted change in weight wkh to actneth x l l h f wkh ok h with lwhl of course only in case of h being an inner neuron there would not be a subsequent layer l. the case of h being an output neuron has already been discussed during the derivation of the delta rule. all in all the result is the generalization of the delta rule called backpropagation of error wkh ok h with h actneth yh outside l l lwhl inside in contrast to the delta rule is treated differently depending on whether h is an output or an inner hidden neuron actneth p if h is an output neuron then ph f actnetph yph thus under our training pattern p the weight wkh from k to h is changed proportionally according to the learning rate the output opk of the predeces sor neuron k the gradient of the activation function at the position of the network input of the successor neuron actnetph and teach. input changed for the outer weight layer backpropagation for inner layers the difference between teaching input tph and output yph of the successor neuron h. in this case backpropagation is working on two neuron layers the output layer with the successor neuron h and the preceding layer with the predecessor neuron k. if h is an inner hidden neuron then pl whl actnetph x ph f l l holds. i want to explicitly mention that backpropagation is now working on three layers. here neuron k is the predecessor of the connection to be changed with the weight wkh the neuron h is the successor of the connection to be changed and the neurons l are lying in the layer following the successor neuron. thus according to our training pattern p the weight wkh from k to h is proportionally changed according to the learning rate the output of the predecessor neuron opk the gradient of the activation function at the position of the network input of the successor neuron actnetph as well as and this according the difference the weighted sum of the changes in weight to all neurons following h p l l pl whl. is to d. kriesel a brief introduction to neural networks backprop expands delta rule dkriesel.com backpropagation of error definition if we summarize formulas on the preceding page and on the facing page we receive the following final formula for backpropagation identifiers p are ommited for reasons of clarity wkh ok h with h actneth p actneth yh outside l l lwhl inside snipe an online variant of backpropagation is implemented in the method trainbackpropagationoferror within the class neuralnetwork. heading back boiling backpropagation down to delta rule as explained above the delta rule is a special case of backpropagation for onestage perceptrons and linear activation functions i want to briefly explain this circumstance and develop the delta rule out of backpropagation in order to augment the understanding of both rules. we have seen that backpropagation is defined by wkh ok h with h actneth p actneth yh outside l l lwhl inside it is obvious that backpropagation initially processes the last weight layer directly by means of the teaching input and then works backwards from layer to layer while considering each preceding change in weights. thus the teaching input leaves traces in all weight layers. here i describe the first rule and the second part of backpropagation delta rule on more layers in one go which may meet the requirements of the matter but not of the research. the first part is obvious which you will soon see in the framework of a mathematical gimmick. decades of development time and work lie between the first and the second recursive part. like many groundbreaking inventions it was not until its development that it was recognized how plausible this invention was. since we only use it for one-stage perceptrons the second part of backpropagation is omitted without substitution. the result is wkh ok h with h actneth oh furthermore we only want to use linear activation functions so that act is constant. as is generally known constants can be combined and therefore we directly merge the constant derivative act and constant for at least one lerning cycle the learning rate light-colored in thus the result is wkh ok h ok oh this exactly corresponds to the delta rule definition. d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com the selection of the learning rate has heavy influence on the learning process variation of the learning rate over time in the meantime we have often seen that the change in weight is in any case proportional to the learning rate thus the selection of is crucial for the behaviour of backpropagation and for learning procedures in general. how fast will be learned? definition rate. speed and accuracy of a learning procedure can always be controlled by and are always proportional to a learning rate which is written as if the value of the chosen is too large the jumps on the error surface are also too large and for example narrow valleys could simply be jumped over. additionally the movements across the error surface would be very uncontrolled. thus a small is the desired input which however can cost a huge often unacceptable amount of time. experience shows that good learning rate values are in the range of the selection of significantly depends on the problem the network and the training data so that it is barely possible to give practical advise. but for instance it is popular to start with a relatively large e.g. and to slowly decrease it down to for simpler problems can often be kept constant. during training another stylistic device can be a variable learning rate in the beginning a large learning rate leads to good results but later it results in inaccurate learning. a smaller learning rate is more time-consuming but the result is more precise. thus during the learning process the learning rate needs to be decreased by one order of magnitude once or repeatedly. a common error also seems to be a very neat solution at first glance is to continually decrease the learning rate. here it quickly happens that the descent of the learning rate is larger than the ascent of a hill of the error function we are climbing. the result is that we simply get stuck at this ascent. solution rather reduce the learning rate gradually as mentioned above. different layers different learning rates the farer we move away from the output layer during the learning process the slower backpropagation is learning. thus it is a good idea to select a larger learning rate for the weight layers close to the input layer than for the weight layers close to the output layer. d. kriesel a brief introduction to neural networks dkriesel.com resilient backpropagation resilient backpropagation is an extension to backpropagation of error we have just raised two backpropagationspecific properties that can occasionally be a problem addition to those which are already caused by gradient descent itself on the one hand users of backpropagation can choose a bad learning rate. on the other hand the further the weights are from the output layer the slower backpropagation learns. for this reason martin riedmiller et al. enhanced backpropagation and called their version resilient backpropagation rprop i want to compare backpropagation and rprop without explicitly declaring one version superior to the other. before actually dealing with formulas let us informally compare the two primary ideas behind rprop their consequences to the already familiar backpropagation. learning rates backpropagation uses by default a learning rate which is selected by the user and applies to the entire network. it remains static until it is manually changed. we have already explored the disadvantages of this approach. here rprop pursues a completely different approach there is no global learning rate. first each weight wij has its own learning rate ij and second these learning rates are not chosen by the user but are automatically set by rprop itself. third the weight changes are not static but are adapted for each time step of rprop. to account for the temporal change we have to correctly call it ijt. this not only enables more focused learning also the problem of an increasingly slowed down learning throughout the layers is solved in an elegant way. weight change when using backpropagation weights are changed proportionally to the gradient of the error function. at first glance this is really intuitive. however we incorporate every jagged feature of the error surface into the weight changes. it is at least questionable whether this is always useful. here rprop takes other ways as well the amount of weight change wij simply directly corresponds to the automatically adjusted learning rate ij. thus the change in weight is not proportional to the gradient it is only influenced by the sign of the gradient. until now we still do not know how exactly the ij are adapted at run time but let me anticipate that the resulting process looks consider- much ably less rugged than an error function. smoother learning in contrast to backprop the weight update step is replaced and an additional step for the adjustment of the learning rate is added. now how exactly are these ideas being implemented? d. kriesel a brief introduction to neural networks one learningrate per weight automatic learning rate adjustment chapter the perceptron backpropagation and its variants dkriesel.com change in definition rprop. weight changes are not proportional to the gradient let us first consider the change in weight. we have already noticed that the weightspecific learning rates directly serve as absolute values for the changes of the respective weights. there remains the question of where the sign comes from this is a point at which the gradient comes into play. as with the derivation of backpropagation we derive the error function errw by the individual weights wij and obtain gradients errw now the big wij difference rather than multiplicatively incorporating the absolute value of the gradient into the weight change we consider only the sign of the gradient. the gradient hence no longer determines the strength but only the direction of the weight change. if the sign of the gradient errw is pos wij itive we must decrease the weight wij. so the weight is reduced by ij. if the sign of the gradient is negative the weight needs to be increased. so ij is added to it. if the gradient is exactly nothing happens at all. let us now create a formula from this colloquial description. the corresponding terms are affixed with a to show that everything happens at the same time step. this might decrease clarity at first glance but is nevertheless important because we will soon look at another formula that operates on different time steps. instead we shorten the gradient to g errw wij gradient determines only direction of the updates wijt ijt ijt if gt if gt otherwise. we now know how the weights are changed now remains the question how the learning rates are adjusted. finally once we have understood the overall system we will deal with the remaining details like initialization and some specific constants. many dynamically adjusted learning rates instead of one static to adjust the learning rate ij we again have to consider the associated gradients g of two time steps the gradient that has just passed and the current one again only the sign of the gradient matters and we now must ask ourselves what can happen to the sign over two time steps? it can stay the same and it can flip. if the sign changes from gt to gt we have skipped a local minimum in the gradient. hence the last update was too large and ijt has to be reduced as compared to the previous ijt one can say that the search needs to be more accurate. in mathematical terms we obtain a new ijt by multiplying the old ijt with a constant which is between and in this case we know that in the last time step something went wrong d. kriesel a brief introduction to neural networks dkriesel.com resilient backpropagation hence we additionally reset the weight update for the weight wij at time step to so that it not applied at all shown in the following formula. however if the sign remains the same one can perform a increase of ij to get past shallow areas of the error function. here we obtain our new ijt by multiplying the old ijt with a constant which is greater than definition of learning rates in rprop. gt gt otherwise. ijt ijt ijt ijt rprop only learns o ine caution this also implies that rprop is exclusively designed for o ine. if the gradients do not have a certain continuity the learning process slows down to the lowest rates remains there. when learning online one changes loosely speaking the error function with each new epoch since it is based on only one training pattern. this may be often well applicable in backpropagation and it is very often even faster than the o ine version which is why it is used there frequently. it lacks however a clear mathematical motivation and that is exactly what we need here. max we are still missing a few details to use rprop in practice a few minor issues remain unanswered namely how large are and how much are learning rates reinforced or weakened? how to choose how are the weight-specific learning rates what are the upper and lower bounds min and max for ij set? we now answer these questions with a quick motivation. the initial value for the learning rates should be somewhere in the order of the initialization of the weights. has proven to be a good choice. the authors of the rprop paper explain in an obvious way that this value as long as it is positive and without an exorbitantly high absolute value does not need to be dealt with very critically as it will be quickly overridden by the automatic adaptation anyway. equally uncritical is max for which they recommend without further mathematical justification a value of which is used throughout most of the literature. one can set this parameter to lower values in order to allow only very cautious updates. small update steps should be allowed in any case so we set min protipp since the ij can be changed only by multiplication would be a rather suboptimal initialization d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com now we have left only the parameters and let us start with if this value is used we have skipped a minimum from which we do not know where exactly it lies on the skipped track. analogous to the procedure of binary search where the target object is often skipped as well we assume it was in the middle of the skipped track. so we need to halve the learning rate which is why the canonical choice is being selected. if the value of is used learning rates shall be increased with caution. here we cannot generalize the principle of binary search and simply use the value otherwise the learning rate update will end up consisting almost exclusively of changes in direction. independent of the particular problems a value of has proven to be promising. slight changes of this value have not significantly affected the rate of convergence. this fact allowed for setting this value as a constant as well. with advancing computational capabilities of computers one can observe a more and more widespread distribution of networks that consist of a big number of layers i.e. deep networks. for such networks it is crucial to prefer rprop over the original backpropagation because backprop as already indicated learns very slowly at weights wich are far from the output layer. for problems with a smaller number of layers i would recommend testing the more widespread backpropagation both o ine and online learning and the less common rprop equivalently. in snipe resilient backpropasnipe gation is supported via the method trainresilientbackpropagation of the class neuralnetwork. furthermore you can also use an additional improvement to resilient propagation which is however not dealt with in this work. there are getters and setters for the different parameters of rprop. backpropagation has often been extended and altered besides rprop backpropagation has often been extended. many of these extensions can simply be implemented as optional features of backpropagation in order to have a larger scope for testing. in the following i want to briefly describe some of them. adding momentum to learning let us assume to descent a steep slope on skis what prevents us from immediately stopping at the edge of the slope to the plateau? exactly our momentum. with backpropagation the momentum term is responsible for the fact that a kind of moment of inertia is added to every step size on the next page by always adding a fraction of the previous change to every new change in weight pwijnow opi pj pwijprevious. d. kriesel a brief introduction to neural networks rprop is very good for deep networks dkriesel.com further variations and extensions to backpropagation moment of inertia of course this notation is only used for a better understanding. generally as already defined by the concept of time when referring to the current cycle as then the previous cycle is identified by which is continued successively. and now we come to the formal definition of the momentum term definition term. the variation of backpropagation by means of the momentum term is defined as follows wijt oi j wijt we accelerate on plateaus quasistandstill on plateaus and slow down on craggy surfaces oscillations. moreover the effect of inertia can be varied via the prefactor common values are between und additionally the momentum enables the positive effect that our skier swings back and forth several times in a minimum and finally lands in the minimum. despite its nice one-dimensional appearance the otherwise very rare error of leaving good minima unfortunately occurs more frequently because of the momentum term which means that this is again no optimal solution we are by now accustomed to this condition. flat spot elimination prevents neurons from getting stuck it must be pointed out that with the hyperbolic tangent as well as with the fermi figure we want to execute the gradient descent like a skier crossing a slope who would hardly stop immediately at the edge to the plateau. neurons get stuck function the derivative outside of the close proximity of is nearly this results in the fact that it becomes very difficult to move neurons away from the limits of the activation spots which could extremely extend the learning time. this problem can be dealt with by modifying the derivative for example by adding a constant which is called flat spot elimination or more colloquial fudging. it is an interesting observation that success has also been achieved by using derivatives defined as constants a nice example making use of this effect is the fast hyperbolic tangent approximation by anguita et al. introduced in section on page in the outer regions of it s d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com well approximated and accelerated derivative it makes use of a small constant. weight decay punishment of large weights the second derivative can be used too according to david parker second order backpropagation also usese the second gradient i.e. the second multi-dimensional derivative of the error function to obtain more precise estimates of the correct wij. even higher derivatives only rarely improve the estimations. thus less training cycles are needed but those require much more computational effort. in general we use further derivatives hessian matrices since the functions are multidimensional for higher order methods. as expected the procedures reduce the number of learning epochs but significantly increase the computational effort of the individual epochs. so in the end these procedures often need more learning time than backpropagation. the quickpropagation learning procedure uses the second derivative of the error propagation and locally understands the error function to be a parabola. we analytically determine the vertex the lowest point of the said parabola and directly jump to this point. thus this learning procedure is a second-order procedure. of course this does not work with error surfaces that cannot locally be approximated by a parabola it is not always possible to directly say whether this is the case. the weight decay according to paul werbos is a modification that extends the error by a term punishing large weights. so the error under weight decay errwd does not only increase proportionally to the actual error but also proportionally to the square of the weights. as a result the network is keeping the weights small during learning. errwd err x w w punishment this approach is inspired by nature where synaptic weights cannot become infinitely strong as well. additionally due to these small weights the error function often shows weaker fluctuations allowing easier and more controlled learning. the prefactor again resulted from simple pragmatics. the factor controls the strength of punishment values from to are often used here. keep weights small cutting networks down pruning and optimal brain damage if we have executed the weight decay long enough and notice that for a neuron in the input layer all successor weights are or close to we can remove the neuron prune the network d. kriesel a brief introduction to neural networks dkriesel.com initial configuration of a multilayer perceptron hence losing this neuron and some weights and thereby reduce the possibility that the network will memorize. this procedure is called pruning. such a method to detect and delete unnecessary weights and neurons is referred to as optimal brain damage i only want to describe it briefly the mean error per output neuron is composed of two competing terms. while one term as usual considers the difference between output and teaching input the other one tries to a weight towards if a weight is strongly needed to minimize the error the first term will win. if this is not the case the second term will win. neurons which only have zero weights can be pruned again in the end. there are many other variations of backprop and whole books only about this subject but since my aim is to offer an overview of neural networks i just want to mention the variations above as a motivation to read on. for some of these extensions it is obvious that they cannot only be applied to feedforward networks with backpropagation learning procedures. we have gotten to know backpropagation and feedforward topology now we have to learn how to build a neural network. it is of course impossible to fully communicate this experience in the framework of this work. to obtain at least some of this knowledge i now advise you to deal with some of the exemplary problems from getting started initial configuration of a multilayer perceptron after having discussed the backpropagation of error learning procedure and knowing how to train an existing network it would be useful to consider how to implement such a network. number of layers two or three may often do the job but more are also used let us begin with the trivial circumstance that a network should have one layer of input neurons and one layer of output neurons which results in at least two layers. additionally we need as we have already learned during the examination of linear separability at least one hidden layer of neurons if our problem is not linearly separable is as we have seen very likely. it is possible as already mentioned to mathematically prove that this mlp with one hidden neuron layer is already capable of approximating arbitrary functions with any accuracy but it is necessary not only to discuss the representability of a problem by means of a perceptron but also the learnability. representability means that a perceptron can in principle realize note we have not indicated the number of neurons in the hidden layer we only mentioned the hypothetical possibility. d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com a mapping but learnability means that we are also able to teach it. in this respect experience shows that two hidden neuron layers three trainable weight layers can be very useful to solve a problem since many problems can be represented by a hidden layer but are very difficult to learn. one should keep in mind that any additional layer generates additional subminima of the error function in which we can get stuck. all these things considered a promising way is to try it with one hidden layer at first and if that fails retry with two layers. only if that fails one should consider more layers. however given the increasing calculation power of current computers deep networks with a lot of layers are also used with success. the number of neurons has to be tested the number of neurons from input and output layer where the number of input and output neurons is already defined by the problem statement principally corresponds to the number of free parameters of the problem to be represented. since we have already discussed the network capacity with respect to memorizing or a too imprecise problem representation it is clear that our goal is to have as few free parameters as possible but as many as necessary. but we also know that there is no standard solution for the question of how many neurons should be used. thus the most useful approach is to initially train with only a few neurons and to repeatedly train new networks with more neurons until the result significantly improves and particularly the generalization performance is not affected approach. selecting an activation function another very important parameter for the way of information processing of a neural network is the selection of an activation function. the activation function for input neurons is fixed to the identity function since they do not process information. the first question to be asked is whether we actually want to use the same activation function in the hidden layer and in the ouput layer no one prevents us from choosing different functions. generally the activation function is the same for all hidden neurons as well as for the output neurons respectively. for tasks of function approximation it has been found reasonable to use the hyperbolic tangent part of fig. on page as activation function of the hidden neurons while a linear activation function is used in the output. the latter is absolutely necessary so that we do not generate a limited output intervall. contrary to the input layer which uses linear activation functions as well the output layer still processes information because it has d. kriesel a brief introduction to neural networks dkriesel.com the encoding problem and related problems threshold values. however linear activation functions in the output can also cause huge learning steps and jumping over good minima in the error surface. this can be avoided by setting the learning rate to very small values in the output layer. an unlimited output interval is not essential for pattern recognition if the hyperbolic tangent is used in any case the output interval will be a bit larger. unlike with the hyperbolic tangent with the fermi function part of fig. on the following page it is difficult to learn something far from the threshold value its result is close to however here a lot of freedom is given for selecting an activation function. but generally the disadvantage of sigmoid functions is the fact that they hardly learn something for values far from thei threshold value unless the network is modified. range of random values could be the interval not including or values very close to this random initialization has a nice side effect chances are that the average of network inputs is close to a value that hits most activation functions the region of the greatest derivative allowing for strong learning impulses right from the start of learning. a in snipe weights are initialsnipe synapse initialized randomly the maximum ization is wanted. synapse of absolute weight value initialized at set in a neuralnetworkdescriptor using the method setsynapseinitialrange. a random can be the encoding problem and related problems weights should be initialized with small randomly chosen values the initialization of weights is not as trivial as one might think. if they are simply initialized with there will be no change in weights at all. if they are all initialized by the same value they will all change equally during training. the simple solution of this problem is called symmetry breaking which is the initialization of weights with small random values. the generally pattern recognition is understood as a special case of function approximation with a few discrete output possibilities. the encoding problem is a classic among the multilayer perceptron test training problems. in our mlp we have an input layer with eight neurons an output layer with eight neurons and one hidden layer with three neurons. thus this network represents a function now the training task is that an input of a value into the neuron ij should lead to an output of a value from the neuron j one neuron should be activated which results in training samples. during the analysis of the trained network we will see that the network with the random initial weights d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com figure as a reminder the illustration of the hyperbolic tangent and the fermi function the fermi function was expanded by a temperature parameter. the original fermi function is thereby represented by dark colors the temperature parameter of the modified fermi functions are ordered ascending by steepness and hidden neurons represents some kind of binary encoding and that the above mapping is possible training time epochs. thus our network is a machine in which the input is first encoded and afterwards decoded again. analogously we can train a encoding problem. but is it possible to improve the efficiency of this procedure? could there be for example a or an network? yes even that is possible since the network does not depend on binary encodings thus an network is sufficient for our problem. but the encoding of the network is far more difficult to understand on the next page and the training of the networks requires a lot more time. the static method snipe getencodersamplelesson in the class trainingsamplelesson allows for creating simple training sample lessons of arbitrary dimensionality for encoder problems like the above. an network however does not work since the possibility that the output of one neuron is compensated by another one is essential and if there is only one hidden neuron there is certainly no compensatory neuron. exercises exercise fig. on page shows a small network for the boolean functions and and or. write tables with all computational parameters of neural networks network input activation etc.. perform the calculations for the four possible inputs of the networks and write down the values of these variables for each input. do the same for the xor network on page d. kriesel a brief introduction to neural networks tangent function with temperature parameter dkriesel.com the encoding problem and related problems exercise list all boolean functions that are linearly separable and characterize them exactly. list those that are not linearly separable and characterize them exactly too. exercise a simple network shall be trained with one single pattern by means of backpropagation of error and verify if the error err errp converges and if so at what value. how does the error curve look like? let the pattern t be defined by p and t randomly initalize the weights in the interval exercise a one-stage perceptron with two input neurons bias neuron and binary threshold function as activation function divides the two-dimensional space into two regions by means of a straight line g. analytically calculate a set of weight values for such a perceptron so that the following set p of the patterns of the form t with is correctly classified. p figure illustration of the functionality of network encoding. the marked points represent the vectors of the inner neuron activation associated to the samples. as you can see it is possible to find inner activation formations so that each point can be separated from the rest of the points by a straight line. the illustration shows an exemplary separation of one point. d. kriesel a brief introduction to neural networks chapter the perceptron backpropagation and its variants dkriesel.com exercise calculate in a comprehensible way one vector w of all changes in weight by means of the backpropagation of error procedure with let a mlp with bias neuron be given and let the pattern be defined by p t for all weights with the target the initial value of the weights should be for all other weights the initial value should be what is conspicuous about the changes? d. kriesel a brief introduction to neural networks chapter radial basis functions rbf networks approximate functions by stretching and compressing gaussian bells and then summing them spatially shifted. description of their functions and their learning process. comparison with multilayer perceptrons. according to poggio and girosi radial basis function networks networks are a paradigm of neural networks which was developed considerably later than that of perceptrons. like perceptrons the rbf networks are built in layers. but in this case they have exactly three layers i.e. only one single layer of hidden neurons. like perceptrons the networks have a feedforward structure and their layers are completely linked. here the input layer again does not participate in information processing. the rbf networks are like mlps universal function approximators. despite all things in common what is the difference between rbf networks and perceptrons? the difference lies in the information processing itself and in the computational rules within the neurons outside of the input layer. so in a moment we will define a so far unknown type of neurons. components and structure of an rbf network initially we want to discuss colloquially and then define some concepts concerning rbf networks. output neurons in an rbf network the output neurons only contain the identity as activation function and one weighted sum as propagation function. thus they do little more than adding all input values and returning the sum. hidden neurons are also called rbf neurons well as the layer in which they are located is referred to as rbf layer. as propagation function each hidden neuron calculates a norm that represents the distance between the input to the network and the so-called position of the neuron this is inserted into a radial activation input is linear again position in the input space important! only sums up chapter radial basis functions dkriesel.com function which calculates and outputs the activation of the neuron. definition input neuron. definition and representation is identical to the definition on page of the input neuron. definition of an rbf neuron. the center ch of an rbf neuron h is the point in the input space where the rbf neuron is located in general the closer the input vector is to the center vector of an rbf neuron the higher is its activation. definition neuron. the socalled rbf neurons h have a propagation function fprop that determines the distance between the center ch of a neuron and the input vector y. this distance represents the network input. then the network input is sent through a radial basis function fact which returns the activation or the output of the neuron. rbf neurons are represented by the symbol wvut pqrs definition output neuron. rbf output neurons use the weighted sum as propagation function fprop and the identity as activation function fact. they are represented by the sym gau bolonml hijk definition network. an rbf network has exactly three layers in the following order the input layer consisting of input neurons the hidden layer called rbf layer consisting of rbf neurons and the output layer consisting of layers feedforward rbf output neurons. each layer is completely linked with the following one shortcuts do not exist on the next page it is a feedforward topology. the connections between input layer and rbf layer are unweighted i.e. they only transmit the input. the connections between rbf layer and output layer are weighted. the original definition of an rbf network only referred to an output neuron but in analogy to the perceptrons it is apparent that such a definition can be generalized. a bias neuron is not used in rbf networks. the set of input neurons shall be represented by i the set of hidden neurons by h o h and the set of output neurons by o. therefore the inner neurons are called radial basis neurons because from their definition follows directly that all input vectors with the same distance from the center of a neuron also produce the same output value on page information processing of an rbf network now the question is what can be realized by such a network and what is its purpose. let us go over the rbf network from top to bottom an rbf network receives the input by means of the unweighted connections. then the input vector is sent through a norm so that the result is a scalar. this scalar by the way can only be positive due to the norm is processed by a radial basis function for exam d. kriesel a brief introduction to neural networks dkriesel.com information processing of an rbf network gau gfed gfed shhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh vllllllllllllllllllll wvut pqrs wvut pqrs wvut pqrs wvut pqrs wvut pqrs thhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh vmmmmmmmmmmmmmmmmmmmm vmmmmmmmmmmmmmmmmmmmm onml hijk onml hijk onml hijk gau gau gau gau ii hh figure an exemplary rbf network with two input neurons five hidden neurons and three output neurons. the connections to the hidden neurons are not weighted they only transmit the input. right of the illustration you can find the names of the neurons which coincide with the names of the mlp neurons input neurons are called i hidden neurons are called h and output neurons are called the associated sets are referred to as i h and o. d. kriesel a brief introduction to neural networks v s v t v chapter radial basis functions dkriesel.com ging compressing and removing gaussian bells and subsequently accumulating them. here the parameters for the superposition of the gaussian bells are in the weights of the connections between the rbf layer and the output layer. furthermore the network architecture offers the possibility to freely define or train height and width of the gaussian bells due to which the network paradigm becomes even more versatile. we will get to know methods and approches for this later. input distance gaussian bell sum output figure let ch be the center of an rbf neuron h. then the activation function facth is radially symmetric around ch. information processing in rbf neurons ple by a gaussian bell on the next page the output values of the different neurons of the rbf layer or of the different gaussian bells are added within the third layer basically in relation to the whole input space gaussian bells are added here. suppose that we have a second a third and a fourth rbf neuron and therefore four differently located centers. each of these neurons now measures another distance from the input to its own center and de facto provides different values even if the gaussian bell is the same. since these values are finally simply accumulated in the output layer one can easily see that any surface can be shaped by drag rbf neurons process information by using norms and radial basis functions at first let us take as an example a simple rbf network. it is apparent that we will receive a one-dimensional output which can be represented as a function on the facing page. additionally the network includes the centers of the four inner neurons and therefore it has gaussian bells which are finally added within the output neuron the network also possesses four values which influence the width of the gaussian bells. on the contrary the height of the gaussian bell is influenced by the subsequent weights since the individual output values of the bells are multiplied by those weights. d. kriesel a brief introduction to neural networks dkriesel.com information processing of an rbf network figure two individual one- or two-dimensional gaussian bells. in both cases holds and the centers of the gaussian bells lie in the coordinate origin. the distance r to the center is simply calculated according to the pythagorean theorem r figure four different gaussian bells in one-dimensional space generated by means of rbf neurons are added by an output neuron of the rbf network. the gaussian bells have different heights widths and positions. their centers are located at the widths at you can see a two-dimensional example in fig. on the following page. d. kriesel a brief introduction to neural networks in in chapter radial basis functions dkriesel.com gau wvut pqrs gau gau wvut pqrs gau wvut pqrs wvut pqrs vmmmmmmmmmmmmmmmmmmm aaaaaaaaaa onml hijk neurons are added by an output neuron of the rbf network. once again r figure four different gaussian bells in two-dimensional space generated by means of rbf applies for the distance. the heights w widths and centers c y are d. kriesel a brief introduction to neural networks gaussian of the gaussians v dkriesel.com information processing of an rbf network since we use a norm to calculate the distance between the input vector and the center of a neuron h we have different choices often the euclidian norm is chosen to calculate the distance rh ch sx i i remember the input vector was referred to as x. here the index i runs through the input neurons and thereby through the input vector components and the neuron center components. as we can see the euclidean distance generates the squared differences of all vector components adds them and extracts the root of the sum. in two-dimensional space this corresponds to the pythagorean theorem. from the definition of a norm directly follows that the distance can only be positive. strictly speaking we hence only use the positive part of the activation function. by the way activation functions other than the gaussian bell are possible. normally functions that are monotonically decreasing over the interval are chosen. now that we know the distance rh between the input vector x and the center ch of the rbf neuron h this distance has to be passed through the activation function. here we use as already mentioned a gaussian bell h h factrh e activation function fact and hence the activation functions should not be referred to as fact simultaneously. one solution would be to number the activation functions like facth with h being the set of hidden neurons. but as a result the explanation would be very confusing. so i simply use the name fact for all activation functions and regard and c as variables that are defined for individual neurons but no directly included in the activation function. the reader will certainly notice that in the literature the gaussian bell is often normalized by a multiplicative factor. we can however avoid this factor because we are multiplying anyway with the subsequent weights and consecutive multiplications first by a normalization factor and then by the connections weights would only yield different factors there. we do not need this factor because for our purpose the integral of the gaussian bell must not always be and therefore simply leave it out. some analytical thoughts prior to the training the output y of an rbf output neuron results from combining the functions of an rbf neuron to wh fact ch y x h h it is obvious that both the center ch and the width h can be seen as part of the suppose that similar to the multilayer perceptron we have a set p that contains d. kriesel a brief introduction to neural networks chapter radial basis functions dkriesel.com training samples t. then we obtain functions of the form wh fact ch y x h h i.e. one function for each training sample. of course with this effort we are aiming at letting the output y for all training patterns p converge to the corresponding teaching input t. weights can simply be computed as solution of a system of equations thus we have equations. now let us assume that the widths k the centers ck and the training samples p including the teaching input t are given. we are looking for the weights wh with weights for one output neuron thus our problem can be seen as a system of equations since the only thing we want to change at the moment are the weights. this demands a distinction of cases concerning the number of training samples and the number of rbf neurons if the number of rbf neurons equals the number of patterns i.e. the equation can be reduced to a matrix multiplication simply calculate weights t m g t m t e g t g m g m m m where t is the vector of the teaching inputs for all training samples m is the matrix of the outputs of all rbf neurons to samples the matrix is squared and we can therefore attempt to invert it g is the vector of the desired weights and e is a unit matrix with the same size as g. mathematically speaking we can simply calculate the weights in the case of there is exactly one rbf neuron available per training sample. this means that the network exactly meets the existing nodes after having calculated the weights i.e. it performs a precise interpolation. to calculate such an equation we certainly do not need an rbf network and therefore we can proceed to the next case. exact interpolation must not be mistaken for the memorizing ability mentioned with the mlps first we are not talking about the training of rbf d. kriesel a brief introduction to neural networks dkriesel.com information processing of an rbf network networks at the moment. second it could be advantageous for us and might in fact be intended if the network exactly interpolates between the nodes. the system of equations is under-determined there are more rbf neurons than training samples certainly this case i.e. normally does not occur very often. in this case there is a huge variety of solutions which we do not need in such detail. we can select one set of weights out of many obviously possible ones. but most interesting for further discussion is the case if there are significantly more training samples than rbf neurons that means thus we again want to use the generalization capability of the neural network. if we have more training samples than rbf neurons we cannot assume that every training sample is exactly hit. so if we cannot exactly hit the points and therefore cannot just interpolate as in the aforementioned ideal case with we must try to find a function that approximates our training set p as closely as possible as with the mlp we try to reduce the sum of the squared error to a minimum. how do we continue the calculation in the case of as above to solve the system of equations we have to find the solution m of a matrix multiplication t m g. the problem is that this time we cannot invert the matrix m because it is not a square matrix is true. here we have to use the moore-penrose pseudo inverse m which is defined by m t m m t although the moore-penrose pseudo inverse is not the inverse of a matrix it can be used similarly in this we get equations that are very similar to those in the case of t m g m t m m g m t e g m t g another reason for the use of the moore-penrose pseudo inverse is the fact that it minimizes the squared error is our goal the estimate of the vector g in equation corresponds to the gauss-markov model known from statistics which is used to minimize the squared error. in the aforementioned equations and the following ones please do not mistake the t in m t the transpose of the matrix m for the t of the vector of all teaching inputs. m particularly m is true if m is invertible. i do not want to go into detail of the reasons for they these circumstances and applications of m can easily be found in literature for linear algebra. d. kriesel a brief introduction to neural networks chapter radial basis functions dkriesel.com the generalization on several outputs is trivial and not quite computationally expensive we have found a mathematically exact way to directly calculate the weights. what will happen if there are several output neurons i.e. with o being as usual the set of the output neurons in this case as we have already indicated it does not change much the additional output neurons have their own set of weights while we do not change the and c of the rbf layer. thus in an rbf network it is easy for given and c to realize a lot of output neurons since we only have to calculate the individual vector of weights g m t for every new output neuron whereas the matrix m which generally requires a lot of computational effort always stays the same so it is quite inexpensive at least concerning the computational complexity to add more output neurons. inexpensive output dimension computational effort and accuracy for realistic problems it normally applies that there are considerably more training samples than rbf neurons i.e. you can without any difficulty use training samples if you like. theoretically we could find the terms for the mathematically correct solution on the blackboard a very long time but such calculations often seem to be imprecise m complex and imprecise and very time-consuming inversions require a lot of computational effort. furthermore our moore-penrose pseudoinverse is in spite of numeric stability no guarantee that the output vector corresponds to the teaching vector because such extensive computations can be prone to many inaccuracies even though the calculation is mathematically correct our computers can only provide us with very good approximations of the pseudo-inverse matrices. this means that we also get only approximations of the correct weights with a lot of accumulated numerical errors and therefore only an approximation very rough or even unrecognizable of the desired output. if we have enough computing power to analytically determine a weight vector we should use it nevertheless only as an initial value for our learning process which leads us to the real training methods but otherwise it would be boring wouldn t it? combinations of equation system and gradient strategies are useful for training analogous to the mlp we perform a gradient descent to find the suitable weights by means of the already well known delta rule. here backpropagation is unnecessary since we only have to train one single retraining delta rule d. kriesel a brief introduction to neural networks dkriesel.com training of rbf networks weight layer which requires less computing time. we know that the delta rule is wh oh in which we now insert as follows wh y factp ch here again i explicitly want to mention that it is very popular to divide the training into two phases by analytically computing a set of weights and then refining it by training with the delta rule. there is still the question whether to learn o ine or online. here the answer is similar to the answer for the multilayer perceptron initially one often trains online movement across the error surface. then after having approximated the solution the errors are once again accumulated and for a more precise approximation one trains o ine in a third learning phase. however similar to the mlps you can be successful by using many methods. as already indicated in an rbf network not only the weights between the hidden and the output layer can be optimized. so let us now take a look at the possibility to vary and c. training in phases it is not always trivial to determine centers and widths of rbf neurons it is obvious that the approximation accuracy of rbf networks can be increased by adapting the widths and positions of the gaussian bells in the input space to the problem that needs to be approximated. there are several methods to deal with the centers c and the widths of the gaussian bells fixed selection the centers and widths can be selected in a fixed manner and regardless of the training samples this is what we have assumed until now. conditional fixed selection again centers and widths are selected fixedly but we have previous knowledge about the functions to be approximated and comply with it. vary and c adaptive to the learning process this too. a realization of is definitely the most elegant variant but certainly the most challenging one this approach will not be discussed in this chapter but it can be found in connection with another network topology fixed selection in any case the goal is to cover the input space as evenly as possible. here widths of of the distance between the d. kriesel a brief introduction to neural networks chapter radial basis functions dkriesel.com responsible for the fact that six- to tendimensional problems in rbf networks are already called mlp for example does not cause any problems here. conditional fixed selection suppose that our training samples are not evenly distributed across the input space. it then seems obvious to arrange the centers and sigmas of the rbf neurons by means of the pattern distribution. so the training patterns can be analyzed by statistical techniques such as a cluster analysis and so it can be determined whether there are statistical factors according to which we should distribute the centers and sigmas on the facing page. a more trivial alternative would be to set centers on positions randomly selected from the set of patterns. so this method would allow for every training pattern p to be directly in the center of a neuron on the next page. this is not yet very elegant but a good solution when time is an issue. generally for this method the widths are fixedly selected. if we have reason to believe that the set of training samples is clustered we can use clustering methods to determine them. there are different methods to determine clusters in an arbitrarily dimensional set of points. we will be introduced to some of them in excursus a. one neural clustering method are the so-called rolfs and self-organizing maps are figure example for an even coverage of a two-dimensional input space by applying radial basis functions. centers can be selected so that the gaussian bells overlap by approx. the closer the bells are set the more precise but the more time-consuming the whole thing becomes. this may seem to be very inelegant but in the field of function approximation we cannot avoid even coverage. here it is useless if the function to be approximated is precisely represented at some positions but at other positions the return value is only however the high input dimension requires a great many rbf neurons which increases the computational effort exponentially with the dimension and is it is apparent that a gaussian bell is mathematically infinitely wide therefore i ask the reader to apologize this sloppy formulation. input dimension very expensive d. kriesel a brief introduction to neural networks dkriesel.com training of rbf networks figure example of an uneven coverage of a two-dimensional input space of which we have previous knowledge by applying radial basis functions. also useful in connection with determining the position of rbf neurons using rolfs one can also receive indicators for useful radii of the rbf neurons. learning vector quantisation has also provided good results. all these methods have nothing to do with the rbf networks themselves but are only used to generate some previous knowledge. therefore we will not discuss them in this chapter but independently in the indicated chapters. another approach is to use the approved methods we could slightly move the positions of the centers and observe how our error function err is changing a gradient descent as already known from the mlps. figure example of an uneven coverage of a two-dimensional input space by applying radial basis functions. the widths were fixedly selected the centers of the neurons were randomly distributed throughout the training patterns. this distribution can certainly lead to slightly unrepresentative results which can be seen at the single data point down to the left. d. kriesel a brief introduction to neural networks chapter radial basis functions dkriesel.com in a similar manner we could look how the error depends on the values analogous to the derivation of backpropagation we derive in the following text only simple mechanisms are sketched. for more information i refer to err hch h and err hch ch neurons are added to places with large error values since the derivation of these terms corresponds to the derivation of backpropagation we do not want to discuss it here. but experience shows that no convincing results are obtained by regarding how the error behaves depending on the centers and sigmas. even if mathematics claim that such methods are promising the gradient descent as we already know leads to problems with very craggy error surfaces. and that is the crucial point naturally rbf networks generate very craggy error surfaces because if we considerably change a c or a we will significantly change the appearance of the error function. growing rbf networks automatically adjust the neuron density in growing rbf networks the number of rbf neurons is not constant. a certain number of neurons as well as their centers ch and widths h are previously selected by means of a clustering method and then extended or reduced. after generating this initial configuration the vector of the weights g is analytically calculated. then all specific errors errp concerning the set p of the training samples are calculated and the maximum specific error max p is sought. the extension of the network is simple we replace this maximum error with a new replace rbf neuron. of course we have to exererror with neuron cise care in doing this if the are small the neurons will only influence each other if the distance between them is short. but if the are large the already exisiting neurons are considerably influenced by the new neuron because of the overlapping of the gaussian bells. so it is obvious that we will adjust the already existing rbf neurons when adding the new neuron. to put it simply this adjustment is made by moving the centers c of the other neurons away from the new neuron and reducing their width a bit. then the current output vector y of the network is compared to the teaching input t and the weight vector g is improved by means of training. subsequently a new neuron can be inserted if necessary. this method is d. kriesel a brief introduction to neural networks dkriesel.com comparing rbf networks and multilayer perceptrons particularly suited for function approximations. two paradigms and look at their advantages and disadvantages. limiting the number of neurons here it is mandatory to see that the network will not grow ad infinitum which can happen very fast. thus it is very useful to previously define a maximum number for neurons less important neurons are deleted which leads to the question whether it is possible to continue learning when this limit is reached. the answer is this would not stop learning. we only have to look for the unimportant neuron and delete it. a neuron is for example unimportant for the network if there is another neuron that has a similar function it often occurs that two gaussian bells exactly overlap and at such a position for instance one single neuron with a higher gaussian bell would be appropriate. but to develop automated procedures in order to find less relevant neurons is highly problem dependent and we want to leave this to the programmer. with rbf networks and multilayer perceptrons we have already become acquainted with and extensivley discussed two network paradigms for similar problems. therefore we want to compare these delete unimportant neurons comparing rbf networks and multilayer perceptrons networks we will compare multilayer perceptrons and rbf networks with respect to different aspects. input dimension we must be careful highwith rbf dimensional functional spaces since the network could very quickly require huge memory storage and computational a multilayer perceptron would cause less problems because its number of neuons does not grow exponentially with the input dimension. in effort. here center selection however selecting the centers c for rbf networks is the introduced approaches still a major problem. please use any previous knowledge you have when applying them. such problems do not occur with the mlp. output dimension the advantage of rbf networks is that the training is not much influenced when the output dimension of the network is high. for an mlp a learning procedure such as backpropagation thereby will be very time-consuming. extrapolation advantage as well as disadvantage of rbf networks is the lack d. kriesel a brief introduction to neural networks chapter radial basis functions dkriesel.com exercises exercise an rbf network with fixed widths and centers of the neurons should approximate a target function u. for this training samples of the form t of the function u are given. let be true. the weights should be analytically determined by means of the moore-penrose pseudo inverse. indicate the running time behavior regarding and as precisely as possible. note there are methods for matrix multiplications and matrix inversions that are more efficient than the canonical methods. for better estimations i recommend to look for such methods their complexity. in addition to your complexity calculations please indicate the used methods together with their complexity. important! of extrapolation capability an rbf network returns the result far away from the centers of the rbf layer. on the one hand it does not extrapolate unlike the mlp it cannot be used for extrapolation we could never know if the extrapolated values of the mlp are reasonable but experience shows that mlps are suitable for that matter. on the other hand unlike the mlp the network is capable to use this to tell us don t know which could be an advantage. lesion tolerance for the output of an mlp it is no so important if a weight it will only or a neuron is missing. worsen a little in total. if a weight or a neuron is missing in an rbf network then large parts of the output remain practically uninfluenced. but one part of the output is heavily affected because a gaussian bell is directly missing. thus we can choose between a strong local error for lesion and a weak but global error. spread here the mlp is since rbf networks are used considerably less often which is not always understood by professionals least as far as low-dimensional input spaces are concerned. the mlps seem to have a considerably longer tradition and they are working too good to take the effort to read some pages of this work about rbf networks d. kriesel a brief introduction to neural networks chapter recurrent perceptron-like networks some thoughts about networks with internal states. generally recurrent networks are networks that are capable of influencing themselves by means of recurrences e.g. by including the network output in the following computation steps. there are many types of recurrent networks of nearly arbitrary form and nearly all of them are referred to as recurrent neural networks. as a result for the few paradigms introduced here i use the name recurrent multilayer perceptrons. apparently such a recurrent network is capable to compute more than the ordinary mlp if the recurrent weights are set to the recurrent network will be reduced to an ordinary mlp. additionally the recurrence generates different network-internal states so that different inputs can produce different outputs in the context of the network state. recurrent networks in themselves have a great dynamic that is mathematically difficult to conceive and has to be discussed extensively. the aim of this chapter is only to briefly discuss how recurrences can be structured and how network-internal states can be generated. thus i will briefly introduce two paradigms of recurrent networks and afterwards roughly outline their training. with a recurrent network an input x that is constant over time may lead to different results on the one hand the network could converge i.e. it could transform itself into a fixed state and at some time return a fixed output value y. on the other hand it could never converge or at least not until a long time later so that it can no longer be recognized and as a consequence y constantly changes. if the network does not converge it is for example possible to check if periodicals or attractors on the following page are returned. here we can expect the complete variety of dynamical systems. that is the reason why i particularly want to refer to the literature concerning dynamical systems. state dynamics more capable than mlp chapter recurrent perceptron-like networks on chapter dkriesel.com further discussions could reveal what will happen if the input of recurrent networks is changed. in this chapter the related paradigms of recurrent networks according to jordan and elman will be introduced. jordan networks a jordan network is a multilayer perceptron with a set k of so-called context neurons kk. there is one context neuron per output neuron on the next page. in principle a context neuron just memorizes an output until it can be processed in the next time step. therefore there are weighted connections between each output neuron and one context neuron. the stored values are returned to the actual network by means of complete links between the context neurons and the input layer. in the originial definition of a jordan network the context neurons are also recurrent to themselves via a connecting weight but most applications omit this recurrence since the jordan network is already very dynamic and difficult to analyze even without these additional recurrences. definition neuron. a context neuron k receives the output value of another neuron i at a time t and then reenters it into the network at a time definition network. a jordan network is a multilayer perceptron output neurons are buffered figure the roessler attractor d. kriesel a brief introduction to neural networks dkriesel.com elman networks gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii aaaaaaaaa aaaaaaaaa gfed gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii aaaaaaaaa aaaaaaaaa gfed gfed gfed gfed bc figure illustration of a jordan network. the network output is buffered in the context neurons and with the next time step it is entered into the network together with the new input. with one context neuron per output neuron. the set of context neurons is called k. the context neurons are completely linked toward the input layer of the network. elman networks the elman networks variation of the jordan networks have context neurons too but one layer of context neurons per information processing neuron layer on the following page. thus the outputs of each hidden neuron or output neuron are led into the associated context layer exactly one context neuron per neuron and from there it is reentered into the complete neuron layer during the next time step again a complete link on the way back. so the complete information processing of the mlp exists a second time as a version which once again considerably increases dynamics and state variety. compared with jordan networks the elman networks often have the advantage to act more purposeful since every layer can access its own context. definition network. an elman network is an mlp with one context neuron per information processing neuron. the set of context neurons is called k. this means that there exists one context layer per information processing remember the input layer does not process in formation. d. kriesel a brief introduction to neural networks nearly everything is buffered t x x v v t o o o o chapter recurrent perceptron-like networks on chapter dkriesel.com gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii gfed gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii gfed gfed onml onml onml hijkk onml onml hijkk figure illustration of an elman network. the entire information processing part of the network exists in a way twice. the output of each neuron for the output of the input neurons is buffered and reentered into the associated layer. for the reason of clarity i named the context neurons on the basis of their models in the actual network but it is not mandatory to do so. neuron layer with exactly the same number of context neurons. every neuron has a weighted connection to exactly one context neuron while the context layer is completely linked towards its original layer. now it is interesting to take a look at the training of recurrent networks since for instance ordinary backpropagation of error cannot work on recurrent networks. once again the style of the following part is rather informal which means that i will not use any formal definitions. training recurrent networks in order to explain the training as comprehensible as possible we have to agree on some simplifications that do not affect the learning principle itself. so for the training let us assume that in the beginning the context neurons are initiated with an input since otherwise they would have an undefined input is no simplification but reality. furthermore we use a jordan network without a hidden neuron layer for our training attempts so that the output neu d. kriesel a brief introduction to neural networks t t u u z z v v w w u u t t v v u u t t u u w w u u v v attach the same network to each context layer dkriesel.com training recurrent networks rons can directly provide input. this approach is a strong simplification because generally more complicated networks are used. but this does not change the learning principle. unfolding in time remember our actual learning procedure for mlps the backpropagation of error which backpropagates the delta values. so in case of recurrent networks the delta values would backpropagate cyclically through the network again and again which makes the training more difficult. on the one hand we cannot know which of the many generated delta values for a weight should be selected for training i.e. which values are useful. on the other hand we cannot definitely know when learning should be stopped. the advantage of recurrent networks are great state dynamics within the network the disadvantage of recurrent networks is that these dynamics are also granted to the training and therefore make it difficult. one learning approach would be the attempt to unfold the temporal states of the network on the next page recursions are deleted by putting a similar network above the context neurons i.e. the context neurons are as a manner of speaking the output neurons of the attached network. more generally spoken we have to backtrack the recurrences and place earlier instances of neurons in the network thus creating a larger but forward-oriented network without recurrences. this enables training a recurrent network with any training strategy developed for non-recurrent ones. here the input is entered as teaching input into every of the input neurons. this can be done for a discrete number of time steps. these training paradigms are called unfolding in time after the unfolding a training by means of backpropagation of error is possible. but obviously for one weight wij several changing values wij are received which can be treated differently accumulation averaging etc. a simple accumulation could possibly result in enormous changes per weight if all changes have the same sign. hence also the average is not to be underestimated. we could also introduce a discounting factor which weakens the influence of wij of the past. unfolding in time is particularly useful if we receive the impression that the closer past is more important for the network than the one being further away. the reason for this is that backpropagation has only little influence in the layers farther away from the output the farther we are from the output layer the smaller the influence of backpropagation. disadvantages the training of such an unfolded network will take a long time since a large number of layers could possibly be produced. a problem that is no longer negligible is the limited computational accuracy of ordinary computers which is exhausted very fast because of so many d. kriesel a brief introduction to neural networks chapter recurrent perceptron-like networks on chapter dkriesel.com gfed gfed gfed gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii wnnnnnnnnnnnnnnnnn wnnnnnnnnnnnnnnnnn aaaaaaaaa gfed gfed bc tjjjjjjjjjjjjjjjjjjjjj woooooooooooooo woooooooooooooo tjjjjjjjjjjjjjjjjjjjjjjj vnnnnnnnnnnnnnnnn wppppppppppppppp gfed gfed gfed gfed gfed tiiiiiiiiiiiiiiiiiiiiiiiiii wnnnnnnnnnnnnnnnnn wnnnnnnnnnnnnnnnnn aaaaaaaaa gfed gfed figure illustration of the unfolding in time with a small exemplary recurrent mlp. top the recurrent mlp. bottom the unfolded network. for reasons of clarity i only added names to the lowest part of the unfolded network. dotted arrows leading into the network mark the inputs. dotted arrows leading out of the network mark the outputs. each copy represents a time step of the network with the most recent time step being at the bottom. d. kriesel a brief introduction to neural networks w t w o o o o w t w v t w w t w dkriesel.com training recurrent networks are chosen suitably so for example neurons and weights can be adjusted and the network topology can be optimized course the result of learning is not necessarily a jordan or elman network. with ordinary mlps however evolutionary strategies are less popular since they certainly need a lot more time than a directed learning procedure such as backpropagation. teaching input applied at context neurons nested computations farther we are from the output layer the smaller the influence of backpropagation so that this limit is reached. furthermore with several levels of context neurons this procedure could produce very large networks to be trained. teacher forcing the other procedures are equivalent teacher forcing and open loop learning. they detach the recurrence during the learning process we simply pretend that the recurrence does not exist and apply the teaching input to the context neurons during the training. so backpropagation becomes possible too. disadvantage with elman networks a teaching input for non-output-neurons is not given. recurrent backpropagation another popular procedure without limited time horizon is the recurrent backpropagation using methods of differential calculus to solve the problem training with evolution due to the already long lasting training time evolutionary algorithms have proved to be of value especially with recurrent networks. one reason for this is that they are not only unrestricted with respect to recurrences but they also have other advantages when the mutation mechanisms d. kriesel a brief introduction to neural networks chapter hopfield networks in a magnetic field each particle applies a force to any other particle so that all particles adjust their movements in the energetically most favorable way. this natural mechanism is copied to adjust noisy inputs in order to match their real models. another supervised learning example of the wide range of neural networks was developed by john hopfield the socalled hopfield networks hopfield and his physically motivated networks have contributed a lot to the renaissance of neural networks. hopfield networks are inspired by particles in a magnetic field the idea for the hopfield networks originated from the behavior of particles in a magnetic field every particle means of magnetic forces with every other particle linked with each particle trying to reach an energetically favorable state a minimum of the energy function. as for the neurons this state is known as activation. thus all particles or neurons rotate and thereby encourage each other to continue this rotation. as a manner of speaking our neural network is a cloud of particles based on the fact that the particles automatically detect the minima of the energy function hopfield had the idea to use the of the particles to process information why not letting the particles search minima on arbitrary functions? even if we only use two of those spins i.e. a binary activation we will recognize that the developed hopfield network shows considerable dynamics. in a hopfield network all neurons influence each other symmetrically briefly speaking a hopfield network consists of a set k of completely linked neu- rons with binary activation we only chapter hopfield networks i ukkkkkkkkkkkkkkkkkkkkkkkk i ukkkkkkkkkkkkkkkkkkkkkkkk figure illustration of an exemplary hopfield network. the arrows and mark the binary due to the completely linked neurons the layers cannot be separated which means that a hopfield network simply includes a set of neurons. dkriesel.com definition network. a hopfield network consists of a set k of completely linked neurons without direct recurrences. the activation function of the neurons is the binary threshold function with outputs definition of a hopfield network. the state of the network consists of the activation states of all neurons. thus the state of the network can be understood as a binary string z input and output of a hopfield network are represented by neuron states use two spins with the weights being symmetric between the individual neurons and without any neuron being directly connected to itself thus the state of neurons with two possible states can be described by a string x the complete link provides a full square matrix of weights between the neurons. the meaning of the weights will be discussed in the following. furthermore we will soon recognize according to which rules the neurons are spinning i.e. are changing their state. additionally the complete link leads to the fact that we do not know any input output or hidden neurons. thus we have to think about how we can input something into the neurons. we have learned that a network i.e. a set of particles that is in a state is automatically looking for a minimum. an input pattern of a hopfield network is exactly such a state a binary string x that initializes the neurons. then the network is looking for the minimum to be taken we have previously defined by the input of training samples on its energy surface. but when do we know that the minimum has been found? this is simple too when the network stops. it can be proven that a hopfield network with a symmetric weight matrix that has zeros on its diagonal always converges i.e. at some point it will stand still. then the output is a binary string y namely the state string of the network that has found a minimum. d. kriesel a brief introduction to neural networks input and output network states always converges completely linked set of neurons i i i o o o o u o o i o j j u o o o o dkriesel.com structure and functionality now let us take a closer look at the contents of the weight matrix and the rules for the state change of the neurons. definition and output of a hopfield network. the input of a hopfield network is binary string x that initializes the state of the network. after the convergence of the network the output is the binary string y generated from the new network state. significance of weights i.e. we have already said that the neurons change their states their direction from to or vice versa. these spins occur dependent on the current states of the other neurons and the associated weights. thus the weights are capable to control the complete change of the network. the weights can be positive negative or colloquially speaking for a weight wij between two neurons i and j the following holds if wij is positive it will try to force the two neurons to become equal the larger they are the harder the network will try. if the neuron i is in state and the neuron j is in state a high positive weight will advise the two neurons that it is energetically more favorable to be equal. if wij is negative its behavior will be analoguous only that i and j are urged to be different. a neuron i in state would try to urge a neuron j into state x j k zero weights lead to the two involved neurons not influencing each other. the weights as a whole apparently take the way from the current state of the network towards the next minimum of the energy function. we now want to discuss how the neurons follow this way. a neuron changes its state according to the influence of the other neurons once a network has been trained and initialized with some starting state the change of state xk of the individual neurons k occurs according to the scheme xkt fact wjk xjt in each time step where the function fact generally is the binary threshold function on the next page with threshold colloquially speaking a neuron k calculates the sum of wjk xjt which indicates how strong and into which direction the neuron k is forced by the other neurons j. thus the new state of the network t results from the state of the network at the previous time t this sum is the direction into which the neuron k is pushed. depending on the sign of the sum the neuron takes state or another difference between hopfield networks and other already known network topologies is the asynchronous update a neuron k is randomly chosen every time which then recalculates the activation. d. kriesel a brief introduction to neural networks chapter hopfield networks dkriesel.com a minimum then there is the question of how to teach the weights to force the network towards a certain minimum. the weight matrix is generated directly out of the training patterns figure illustration of the binary threshold function. thus the new activation of the previously changed neurons immediately influences the network i.e. one time step indicates the change of a single neuron. regardless of the aforementioned random selection of the neuron a hopfield network is often much easier to implement the neurons are simply processed one after the other and their activations are recalculated until no more changes occur. definition in the state of a hopfield network. the change of state of the neurons occurs asynchronously with the neuron to be updated being randomly chosen and the new state being generated by means of this rule xkt fact wjk xjt x j j the aim is to generate minima on the mentioned energy surface so that at an input the network can converge to them. as with many other network paradigms we use a set p of training patterns p representing the minima of our energy surface. unlike many other network paradigms we do not look for the minima of an unknown error function but define minima on such a function. the purpose is that the network shall automatically take the closest minimum when the input is presented. for now this seems unusual but we will understand the whole purpose later. roughly speaking the training of a hopfield network is done by training each training pattern exactly once using the rule described in the following shot learning where pi and pj are the states of the neurons i and j under p p wij x p p pi pj random neuron calculates new activation now that we know how the weights influence the changes in the states of the neurons and force the entire network towards this results in the weight matrix w. colloquially speaking we initialize the network by means of a training pattern and then process weights wij one after another. d. kriesel a brief introduction to neural networks function dkriesel.com autoassociation and traditional application for each of these weights we verify are the neurons i j n the same state or do the states vary? in the first case we add to the weight in the second case we add this we repeat for each training pattern p p. finally the values of the weights wij are high when i and j corresponded with many training patterns. colloquially speaking this high value tells the neurons it is energetically favorable to hold the same state. the same applies to negative weights. due to this training we can store a certain fixed number of patterns p in the weight matrix. at an input x the network will converge to the stored pattern that is closest to the input p. unfortunately the number of the maximum storable and reconstructible patterns p is limited to which in turn only applies to orthogonal patterns. this was shown by precise time-consuming mathematical analyses which we do not want to specify now. if more patterns are entered already stored information will be destroyed. definition rule for hopfield networks. the individual elements of the weight matrix w are defined by a single processing of the learning rule wij x p p pi pj where the diagonal of the matrix is covered with zeros. here no more than training samples can be trained and at the same time maintain their function. now we know the functionality of hopfield networks but nothing about their practical use. autoassociation and traditional application hopfield networks like those mentioned above are called autoassociators. an autoassociator a exactly shows the afore- mentioned behavior firstly when a known pattern p is entered exactly this known pattern is returned. thus ap p with a being the associative mapping. secondly and that is the practical use this also works with inputs that are close to a pattern ap p. afterwards the autoassociator is in any case in a stable state namely in the state p. if the set of patterns p consists of for example letters or other characters in the form of pixels the network will be able to correctly recognize deformed or noisy letters with high probability on the following page. the primary fields of application of hopfield networks are pattern recognition and pattern completion such as the zip network restores damaged inputs d. kriesel a brief introduction to neural networks chapter hopfield networks dkriesel.com code recognition on letters in the eighties. but soon the hopfield networks were replaced by other systems in most of their fields of application for example by ocr systems in the field of letter recognition. today hopfield networks are virtually no longer used they have not become established in practice. heteroassociation and analogies to neural data storage so far we have been introduced to hopfield networks that converge from an arbitrary input into the closest minimum of a static energy surface. another variant is a dynamic energy surface here the appearance of the energy surface depends on the current state and we receive a heteroassociator instead of an autoassociator. for a heteroassociator ap p is no longer true but rather hp q which means that a pattern is mapped onto another one. h is the heteroasso- ciative mapping. such heteroassociations are achieved by means of an asymmetric weight matrix v figure illustration of the convergence of an exemplary hopfield network. each of the pictures has binary pixels. in the hopfield network each pixel corresponds to one neuron. the upper illustration shows the training samples the lower shows the convergence of a heavily noisy to the corresponding training sample. d. kriesel a brief introduction to neural networks dkriesel.com heteroassociation and analogies to neural data storage heteroassociations connected in series of the form hp q hq r hr s hz p can provoke a fast cycle of states p q r s z p whereby a single pattern is never completely accepted before a pattern is entirely completed the heteroassociation already tries to generate the successor of this pattern. additionally the network would never stop since after having reached the last state z it would proceed to the first state p again. generating the heteroassociative matrix v we generate the matrix v by means of elements v very similar to the autoassociative matrix with p being transition the training sample before the transition and q being the training sample to be generated from p vij x pq piqj netword is instable while changing states the diagonal of the matrix is again filled with zeros. the neuron states are as always adapted during operation. several transitions can be introduced into the matrix by a simple addition whereby the said limitation exists here too. definition rule for the heteroassociative matrix. for two training samples p being predecessor and q being successor of a heteroassociative transition the weights of the heteroassociative matrix v result from the learning rule vij x piqj pq with several heteroassociations being introduced into the network by a simple addition. stabilizing the heteroassociations we have already mentioned the problem that the patterns are not completely generated but that the next pattern is already beginning before the generation of the previous pattern is finished. this problem can be avoided by not only influencing the network by means of the heteroassociative matrix v but also by the already known autoassociative matrix w. additionally the neuron adaptation rule is changed so that competing terms are generated one term autoassociating an existing pattern and one term trying to convert the very same pattern into its successor. the associative rule provokes that the network stabilizes a pattern remains d. kriesel a brief introduction to neural networks chapter hopfield networks dkriesel.com there for a while goes on to the next pattern and so on. xit x fact wijxjt j k autoassociation k k vikxkt t heteroassociation stable change in states here the value t causes descriptively speaking the influence of the matrix v to be delayed since it only refers to a network being t versions behind. the result is a change in state during which the individual states are stable for a short while. if t is set to for example twenty steps then the asymmetric weight matrix will realize any change in the network only twenty steps later so that it initially works with the autoassociative matrix it still perceives the predecessor pattern of the current one and only after that it will work against it. biological motivation of heterassociation from a biological point of view the transition of stable states into other stable states is highly motivated at least in the beginning of the nineties it was assumed that the hopfield modell will achieve an approximation of the state dynamics in the brain which realizes much by means of state chains when i would ask you dear reader to recite the alphabet you generally will manage this better than try it immediately to answer the following question which letter in the alphabet follows the letter p another example is the phenomenon that one cannot remember a situation but the place at which one memorized it the last time is perfectly known. if one returns to this place the forgotten situation often comes back to mind. continuous hopfield networks so far we only have discussed hopfield networks with binary activations. but hopfield also described a version of his networks with continuous activations which we want to cover at least briefly continuous hopfield networks. here the activation is no longer calculated by the binary threshold function but by the fermi function with temperature parameters on the next page. here the network is stable for symmetric weight matrices with zeros on the diagonal too. hopfield also stated that continuous hopfield networks can be applied to find acceptable solutions for the np-hard travelling salesman problem according to some verification trials this statement can t be kept up any more. but today there are faster algorithms for handling this problem and therefore the hopfield network is no longer used here. d. kriesel a brief introduction to neural networks dkriesel.com continuous hopfield networks figure the already known fermi function with different temperature parameter variations. exercises exercise indicate the storage requirements for a hopfield network with neurons when the weights wij shall be stored as integers. is it possible to limit the value range of the weights in order to save storage space? exercise compute the weights wij for a hopfield network using the training set p d. kriesel a brief introduction to neural networks function with temperature parameter chapter learning vector quantization learning vector quantization is a learning procedure with the aim to represent the vector training sets divided into predefined classes as well as possible by using a few representative vectors. if this has been managed vectors which were unkown until then could easily be assigned to one of these classes. slowly part ii of this text is nearing its end and therefore i want to write a last chapter for this part that will be a smooth transition into the next one a chapter about the learning vector quantization lvq described by teuvo kohonen which can be characterized as being related to the self organizing feature maps. these soms are described in the next chapter that already belongs to part iii of this text since soms learn unsupervised. thus after the exploration of lvq i want to bid farewell to supervised learning. previously i want to announce that there are different variations of lvq which will be mentioned but not exactly represented. the goal of this chapter is rather to analyze the underlying principle. about quantization in order to explore the learning vector quantization we should at first get a clearer picture of what quantization can also be referred to as discretization is. everybody knows the sequence of discrete numbers n which contains the natural numbers. discrete means that this sequence consists of separated elements that are not interconnected. the elements of our example are exactly such numbers because the natural numbers do not include for example numbers between and on the other hand the sequence of real numbers r for instance is continuous it does not matter how close two selected numbers are there will always be a number between them. discrete separated chapter learning vector quantization dkriesel.com quantization means that a continuous space is divided into discrete sections by deleting for example all decimal places of the real number it could be assigned to the natural number here it is obvious that any other number having a in front of the comma would also be assigned to the natural number i.e. would be some kind of representative for all real numbers within the interval it must be noted that a sequence can be irregularly quantized too for instance the timeline for a week could be quantized into working days and weekend. a special case of quantization is digitization in case of digitization we always talk about regular quantization of a continuous space into a number system with respect to a certain basis. if we enter for example some numbers into the computer these numbers will be digitized into the binary system definition separation of a continuous space into discrete sections. definition regular quantization. lvq divides the input space into separate areas now it is almost possible to describe by means of its name what lvq should enable us to do a set of representatives should be used to divide an input space input space reduced to vector representatives into classes that reflect the input space as well as possible on the facing page. thus each element of the input space should be assigned to a vector as a representative i.e. to a class where the set of these representatives should represent the entire input space as precisely as possible. such a vector is called codebook vector. a codebook vector is the representative of exactly those input space vectors lying closest to it which divides the input space into the said discrete areas. it is to be emphasized that we have to know in advance how many classes we have and which training sample belongs to which class. furthermore it is important that the classes must not be disjoint which means they may overlap. such separation of data into classes is interesting for many problems for which it is useful to explore only some characteristic representatives instead of the possibly huge set of all vectors be it because it is less time-consuming or because it is sufficiently precise. using codebook vectors the nearest one is the winner the use of a prepared set of codebook vectors is very simple for an input vector y the class association is easily decided by considering which codebook vector is the closest so the codebook vectors build a voronoi diagram out of the set. since closest vector wins d. kriesel a brief introduction to neural networks dkriesel.com adjusting codebook vectors figure bexamples for quantization of a two-dimensional input space. dthe lines represent the class limit the mark the codebook vectors. each codebook vector can clearly be associated to a class each input vector is associated to a class too. are used to cause a previously defined number of randomly initialized codebook vectors to reflect the training data as precisely as possible. adjusting codebook vectors as we have already indicated the lvq is a supervised learning procedure. thus we have a teaching input that tells the learning procedure whether the classification of the input pattern is right or wrong in other words we have to know in advance the number of classes to be represented or the number of codebook vectors. roughly speaking it is the aim of the learning procedure that training samples the procedure of learning learning works according to a simple scheme. we have learning is supervised a set p of training samples. additionally we already know that classes are predefined too i.e. we also have a set of classes c. a codebook vector is clearly assigned to each class. thus we can say that the set of classes contains many codebook vectors cc. this leads to the structure of the training samples they are of the form c and d. kriesel a brief introduction to neural networks chapter learning vector quantization dkriesel.com therefore contain the training input vector p and its class affiliation c. for the class affiliation c holds which means that it clearly assigns the training sample to a class or a codebook vector. intuitively we could say about learning a learning procedure? we calculate the average of all class members and place their codebook vectors there and that s it. but we will see soon that our learning procedure can do a lot more. i only want to briefly discuss the steps of the fundamental lvq learning procedure initialization we place our set of codebook vectors on random positions in the input space. training sample a training sample p of our training set p is selected and presented. distance measurement we measure the distance c between all codebook vectors cc and our input p. winner the wins i.e. the one with closest codebook vector ci. min ci c learning process the learning process takes place according to the rule ci hp ci ci cit cit ci which we now want to break down. we have already seen that the first factor is a time-dependent learning rate allowing us to differentiate between large learning steps and fine tuning. the last factor ci is obviously the direction toward which the codebook vector is moved. but the function hp ci is the core of the rule it implements a distinction of cases. assignment is correct the winner vector is the codebook vector of the class that includes p. in this case the function provides positive values and the codebook vector moves towards p. assignment is wrong the winner vector does not represent the class that includes p. therefore it moves away from p. we can see that our definition of the function h was not precise enough. with good reason from here on the lvq is divided into different nuances dependent of how exactly h and the learning rate should be defined important! d. kriesel a brief introduction to neural networks dkriesel.com connection to neural networks olvq etc. the differences are for instance in the strength of the codebook vector movements. they are not all based on the same principle described here and as announced i don t want to discuss them any further. therefore i don t give any formal definition regarding the aforementioned learning rule and lvq. exercises exercise indicate a quantization which equally distributes all vectors h h in the five-dimensional unit cube h into one of classes. vectors neurons? connection to neural networks until now in spite of the learning process the question was what lvq has to do with neural networks. the codebook vectors can be understood as neurons with a fixed position within the input space similar to rbf networks. additionally in nature it often occurs that in a group one neuron may fire winner neuron here a codebook vector and in return inhibits all other neurons. i decided to place this brief chapter about learning vector quantization here so that this approach can be continued in the following chapter about self-organizing maps we will classify further inputs by means of neurons distributed throughout the input space only that this time we do not know which input belongs to which class. now let us take a look at the unsupervised learning networks! d. kriesel a brief introduction to neural networks part iii unsupervised learning network paradigms chapter self-organizing feature maps a paradigm of unsupervised learning neural networks which maps an input space by its fixed topology and thus independently looks for simililarities. function learning procedure variations and neural gas. no output but active neuron if you take a look at the concepts of biological neural networks mentioned in the introduction one question will arise how does our brain store and recall the impressions it receives every day. let me point out that the brain does not have any training samples and therefore no output. and while already considering this subject we realize that there is no output in this sense at all too. our brain responds to external input by changes in state. these are so to speak its output. how are data stored in the brain? based on this principle and exploring the question of how biological neural networks organize themselves teuvo kohonen developed in the eighties his selforganizing feature maps shortly referred to as self-organizing maps or soms. a paradigm of neural networks where the output is the state of the network which learns completely unsupervised i.e. without a teacher. unlike the other network paradigms we have already got to know for soms it is unnecessary to ask what the neurons calculate. we only ask which neuron is active at the moment. biologically this is very motivated if in biology the neurons are connected to certain muscles it will be less interesting to know how strong a certain muscle is contracted but which muscle is activated. in other words we are not interested in the exact output of the neuron but in knowing which neuron provides output. thus soms are considerably more related to biology than for example the feedforward networks which are increasingly used for calculations. structure of a self-organizing map typically soms have like our brain the task to map a high-dimensional input dimensions onto areas in a low chapter self-organizing feature maps dkriesel.com dimensional grid of cells dimensions to draw a map of the high-dimensional space so to speak. to generate this map the som simply obtains arbitrary many points of the input space. during the input of the points the som will try to cover as good as possible the positions on which the points appear by its neurons. this particularly means that every neuron can be assigned to a certain position in the input space. at first these facts seem to be a bit confusing and it is recommended to briefly reflect about them. there are two spaces in which soms are working the n-dimensional input space and the g-dimensional grid on which the neurons are lying and which indicates the neighborhood relationships between the neurons and therefore the network topology. in a one-dimensional grid the neurons could be for instance like pearls on a string. every neuron would have exactly two neighbors for the two end neurons. a two-dimensional grid could be a square array of neurons another possible array in two-dimensional space would be some kind of honeycomb shape. irregular topologies are possible too but not very often. topolgies with more dimensions and considerably more neighborhood relationships would also be possible but due to their lack of visualization capability they are not employed very often. high-dim. input low-dim. map input space and topology important! topologies of a selffigure example organizing map. above we can see a onedimensional topology below a two-dimensional one. even if n g is true the two spaces are not equal and have to be distinguished. in this special case they only have the same dimension. initially we will briefly and formally regard the functionality of a self-organizing map and then make it clear by means of some examples. definition neuron. similar to the neurons in an rbf network a som neuron k does not occupy a fixed position ck center in the input space. definition map. a self-organizing map is a set k of som neurons. if an input vector is entered ex- actly that neuron k k is activated which d. kriesel a brief introduction to neural networks input winner dkriesel.com training is closest to the input pattern in the input space. the dimension of the input space is referred to as n. definition the neurons are interconnected by neighborhood relationships. these neighborhood relationships are called topology. the training of a som is highly influenced by the it is defined by the topology topology. function hi k t where i is the winner ist k the neuron to be adapted will be discussed later and t the timestep. the dimension of the topology is referred to as g. soms always activate the neuron with the least distance to an input pattern like many other neural networks the som has to be trained before it can be used. but let us regard the very simple functionality of a complete self-organizing map before training since there are many analogies to the training. functionality consists of the following steps input of an arbitrary value p of the input space rn. calculation of the distance between every neuron k and p by means of a norm i.e. calculation of ck. one neuron becomes active namely such neuron i with the shortest we will learn soon what a winner neuron is. calculated distance to the input. all other neurons remain inactive.this paradigm of activity is also called winner-takes-all scheme. the output we expect due to the input of a som shows which neuron becomes active. in many literature citations the description of soms is more formal often an input layer is described that is completely linked towards an som layer. then the input layer neurons forwards all inputs to the som layer. the som layer is laterally linked in itself so that a winner neuron can be established and inhibit the other neurons. i think that this explanation of a som is not very descriptive and therefore i tried to provide a clearer description of the network structure. now the question is which neuron is activated by which input and the answer is given by the network itself during training. training makes the som topology cover the input space the training of a som is nearly as straightforward as the functionality described above. basically it is structured into five steps which partially correspond to those of functionality. initialization the network starts with random neuron centers ck rn from the input space. creating an input pattern a stimulus i.e. a point p is selected from the d. kriesel a brief introduction to neural networks chapter self-organizing feature maps dkriesel.com training input winner i change in position i and neighbors input space rn. now this stimulus is entered into the network. distance measurement then the distance ck is determined for every neuron k in the network. winner takes all the winner neuron i is determined which has the smallest distance to p i.e. which fulfills the condition ci ck k i you can see that from several winner neurons one can be selected at will. adapting the centers the neuron centers are moved within the input space according to the ck hi k t ck where the values ck are simply added to the existing centers. the last factor shows that the change in position of the neurons k is proportional to the distance to the input pattern p and as usual to a timedependent learning rate the above-mentioned network topology exerts its influence by means of the function hi k t which will be discussed in the following. note in many sources this rule is written hp ck which wrongly leads the reader to believe that h is a constant. this problem can easily be solved by not omitting the multiplication dots definition learning rule. a som is trained by presenting an input pattern and determining the associated winner neuron. the winner neuron and its neighbor neurons which are defined by the topology function then adapt their centers according to the rule ck hi k t ck ckt ckt ckt. the topology function defines how a learning neuron influences its neighbors the topology function h is not defined on the input space but on the grid and represents the neighborhood relationships between the neurons i.e. the topology of the network. it can be time-dependent it often is which explains the parameter t. the parameter k is the index running through all neurons and the parameter i is the index of the winner neuron. in principle the function shall take a large value if k is the neighbor of the winner neuron or even the winner neuron itself and small values if not. smore precise definition the topology function must be unimodal i.e. it must have exactly one maximum. this maximum must be next to the winner neuron i for which the distance to itself certainly is additionally the time-dependence enables us for example to reduce the neighborhood in the course of time. defined on the grid only maximum for the winner d. kriesel a brief introduction to neural networks dkriesel.com training in order to be able to output large values for the neighbors of i and small values for non-neighbors the function h needs some kind of distance notion on the grid because from somewhere it has to know how far i and k are apart from each other on the grid. there are different methods to calculate this distance. on a two-dimensional grid we could apply for instance the euclidean distance part of fig. or on a one-dimensional grid we could simply use the number of the connections between the neurons i and k part of the same figure. definition function. the topology function hi k t describes the neighborhood relationships in the topology. it can be any unimodal function that reaches its maximum when i k gilt. time-dependence is optional but often used. introduction of common distance and topology functions a common distance function would be for example the already known gaussian bell fig. on page it is unimodal with a maximum close to additionally its width can be changed by applying its parameter which can be used to realize the neighborhood being reduced in the course of time we simply relate the time-dependence to the and the result is figure example distances of a onedimensional som topology and a twodimensional som topology between two neurons i and k. in the lower case the euclidean distance is determined two-dimensional space equivalent to the pythagoream theorem. in the upper case we simply count the discrete path length between i and k. to simplify matters i required a fixed grid edge length of in both cases. d. kriesel a brief introduction to neural networks o o o x x o o o chapter self-organizing feature maps dkriesel.com a monotonically decreasing then our topology function could look like this typical sizes of the target value of a learning rate are two sizes smaller than the initial value e.g hi k t e where gi and gk represent the neuron positions on the grid not the neuron positions in the input space which would be referred to as ci and ck. other functions that can be used instead of the gaussian function are for instance the cone function the cylinder function or the mexican hat function on the facing page. here the mexican hat function offers a particular biological motivation due to its negative digits it rejects some neurons close to the winner neuron a behavior that has already been observed in nature. this can cause sharply separated map areas and that is exactly why the mexican hat function has been suggested by teuvo kohonen himself. but this adjustment characteristic is not necessary for the functionality of the map it could even be possible that the map would diverge i.e. it could virtually explode. learning rates and neighborhoods can decrease monotonically over time to avoid that the later training phases forcefully pull the entire map towards a new pattern the soms often work with temporally monotonically decreasing learning rates and neighborhood sizes. at first let us talk about the learning rate could be true. but this size must also depend on the network topology or the size of the neighborhood. as we have already seen a decreasing neighborhood size can be realized for example by means of a time-dependent monotonically decreasing with the gaussin bell being used in the topology function. the advantage of a decreasing neighborhood size is that in the beginning a moving neuron along many neurons in its vicinity i.e. the randomly initialized network can unfold fast and properly in the beginning. in the end of the learning process only a few neurons are influenced at the same time which stiffens the network as a whole but enables a good tuning of the individual neurons. it must be noted that h must always be true since otherwise the neurons would constantly miss the current training sample. but enough of theory let us take a look at a som in action! d. kriesel a brief introduction to neural networks dkriesel.com training figure gaussian bell cone function cylinder function and the mexican hat function suggested by kohonen as examples for topology functions of a som.. d. kriesel a brief introduction to neural networks in function funktion hat function chapter self-organizing feature maps dkriesel.com p figure illustration of the two-dimensional input space and the one-dimensional topolgy space of a self-organizing map. neuron is the winner neuron since it is closest to p. in the topology the neurons and are the neighbors of the arrows mark the movement of the winner neuron and its neighbors towards the training sample p. to illustrate the one-dimensional topology of the network it is plotted into the input space by the dotted line. the arrows mark the movement of the winner neuron and its neighbors towards the pattern. d. kriesel a brief introduction to neural networks dkriesel.com examples examples for the functionality of soms let us begin with a simple mentally comprehensible example. in this example we use a two-dimensional input space i.e. n is true. let the grid structure be one-dimensional furthermore our example som should consist of neurons and the learning rate should be the neighborhood function is also kept simple so that we will be able to mentally comprehend the network hi k t k direct neighbor of i k i otherw. now let us take a look at the abovementioned network with random initialization of the centers on the preceding page and enter a training sample p. obviously in our example the input pattern is closest to neuron i.e. this is the winning neuron. we remember soms learning rule the for ck hi k t ck thus the factor ck indicates the vector of the neuron k to the pattern p. this is now multiplied by different scalars our topology function h indicates that only the winner neuron and its two closest neighbors and are allowed to learn by returning for all other neurons. a time-dependence is not specified. thus our vector ck is multiplied by either or the learning rate indicates as always the strength of learning. as already mentioned i. e. all in all the result is that the winner neuron and its neighbors and approximate the pattern p half the way the figure marked by arrows. although the center of neuron seen from the input space is considerably closer to the input pattern p than neuron neuron is learning and neuron is not. i want to remind that the network topology specifies which neuron is allowed to learn and not its position in the input space. this is exactly the mechanism by which a topology can significantly cover an input space without having to be related to it by any sort. topology specifies who will learn and process the three factors from the back learning direction remember that the neuron centers ck are vectors in the input space as well as the pattern p. after the adaptation of the neurons and the next pattern is applied and so on. another example of how such a onedimensional som can develop in a twodimensional input space with uniformly distributed input patterns in the course of d. kriesel a brief introduction to neural networks chapter self-organizing feature maps dkriesel.com time can be seen in figure on the facing page. end states of one- and two-dimensional soms with differently shaped input spaces can be seen in figure on page as we can see not every input space can be neatly covered by every network topology. there are so called exposed neurons neurons which are located in an area where no input pattern has ever been occurred. a one-dimensional topology generally produces less exposed neurons than a two-dimensional one for instance during training on circularly arranged input patterns it is nearly impossible with a twodimensional squared topology to avoid the exposed neurons in the center of the circle. these are pulled in every direction during the training so that they finally remain in the center. but this does not make the one-dimensional topology an optimal topology since it can only find less complex neighborhood relationships than a multi-dimensional one. topological defects are failures in som unfolding in map during the unfolding of a som it could happen that a topological defect occurs i.e. the som does not unfold correctly. a topological defect can be described at best by means of the word figure a topological defect in a twodimensional som. neighborhood size because the more complex the topology is the more neighbors each neuron has respectively since a three-dimensional or a honeycombed twodimensional topology could also be generated the more difficult it is for a randomly initialized map to unfold. it is possible to adjust the resolution of certain areas in a som a remedy for topological defects could be to increase the initial values for the we have seen that a som is trained by entering input patterns of the input space d. kriesel a brief introduction to neural networks dkriesel.com adjustment of resolution and position-dependent learning rate figure behavior of a som with one-dimensional topology after the input of and randomly distributed input patterns p during the training decreased from to the parameter of the gauss function decreased from to d. kriesel a brief introduction to neural networks chapter self-organizing feature maps dkriesel.com figure end states of one-dimensional column and two-dimensional column soms on different input spaces. neurons were used for the one-dimensional topology neurons for the two-dimensionsal topology and input patterns for all maps. d. kriesel a brief introduction to neural networks more patterns higher resolution dkriesel.com application rn one after another again and again so that the som will be aligned with these patterns and map them. it could happen that we want a certain subset u of the input space to be mapped more precise than the other ones. this problem can easily be solved by means of soms during the training disproportionally many input patterns of the area u are presented to the som. if the number of training patterns of u rn presented to the som exceeds the number of those patterns of the remaining rn u then more neurons will group there while the remaining neurons are sparsely distributed on rn u on the next page. as you can see in the illustration the edge of the som could be deformed. this can be compensated by assigning to the edge of the input space a slightly higher probability of being hit by training patterns often applied approach for reaching every corner with the soms. also a higher learning rate is often used for edge and corner neurons since they are only pulled into the center by the topology. this also results in a significantly improved corner coverage. application of soms regarding the biologically inspired associative data storage there are many fields of application for self-organizing maps and their variations. for example the different phonemes of the finnish language have successfully been mapped onto a som with a two dimensional discrete grid topology and therefore neighborhoods have been found som does nothing else than finding neighborhood relationships. so one tries once more to break down a high-dimensional space into a low-dimensional space topology looks if some structures have been developed et voil clearly defined areas for the individual phenomenons are formed. teuvo kohonen himself made the effort to search many papers mentioning his soms in their keywords. in this large input space the individual papers now individual positions depending on the occurrence of keywords. then kohonen created a som with g and used it to map the high-dimensional space developed by him. thus it is possible to enter any paper into the completely trained som and look which neuron in the som is activated. it will be likely to discover that the neighbored papers in the topology are interesting too. this type of brain-like contextbased search also works with many other input spaces. it is to be noted that the system itself similar defines what is neighbored i.e. within the topology and that s why it is so interesting. this example shows that the position c of the neurons in the input space is not significant. it is rather interesting to see which d. kriesel a brief introduction to neural networks som finds similarities chapter self-organizing feature maps dkriesel.com figure training of a som with g on a two-dimensional input space. on the left side the chance to become a training pattern was equal for each coordinate of the input space. on the right side for the central circle in the input space this chance is more than ten times larger than for the remaining input space in the larger pattern density in the background. in this circle the neurons are obviously more crowded and the remaining area is covered less dense but in both cases the neurons are still evenly distributed. the two soms were trained by means of training samples and decreasing as well as decreasing d. kriesel a brief introduction to neural networks dkriesel.com variations neuron is activated when an unknown input pattern is entered. next we can look at which of the previous inputs this neuron was also activated and will immediately discover a group of very similar inputs. the more the inputs within the topology are diverging the less things they have in common. virtually the topology generates a map of the input characteristics reduced to descriptively few dimensions in relation to the input dimension. therefore the topology of a som often is two-dimensional so that it can be easily visualized while the input space can be very high-dimensional. soms can be used to determine centers for rbf neurons soms arrange themselves exactly towards the positions of the outgoing inputs. as a result they are used for example to select the centers of an rbf network. we have already been introduced to the paradigm of the rbf network in chapter as we have already seen it is possible to control which areas of the input space should be covered with higher resolution or in connection with rbf networks on which areas of our function should the rbf network work with more neurons i.e. work more exactly. as a further useful feature of the combination of rbf networks with soms one can use the topology obtained through the som during the final training of a rbf neuron it can be used to influence neighboring rbf neurons in different ways. for this many neural network simulators offer an additional so-called som layer in connection with the simulation of rbf networks. variations of soms there are different variations of soms for different variations of representation tasks a neural gas is a som without a static topology the neural gas is a variation of the selforganizing maps of thomas martinetz which has been developed from the difficulty of mapping complex input information that partially only occur in the subspaces of the input space or even change the subspaces on the following page. the idea of a neural gas is roughly speaking to realize a som without a grid structure. due to the fact that they are derived from the soms the learning steps are very similar to the som learning steps but they include an additional intermediate step again random initialization of ck rn selection and presentation of a pat tern of the input space p rn d. kriesel a brief introduction to neural networks chapter self-organizing feature maps dkriesel.com dynamic neighborhood figure a figure filling different subspaces of the actual input space of different positions therefore can hardly be filled by a som. neuron distance measurement identification of the winner neuron i intermediate step generation of a list l of neurons sorted in ascending order by their distance to the winner neuron. thus the first neuron in the list l is the neuron that is closest to the winner neuron. changing the centers by means of the known rule but with the slightly modified topology function hli k t. the function hli k t which is slightly modified compared with the original function hi k t now regards the first elements of the list as the neighborhood of the winner neuron i. the direct result is that similar to the free-floating molecules in a gas the neighborhood relationships between the neurons can change anytime and the number of neighbors is almost arbitrary too. the distance within the neighborhood is now represented by the distance within the input space. the bulk of neurons can become as stiffened as a som by means of a constantly decreasing neighborhood size. it does not have a fixed dimension but it can take the dimension that is locally needed at the moment which can be very advantageous. a disadvantage could be that there is no fixed grid forcing the input space to become regularly covered and therefore wholes can occur in the cover or neurons can be isolated. d. kriesel a brief introduction to neural networks can classify complex figure dkriesel.com variations in spite of all practical hints it is as always the user s responsibility not to understand this text as a catalog for easy answers but to explore all advantages and disadvantages himself. unlike a som the neighborhood of a neural gas must initially refer to all neurons since otherwise some outliers of the random initialization may never reach the remaining group. to forget this is a popular error during the implementation of a neural gas. with a neural gas it is possible to learn a kind of complex input such as in fig. on the preceding page since we are not bound to a fixed-dimensional grid. but some computational effort could be necessary for the permanent sorting of the list it could be effective to store the list in an ordered data structure right from the start. definition gas. a neural gas differs from a som by a completely dynamic neighborhood function. with every learning cycle it is decided anew which neurons are the neigborhood neurons of the winner neuron. generally the criterion for this decision is the distance between the neurosn and the winner neuron in the input space. a multi-som consists of several separate soms in order to present another variant of the soms i want to formulate an extended problem what do we do with input patterns from which we know that they are confined in different disjoint areas? here the idea is to use not only one som but several ones a multi-selforganizing map shortly referred to as m-som it is unnecessary that the soms have the same topology or size an m-som is just a combination of m soms. this learning process is analog to that of the soms. however only the neurons belonging to the winner som of each training step are adapted. thus it is easy to represent two disjoint clusters of data by means of two soms even if one of the clusters is not represented in every dimension of the input space rn. actually the individual soms exactly reflect these clusters. definition a multisom is nothing more than the simultaneous use of m soms. a multi-neural gas consists of several separate neural gases analogous to the multi-som we also have a set of m neural gases a multi-neural gas this construct behaves analogous to neural gas and m-som again only the neurons of the winner gas are adapted. the reader certainly wonders what advantage is there to use a multi-neural gas since d. kriesel a brief introduction to neural networks several soms several gases less computational effort chapter self-organizing feature maps dkriesel.com an individual neural gas is already capable to divide into clusters and to work on complex input patterns with changing dimensions. basically this is correct but a multi-neural gas has two serious advantages over a simple neural gas. with several gases we can directly tell which neuron belongs to which gas. this is particularly important for clustering tasks for which multineural gases have been used recently. simple neural gases can also find and cover clusters but now we cannot recognize which neuron belongs to which cluster. a lot of computational effort is saved when large original gases are divided into several smaller ones since already mentioned the sorting of the list l could use a lot of computational effort while the sorting of several smaller lists lm is less time-consuming even if these lists in total contain the same number of neurons. as a result we will only obtain local instead of global sortings but in most cases these local sortings are sufficient. now we can choose between two extreme cases of multi-neural gases one extreme case is the ordinary neural gas m i.e. we only use one single neural gas. interesting enough the other extreme case large m a few or only one neuron per gas behaves analogously to the k-means clustering more information on clustering procedures see excursus a. definition gas. a multi-neural gas is nothing more than the simultaneous use of m neural gases. growing neural gases can add neurons to themselves a growing neural gas is a variation of the aforementioned neural gas to which more and more neurons are added according to certain rules. thus this is an attempt to work against the isolation of neurons or the generation of larger wholes in the cover. here this subject should only be mentioned but not discussed. to build a growing som is more difficult because new neurons have to be integrated in the neighborhood. exercises exercise a regular two-dimensional grid shall cover a two-dimensional surface as as possible. which grid structure would suit best for this purpose? which criteria did you use for and the very imprecise formulation of this exercise is intentional. d. kriesel a brief introduction to neural networks chapter adaptive resonance theory an art network in its original form shall classify binary input vectors i.e. to assign them to a output. simultaneously the so far unclassified patterns shall be recognized and assigned to a new class. as in the other smaller chapters we want to try to figure out the basic idea of the adaptive resonance theory art without discussing its theory profoundly. in several sections we have already mentioned that it is difficult to use neural networks for the learning of new information in addition to but without destroying the already existing information. this circumstance is called stability plasticity dilemma. in stephen grossberg and gail carpenter published the first version of their art network in order to alleviate this problem. this was followed by a whole family of art improvements we want to discuss briefly too. it is the idea of unsupervised learning whose aim is the binary pattern recognition or more precisely the categorization of patterns into classes. but addi tionally an art network shall be capable to find new classes. task and structure of an art network an art network comprises exactly two layers the input layer i and the recognition layer o with the input layer being completely linked towards the recognition layer. this complete link induces a top-down weight matrix w that contains the weight values of the connections between each neuron in the input layer and each neuron in the recognition layer on the following page. simple binary patterns are entered into the input layer and transferred to the recognition layer while the recognition layer shall return a encoding i.e. it should follow the winner-takes-all pattern recognition chapter adaptive resonance theory dkriesel.com gfed gfed gfed gfed issssssssssssssssssssssssssssssssssssss gooooooooooooooooooooooooooooo gooooooooooooooooooooooooooooo ukkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk cfffffffffffffffffffff cfffffffffffffffffffff cfffffffffffffffffffff e e e e wooooooooooooooooooooooooooooo wooooooooooooooooooooooooooooo gfed gfed gfed gfed gfed gfed figure simplified illustration of the art network structure. top the input layer bottom the recognition layer. in this illustration the lateral inhibition of the recognition layer and the control neurons are omitted. scheme. for instance to realize this encoding the principle of lateral inhibition can be used or in the implementation the most activated neuron can be searched. for practical reasons an if query would suit this task best. put layer causes an activity within the recognition layer while in turn in the recognition layer every activity causes an activity within the input layer. layers activate one another resonance takes place by activities being tossed and turned v but there also exists a bottom-up weight matrix v which propagates the activities within the recognition layer back into the input layer. now it is obvious that these activities are bounced forth and back again and again a fact that leads us to resonance. every activity within the in in addition to the two mentioned layers in an art network also exist a few neurons that exercise control functions such as signal enhancement. but we do not want to discuss this theory further since here only the basic principle of the art network should become explicit. i have only mentioned it to explain that in spite of the recurrences the art network will achieve a stable state after an input. d. kriesel a brief introduction to neural networks w u w e o o e y o o e c y o o e g c y o o i g c y dkriesel.com extensions the learning process of an art network is divided to top-down and bottom-up learning the trick of adaptive resonance theory is not only the configuration of the art network but also the two-piece learning procedure of the theory on the one hand we train the top-down matrix w on the other hand we train the bottom-up matrix v on the next page. pattern input and top-down learning when a pattern is entered into the network it causes as already mentioned an activation at the output neurons and the strongest neuron wins. then the weights of the matrix w going towards the output neuron are changed such that the output of the strongest neuron is still enhanced i.e. the class affiliation of the input vector to the class of the output neuron becomes enhanced. resonance and bottom-up learning the training of the backward weights of the matrix v is a bit tricky only the weights of the respective winner neuron are trained towards the input layer and our current input pattern is used as teaching input. thus the network is trained to enhance input vectors. adding an output neuron of course it could happen that the neurons are nearly equally activated or that several neurons are activated i.e. that the network is indecisive. in this case the mechanisms of the control neurons activate a signal that adds a new output neuron. then the current pattern is assigned to this output neuron and the weight sets of the new neuron are trained as usual. thus the advantage of this system is not only to divide inputs into classes and to find new classes it can also tell us after the activation of an output neuron what a typical representative of a class looks like which is a significant feature. often however the system can only moderately distinguish the patterns. the question is when a new neuron is permitted to become active and when it should learn. in an art network there are different additional control neurons which answer this question according to different mathematical rules and which are responsible for intercepting special cases. at the same time one of the largest objections to an art is the fact that an art network uses a special distinction of cases similar to an if query that has been forced into the mechanism of a neural network. extensions as already mentioned above the art networks have often been extended. d. kriesel a brief introduction to neural networks winner neuron is amplified input is teach. inp. for backward weights chapter adaptive resonance theory dkriesel.com is extended to continuous inputs and additionally offers an extension called enhancements of the learning speed which results in additional control neurons and layers. improves the learning ability of by adapting additional biological processes such as the chemical processes within the apart from the described ones there exist many other extensions. figure simplified illustration of the twopiece training of an art network the trained weights are represented by solid lines. let us assume that a pattern has been entered into the network and that the numbers mark the outputs. top we can see that is the winner neuron. middle so the weights are trained towards the winner neuron and the weights of the winner neuron are trained towards the input layer. because of the frequent extensions of the adaptive resonance theory wagging tongues already call them networks. d. kriesel a brief introduction to neural networks gfedabc atzlichebiolo-gischevorg angewiez.b.diechemischenvorg aufigenerweiterungenderadaptiveresonancetheorysprechenb osezungenbereitsvon art-n-netzen einkleiner uberblick uberneuronalenetzeepsilon-de part iv excursi appendices and registers appendix a excursus cluster analysis and regional and online learnable fields in grimm s dictionary the extinct german word is described by dicht und dick zusammensitzet thick and dense group of sth.. in static cluster analysis the formation of groups within point clouds is explored. introduction of some procedures comparison of their advantages and disadvantages. discussion of an adaptive clustering method based on neural networks. a regional and online learnable field models from a point cloud possibly with a lot of points a comparatively small set of neurons being representative for the point cloud. as already mentioned many problems can be traced back to problems in cluster analysis. therefore it is necessary to research procedures that examine whether groups clusters exist within point clouds. since cluster analysis procedures need a notion of distance between two points a metric must be defined on the space where these points are situated. we briefly want to specify what a metric is. definition a relation defined for two objects is referred to as metric if each of the following criteria applies if and only if i.e. metry sym i.e. inequality holds. the triangle colloquially speaking a metric is a tool for determining distances between points in any space. here the distances have to be symmetrical and the distance between to points may only be if the two points are equal. additionally the triangle inequality must apply. metrics are provided by for example the squared distance and the euclidean distance which have already been introduced. based on such metrics we can de number of cluster must be known previously appendix a excursus cluster analysis and regional and online learnable fieldsdkriesel.com fine a clustering procedure that uses a metric as distance measure. continue with until the assignments are no longer changed. now we want to introduce and briefly discuss different clustering procedures. k-means clustering allocates data to a predefined number of clusters k-means clustering according to j. macqueen is an algorithm that is often used because of its low computation and storage complexity and which is regarded as and good. the operation sequence of the k-means clustering algorithm is the following provide data to be examined. define k which is the number of clus ter centers. select k random vectors for the cluster centers referred to as codebook vectors. assign each data point to the next codebook compute cluster centers for all clus ters. set codebook vectors to new cluster centers. the name codebook vector was created because the often used name cluster vector was too unclear. step already shows one of the great questions of the k-means algorithm the number k of the cluster centers has to be determined in advance. this cannot be done by the algorithm. the problem is that it is not necessarily known in advance how k can be determined best. another problem is that the procedure can become quite instable if the codebook vectors are badly initialized. but since this is random it is often useful to restart the procedure. this has the advantage of not requiring much computational effort. if you are fully aware of those weaknesses you will receive quite good results. however complex structures such as in clusters cannot be recognized. if k is high the outer ring of the construction in the following illustration will be recognized as many single clusters. if k is low the ring with the small inner clusters will be recognized as one cluster. for an illustration see the upper right part of fig. on page k-nearest neighboring looks for the k nearest neighbors of each data point the k-nearest neighboring procedure connects each data point to the k closest neighbors which often results in a division of the groups. then such a group d. kriesel a brief introduction to neural networks clustering radii around points dkriesel.com the silhouette coefficient builds a cluster. the advantage is that the number of clusters occurs all by itself. the disadvantage is that a large storage and computational effort is required to find the next neighbor distances between all data points must be computed and stored. clustering next points there are some special cases in which the procedure combines data points belonging to different clusters if k is too high. the two small clusters in the upper right of the illustration. clusters consisting of only one single data point are basically conncted to another cluster which is not always intentional. furthermore it is not mandatory that the links between the points are symmetric. but this procedure allows a recognition of rings and therefore of in clusters which is a clear advantage. another advantage is that the procedure adaptively responds to the distances in and between the clusters. for an illustration see the lower left part of fig. which is the reason for the name epsilonnearest neighboring. points are neigbors if they are at most apart from each other. here the storage and computational effort is obviously very high which is a disadvantage. but note that there are some special cases two separate clusters can easily be connected due to the unfavorable situation of a single data point. this can also happen with k-nearest neighboring but it would be more difficult since in this case the number of neighbors per point is limited. an advantage is the symmetric nature of the neighborhood relationships. another advantage is that the combination of minimal clusters due to a fixed number of neighbors is avoided. on the other hand it is necessary to skillfully initialize in order to be successful i.e. smaller than half the smallest distance between two clusters. with variable cluster and point distances within clusters this can possibly be a problem. for an illustration see the lower right part of fig. neighboring looks for neighbors within the radius for each data point the silhouette coefficient determines how accurate a given clustering is another approach of neighboring here the neighborhood detection does not use a fixed number k of neighbors but a radius as we can see above there is no easy answer for clustering problems. each procedure described has very specific disadvantages. in this respect it is useful to have d. kriesel a brief introduction to neural networks appendix a excursus cluster analysis and regional and online learnable fieldsdkriesel.com figure top left our set of points. we will use this set to explore the different clustering methods. top right k-means clustering. using this procedure we chose k as we can see the procedure is not capable to recognize in clusters left of the illustration. long of points are a problem too they would be recognized as many small clusters k is sufficiently large. bottom left k-nearest neighboring. if k is selected too high than the number of points in the smallest cluster this will result in cluster combinations shown in the upper right of the illustration. bottom right neighboring. this procedure will cause difficulties if is selected larger than the minimum distance between two clusters upper left of the illustration which will then be combined. d. kriesel a brief introduction to neural networks dkriesel.com regional and online learnable fields a criterion to decide how good our cluster division is. this possibility is offered by the silhouette coefficient according to this coefficient measures how well the clusters are delimited from each other and indicates if points may be assigned to the wrong clusters. apparently the whole term sp can only be within the interval a value close to indicates a bad classification of p. the silhouette coefficient sp results from the average of all values sp sp x p p sp. as above the total quality of the cluster division is expressed by the interval as different clustering strategies with different characteristics have been presented now of further material is presented in as well as a measure to indicate the quality of an existing arrangement of given data into clusters i want to introduce a clustering method based on an unsupervised learning neural network which was published in like all the other methods this one may not be perfect but it eliminates large standard weaknesses of the known clustering methods regional and online learnable fields are a neural clustering strategy the paradigm of neural networks which i want to introduce now are the regional and online learnable fields shortly referred to as rolfs. clustering quality is measureable let p be a point cloud and p a point in p. let c p be a cluster within the point cloud and p be part of this cluster i.e. p c. the set of clusters is called c. summary p c p applies. to calculate the silhouette coefficient we initially need the average distance between point p and all its cluster neighbors. this variable is referred to as ap and defined as follows ap x q distp q furthermore let bp be the average distance between our point p and all points of the next cluster represents all clusters except for c bp min g distp q x q g the point p is classified well if the distance to the center of the own cluster is minimal and the distance to the centers of the other clusters is maximal. in this case the following term provides a value close to sp bp ap maxap bp d. kriesel a brief introduction to neural networks appendix a excursus cluster analysis and regional and online learnable fieldsdkriesel.com network covers point cloud rolfs try to cover data with neurons roughly speaking the regional and online learnable fields are a set k of neurons which try to cover a set of points as well as possible by means of their distribution in the input space. for this neurons are added moved or changed in their size during training if necessary. the parameters of the individual neurons will be discussed later. definition and online learnable field. a regional and online learnable field rolf or rolf network is a set k of neurons that are trained to cover a certain set in the input space as well as possible. figure structure of a rolf neuron. rolf neurons feature a position and a radius in the input space here a rolf neuron k k has two parameters similar to the rbf networks it has a center ck i.e. a position in the input space. but it has yet another parameter the radius which defines the radius of the perceptive surface surrounding the a neuron covers the part of the input space that is situated within this radius. neuron represents surface ck and k are locally defined for each neu i write and not because the actual radius is specified by ron. this particularly means that the neurons are capable to cover surfaces of different sizes. the radius of the perceptive surface is specified by r with the multiplier being globally defined and previously specified for all neurons. intuitively the reader will wonder what this multiplicator is used for. its significance will be discussed later. furthermore the following has to be observed it is not necessary for the perceptive surface of the different neurons to be of the same size. definition neuron. the parameters of a rolf neuron k are a center ck and a radius k. definition surface. the perceptive surface of a rolf neuron d. kriesel a brief introduction to neural networks dkriesel.com regional and online learnable fields k consists of all points within the radius in the input space. a rolf learns unsupervised by presenting training samples online like many other paradigms of neural networks our rolf network learns by receiving many training samples p of a training set p. the learning is unsupervised. for each training sample p entered into the network two cases can occur there is one accepting neuron k for p or there is no accepting neuron at all. if in the first case several neurons are suitable then there will be exactly one accepting neuron insofar as the closest neuron is the accepting one. for the accepting neuron k ck and k are adapted. definition neuron. the criterion for a rolf neuron k to be an accepting neuron of a point p is that the point p must be located within the perceptive surface of k. if p is located in the perceptive surfaces of several neurons then the closest neuron will be the accepting one. if there are several closest neurons one can be chosen randomly. both positions and radii are adapted throughout learning adapting existing neurons let us assume that we entered a training sample p into the network and that there is an accepting neuron k. then the radius moves towards ck towards the distance between p and ck and the center ck towards p. additionally let us define the two learning rates and c for radii c and centers. ckt ckt cp ckt kt kt ckt kt note that here k is a scalar while ck is a vector in the input space. definition a rolf neuron. a neuron k accepted by a point p is adapted according to the following rules ckt ckt cp ckt kt kt ckt kt the radius multiplier allows neurons to be able not only to shrink now we can understand the function of the multiplier due to this multiplier the per- ceptive surface of a neuron includes more than only all points surrounding the neuron in the radius this means that due to the aforementioned learning rule cannot only decrease but also increase. definition multiplier. the radius multiplier is globally defined and expands the perceptive surface of a neuron k to a multiple of k. so it is ensured that the radius k cannot only decrease but also increase. so the neurons can grow d. kriesel a brief introduction to neural networks appendix a excursus cluster analysis and regional and online learnable fieldsdkriesel.com generally the radius multiplier is set to values in the lower one-digit range such as or so far we only have discussed the case in the rolf training that there is an accepting neuron for the training sample p. as required new neurons are generated this suggests to discuss the approach for the case that there is no accepting neuron. in this case a new accepting neuron k is generated for our training sample. the result is of course that ck and k have to be initialized. the initialization of ck can be understood intuitively the center of the new neuron is simply set on the training sample i.e. ck p. we generate a new neuron because there is no neuron close to p for logical reasons we place the neuron exactly on p. but how to set a when a new neuron is generated? for this purpose there exist different options init- we always select a predefined static minimum we take a look at the of each neuron and select the minimum. maximum we take a look at the of each neuron and select the maximum. mean we select the mean of all neu rons. currently the mean- variant is the favorite one although the learning procedure also works with the other ones. in the minimum- variant the neurons tend to cover less of the surface in the maximum variant they tend to cover more of the surface. definition a rolf neuron. if a new rolf neuron k is generated by entering a training sample p then ck is intialized with p and k according to one of the aforementioned strategies minimum- maximum- mean- the training is complete when after repeated randomly permuted pattern presentation no new neuron has been generated in an epoch and the positions of the neurons barely change. evaluating a rolf the result of the training algorithm is that the training set is gradually covered well and precisely by the rolf neurons and that a high concentration of points on a spot of the input space does not automatically generate more neurons. thus a possibly very large point cloud is reduced to very few representatives on the input set. then it is very easy to define the number of clusters two neurons are to the definition of the rolf connected when their perceptive surfaces over initialization of a neurons cluster connected neurons d. kriesel a brief introduction to neural networks dkriesel.com regional and online learnable fields some kind of nearest neighborlap ing is executed with the variable perceptive surfaces. a cluster is a group of connected neurons or a group of points of the input space covered by these neurons of course the complete rolf network can be evaluated by means of other clustering methods i.e. the neurons can be searched for clusters. particularly with clustering methods whose storage effort grows quadratic to the storage effort can be reduced dramatically since generally there are considerably less rolf neurons than original data points but the neurons represent the data points quite well. comparison with popular clustering methods it is obvious that storing the neurons rather than storing the input points takes the biggest part of the storage effort of the rolfs. this is a great advantage for huge point clouds with a lot of points. since it is unnecessary to store the entire point cloud our rolf as a neural clustering method has the capability to learn online which is definitely a great advantage. furthermore it can to nearest neighboring or k nearest neighboring distinguish clusters from enclosed clusters but due to the online presentation of the data without a quadratically growing storage effort which is by far the greatest disadvantage of the two neighboring methods. less storage effort! recognize in clusters figure the clustering process. top the input set middle the input space covered by rolf neurons bottom the input space only covered by the neurons d. kriesel a brief introduction to neural networks appendix a excursus cluster analysis and regional and online learnable fieldsdkriesel.com additionally the issue of the size of the individual clusters proportional to their distance from each other is addressed by using variable perceptive surfaces which is also not always the case for the two mentioned methods. the rolf compares favorably with kmeans clustering as well firstly it is unnecessary to previously know the number of clusters and secondly k-means clustering recognizes clusters enclosed by other clusters as separate clusters. initializing radii learning rates and multiplier is not trivial certainly the disadvantages of the rolf shall not be concealed it is not always easy to select the appropriate initial value for and the previous knowledge about the data set can so to say be included in and the initial value of of the rolf fine-grained data clusters should use a small and a small initial value. but the smaller the the smaller the chance that the neurons will grow if necessary. here again there is no easy answer just like for the learning rates c and for the multipliers in the lower singledigit range such as or are very popular. c and successfully work with values about to variations during run-time are also imaginable for this type of network. initial values for generally depend on the cluster and data distribution they often have to be tested. but compared to wrong initializations at least with the mean- strategy they are relatively robust after some training time. as a whole the rolf is on a par with the other clustering methods and is particularly very interesting for systems with low storage capacity or huge data sets. application examples a first application example could be finding color clusters in rgb images. another field of application directly described in the rolf publication is the recognition of words transferred into a feature space. thus we can see that rolfs are relatively robust against higher dimensions. further applications can be found in the field of analysis of attacks on network systems and their classification. exercises exercise determine at least four adaptation steps for one single rolf neuron k if the four patterns stated below are presented one after another in the indicated order. let the initial values for the rolf neuron be ck and k furthermore let c and let p d. kriesel a brief introduction to neural networks appendix b excursus neural networks used for prediction discussion of an application of neural networks a look ahead into the future of time series. after discussing the different paradigms of neural networks it is now useful to take a look at an application of neural networks which is brought up often and we will see is also used for fraud the application of time series prediction. this excursus is structured into the description of time series and estimations about the requirements that are actually needed to predict the values of a time series. finally i will say something about the range of software which should predict share prices or other economic characteristics by means of neural networks or other procedures. about time series a time series is a series of values discretized in time. for example daily measured temperature values or other meteorological data of a specific site could be represented by a time series. share price values also represent a time series. often the measurement of time series is timely equidistant and in many time series the future development of their values is very interesting e.g. the daily weather forecast. this chapter should not be a detailed description but rather indicate some approaches for time series prediction. in this respect i will again try to avoid formal definitions. time series of values time series can also be values of an actually continuous function read in a certain distance of time t on the next t page. if we want to predict a time series we will look for a neural network that maps the previous series values to future developments of the time series i.e. if we know longer sections of the time series we will appendix b excursus neural networks used for prediction dkriesel.com have enough training samples. of course these are not examples for the future to be predicted but it is tried to generalize and to extrapolate the past by means of the said samples. but before we begin to predict a time series we have to answer some questions about this time series we are dealing with and ensure that it fulfills some requirements. do we have any evidence which suggests that future values depend in any way on the past values of the time series? does the past of a time series include information about its future? do we have enough past values of the time series that can be used as training patterns? in case of a prediction of a continuous function what must a useful t look like? now these questions shall be explored in detail. how much information about the future is included in the past values of a time series? this is the most important question to be answered for any time series that should be mapped into the future. if the future values of a time series for instance do not depend on the past values then a time series prediction based on them will be impossible. in this chapter we assume systems whose future values can be deduced from their states the deterministic systems. this figure a function x that depends on the time is sampled at discrete time steps discretized this means that the result is a time series. the sampled values are entered into a neural network this example an slp which shall learn to predict the future values of the time series. d. kriesel a brief introduction to neural networks dkriesel.com one-step-ahead prediction leads us to the question of what a system state is. one-step-ahead prediction a system state completely describes a system for a certain point of time. the future of a deterministic system would be clearly defined by means of the complete description of its current state. the problem in the real world is that such a state concept includes all things that influence our system by any means. in case of our weather forecast for a specific site we could definitely determine the temperature the atmospheric pressure and the cloud density as the meteorological state of the place at a time t. but the whole state would include significantly more information. here the worldwide phenomena that control the weather would be interesting as well as small local pheonomena such as the cooling system of the local power plant. so we shall note that the system state is desirable for prediction but not always possible to obtain. often only fragments of the current states can be acquired e.g. for a weather forecast these fragments are the said weather data. however we can partially overcome these weaknesses by using not only one single state last one for the prediction but by using several past states. from this we want to derive our first prediction system predict the next value the first attempt to predict the next future value of a time series out of past values is called one-step-ahead prediction on the following page. such a predictor system receives the last n observed state parts of the system as input and outputs the prediction for the next state state part. the idea of a state space with predictable states is called state space forecasting. the aim of the predictor is to realize a function fxt xt xt which receives exactly n past values in order to predict the future value. predicted values shall be headed by a tilde x x to distinguish them from the actual future values. the most intuitive and simplest approach would be to find a linear combination ajxi j that approximately fulfills our tions. condi such a construction is called digital filter. here we use the fact that time series d. kriesel a brief introduction to neural networks appendix b excursus neural networks used for prediction dkriesel.com xt xt xt xt predictor figure representation of the one-step-ahead prediction. it is tried to calculate the future value from a series of past values. the predicting element this case a neural network is referred to as predictor. usually have a lot of past values so that we can set up a series of means of the delta rule provides results very close to the analytical solution. xt ajxt xt ajxt xt n n ajxt n thus n equations could be found for n unknown coefficients and solve them possible. or another better approach we could use m n equations for n unknowns in such a way that the sum of the mean squared errors of the already known prediction is minimized. this is called moving average procedure. but this linear structure corresponds to a singlelayer perceptron with a linear activation function which has been trained by means of data from the past experimental setup would comply with fig. on page in fact the training by without going into detail i want to remark that the prediction becomes easier the more past values of the time series are available. i would like to ask the reader to read up on the nyquist-shannon sampling theorem even if this approach often provides satisfying results we have seen that many problems cannot be solved by using a singlelayer perceptron. additional layers with linear activation function are useless as well since a multilayer perceptron with only linear activation functions can be reduced to a singlelayer perceptron. such considerations lead to a non-linear approach. the multilayer perceptron and non-linear activation functions provide a universal non-linear function approximator i.e. we can use an for n n inputs out of the past. an rbf network could also be used. but remember that here the number n has to remain low since in rbf networks high input dimensions are very complex to realize. so if we want to include many past values a multilayer perceptron will require considerably less computational effort. d. kriesel a brief introduction to neural networks k k dkriesel.com additional optimization approaches for prediction two-step-ahead prediction additional optimization approaches for prediction what approaches can we use to to see farther into the future? recursive two-step-ahead prediction in order to extend the prediction to for instance two time steps into the future we could perform two one-step-ahead predictions in a row on the following i.e. a recursive two-step-ahead page prediction. unfortunately the value determined by means of a one-step-ahead prediction is generally imprecise so that errors can be built up and the more predictions are performed in a row the more imprecise becomes the result. direct two-step-ahead prediction we have already guessed that there exists a better approach just like the system can be trained to predict the next value we can certainly train it to predict the next but one value. this means we directly train for example a neural network to look two time steps ahead into the future which is referred to as direct twostep-ahead prediction on the next page. obviously the direct two-stepahead prediction is technically identical to the one-step-ahead prediction. the only difference is the training. the possibility to predict values far away in the future is not only important because we try to look farther ahead into the future. there can also be periodic time series where other approaches are hardly possible if a lecture begins at a.m. every thursday it is not very useful to know how many people sat in the lecture room on monday to predict the number of lecture participants. the same applies for example to periodically occurring commuter jams. changing temporal parameters thus it can be useful to intentionally leave gaps in the future values as well as in the past values of the time series i.e. to introduce the parameter t which indicates which past value is used for prediction. technically speaking we still use a onestep-ahead prediction only that we extend the input space or train the system to predict values lying farther away. it is also possible to combine different t in case of the traffic jam prediction for a monday the values of the last few days could be used as data input in addition to the values of the previous mondays. thus we use the last values of several periods in this case the values of a weekly and a daily period. we could also include an annual period in the form of the beginning of the holidays sure everyone of us has d. kriesel a brief introduction to neural networks predict future values direct prediction is better extent input period appendix b excursus neural networks used for prediction dkriesel.com predictor xt xt xt xt predictor figure representation of the two-step-ahead prediction. attempt to predict the second future value out of a past value series by means of a second predictor and the involvement of an already predicted value. xt xt xt xt predictor figure representation of the direct two-step-ahead prediction. here the second time step is predicted directly the first one is omitted. technically it does not differ from a one-step-ahead prediction. d. kriesel a brief introduction to neural networks o o j j e e use information outside of time series dkriesel.com remarks on the prediction of share prices already spent a lot of time on the highway because he forgot the beginning of the holidays. heterogeneous prediction another prediction approach would be to predict the future values of a single time series out of several time series if it is assumed that the additional time series is related to the future of the first one one-step-ahead prediction fig. on the following page. if we want to predict two outputs of two related time series it is certainly possible to perform two parallel one-step-ahead predictions this is done very often because otherwise the equations would become very confusing or in case of the neural networks an additional output neuron is attached and the knowledge of both time series is used for both outputs on the next page. you ll find more and more general material on time series in remarks on the prediction of share prices many people observe the changes of a share price in the past and try to conclude the future from those values in order to benefit from this knowledge. share prices are discontinuous and therefore they are principally difficult functions. furthermore the functions can only be used for discrete values often for example in a daily rhythm the maximum and minimum values per day if we are lucky with the daily variations certainly being eliminated. but this makes the whole thing even more difficult. there are chartists i.e. people who look at many diagrams and decide by means of a lot of background knowledge and decade-long experience whether the equities should be bought or not often they are very successful. apart from the share prices it is very interesting to predict the exchange rates of currencies if we exchange euros into dollars the dollars into pounds and the pounds back into euros it could be possible that we will finally receive euros. but once found out we would do this more often and thus we would change the exchange rates into a state in which such an increasing circulation would no longer be possible we could produce money by generating so to speak a financial perpetual motion machine. at the stock exchange successful stock and currency brokers raise or lower their thumbs and thereby indicate whether in their opinion a share price or an exchange rate will increase or decrease. mathematically speaking they indicate the first bit of the first derivative of the exchange rate. in that way excellent worldclass brokers obtain success rates of about in great britain the heterogeneous onestep-ahead prediction was successfully d. kriesel a brief introduction to neural networks appendix b excursus neural networks used for prediction dkriesel.com xt xt xt xt predictor yt yt yt yt figure representation of the heterogeneous one-step-ahead prediction. prediction of a time series under consideration of a second one. xt xt xt xt yt yt yt yt predictor figure heterogeneous one-step-ahead prediction of two time series at the same time. d. kriesel a brief introduction to neural networks k k k k dkriesel.com remarks on the prediction of share prices again and again some software appears which uses scientific key words such as neural networks to purport that it is capable to predict where share prices are going. do not buy such software! in addition to the aforementioned scientific exclusions there is one simple reason for this if these tools work why should the manufacturer sell them? normally useful economic knowledge is kept secret. if we knew a way to definitely gain wealth by means of shares we would earn our millions by using this knowledge instead of selling it for euros wouldn t we? used to increase the accuracy of such predictions to in addition to the time series of the values indicators such as the oil price in rotterdam or the us national debt were included. this is just an example to show the magnitude of the accuracy of stock-exchange evaluations since we are still talking only about the first bit of the first derivation! we still do not know how strong the expected increase or decrease will be and also whether the effort will pay off probably one wrong prediction could nullify the profit of one hundred correct predictions. how can neural networks be used to predict share prices? intuitively we assume that future share prices are a function of the previous share values. but this assumption is wrong share prices are no function of their past values but a function of their assumed future value. we do not buy shares because their values have been increased during the last days but because we believe that they will futher increase tomorrow. if as a consequence many people buy a share they will boost the price. therefore their assumption was right a self-fulfilling prophecy has been generated a phenomenon long known in economics. the same applies the other way around we sell shares because we believe that tomorrow the prices will decrease. this will beat down the prices the next day and generally even more the day after the next. share price function of assumed future value! d. kriesel a brief introduction to neural networks appendix c excursus reinforcement learning what if there were no training samples but it would nevertheless be possible to evaluate how well we have learned to solve a problem? let us examine a learning paradigm that is situated between supervised and unsupervised learning. i now want to introduce a more exotic approach of learning just to leave the usual paths. we know learning procedures in which the network is exactly told what to do i.e. we provide exemplary output values. we also know learning procedures like those of the self-organizing maps into which only input values are entered. now we want to explore something inbetween the learning paradigm of reinforcement learning reinforcement learning according to sutton and barto reinforcement learning in itself is no neural network but only one of the three learning paradigms already mentioned in chapter in some sources it is counted among the supervised learning procedures since a feedback is given. due to its very rudimentary feedback it is reasonable to separate it from the supervised learning procedures apart from the fact that there are no training samples at all. no samples but feedback from cognitive science i.e. term reinforcement while it is generally known that procedures such as backpropagation cannot work in the human brain itself reinforcement learning is usually considered as being biologically more motivated. learning the and comes psychology and it describes the learning system of carrot and stick which occurs everywhere in nature learning by means of good or bad experience reward and punishment. but there is no learning aid that exactly explains what we have to do we only receive a total result for a process we win the game of chess or not? and how sure was this victory? but no results for the individual intermediate steps. for example if we ride our bike with worn tires and at a speed of exactly km h through a turn over some sand with a grain size of on the average then nobody could tell us exactly which han appendix c excursus reinforcement learning dkriesel.com dlebar angle we have to adjust or even worse how strong the great number of muscle parts in our arms or legs have to contract for this. depending on whether we reach the end of the curve unharmed or not we soon have to face the learning experience a feedback or a reward be it good or bad. thus the reward is very simple but on the other hand it is considerably easier to obtain. if we now have tested different velocities and turning angles often enough and received some rewards we will get a feel for what works and what does not. the aim of reinforcement learning is to maintain exactly this feeling. another the quasiimpossibility to achieve a sort of cost or utility function is a tennis player who tries to maximize his athletic success on the long term by means of complex movements and ballistic trajectories in the three-dimensional space including the wind direction the importance of the tournament private factors and many more. to get straight to the point since we receive only little feedback reinforcement learning often means trial and error and therefore it is very slow. example for system structure now we want to briefly discuss different sizes and components of the system. we will define them more precisely in the following sections. broadly speaking reinforcement learning represents the mutual interaction between an agent and an environmental system the agent shall solve some problem. he could for instance be an autonomous robot that shall avoid obstacles. the agent performs some actions within the environment and in return receives a feedback from the environment which in the following is called reward. this cycle of action and reward is characteristic for reinforcement learning. the agent influences the system the system provides a reward and then changes. the reward is a real or discrete scalar which describes as mentioned above how well we achieve our aim but it does not give any guidance how we can achieve it. the aim is always to make the sum of rewards as high as possible on the long term. the gridworld as a learning example for reinforcement learning i would like to use the so-called gridworld. we will see that its structure is very simple and easy to figure out and therefore reinforcement is actually not necessary. however it is very suitable for representing the approach of reinforcement learning. now let us exemplary define the individual components of the reinforcement system by means of the gridworld. later each of these components will be examined more exactly. environment the gridworld on the facing page is a simple discrete simple examplary world d. kriesel a brief introduction to neural networks dkriesel.com system structure world in two dimensions which in the following we want to use as environmental system. agent as an agent we use a simple robot being situated in our gridworld. state space as we can see our gridworld has fields with fields being unaccessible. therefore our agent can occupy positions in the grid world. these positions are regarded as states for the agent. action space the actions are still missing. we simply define that the robot could move one field up or down to the right or to the left long as there is no obstacle or the edge of our gridworld. task our agent s task is to leave the gridworld. the exit is located on the right of the light-colored field. non-determinism the two obstacles can be connected by a when the door is closed part of the illustration the corresponding field is inaccessible. the position of the door cannot change during a cycle but only between the cycles. we now have created a small world that will accompany us through the following learning strategies and illustrate them. agent und environment our aim is that the agent learns what happens by means of the reward. thus it figure a graphical representation of our gridworld. dark-colored cells are obstacles and therefore inaccessible. the exit is located on the right side of the light-colored field. the symbol marks the starting position of our agent. in the upper part of our figure the door is open in the lower part it is closed. agent reward new situation action environment figure the agent performs some actions within the environment and in return receives a reward. d. kriesel a brief introduction to neural networks agent acts in environment appendix c excursus reinforcement learning dkriesel.com is trained over of and by means of a dynamic system the environment in order to reach an aim. but what does learning mean in this context? the agent shall learn a mapping of situations to actions policy i.e. it shall learn what to do in which situation to achieve a certain aim. the aim is simply shown to the agent by giving an award for the achievement. such an award must not be mistaken for the reward on the agent s way to the solution it may sometimes be useful to receive a smaller award or a punishment when in return the longterm result is maximum to the situation when an investor just sits out the downturn of the share price or to a pawn sacrifice in a chess game. so if the agent is heading into the right direction towards the target it receives a positive reward and if not it receives no reward at all or even a negative reward the award is so to speak the final sum of all rewards which is also called return. after having colloquially named all the basic components we want to discuss more precisely which components can be used to make up our abstract reinforcement learning system. in the gridworld in the gridworld the agent is a simple robot that should find the exit of the gridworld. the environment is the gridworld itself which is a discrete gridworld. definition in reinforcement learning the agent can be formally described as a mapping of the situation space s into the action space ast. the meaning of situations st will be defined later and should only indicate that the action space depends on the current situation. agent s ast definition the environment represents a stochastic mapping of an action a in the current situation st to a reward rt and a new situation environment s a ps rt states situations and actions as already mentioned an agent can be in different states in case of the gridworld for example it can be in different positions we get a two-dimensional state vector. for an agent is ist not always possible to realize all information about its current state so that we have to introduce the term situation. a situation is a state from the agent s point of view i.e. only a more or less precise approximation of a state. therefore situations generally do not allow to clearly successor situations even with a completely deterministic system this may not be applicable. if we knew all states and the transitions between them exactly the complete system it would be possible to plan optimally and also easy to find an optimal d. kriesel a brief introduction to neural networks dkriesel.com system structure policy are provided for example by dynamic programming. now we know that reinforcement learning is an interaction between the agent and the system including actions at and situations st. the agent cannot determine by itself whether the current situation is good or bad this is exactly the reason why it receives the said reward from the environment. in the gridworld states are positions where the agent can be situated. simply said the situations equal the states in the gridworld. possible actions would be to move towards north south east or west. situation and action can be vectorial the reward is always a scalar an extreme case even only a binary value since the aim of reinforcement learning is to get along with little feedback. a complex vectorial reward would equal a real teaching input. by the way the cost function should be minimized which would not be possible however with a vectorial reward since we do not have any intuitive order relations in multi-dimensional space i.e. we do not directly know what is better or worse. definition within its environment the agent is in a state. states contain any information about the agent within the environmental system. thus it is theoretically possible to clearly predict a successor state to a performed action within a deterministic system out of this godlike state knowledge. definition situations st at time t of a situation space s are the agent s limited approximate knowledge about its state. this approximation which the agent cannot even know how good it is makes clear predictions impossible. definition actions at can be performed by the agent it could be possible that depending on the situation another action space as ex- ists. they cause state transitions and therefore a new situation from the agent s point of view. reward and return as in real life it is our aim to receive an award that is as high as possible i.e. to maximize the sum of the expected rewards r called return r on the long term. for finitely many time the rewards can simply be added rt x rtx certainly the return is only estimated here we knew all rewards and therefore the return completely it would no longer be necessary to learn. definition a reward rt is a scalar real or discrete sometimes only binary reward or punishment which in practice only finitely many time steps will be possible even though the formulas are stated with an infinite sum in the first place d. kriesel a brief introduction to neural networks appendix c excursus reinforcement learning dkriesel.com the environmental system returns to the agent as reaction to an action. definition the return rt is the accumulation of all received rewards until time t. dealing with long periods of time however not every problem has an explicit target and therefore a finite sum our agent can be a robot having the task to drive around again and again and to avoid obstacles. in order not to receive a diverging sum in case of an infinite series of reward estimations a weakening factor is used which weakens the influence of future rewards. this is not only useful if there exists no target but also if the target is very far away rt x x the farther the reward is away the smaller is the influence it has in the agent s decisions. another possibility to handle the return sum would be a limited time horizon so that only many following rewards rt are regarded rt x x the thus we divide into episodes. usually one of the two methods is used to limit the sum if not both methods together. timeline as in daily living we try to approximate our current situation to a desired state. since it is not mandatory that only the next expected reward but the expected total sum decides what the agent will do it is also possible to perform actions that on short notice result in a negative reward the pawn sacrifice in a chess game but will pay off later. the policy after having considered and formalized some system components of reinforcement learning the actual aim is still to be discussed during reinforcement learning the agent learns a policy s pa thus it continuously adjusts a mapping of the situations to the probabilities pa with which any action a is performed in any situation s. a policy can be defined as a strategy to select actions that would maximize the reward in the long term. in the gridworld in the gridworld the policy is the strategy according to which the agent tries to exit the gridworld. definition the policy s a mapping of situations to probabilities d. kriesel a brief introduction to neural networks dkriesel.com system structure to perform every action out of the action space. so it can be formalized as s pa. basically we distinguish between two policy paradigms an open loop policy represents an open control chain and creates out of an initial situation a sequence of actions with ai aisi i thus in the beginning the agent develops a plan and consecutively executes it to the end without considering the intermediate situations ai aisi actions after do not depend on the situations. in the gridworld in the gridworld an open-loop policy would provide a precise direction towards the exit such as the way from the given starting position to abbreviations of the directions eeeen. so an open-loop policy is a sequence of actions without interim feedback. a sequence of actions is generated out of a starting situation. if the system is known well and truly such an open-loop policy can be used successfully and lead to useful results. but for example to know the chess game well and truly it would be necessary to try every possible move which would be very time-consuming. thus for such problems we have to find an alternative to the open-loop policy which incorporates the current situations into the action plan a closed loop policy is a closed loop a function si ai with ai aisi in a manner of speaking. here the environment influences our action or the agent responds to the input of the environment respectively as already illustrated in fig. a closed-loop policy so to speak is a reactive plan to map current situations to actions to be performed. in the gridworld a closed-loop policy would be responsive to the current position and choose the direction according to the action. in particular when an obstacle appears dynamically such a policy is the better choice. when selecting the actions to be performed again two basic strategies can be examined. exploitation vs. exploration as in real life during reinforcement learning often the question arises whether the exisiting knowledge is only willfully exploited or new ways are also explored. initially we want to discuss the two extremes a greedy policy always chooses the way of the highest reward that can be determined in advance i.e. the way of the highest known reward. this policy represents the exploitation approach and is very promising when the used system is already known. in contrast to the exploitation approach it is the aim of the exploration approach to explore a system as detailed as possible so that also such paths leading to the target can be found which may be not very d. kriesel a brief introduction to neural networks research or safety? appendix c excursus reinforcement learning dkriesel.com promising at first glance but are in fact very successful. let us assume that we are looking for the way to a restaurant a safe policy would be to always take the way we already know not matter how unoptimal and long it may be and not to try to explore better ways. another approach would be to explore shorter ways every now and then even at the risk of taking a long time and being unsuccessful and therefore finally having to take the original way and arrive too late at the restaurant. in reality often a combination of both methods is applied in the beginning of the learning process it is researched with a higher probability while at the end more existing knowledge is exploited. here a static probability distribution is also possible and often applied. in the gridworld for finding the way in the gridworld the restaurant example applies equally. learning process let us again take a look at daily life. actions can lead us from one situation into different subsituations from each subsituation into further sub-subsituations. in a sense we get a situation tree where links between the nodes must be considered there are several ways to reach a situation so the tree could more accurately be referred to as a situation graph. he leaves of such a tree are the end situations of the system. the exploration approach would search the tree as thoroughly as possible and become acquainted with all leaves. the exploitation approach would unerringly go to the best known leave. analogous to the situation tree we also can create an action tree. here the rewards for the actions are within the nodes. now we have to adapt from daily life how we learn exactly. rewarding strategies interesting and very important is the question for what a reward and what kind of reward is awarded since the design of the reward significantly controls system behavior. as we have seen above there generally are as in daily life various actions that can be performed in any situation. there are different strategies to evaluate the selected situations and to learn which series of actions would lead to the target. first of all this principle should be explained in the following. we now want to indicate some extreme cases as design examples for the reward a rewarding similar to the rewarding in a chess game is referred to as pure delayed reward we only receive the reward at the end of and not during the game. this method is always advantageous when we finally can say whether we were succesful or not but the interim steps do not allow d. kriesel a brief introduction to neural networks dkriesel.com learning process an estimation of our situation. if we win then rt t as well as r if we lose then r with this rewarding strategy a reward is only returned by the leaves of the situation tree. pure negative reward here rt t this system finds the most rapid way to reach the target because this way is automatically the most favorable one in respect of the reward. the agent receives punishment for anything it does even if it does nothing. as a result it is the most inexpensive method for the agent to reach the target fast. another strategy is the avoidance strategy harmful situations are avoided. here rt most situations do not receive any reward only a few of them receive a negative reward. the agent agent will avoid getting too close to such negative situations warning rewarding strategies can have unexpected consequences. a robot that is told it your own way but if you touch an obstacle you will be punished will simply stand still. if standing still is also punished it will drive in small circles. reconsidering this we will understand that this behavior optimally fulfills the return of the robot but unfortunately was not intended to do so. furthermore we can show that especially small tasks can be solved better by means of negative rewards while positive more differentiated rewards are useful for large complex tasks. for our gridworld we want to apply the pure negative reward strategy the robot shall find the exit as fast as possible. the state-value function evaluation unlike our agent we have a godlike view state of our gridworld so that we can swiftly determine which robot starting position can provide which optimal return. in figure on the next page these optimal returns are applied per field. in the gridworld the state-value function for our gridworld exactly represents such a function per situation position with the difference being that here the function is unknown and has to be learned. thus we can see that it would be more practical for the robot to be capable to evaluate the current and future situations. so let us take a look at another system component of reinforcement learning the state-value function v which with regard to a policy is often called v because whether a situation is bad often depends on the general behavior of the agent. a situation being bad under a policy that is searching risks and checking out limits d. kriesel a brief introduction to neural networks appendix c excursus reinforcement learning dkriesel.com figure representation of each optimal return per field in our gridworld by means of pure negative reward awarding at the top with an open and at the bottom with a closed door. would be for instance if an agent on a bicycle turns a corner and the front wheel begins to slide out. and due to its daredevil policy the agent would not brake in this situation. with a risk-aware policy the same situations would look much better thus it would be evaluated higher by a good state-value function v simply returns the value the current situation s has for the agent under policy abstractly speaking according to the above definitions the value of the statevalue function corresponds to the return rt expected value of a situation st. v e denotes the set of the expected returns under and the current situation st. v e st definition function. the state-value function v has the task of determining the value of situations under a policy i.e. to answer the agent s question of whether a situation s is good or bad or how good or bad it is. for this purpose it returns the expectation of the return under the situation v e st the optimal state-value function is called v unfortunaely unlike us our robot does not have a godlike view of its environment. it does not have a table with optimal returns like the one shown above to orient itself. the aim of reinforcement learning is that the robot generates its state-value function bit by bit on the basis of the returns of many trials and approximates the optimal state-value function v there is one. in this context i want to introduce two terms closely related to the cycle between state-value function and policy policy evaluation policy evaluation is the approach to try a policy a few times to provide many rewards that way and to gradually accumulate a state-value function by means of these rewards. d. kriesel a brief introduction to neural networks dkriesel.com learning process v v figure the cycle of reinforcement learning which ideally leads to optimal and v policy improvement policy improvement means to improve a policy itself i.e. to turn it into a new and better one. in order to improve the policy we have to aim at the return finally having a larger value than before i.e. until we have found a shorter way to the restaurant and have walked it successfully the principle of reinforcement learning is to realize an interaction. it is tried to evaluate how good a policy is in individual situations. the changed state-value function provides information about the system with which we again improve our policy. these two values lift each other which can mathematically be proved so that the final result is an optimal policy and an optimal state-value function v this cycle sounds simple but is very timeconsuming. at first let us regard a simple random policy by which our robot could slowly fulfill and improve its state-value function without any previous knowledge. monte carlo method the easiest approach to accumulate a state-value function is mere trial and error. thus we select a randomly behaving policy which does not consider the accumulated state-value function for its random decisions. it can be proved that at some point we will find the exit of our gridworld by chance. inspired by random-based games of chance this approach is called monte carlo method. if we additionally assume a pure negative reward it is obvious that we can receive an optimum value of for our starting field in the state-value function. depending on the random way the random policy takes values other than can occur for the starting field. intuitively we want to memorize only the better value for one state one field. but here caution is advised in this way the learning procedure would work only with deterministic systems. our door which can be open or closed during a cycle would produce oscillations for all fields and such oscillations would influence their shortest way to the target. with the monte carlo method we prefer to use the learning v v v in which the update of the state-value function is obviously influenced by both the the learning rule is among others derived by means of the bellman equation but this derivation is not discussed in this chapter. d. kriesel a brief introduction to neural networks i i appendix c excursus reinforcement learning dkriesel.com old state value and the received return is the learning rate. thus the agent gets some kind of memory new findings always change the situation value just a little bit. an exemplary learning step is shown in fig. in this example the computation of the state value was applied for only one single state initial state. it should be obvious that it is possible often done to train the values for the states visited inbetween case of the gridworld our ways to the target at the same time. the result of such a calculation related to our example is illustrated in fig. on the facing page. the monte carlo method seems to be suboptimal and usually it is significantly slower than the following methods of reinforcement learning. but this method is the only one for which it can be mathematically proved that it works and therefore it is very useful for theoretical considerations. definition carlo learning. actions are randomly performed regardless of the state-value function and in the long term an expressive state-value function is accumulated by means of the following learning rule. v v v temporal difference learning most of the learning is the result of experiences e.g. walking or riding a bicycle figure application of the monte carlo learning rule with a learning rate of top two exemplary ways the agent randomly selects are applied with an open and one with a closed door. bottom the result of the learning rule for the value of the initial state considering both ways. due to the fact that in the course of time many different ways are walked given a random policy a very expressive statevalue function is obtained. d. kriesel a brief introduction to neural networks dkriesel.com learning process figure extension of the learning example in fig. in which the returns for intermediate states are also used to accumulate the statevalue function. here the low value on the door field can be seen very well if this state is possible it must be very positive. if the door is closed this state is impossible. evaluation q policy improvement figure we try different actions within the environment and as a result we learn and improve the policy. without getting injured not even mental skills like mathematical problem solving benefit a lot from experience and simple trial and error. thus we initialize our policy with arbitrary values we try learn and improve the policy due to experience in contrast to the monte carlo method we want to do this in a more directed manner. just as we learn from experience to react on different situations in different ways the temporal difference learning td learning does the same by training v the agent learns to estimate which situations are worth a lot and which are not. again the current situation is identified with st the following situations with and so on. thus the learning formula for the state-value function v is v v v change of previous value we can see that the change in value of the current situation st which is proportional to the learning rate is influenced by the received reward the previous return weighted with a factor of the following situation v the previous value of the situation v unlike definition difference learning. the monte carlo method td learning looks ahead by regarding the following situation thus the learning rule is given by v v v change of previous value the action-value function analogous v to the state-value function the action-value function action evaluation d. kriesel a brief introduction to neural networks a a appendix c excursus reinforcement learning dkriesel.com q learning this implies q a as learning fomula for the action-value function and analogously to td learning its application is called q learning figure exemplary values of an actionvalue function for the position moving right one remains on the fastest way towards the target moving up is still a quite fast way moving down is not a good way at all that the door is open for all cases. q a is another system component of q reinforcement learning which evaluates a certain action a under a certain situation s and the policy in the gridworld in the gridworld the action-value function tells us how good it is to move from a certain field into a certain direction definition function. like the state-value function the actionvalue function q a evaluates certain actions on the basis of certain situations under a policy. the optimal action-value function is called q a. q as shown in fig. the actions are performed until a target situation referred to as s is achieved there exists a target situation otherwise the actions are simply performed again and again. qst anew a max a qst a a greedy strategy change of previous value again we break down the change of the current action value to the learning rate under the current situation. it is influenced by the received reward the maximum action over the following actions weighted with a greedy strategy is applied since it can be assumed that the best known action is selected. with td learning on the other hand we do not mind to always get into the best known next situation. the previous value of the action under our situation st known as qst a that this is also weighted by means of usually the action-value function learns considerably faster than the state-value function. but we must not disregard that reinforcement learning is generally quite slow the system has to find out itself what is good. but the advantage of q d. kriesel a brief introduction to neural networks dkriesel.com example applications gfed gfed direction of actions gfed a r direction of reward s onml hijk a r gfed figure actions are performed until the desired target situation is achieved. attention should be paid to numbering rewards are numbered beginning with actions and situations beginning with has simply been adopted as a convention. qst anew a max a a qst a. played backgammon knows that the situation space is huge situations. as a result the state-value functions cannot be computed explicitly in the late eighties when td gammon was introduced. the selected rewarding strategy was the pure delayed reward i.e. the system receives the reward not before the end of the game and at the same time the reward is the return. then the system was allowed to practice itself against a backgammon program then against an entity of itself. the result was that it achieved the highest ranking in a computer-backgammon league and strikingly disproved the theory that a computer programm is not capable to master a task better than its programmer. learning is can be initialized arbitrarily and by means of q learning the result is always q definition learning. q learning trains the action-value function by means of the learning rule and thus finds q in any case. example applications td gammon the car in the pit td gammon is a very successful backgammon game based on td learning invented by gerald tesauro. the situation here is the current configuration of the board. anyone who has ever let us take a look at a car parking on a one-dimensional road at the bottom of a deep pit without being able to get over the slope on both sides straight away by means of its engine power in order to leave d. kriesel a brief introduction to neural networks k k k k k k l l h h appendix c excursus reinforcement learning dkriesel.com the pit. trivially the executable actions here are the possibilities to drive forwards and backwards. the intuitive solution we think of immediately is to move backwards to gain momentum at the opposite slope and oscillate in this way several times to dash out of the pit. the actions of a reinforcement learning system would be throttle forward reverse and nothing. here costs would be a good choice for awarding the reward so that the system learns fast how to leave the pit and realizes that our problem cannot be solved by means of mere forward directed engine power. so the system will slowly build up the movement. the policy can no longer be stored as a table since the state space is hard to discretize. as policy a function has to be generated. the pole balancer the pole balancer was developed by barto sutton and anderson. let be given a situation including a vehicle that is capable to move either to the right at full throttle or to the left at full throttle bang control. only these two actions can be performed standing still is impossible. on the top of this car is hinged an upright pole that could tip over to both sides. the pole is built in such a way that it always tips over to one side so it never stands still us assume that the pole is rounded at the lower end. the angle of the pole relative to the vertical line is referred to as furthermore the vehicle always has a fixed position x an our one-dimensional world and a velocity of x. our one-dimensional world is limited i.e. there are maximum values and minimum values x can adopt. the aim of our system is to learn to steer the car in such a way that it can balance the pole to prevent the pole from tipping over. this is achieved best by an avoidance strategy as long as the pole is balanced the reward is if the pole tips over the reward is interestingly the system is soon capable to keep the pole balanced by tilting it sufficiently fast and with small movements. at this the system mostly is in the center of the space since this is farthest from the walls which it understands as negative it touches the wall the pole will tip over. swinging up an inverted pendulum more difficult for the system is the following initial situation the pole initially hangs down has to be swung up over the vehicle and finally has to be stabilized. in the literature this task is called swing up an inverted pendulum. d. kriesel a brief introduction to neural networks dkriesel.com reinforcement learning in connection with neural networks ment learning to find a strategy in order to exit a maze as fast as possible. what could an appropriate state value function look like? how would you generate an appropri ate reward? assume that the robot is capable to avoid obstacles and at any time knows its position y and orientation exercise describe the function of the two components ase and ace as they have been proposed by barto sutton and anderson to control the pole balancer. bibliography exercise indicate several problems of informatics which could be solved efficiently by means of reinforcement learning. please give reasons for your answers. reinforcement learning in connection with neural networks finally the reader would like to ask why a text on networks includes a chapter about reinforcement learning. the answer is very simple. we have already been introduced to supervised and unsupervised learning procedures. although we do not always have an omniscient teacher who makes unsupervised learning possible this does not mean that we do not receive any feedback at all. there is often something in between some kind of criticism or school mark. problems like this can be solved by means of reinforcement learning. but not every problem is that easily solved like our gridworld in our backgammon example we have approx. situations and the situation tree has a large branching factor let alone other games. here the tables used in the gridworld can no longer be realized as state- and action-value functions. thus we have to find approximators for these functions. and which learning approximators for these reinforcement learning components come immediately into our mind? exactly neural networks. exercises exercise a robot control system shall be persuaded by means of reinforce d. kriesel a brief introduction to neural networks bibliography james a. anderson. a simple neural network generating an interactive memory. mathematical biosciences d. anguita g. parodi and r. zunino. speed improvement of the backpropagation on current-generation workstations. in wcnn portland world congress on neural networks july oregon convention center portland oregon volume lawrence erlbaum a. barto r. sutton and c. anderson. neuron-like adaptive elements that can solve difficult learning control problems. ieee transactions on systems man and cybernetics september g. a. carpenter and s. grossberg. self-organization of stable category recognition codes for analog input patterns. applied optics m.a. cohen and s. grossberg. absolute stability of global pattern formation and parallel memory storage by competitive neural networks. computer society press technology series neural networks pages g. a. carpenter and s. grossberg. art hierarchical search using chemical transmitters in self-organising pattern recognition architectures. neural networks t. cover and p. hart. nearest neighbor pattern classification. transactions on information theory n.a. campbell and jb reece. biologie. spektrum. akademischer verlag g. cybenko. approximation by superpositions of a sigmoidal function. mathematics of control signals and systems r.o. duda p.e. hart and d.g. stork. pattern classification. wiley new york ieee bibliography dkriesel.com jeffrey l. elman. finding structure in time. cognitive science april s. e. fahlman. an empirical sudy of learning speed in back-propagation networks. technical report cmu k. fukushima s. miyake and t. ito. neocognitron a neural network model for a mechanism of visual pattern recognition. ieee transactions on systems man and cybernetics septemberoctober b. fritzke. fast learning with incremental rbf networks. neural processing letters n. goerke f. kintzler and r. eckmiller. self organized classification of chaotic domains from a nonlinearattractor. in neural networks proceedings. ijcnn international joint conference on volume n. goerke f. kintzler and r. eckmiller. self organized partitioning of chaotic attractors for control. lecture notes in computer science pages s. grossberg. adaptive pattern classification and universal recoding i parallel development and coding of neural feature detectors. biological cybernetics nils goerke and alexandra scherbart. classification using multi-soms and multi-neural gas. in ijcnn pages donald o. hebb. the organization of behavior a neuropsychological theory. wiley new york john j. hopfield. neural networks and physical systems with emergent collective computational abilities. proc. of the national academy of science usa jj hopfield. neurons with graded response have collective computational properties like those of two-state neurons. proceedings of the national academy of sciences jj hopfield and dw tank. neural computation of decisions in optimization problems. biological cybernetics m. i. jordan. attractor dynamics and parallelism in a connectionist sequential machine. in proceedings of the eighth conference of the cognitive science society pages erlbaum d. kriesel a brief introduction to neural networks dkriesel.com bibliography l. kaufman. finding groups in data an introduction to cluster analysis. in finding groups in data an introduction to cluster analysis. wiley new york t. kohonen. correlation matrix memories. ieeetc teuvo kohonen. self-organized formation of topologically correct feature maps. biological cybernetics teuvo kohonen. self-organization and associative memory. springerverlag berlin third edition t. kohonen. the self-organizing map. neurocomputing e.r. kandel j.h. schwartz and t.m. jessell. principles of neural science. appleton lange y. le cun j. s. denker and s. a. solla. optimal brain damage. in d. touretzky editor advances in neural information processing systems pages morgan kaufmann j. macqueen. some methods for classification and analysis of multivariate observations. in proceedings of the fifth berkeley symposium on mathematics statistics and probability vol. pages thomas m. martinetz stanislav g. berkovich and klaus j. schulten. neural-gas network for vector quantization and its application to timeseries prediction. ieee trans. on neural networks k.d. micheva b. busse n.c. weiler n. o rourke and s.j. smith. singlesynapse analysis of a diverse synapse population proteomic imaging methods and markers. neuron w.s. mcculloch and w. pitts. a logical calculus of the ideas immanent in nervous activity. bulletin of mathematical biology m. minsky and s. papert. perceptrons. mit press cambridge mass j. l. mcclelland and d. e. rumelhart. parallel distributed processing explorations in the microstructure of cognition volume mit press cambridge d. kriesel a brief introduction to neural networks bibliography dkriesel.com david r. parker. optimal algorithms for adaptive networks second order back propagation second order direct propagation and second order hebbian learning. in maureen caudill and charles butler editors ieee first international conference on neural networks volume ii pages ii ii san diego ca june ieee. t. poggio and f. girosi. a theory of networks for approximation and learning. mit press cambridge mass. f. j. pineda. generalization of back-propagation to recurrent neural networks. physical review letters w. pitts and w.s. mcculloch. how we know universals the perception of auditory and visual forms. bulletin of mathematical biology l. prechelt. a set of neural network benchmark problems and benchmarking rules. technical report m. riedmiller and h. braun. a direct adaptive method for faster backpropagation learning the rprop algorithm. in neural networks ieee international conference on pages ieee g. roth and u. dicke. evolution of the brain and intelligence. trends in cognitive sciences d. rumelhart g. hinton and r. williams. learning representations by back-propagating errors. nature october david e. rumelhart geoffrey e. hinton and r. j. williams. learning internal representations by error propagation. in d. e. rumelhart j. l. mcclelland and the pdp research group. editors parallel distributed processing explorations in the microstructure of cognition volume foundations. mit press m. riedmiller. rprop description and implementation details. technical report university of karlsruhe f. rosenblatt. the perceptron a probabilistic model for information storage and organization in the brain. psychological review f. rosenblatt. principles of neurodynamics. spartan new york r. s. sutton and a. g. barto. reinforcement learning an introduction. mit press cambridge ma d. kriesel a brief introduction to neural networks dkriesel.com bibliography a. scherbart and n. goerke. unsupervised system for discovering patterns in time-series rolf schatten nils goerke and rolf eckmiller. regional and online learnable fields. in sameer singh maneesha singh chidanand apt and petra perner editors icapr volume of lecture notes in computer science pages springer k. steinbuch. die lernmatrix. kybernetik cybernetics c. von der malsburg. self-organizing of orientation sensitive cells in striate cortex. kybernetik p. d. wasserman. neural computing theory and practice. new york van nostrand reinhold p. j. werbos. beyond regression new tools for prediction and analysis in the behavioral sciences. phd thesis harvard university p. j. werbos. backpropagation past and future. in proceedings san diego pages a.s. weigend and n.a. gershenfeld. time series prediction. addisonwesley b. widrow and m. e. hoff. adaptive switching circuits. in proceedings wescon pages r. widner. single-stage logic. aiee fall general meeting wasserman p. neural computing theory and practice van nostrand reinhold andreas zell. simulation neuronaler netze. addison-wesley german. d. kriesel a brief introduction to neural networks list of figures robot with sensors and motors black box with eight inputs and two outputs learning samples for the example robot institutions of the field of neural networks central nervous system brain biological neuron action potential compound eye data processing of a neuron various popular activation functions feedforward network feedforward network with shortcuts directly recurrent network indirectly recurrent network laterally recurrent network completely linked network examples for different types of neurons example network with and without bias neuron training samples and network capacities learning curve with different scalings gradient descent visualization possible errors during a gradient descent the problem checkerboard problem the perceptron in three different views singlelayer perceptron singlelayer perceptron with several output neurons and and or singlelayer perceptron list of figures dkriesel.com error surface of a network with connections sketch of a xor-slp two-dimensional linear separation three-dimensional linear separation the xor network multilayer perceptrons and output sets position of an inner neuron for derivation of backpropagation illustration of the backpropagation derivation momentum term fermi function and hyperbolic tangent functionality of encoding rbf network distance function in the rbf network individual gaussian bells in one- and two-dimensional space accumulating gaussian bells in one-dimensional space accumulating gaussian bells in two-dimensional space even coverage of an input space with radial basis functions uneven coverage of an input space with radial basis functions random uneven coverage of an input space with radial basis functions roessler attractor jordan network elman network unfolding in time hopfield network binary threshold function convergence of a hopfield network fermi function examples for quantization example topologies of a som example distances of som topologies som topology functions first example of a som topological defect of a som training a som with one-dimensional topology soms with one- and two-dimensional topologies and different input resolution optimization of a som to certain areas d. kriesel a brief introduction to neural networks dkriesel.com list of figures shape to be classified by neural gas structure of an art network learning process of an art network comparing cluster analysis methods rolf neuron clustering by means of a rolf neural network reading time series one-step-ahead prediction two-step-ahead prediction direct two-step-ahead prediction heterogeneous one-step-ahead prediction heterogeneous one-step-ahead prediction with two outputs gridworld reinforcement learning gridworld with optimal returns reinforcement learning cycle the monte carlo method extended monte carlo method improving the policy action-value function reinforcement learning timeline d. kriesel a brief introduction to neural networks index rule atp attractor autoassociator. axon. a action action potential action space action-value function activation activation function selection of adaline see adaptive linear neuron adaptive linear element see adaptive linear neuron adaptive linear neuron. adaptive resonance theory agent. algorithm. amacrine cell approximation. art see adaptive resonance theory artificial intelligence associative data storage b backpropagation. second order backpropagation of error. recurrent bar basis bias neuron. binary threshold function bipolar cell black box. brain brainstem c capability to learn center of a rolf neuron of a som neuron. index dkriesel.com of an rbf neuron. distance to the central nervous system cerebellum cerebral cortex cerebrum change in weight. cluster analysis clusters cns see central nervous system codebook vector complete linkage. compound eye concentration gradient. cone function. connection. context-based search continuous cortex see cerebral cortex visual cortical field association primary cylinder function d dartmouth summer research deep networks delta. delta rule. dendrite tree depolarization diencephalon see interbrain difference vector see error vector digital filter digitization. discrete discretization see quantization distance euclidean squared. dynamical system e early stopping electronic brain elman network environment. episode. epoch epsilon-nearest neighboring error specific total error function specific error vector evolutionary algorithms exploitation approach exploration approach exteroceptor. f fastprop fault tolerance feedforward. fermi function flat spot elimination d. kriesel a brief introduction to neural networks dkriesel.com index fudging see flat spot elimination function approximation function approximator universal g ganglion cell. gauss-markov model gaussian bell. generalization glial cell gradient gradient descent problems grid gridworld. h heaviside function see binary threshold function hebbian rule generalized form. heteroassociator hinton diagram history of development. hopfield networks continuous horizontal cell hyperbolic tangent hyperpolarization. hypothalamus i individual eye see ommatidium input dimension input patterns input vector interbrain internodes interoceptor interpolation precise ion iris j jordan network. k k-means clustering k-nearest neighboring. l layer hidden input. output learnability learning d. kriesel a brief introduction to neural networks index dkriesel.com batch. learning o ine o ine online reinforcement supervised. unsupervised learning rate variable learning strategy learning vector quantization lens linear separability linearer associator locked-in syndrome logistic function see fermi function temperature parameter lvq see learning vector quantization m m-som see self-organizing map multi mark i perceptron mathematical symbols see time concept as see action space ep see error vector g see topology n see self-organizing map input dimension p see training set q a see action-value function q a see action-value function rt see return optimal optimal s see situation space t see temperature parameter v see state-value function v see state-value function w see weight matrix wij see change in weight see policy threshold value see momentum see weight decay see delta learning rate see rprop see rprop max rprop min see rprop ij see rprop see nabla operator see radius multiplier err see error total errw see error function errp see error specific errpwsee error function specific errwd see weight decay at see action c. center of an rbf neuron see neuron self-organizing map center m see output dimension n see input dimension p see training pattern rh see center of an rbf neuron distance to the rt see reward st see situation t see teaching input wij see weight x see input vector y see output vector d. kriesel a brief introduction to neural networks dkriesel.com index fact see activation function fout see output function membrane memorized metric. mexican hat function. mlp. perceptron multilayer momentum momentum term. monte carlo method moore-penrose pseudo inverse moving average procedure myelin sheath n nabla operator. neocognitron nervous system network input neural gas growing multi- neural network recurrent neuron accepting binary. context. fermi identity information processing input. rbf output rolf. self-organizing map. tanh winner neuron layers. layer neurotransmitters nodes of ranvier o oligodendrocytes olvq. on-neuron see bias neuron one-step-ahead prediction heterogeneous open loop learning. optimal brain damage order of activation asynchronous fixed order random order randomly permuted order topological order synchronous output dimension. output function. output vector. p parallelism pattern see training pattern pattern recognition perceptron multilayer recurrent. d. kriesel a brief introduction to neural networks index dkriesel.com singlelayer. perceptron convergence theorem perceptron learning algorithm period. peripheral nervous system persons anderson f. anderson james a. anguita barto f. carpenter gail. elman fukushima girosi grossberg stephen hebb donald o. hinton hoff marcian e. hopfield john f. ito jordan kohonen teuvo lashley karl macqueen j. martinetz thomas. mcculloch warren f. minsky marvin f. miyake nilsson nils. papert seymour parker david pitts walter f. poggio pythagoras. riedmiller martin rosenblatt frank rumelhart steinbuch karl sutton f. tesauro gerald von der malsburg christoph werbos paul widrow bernard. wightman charles. williams zuse konrad. pinhole eye pns see peripheral nervous system pole balancer policy closed loop evaluation. greedy improvement open loop pons propagation function pruning pupil q q learning quantization. quickpropagation r rbf network. growing receptive field receptor cell photo-. primary secondary d. kriesel a brief introduction to neural networks dkriesel.com index recurrence direct indirect lateral. refractory period regional and online learnable fields reinforcement learning. repolarization representability. resilient backpropagation resonance retina. return reward avoidance strategy pure delayed pure negative rms see root mean square rolfs. regional and online learnable fields root mean square rprop see resilient backpropagation s saltatory conductor. schwann cell self-fulfilling prophecy self-organizing feature maps self-organizing map. multi- sensory adaptation sensory transduction. shortcut connections. silhouette coefficient. single lense eye single shot learning situation situation space situation tree. slp see perceptron singlelayer snark snipe. sodium-potassium pump. som see self-organizing map soma spin spinal cord stability plasticity dilemma state state space forecasting. state-value function stimulus stimulus-conducting apparatus. surface perceptive. swing up an inverted pendulum. symmetry breaking synapse chemical electrical synapses. synaptic cleft t target td gammon td learning. temporal difference learning teacher forcing teaching input telencephalon see cerebrum temporal difference learning thalamus d. kriesel a brief introduction to neural networks index dkriesel.com weighted sum. widrow-hoff rule see delta rule winner-takes-all scheme. threshold potential threshold value time concept time horizon time series time series prediction. topological defect. topology topology function training pattern set of. training set. transfer functionsee activation function truncus cerebri see brainstem two-step-ahead prediction direct u unfolding in time v voronoi diagram w weight. weight matrix bottom-up top-down. weight vector d. kriesel a brief introduction to neural networks