understanding machine learning from theory to algorithms by shai shalev-shwartz and shai ben-david published by cambridge university press. this copy is for personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning please note this copy is almost but not entirely identical to the printed version of the book. in particular page numbers are not identical section numbers are the same. understandingmachinelearningmachinelearningisoneofthefastestgrowingareasofcomputersciencewithfar-reachingapplications.theaimofthistextbookistointroducemachinelearningandthealgorithmicparadigmsitoffersinaprinci-pledway.thebookprovidesanextensivetheoreticalaccountofthefundamentalideasunderlyingmachinelearningandthemathematicalderivationsthattransformtheseprinciplesintopracticalalgorithms.fol-lowingapresentationofthebasicsofthefieldthebookcoversawidearrayofcentraltopicsthathavenotbeenaddressedbyprevioustext-books.theseincludeadiscussionofthecomputationalcomplexityoflearningandtheconceptsofconvexityandstabilityimportantalgorith-micparadigmsincludingstochasticgradientdescentneuralnetworksandstructuredoutputlearningandemergingtheoreticalconceptssuchasthepac-bayesapproachandcompression-basedbounds.designedforanadvancedundergraduateorbeginninggraduatecoursethetextmakesthefundamentalsandalgorithmsofmachinelearningaccessibletostu-dentsandnonexpertreadersinstatisticscomputersciencemathematicsandengineering.shaishalev-shwartzisanassociateprofessorattheschoolofcomputerscienceandengineeringatthehebrewuniversityisrael.shaiben-davidisaprofessorintheschoolofcomputerscienceattheuniversityofwaterloocanada. understandingmachinelearningfromtheorytoalgorithmsshaishalev-shwartzthehebrewuniversityjerusalemshaiben-daviduniversityofwaterloocanada triple-sdedicatesthebooktotriple-m vii preface the term machine learning refers to the automated detection of meaningful patterns in data. in the past couple of decades it has become a common tool in almost any task that requires information extraction from large data sets. we are surrounded by a machine learning based technology search engines learn how to bring us the best results placing profitable ads anti-spam software learns to filter our email messages and credit card transactions are secured by a software that learns how to detect frauds. digital cameras learn to detect faces and intelligent personal assistance applications on smart-phones learn to recognize voice commands. cars are equipped with accident prevention systems that are built using machine learning algorithms. machine learning is also widely used in scientific applications such as bioinformatics medicine and astronomy. one common feature of all of these applications is that in contrast to more traditional uses of computers in these cases due to the complexity of the patterns that need to be detected a human programmer cannot provide an explicit finedetailed specification of how such tasks should be executed. taking example from intelligent beings many of our skills are acquired or refined through learning from our experience than following explicit instructions given to us. machine learning tools are concerned with endowing programs with the ability to learn and adapt. the first goal of this book is to provide a rigorous yet easy to follow introduction to the main concepts underlying machine learning what is learning? how can a machine learn? how do we quantify the resources needed to learn a given concept? is learning always possible? can we know if the learning process succeeded or failed? the second goal of this book is to present several key machine learning algorithms. we chose to present algorithms that on one hand are successfully used in practice and on the other hand give a wide spectrum of different learning techniques. additionally we pay specific attention to algorithms appropriate for large scale learning big data since in recent years our world has become increasingly digitized and the amount of data available for learning is dramatically increasing. as a result in many applications data is plentiful and computation time is the main bottleneck. we therefore explicitly quantify both the amount of data and the amount of computation time needed to learn a given concept. the book is divided into four parts. the first part aims at giving an initial rigorous answer to the fundamental questions of learning. we describe a generalization of valiant s probably approximately correct learning model which is a first solid answer to the question what is learning? we describe the empirical risk minimization structural risk minimization and minimum description length learning rules which shows how can a machine learn we quantify the amount of data needed for learning using the erm srm and mdl rules and show how learning might fail by deriving viii a no-free-lunch theorem. we also discuss how much computation time is required for learning. in the second part of the book we describe various learning algorithms. for some of the algorithms we first present a more general learning principle and then show how the algorithm follows the principle. while the first two parts of the book focus on the pac model the third part extends the scope by presenting a wider variety of learning models. finally the last part of the book is devoted to advanced theory. we made an attempt to keep the book as self-contained as possible. however the reader is assumed to be comfortable with basic notions of probability linear algebra analysis and algorithms. the first three parts of the book are intended for first year graduate students in computer science engineering mathematics or statistics. it can also be accessible to undergraduate students with the adequate background. the more advanced chapters can be used by researchers intending to gather a deeper theoretical understanding. acknowledgements the book is based on introduction to machine learning courses taught by shai shalev-shwartz at the hebrew university and by shai ben-david at the university of waterloo. the first draft of the book grew out of the lecture notes for the course that was taught at the hebrew university by shai shalev-shwartz during we greatly appreciate the help of ohad shamir who served as a ta for the course in and of alon gonen who served as a ta for the course in ohad and alon prepared few lecture notes and many of the exercises. alon to whom we are indebted for his help throughout the entire making of the book has also prepared a solution manual. we are deeply grateful for the most valuable work of dana rubinstein. dana has scientifically proofread and edited the manuscript transforming it from lecture-based chapters into fluent and coherent text. special thanks to amit daniely who helped us with a careful read of the advanced part of the book and also wrote the advanced chapter on multiclass learnability. we are also grateful for the members of a book reading club in jerusalem that have carefully read and constructively criticized every line of the manuscript. the members of the reading club are maya alroy yossi arjevani aharon birnbaum alon cohen alon gonen roi livni ofer meshi dan rosenbaum dana rubinstein shahar somin alon vinnikov and yoav wald. we would also like to thank gal elidan amir globerson nika haghtalab shie mannor amnon shashua nati srebro and ruth urner for helpful discussions. shai shalev-shwartz jerusalem israel shai ben-david waterloo canada contents preface page vii introduction what is learning? when do we need machine learning? types of learning relations to other fields how to read this book notation possible course plans based on this book part i foundations a gentle start a formal model the statistical learning framework empirical risk minimization something may go wrong overfitting empirical risk minimization with inductive bias exercises finite hypothesis classes a formal learning model pac learning a more general learning model releasing the realizability assumption agnostic pac learning the scope of learning problems modeled summary bibliographic remarks exercises learning via uniform convergence uniform convergence is sufficient for learnability finite classes are agnostic pac learnable understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning x contents summary bibliographic remarks exercises the bias-complexity tradeoff the no-free-lunch theorem no-free-lunch and prior knowledge error decomposition summary bibliographic remarks exercises the vc-dimension intervals finite classes infinite-size classes can be learnable the vc-dimension examples threshold functions axis aligned rectangles vc-dimension and the number of parameters the fundamental theorem of pac learning proof of theorem uniform convergence for classes of small effective size summary bibliographic remarks exercises sauer s lemma and the growth function nonuniform learnability nonuniform learnability characterizing nonuniform learnability structural risk minimization minimum description length and occam s razor occam s razor other notions of learnability consistency discussing the different notions of learnability the no-free-lunch theorem revisited summary bibliographic remarks exercises the runtime of learning computational complexity of learning contents xi finite classes formal definition boolean conjunctions learning dnf implementing the erm rule axis aligned rectangles efficiently learnable but not by a proper erm hardness of learning summary bibliographic remarks exercises part ii from theory to algorithms linear programming for the class of halfspaces perceptron for halfspaces least squares linear regression for polynomial regression tasks linear predictors halfspaces the vc dimension of halfspaces linear regression logistic regression summary bibliographic remarks exercises boosting weak learnability efficient implementation of erm for decision stumps adaboost linear combinations of base hypotheses the vc-dimension of lb t adaboost for face recognition summary bibliographic remarks exercises model selection and validation model selection using srm validation hold out set validation for model selection the model-selection curve xii contents k-fold cross validation train-validation-test split what to do if learning fails summary exercises convex learning problems convexity lipschitzness and smoothness convexity lipschitzness smoothness convex learning problems learnability of convex learning problems convex-lipschitzsmooth-bounded learning problems surrogate loss functions summary bibliographic remarks exercises regularization and stability regularized loss minimization ridge regression stable rules do not overfit tikhonov regularization as a stabilizer lipschitz loss smooth and nonnegative loss controlling the fitting-stability tradeoff summary bibliographic remarks exercises stochastic gradient descent gradient descent analysis of gd for convex-lipschitz functions subgradients calculating subgradients subgradients of lipschitz functions subgradient descent stochastic gradient descent analysis of sgd for convex-lipschitz-bounded functions variants adding a projection step variable step size other averaging techniques contents xiii strongly convex functions learning with sgd sgd for risk minimization analyzing sgd for convex-smooth learning problems sgd for regularized loss minimization summary bibliographic remarks exercises support vector machines margin and hard-svm the homogenous case the sample complexity of hard-svm soft-svm and norm regularization the sample complexity of soft-svm margin and norm-based bounds versus dimension the ramp loss implementing soft-svm using sgd optimality conditions and support vectors duality summary bibliographic remarks exercises kernel methods embeddings into feature spaces the kernel trick kernels as a way to express prior knowledge characterizing kernel functions implementing soft-svm with kernels summary bibliographic remarks exercises multiclass ranking and complex prediction problems one-versus-all and all-pairs linear multiclass predictors how to construct cost-sensitive classification erm generalized hinge loss multiclass svm and sgd structured output prediction ranking xiv contents linear predictors for ranking bipartite ranking and multivariate performance measures linear predictors for bipartite ranking summary bibliographic remarks exercises decision trees sample complexity decision tree algorithms implementations of the gain measure pruning threshold-based splitting rules for real-valued features random forests summary bibliographic remarks exercises nearest neighbor k nearest neighbors analysis a generalization bound for the rule the curse of dimensionality efficient implementation summary bibliographic remarks exercises neural networks feedforward neural networks learning neural networks the expressive power of neural networks geometric intuition the sample complexity of neural networks the runtime of learning neural networks sgd and backpropagation summary bibliographic remarks exercises part iii additional learning models online learning online classification in the realizable case contents xv online learnability online classification in the unrealizable case weighted-majority online convex optimization the online perceptron algorithm summary bibliographic remarks exercises clustering linkage-based clustering algorithms k-means and other cost minimization clusterings the k-means algorithm spectral clustering graph cut graph laplacian and relaxed graph cuts unnormalized spectral clustering information bottleneck a high level view of clustering summary bibliographic remarks exercises dimensionality reduction principal component analysis a more efficient solution for the case d m implementation and demonstration random projections compressed sensing proofs pca or compressed sensing? summary bibliographic remarks exercises generative models maximum likelihood estimator maximum likelihood estimation for continuous ran dom variables maximum likelihood and empirical risk minimization generalization analysis naive bayes linear discriminant analysis latent variables and the em algorithm xvi contents em as an alternate maximization algorithm em for mixture of gaussians k-means bayesian reasoning summary bibliographic remarks exercises feature selection and generation feature selection filters greedy selection approaches sparsity-inducing norms feature manipulation and normalization examples of feature transformations feature learning dictionary learning using auto-encoders summary bibliographic remarks exercises part iv advanced theory rademacher complexities the rademacher complexity rademacher calculus rademacher complexity of linear classes generalization bounds for svm generalization bounds for predictors with low norm bibliographic remarks covering numbers covering properties from covering to rademacher complexity via chaining bibliographic remarks proof of the fundamental theorem of learning theory the upper bound for the agnostic case the lower bound for the agnostic case showing that m showing that m the upper bound for the realizable case from to pac learnability multiclass learnability the natarajan dimension the multiclass fundamental theorem on the proof of theorem calculating the natarajan dimension one-versus-all based classes general multiclass-to-binary reductions linear multiclass predictors on good and bad erms bibliographic remarks exercises compression bounds compression bounds examples axis aligned rectangles halfspaces separating polynomials separation with margin bibliographic remarks pac-bayes pac-bayes bounds bibliographic remarks exercises appendix a technical lemmas appendix b measure concentration appendix c linear algebra notes references index contents xvii introduction the subject of this book is automated learning or as we will more often call it machine learning that is we wish to program computers so that they can learn from input available to them. roughly speaking learning is the process of converting experience into expertise or knowledge. the input to a learning algorithm is training data representing experience and the output is some expertise which usually takes the form of another computer program that can perform some task. seeking a formal-mathematical understanding of this concept we ll have to be more explicit about what we mean by each of the involved terms what is the training data our programs will access? how can the process of learning be automated? how can we evaluate the success of such a process the quality of the output of a learning program? what is learning? let us begin by considering a couple of examples from naturally occurring animal learning. some of the most fundamental issues in ml arise already in that context which we are all familiar with. bait shyness rats learning to avoid poisonous baits when rats encounter food items with novel look or smell they will first eat very small amounts and subsequent feeding will depend on the flavor of the food and its physiological effect. if the food produces an ill effect the novel food will often be associated with the illness and subsequently the rats will not eat it. clearly there is a learning mechanism in play here the animal used past experience with some food to acquire expertise in detecting the safety of this food. if past experience with the food was negatively labeled the animal predicts that it will also have a negative effect when encountered in the future. inspired by the preceding example of successful learning let us demonstrate a typical machine learning task. suppose we would like to program a machine that learns how to filter spam e-mails. a naive solution would be seemingly similar to the way rats learn how to avoid poisonous baits. the machine will simply memorize all previous e-mails that had been labeled as spam e-mails by the human user. when a new e-mail arrives the machine will search for it in the set understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning introduction of previous spam e-mails. if it matches one of them it will be trashed. otherwise it will be moved to the user s inbox folder. while the preceding learning by memorization approach is sometimes useful it lacks an important aspect of learning systems the ability to label unseen e-mail messages. a successful learner should be able to progress from individual examples to broader generalization. this is also referred to as inductive reasoning or inductive inference. in the bait shyness example presented previously after the rats encounter an example of a certain type of food they apply their attitude toward it on new unseen examples of food of similar smell and taste. to achieve generalization in the spam filtering task the learner can scan the previously seen e-mails and extract a set of words whose appearance in an e-mail message is indicative of spam. then when a new e-mail arrives the machine can check whether one of the suspicious words appears in it and predict its label accordingly. such a system would potentially be able correctly to predict the label of unseen e-mails. however inductive reasoning might lead us to false conclusions. to illustrate this let us consider again an example from animal learning. pigeon superstition in an experiment performed by the psychologist b. f. skinner he placed a bunch of hungry pigeons in a cage. an automatic mechanism had been attached to the cage delivering food to the pigeons at regular intervals with no reference whatsoever to the birds behavior. the hungry pigeons went around the cage and when food was first delivered it found each pigeon engaged in some activity turning the head etc.. the arrival of food reinforced each bird s specific action and consequently each bird tended to spend some more time doing that very same action. that in turn increased the chance that the next random food delivery would find each bird engaged in that activity again. what results is a chain of events that reinforces the pigeons association of the delivery of the food with whatever chance actions they had been performing when it was first delivered. they subsequently continue to perform these same actions what distinguishes learning mechanisms that result in superstition from useful learning? this question is crucial to the development of automated learners. while human learners can rely on common sense to filter out random meaningless learning conclusions once we export the task of learning to a machine we must provide well defined crisp principles that will protect the program from reaching senseless or useless conclusions. the development of such principles is a central goal of the theory of machine learning. what then made the rats learning more successful than that of the pigeons? as a first step toward answering this question let us have a closer look at the bait shyness phenomenon in rats. bait shyness revisited rats fail to acquire conditioning between food and electric shock or between sound and nausea the bait shyness mechanism in see httppsychclassics.yorku.caskinnerpigeon when do we need machine learning? rats turns out to be more complex than what one may expect. in experiments carried out by garcia koelling it was demonstrated that if the unpleasant stimulus that follows food consumption is replaced by say electrical shock than nausea then no conditioning occurs. even after repeated trials in which the consumption of some food is followed by the administration of unpleasant electrical shock the rats do not tend to avoid that food. similar failure of conditioning occurs when the characteristic of the food that implies nausea as taste or smell is replaced by a vocal signal. the rats seem to have some built in prior knowledge telling them that while temporal correlation between food and nausea can be causal it is unlikely that there would be a causal relationship between food consumption and electrical shocks or between sounds and nausea. we conclude that one distinguishing feature between the bait shyness learning and the pigeon superstition is the incorporation of prior knowledge that biases the learning mechanism. this is also referred to as inductive bias. the pigeons in the experiment are willing to adopt any explanation for the occurrence of food. however the rats know that food cannot cause an electric shock and that the co-occurrence of noise with some food is not likely to affect the nutritional value of that food. the rats learning process is biased toward detecting some kind of patterns while ignoring other temporal correlations between events. it turns out that the incorporation of prior knowledge biasing the learning process is inevitable for the success of learning algorithms is formally stated and proved as the no-free-lunch theorem in chapter the development of tools for expressing domain expertise translating it into a learning bias and quantifying the effect of such a bias on the success of learning is a central theme of the theory of machine learning. roughly speaking the stronger the prior knowledge prior assumptions that one starts the learning process with the easier it is to learn from further examples. however the stronger these prior assumptions are the less flexible the learning is it is bound a priori by the commitment to these assumptions. we shall discuss these issues explicitly in chapter when do we need machine learning? when do we need machine learning rather than directly program our computers to carry out the task at hand? two aspects of a given problem may call for the use of programs that learn and improve on the basis of their experience the problem s complexity and the need for adaptivity. tasks that are too complex to program. tasks performed by animalshumans there are numerous tasks that we human beings perform routinely yet our introspection concerning how we do them is not sufficiently elaborate to extract a well introduction defined program. examples of such tasks include driving speech recognition and image understanding. in all of these tasks state of the art machine learning programs programs that learn from their experience achieve quite satisfactory results once exposed to sufficiently many training examples. tasks beyond human capabilities another wide family of tasks that benefit from machine learning techniques are related to the analysis of very large and complex data sets astronomical data turning medical archives into medical knowledge weather prediction analysis of genomic data web search engines and electronic commerce. with more and more available digitally recorded data it becomes obvious that there are treasures of meaningful information buried in data archives that are way too large and too complex for humans to make sense of. learning to detect meaningful patterns in large and complex data sets is a promising domain in which the combination of programs that learn with the almost unlimited memory capacity and ever increasing processing speed of computers opens up new horizons. adaptivity. one limiting feature of programmed tools is their rigidity once the program has been written down and installed it stays unchanged. however many tasks change over time or from one user to another. machine learning tools programs whose behavior adapts to their input data offer a solution to such issues they are by nature adaptive to changes in the environment they interact with. typical successful applications of machine learning to such problems include programs that decode handwritten text where a fixed program can adapt to variations between the handwriting of different users spam detection programs adapting automatically to changes in the nature of spam e-mails and speech recognition programs. types of learning learning is of course a very wide domain. consequently the field of machine learning has branched into several subfields dealing with different types of learning tasks. we give a rough taxonomy of learning paradigms aiming to provide some perspective of where the content of this book sits within the wide field of machine learning. we describe four parameters along which learning paradigms can be classified. supervised versus unsupervised since learning involves an interaction between the learner and the environment one can divide learning tasks according to the nature of that interaction. the first distinction to note is the difference between supervised and unsupervised learning. as an types of learning illustrative example consider the task of learning to detect spam e-mail versus the task of anomaly detection. for the spam detection task we consider a setting in which the learner receives training e-mails for which the label spamnot-spam is provided. on the basis of such training the learner should figure out a rule for labeling a newly arriving e-mail message. in contrast for the task of anomaly detection all the learner gets as training is a large body of e-mail messages no labels and the learner s task is to detect unusual messages. more abstractly viewing learning as a process of using experience to gain expertise supervised learning describes a scenario in which the experience a training example contains significant information the spamnot-spam labels that is missing in the unseen test examples to which the learned expertise is to be applied. in this setting the acquired expertise is aimed to predict that missing information for the test data. in such cases we can think of the environment as a teacher that supervises the learner by providing the extra information in unsupervised learning however there is no distinction between training and test data. the learner processes input data with the goal of coming up with some summary or compressed version of that data. clustering a data set into subsets of similar objets is a typical example of such a task. there is also an intermediate learning setting in which while the training examples contain more information than the test examples the learner is required to predict even more information for the test examples. for example one may try to learn a value function that describes for each setting of a chess board the degree by which white s position is better than the black s. yet the only information available to the learner at training time is positions that occurred throughout actual chess games labeled by who eventually won that game. such learning frameworks are mainly investigated under the title of reinforcement learning. active versus passive learners learning paradigms can vary by the role played by the learner. we distinguish between active and passive learners. an active learner interacts with the environment at training time say by posing queries or performing experiments while a passive learner only observes the information provided by the environment the teacher without influencing or directing it. note that the learner of a spam filter is usually passive waiting for users to mark the e-mails coming to them. in an active setting one could imagine asking users to label specific e-mails chosen by the learner or even composed by the learner to enhance what spam is. understanding its of helpfulness of the teacher when one thinks about human learning of a baby at home or a student at school the process often involves a helpful teacher who is trying to feed the learner with the information most use introduction ful for achieving the learning goal. in contrast when a scientist learns about nature the environment playing the role of the teacher can be best thought of as passive apples drop stars shine and the rain falls without regard to the needs of the learner. we model such learning scenarios by postulating that the training data the learner s experience is generated by some random process. this is the basic building block in the branch of statistical learning. finally learning also occurs when the learner s input is generated by an adversarial teacher. this may be the case in the spam filtering example the spammer makes an effort to mislead the spam filtering designer or in learning to detect fraud. one also uses an adversarial teacher model as a worst-case scenario when no milder setup can be safely assumed. if you can learn against an adversarial teacher you are guaranteed to succeed interacting any odd teacher. online versus batch learning protocol the last parameter we mention is the distinction between situations in which the learner has to respond online throughout the learning process and settings in which the learner has to engage the acquired expertise only after having a chance to process large amounts of data. for example a stockbroker has to make daily decisions based on the experience collected so far. he may become an expert over time but might have made costly mistakes in the process. in contrast in many data mining settings the learner the data miner has large amounts of training data to play with before having to output conclusions. in this book we shall discuss only a subset of the possible learning paradigms. our main focus is on supervised statistical batch learning with a passive learner example trying to learn how to generate patients prognoses based on large archives of records of patients that were independently collected and are already labeled by the fate of the recorded patients. we shall also briefly discuss online learning and batch unsupervised learning particular clustering. relations to other fields as an interdisciplinary field machine learning shares common threads with the mathematical fields of statistics information theory game theory and optimization. it is naturally a subfield of computer science as our goal is to program machines so that they will learn. in a sense machine learning can be viewed as a branch of ai intelligence since after all the ability to turn experience into expertise or to detect meaningful patterns in complex sensory data is a cornerstone of human animal intelligence. however one should note that in contrast with traditional ai machine learning is not trying to build automated imitation of intelligent behavior but rather to use the strengths and how to read this book special abilities of computers to complement human intelligence often performing tasks that fall way beyond human capabilities. for example the ability to scan and process huge databases allows machine learning programs to detect patterns that are outside the scope of human perception. the component of experience or training in machine learning often refers to data that is randomly generated. the task of the learner is to process such randomly generated examples toward drawing conclusions that hold for the environment from which these examples are picked. this description of machine learning highlights its close relationship with statistics. indeed there is a lot in common between the two disciplines in terms of both the goals and techniques used. there are however a few significant differences of emphasis if a doctor comes up with the hypothesis that there is a correlation between smoking and heart disease it is the statistician s role to view samples of patients and check the validity of that hypothesis is the common statistical task of hypothesis testing. in contrast machine learning aims to use the data gathered from samples of patients to come up with a description of the causes of heart disease. the hope is that automated techniques may be able to figure out meaningful patterns hypotheses that may have been missed by the human observer. in contrast with traditional statistics in machine learning in general and in this book in particular algorithmic considerations play a major role. machine learning is about the execution of learning by computers hence algorithmic issues are pivotal. we develop algorithms to perform the learning tasks and are concerned with their computational efficiency. another difference is that while statistics is often interested in asymptotic behavior the convergence of sample-based statistical estimates as the sample sizes grow to infinity the theory of machine learning focuses on finite sample bounds. namely given the size of available samples machine learning theory aims to figure out the degree of accuracy that a learner can expect on the basis of such samples. there are further differences between these two disciplines of which we shall mention only one more here. while in statistics it is common to work under the assumption of certain presubscribed data models as assuming the normality of data-generating distributions or the linearity of functional dependencies in machine learning the emphasis is on working under a distribution-free setting where the learner assumes as little as possible about the nature of the data distribution and allows the learning algorithm to figure out which models best approximate the data-generating process. a precise discussion of this issue requires some technical preliminaries and we will come back to it later in the book and in particular in chapter how to read this book the first part of the book provides the basic theoretical principles that underlie machine learning in a sense this is the foundation upon which the rest introduction of the book is built. this part could serve as a basis for a minicourse on the theoretical foundations of ml. the second part of the book introduces the most commonly used algorithmic approaches to supervised machine learning. a subset of these chapters may also be used for introducing machine learning in a general ai course to computer science math or engineering students. the third part of the book extends the scope of discussion from statistical classification to other learning models. it covers online learning unsupervised learning dimensionality reduction generative models and feature learning. the fourth part of the book advanced theory is geared toward readers who have interest in research and provides the more technical mathematical techniques that serve to analyze and drive forward the field of theoretical machine learning. the appendixes provide some technical tools used in the book. in particular we list basic results from measure concentration and linear algebra. a few sections are marked by an asterisk which means they are addressed to more advanced students. each chapter is concluded with a list of exercises. a solution manual is provided in the course web site. possible course plans based on this book a week introduction course for graduate students chapters chapter the vc calculation. chapters proofs. chapter chapters proofs. chapters some of the easier proofs. chapter some of the easier proofs. chapter chapter chapter chapter chapter proofs for compressed sensing. chapter chapter a week advanced course for graduate students chapters chapters chapter chapter notation chapter chapters chapter chapter chapter chapter chapter chapter chapter notation most of the notation we use throughout the book is either standard or defined on the spot. in this section we describe our main conventions and provide a table summarizing our notation the reader is encouraged to skip this section and return to it if during the reading of the book some notation is unclear. we denote scalars and abstract objects with lowercase letters x and often we would like to emphasize that some object is a vector and then we use boldface letters x and the ith element of a vector x is denoted by xi. we use uppercase letters to denote matrices sets and sequences. the meaning should be clear from the context. as we will see momentarily the input of a learning algorithm is a sequence of training examples. we denote by z an abstract example and by s zm a sequence of m examples. historically s is often referred to as a training set however we will always assume that s is a sequence rather than a set. a sequence of m vectors is denoted by xm. the ith element of xt is denoted by xti. throughout the book we make use of basic notions from probability. we denote by d a distribution over some for example z. we use the notation z d to denote that z is sampled according to d. given a random variable f z r its expected value is denoted by ez df we sometimes use the shorthand ef when the dependence on z is clear from the context. for f z false we also use pz df to denote dz f true. in the next chapter we will also introduce the notation dm to denote the probability over z m induced by sampling zm where each point zi is sampled from d independently of the other points. in general we have made an effort to avoid asymptotic notation. however we occasionally use it to clarify the main results. in particular given f r r and g r r we write f og if there exist r such that for all x we have f gx. we write f og if for every there exists to be mathematically precise d should be defined over some of subsets of z. the user who is not familiar with measure theory can skip the few footnotes and remarks regarding more formal measurability definitions and assumptions. introduction symbol r rd r n o o o expression x v w xi vi wi or a rdk aij x xm xij wt wt x y z h h z r d da z d s zm s dm p e pz df ez df n c i f wi f f minx c f maxx c f argminx c f argmaxx c f log table summary of notation meaning the set of real numbers the set of d-dimensional vectors over r the set of non-negative real numbers the set of natural numbers asymptotic notation text indicator function if expression is true and o.w. a the set n n n vectors the ith element of a vector norm of x xivi product norm of x maxi norm of x the number of nonzero elements of x a d k matrix over r the transpose of a the j element of a the d d matrix a s.t. aij xixj x rd a sequence of m vectors the jth element of the ith vector in the sequence the values of a vector w during an iterative algorithm the ith element of the vector wt instances domain set labels domain set examples domain set hypothesis class set loss function a distribution over some set over z or over x the probability of a set a z according to d sampling z according to d a sequence of m examples sampling s zm i.i.d. according to d probability and expectation of a random variable dz f true for f z false expectation of the random variable f z r gaussian distribution with expectation and covariance c the derivative of a function f r r at x the second derivative of a function f r r at x the partial derivative of a function f rd r at w w.r.t. wi the gradient of a function f rd r at w the differential set of a function f rd r at w minf x c value of f over c maxf x c value of f over c the set c f minz c f the set c f maxz c f the natural logarithm notation such that for all x we have f gx. we write f if there exist r such that for all x we have f gx. the notation f is defined analogously. the notation f means that f og and g of finally the notation f og means that there exists k n such that f ogx logkgx. the inner product between vectors x and w is denoted by whenever we do not specify the vector space we assume that it is the d-dimensional euclidean xiwi. the euclidean norm of a vector w is i and in particular space and then we omit the subscript from the norm when it is clear from the context. we also use other norms i and maxi we use the notation minx c f to denote the minimum value of the set x c. to be mathematically more precise we should use inf x c f whenever the minimum is not achievable. however in the context of this book the distinction between infimum and minimum is often of little interest. hence to simplify the presentation we sometimes use the min notation even when inf is more adequate. an analogous remark applies to max versus sup. part i foundations a gentle start let us begin our mathematical analysis by showing how successful learning can be achieved in a relatively simplified setting. imagine you have just arrived in some small pacific island. you soon find out that papayas are a significant ingredient in the local diet. however you have never before tasted papayas. you have to learn how to predict whether a papaya you see in the market is tasty or not. first you need to decide which features of a papaya your prediction should be based on. on the basis of your previous experience with other fruits you decide to use two features the papaya s color ranging from dark green through orange and red to dark brown and the papaya s softness ranging from rock hard to mushy. your input for figuring out your prediction rule is a sample of papayas that you have examined for color and softness and then tasted and found out whether they were tasty or not. let us analyze this task as a demonstration of the considerations involved in learning problems. our first step is to describe a formal model aimed to capture such learning tasks. a formal model the statistical learning framework the learner s input in the basic statistical learning setting the learner has access to the following domain set an arbitrary set x this is the set of objects that we may wish to label. for example in the papaya learning problem mentioned before the domain set will be the set of all papayas. usually these domain points will be represented by a vector of features the papaya s color and softness. we also refer to domain points as instances and to x as instance space. label set for our current discussion we will restrict the label set to be a two-element set usually or let y denote our set of possible labels. for our papayas example let y be where represents being tasty and stands for being not-tasty. training data s ym is a finite sequence of pairs in x y that is a sequence of labeled domain points. this is the input that the learner has access to a set of papayas that have been understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning a gentle start tasted and their color softness and tastiness. such labeled examples are often called training examples. we sometimes also refer to s as a training the learner s output the learner is requested to output a prediction rule h x y. this function is also called a predictor a hypothesis or a classifier. the predictor can be used to predict the label of new domain points. in our papayas example it is a rule that our learner will employ to predict whether future papayas he examines in the farmers market are going to be tasty or not. we use the notation as to denote the hypothesis that a learning algorithm a returns upon receiving the training sequence s. a simple data-generation model we now explain how the training data is generated. first we assume that the instances papayas we encounter are generated by some probability distribution this case representing the environment. let us denote that probability distribution over x by d. it is important to note that we do not assume that the learner knows anything about this distribution. for the type of learning tasks we discuss this could be any arbitrary probability distribution. as to the labels in the current discussion we assume that there is some correct labeling function f x y and that yi f for all i. this assumption will be relaxed in the next chapter. the labeling function is unknown to the learner. in fact this is just what the learner is trying to figure out. in summary each pair in the training data s is generated by first sampling a point xi according to d and then labeling it by f measures of success we define the error of a classifier to be the probability that it does not predict the correct label on a random data point generated by the aforementioned underlying distribution. that is the error of h is the probability to draw a random instance x according to the distribution d such that hx does not equal f formally given a domain a x the probability distribution d assigns a number da which determines how likely it is to observe a point x a. in many cases we refer to a as an event and express it using a function x namely a x in that case we also use the notation px d to express da. we define the error of a prediction rule h x y to be x dhx f def dx hx f ldf def p that is the error of such h is the probability of randomly choosing an example x for which hx f the subscript f indicates that the error is measured with respect to the probability distribution d and the despite the set notation s is a sequence. in particular the same example may appear strictly speaking we should be more careful and require that a is a member of some twice in s and some algorithms can take into account the order of examples in s. of subsets of x over which d is defined. we will formally define our measurability assumptions in the next chapter. empirical risk minimization correct labeling function f we omit this subscript when it is clear from the context. ldf has several synonymous names such as the generalization error the risk or the true error of h and we will use these names interchangeably throughout the book. we use the letter l for the error since we view this error as the loss of the learner. we will later also discuss other possible formulations of such loss. a note about the information available to the learner the learner is blind to the underlying distribution d over the world and to the labeling function f. in our papayas example we have just arrived in a new island and we have no clue as to how papayas are distributed and how to predict their tastiness. the only way the learner can interact with the environment is through observing the training set. in the next section we describe a simple learning paradigm for the preceding setup and analyze its performance. empirical risk minimization as mentioned earlier a learning algorithm receives as input a training set s sampled from an unknown distribution d and labeled by some target function f and should output a predictor hs x y subscript s emphasizes the fact that the output predictor depends on s. the goal of the algorithm is to find hs that minimizes the error with respect to the unknown d and f since the learner does not know what d and f are the true error is not directly available to the learner. a useful notion of error that can be calculated by the learner is the training error the error the classifier incurs over the training sample lsh def where m. hxi yi m the terms empirical error and empirical risk are often used interchangeably for this error. since the training sample is the snapshot of the world that is available to the learner it makes sense to search for a solution that works well on that data. this learning paradigm coming up with a predictor h that minimizes lsh is called empirical risk minimization or erm for short. something may go wrong overfitting although the erm rule seems very natural without being careful this approach may fail miserably. to demonstrate such a failure let us go back to the problem of learning to a gentle start predict the taste of a papaya on the basis of its softness and color. consider a sample as depicted in the following assume that the probability distribution d is such that instances are distributed uniformly within the gray square and the labeling function f determines the label to be if the instance is within the inner blue square and otherwise. the area of the gray square in the picture is and the area of the blue square is consider the following predictor if i s.t. xi x otherwise. yi hsx while this predictor might seem rather artificial in exercise we show a natural representation of it using polynomials. clearly no matter what the sample is lshs and therefore this predictor may be chosen by an erm algorithm is one of the empirical-minimum-cost hypotheses no classifier can have smaller error. on the other hand the true error of any classifier that predicts the label only on a finite number of instances is in this case thus ldhs we have found a predictor whose performance on the training set is excellent yet its performance on the true world is very poor. this phenomenon is called overfitting. intuitively overfitting occurs when our hypothesis fits the training data too well like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion. empirical risk minimization with inductive bias we have just demonstrated that the erm rule might lead to overfitting. rather than giving up on the erm paradigm we will look for ways to rectify it. we will search for conditions under which there is a guarantee that erm does not overfit namely conditions under which when the erm predictor has good performance with respect to the training data it is also highly likely to perform well over the underlying data distribution. a common solution is to apply the erm learning rule over a restricted search space. formally the learner should choose in advance seeing the data a set of predictors. this set is called a hypothesis class and is denoted by h. each h h is a function mapping from x to y. for a given class h and a training sample s the ermh learner uses the erm rule to choose a predictor h h empirical risk minimization with inductive bias with the lowest possible error over s. formally ermhs argmin h h lsh where argmin stands for the set of hypotheses in h that achieve the minimum value of lsh over h. by restricting the learner to choosing a predictor from h we bias it toward a particular set of predictors. such restrictions are often called an inductive bias. since the choice of such a restriction is determined before the learner sees the training data it should ideally be based on some prior knowledge about the problem to be learned. for example for the papaya taste prediction problem we may choose the class h to be the set of predictors that are determined by axis aligned rectangles the space determined by the color and softness coordinates. we will later show that ermh over this class is guaranteed not to overfit. on the other hand the example of overfitting that we have seen previously demonstrates that choosing h to be a class of predictors that includes all functions that assign the value to a finite set of domain points does not suffice to guarantee that ermh will not overfit. a fundamental question in learning theory is over which hypothesis classes ermh learning will not result in overfitting. we will study this question later in the book. intuitively choosing a more restricted hypothesis class better protects us against overfitting but at the same time might cause us a stronger inductive bias. we will get back to this fundamental tradeoff later. finite hypothesis classes the simplest type of restriction on a class is imposing an upper bound on its size is the number of predictors h in h. in this section we show that if h is a finite class then ermh will not overfit provided it is based on a sufficiently large training sample size requirement will depend on the size of h. limiting the learner to prediction rules within some finite hypothesis class may be considered as a reasonably mild restriction. for example h can be the set of all predictors that can be implemented by a c program written in at most bits of code. in our papayas example we mentioned previously the class of axis aligned rectangles. while this is an infinite class if we discretize the representation of real numbers say by using a bits floating-point representation the hypothesis class becomes a finite class. let us now analyze the performance of the ermh learning rule assuming that h is a finite class. for a training sample s labeled according to some f x y let hs denote a result of applying ermh to s namely hs argmin h h lsh. in this chapter we make the following simplifying assumption will be relaxed in the next chapter. a gentle start definition realizability assumption there exists h s.t. ldf note that this assumption implies that with probability over random samples s where the instances of s are sampled according to d and are labeled by f we have the realizability assumption implies that for every erm hypothesis we have lshs however we are interested in the true risk of hs ldf rather than its empirical risk. clearly any guarantee on the error with respect to the underlying distribution d for an algorithm that has access only to a sample s should depend on the relationship between d and s. the common assumption in statistical machine learning is that the training sample s is generated by sampling points from the distribution d independently of each other. formally the i.i.d. assumption the examples in the training set are independently and identically distributed according to the distribution d. that is every xi in s is freshly sampled according to d and then labeled according to the labeling function f we denote this assumption by s dm where m is the size of s and dm denotes the probability over m-tuples induced by applying d to pick each element of the tuple independently of the other members of the tuple. intuitively the training set s is a window through which the learner gets partial information about the distribution d over the world and the labeling function f the larger the sample gets the more likely it is to reflect more accurately the distribution and labeling used to generate it. since ldf depends on the training set s and that training set is picked by a random process there is randomness in the choice of the predictor hs and consequently in the risk ldf formally we say that it is a random variable. it is not realistic to expect that with full certainty s will suffice to direct the learner toward a good classifier the point of view of d as there is always some probability that the sampled training data happens to be very nonrepresentative of the underlying d. if we go back to the papaya tasting example there is always some chance that all the papayas we have happened to taste were not tasty in spite of the fact that say of the papayas in our island are tasty. in such a case ermhs may be the constant function that labels every papaya as not tasty has error on the true distribution of papapyas in the island. we will therefore address the probability to sample a training set for which ldf is not too large. usually we denote the probability of getting a nonrepresentative sample by and call the confidence parameter of our prediction. on top of that since we cannot guarantee perfect label prediction we introduce another parameter for the quality of prediction the accuracy parameter mathematically speaking this holds with probability to simplify the presentation we sometimes omit the with probability specifier. empirical risk minimization with inductive bias commonly denoted by we interpret the event ldf as a failure of the learner while if ldf we view the output of the algorithm as an approximately correct predictor. therefore some labeling function f x y we are interested in upper bounding the probability to sample m-tuple of instances that will lead to failure of the learner. formally let sx xm be the instances of the training set. we would like to upper bound dmsx ldf let hb be the set of bad hypotheses that is hb h ldf in addition let m h hb lsh be the set of misleading samples namely for every sx m there is a bad hypothesis h hb that looks like a good hypothesis on sx. now recall that we would like to bound the probability of the event ldf but since the realizability assumption implies that lshs it follows that the event ldf can only happen if for some h hb we have lsh in other words this event will only happen if our sample is in the set of misleading samples m formally we have shown that ldf m note that we can rewrite m as h hb m lsh hence dmsx ldf dmm dm h hbsx lsh next we upper bound the right-hand side of the preceding equation using the union bound a basic property of probabilities. lemma bound for any two sets a b and a distribution d we have da b da db. applying the union bound to the right-hand side of equation yields dmsx ldf h hb dmsx lsh next let us bound each summand of the right-hand side of the preceding inequality. fix some bad hypothesis h hb. the event lsh is equivalent a gentle start to the event i hxi f since the examples in the training set are sampled i.i.d. we get that dmsx lsh dmsx i hxi f dxi hxi f for each individual sampling of an element of the training set we have dxi hxi yi ldf where the last inequality follows from the fact that h hb. combining the previous equation with equation and using the inequality e we obtain that for every h hb dmsx lsh e combining this equation with equation we conclude that dmsx ldf e e m. a graphical illustration which explains how we used the union bound is given in figure figure each point in the large circle represents a possible m-tuple of instances. each colored oval represents the set of misleading m-tuple of instances for some bad predictor h hb. the erm can potentially overfit whenever it gets a misleading training set s. that is for some h hb we have lsh equation guarantees that for each individual bad hypothesis h hb at most of the training sets would be misleading. in particular the larger m is the smaller each of these colored ovals becomes. the union bound formalizes the fact that the area representing the training sets that are misleading with respect to some h hb is the training sets in m is at most the sum of the areas of the colored ovals. therefore it is bounded by times the maximum size of a colored oval. any sample s outside the colored ovals cannot cause the erm rule to overfit. corollary let h be a finite hypothesis class. let and and let m be an integer that satisfies m logh exercises then for any labeling function f and for any distribution d for which the realizability assumption holds is for some h h ldf with probability of at least over the choice of an i.i.d. sample s of size m we have that for every erm hypothesis hs it holds that ldf the preceeding corollary tells us that for a sufficiently large m the ermh rule over a finite hypothesis class will be probably confidence approximately to an error of correct. in the next chapter we formally define the model of probably approximately correct learning. exercises overfitting of polynomial matching we have shown that the predictor defined in equation leads to overfitting. while this predictor seems to be very unnatural the goal of this exercise is to show that it can be described as a thresholded polynomial. that is show that given a training set s f there exists a polynomial ps such that hsx if and only if psx where hs is as defined in equation it follows that learning the class of all thresholded polynomials using the erm rule may lead to overfitting. let h be a class of binary classifiers over a domain x let d be an unknown distribution over x and let f be the target hypothesis in h. fix some h h. show that the expected value of lsh over the choice of sx equals ldf namely e sx dm ldf axis aligned rectangles an axis aligned rectangle classifier in the plane is a classifier that assigns the value to a point if and only if it is inside a certain rectangle. formally given real numbers define the classifier by if and otherwise the class of all axis aligned rectangles in the plane is defined as rec and note that this is an infinite size hypothesis class. throughout this exercise we rely on the realizability assumption. a gentle start then with proba let a be the algorithm that returns the smallest rectangle enclosing all show that if a receives a training set of size positive examples in the training set. show that a is an erm. bility of at least it returns a hypothesis with error of at most hint fix some distribution d over x let r ra be the rectangle that generates the labels and let f be the corresponding hypothesis. let a be a number such that the probability mass respect to d of the rectangle ra is exactly similarly let be numbers such that the probability masses of the rectangles b ra are all exactly let rs be the rectangle returned by a. see illustration in figure b ra a b a b a b a b b b r rs figure axis aligned rectangles. show that rs r show that if s contains examples in all of the rectangles then the hypothesis returned by a has error of at most for each i upper bound the probability that s does not use the union bound to conclude the argument. contain an example from ri. repeat the previous question for the class of axis aligned rectangles in rd. show that the runtime of applying the algorithm a mentioned earlier is polynomial in d and in a formal learning model in this chapter we define our main formal learning model the pac learning model and its extensions. we will consider other notions of learnability in chapter pac learning in the previous chapter we have shown that for a finite hypothesis class if the erm rule with respect to that class is applied on a sufficiently large training sample size is independent of the underlying distribution or labeling function then the output hypothesis will be probably approximately correct. more generally we now define probably approximately correct learning. definition learnability a hypothesis class h is pac learnable if there exist a function mh n and a learning algorithm with the following property for every for every distribution d over x and for every labeling function f x if the realizable assumption holds with respect to hd f then when running the learning algorithm on m mh i.i.d. examples generated by d and labeled by f the algorithm returns a hypothesis h such that with probability of at least the choice of the examples ldf the definition of probably approximately correct learnability contains two approximation parameters. the accuracy parameter determines how far the output classifier can be from the optimal one corresponds to the approximately correct and a confidence parameter indicating how likely the classifier is to meet that accuracy requirement to the probably part of pac under the data access model that we are investigating these approximations are inevitable. since the training set is randomly generated there may always be a small chance that it will happen to be noninformative example there is always some chance that the training set will contain only one domain point sampled over and over again. furthermore even when we are lucky enough to get a training sample that does faithfully represent d because it is just a finite sample there may always be some fine details of d that it fails understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning a formal learning model to reflect. our accuracy parameter allows forgiving the learner s classifier for making minor errors. sample complexity the function mh n determines the sample complexity of learning h that is how many examples are required to guarantee a probably approximately correct solution. the sample complexity is a function of the accuracy and confidence parameters. it also depends on properties of the hypothesis class h for example for a finite class we showed that the sample complexity depends on log the size of h. note that if h is pac learnable there are many functions mh that satisfy the requirements given in the definition of pac learnability. therefore to be precise we will define the sample complexity of learning h to be the minimal function in the sense that for any mh is the minimal integer that satisfies the requirements of pac learning with accuracy and confidence let us now recall the conclusion of the analysis of finite hypothesis classes from the previous chapter. it can be rephrased as stating corollary every finite hypothesis class is pac learnable with sample complexity logh mh there are infinite classes that are learnable as well for example exercise later on we will show that what determines the pac learnability of a class is not its finiteness but rather a combinatorial measure called the vc dimension. a more general learning model the model we have just described can be readily generalized so that it can be made relevant to a wider scope of learning tasks. we consider generalizations in two aspects removing the realizability assumption we have required that the learning algorithm succeeds on a pair of data distribution d and labeling function f provided that the realizability assumption is met. for practical learning tasks this assumption may be too strong we really guarantee that there is a rectangle in the color-hardness space that fully determines which papayas are tasty?. in the next subsection we will describe the agnostic pac model in which this realizability assumption is waived. a more general learning model learning problems beyond binary classification the learning task that we have been discussing so far has to do with predicting a binary label to a given example being tasty or not. however many learning tasks take a different form. for example one may wish to predict a real valued number the temperature at p.m. tomorrow or a label picked from a finite set of labels the topic of the main story in tomorrow s paper. it turns out that our analysis of learning can be readily extended to such and many other scenarios by allowing a variety of loss functions. we shall discuss that in section later. releasing the realizability assumption agnostic pac learning a more realistic model for the data-generating distribution recall that the realizability assumption requires that there exists h such that px f in many practical problems this assumption does not hold. furthermore it is maybe more realistic not to assume that the labels are fully determined by the features we measure on input elements the case of the papayas it is plausible that two papayas of the same color and softness will have different taste. in the following we relax the realizability assumption by replacing the target labeling function with a more flexible notion a data-labels generating distribution. formally from now on let d be a probability distribution over x y where as before x is our domain set and y is a set of labels we will consider y that is d is a joint distribution over domain points and labels. one can view such a distribution as being composed of two parts a distribution dx over unlabeled domain points called the marginal distribution and a conditional probability over labels for each domain point dx yx. in the papaya example dx determines the probability of encountering a papaya whose color and hardness fall in some color-hardness values domain and the conditional probability is the probability that a papaya with color and hardness represented by x is tasty. indeed such modeling allows for two papayas that share the same color and hardness to belong to different taste categories. the empirical and the true error revised for a probability distribution d over x y one can measure how likely h is to make an error when labeled points are randomly drawn according to d. we redefine the true error risk of a prediction rule h to be ldh def p dhx y def dx y hx y. we would like to find a predictor h for which that error will be minimized. however the learner does not know the data generating d. what the learner does have access to is the training data s. the definition of the empirical risk a formal learning model remains the same as before namely lsh def hxi yi m given s a learner can compute lsh for any function h x note that lsh lduniform over sh. the goal we wish to find some hypothesis h x y that approximately minimizes the true risk ldh. the bayes optimal predictor. given any probability distribution d over x the best label predicting function from x to will be fdx if py otherwise it is easy to verify exercise that for every probability distribution d the bayes optimal predictor fd is optimal in the sense that no other classifier g x has a lower error. that is for every classifier g ldfd ldg. unfortunately since we do not know d we cannot utilize this optimal predictor fd. what the learner does have access to is the training sample. we can now present the formal definition of agnostic pac learnability which is a natural extension of the definition of pac learnability to the more realistic nonrealizable learning setup we have just discussed. clearly we cannot hope that the learning algorithm will find a hypothesis whose error is smaller than the minimal possible error that of the bayes predictor. furthermore as we shall prove later once we make no prior assumptions about the data-generating distribution no algorithm can be guaranteed to find a predictor that is as good as the bayes optimal one. instead we require that the learning algorithm will find a predictor whose error is not much larger than the best possible error of a predictor in some given benchmark hypothesis class. of course the strength of such a requirement depends on the choice of that hypothesis class. definition pac learnability a hypothesis class h is agnostic pac learnable if there exist a function mh n and a learning algorithm with the following property for every and for every distribution d over x y when running the learning algorithm on m mh i.i.d. examples generated by d the algorithm returns a hypothesis h such that with probability of at least the choice of the m training examples ldh min h a more general learning model clearly if the realizability assumption holds agnostic pac learning provides the same guarantee as pac learning. in that sense agnostic pac learning generalizes the definition of pac learning. when the realizability assumption does not hold no learner can guarantee an arbitrarily small error. nevertheless under the definition of agnostic pac learning a learner can still declare success if its error is not much larger than the best error achievable by a predictor from the class h. this is in contrast to pac learning in which the learner is required to achieve a small error in absolute terms and not relative to the best error achievable by the hypothesis class. the scope of learning problems modeled we next extend our model so that it can be applied to a wide variety of learning tasks. let us consider some examples of different learning tasks. multiclass classification our classification does not have to be binary. take for example the task of document classification we wish to design a program that will be able to classify given documents according to topics news sports biology medicine. a learning algorithm for such a task will have access to examples of correctly classified documents and on the basis of these examples should output a program that can take as input a new document and output a topic classification for that document. here the domain set is the set of all potential documents. once again we would usually represent documents by a set of features that could include counts of different key words in the document as well as other possibly relevant features like the size of the document or its origin. the label set in this task will be the set of possible document topics y will be some large finite set. once we determine our domain and label sets the other components of our framework look exactly the same as in the papaya tasting example our training sample will be a finite sequence of vector label pairs the learner s output will be a function from the domain set to the label set and finally for our measure of success we can use the probability over topic pairs of the event that our predictor suggests a wrong label. regression in this task one wishes to find some simple pattern in the data a functional relationship between the x and y components of the data. for example one wishes to find a linear function that best predicts a baby s birth weight on the basis of ultrasound measures of his head circumference abdominal circumference and femur length. here our domain set x is some subset of three ultrasound measurements and the set of labels y is the the set of real numbers weight in grams. in this context it is more adequate to call y the target set. our training data as well as the learner s output are as before finite sequence of y pairs and a function from x to y respectively. however our measure of success is a formal learning model different. we may evaluate the quality of a hypothesis function h x y by the expected square difference between the true labels and their predicted values namely ldh def dhx e to accommodate a wide range of learning tasks we generalize our formalism of the measure of success as follows generalized loss functions given any set h plays the role of our hypotheses or models and some domain z let be any function from h z to the set of nonnegative real numbers h z r. we call such functions loss functions. note that for prediction problems we have that z x y. however our notion of the loss function is generalized beyond prediction tasks and therefore it allows z to be any domain of examples instance in unsupervised learning tasks such as the one described in chapter z is not a product of an instance domain and a label domain. we now define the risk function to be the expected loss of a classifier h h with respect to a probability distribution d over z namely ldh def e z z. that is we consider the expectation of the loss of h over objects z picked randomly according to d. similarly we define the empirical risk to be the expected loss over a given sample s zm z m namely lsh def m zi. the loss functions used in the preceding examples of classification and regres sion tasks are as follows loss here our random variable z ranges over the set of pairs x y and the loss function is y def if hx y if hx y this loss function is used in binary or multiclass classification problems. one should note that for a random variable taking the values e d p d consequently for this loss function the definitions of ldh given in equation and equation coincide. square loss here our random variable z ranges over the set of pairs x y and the loss function is y def summary this loss function is used in regression problems. we will later see more examples of useful instantiations of loss functions. to summarize we formally define agnostic pac learnability for general loss functions. definition pac learnability for general loss functions a hypothesis class h is agnostic pac learnable with respect to a set z and a loss function h z r if there exist a function mh n and a learning algorithm with the following property for every and for every distribution d over z when running the learning algorithm on m mh i.i.d. examples generated by d the algorithm returns h h such that with probability of at least the choice of the m training examples ldh min h where ldh ez z. remark note about measurability in the aforementioned definition for every h h we view the function z r as a random variable and define ldh to be the expected value of this random variable. for that we need to require that the function is measurable. formally we assume that there is a of subsets of z over which the probability d is defined and that the preimage of every initial segment in r is in this in the specific case of binary classification with the loss the is over x and our assumption on is equivalent to the assumption that for every h the set hx x x is in the remark versus representation-independent learning in the preceding definition we required that the algorithm will return a hypothesis from h. in some situations h is a subset of a set and the loss function can be naturally extended to be a function from z to the reals. in this case we may allow the algorithm to return a hypothesis as long as it satisfies the requirement minh h ldh allowing the algorithm to output a hypothesis from is called representation independent learning while proper learning occurs when the algorithm must output a hypothesis from h. representation independent learning is sometimes called improper learning although there is nothing improper in representation independent learning. summary in this chapter we defined our main formal learning model pac learning. the basic model relies on the realizability assumption while the agnostic variant does a formal learning model not impose any restrictions on the underlying distribution over the examples. we also generalized the pac model to arbitrary loss functions. we will sometimes refer to the most general model simply as pac learning omitting the agnostic prefix and letting the reader infer what the underlying loss function is from the context. when we would like to emphasize that we are dealing with the original pac setting we mention that the realizability assumption holds. in chapter we will discuss other notions of learnability. bibliographic remarks our most general definition of agnostic pac learning with general loss functions follows the works of vladimir vapnik and alexey chervonenkis chervonenkis in particular we follow vapnik s general setting of learning vapnik vapnik vapnik pac learning was introduced by valiant valiant was named the winner of the turing award for the introduction of the pac model. valiant s definition requires that the sample complexity will be polynomial in and in as well as in the representation size of hypotheses in the class also kearns vazirani as we will see in chapter if a problem is at all pac learnable then the sample complexity depends polynomially on and valiant s definition also requires that the runtime of the learning algorithm will be polynomial in these quantities. in contrast we chose to distinguish between the statistical aspect of learning and the computational aspect of learning. we will elaborate on the computational aspect later on in chapter where we introduce the full pac learning model of valiant. for expository reasons we use the term pac learning even when we ignore the runtime aspect of learning. finally the formalization of agnostic pac learning is due to haussler exercises monotonicity of sample complexity let h be a hypothesis class for a binary classification task. suppose that h is pac learnable and its sample complexity is given by mh show that mh is monotonically nonincreasing in each of its parameters. that is show that given and given we have that similarly show that given and given we have that mh mh let x be a discrete domain and let hsingleton z x where for each z x hz is the function defined by hzx if x z and hzx if x z. h is simply the all-negative hypothesis namely x x h the realizability assumption here implies that the true hypothesis f labels negatively all examples in the domain perhaps except one. exercises in the realizable setup. describe an algorithm that implements the erm rule for learning hsingleton show that hsingleton is pac learnable. provide an upper bound on the let x y and let h be the class of concentric circles in the plane that is h r r where hrx r. prove that h is pac learnable realizability and its sample complexity is bounded by sample complexity. mh in this question we study the hypothesis class of boolean conjunctions defined as follows. the instance space is x and the label set is y a literal over the variables xd is a simple boolean function that takes the form f xi for some i or f xi for some i we use the notation xi as a shorthand for xi. a conjunction is any product of literals. in boolean logic the product is denoted using the sign. for example the function hx is written as we consider the hypothesis class of all conjunctions of literals over the d variables. the empty conjunction is interpreted as the all-positive hypothesis the function that returns hx for all x. the conjunction similarly any conjunction involving a literal and its negation is allowed and interpreted as the all-negative hypothesis the conjunction that returns hx for all x. we assume realizability namely we assume that there exists a boolean conjunction that generates the labels. thus each example y x y consists of an assignment to the d boolean variables xd and its truth value for false and for true. for instance let d and suppose that the true conjunction is then the training set s might contain the following instances prove that the hypothesis class of all conjunctions over d variables is pac learnable and bound its sample complexity. propose an algorithm that implements the erm rule whose runtime is polynomial in d m. let x be a domain and let be a sequence of distributions over x let h be a finite class of binary classifiers over x and let f h. suppose we are getting a sample s of m examples such that the instances are independent but are not identically distributed the ith instance is sampled from di and then yi is set to be f let dm denote the average that is dm dmm. fix an accuracy parameter show that h h s.t. l dmf and lsf a formal learning model hint use the geometric-arithmetic mean inequality. let h be a hypothesis class of binary classifiers. show that if h is agnostic pac learnable then h is pac learnable as well. furthermore if a is a successful agnostic pac learner for h then a is also a successful pac learner for h. the bayes optimal predictor show that for every probability distribution d the bayes optimal predictor fd is optimal in the sense that for every classifier g from x to ldfd ldg. probability distribution d if we say that a learning algorithm a is better than b with respect to some ldas ldbs for all samples s we say that a learning algorithm a is better than b if it is better than b with respect to all probability distributions d over x a probabilistic label predictor is a function that assigns to every domain point x a probability value hx that determines the probability of predicting the label that is given such an h and an input x the label for x is predicted by tossing a coin with bias hx toward heads and predicting iff the coin comes up heads. formally we define a probabilistic label predictor as a function h x the loss of such h on an example y is defined to be y which is exactly the probability that the prediction of h will not be equal to y. note that if h is deterministic that is returns values in then y prove that for every data-generating distribution d over x the bayes optimal predictor has the smallest risk the loss function y y among all possible label predictors including probabilistic ones. let x be a domain and be a set of labels. prove that for every distribution d over x there exist a learning algorithm ad that is better than any other learning algorithm with respect to d. prove that for every learning algorithm a there exist a probability distribution d and a learning algorithm b such that a is not better than b w.r.t. d. consider a variant of the pac model in which there are two example oracles one that generates positive examples and one that generates negative examples both according to the underlying distribution d on x formally given a target function f x let d be the distribution over x x f defined by da dadx for every a x similarly d is the distribution over x induced by d. the definition of pac learnability in the two-oracle model is the same as the standard definition of pac learnability except that here the learner has access to mh i.i.d. examples from d and m i.i.d. examples from d the learner s goal is to output h s.t. with probability at least the choice exercises of the two training sets and possibly over the nondeterministic decisions made by the learning algorithm both ldf and ld show that if h is pac learnable the standard one-oracle model then h is pac learnable in the two-oracle model. define h to be the always-plus hypothesis and h to be the alwaysminus hypothesis. assume that h h h. show that if h is pac learnable in the two-oracle model then h is pac learnable in the standard one-oracle model. learning via uniform convergence the first formal learning model that we have discussed was the pac model. in chapter we have shown that under the realizability assumption any finite hypothesis class is pac learnable. in this chapter we will develop a general tool uniform convergence and apply it to show that any finite class is learnable in the agnostic pac model with general loss functions as long as the range loss function is bounded. uniform convergence is sufficient for learnability the idea behind the learning condition discussed in this chapter is very simple. recall that given a hypothesis class h the erm learning paradigm works as follows upon receiving a training sample s the learner evaluates the risk error of each h in h on the given sample and outputs a member of h that minimizes this empirical risk. the hope is that an h that minimizes the empirical risk with respect to s is a risk minimizer has risk close to the minimum with respect to the true data probability distribution as well. for that it suffices to ensure that the empirical risks of all members of h are good approximations of their true risk. put another way we need that uniformly over all hypotheses in the hypothesis class the empirical risk will be close to the true risk as formalized in the following. definition sample a training set s is called domain z hypothesis class h loss function and distribution d if h h ldh the next simple lemma states that whenever the sample is the erm learning rule is guaranteed to return a good hypothesis. lemma assume that a training set s is domain z hypothesis class h loss function and distribution d. then any output of ermhs namely any hs argminh h lsh satisfies ldhs min h h ldh understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning finite classes are agnostic pac learnable proof for every h h ldhs lshs lsh ldh ldh where the first and third inequalities are due to the assumption that s is representative and the second inequality holds since hs is an erm predictor. the preceding lemma implies that to ensure that the erm rule is an agnostic pac learner it suffices to show that with probability of at least over the random choice of a training set it will be an training set. the uniform convergence condition formalizes this requirement. definition convergence we say that a hypothesis class h has the uniform convergence property a domain z and a loss function if there exists a function much n such that for every and for every probability distribution d over z if s is a sample of m much examples drawn i.i.d. according to d then with probability of at least s is similar to the definition of sample complexity for pac learning the function much measures the sample complexity of obtaining the uniform convergence property namely how many examples we need to ensure that with probability of at least the sample would be the term uniform here refers to having a fixed sample size that works for all members of h and over all possible probability distributions over the domain. the following corollary follows directly from lemma and the definition of uniform convergence. if a class h has the uniform convergence property with a corollary function much then the class is agnostically pac learnable with the sample complexity mh much furthermore in that case the ermh paradigm is a successful agnostic pac learner for h. finite classes are agnostic pac learnable in view of corollary the claim that every finite hypothesis class is agnostic pac learnable will follow once we establish that uniform convergence holds for a finite hypothesis class. to show that uniform convergence holds we follow a two step argument similar to the derivation in chapter the first step applies the union bound while the second step employs a measure concentration inequality. we now explain these two steps in detail. fix some we need to find a sample size m that guarantees that for any d with probability of at least of the choice of s zm sampled learning via uniform convergence i.i.d. from d we have that for all h h ldh that is dms h hlsh ldh equivalently we need to show that dms h hlsh ldh writing h hlsh ldh h hs ldh and applying the union bound we obtain dms h hlsh ldh dms ldh h h our second step will be to argue that each summand of the right-hand side of this inequality is small enough a sufficiently large m. that is we will show that for any fixed hypothesis h is chosen in advance prior to the sampling of the training set the gap between the true and empirical risks ldh is likely to be small. zi. since each zi is sampled i.i.d. from d the expected value of the random variable zi is ldh. by the linearity of expectation it follows that ldh is also the expected value of lsh. hence the quantity lsh is the deviation of the random variable lsh from its expectation. we therefore need to show that the measure of lsh is concentrated around its expected value. recall that ldh ez z and that lsh m a basic statistical fact the law of large numbers states that when m goes to infinity empirical averages converge to their true expectation. this is true for lsh since it is the empirical average of m i.i.d random variables. however since the law of large numbers is only an asymptotic result it provides no information about the gap between the empirically estimated error and its true value for any given finite sample size. instead we will use a measure concentration inequality due to hoeffding which quantifies the gap between empirical averages and their expected value. lemma s inequality let m be a sequence of i.i.d. random variables and assume that for all i e i and pa i b then for any m p i m the proof can be found in appendix b. getting back to our problem let i be the random variable zi. since h is fixed and zm are sampled i.i.d. it follows that m are also i.i.d. random variables. furthermore lsh i and ldh let us m finite classes are agnostic pac learnable m further assume that the range of is and therefore i we therefore obtain that dms ldh p m dms h hlsh ldh m m combining this with equation yields i h h finally if we choose m then dms h hlsh ldh corollary let h be a finite hypothesis class let z be a domain and let h z be a loss function. then h enjoys the uniform convergence property with sample complexity much furthermore the class is agnostically pac learnable using the erm algorithm with sample complexity mh much remark discretization trick while the preceding corollary only applies to finite hypothesis classes there is a simple trick that allows us to get a very good estimate of the practical sample complexity of infinite hypothesis classes. consider a hypothesis class that is parameterized by d parameters. for example let x r y and the hypothesis class h be all functions of the form h signx that is each hypothesis is parameterized by one parameter r and the hypothesis outputs for all instances larger than and outputs for instances smaller than this is a hypothesis class of an infinite size. however if we are going to learn this hypothesis class in practice using a computer we will probably maintain real numbers using floating point representation say of bits. it follows that in practice our hypothesis class is parameterized by the set of scalars that can be represented using a bits floating point number. there are at most such numbers hence the actual size of our hypothesis class is at most more generally if our hypothesis class is parameterized by d numbers in practice we learn a hypothesis class of size at most applying corollary we obtain that the sample complexity of such learning via uniform convergence classes is bounded by this upper bound on the sample complexity has the deficiency of being dependent on the specific representation of real numbers used by our machine. in chapter we will introduce a rigorous way to analyze the sample complexity of infinite size hypothesis classes. nevertheless the discretization trick can be used to get a rough estimate of the sample complexity in many practical situations. summary if the uniform convergence property holds for a hypothesis class h then in most cases the empirical risks of hypotheses in h will faithfully represent their true risks. uniform convergence suffices for agnostic pac learnability using the erm rule. we have shown that finite hypothesis classes enjoy the uniform convergence property and are hence agnostic pac learnable. bibliographic remarks classes of functions for which the uniform convergence property holds are also called glivenko-cantelli classes named after valery ivanovich glivenko and francesco paolo cantelli who proved the first uniform convergence result in the see gine zinn the relation between uniform convergence and learnability was thoroughly studied by vapnik see vapnik vapnik in fact as we will see later in chapter the fundamental theorem of learning theory states that in binary classification problems uniform convergence is not only a sufficient condition for learnability but is also a necessary condition. this is not the case for more general learning problems shamir srebro sridharan exercises in this exercise we show that the requirement on the convergence of errors in our definitions of pac learning is in fact quite close to a simpler looking requirement about averages expectations. prove that the following two statements are equivalent any learning algorithm a any probability distribution d and any loss function whose range is for every there exists m such that m m p s dm lim m e s dm exercises es dm denotes the expectation over samples s of size m. bounded loss functions in corollary we assumed that the range of the loss function is prove that if the range of the loss function is b then the sample complexity satisfies mh much the bias-complexity tradeoff in chapter we saw that unless one is careful the training data can mislead the learner and result in overfitting. to overcome this problem we restricted the search space to some hypothesis class h. such a hypothesis class can be viewed as reflecting some prior knowledge that the learner has about the task a belief that one of the members of the class h is a low-error model for the task. for example in our papayas taste problem on the basis of our previous experience with other fruits we may assume that some rectangle in the color-hardness plane predicts least approximately the papaya s tastiness. is such prior knowledge really necessary for the success of learning? maybe there exists some kind of universal learner that is a learner who has no prior knowledge about a certain task and is ready to be challenged by any task? let us elaborate on this point. a specific learning task is defined by an unknown distribution d over x y where the goal of the learner is to find a predictor h x y whose risk ldh is small enough. the question is therefore whether there exist a learning algorithm a and a training set size m such that for every distribution d if a receives m i.i.d. examples from d there is a high chance it outputs a predictor h that has a low risk. the first part of this chapter addresses this question formally. the no-freelunch theorem states that no such universal learner exists. to be more precise the theorem states that for binary classification prediction tasks for every learner there exists a distribution on which it fails. we say that the learner fails if upon receiving i.i.d. examples from that distribution its output hypothesis is likely to have a large risk say whereas for the same distribution there exists another learner that will output a hypothesis with a small risk. in other words the theorem states that no learner can succeed on all learnable tasks every learner has tasks on which it fails while other learners succeed. therefore when approaching a particular learning problem defined by some distribution d we should have some prior knowledge on d. one type of such prior knowledge is that d comes from some specific parametric family of distributions. we will study learning under such assumptions later on in chapter another type of prior knowledge on d which we assumed when defining the pac learning model is that there exists h in some predefined hypothesis class h such that ldh a softer type of prior knowledge on d is assuming that minh h ldh is small. in a sense this weaker assumption on d is a prerequisite for using the understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning the no-free-lunch theorem agnostic pac model in which we require that the risk of the output hypothesis will not be much larger than minh h ldh. in the second part of this chapter we study the benefits and pitfalls of using a hypothesis class as a means of formalizing prior knowledge. we decompose the error of an erm algorithm over a class h into two components. the first component reflects the quality of our prior knowledge measured by the minimal risk of a hypothesis in our hypothesis class minh h ldh. this component is also called the approximation error or the bias of the algorithm toward choosing a hypothesis from h. the second component is the error due to overfitting which depends on the size or the complexity of the class h and is called the estimation error. these two terms imply a tradeoff between choosing a more complex h can decrease the bias but increases the risk of overfitting or a less complex h might increase the bias but decreases the potential overfitting. the no-free-lunch theorem in this part we prove that there is no universal learner. we do this by showing that no learner can succeed on all learning tasks as formalized in the following theorem theorem let a be any learning algorithm for the task of binary classification with respect to the loss over a domain x let m be any number smaller than representing a training set size. then there exists a distribution d over x such that there exists a function f x with ldf with probability of at least over the choice of s dm we have that ldas this theorem states that for every learner there exists a task on which it fails even though that task can be successfully learned by another learner. indeed a trivial successful learner in this case would be an erm learner with the hypothesis class h or more generally erm with respect to any finite hypothesis class that contains f and whose size satisfies the equation m corollary proof let c be a subset of x of size the intuition of the proof is that any learning algorithm that observes only half of the instances in c has no information on what should be the labels of the rest of the instances in c. therefore there exists a reality that is some target function f that would contradict the labels that as predicts on the unobserved instances in c. note that there are t possible functions from c to denote these functions by ft for each such function let di be a distribution over the bias-complexity tradeoff c defined by dix y if y fix otherwise. that is the probability to choose a pair y is if the label y is indeed the true label according to fi and the probability is if y fix. clearly ldifi we will show that for every algorithm a that receives a training set of m examples from c and returns a function as c it holds that max i e s dm i clearly this means that for every algorithm that receives a training set of m examples from x there exist a function f x and a distribution d over x such that ldf and e it is easy to verify that the preceding suffices for showing that which is what we need to prove exercise s dm we now turn to proving that equation holds. there are k possible sequences of m examples from c. denote these sequences by sk. also if sj xm we denote by si j the sequence containing the instances in sj labeled by the function fi namely si j fixm. if the distribution is di then the possible training sets a can receive are si si k and all these training sets have the same probability of being sampled. therefore e s dm i k ldiasi j. using the facts that maximum is larger than average and that average is larger than minimum we have k t min j t max i k ldiasi j t k ldiasi j ldiasi j ldiasi j. next fix some j denote sj xm and let vp be the examples in c that do not appear in sj. clearly p m. therefore for every the no-free-lunch theorem function h c and every i we have ldih x c hence t ldiasi j t t j j min r t j next fix some r we can partition all the functions in ft into t disjoint pairs where for a pair we have that for every c c fic if and only if c vr. since for such a pair we must have si j it follows that j j j which yields t j combining this with equation equation and equation we obtain that equation holds which concludes our proof. no-free-lunch and prior knowledge how does the no-free-lunch result relate to the need for prior knowledge? let us consider an erm predictor over the hypothesis class h of all the functions f from x to this class represents lack of prior knowledge every possible function from the domain to the label set is considered a good candidate. according to the no-free-lunch theorem any algorithm that chooses its output from hypotheses in h and in particular the erm predictor will fail on some learning task. therefore this class is not pac learnable as formalized in the following corollary corollary let x be an infinite domain set and let h be the set of all functions from x to then h is not pac learnable. the bias-complexity tradeoff proof assume by way of contradiction that the class is learnable. choose some and by the definition of pac learnability there must be some learning algorithm a and an integer m m such that for any data-generating distribution over x if for some function f x ldf then with probability greater than when a is applied to samples s of size m generated i.i.d. by d ldas however applying the no-free-lunch theorem since for every learning algorithm in particular for the algorithm a there exists a distribution d such that with probability greater than ldas which leads to the desired contradiction. how can we prevent such failures? we can escape the hazards foreseen by the no-free-lunch theorem by using our prior knowledge about a specific learning task to avoid the distributions that will cause us to fail when learning that task. such prior knowledge can be expressed by restricting our hypothesis class. but how should we choose a good hypothesis class? on the one hand we want to believe that this class includes the hypothesis that has no error at all the pac setting or at least that the smallest error achievable by a hypothesis from this class is indeed rather small the agnostic setting. on the other hand we have just seen that we cannot simply choose the richest class the class of all functions over the given domain. this tradeoff is discussed in the following section. error decomposition to answer this question we decompose the error of an ermh predictor into two components as follows. let hs be an ermh hypothesis. then we can write ldhs where min h h ldh ldhs the approximation error the minimum risk achievable by a predictor in the hypothesis class. this term measures how much risk we have because we restrict ourselves to a specific class namely how much inductive bias we have. the approximation error does not depend on the sample size and is determined by the hypothesis class chosen. enlarging the hypothesis class can decrease the approximation error. under the realizability assumption the approximation error is zero. in the agnostic case however the approximation error can be in fact it always includes the error of the bayes optimal predictor chapter the minimal yet inevitable error because of the possible nondeterminism of the world in this model. sometimes in the literature the term approximation error refers not to minh h ldh but rather to the excess error over that of the bayes optimal predictor namely minh h ldh summary the estimation error the difference between the approximation error and the error achieved by the erm predictor. the estimation error results because the empirical risk training error is only an estimate of the true risk and so the predictor minimizing the empirical risk is only an estimate of the predictor minimizing the true risk. the quality of this estimation depends on the training set size and on the size or complexity of the hypothesis class. as we have shown for a finite hypothesis class increases with and decreases with m. we can think of the size of h as a measure of its complexity. in future chapters we will define other complexity measures of hypothesis classes. since our goal is to minimize the total risk we face a tradeoff called the biascomplexity tradeoff. on one hand choosing h to be a very rich class decreases the approximation error but at the same time might increase the estimation error as a rich h might lead to overfitting. on the other hand choosing h to be a very small set reduces the estimation error but might increase the approximation error or in other words might lead to underfitting. of course a great choice for h is the class that contains only one classifier the bayes optimal classifier. but the bayes optimal classifier depends on the underlying distribution d which we do not know learning would have been unnecessary had we known d. learning theory studies how rich we can make h while still maintaining reasonable estimation error. in many cases empirical research focuses on designing good hypothesis classes for a certain domain. here good means classes for which the approximation error would not be excessively high. the idea is that although we are not experts and do not know how to construct the optimal classifier we still have some prior knowledge of the specific problem at hand which enables us to design hypothesis classes for which both the approximation error and the estimation error are not too large. getting back to our papayas example we do not know how exactly the color and hardness of a papaya predict its taste but we do know that papaya is a fruit and on the basis of previous experience with other fruit we conjecture that a rectangle in the color-hardness space may be a good predictor. summary the no-free-lunch theorem states that there is no universal learner. every learner has to be specified to some task and use some prior knowledge about that task in order to succeed. so far we have modeled our prior knowledge by restricting our output hypothesis to be a member of a chosen hypothesis class. when choosing this hypothesis class we face a tradeoff between a larger or more complex class that is more likely to have a small approximation error and a more restricted class that would guarantee that the estimation error will the bias-complexity tradeoff be small. in the next chapter we will study in more detail the behavior of the estimation error. in chapter we will discuss alternative ways to express prior knowledge. bibliographic remarks macready proved several no-free-lunch theorems for optimization but these are rather different from the theorem we prove here. the theorem we prove here is closely related to lower bounds in vc theory as we will study in the next chapter. exercises prove that equation suffices for showing that pldas hint let be a random variable that receives values in and whose expectation satisfies e use lemma to show that p assume you are asked to design a learning algorithm to predict whether patients are going to suffer a heart attack. relevant patient features the algorithm may have access to include blood pressure body-mass index age level of physical activity and income your choice. you have to choose between two algorithms the first picks an axis aligned rectangle in the two dimensional space spanned by the features bp and bmi and the other picks an axis aligned rectangle in the five dimensional space spanned by all the preceding features. explain the pros and cons of each choice. explain how the number of available labeled training samples will affect prove that if km for a positive integer k then we can replace the lower bound of in the no-free-lunch theorem with k namely let a be a learning algorithm for the task of binary classification. let m be any number smaller than representing a training set size. then there exists a distribution d over x such that there exists a function f x with ldf es dmldas the vc-dimension in the previous chapter we decomposed the error of the ermh rule into approximation error and estimation error. the approximation error depends on the fit of our prior knowledge reflected by the choice of the hypothesis class h to the underlying unknown distribution. in contrast the definition of pac learnability requires that the estimation error would be bounded uniformly over all distributions. our current goal is to figure out which classes h are pac learnable and to characterize exactly the sample complexity of learning a given hypothesis class. so far we have seen that finite classes are learnable but that the class of all functions an infinite size domain is not. what makes one class learnable and the other unlearnable? can infinite-size classes be learnable and if so what determines their sample complexity? we begin the chapter by showing that infinite classes can indeed be learnable and thus finiteness of the hypothesis class is not a necessary condition for learnability. we then present a remarkably crisp characterization of the family of learnable classes in the setup of binary valued classification with the zero-one loss. this characterization was first discovered by vladimir vapnik and alexey chervonenkis in and relies on a combinatorial notion called the vapnikchervonenkis dimension we formally define the vc-dimension provide several examples and then state the fundamental theorem of statistical learning theory which integrates the concepts of learnability vc-dimension the erm rule and uniform convergence. infinite-size classes can be learnable in chapter we saw that finite classes are learnable and in fact the sample complexity of a hypothesis class is upper bounded by the log of its size. to show that the size of the hypothesis class is not the right characterization of its sample complexity we first present a simple example of an infinite-size hypothesis class that is learnable. example let h be the set of threshold functions over the real line namely h a r where ha r is a function such that hax to remind the reader is if x a and otherwise. clearly h is of infinite understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning the vc-dimension size. nevertheless the following lemma shows that h is learnable in the pac model using the erm algorithm. lemma let h be the class of thresholds as defined earlier. then h is pac learnable using the erm rule with sample complexity of mh proof let be a threshold such that the hypothesis achieves let dx be the marginal distribution over the domain x and let be such that p x dx p x dx mass mass dx we set and similarly for given a training set s let maxx s and minx s no example in s is positive we set and if no example in s is negative we set let bs be a threshold corresponding to an erm hypothesis hs which implies that bs therefore a sufficient condition for ldhs is that both and in other words p s dm p s dm and using the union bound we can bound the preceding by p s dm p s dm p s dm the event happens if and only if all examples in s are not in the interval a whose probability mass is defined to be namely p s dm p s dm y s x e m. since we assume m it follows that the equation is at most in the same way it is easy to see that ps dm combining with equation we conclude our proof. the vc-dimension we see therefore that while finiteness of h is a sufficient condition for learnability it is not a necessary condition. as we will show a property called the vc-dimension of a hypothesis class gives the correct characterization of its learnability. to motivate the definition of the vc-dimension let us recall the no-freelunch theorem and its proof. there we have shown that without the vc-dimension restricting the hypothesis class for any learning algorithm an adversary can construct a distribution for which the learning algorithm will perform poorly while there is another learning algorithm that will succeed on the same distribution. to do so the adversary used a finite set c x and considered a family of distributions that are concentrated on elements of c. each distribution was derived from a true target function from c to to make any algorithm fail the adversary used the power of choosing a target function from the set of all possible functions from c to when considering pac learnability of a hypothesis class h the adversary is restricted to constructing distributions for which some hypothesis h h achieves a zero risk. since we are considering distributions that are concentrated on elements of c we should study how h behaves on c which leads to the following definition. definition of h to c let h be a class of functions from x to and let c cm x the restriction of h to c is the set of functions from c to that can be derived from h. that is hc hcm h h where we represent each function from c to as a vector in if the restriction of h to c is the set of all functions from c to then we say that h shatters the set c. formally definition a hypothesis class h shatters a finite set c x if the restriction of h to c is the set of all functions from c to that is example let h be the class of threshold functions over r. take a set c now if we take a then we have and if we take a then we have therefore hc is the set of all functions from c to and h shatters c. now take a set c where no h h can account for the labeling because any threshold that assigns the label to must assign the label to as well. therefore not all functions from c to are included in hc hence c is not shattered by h. getting back to the construction of an adversarial distribution as in the proof of the no-free-lunch theorem we see that whenever some set c is shattered by h the adversary is not restricted by h as they can construct a distribution over c based on any target function from c to while still maintaining the realizability assumption. this immediately yields corollary let h be a hypothesis class of functions from x to let m be a training set size. assume that there exists a set c x of size that is shattered by h. then for any learning algorithm a there exist a distribution d over x and a predictor h h such that ldh but with probability of at least over the choice of s dm we have that ldas the vc-dimension corollary tells us that if h shatters some set c of size then we cannot learn h using m examples. intuitively if a set c is shattered by h and we receive a sample containing half the instances of c the labels of these instances give us no information about the labels of the rest of the instances in c every possible labeling of the rest of the instances can be explained by some hypothesis in h. philosophically if someone can explain every phenomenon his explanations are worthless. this leads us directly to the definition of the vc dimension. definition the vc-dimension of a hypothesis class h denoted vcdimh is the maximal size of a set c x that can be shattered by h. if h can shatter sets of arbitrarily large size we say that h has infinite vc-dimension. a direct consequence of corollary is therefore theorem let h be a class of infinite vc-dimension. then h is not pac learnable. proof since h has an infinite vc-dimension for any training set size m there exists a shattered set of size and the claim follows by corollary we shall see later in this chapter that the converse is also true a finite vcdimension guarantees learnability. hence the vc-dimension characterizes pac learnability. but before delving into more theory we first show several examples. examples in this section we calculate the vc-dimension of several hypothesis classes. to show that vcdimh d we need to show that there exists a set c of size d that is shattered by h. every set c of size d is not shattered by h. threshold functions let h be the class of threshold functions over r. recall example where we have shown that for an arbitrary set c h shatters c therefore vcdimh we have also shown that for an arbitrary set c where h does not shatter c. we therefore conclude that vcdimh examples intervals let h be the class of intervals over r namely h a b r a b where hab r is a function such that habx take the set c then h shatters c sure you understand why and therefore vcdimh now take an arbitrary set c and assume without loss of generality that then the labeling cannot be obtained by an interval and therefore h does not shatter c. we therefore conclude that vcdimh axis aligned rectangles let h be the class of axis aligned rectangles formally h and where if and otherwise we shall show in the following that vcdimh to prove this we need to find a set of points that are shattered by h and show that no set of points can be shattered by h. finding a set of points that are shattered is easy figure now consider any set c of points. in c take a leftmost point first coordinate is the smallest in c a rightmost point coordinate is the largest a lowest point coordinate is the smallest and a highest point coordinate is the largest. without loss of generality denote c and let be the point that was not selected. now define the labeling it is impossible to obtain this labeling by an axis aligned rectangle. indeed such a rectangle must contain but in this case the rectangle contains as well because its coordinates are within the intervals defined by the selected points. so c is not shattered by h and therefore vcdimh figure left points that are shattered by axis aligned rectangles. right any axis aligned rectangle cannot label by and the rest of the points by the vc-dimension finite classes let h be a finite class. then clearly for any set c we have and thus c cannot be shattered if this implies that vcdimh this shows that the pac learnability of finite classes follows from the more general statement of pac learnability of classes with finite vc-dimension which we shall see in the next section. note however that the vc-dimension of a finite class h can be significantly smaller than for example let x k for some integer k and consider the class of threshold functions defined in example then k but vcdimh since k can be arbitrarily large the gap between and vcdimh can be arbitrarily large. vc-dimension and the number of parameters in the previous examples the vc-dimension happened to equal the number of parameters defining the hypothesis class. while this is often the case it is not always true. consider for example the domain x r and the hypothesis class h r where h x is defined by h sin it is possible to prove that vcdimh namely for every d one can find d points that are shattered by h exercise the fundamental theorem of pac learning we have already shown that a class of infinite vc-dimension is not learnable. the converse statement is also true leading to the fundamental theorem of statistical learning theory theorem fundamental theorem of statistical learning let h be a hypothesis class of functions from a domain x to and let the loss function be the loss. then the following are equivalent h has the uniform convergence property. any erm rule is a successful agnostic pac learner for h. h is agnostic pac learnable. h is pac learnable. any erm rule is a successful pac learner for h. h has a finite vc-dimension. the proof of the theorem is given in the next section. not only does the vc-dimension characterize pac learnability it even deter mines the sample complexity. theorem fundamental theorem of statistical learning quantitative version let h be a hypothesis class of functions from a domain x to and let the loss function be the loss. assume that vcdimh d then there are absolute constants such that proof of theorem h has the uniform convergence property with sample complexity d much d h is agnostic pac learnable with sample complexity d mh d h is pac learnable with sample complexity mh d d the proof of this theorem is given in chapter remark we stated the fundamental theorem for binary classification tasks. a similar result holds for some other learning problems such as regression with the absolute loss or the squared loss. however the theorem does not hold for all learning tasks. in particular learnability is sometimes possible even though the uniform convergence property does not hold will see an example in chapter exercise furthermore in some situations the erm rule fails but learnability is possible with other learning rules. proof of theorem we have already seen that in chapter the implications and are trivial and so is the implications and follow from the no-free-lunch theorem. the difficult part is to show that the proof is based on two main claims if vcdimh d then even though h might be infinite when restricting it to a finite set c x its effective size is only ocd. that is the size of hc grows polynomially rather than exponentially with this claim is often referred to as sauer s lemma but it has also been stated and proved independently by shelah and by perles. the formal statement is given in section later. in section we have shown that finite hypothesis classes enjoy the uniform convergence property. in section later we generalize this result and show that uniform convergence holds whenever the hypothesis class has a small effective size. by small effective size we mean classes for which grows polynomially with sauer s lemma and the growth function we defined the notion of shattering by considering the restriction of h to a finite set of instances. the growth function measures the maximal effective size of h on a set of m examples. formally the vc-dimension definition function let h be a hypothesis class. then the growth function of h denoted h n n is defined as hm max c x in words h is the number of different functions from a set c of size m to that can be obtained by restricting h to c. obviously if vcdimh d then for any m d we have hm in such cases h induces all possible functions from c to the following beautiful lemma proposed independently by sauer shelah and perles shows that when m becomes larger than the vc-dimension the growth function increases polynomially rather than exponentially with m. lemma let h be a hypothesis class with vcdimh hm d then for all m hm in particular if m d then i proof of sauer s lemma to prove the lemma it suffices to prove the following stronger claim for any c cm we have h c h shatters b. the reason why equation is sufficient to prove the lemma is that if vcdimh d then no set whose size is larger than d is shattered by h and therefore c h shatters b i when m d the right-hand side of the preceding is at most lemma in appendix a. we are left with proving equation and we do it using an inductive argument. for m no matter what h is either both sides of equation equal or both sides equal empty set is always considered to be shattered by h. assume equation holds for sets of size k m and let us prove it for sets of size m. fix h and c cm. denote cm and in addition define the following two sets ym ym hc ym hc and ym ym hc ym hc. it is easy to verify that additionally since using the induction assumption on h and we have that h shatters b c b h shatters b. proof of theorem next define h to be h h s.t. hcm namely contains pairs of hypotheses that agree on and differ on using this definition it is clear that if shatters a set b then it also shatters the set b and vice versa. combining this with the fact that and using the inductive assumption applied on and we obtain that shatters b shatters b c b shatters b c b h shatters b. overall we have shown that c b h shatters b c b h shatters b c h shatters b which concludes our proof. uniform convergence for classes of small effective size in this section we prove that if h has small effective size then it enjoys the uniform convergence property. formally theorem let h be a class and let h be its growth function. then for every d and every with probability of at least over the choice of s dm we have lsh before proving the theorem let us first conclude the proof of theorem proof of theorem it suffices to prove that if the vc-dimension is finite then the uniform convergence property holds. we will prove that much log d from sauer s lemma we have that for m d combining this with theorem we obtain that with probability of at least ldh for simplicity assume hence ldh m the vc-dimension to ensure that the preceding is at most we need that m logm d standard algebraic manipulations lemma in appendix a show that a sufficient condition for the preceding to hold is that m log d remark the upper bound on much we derived in the proof theorem is not the tightest possible. a tighter analysis that yields the bounds given in theorem can be found in chapter proof of theorem we will start by showing that e lsh s dm sup h h since the random variable suph h lsh is nonnegative the proof of the theorem follows directly from the preceding using markov s inequality section h h we can rewrite ldh dm where additional i.i.d. sample. therefore lsh to bound the left-hand side of equation we first note that for every m is an lsh e e dm e s dm sup h h sup h h a generalization of the triangle inequality yields lsh lsh e dm s dm e dm and the fact that supermum of expectation is smaller than expectation of supremum yields e dm sup h h lsh e dm sup h h lsh. formally the previous two inequalities follow from jensen s inequality. combining all we obtain e s dm sup h h lsh lsh e dm e dm sup h h m sup h h i zi m m sup h h proof of theorem the expectation on the right-hand side is over a choice of two i.i.d. samples s zm and m. since all of these vectors are chosen i.i.d. nothing will change if we replace the name of the random vector zi with the i zi name of the random vector i zi. it follows that for in equation we will have the term every we have that equation equals i. if we do it instead of the term e dm m sup h h i zi since this holds for every it also holds if we sample each component of uniformly at random from the uniform distribution over denoted u hence equation also equals e u m e dm sup h h i zi and by the linearity of expectation it also equals e dm e u m i zi next fix s and and let c be the instances appearing in s and then we can take the supremum only over h hc. therefore i zi e u m sup h h m u m max h hc e m fix some h hc and denote h i zi. since e h and h is an average of independent variables each of which takes values in we have by hoeffding s inequality that for every i zi m p h m applying the union bound over h hc we obtain that for any finally lemma in appendix a tells us that the preceding implies p h max h hc e h max h hc m e s dm sup h h lsh combining all with the definition of h we have shown that the vc-dimension summary the fundamental theorem of learning theory characterizes pac learnability of classes of binary classifiers using vc-dimension. the vc-dimension of a class is a combinatorial property that denotes the maximal sample size that can be shattered by the class. the fundamental theorem states that a class is pac learnable if and only if its vc-dimension is finite and specifies the sample complexity required for pac learning. the theorem also shows that if a problem is at all learnable then uniform convergence holds and therefore the problem is learnable using the erm rule. bibliographic remarks the definition of vc-dimension and its relation to learnability and to uniform convergence is due to the seminal work of vapnik chervonenkis the relation to the definition of pac learnability is due to blumer ehrenfeucht haussler warmuth several generalizations of the vc-dimension have been proposed. for example the fat-shattering dimension characterizes learnability of some regression problems schapire sellie alon ben-david cesa-bianchi haussler bartlett long williamson anthony bartlet and the natarajan dimension characterizes learnability of some multiclass learning problems however in general there is no equivalence between learnability and uniform convergence. see shamir srebro sridharan daniely sabato ben-david shalev-shwartz sauer s lemma has been proved by sauer in response to a problem of erdos shelah perles proved it as a useful lemma for shelah s theory of stable models gil kalai us that at some later time benjy weiss asked perles about such a result in the context of ergodic theory and perles who forgot that he had proved it once proved it again. vapnik and chervonenkis proved the lemma in the context of statistical learning theory. exercises hypothesis classes if h then vcdimh. show the following monotonicity property of vc-dimension for every two given some finite domain set x and a number k figure out the vck hx k. that is the set of all functions that assign the value to exactly k elements of x dimension of each of the following classes prove your claims hx extremal-combinatorics-iii-some-basic-theorems exercises hat most k hx k or hx k. let x be the boolean hypercube for a set i n we define a parity function hi as follows. on a binary vector x xn i i hi xi mod is hi computes parity of bits in i. what is the vc-dimension of the class of all such parity functions hn-parity i n? we proved sauer s lemma by proving that for every class h of finite vc dimension d and every subset a of the domain a h shatters b i show that there are cases in which the previous two inequalities are strict the can be replaced by and cases in which they can be replaced by equalities. demonstrate all four combinations of and vc-dimension of axis aligned rectangles in rd let hd axis aligned rectangles in rd. we have already seen that prove that in general vcdimhd rec be the class of rec rec vc-dimension of boolean conjunctions let hd con. con be the class of boolean conjunctions over the variables xd we already know that this class is finite and thus pac learnable. in this question we calculate vcdimhd show that conclude that vcdimh d log show that hd show that vcdimhd con shatters the set of unit vectors i d. con hint assume by contradiction that there exists a set c that is shattered by hd con that satisfy con. let be hypotheses in hd con d. i j hicj i j otherwise for each i hi more accurately the conjunction that corresponds to hi contains some literal which is false on ci and true on cj for each j i. use the pigeonhole principle to show that there must be a pair i j d such that and use the same xk and use that fact to derive a contradiction to the requirements from the conjunctions hi hj. mcon of monotone boolean conjunctions over monotonicity here means that the conjunctions do not contain negations. consider the class hd the vc-dimension as in hd pothesis. we augment hd that vcdimhd mcon d. con the empty conjunction is interpreted as the all-positive hymcon with the all-negative hypothesis h show we have shown that for a finite hypothesis class h vcdimh however this is just an upper bound. the vc-dimension of a class can be much lower than that find an example of a class h of functions over the real interval x give an example of a finite hypothesis class h over the domain x such that h is infinite while vcdimh where vcdimh it is often the case that the vc-dimension of a hypothesis class equals can be bounded above by the number of parameters one needs to set in order to define each hypothesis in the class. for instance if h is the class of axis aligned rectangles in rd then vcdimh which is equal to the number of parameters used to define a rectangle in rd. here is an example that shows that this is not always the case. we will see that a hypothesis class might be very complex and even not learnable although it has a small number of parameters. consider the domain x r and the hypothesis class h r we take prove that vcdimh hint there is more than one way to prove the required result. one option is by applying the following lemma if is the binary expansion of x then for any natural number m xm provided that k m s.t. xk h a b s where let h be the class of signed intervals that is habsx s s if x b if x b calculate vcdimh. let h be a class of functions from x to prove that if vcdimh d for any d then for some probability distri bution d over x for every sample size m e s dm min h h ldh d m hint use exercise in chapter prove that for every h that is pac learnable vcdimh that this is the implication in theorem vc of union let be hypothesis classes over some fixed domain set x let d maxi vcdimhi and assume for simplicity that d exercises prove that vcdim r logr hint take a set of k examples and assume that they are shattered by the union class. therefore the union class can produce all possible labelings on these examples. use sauer s lemma to show that the union class cannot produce more than rkd labelings. therefore rkd. now use lemma prove that for r it holds that vcdim dudley classes in this question we discuss an algebraic framework for defining concept classes over rn and show a connection between the vc dimension of such classes and their algebraic properties. given a function f rn r we define the corresponding function pos for a class f of real valued functions we define a corresponding class of functions pos f f. we say that a family f of real valued functions is linearly closed if for all f g f and r r rg f addition and scalar multiplication of functions are defined point wise namely for all x rn rgx f rgx. note that if a family of functions is linearly closed then we can view it as a vector space over the reals. for a function g rn r and a family of functions f let f def f f. hypothesis classes that have a representation as pos g for some vector space of functions f and some function g are called dudley classes. show that for every g rn r and every vector space of functions f as defined earlier vcdimpos g vcdimpos for every linearly closed family of real valued functions f the vcdimension of the corresponding class pos equals the linear dimension of f a vector space. hint let fd be a basis for the vector space f. consider the mapping x fdx rn to rd. note that this mapping induces a matching between functions over rn of the form pos and homogeneous linear halfspaces in rd vc-dimension of the class of homogeneous linear halfspaces is analyzed in chapter show that each of the following classes can be represented as a dudley class the class hsn of halfspaces over rn chapter the class hhsn of all homogeneous halfspaces over rn chapter the class bd of all functions defined by balls in rd. use the dudley representation to figure out the vc-dimension of this class. let p d n denote the class of functions defined by polynomial inequalities of degree d namely n p is a polynomial of degree d in the variables xn p d the vc-dimension where for x xn hpx degree of a multivariable polynomial is the maximal sum of variable exponents over all of its terms. for example the degree of px use the dudley representation to figure out the vc-dimension of the is class p d the class of all d-degree polynomials over r. prove that the class of all polynomial classifiers over r has infinite vc-dimension. use the dudley representation to figure out the vc-dimension of the class p d n a function of d and n. nonuniform learnability the notions of pac learnability discussed so far in the book allow the sample sizes to depend on the accuracy and confidence parameters but they are uniform with respect to the labeling rule and the underlying data distribution. consequently classes that are learnable in that respect are limited must have a finite vc-dimension as stated by theorem in this chapter we consider more relaxed weaker notions of learnability. we discuss the usefulness of such notions and provide characterization of the concept classes that are learnable using these definitions. we begin this discussion by defining a notion of nonuniform learnability that allows the sample size to depend on the hypothesis to which the learner is compared. we then provide a characterization of nonuniform learnability and show that nonuniform learnability is a strict relaxation of agnostic pac learnability. we also show that a sufficient condition for nonuniform learnability is that h is a countable union of hypothesis classes each of which enjoys the uniform convergence property. these results will be proved in section by introducing a new learning paradigm which is called structural risk minimization in section we specify the srm paradigm for countable hypothesis classes which yields the minimum description length paradigm. the mdl paradigm gives a formal justification to a philosophical principle of induction called occam s razor. next in section we introduce consistency as an even weaker notion of learnability. finally we discuss the significance and usefulness of the different notions of learnability. nonuniform learnability nonuniform learnability allows the sample size to be nonuniform with respect to the different hypotheses with which the learner is competing. we say that a hypothesis h is with another hypothesis if with probability higher than ldh in pac learnability this notion of competitiveness is not very useful as we are looking for a hypothesis with an absolute low risk the realizable case or understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning nonuniform learnability with a low risk compared to the minimal risk achieved by hypotheses in our class the agnostic case. therefore the sample size depends only on the accuracy and confidence parameters. in nonuniform learnability however we allow the sample size to be of the form mh h namely it depends also on the h with which we are competing. formally definition a hypothesis class h is nonuniformly learnable if there exist a learning algorithm a and a function mnulh h n such that for every and for every h h if m mnulh h then for every distribution d with probability of at least over the choice of s dm it holds that ldas ldh at this point it might be useful to recall the definition of agnostic pac learnability a hypothesis class h is agnostically pac learnable if there exist a learning algorithm a and a function mh n such that for every and for every distribution d if m mh then with probability of at least over the choice of s dm it holds that ldas min note that this implies that for every h h h ldas ldh in both types of learnability we require that the output hypothesis will be with every other hypothesis in the class. but the difference between these two notions of learnability is the question of whether the sample size m may depend on the hypothesis h to which the error of as is compared. note that that nonuniform learnability is a relaxation of agnostic pac learnability. that is if a class is agnostic pac learnable then it is also nonuniformly learnable. characterizing nonuniform learnability our goal now is to characterize nonuniform learnability. in the previous chapter we have found a crisp characterization of pac learnable classes by showing that a class of binary classifiers is agnostic pac learnable if and only if its vcdimension is finite. in the following theorem we find a different characterization for nonuniform learnable classes for the task of binary classification. theorem a hypothesis class h of binary classifiers is nonuniformly learnable if and only if it is a countable union of agnostic pac learnable hypothesis classes. the proof of theorem relies on the following result of independent interest structural risk minimization union of hypothesis classes h theorem let h be a hypothesis class that can be written as a countable n n hn where each hn enjoys the uniform convergence property. then h is nonuniformly learnable. recall that in chapter we have shown that uniform convergence is sufficient for agnostic pac learnability. theorem generalizes this result to nonuniform learnability. the proof of this theorem will be given in the next section by introducing a new learning paradigm. we now turn to proving theorem proof of theorem first assume that h n n hn where each hn is agnostic pac learnable. using the fundamental theorem of statistical learning it follows that each hn has the uniform convergence property. therefore using theorem we obtain that h is nonuniform learnable. for the other direction assume that h is nonuniform learnable using some algorithm a. for every n n let hn h mnulh h n. clearly h n nhn. in addition using the definition of mnulh we know that for any distribution d that satisfies the realizability assumption with respect to hn with probability of at least over s dn we have that ldas using the fundamental theorem of statistical learning this implies that the vcdimension of hn must be finite and therefore hn is agnostic pac learnable. the following example shows that nonuniform learnability is a strict relaxation of agnostic pac learnability namely there are hypothesis classes that are nonuniform learnable but are not agnostic pac learnable. where p r r is a polynomial of degree n. let h example consider a binary classification problem with the instance domain being x r. for every n n let hn be the class of polynomial classifiers of degree n namely hn is the set of all classifiers of the form hx signpx n n hn. therefore h is the class of all polynomial classifiers over r. it is easy to verify that vcdimh while vcdimhn n exercise hence h is not pac learnable while on the basis of theorem h is nonuniformly learnable. structural risk minimization we do so by first assuming that h can be written as h so far we have encoded our prior knowledge by specifying a hypothesis class h which we believe includes a good predictor for the learning task at hand. yet another way to express our prior knowledge is by specifying preferences over hypotheses within h. in the structural risk minimization paradigm n n hn and then specifying a weight function w n which assigns a weight to each hypothesis class hn such that a higher weight reflects a stronger preference for the hypothesis class. in this section we discuss how to learn with such prior knowledge. in the next section we describe a couple of important weighting schemes including minimum description length. nonuniform learnability concretely let h be a hypothesis class that can be written as h n n hn. for example h may be the class of all polynomial classifiers where each hn is the class of polynomial classifiers of degree n example assume that for each n the class hn enjoys the uniform convergence property definition let us also define in chapter with a sample complexity function muchn the function n by min muchn m. in words we have a fixed sample size m and we are interested in the lowest possible upper bound on the gap between empirical and true risks achievable by using a sample of m examples. from the definitions of uniform convergence and it follows that for every m and with probability of at least over the choice of s dm we have that h hn lsh let w n be a function such wn we refer to w as a weight function over the hypothesis classes such a weight function can reflect the importance that the learner attributes to each hypothesis class or some measure of the complexity of different hypothesis classes. if h is a finite union of n hypothesis classes one can simply assign the same weight of to all hypothesis classes. this equal weighting corresponds to no a priori preference to any hypothesis class. of course if one believes prior knowledge that a certain hypothesis class is more likely to contain the correct target function then it should be assigned a larger weight reflecting this prior knowledge. when h is a infinite union of hypothesis classes a uniform weighting is not possible but many other weighting schemes may work. for example one can or wn n. later in this chapter we will provide another choose wn convenient way to define weighting functions using description languages. the srm rule follows a bound minimization approach. this means that the goal of the paradigm is to find a hypothesis that minimizes a certain upper bound on the true risk. the bound that the srm rule wishes to minimize is given in the following theorem. theorem let w n be a function such that h be a hypothesis class that can be written as h wn let n n hn where for each n hn satisfies the uniform convergence property with a sample complexity function let be as defined in equation then for every and muchn distribution d with probability of at least over the choice of s dm the following bound holds for every n n and h hn. lsh wn therefore for every and distribution d with probability of at least structural risk minimization it holds that h h ldh lsh min nh hn wn proof for each n define n wn applying the assumption that uniform convergence holds for all n with the rate given in equation we obtain that if we fix n in advance then with probability of at least n over the choice of s dm lsh n. at least h hn n n applying the union bound over n we obtain that with probability of n wn the preceding holds for all n which concludes our proof. denote nh minn h hn and then equation implies that ldh lsh wnh the srm paradigm searches for h that minimizes this bound as formalized in the following pseudocode prior knowledge structural risk minimization n hn where hn has uniform convergence with muchn h w n output h argminh wnh define as in equation nh as in equation input training set s dm confidence n wn unlike the erm paradigm discussed in previous chapters we no longer just care about the empirical risk lsh but we are willing to trade some of our bias toward low empirical risk with a bias toward classes for which wnh is smaller for the sake of a smaller estimation error. next we show that the srm paradigm can be used for nonuniform learning of every class which is a countable union of uniformly converging hypothesis classes. theorem let h be a hypothesis class such that h each hn has the uniform convergence property with sample complexity muchn w n be such that wn using the srm rule with rate n n hn where let then h is nonuniformly learnable mnulh h muchnh nonuniform learnability proof let a be the srm algorithm with respect to the weighting function w. wnh using the fact that n wn we can apply theorem to get that with probability of at least for every h h and let m muchnh over the choice of s dm we have that for every h lsh wnh the preceding holds in particular for the hypothesis as returned by the srm rule. by the definition of srm we obtain that ldas min finally if m muchnh wnh then clearly wnh in addition from the uniform convergence property of each hn we have that with probability of more than lsh ldh combining all the preceding we obtain that ldas ldh which concludes our proof. note that the previous theorem also proves theorem remark for nonuniform learnability we have shown that any countable union of classes of finite vc-dimension is nonuniformly learnable. it turns out that for any infinite domain set x the class of all binary valued functions over x is not a countable union of classes of finite vc-dimension. we leave the proof of this claim as a exercise exercise it follows that in some sense the no free lunch theorem holds for nonuniform learning as well namely whenever the domain is not finite there exists no nonuniform learner with respect to the class of all deterministic binary classifiers for each such classifier there exists a trivial algorithm that learns it erm with respect to the hypothesis class that contains only this classifier. it is interesting to compare the nonuniform learnability result given in theorem to the task of agnostic pac learning any specific hn separately. the prior knowledge or bias of a nonuniform learner for h is weaker it is searching for a model throughout the entire class h rather than being focused on one specific hn. the cost of this weakening of prior knowledge is the increase in sample complexity needed to compete with any specific h hn. for a concrete evaluation of this gap consider the task of binary classification with the zero-one loss. assume that for all n vcdimhn n. since muchn c is the contant appearing in theorem a straightforward calculation shows that c mnulh h muchn that is the cost of relaxing the learner s prior knowledge from a specific hn that contains the target h to a countable union of classes depends on the log of minimum description length and occam s razor the index of the first class in which h resides. that cost increases with the index of the class which can be interpreted as reflecting the value of knowing a good priority order on the hypotheses in h. union of singleton classes namely h minimum description length and occam s razor let h be a countable hypothesis class. then we can write h as a countable n nhn. by hoeffding s inequality each singleton class has the uniform convergence property with rate muc therefore the function given in equation becomes argmin hn h lsh and the srm rule becomes logwn equivalently we can think of w as a function from h to and then the srm rule becomes logwh argmin h h lsh it follows that in this case the prior knowledge is solely determined by the weight we assign to each hypothesis. we assign higher weights to hypotheses that we believe are more likely to be the correct one and in the learning algorithm we prefer hypotheses that have higher weights. in this section we discuss a particular convenient way to define a weight function over h which is derived from the length of descriptions given to hypotheses. having a hypothesis class one can wonder about how we describe or represent each hypothesis in the class. we naturally fix some description language. this can be english or a programming language or some set of mathematical formulas. in any of these languages a description consists of finite strings of symbols characters drawn from some fixed alphabet. we shall now formalize these notions. let h be the hypothesis class we wish to describe. fix some finite set of symbols characters which we call the alphabet. for concreteness we let a string is a finite sequence of symbols from for example is a string of length we denote by the length of a string. the set of all finite length strings is denoted a description language for h is a function d h mapping each member h of h to a string dh. dh is called the description of h and its length is denoted by we shall require that description languages be prefix-free namely for every distinct h dh is not a prefix of that is we do not allow that any string dh is exactly the first symbols of any longer string prefix-free collections of strings enjoy the following combinatorial property nonuniform learnability lemma inequality if s is a prefix-free set of strings then s proof define a probability distribution over the members of s as follows repeatedly toss an unbiased coin with faces labeled and until the sequence of outcomes is a member of s at that point stop. for each s let p be the probability that this process generates the string note that since s is prefix-free for every s if the coin toss outcomes follow the bits of then we will stop only once the sequence of outcomes equals we therefore get that for every s p since probabilities add up to at most our proof is concluded. in light of kraft s inequality any prefix-free description language of a hypothesis class h gives rise to a weighting function w over that hypothesis class we will simply set wh this observation immediately yields the following theorem let h be a hypothesis class and let d h be a prefixfree description language for h. then for every sample size m every confidence parameter and every probability distribution d with probability greater than over the choice of s dm we have that h h ldh lsh where is the length of dh. proof choose wh apply theorem with note that and as was the case with theorem this result suggests a learning paradigm for h given a training set s search for a hypothesis h h that minimizes the bound lsh in particular it suggests trading off empirical risk for saving description length. this yields the minimum description length learning paradigm. minimum description length prior knowledge h is a countable hypothesis class h is described by a prefix-free language over for every h h is the length of the representation of h input a training set s dm confidence output h argminh h lsh example let h be the class of all predictors that can be implemented using some programming language say c. let us represent each program using the minimum description length and occam s razor binary string obtained by running the gzip command on the program yields a prefix-free description language over the alphabet then is simply the length bits of the output of gzip when running on the c program corresponding to h. occam s razor theorem suggests that having two hypotheses sharing the same empirical risk the true risk of the one that has shorter description can be bounded by a lower value. thus this result can be viewed as conveying a philosophical message a short explanation is a hypothesis that has a short length tends to be more valid than a long explanation. this is a well known principle called occam s razor after william of ockham a english logician who is believed to have been the first to phrase it explicitly. here we provide one possible justification to this principle. the inequality of theorem shows that the more complex a hypothesis h is the sense of having a longer description the larger the sample size it has to fit to guarantee that it has a small true risk ldh. at a second glance our occam razor claim might seem somewhat problematic. in the context in which the occam razor principle is usually invoked in science the language according to which complexity is measured is a natural language whereas here we may consider any arbitrary abstract description language. assume that we have two hypotheses such that is much smaller than by the preceding result if both have the same error on a given training set s then the true error of h may be much higher than the true error of so one should prefer over h. however we could have chosen a different description language say one that assigns a string of length to h and a string of length to suddenly it looks as if one should prefer h over but these are the same h and for which we argued two sentences ago that should be preferable. where is the catch here? indeed there is no inherent generalizability difference between hypotheses. the crucial aspect here is the dependency order between the initial choice of language preference over hypotheses and the training set. as we know from the basic hoeffding s bound if we commit to any hypothesis before seeing the data then we are guaranteed a rather small estimation error term ldh lsh choosing a description language equivalently some weighting of hypotheses is a weak form of committing to a hypothesis. rather than committing to a single hypothesis we spread out our commitment among many. as long as it is done independently of the training sample our generalization bound holds. just as the choice of a single hypothesis to be evaluated by a sample can be arbitrary so is the choice of description language. nonuniform learnability other notions of learnability consistency the notion of learnability can be further relaxed by allowing the needed sample sizes to depend not only on and h but also on the underlying data-generating probability distribution d is used to generate the training sample and to determine the risk. this type of performance guarantee is captured by the notion of consistency of a learning rule. definition let z be a domain set let p be a set of probability distributions over z and let h be a hypothesis class. a learning rule a is consistent with respect to h and p if there exists a function mconh h p n such that for every every h h and every d p if m mnulh hd then with probability of at least over the choice of s dm it holds that ldas ldh if p is the set of all we say that a is universally consistent with respect to h. the notion of consistency is of course a relaxation of our previous notion of nonuniform learnability. clearly if an algorithm nonuniformly learns a class h it is also universally consistent for that class. the relaxation is strict in the sense that there are consistent learning rules that are not successful nonuniform learners. for example the algorithm memorize defined in example later is universally consistent for the class of all binary classifiers over n. however as we have argued before this class is not nonuniformly learnable. example consider the classification prediction algorithm memorize defined as follows. the algorithm memorizes the training examples and given a test point x it predicts the majority label among all labeled instances of x that exist in the training sample some fixed default label if no instance of x appears in the training set. it is possible to show exercise that the memorize algorithm is universally consistent for every countable domain x and a finite label set y the zero-one loss. intuitively it is not obvious that the memorize algorithm should be viewed as a learner since it lacks the aspect of generalization namely of using observed data to predict the labels of unseen examples. the fact that memorize is a consistent algorithm for the class of all functions over any countable domain set therefore raises doubt about the usefulness of consistency guarantees. furthermore the sharp-eyed reader may notice that the bad learner we introduced in chapter in the literature consistency is often defined using the notion of either convergence in probability to weak consistency or almost sure convergence to strong consistency. formally we assume that z is endowed with some sigma algebra of subsets and by all distributions we mean all probability distributions that have contained in their associated family of measurable subsets. discussing the different notions of learnability which led to overfitting is in fact the memorize algorithm. in the next section we discuss the significance of the different notions of learnability and revisit the no-free-lunch theorem in light of the different definitions of learnability. discussing the different notions of learnability we have given three definitions of learnability and we now discuss their usefulness. as is usually the case the usefulness of a mathematical definition depends on what we need it for. we therefore list several possible goals that we aim to achieve by defining learnability and discuss the usefulness of the different definitions in light of these goals. what is the risk of the learned hypothesis? the first possible goal of deriving performance guarantees on a learning algorithm is bounding the risk of the output predictor. here both pac learning and nonuniform learning give us an upper bound on the true risk of the learned hypothesis based on its empirical risk. consistency guarantees do not provide such a bound. however it is always possible to estimate the risk of the output predictor using a validation set will be described in chapter how many examples are required to be as good as the best hypothesis in h? when approaching a learning problem a natural question is how many examples we need to collect in order to learn it. here pac learning gives a crisp answer. however for both nonuniform learning and consistency we do not know in advance how many examples are required to learn h. in nonuniform learning this number depends on the best hypothesis in h and in consistency it also depends on the underlying distribution. in this sense pac learning is the only useful definition of learnability. on the flip side one should keep in mind that even if the estimation error of the predictor we learn is small its risk may still be large if h has a large approximation error. so for the question how many examples are required to be as good as the bayes optimal predictor? even pac guarantees do not provide us with a crisp answer. this reflects the fact that the usefulness of pac learning relies on the quality of our prior knowledge. pac guarantees also help us to understand what we should do next if our learning algorithm returns a hypothesis with a large risk since we can bound the part of the error that stems from estimation error and therefore know how much of the error is attributed to approximation error. if the approximation error is large we know that we should use a different hypothesis class. similarly if a nonuniform algorithm fails we can consider a different weighting function over of hypotheses. however when a consistent algorithm fails we have no idea whether this is because of the estimation error or the approximation error. furthermore even if we are sure we have a problem with the estimation nonuniform learnability error term we do not know how many more examples are needed to make the estimation error small. how to learn? how to express prior knowledge? maybe the most useful aspect of the theory of learning is in providing an answer to the question of how to learn. the definition of pac learning yields the limitation of learning the no-free-lunch theorem and the necessity of prior knowledge. it gives us a crisp way to encode prior knowledge by choosing a hypothesis class and once this choice is made we have a generic learning rule erm. the definition of nonuniform learnability also yields a crisp way to encode prior knowledge by specifying weights over of hypotheses of h. once this choice is made we again have a generic learning rule srm. the srm rule is also advantageous in model selection tasks where prior knowledge is partial. we elaborate on model selection in chapter and here we give a brief example. consider the problem of fitting a one dimensional polynomial to data namely our goal is to learn a function h r r and as prior knowledge we consider the hypothesis class of polynomials. however we might be uncertain regarding which degree d would give the best results for our data set a small degree might not fit the data well it will have a large approximation error whereas a high degree might lead to overfitting it will have a large estimation error. in the following we depict the result of fitting a polynomial of degrees and to the same training set. degree degree degree it is easy to see that the empirical risk decreases as we enlarge the degree. therefore if we choose h to be the class of all polynomials up to degree then the erm rule with respect to this class would output a degree polynomial and would overfit. on the other hand if we choose too small a hypothesis class say polynomials up to degree then the erm would suffer from underfitting a large approximation error. in contrast we can use the srm rule on the set of all polynomials while ordering subsets of h according to their degree and this will yield a degree polynomial since the combination of its empirical risk and the bound on its estimation error is the smallest. in other words the srm rule enables us to select the right model on the basis of the data itself. the price we pay for this flexibility a slight increase of the estimation error relative to pac learning w.r.t. the optimal degree is that we do not know in discussing the different notions of learnability advance how many examples are needed to compete with the best hypothesis in h. unlike the notions of pac learnability and nonuniform learnability the definition of consistency does not yield a natural learning paradigm or a way to encode prior knowledge. in fact in many cases there is no need for prior knowledge at all. for example we saw that even the memorize algorithm which intuitively should not be called a learning algorithm is a consistent algorithm for any class defined over a countable domain and a finite label set. this hints that consistency is a very weak requirement. which learning algorithm should we prefer? one may argue that even though consistency is a weak requirement it is desirable that a learning algorithm will be consistent with respect to the set of all functions from x to y which gives us a guarantee that for enough training examples we will always be as good as the bayes optimal predictor. therefore if we have two algorithms where one is consistent and the other one is not consistent we should prefer the consistent algorithm. however this argument is problematic for two reasons. first maybe it is the case that for most natural distributions we will observe in practice that the sample complexity of the consistent algorithm will be so large so that in every practical situation we will not obtain enough examples to enjoy this guarantee. second it is not very hard to make any pac or nonuniform learner consistent with respect to the class of all functions from x to y. concretely consider a countable domain x a finite label set y and a hypothesis class h of functions from x to y. we can make any nonuniform learner for h be consistent with respect to the class of all classifiers from x to y using the following simple trick upon receiving a training set we will first run the nonuniform learner over the training set and then we will obtain a bound on the true risk of the learned predictor. if this bound is small enough we are done. otherwise we revert to the memorize algorithm. this simple modification makes the algorithm consistent with respect to all functions from x to y. since it is easy to make any algorithm consistent it may not be wise to prefer one algorithm over the other just because of consistency considerations. the no-free-lunch theorem revisited recall that the no-free-lunch theorem from chapter implies that no algorithm can learn the class of all classifiers over an infinite domain. in contrast in this chapter we saw that the memorize algorithm is consistent with respect to the class of all classifiers over a countable infinite domain. to understand why these two statements do not contradict each other let us first recall the formal statement of the no-free-lunch theorem. let x be a countable infinite domain and let y the no-free-lunch theorem implies the following for any algorithm a and a training set size m there exist a distribution over x and a function x y such that if a nonuniform learnability will get a sample of m i.i.d. training examples labeled by then a is likely to return a classifier with a larger error. the consistency of memorize implies the following for every distribution over x and a labeling function x y there exists a training set size m depends on the distribution and on such that if memorize receives at least m examples it is likely to return a classifier with a small error. we see that in the no-free-lunch theorem we first fix the training set size and then find a distribution and a labeling function that are bad for this training set size. in contrast in consistency guarantees we first fix the distribution and the labeling function and only then do we find a training set size that suffices for learning this particular distribution and labeling function. summary we introduced nonuniform learnability as a relaxation of pac learnability and consistency as a relaxation of nonuniform learnability. this means that even classes of infinite vc-dimension can be learnable in some weaker sense of learnability. we discussed the usefulness of the different definitions of learnability. for hypothesis classes that are countable we can apply the minimum description length scheme where hypotheses with shorter descriptions are preferred following the principle of occam s razor. an interesting example is the hypothesis class of all predictors we can implement in c any other programming language which we can learn using the mdl scheme. arguably the class of all predictors we can implement in c is a powerful class of functions and probably contains all that we can hope to learn in practice. the ability to learn this class is impressive and seemingly this chapter should have been the last chapter of this book. this is not the case because of the computational aspect of learning that is the runtime needed to apply the learning rule. for example to implement the mdl paradigm with respect to all c programs we need to perform an exhaustive search over all c programs which will take forever. even the implementation of the erm paradigm with respect to all c programs of description length at most bits requires an exhaustive search over hypotheses. while the sample complexity the runtime is this is a huge of learning this class is just number much larger than the number of atoms in the visible universe. in the next chapter we formally define the computational complexity of learning. in the second part of this book we will study hypothesis classes for which the erm or srm schemes can be implemented efficiently. bibliographic remarks bibliographic remarks our definition of nonuniform learnability is related to the definition of an occamalgorithm in blumer ehrenfeucht haussler warmuth the concept of srm is due to chervonenkis vapnik the concept of mdl is due to rissanen the relation between srm and mdl is discussed in vapnik these notions are also closely related to the notion of regularization tikhonov we will elaborate on regularization in the second part of this book. the notion of consistency of estimators dates back to fisher our presentation of consistency follows steinwart christmann who also derived several no-free-lunch theorems. exercises prove that for any finite class h and any description language d h the vc-dimension of h is at most supdh h h the maximum description length of a predictor in h. furthermore if d is a prefix-free description then vcdimh supdh h h. let h n n be an infinite countable hypothesis class for binary classification. show that it is impossible to assign weights to the hypotheses in h such that h could be learnt nonuniformly using these weights. that is the weighting h h wh the weights would be monotonically nondecreasing. that is if i j then function w h should satisfy the finite. find a weighting function w h such hn where for every n n hn is h h wh and so that for all h h wh is determined by nh minn h hn and by consider a hypothesis class h whi whj. infinite. define such a function w when for all n hn is countable let h be some hypothesis class. for any h h let denote the description length of h according to some fixed description language. consider the mdl learning paradigm in which the algorithm returns hs arg min h h lsh where s is a sample of size m. for any b let hb h b and define h b arg min h hb ldh. nonuniform learnability prove a bound on ldhs ldh b in terms of b the confidence parameter and the size of the training set m. note such bounds are known as oracle inequalities in the literature we wish to estimate how good we are compared to a reference classifier oracle h b. in this question we wish to show a no-free-lunch result for nonuniform learnability namely that over any infinite domain the class of all functions is not learnable even under the relaxed nonuniform variation of learning. recall that an algorithm a nonuniformly learns a hypothesis class h if there exists a function mnulh h n such that for every and for every h h if m mnulh h then for every distribution d with probability of at least over the choice of s dm it holds that ldas ldh n n hn and for every n n vcdimhn is finite. so that h of classes n n such that h if such an algorithm exists then we say that h is nonuniformly learnable. let a be a nonuniform learner for a class h. for each n n define ha n h h n. prove that each such class hn has a finite vc-dimension. prove that if a class h is nonuniformly learnable then there are classes hn let h be a class that shatters an infinite set. then for every sequence n n hn there exists some n for which vcdimhn hint given a class h that shatters some infinite set k and a sequence of classes n n each having a finite vc-dimension start by defining subsets kn k such that for all n vcdimhn and for any n m kn km now pick for each such kn a function fn kn so that no h hn agrees with fn on the domain kn. finally define n n hn construct a class of functions from the unit interval to that construct a class of functions from the unit interval to that f x by combining these fn s and prove that f is nonuniformly learnable but not pac learnable. is not nonuniformly learnable. in this question we wish to show that the algorithm memorize is a consistent learner for every class of functions over any countable domain. let x be a countable domain and let d be a probability distribution over x let i n be an enumeration of the elements of x so that for all i j dxi dxj. prove that i n lim n dxi given any prove that there exists such that dx x dx exercises for every m n p s dm xi and xi s ne m. prove that for every if n is such that dxi for all i n then conclude that if x is countable then for every probability distribution d over x there exists a function md n such that for every if m md then p s dm x s prove that memorize is a consistent learner for every class of valued functions over any countable domain. the runtime of learning so far in the book we have studied the statistical perspective of learning namely how many samples are needed for learning. in other words we focused on the amount of information learning requires. however when considering automated learning computational resources also play a major role in determining the complexity of a task that is how much computation is involved in carrying out a learning task. once a sufficient training sample is available to the learner there is some computation to be done to extract a hypothesis or figure out the label of a given test instance. these computational resources are crucial in any practical application of machine learning. we refer to these two types of resources as the sample complexity and the computational complexity. in this chapter we turn our attention to the computational complexity of learning. the computational complexity of learning should be viewed in the wider context of the computational complexity of general algorithmic tasks. this area has been extensively investigated see for example the introductory comments that follow summarize the basic ideas of that general theory that are most relevant to our discussion. the actual runtime seconds of an algorithm depends on the specific machine the algorithm is being implemented on what the clock rate of the machine s cpu is. to avoid dependence on the specific machine it is common to analyze the runtime of algorithms in an asymptotic sense. for example we say that the computational complexity of the merge-sort algorithm which sorts a list of n items is on logn. this implies that we can implement the algorithm on any machine that satisfies the requirements of some accepted abstract model of computation and the actual runtime in seconds will satisfy the following there exist constants c and which can depend on the actual machine such that for any value of n the runtime in seconds of sorting any n items will be at most c n logn. it is common to use the term feasible or efficiently computable for tasks that can be performed by an algorithm whose running time is opn for some polynomial function p. one should note that this type of analysis depends on defining what is the input size n of any instance to which the algorithm is expected to be applied. for purely algorithmic tasks as discussed in the common computational complexity literature this input size is clearly defined the algorithm gets an input instance say a list to be sorted or an arithmetic operation to be calculated which has a well defined size the understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning computational complexity of learning number of bits in its representation. for machine learning tasks the notion of an input size is not so clear. an algorithm aims to detect some pattern in a data set and can only access random samples of that data. we start the chapter by discussing this issue and define the computational complexity of learning. for advanced students we also provide a detailed formal definition. we then move on to consider the computational complexity of implementing the erm rule. we first give several examples of hypothesis classes where the erm rule can be efficiently implemented and then consider some cases where although the class is indeed efficiently learnable erm implementation is computationally hard. it follows that hardness of implementing erm does not imply hardness of learning. finally we briefly discuss how one can show hardness of a given learning task namely that no learning algorithm can solve it efficiently. computational complexity of learning recall that a learning algorithm has access to a domain of examples z a hypothesis class h a loss function and a training set of examples from z that are sampled i.i.d. according to an unknown distribution d. given parameters the algorithm should output a hypothesis h such that with probability of at least ldh min h as mentioned before the actual runtime of an algorithm in seconds depends on the specific machine. to allow machine independent analysis we use the standard approach in computational complexity theory. first we rely on a notion of an abstract machine such as a turing machine a turing machine over the reals shub smale second we analyze the runtime in an asymptotic sense while ignoring constant factors thus the specific machine is not important as long as it implements the abstract machine. usually the asymptote is with respect to the size of the input to the algorithm. for example for the merge-sort algorithm mentioned before we analyze the runtime as a function of the number of items that need to be sorted. in the context of learning algorithms there is no clear notion of input size. one might define the input size to be the size of the training set the algorithm receives but that would be rather pointless. if we give the algorithm a very large number of examples much larger than the sample complexity of the learning problem the algorithm can simply ignore the extra examples. therefore a larger training set does not make the learning problem more difficult and consequently the runtime available for a learning algorithm should not increase as we increase the size of the training set. just the same we can still analyze the runtime as a function of natural parameters of the problem such as the target accuracy the confidence of achieving that accuracy the dimensionality of the the runtime of learning domain set or some measures of the complexity of the hypothesis class with which the algorithm s output is compared. to illustrate this consider a learning algorithm for the task of learning axis aligned rectangles. a specific problem of learning axis aligned rectangles is derived by specifying and the dimension of the instance space. we can define a sequence of problems of the type rectangles learning by fixing and varying the dimension to be d we can also define another sequence of rectangles learning problems by fixing d and varying the target accuracy to be one can of course choose other sequences of such problems. once a sequence of the problems is fixed one can analyze the asymptotic runtime as a function of variables of that sequence. before we introduce the formal definition there is one more subtlety we need to tackle. on the basis of the preceding a learning algorithm can cheat by transferring the computational burden to the output hypothesis. for example the algorithm can simply define the output hypothesis to be the function that stores the training set in its memory and whenever it gets a test example x it calculates the erm hypothesis on the training set and applies it on x. note that in this case our algorithm has a fixed output the function that we have just described and can run in constant time. however learning is still hard the hardness is now in implementing the output classifier to obtain a label prediction. to prevent this cheating we shall require that the output of a learning algorithm must be applied to predict the label of a new example in time that does not exceed the runtime of training is computing the output classifier from the input training sample. in the next subsection the advanced reader may find a formal definition of the computational complexity of learning. formal definition the definition that follows relies on a notion of an underlying abstract machine which is usually either a turing machine or a turing machine over the reals. we will measure the computational complexity of an algorithm using the number of operations it needs to perform where we assume that for any machine that implements the underlying abstract machine there exists a constant c such that any such operation can be performed on the machine using c seconds. definition computational complexity of a learning algorithm we define the complexity of learning in two steps. first we consider the computational complexity of a fixed learning problem by a triplet a domain set a benchmark hypothesis class and a loss function. then in the second step we consider the rate of change of that complexity along a sequence of such tasks. given a function f n a learning task and a learning algorithm a we say that a solves the learning task in time of if there exists some constant number c such that for every probability distribution d implementing the erm rule over z and input when a has access to samples generated i.i.d. by d a terminates after performing at most cf operations the output of a denoted ha can be applied to predict the label of a new the output of a is probably approximately correct namely with probability of at least the random samples a receives ldha h example while performing at most cf operations consider a sequence of learning problems where problem n is defined by a domain zn a hypothesis class hn and a loss function let a be a learning algorithm designed for solving learning problems of this form. given a function g n n we say that the runtime of a with respect to the preceding sequence is og if for all n a solves the problem in time ofn where fn n is defined by fn gn we say that a is an efficient algorithm with respect to a sequence if its runtime is opn for some polynomial p. from this definition we see that the question whether a general learning problem can be solved efficiently depends on how it can be broken into a sequence of specific learning problems. for example consider the problem of learning a finite hypothesis class. as we showed in previous chapters the erm rule over h is guaranteed to h if the number of training examples is order of mh logh assuming that the evaluation of a hypothesis on an example takes a constant time it is possible to implement the erm rule in time oh mh by performing an exhaustive search over h with a training set of size mh for any fixed finite h the exhaustive search algorithm runs in polynomial time. furthermore if we define a sequence of problems in which n then the exhaustive search is still considered to be efficient. however if we define a sequence of problems for which then the sample complexity is still polynomial in n but the computational complexity of the exhaustive search algorithm grows exponentially with n rendered inefficient. implementing the erm rule given a hypothesis class h the ermh rule is maybe the most natural learning paradigm. furthermore for binary classification problems we saw that if learning is at all possible it is possible with the erm rule. in this section we discuss the computational complexity of implementing the erm rule for several hypothesis classes. given a hypothesis class h a domain set z and a loss function the corre sponding ermh rule can be defined as follows the runtime of learning on a finite input sample s zm output some h h that minimizes the empirical loss lsh z s z. this section studies the runtime of implementing the erm rule for several examples of learning tasks. finite classes limiting the hypothesis class to be a finite class may be considered as a reasonably mild restriction. for example h can be the set of all predictors that can be implemented by a c program written in at most bits of code. other examples of useful finite classes are any hypothesis class that can be parameterized by a finite number of parameters where we are satisfied with a representation of each of the parameters using a finite number of bits for example the class of axis aligned rectangles in the euclidean space rd when the parameters defining any given rectangle are specified up to some limited precision. as we have shown in previous chapters the sample complexity of learning a finite class is upper bounded by mh c logch where c in the realizable case and c in the nonrealizable case. therefore the sample complexity has a mild dependence on the size of h. in the example of c programs mentioned before the number of hypotheses is but the sample complexity is only logc a straightforward approach for implementing the erm rule over a finite hypothesis class is to perform an exhaustive search. that is for each h h we calculate the empirical risk lsh and return a hypothesis that minimizes the empirical risk. assuming that the evaluation of z on a single example takes a constant amount of time k the runtime of this exhaustive search becomes khm where m is the size of the training set. if we let m to be the upper bound on the sample complexity mentioned then the runtime becomes khc logch the linear dependence of the runtime on the size of h makes this approach inefficient unrealistic for large classes. formally if we define a sequence of such that loghn n then the exhaustive search problems approach yields an exponential runtime. in the example of c programs if hn is the set of functions that can be implemented by a c program written in at most n bits of code then the runtime grows exponentially with n implying that the exhaustive search approach is unrealistic for practical use. in fact this problem is one of the reasons we are dealing with other hypothesis classes like classes of linear predictors which we will encounter in the next chapter and not just focusing on finite classes. it is important to realize that the inefficiency of one algorithmic approach as the exhaustive search does not yet imply that no efficient erm implementation exists. indeed we will show examples in which the erm rule can be implemented efficiently. implementing the erm rule axis aligned rectangles let hn be the class of axis aligned rectangles in rn namely hn i ai bi where y if i xi bi otherwise efficiently learnable in the realizable case consider implementing the erm rule in the realizable case. that is we are given a training set s ym of examples such that there exists an axis aligned rectangle h hn for which hxi yi for all i. our goal is to find such an axis aligned rectangle with a zero training error namely a rectangle that is consistent with all the labels in s. we show later that this can be done in time onm. indeed for each i set ai minxi s and bi maxxi s. in words we take ai to be the minimal value of the i th coordinate of a positive example in s and bi to be the maximal value of the i th coordinate of a positive example in s. it is easy to verify that the resulting rectangle has zero training error and that the runtime of finding each ai and bi is om. hence the total runtime of this procedure is onm. not efficiently learnable in the agnostic case in the agnostic case we do not assume that some hypothesis h perfectly predicts the labels of all the examples in the training set. our goal is therefore to find h that minimizes the number of examples for which yi hxi. it turns out that for many common hypothesis classes including the classes of axis aligned rectangles we consider here solving the erm problem in the agnostic setting is np-hard in most cases it is even np-hard to find some h h whose error is no more than some constant c times that of the empirical risk minimizer in h. that is unless p np there is no algorithm whose running time is polynomial in m and n that is guaranteed to find an erm hypothesis for these problems eiron long on the other hand it is worthwhile noticing that if we fix one specific hypothesis class say axis aligned rectangles in some fixed dimension n then there exist efficient learning algorithms for this class. in other words there are successful agnostic pac learners that run in time polynomial in and their dependence on the dimension n is not polynomial. to see this recall the implementation of the erm rule we presented for the realizable case from which it follows that an axis aligned rectangle is determined by at most examples. therefore given a training set of size m we can perform an exhaustive search over all subsets of the training set of size at most examples and construct a rectangle from each such subset. then we can pick the runtime of learning the rectangle with the minimal training error. this procedure is guaranteed to find an erm hypothesis and the runtime of the procedure is mon. it follows that if n is fixed the runtime is polynomial in the sample size. this does not contradict the aforementioned hardness result since there we argued that unless pnp one cannot have an algorithm whose dependence on the dimension n is polynomial as well. boolean conjunctions a boolean conjunction is a mapping from x to y that can be expressed as a proposition formula of the form xik xjr for some indices ik jr the function that such a proposition formula defines is hx if xik and xjr otherwise let hn c be the class of all boolean conjunctions over the size of hn c is at most in a conjunction formula each element of x either appears or appears with a negation sign or does not appear at all and we also have the all negative formula. hence the sample complexity of learning hn c using the erm rule is at most n efficiently learnable in the realizable case next we show that it is possible to solve the erm problem for hn c in time polynomial in n and m. the idea is to define an erm conjunction by including in the hypothesis conjunction all the literals that do not contradict any positively labeled example. let vm be all the positively labeled instances in the input sample s. we define by induction on i m a sequence of hypotheses conjunctions. let be the conjunction of all possible literals. that is xn xn. note that assigns the label to all the elements of x we obtain by deleting from the conjunction hi all the literals that are not satisfied by the algorithm outputs the hypothesis hm. note that hm labels positively all the positively labeled examples in s. furthermore for every i m hi is the most restrictive conjunction that labels vi positively. now since we consider learning in the realizable setup there exists a conjunction hypothesis f hn c that is consistent with all the examples in s. since hm is the most restrictive conjunction that labels positively all the positively labeled members of s any instance labeled by f is also labeled by hm it follows that hm has zero training error s and is therefore a legal erm hypothesis. note that the running time of this algorithm is omn. efficiently learnable but not by a proper erm not efficiently learnable in the agnostic case as in the case of axis aligned rectangles unless p np there is no algorithm whose running time is polynomial in m and n that guaranteed to find an erm hypothesis for the class of boolean conjunctions in the unrealizable case. learning dnf we next show that a slight generalization of the class of boolean conjunctions leads to intractability of solving the erm problem even in the realizable case. consider the class of disjunctive normal form formulae dnf. the instance space is x and each hypothesis is represented by the boolean formula of the form hx where each aix is a boolean conjunction defined in the previous section. the output of hx is if either or or outputs the label if all three conjunctions output the label then hx of hn the erm rule is at most let hn is at most hence the sample complexity of learning hn be the hypothesis class of all such dnf formulae. the size using however from the computational perspective this learning problem is hard. it has been shown valiant kearns et al. that unless rp np there is no polynomial time algorithm that properly learns a sequence of dnf learning problems in which the dimension of the n th problem is n. by properly we mean that the algorithm should output a hypothesis that is a dnf formula. in particular since ermhn outputs a dnf formula it is a proper learner and therefore it is hard to implement it. the proof uses a reduction of the graph problem to the problem of pac learning dnf. the detailed technique is given in exercise see also vazirani section f efficiently learnable but not by a proper erm in the previous section we saw that it is impossible to implement the erm rule efficiently for the class hn of formulae. in this section we show that it is possible to learn this class efficiently but using erm with respect to a larger class. representation independent learning is not hard next we show that it is possible to learn dnf formulae efficiently. there is no contradiction to the hardness result mentioned in the previous section as we now allow representation independent learning. that is we allow the learning algorithm to output a hypothesis that is not a dnf formula. the basic idea is to replace the original hypothesis class of dnf formula with a larger hypothesis class so that the new class is easily learnable. the learning the runtime of learning algorithm might return a hypothesis that does not belong to the original hypothesis class hence the name representation independent learning. we emphasize that in most situations returning a hypothesis with good predictive ability is what we are really interested in doing. we start by noting that because distributes over each dnf formula can be rewritten as v w u next let us define such that for each triplet of literals u v w there is a variable in the range of indicating if u v w is true or false. so for each formula over there is a conjunction over with the same truth table. since we assume that the data is realizable we can solve the erm problem with respect to the class of conjunctions over furthermore the sample complexity of learning the class of conjunctions in the higher dimensional space is at most thus the overall runtime of this approach is polynomial in n. intuitively the idea is as follows. we started with a hypothesis class for which learning is hard. we switched to another representation where the hypothesis class is larger than the original class but has more structure which allows for a more efficient erm search. in the new representation solving the erm problem is easy. n o v e r i o n s c o n j u n c t formulae over hardness of learning we have just demonstrated that the computational hardness of implementing ermh does not imply that such a class h is not learnable. how can we prove that a learning problem is computationally hard? one approach is to rely on cryptographic assumptions. in some sense cryptography is the opposite of learning. in learning we try to uncover some rule underlying the examples we see whereas in cryptography the goal is to make sure that nobody will be able to discover some secret in spite of having access hardness of learning to some partial information about it. on that high level intuitive sense results about the cryptographic security of some system translate into results about the unlearnability of some corresponding task. regrettably currently one has no way of proving that a cryptographic protocol is not breakable. even the common assumption of p np does not suffice for that it can be shown to be necessary for most common cryptographic scenarios. the common approach for proving that cryptographic protocols are secure is to start with some cryptographic assumptions. the more these are used as a basis for cryptography the stronger is our belief that they really hold at least that algorithms that will refute them are hard to come by. we now briefly describe the basic idea of how to deduce hardness of learnability from cryptographic assumptions. many cryptographic systems rely on the assumption that there exists a one way function. roughly speaking a one way function is a function f formally it is a sequence of functions one for each dimension n that is easy to compute but is hard to invert. more formally f can be computed in time polyn but for any randomized polynomial time algorithm a and for every polynomial p pf f pn where the probability is taken over a random choice of x according to the uniform distribution over and the randomness of a. a one way function f is called trapdoor one way function if for some polynomial function p for every n there exists a bit-string sn a secret key of length pn such that there is a polynomial time algorithm that for every n and every x on input sn outputs x. in other words although f is hard to invert once one has access to its secret key inverting f becomes feasible. such functions are parameterized by their secret key. now let fn be a family of trapdoor functions over that can be calculated by some polynomial time algorithm. that is we fix an algorithm that given a secret key one function in fn and an input vector it calculates the value of the function corresponding to the secret key on the input vector in polynomial time. consider the task of learning the class of the corresponding f f fn. since each function in this class can be inverted inverses h n by some secret key sn of size polynomial in n the class h n f can be parameterized by these keys and its size is at most its sample complexity is therefore polynomial in n. we claim that there can be no efficient learner for this class. if there were such a learner l then by sampling uniformly at random a polynomial number of strings in and computing f over them we could generate a labeled training sample of pairs x which should suffice for our learner to figure out an approximation of f the uniform distribution over the range of f which would violate the one way property of f a more detailed treatment as well as a concrete example can be found in vazirani chapter using reductions they also show that the runtime of learning the class of functions that can be calculated by small boolean circuits is not efficiently learnable even in the realizable case. summary the runtime of learning algorithms is asymptotically analyzed as a function of different parameters of the learning problem such as the size of the hypothesis class our measure of accuracy our measure of confidence or the size of the domain set. we have demonstrated cases in which the erm rule can be implemented efficiently. for example we derived efficient algorithms for solving the erm problem for the class of boolean conjunctions and the class of axis aligned rectangles under the realizability assumption. however implementing erm for these classes in the agnostic case is np-hard. recall that from the statistical perspective there is no difference between the realizable and agnostic cases a class is learnable in both cases if and only if it has a finite vc-dimension. in contrast as we saw from the computational perspective the difference is immense. we have also shown another example the class of dnf where implementing erm is hard even in the realizable case yet the class is efficiently learnable by another algorithm. hardness of implementing the erm rule for several natural hypothesis classes has motivated the development of alternative learning methods which we will discuss in the next part of this book. bibliographic remarks valiant introduced the efficient pac learning model in which the runtime of the algorithm is required to be polynomial in and the representation size of hypotheses in the class. a detailed discussion and thorough bibliographic notes are given in kearns vazirani exercises let h be the class of intervals on the line equivalent to axis aligned rectangles in dimension n propose an implementation of the ermh learning rule the agnostic case that given a training set of size m runs in time hint use dynamic programming. let be a sequence of hypothesis classes for binary classification. assume that there is a learning algorithm that implements the erm rule in the realizable case such that the output hypothesis of the algorithm for each class hn only depends on on examples out of the training set. furthermore exercises assume that such a hypothesis can be calculated given these on examples in time on and that the empirical risk of each such hypothesis can be evaluated in time omn. for example if hn is the class of axis aligned rectangles in rn we saw that it is possible to find an erm hypothesis in the realizable case that is defined by at most examples. prove that in such cases it is possible to find an erm hypothesis for hn in the unrealizable case in time omn mon. in this exercise we present several classes for which finding an erm classifier is computationally hard. first we introduce the class of n-dimensional halfspaces hsn for a domain x rn. this is the class of all functions of the form hwbx b where w x rn is their inner product and b r. see a detailed description in chapter show that ermh over the class h hsn of linear predictors is computationally hard. more precisely we consider the sequence of problems in which the dimension n grows linearly and the number of examples m is set to be some constant times n. hint you can prove the hardness by a reduction from the following problem max fs given a system of linear inequalities ax b with a rm n and b rm is a system of m linear inequalities in n variables x xn find a subsystem containing as many inequalities as possible that has a solution a subsystem is called feasible. it has been shown that the problem max fs is np-hard. show that any algorithm that finds an ermhsn hypothesis for any training sample s can be used to solve the max fs problem of size m n. hint define a mapping that transforms linear inequalities in n variables into labeled points in rn and a mapping that transforms vectors in rn to halfspaces such that a vector w satisfies an inequality q if and only if the labeled point that corresponds to q is classified correctly by the halfspace corresponding to w. conclude that the problem of empirical risk minimization for halfspaces in also np-hard is if it can be solved in time polynomial in the sample size m and the euclidean dimension n then every problem in the class np can be solved in polynomial time. let x rn and let hn k be the class of all intersections of k-many linear halfspaces in rn. in this exercise we wish to show that ermhn is computationally hard for every k precisely we consider a sequence of problems where k is a constant and n grows linearly. the training set size m also grows linearly with n. towards this goal consider the k-coloring problem for graphs defined as follows given a graph g e and a number k determine whether there exists a function f v k so that for every v e f f the k-coloring problem is known to be np-hard for every k k the runtime of learning k k we wish to reduce the k-coloring problem to ermhn that is to prove that if there is an algorithm that solves the ermhn problem in time polynomial in k n and the sample size m then there is a polynomial time algorithm for the graph k-coloring problem. given a graph g e let vn be the vertices in v construct a sample sg where m as follows for every vi v construct an instance ei with a negative label. for every edge vj e construct an instance with a prove that if there exists some h hn k that has zero error over sg positive label. hint let h hj be an erm classifier in hn then g is k-colorable. k over s. define a coloring of v by setting f to be the minimal j such that hjei use the fact that halfspaces are convex sets to show that it cannot be true that two vertices that are connected by an edge have the same color. prove that if g is k-colorable then there exists some h h n k that has zero error over sg. hint given a coloring f of the vertices of g we should come up with k hyperplanes hk whose intersection is a perfect classifier for sg. let b for all of these hyperplanes and for t k let the i th weight of the t th hyperplane wti be if f t and otherwise. based on the above prove that for any k the ermhn problem is k np-hard. in this exercise we show that hardness of solving the erm problem is equivalent to hardness of proper pac learning. recall that by properness of the algorithm we mean that it must output a hypothesis from the hypothesis class. to formalize this statement we first need the following definition. definition the complexity class randomized polynomial time is the class of all decision problems is problems in which on any instance one has to find out whether the answer is yes or no for which there exists a probabilistic algorithm the algorithm is allowed to flip random coins while it is running with these properties on any input instance the algorithm runs in polynomial time in the input size. if the correct answer is no the algorithm must return no. if the correct answer is yes the algorithm returns yes with probability a and returns no with probability clearly the class rp contains the class p. it is also known that rp is contained in the class np. it is not known whether any equality holds among these three complexity classes but it is widely believed that np is strictly the constant in the definition can be replaced by any constant in exercises larger than rp. in particular it is believed that np-hard problems cannot be solved by a randomized polynomial time algorithm. show that if a class h is properly pac learnable by a polynomial time algorithm then the ermh problem is in the class rp. in particular this implies that whenever the ermh problem is np-hard example the class of intersections of halfspaces discussed in the previous exercise then unless np rp there exists no polynomial time proper pac learning algorithm for h. hint assume you have an algorithm a that properly pac learns a class h in time polynomial in some class parameter n as well as in and your goal is to use that algorithm as a subroutine to contract an algorithm b for solving the ermh problem in random polynomial time. given a training set s and some h h whose error on s is zero apply the pac learning algorithm to the uniform distribution over s and run it so that with probability it finds a function h h that has error less than respect to that uniform distribution. show that the algorithm just described satisfies the requirements for being a rp solver for ermh. part ii from theory to algorithms linear predictors in this chapter we will study the family of linear predictors one of the most useful families of hypothesis classes. many learning algorithms that are being widely used in practice rely on linear predictors first and foremost because of the ability to learn them efficiently in many cases. in addition linear predictors are intuitive are easy to interpret and fit the data reasonably well in many natural learning problems. we will introduce several hypothesis classes belonging to this family halfspaces linear regression predictors and logistic regression predictors and present relevant learning algorithms linear programming and the perceptron algorithm for the class of halfspaces and the least squares algorithm for linear regression. this chapter is focused on learning linear predictors using the erm approach however in later chapters we will see alternative paradigms for learning these hypothesis classes. first we define the class of affine functions as where hwbx b wixi b. ld w rd b r it will be convenient also to use the notation ld b w rd b r which reads as follows ld is a set of functions where each function is parameterized by w rd and b r and each such function takes as input a vector x and returns as output the scalar b. the different hypothesis classes of linear predictors are compositions of a function r y on ld. for example in binary classification we can choose to be the sign function and for regression problems where y r is simply the identity function. it may be more convenient to incorporate b called the bias into w as an extra coordinate and add an extra coordinate with a value of to all x x namely let wd and let xd understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning linear predictors therefore hwbx b it follows that each affine function in rd can be rewritten as a homogenous linear function in applied over the transformation that appends the constant to each input vector. therefore whenever it simplifies the presentation we will omit the bias term and refer to ld as the class of homogenous linear functions of the form hwx throughout the book we often use the general term linear functions for both affine functions and linear functions. halfspaces the first hypothesis class we consider is the class of halfspaces designed for binary classification problems namely x rd and y the class of halfspaces is defined as follows hsd sign ld signhwbx hwb ld. in other words each halfspace hypothesis in hsd is parameterized by w rd and b r and upon receiving a vector x the hypothesis returns the label b. to illustrate this hypothesis class geometrically it is instructive to consider the case d each hypothesis forms a hyperplane that is perpendicular to the vector w and intersects the vertical axis at the point the instances that are above the hyperplane that is share an acute angle with w are labeled positively. instances that are below the hyperplane that is share an obtuse angle with w are labeled negatively. w in section we will show that vcdimhsd d it follows that we can learn halfspaces using the erm paradigm as long as the sample size is therefore we now discuss how to implement an erm procedure for halfspaces. we introduce below two solutions to finding an erm halfspace in the realizable case. in the context of halfspaces the realizable case is often referred to as the separable case since it is possible to separate with a hyperplane all the positive examples from all the negative examples. implementing the erm rule halfspaces in the nonseparable case the agnostic case is known to be computationally hard simon there are several approaches to learning nonseparable data. the most popular one is to use surrogate loss functions namely to learn a halfspace that does not necessarily minimize the empirical risk with the loss but rather with respect to a diffferent loss function. for example in section we will describe the logistic regression approach which can be implemented efficiently even in the nonseparable case. we will study surrogate loss functions in more detail later on in chapter linear programming for the class of halfspaces linear programs are problems that can be expressed as maximizing a linear function subject to linear inequalities. that is max w rd subject to aw v where w rd is the vector of variables we wish to determine a is an m d matrix and v rm u rd are vectors. linear programs can be solved and furthermore there are publicly available implementations of lp solvers. we will show that the erm problem for halfspaces in the realizable case can be expressed as a linear program. for simplicity we assume the homogenous case. let s yim be a training set of size m. since we assume the realizable case an erm predictor should have zero errors on the training set. that is we are looking for some vector w rd for which i m. yi equivalently we are looking for some vector w for which i m. let w be a vector that satisfies this condition must exist since we assume realizability. define and let w w therefore for all i we have w we have thus shown that there exists a vector that satisfies i m. and clearly such a vector is an erm predictor. to find a vector that satisfies equation we can rely on an lp solver as follows. set a to be the m d matrix whose rows are the instances multiplied namely in time polynomial in m d and in the representation size of real numbers. linear predictors by yi. that is aij yi xij where xij is the j th element of the vector xi. let v be the vector rm. then equation can be rewritten as aw v. the lp form requires a maximization objective yet all the w that satisfy the constraints are equal candidates as output hypotheses. thus we set a dummy objective u rd. perceptron for halfspaces a different implementation of the erm rule is the perceptron algorithm of rosenblatt the perceptron is an iterative algorithm that constructs a sequence of vectors initially is set to be the all-zeros vector. at iteration t the perceptron finds an example i that is mislabeled by wt namely an example for which yi. then the perceptron updates wt by adding to it the instance xi scaled by the label yi. that is wt yixi. recall that our goal is to have for all i and note that yixi hence the update of the perceptron guides the solution to be more correct on the i th example. batch perceptron input a training set ym initialize for t if i s.t. then wt yixi else output wt the following theorem guarantees that in the realizable case the algorithm stops with all sample points correctly classified. theorem assume that ym is separable let b and let r maxi then the perceptron al i gorithm stops after at most iterations and when it stops it holds that i proof by the definition of the stopping condition if the perceptron stops it must have separated all the examples. we will show that if the perceptron runs for t iterations then we must have t which implies the perceptron must stop after at most iterations. let be a vector that achieves the minimum in the definition of b. that is halfspaces for all i and among all vectors that satisfy these constraints is of minimal norm. the idea of the proof is to show that after performing t iterations the cosine of the angle between and wt is at least rb that is we will show that t wt t rb by the cauchy-schwartz inequality the left-hand side of equation is at most therefore equation would imply that t rb t which will conclude our proof. to show that equation holds we first show that wt t indeed at the first iteration and therefore while on iteration t if we update using example yi we have that therefore after performing t iterations we get t wt as required. next we upper bound for each iteration t we have that i where the last inequality is due to the fact that example i is necessarily such that and the norm of xi is at most r. now since if we use equation recursively for t iterations we obtain that combining equation with equation and using the fact that b we obtain that t r. t t r b t b r we have thus shown that equation holds and this concludes our proof. linear predictors remark the perceptron is simple to implement and is guaranteed to converge. however the convergence rate depends on the parameter b which in some situations might be exponentially large in d. in such cases it would be better to implement the erm problem by solving a linear program as described in the previous section. nevertheless for many natural data sets the size of b is not too large and the perceptron converges quite fast. the vc dimension of halfspaces to compute the vc dimension of halfspaces we start with the homogenous case. theorem the vc dimension of the class of homogenous halfspaces in rd is d. proof first consider the set of vectors ed where for every i the vector ei is the all zeros vector except in the i th coordinate. this set is shattered by the class of homogenous halfspaces. indeed for every labeling yd set w yd and then yi for all i. real numbers not all of them are zero such that next let be a set of d vectors in rd. then there must exist aixi let i ai and j aj either i or j is nonempty. let us first assume that both of them are nonempty. then aixi now suppose that are shattered by the class of homogenous classes. then there must exist a vector w such that for all i i while for every j j. it follows that aixi w w j j j j i i i i i i j j which leads to a contradiction. finally if j i is empty then the right-most left-most inequality should be replaced by an equality which still leads to a contradiction. theorem the vc dimension of the class of nonhomogenous halfspaces in rd is d proof first as in the proof of theorem it is easy to verify that the set of vectors ed is shattered by the class of nonhomogenous halfspaces. second suppose that the vectors are shattered by the class of nonhomogenous halfspaces. but using the reduction we have shown in the beginning of this chapter it follows that there are d vectors in that are shattered by the class of homogenous halfspaces. but this contradicts theorem linear regression r r r r r r r rr r r figure linear regression for d for instance the x-axis may denote the age of the baby and the y-axis her weight. linear regression linear regression is a common statistical tool for modeling the relationship between some explanatory variables and some real valued outcome. cast as a learning problem the domain set x is a subset of rd for some d and the label set y is the set of real numbers. we would like to learn a linear function h rd r that best approximates the relationship between our variables for example predicting the weight of a baby as a function of her age and weight at birth. figure shows an example of a linear regression predictor for d the hypothesis class of linear regression predictors is simply the set of linear functions hreg ld b w rd b r. next we need to define a loss function for regression. while in classification the definition of the loss is straightforward as y simply indicates whether hx correctly predicts y or not in regression if the baby s weight is kg both the predictions kg and kg are wrong but we would clearly prefer the former over the latter. we therefore need to define how much we shall be penalized for the discrepancy between hx and y. one common way is to use the squared-loss function namely y for this loss function the empirical risk function is called the mean squared error namely lsh m linear predictors in the next subsection we will see how to implement the erm rule for linear regression with respect to the squared loss. of course there are a variety of other loss functions that one can use for example the absolute value loss function y y. the erm rule for the absolute value loss function can be implemented using linear programming exercise note that since linear regression is not a binary prediction task we cannot analyze its sample complexity using the vc-dimension. one possible analysis of the sample complexity of linear regression is by relying on the discretization trick remark in chapter namely if we are happy with a representation of each element of the vector w and the bias b using a finite number of bits a bits floating point representation then the hypothesis class becomes finite and its size is at most we can now rely on sample complexity bounds for finite hypothesis classes as described in chapter note however that to apply the sample complexity bounds from chapter we also need that the loss function will be bounded. later in the book we will describe more rigorous means to analyze the sample complexity of regression problems. least squares least squares is the algorithm that solves the erm problem for the hypothesis class of linear regression predictors with respect to the squared loss. the erm problem with respect to this class given a training set s and using the homogenous version of ld is to find argmin w lshw argmin w m to solve the problem we calculate the gradient of the objective function and compare it to zero. that is we need to solve m yixi we can rewrite the problem as the problem aw b where a xi i and b yixi. or in matrix form a b linear regression xm xm ym xm if a is invertible then the solution to the erm problem is w a b. the case in which a is not invertible requires a few standard tools from linear algebra which are available in appendix c. it can be easily shown that if the training instances do not span the entire space of rd then a is not invertible. nevertheless we can always find a solution to the system aw b because b is in the range of a. indeed since a is symmetric we can write it using its eigenvalue decomposition as a v dv where d is a diagonal matrix and v is an orthonormal matrix is v is the identity d d matrix. define d to be the diagonal matrix such that d ii if dii and otherwise d ii now define a v dv and w ab. let vi denote the i th column of v then we have a w aab v dv dv v ddv i b. that is a w is the projection of b onto the span of those vectors vi for which dii since the linear span of xm is the same as the linear span of those vi and b is in the linear span of the xi we obtain that a w b which concludes our argument. linear regression for polynomial regression tasks some learning tasks call for nonlinear predictors such as polynomial predictors. take for instance a one dimensional polynomial function of degree n that is px anxn where an is a vector of coefficients of size n in the following we depict a training set that is better fitted using a degree polynomial predictor than using a linear predictor. linear predictors we will focus here on the class of one dimensional n-degree polynomial re gression predictors namely hn poly px where p is a one dimensional polynomial of degree n parameterized by a vector of coefficients an. note that x r since this is a one dimensional polynomial and y r as this is a regression problem. one way to learn this class is by reduction to the problem of linear regression which we have already shown how to solve. to translate a polynomial regression problem to a linear regression problem we define the mapping r such that x xn. then we have that p anxn and we can find the optimal vector of coefficients a by using the least squares algorithm as shown earlier. logistic regression in logistic regression we learn a family of functions h from rd to the interval however logistic regression is used for classification tasks we can interpret hx as the probability that the label of x is the hypothesis class associated with logistic regression is the composition of a sigmoid function sig r over the class of linear functions ld. in particular the sigmoid function used in logistic regression is the logistic function defined as sigz exp z the name sigmoid means s-shaped referring to the plot of this function shown in the figure logistic regression the hypothesis class is therefore for simplicity we are using homogenous linear functions hsig sig ld w rd. note that when is very large then is close to whereas if is very small then is close to recall that the prediction of the halfspace corresponding to a vector w is therefore the predictions of the halfspace hypothesis and the logistic hypothesis are very similar whenever is large. however when is close to we have that intuitively the logistic hypothesis is not sure about the value of the label so it guesses that the label is with probability slightly larger than in contrast the halfspace hypothesis always outputs a deterministic prediction of either or even if is very close to next we need to specify a loss function. that is we should define how bad it is to predict some hwx given that the true label is y clearly we would like that hwx would be large if y and that hwx the probability of predicting would be large if y note that hwx exp exp exp therefore any reasonable loss function would increase monotonically with or equivalently would increase monotonically with exp the logistic loss function used in logistic regression penalizes hw based on the log of exp that log is a monotonic function. that is y log exp therefore given a training set s ym the erm problem associated with logistic regression is argmin w rd m log exp the advantage of the logistic loss function is that it is a convex function with respect to w hence the erm problem can be solved efficiently using standard methods. we will study how to learn with convex functions and in particular specify a simple algorithm for minimizing convex functions in later chapters. the erm problem associated with logistic regression is identical to the problem of finding a maximum likelihood estimator a well-known statistical approach for finding the parameters that maximize the joint probability of a given data set assuming a specific parametric probability function. we will study the maximum likelihood approach in chapter linear predictors summary the family of linear predictors is one of the most useful families of hypothesis classes and many learning algorithms that are being widely used in practice rely on linear predictors. we have shown efficient algorithms for learning linear predictors with respect to the zero-one loss in the separable case and with respect to the squared and logistic losses in the unrealizable case. in later chapters we will present the properties of the loss function that enable efficient learning. naturally linear predictors are effective whenever we assume as prior knowledge that some linear predictor attains low risk with respect to the underlying distribution. in the next chapter we show how to construct nonlinear predictors by composing linear predictors on top of simple classes. this will enable us to employ linear predictors for a variety of prior knowledge assumptions. bibliographic remarks the perceptron algorithm dates back to rosenblatt the proof of its convergence rate is due to novikoff least squares regression goes back to gauss legendre and adrain exercises show how to cast the erm problem of linear regression with respect to the absolute value loss function y y as a linear program namely show how to write the problem yi min w as a linear program. hint start with proving that for any c r min a a s.t. c a and c a. show that the matrix a defined in equation is invertible if and only if xm span rd. show that theorem is tight in the following sense for any positive integer m there exist a vector w rd some appropriate d and a sequence of examples ym such that the following hold r maxi m and for all i m w note that using the notation in theorem we therefore get b i m. exercises thus m. when running the perceptron on this sequence of examples it makes m updates before converging. hint choose d m and for every i choose xi ei. given any number m find an example of a sequence of labeled examples ym on which the upper bound of theorem equals m and the perceptron algorithm is bound to make m mistakes. hint set each xi to be a third dimensional vector of the form b yi where let w be the vector now go over the proof of the perceptron s upper bound see where we used inequalities rather than equalities and figure out scenarios where the inequality actually holds with equality. suppose we modify the perceptron algorithm as follows in the update step instead of performing wt yixi whenever we make a mistake we perform wt yixi for some prove that the modified perceptron will perform the same number of iterations as the vanilla perceptron and will converge to a vector that points to the same direction as the output of the vanilla perceptron. in this problem we will get bounds on the vc-dimension of the class of balls in rd that is bd v rd r where bvrx if r otherwise consider the mapping rd defined by show that if xm are shattered by bd then are shattered by the class of halfspaces in this question we assume that what does this tell us about vcdimbd? find a set of d points in rd that is shattered by bd. conclude that d vcdimbd d boosting boosting is an algorithmic paradigm that grew out of a theoretical question and became a very practical machine learning tool. the boosting approach uses a generalization of linear predictors to address two major issues that have been raised earlier in the book. the first is the bias-complexity tradeoff. we have seen chapter that the error of an erm learner can be decomposed into a sum of approximation error and estimation error. the more expressive the hypothesis class the learner is searching over the smaller the approximation error is but the larger the estimation error becomes. a learner is thus faced with the problem of picking a good tradeoff between these two considerations. the boosting paradigm allows the learner to have smooth control over this tradeoff. the learning starts with a basic class might have a large approximation error and as it progresses the class that the predictor may belong to grows richer. the second issue that boosting addresses is the computational complexity of learning. as seen in chapter for many interesting concept classes the task of finding an erm hypothesis may be computationally infeasible. a boosting algorithm amplifies the accuracy of weak learners. intuitively one can think of a weak learner as an algorithm that uses a simple rule of thumb to output a hypothesis that comes from an easy-to-learn hypothesis class and performs just slightly better than a random guess. when a weak learner can be implemented efficiently boosting provides a tool for aggregating such weak hypotheses to approximate gradually good predictors for larger and harder to learn classes. in this chapter we will describe and analyze a practically useful boosting algorithm adaboost shorthand for adaptive boosting. the adaboost algorithm outputs a hypothesis that is a linear combination of simple hypotheses. in other words adaboost relies on the family of hypothesis classes obtained by composing a linear predictor on top of simple classes. we will show that adaboost enables us to control the tradeoff between the approximation and estimation errors by varying a single parameter. adaboost demonstrates a general theme that will recur later in the book of expanding the expressiveness of linear predictors by composing them on top of other functions. this will be elaborated in section adaboost stemmed from the theoretical question of whether an efficient weak learner can be boosted into an efficient strong learner. this question was raised understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning weak learnability by kearns and valiant in and solved in by robert schapire then a graduate student at mit. however the proposed mechanism was not very practical. in robert schapire and yoav freund proposed the adaboost algorithm which was the first truly practical implementation of boosting. this simple and elegant algorithm became hugely popular and freund and schapire s work has been recognized by numerous awards. furthermore boosting is a great example for the practical impact of learning theory. while boosting originated as a purely theoretical problem it has led to popular and widely used algorithms. indeed as we shall demonstrate later in this chapter adaboost has been successfully used for learning to detect faces in images. weak learnability recall the definition of pac learning given in chapter a hypothesis class h is pac learnable if there exist mh n and a learning algorithm with the following property for every for every distribution d over x and for every labeling function f x if the realizable assumption holds with respect to hd f then when running the learning algorithm on m mh i.i.d. examples generated by d and labeled by f the algorithm returns a hypothesis h such that with probability of at least ldf furthermore the fundamental theorem of learning theory in chapter characterizes the family of learnable classes and states that every pac learnable class can be learned using any erm algorithm. however the definition of pac learning and the fundamental theorem of learning theory ignores the computational aspect of learning. indeed as we have shown in chapter there are cases in which implementing the erm rule is computationally hard in the realizable case. however perhaps we can trade computational hardness with the requirement for accuracy. given a distribution d and a target labeling function f maybe there exists an efficiently computable learning algorithm whose error is just slightly better than a random guess? this motivates the following definition. definition a learning algorithm a is a for a class h if there exists a function mh n such that for every for every distribution d over x and for every labeling function f x if the realizable assumption holds with respect to hd f then when running the learning algorithm on m mh i.i.d. examples generated by d and labeled by f the algorithm returns a hypothesis h such that with probability of at least ldf a hypothesis class h is if there exists a for that class. boosting this definition is almost identical to the definition of pac learning which here we will call strong learning with one crucial difference strong learnability implies the ability to find an arbitrarily good classifier error rate at most for an arbitrarily small in weak learnability however we only need to output a hypothesis whose error rate is at most namely whose error rate is slightly better than what a random labeling would give us. the hope is that it may be easier to come up with efficient weak learners than with efficient pac learners. the fundamental theorem of learning states that if a hypothesis class h has a vc dimension d then the sample complexity of pac learning h satisfies mh where is a constant. applying this with we immediately obtain that if d then h is not this implies that from the statistical perspective if we ignore computational complexity weak learnability is also characterized by the vc dimension of h and therefore is just as hard as pac learning. however when we do consider computational complexity the potential advantage of weak learning is that maybe there is an algorithm that satisfies the requirements of weak learning and can be implemented efficiently. one possible approach is to take a simple hypothesis class denoted b and to apply erm with respect to b as the weak learning algorithm. for this to work we need that b will satisfy two requirements ermb is efficiently implementable. for every sample that is labeled by some hypothesis from h any ermb hypothesis will have an error of at most then the immediate question is whether we can boost an efficient weak learner into an efficient strong learner. in the next section we will show that this is indeed possible but before that let us show an example in which efficient weak learnability of a class h is possible using a base hypothesis class b. example learning of classifiers using decision stumps let x r and let h be the class of classifiers namely h r b where for every x h b if x or x if x an example hypothesis b is illustrated as follows let b be the class of decision stumps that is b signx b r b in the following we show that ermb is a learner for h for weak learnability to see that we first show that for every distribution that is consistent with h there exists a decision stump with ldh indeed just note that every classifier in h consists of three regions unbounded rays and a center interval with alternate labels. for any pair of such regions there exists a decision stump that agrees with the labeling of these two components. note that for every distribution d over r and every partitioning of the line into three such regions one of these regions must have d-weight of at most let h h be a zero error hypothesis. a decision stump that disagrees with h only on such a region has an error of at most finally since the vc-dimension of decision stumps is if the sample size is greater than then with probability of at least the ermb rule returns a hypothesis with an error of at most setting we obtain that the error of ermb is at most we see that ermb is a learner for h. we next show how to implement the erm rule efficiently for decision stumps. efficient implementation of erm for decision stumps let x rd and consider the base hypothesis class of decision stumps over rd namely hds sign xi b r i b for simplicity assume that b that is we focus on all the hypotheses in hds of the form sign xi. let s ym be a training set. we will show how to implement an erm rule namely how to find a decision stump that minimizes lsh. furthermore since in the next section we will show that adaboost requires finding a hypothesis with a small risk relative to some distribution over s we will show here how to minimize such risk functions. concretely let d be a probability vector in rm is all elements of d are i di the weak learner we describe later receives d and s and outputs a decision stump h x y that minimizes the risk w.r.t. d nonnegative ldh note that if d then ldh lsh. recall that each decision stump is parameterized by an index j and a threshold therefore minimizing ldh amounts to solving the problem iyi min j min r fix j and let us sort the examples so that xmj. define i note that for any r j xij there exists j that yields the same predictions for the sample s as the boosting threshold therefore instead of minimizing over r we can minimize over j. this already gives us an efficient procedure choose j and j that minimize the objective value of equation for every j and j we have to calculate a sum over m examples therefore the runtime of this approach would be we next show a simple trick that enables us to minimize the objective in time odm. the observation is as follows. suppose we have calculated the objective for xij. let f be the value of the objective. then when we consider we have that f f f yidi. therefore we can calculate the objective at in a constant time given the objective at the previous threshold it follows that after a preprocessing step in which we sort the examples with respect to each coordinate the minimization problem can be performed in time odm. this yields the following pseudocode. erm for decision stumps input training set s ym distribution vector d goal find that solve equation initialize f for j d sort s using the j th coordinate and denote xmj def xmj f di if f f f f j for i m f f yidi if f f and xij f f j output adaboost adaboost for adaptive boosting is an algorithm that has access to a weak learner and finds a hypothesis with a low empirical risk. the adaboost algorithm receives as input a training set of examples s ym where for each i yi f for some labeling function f the boosting process proceeds in a sequence of consecutive rounds. at round t the booster first defines adaboost dt a distribution over the examples in s denoted dt. that is dt rm and i then the booster passes the distribution dt and the sample s to the weak learner. way the weak learner can construct i.i.d. examples according to dt and f the weak learner is assumed to return a weak hypothesis ht whose error def ldt def dt i course there is a probability of at most that the weak learner is at most fails. then adaboost assigns a weight for ht as follows wt that is the weight of ht is inversely proportional to the error of ht. at the end of the round adaboost updates the distribution so that examples on which ht errs will get a higher probability mass while examples on which ht is correct will get a lower probability mass. intuitively this will force the weak learner to focus on the problematic examples in the next round. the output of the adaboost algorithm is a strong classifier that is based on a weighted sum of all the weak hypotheses. the pseudocode of adaboost is presented in the following. log adaboost input training set s ym weak learner wl number of rounds t invoke weak learner ht wldt s m m initialize for t t compute dt let wt log update i i i dt dt j exp wtyihtxi exp wtyj htxj output the hypothesis hsx sign wthtx for all i m the following theorem shows that the training error of the output hypothesis decreases exponentially fast with the number of boosting rounds. theorem let s be a training set and assume that at each iteration of adaboost the weak learner returns a hypothesis for which then the training error of the output hypothesis of adaboost is at most proof for each t denote ft lshs m exp t p t wphp. therefore the output of adaboost boosting is ft in addition denote zt m e yiftxi. note that for any hypothesis we have that e yhx. therefore lsft zt so it suffices to show that zt e to upper bound zt we rewrite it as zt where we used the fact that because therefore it suffices to show that for every round t zt zt zt zt zt e zt to do so we first note that using a simple inductive argument for all t and i i i by our assumption tonically increasing in we obtain that since the function ga a is mono hence zt i e yiftxi e yj ftxj i e yj ftxj e e e yj ftxj e yiftxie e e linear combinations of base hypotheses finally using the inequality a e a we have that e this shows that equation holds and thus concludes our proof. e each iteration of adaboost involves om operations as well as a single call to the weak learner. therefore if the weak learner can be implemented efficiently happens in the case of erm with respect to decision stumps then the total training process will be efficient. remark theorem assumes that at each iteration of adaboost the weak learner returns a hypothesis with weighted sample error of at most according to the definition of a weak learner it can fail with probability using the union bound the probability that the weak learner will not fail at all of the iterations is at least t as we show in exercise the dependence of the sample complexity on can always be logarithmic in and therefore invoking the weak learner with a very small is not problematic. we can therefore assume that t is also small. furthermore since the weak learner is only applied with distributions over the training set in many cases we can implement the weak learner so that it will have a zero probability of failure this is the case for example in the weak learner that finds the minimum value of ldh for decision stumps as described in the previous section. theorem tells us that the empirical risk of the hypothesis constructed by adaboost goes to zero as t grows. however what we really care about is the true risk of the output hypothesis. to argue about the true risk we note that the output of adaboost is in fact a composition of a halfspace over the predictions of the t weak hypotheses constructed by the weak learner. in the next section we show that if the weak hypotheses come from a base hypothesis class of low vc-dimension then the estimation error of adaboost will be small namely the true risk of the output of adaboost would not be very far from its empirical risk. linear combinations of base hypotheses as mentioned previously a popular approach for constructing a weak learner is to apply the erm rule with respect to a base hypothesis class erm over decision stumps. we have also seen that boosting outputs a composition of a halfspace over the predictions of the weak hypotheses. therefore given a base hypothesis class b decision stumps the output of adaboost will be a member of the following class lb t x sign wthtx w rt t ht b that is each h lb t is parameterized by t base hypotheses from b and by a vector w rt the prediction of such an h on an instance x is obtained by first applying the t base hypotheses to construct the vector boosting ht rt and then applying the halfspace defined by w on in this section we analyze the estimation error of lb t by bounding the vc-dimension of lb t in terms of the vc-dimension of b and t we will show that up to logarithmic factors the vc-dimension of lb t is bounded by t times the vc-dimension of b. it follows that the estimation error of adaboost grows linearly with t on the other hand the empirical risk of adaboost decreases with t in fact as we demonstrate later t can be used to decrease the approximation error of lb t therefore the parameter t of adaboost enables us to control the bias-complexity tradeoff. the simple example in which x r and the base class is decision stumps to demonstrate how the expressive power of lb t increases with t consider signx b r b note that in this one dimensional case is in fact equivalent to halfspaces on r. now let h be the rather complex class to halfspaces on the line of piece-wise constant functions. let gr be a piece-wise constant function with at most r pieces that is there exist thresholds r such that grx i i i i denote by gr the class of all such piece-wise constant classifiers with at most r pieces. in the following we show that gt t namely the class of halfspaces over t decision stumps yields all the piece-wise constant classifiers with at most t pieces. indeed without loss of generality consider any g gt with t this implies that if x is in the interval t t then gx for example now the function hx sign wt signx t where and for t wt is in t and is equal to g exercise linear combinations of base hypotheses from this example we obtain that t can shatter any set of t instances in r hence the vc-dimension of t is at least t therefore t is a parameter that can control the bias-complexity tradeoff enlarging t yields a more expressive hypothesis class but on the other hand might increase the estimation error. in the next subsection we formally upper bound the vcdimension of lb t for any base class b. the vc-dimension of lb t the following lemma tells us that the vc-dimension of lb t is upper bounded by ovcdimb t o notation ignores constants and logarithmic factors. lemma let b be a base class and let lb t be as defined in equation assume that both t and vcdimb are at least then vcdimlb t t logt proof denote d vcdimb. let c xm be a set that is shattered by lb t each labeling of c by h lb t is obtained by first choosing ht b and then applying a halfspace hypothesis over the vector ht by sauer s lemma there are at most different dichotomies labelings induced by b over c. therefore we need to choose t hypotheses out of at most different hypotheses. there are at most ways to do it. next for each such choice we apply a linear predictor which yields at most dichotomies. therefore the overall number of dichotomies we can construct is upper bounded by where we used the assumption that both d and t are at least since we assume that c is shattered we must have that the preceding is at least which yields therefore m logm lemma in chapter a tells us that a necessary condition for the above to hold is that m log logd which concludes our proof. in exercise we show that for some base classes b it also holds that vcdimlb t t boosting a c b d figure the four types of functions g used by the base hypotheses for face recognition. the value of g for type a or b is the difference between the sum of the pixels within two rectangular regions. these regions have the same size and shape and are horizontally or vertically adjacent. for type c the value of g is the sum within two outside rectangles subtracted from the sum in a center rectangle. for type d we compute the difference between diagonal pairs of rectangles. adaboost for face recognition we now turn to a base hypothesis that has been proposed by viola and jones for the task of face recognition. in this task the instance space is images represented as matrices of gray level values of pixels. to be concrete let us take images of size pixels and therefore our instance space is the set of real valued matrices of size the goal is to learn a classifier h x that given an image as input should output whether the image is of a human face or not. each hypothesis in the base class is of the form hx f where f is a decision stump hypothesis and g r is a function that maps an image to a scalar. each function g is parameterized by an axis aligned rectangle r. since each image is of size there are at most axis aligned rectangles. a type t b c d. each type corresponds to a mask as depicted in figure to calculate g we stretch the mask t to fit the rectangle r and then calculate the sum of the pixels is sum of their gray level values that lie within the red rectangles and subtract it from the sum of pixels in the blue rectangles. since the number of such functions g is at most we can implement a weak learner for the base hypothesis class by first calculating all the possible outputs of g on each image and then apply the weak learner of decision stumps described in the previous subsection. it is possible to perform the first step very summary figure the first and second features selected by adaboost as implemented by viola and jones. the two features are shown in the top row and then overlaid on a typical training face in the bottom row. the first feature measures the difference in intensity between the region of the eyes and a region across the upper cheeks. the feature capitalizes on the observation that the eye region is often darker than the cheeks. the second feature compares the intensities in the eye regions to the intensity across the bridge of the nose. efficiently by a preprocessing step in which we calculate the integral image of each image in the training set. see exercise for details. in figure we depict the first two features selected by adaboost when running it with the base features proposed by viola and jones. summary boosting is a method for amplifying the accuracy of weak learners. in this chapter we described the adaboost algorithm. we have shown that after t iterations of adaboost it returns a hypothesis from the class lb t obtained by composing a linear classifier on t hypotheses from a base class b. we have demonstrated how the parameter t controls the tradeoff between approximation and estimation errors. in the next chapter we will study how to tune parameters such as t based on the data. bibliographic remarks as mentioned before boosting stemmed from the theoretical question of whether an efficient weak learner can be boosted into an efficient strong learner valiant and solved by schapire the adaboost algorithm has been proposed in freund schapire boosting can be viewed from many perspectives. in the purely theoretical context adaboost can be interpreted as a negative result if strong learning of a hypothesis class is computationally hard so is weak learning of this class. this negative result can be useful for showing hardness of agnostic pac learning of a class b based on hardness of pac learning of some other class h as long as boosting h is weakly learnable using b. for example klivans sherstov have shown that pac learning of the class of intersection of halfspaces is hard in the realizable case. this hardness result can be used to show that agnostic pac learning of a single halfspace is also computationally hard shamir sridharan the idea is to show that an agnostic pac learner for a single halfspace can yield a weak learner for the class of intersection of halfspaces and since such a weak learner can be boosted we will obtain a strong learner for the class of intersection of halfspaces. adaboost also shows an equivalence between the existence of a weak learner and separability of the data using a linear classifier over the predictions of base hypotheses. this result is closely related to von neumann s minimax theorem neumann a fundamental result in game theory. adaboost is also related to the concept of margin which we will study later on in chapter it can also be viewed as a forward greedy selection algorithm a topic that will be presented in chapter a recent book by schapire freund covers boosting from all points of view and gives easy access to the wealth of research that this field has produced. exercises boosting the confidence let a be an algorithm that guarantees the following there exist some constant and a function mh n such that for every if m mh then for every distribution d it holds that with probability of at least ldas minh h ldh suggest a procedure that relies on a and learns h in the usual agnostic pac learning model and has a sample complexity of mh k mh where k log hint divide the data into k chunks where each of the first k chunks is of size mh examples. train the first k chunks using a. argue that the probability that for all of these chunks we have ldas minh h ldh finally use the last chunk to choose from the k hypotheses is at most k that a generated from the k chunks relying on corollary prove that the function h given in equation equals the piece-wise con stant function defined according to the same thresholds as h. we have informally argued that the adaboost algorithm uses the weighting mechanism to force the weak learner to focus on the problematic examples in the next iteration. in this question we will find some rigorous justification for this argument. exercises show that the error of ht w.r.t. the distribution is exactly that is show that for every t i in this exercise we discuss the vc-dimension of classes of the form lb t we proved an upper bound of odt logdt where d vcdimb. here we wish to prove an almost matching lower bound. however that will not be the case for all classes b. note that for every class b and every number t vcdimb vcdimlb t find a class b for which vcdimb vcdimlb t for every t hint take x to be a finite set. vcdimbd logd. hints for the upper bound rely on exercise for the lower bound assume d let a be a k d matrix whose columns are all the d binary vectors in the rows of a form a set of k vectors in rd. show that this set is shattered by decision stumps over rd. let bd be the class of decision stumps over rd. prove that logd let t be any integer. prove that vcdimlbd t t logd. hint construct a set of t from the previous question and the rows of the matrices t show that the resulting set is shattered by lbd t k instances by taking the rows of the matrix a a. image of a denoted by ia is the matrix b such that bij efficiently calculating the viola and jones features using an integral image let a be a matrix representing an image. the integral j aij. show that ia can be calculated from a in time linear in the size of a. show how every viola and jones feature can be calculated from ia in a constant amount of time is the runtime does not depend on the size of the rectangle defining the feature. model selection and validation in the previous chapter we have described the adaboost algorithm and have shown how the parameter t of adaboost controls the bias-complexity tradeoff. but how do we set t in practice? more generally when approaching some practical problem we usually can think of several algorithms that may yield a good solution each of which might have several parameters. how can we choose the best algorithm for the particular problem at hand? and how do we set the algorithm s parameters? this task is often called model selection. to illustrate the model selection task consider the problem of learning a one dimensional regression function h r r. suppose that we obtain a training set as depicted in the figure. we can consider fitting a polynomial to the data as described in chapter however we might be uncertain regarding which degree d would give the best results for our data set a small degree may not fit the data well it will have a large approximation error whereas a high degree may lead to overfitting it will have a large estimation error. in the following we depict the result of fitting a polynomial of degrees and it is easy to see that the empirical risk decreases as we enlarge the degree. however looking at the graphs our intuition tells us that setting the degree to may be better than setting it to it follows that the empirical risk alone is not enough for model selection. degree degree degree understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning model selection using srm in this chapter we will present two approaches for model selection. the first approach is based on the structural risk minimization paradigm we have described and analyzed in chapter srm is particularly useful when a learning algorithm depends on a parameter that controls the bias-complexity tradeoff as the degree of the fitted polynomial in the preceding example or the parameter t in adaboost. the second approach relies on the concept of validation. the basic idea is to partition the training set into two sets. one is used for training each of the candidate models and the second is used for deciding which of them yields the best results. in model selection tasks we try to find the right balance between approximation and estimation errors. more generally if our learning algorithm fails to find a predictor with a small risk it is important to understand whether we suffer from overfitting or underfitting. in section we discuss how this can be achieved. model selection using srm the srm paradigm has been described and analyzed in section here we show how srm can be used for tuning the tradeoff between bias and complexity without deciding on a specific hypothesis class in advance. consider a countable sequence of hypothesis classes for example in the problem of polynomial regression mentioned we can take hd to be the set of polynomials of degree at most d. another example is taking hd to be the class lb d used by adaboost as described in the previous chapter. we assume that for every d the class hd enjoys the uniform convergence property definition in chapter with a sample complexity function of the form gd muchd where g n r is some monotonically increasing function. for example in the case of binary classification problems we can take gd to be the vc-dimension of the class hd multiplied by a universal constant one appearing in the fundamental theorem of learning see theorem for the classes lb d used by adaboost the function g will simply grow with d. recall that the srm rule follows a bound minimization approach where in our case the bound is as follows with probability of at least for every d n and h hd ldh lsh logd log m this bound which follows directly from theorem shows that for every d and every h hd the true risk is bounded by two terms the empirical risk lsh model selection and validation and a complexity term that depends on d. the srm rule will search for d and h hd that minimize the right-hand side of equation getting back to the example of polynomial regression described earlier even though the empirical risk of the degree polynomial is smaller than that of the degree polynomial we would still prefer the degree polynomial since its complexity reflected by the value of the function gd is much smaller. while the srm approach can be useful in some situations in many practical cases the upper bound given in equation is pessimistic. in the next section we present a more practical approach. validation we would often like to get a better estimation of the true risk of the output predictor of a learning algorithm. so far we have derived bounds on the estimation error of a hypothesis class which tell us that for all hypotheses in the class the true risk is not very far from the empirical risk. however these bounds might be loose and pessimistic as they hold for all hypotheses and all possible data distributions. a more accurate estimation of the true risk can be obtained by using some of the training data as a validation set over which one can evalutate the success of the algorithm s output predictor. this procedure is called validation. naturally a better estimation of the true risk is useful for model selection as we will describe in section hold out set the simplest way to estimate the true error of a predictor h is by sampling an additional set of examples independent of the training set and using the empirical error on this validation set as our estimator. formally let v ymv be a set of fresh mv examples that are sampled according to d of the m examples of the training set s. using hoeffding s inequality lemma we have the following theorem let h be some predictor and assume that the loss function is in then for every with probability of at least over the choice of a validation set v of size mv we have ldh mv the bound in theorem does not depend on the algorithm or the training set used to construct h and is tighter than the usual bounds that we have seen so far. the reason for the tightness of this bound is that it is in terms of an estimate on a fresh validation set that is independent of the way h was generated. to illustrate this point suppose that h was obtained by applying an erm predictor validation with respect to a hypothesis class of vc-dimension d over a training set of m examples. then from the fundamental theorem of learning we obtain the bound ldh lsh c d m where c is the constant appearing in theorem in contrast from theorem we obtain the bound ldh lv therefore taking mv to be order of m we obtain an estimate that is more accurate by a factor that depends on the vc-dimension. on the other hand the price we pay for using such an estimate is that it requires an additional sample on top of the sample used for training the learner. sampling a training set and then sampling an independent validation set is equivalent to randomly partitioning our random set of examples into two parts using one part for training and the other one for validation. for this reason the validation set is often referred to as a hold out set. validation for model selection validation can be naturally used for model selection as follows. we first train different algorithms the same algorithm with different parameters on the given training set. let h hr be the set of all output predictors of the different algorithms. for example in the case of training polynomial regressors we would have each hr be the output of polynomial regression of degree r. now to choose a single predictor from h we sample a fresh validation set and choose the predictor that minimizes the error over the validation set. in other words we apply ermh over the validation set. this process is very similar to learning a finite hypothesis class. the only difference is that h is not fixed ahead of time but rather depends on the training set. however since the validation set is independent of the training set we get that it is also independent of h and therefore the same technique we used to derive bounds for finite hypothesis classes holds here as well. in particular combining theorem with the union bound we obtain theorem let h hr be an arbitrary set of predictors and assume that the loss function is in assume that a validation set v of size mv is sampled independent of h. then with probability of at least over the choice of v we have h h lv mv model selection and validation this theorem tells us that the error on the validation set approximates the true error as long as h is not too large. however if we try too many methods in that is large relative to the size of the validation set then we re in danger of overfitting. to illustrate how validation is useful for model selection consider again the example of fitting a one dimensional polynomial as described in the beginning of this chapter. in the following we depict the same training set with erm polynomials of degree and but this time we also depict an additional validation set as red unfilled circles. the polynomial of degree has minimal training error yet the polynomial of degree has the minimal validation error and hence it will be chosen as the best model. the model-selection curve the model selection curve shows the training error and validation error as a function of the complexity of the model considered. for example for the polynomial fitting problem mentioned previously the curve will look like validation train validation r o r r e d as can be shown the training error is monotonically decreasing as we increase the polynomial degree is the complexity of the model in our case. on the other hand the validation error first decreases but then starts to increase which indicates that we are starting to suffer from overfitting. plotting such curves can help us understand whether we are searching the correct regime of our parameter space. often there may be more than a single parameter to tune and the possible number of values each parameter can take might be quite large. for example in chapter we describe the concept of regularization in which the parameter of the learning algorithm is a real number. in such cases we start with a rough grid of values for the parameters and plot the corresponding model-selection curve. on the basis of the curve we will zoom in to the correct regime and employ a finer grid to search over. it is important to verify that we are in the relevant regime. for example in the polynomial fitting problem described if we start searching degrees from the set of values and do not employ a finer grid based on the resulting curve we will end up with a rather poor model. k-fold cross validation the validation procedure described so far assumes that data is plentiful and that we have the ability to sample a fresh validation set. but in some applications data is scarce and we do not want to waste data on validation. the k-fold cross validation technique is designed to give an accurate estimate of the true error without wasting too much data. in k-fold cross validation the original training set is partitioned into k subsets of size mk simplicity assume that mk is an integer. for each fold the algorithm is trained on the union of the other folds and then the error of its output is estimated using the fold. finally the average of all these errors is the model selection and validation estimate of the true error. the special case k m where m is the number of examples is called leave-one-out k-fold cross validation is often used for model selection parameter tuning and once the best parameter is chosen the algorithm is retrained using this parameter on the entire training set. a pseudocode of k-fold cross validation for model selection is given in the following. the procedure receives as input a training set s a set of possible parameter values an integer k representing the number of folds and a learning algorithm a which receives as input a training set as well as a parameter it outputs the best parameter as well as the hypothesis trained by this parameter on the entire training set. k-fold cross validation for model selection input training set s ym set of parameter values learning algorithm a integer k partition s into sk foreach error k output lsihi for i k hi as si argmin h as the cross validation method often works very well in practice. however it might sometime fail as the artificial example given in exercise shows. rigorously understanding the exact behavior of cross validation is still an open problem. rogers and wagner wagner have shown that for k local rules k nearest neighbor see chapter the cross validation procedure gives a very good estimate of the true error. other papers show that cross validation works for stable algorithms will study stability and its relation to learnability in chapter train-validation-test split in most practical applications we split the available examples into three sets. the first set is used for training our algorithm and the second is used as a validation set for model selection. after we select the best model we test the performance of the output predictor on the third set which is often called the test set. the number obtained is used as an estimator of the true error of the learned predictor. what to do if learning fails what to do if learning fails consider the following scenario you were given a learning task and have approached it with a choice of a hypothesis class a learning algorithm and parameters. you used a validation set to tune the parameters and tested the learned predictor on a test set. the test results unfortunately turn out to be unsatisfactory. what went wrong then and what should you do next? there are many elements that can be fixed. the main approaches are listed in the following get a larger sample change the hypothesis class by enlarging it reducing it completely changing it changing the parameters you consider change the feature representation of the data change the optimization algorithm used to apply your learning rule in order to find the best remedy it is essential first to understand the cause of the bad performance. recall that in chapter we decomposed the true error of the learned predictor into approximation error and estimation error. the approximation error is defined to be for some argminh h ldh while the estimation error is defined to be ldhs where hs is the learned predictor is based on the training set s. the approximation error of the class does not depend on the sample size or on the algorithm being used. it only depends on the distribution d and on the hypothesis class h. therefore if the approximation error is large it will not help us to enlarge the training set size and it also does not make sense to reduce the hypothesis class. what can be beneficial in this case is to enlarge the hypothesis class or completely change it we have some alternative prior knowledge in the form of a different hypothesis class. we can also consider applying the same hypothesis class but on a different feature representation of the data chapter the estimation error of the class does depend on the sample size. therefore if we have a large estimation error we can make an effort to obtain more training examples. we can also consider reducing the hypothesis class. however it doesn t make sense to enlarge the hypothesis class in that case. error decomposition using validation we see that understanding whether our problem is due to approximation error or estimation error is very useful for finding the best remedy. in the previous section we saw how to estimate ldhs using the empirical risk on a validation set. however it is more difficult to estimate the approximation error of the class. model selection and validation instead we give a different error decomposition one that can be estimated from the train and validation sets. ldhs lv lshs lshs. the first term lv can be bounded quite tightly using theorem intuitively when the second term lshs is large we say that our algorithm suffers from overfitting while when the empirical risk term lshs is large we say that our algorithm suffers from underfitting. note that these two terms are not necessarily good estimates of the estimation and approximation errors. to illustrate this consider the case in which h is a class of vc-dimension d and d is a distribution such that the approximation error of h with respect to d is as long as the size of our training set is smaller than d we will have lshs for every erm hypothesis. therefore the training risk lshs and the approximation error can be significantly different. nevertheless as we show later the values of lshs and lshs still provide us useful information. consider first the case in which lshs is large. we can write lshs when hs is an ermh hypothesis we have that lshs in addition since does not depend on s the term can be bounded quite tightly in theorem the last term is the approximation error. it follows that if lshs is large then so is the approximation error and the remedy to the failure of our algorithm should be tailored accordingly discussed previously. remark it is possible that the approximation error of our class is small yet the value of lshs is large. for example maybe we had a bug in our erm implementation and the algorithm returns a hypothesis hs that is not an erm. it may also be the case that finding an erm hypothesis is computationally hard and our algorithm applies some heuristic trying to find an approximate erm. in some cases it is hard to know how good hs is relative to an erm hypothesis. but sometimes it is possible at least to know whether there are better hypotheses. for example in the next chapter we will study convex learning problems in which there are optimality conditions that can be checked to verify whether our optimization algorithm converged to an erm solution. in other cases the solution may depend on randomness in initializing the algorithm so we can try different randomly selected initial points to see whether better solutions pop out. next consider the case in which lshs is small. as we argued before this does not necessarily imply that the approximation error is small. indeed consider two scenarios in both of which we are trying to learn a hypothesis class of vc-dimension d using the erm learning rule. in the first scenario we have a training set of m d examples and the approximation error of the class is high. in the second scenario we have a training set of m examples and the what to do if learning fails error error validation error train error m validationerror train error m figure examples of learning curves. left this learning curve corresponds to the scenario in which the number of examples is always smaller than the vc dimension of the class. right this learning curve corresponds to the scenario in which the approximation error is zero and the number of examples is larger than the vc dimension of the class. approximation error of the class is zero. in both cases lshs how can we distinguish between the two cases? learning curves one possible way to distinguish between the two cases is by plotting learning curves. to produce a learning curve we train the algorithm on prefixes of the data of increasing sizes. for example we can first train the algorithm on the first of the examples then on of them and so on. for each prefix we calculate the training error the prefix the algorithm is being trained on and the validation error a predefined validation set. such learning curves can help us distinguish between the two aforementioned scenarios. in the first scenario we expect the validation error to be approximately for all prefixes as we didn t really learn anything. in the second scenario the validation error will start as a constant but then should start decreasing must start decreasing once the training set size is larger than the vc-dimension. an illustration of the two cases is given in figure in general as long as the approximation error is greater than zero we expect the training error to grow with the sample size as a larger amount of data points makes it harder to provide an explanation for all of them. on the other hand the validation error tends to decrease with the increase in sample size. if the vc-dimension is finite when the sample size goes to infinity the validation and train errors converge to the approximation error. therefore by extrapolating the training and validation curves we can try to guess the value of the approximation error or at least to get a rough estimate on an interval in which the approximation error resides. getting back to the problem of finding the best remedy for the failure of our algorithm if we observe that lshs is small while the validation error is large then in any case we know that the size of our training set is not sufficient for learning the class h. we can then plot a learning curve. if we see that the model selection and validation validation error is starting to decrease then the best solution is to increase the number of examples we can afford to enlarge the data. another reasonable solution is to decrease the complexity of the hypothesis class. on the other hand if we see that the validation error is kept around then we have no evidence that the approximation error of h is good. it may be the case that increasing the training set size will not help us at all. obtaining more data can still help us as at some point we can see whether the validation error starts to decrease or whether the training error starts to increase. but if more data is expensive it may be better first to try to reduce the complexity of the hypothesis class. to summarize the discussion the following steps should be applied if learning involves parameter tuning plot the model-selection curve to make sure that you tuned the parameters appropriately section if the training error is excessively large consider enlarging the hypothesis class completely change it or change the feature representation of the data. if the training error is small plot learning curves and try to deduce from them whether the problem is estimation error or approximation error. if the approximation error seems to be small enough try to obtain more data. if this is not possible consider reducing the complexity of the hypothesis class. if the approximation error seems to be large as well try to change the hy pothesis class or the feature representation of the data completely. summary model selection is the task of selecting an appropriate model for the learning task based on the data itself. we have shown how this can be done using the srm learning paradigm or using the more practical approach of validation. if our learning algorithm fails a decomposition of the algorithm s error should be performed using learning curves so as to find the best remedy. exercises failure of k-fold cross validation consider a case in that the label is chosen at random according to py py consider a learning algorithm that outputs the constant predictor hx if the parity of the labels on the training set is and otherwise the algorithm outputs the constant predictor hx prove that the difference between the leave-oneout estimate and the true error in such a case is always let be k hypothesis classes. suppose you are given m i.i.d. training consider two examples and you would like to learn the class h k alternative approaches learn h on the m examples using the erm rule exercises divide the m examples into a training set of size and a validation set of size m for some then apply the approach of model selection using validation. that is first train each class hi on the training examples using the erm rule with respect to hi and let hk be the resulting hypotheses. second apply the erm rule with respect to the finite class hk on the m validation examples. describe scenarios in which the first method is better than the second and vice versa. convex learning problems in this chapter we introduce convex learning problems. convex learning comprises an important family of learning problems mainly because most of what we can learn efficiently falls into it. we have already encountered linear regression with the squared loss and logistic regression which are convex problems and indeed they can be learned efficiently. we have also seen nonconvex problems such as halfspaces with the loss which is known to be computationally hard to learn in the unrealizable case. in general a convex learning problem is a problem whose hypothesis class is a convex set and whose loss function is a convex function for each example. we begin the chapter with some required definitions of convexity. besides convexity we will define lipschitzness and smoothness which are additional properties of the loss function that facilitate successful learning. we next turn to defining convex learning problems and demonstrate the necessity for further constraints such as boundedness and lipschitzness or smoothness. we define these more restricted families of learning problems and claim that convex-smoothlipschitz-bounded problems are learnable. these claims will be proven in the next two chapters in which we will present two learning paradigms that successfully learn all problems that are either convex-lipschitz-bounded or convex-smooth-bounded. finally in section we show how one can handle some nonconvex problems by minimizing surrogate loss functions that are convex of the original nonconvex loss function. surrogate convex loss functions give rise to efficient solutions but might increase the risk of the learned predictor. convexity lipschitzness and smoothness convexity definition set a set c in a vector space is convex if for any two vectors u v in c the line segment between u and v is contained in c. that is for any we have that u c. examples of convex and nonconvex sets in are given in the following. for the nonconvex sets we depict two points in the set such that the line between the two points is not contained in the set. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning convexity lipschitzness and smoothness non-convex convex given the combination u of the points u v is called a convex combination. definition function let c be a convex set. a function f c r is convex if for every u v c and f u f in words f is convex if for any u v the graph of f between u and v lies below the line segment joining f and f an illustration of a convex function f r r is depicted in the following. f f f u f u u v the epigraph of a function f is the set epigraphf f it is easy to verify that a function f is convex if and only if its epigraph is a convex set. an illustration of a nonconvex function f r r along with its epigraph is given in the following. convex learning problems f x an important property of convex functions is that every local minimum of the function is also a global minimum. formally let bu r r be a ball of radius r centered around u. we say that f is a local minimum of f at u if there exists some r such that for all v bu r we have f f it follows that for any v necessarily in b there is a small enough such that u u bu r and therefore f f u if f is convex we also have that f u f v f combining these two equations and rearranging terms we conclude that f f since this holds for every v it follows that f is also a global minimum of f another important property of convex functions is that for every w we can construct a tangent to f at w that lies below f everywhere. if f is differentiable this tangent is the linear function lu f f u where f is the gradient of f at w namely the vector of partial derivatives of f f that is for convex differentiable functions u f f f u f f wd in chapter we will generalize this inequality to nondifferentiable functions. an illustration of equation is given in the following. convexity lipschitzness and smoothness w f w f f w f w u if f is a scalar differentiable function there is an easy way to check if it is convex. lemma let f r r be a scalar twice differential function and let be its first and second derivatives respectively. then the following are equivalent f is convex is monotonically nondecreasing is nonnegative example the scalar function f is convex. to see this note that and the scalar function f expx is convex. to see this observe that exp this is a monotonically increasing function expx since the exponent function is a monotonically increasing function. the following claim shows that the composition of a convex scalar function with a linear function yields a convex vector-valued function. claim assume that f rd r can be written as f y for some x rd y r and g r r. then convexity of g implies the convexity of f proof let rd and we have f y g y g y y y y where the last inequality follows from the convexity of g. example convex learning problems given some x rd and y r let f rd r be defined as f then f is a composition of the function ga onto a linear function and hence f is a convex function. given some x rd and y let f rd r be defined as f exp then f is a composition of the function ga expa onto a linear function and hence f is a convex function. finally the following lemma shows that the maximum of convex functions is convex and that a weighted sum of convex functions with nonnegative weights is also convex. claim for i r let fi following functions from rd to r are also convex. gx maxi fix wifix where for all i wi gx rd r be a convex function. the proof the first claim follows by i g u max max max gu fi u fiu fiu max i i i fiv wifi u wi fiu wifiu gu i wifiv i for the second claim g u i i example the function gx is convex. to see this note that gx maxx x and that both the function x and x are convex. lipschitzness the definition of lipschitzness below is with respect to the euclidean norm over rd. however it is possible to define lipschitzness with respect to any norm. definition let c rd. a function f rd rk is over c if for every c we have that f convexity lipschitzness and smoothness intuitively a lipschitz function cannot change too fast. note that if f r r is differentiable then by the mean value theorem we have f f where u is some point between and it follows that if the derivative of f is everywhere bounded absolute value by then the function is example the function f is over r. this follows from the triangle inequality for every since this holds for both and we obtain that the function f expx is over r. to see this observe that expx expx exp x the function f is not over r for any to see this take and then f f however this function is over the set c indeed for any c we have the linear function f rd r defined by f b where v rd is indeed using cauchy-schwartz inequality f the following claim shows that composition of lipschitz functions preserves lipschitzness. claim let f where is and is then f is in particular if is the linear function b for some v rd b r then f is proof f convex learning problems smoothness the definition of a smooth function relies on the notion of gradient. recall that the gradient of a differentiable function f rd r at w denoted f is the vector of partial derivatives of f namely f definition a differentiable function f rd r is smooth if its gradient is namely for all v w we have f f f wd f it is possible to show that smoothness implies that for all v w we have f f f v recall that convexity of f implies that f f f v therefore when a function is both convex and smooth we have both upper and lower bounds on the difference between the function and its first order approximation. setting v w f in the right-hand side of equation and rear ranging terms we obtain f f f if we further assume that f for all v we conclude that smoothness implies the following f f a function that satisfies this property is also called a self-bounded function. example the function f is this follows directly from the fact that note that for this particular function equation and equation hold with equality. the function f expx is indeed since x we have that exp x exp exp expx hence tion holds as well. is since this function is nonnegative equa the following claim shows that a composition of a smooth scalar function over a linear function preserves smoothness. claim let f b where g r r is a function x rd and b r. then f is convex learning problems proof by the chain rule we have that f bx where is the derivative of g. using the smoothness of g and the cauchy-schwartz inequality we therefore obtain f b b w b w f f v w example for any x rd and y r let f then f is for any x rd and y let f exp then f is smooth. convex learning problems recall that in our general definition of learning in chapter we have a hypothesis class h a set of examples z and a loss function h z r. so far in the book we have mainly thought of z as being the product of an instance space and a target space z x y and h being a set of functions from x to y. however h can be an arbitrary set. indeed throughout this chapter we consider hypothesis classes h that are subsets of the euclidean space rd. that is every hypothesis is some real-valued vector. we shall therefore denote a hypothesis in h by w. now we can finally define convex learning problems definition learning problem a learning problem z is called convex if the hypothesis class h is a convex set and for all z z the loss function z is a convex function for any z z denotes the function f h r defined by f z. example regression with the squared loss recall that linear regression is a tool for modeling the relationship between some explanatory variables and some real valued outcome chapter the domain set x is a subset of rd for some d and the label set y is the set of real numbers. we would like to learn a linear function h rd r that best approximates the relationship between our variables. in chapter we defined the hypothesis class as the set of homogenous linear functions h w rd and used the squared loss function y however we can equivalently model the learning problem as a convex learning problem as follows. convex learning problems each linear function is parameterized by a vector w rd. hence we can define h to be the set of all such parameters namely h rd. the set of examples is z x y rd r and the loss function is y clearly the set h is a convex set. the loss function is also convex with respect to its first argument example if is a convex loss function and the class h is convex then the lemma ermh problem of minimizing the empirical loss over h is a convex optimization problem is a problem of minimizing a convex function over a convex set. proof recall that the ermh problem is defined by ermhs argmin w h lsw. since for a sample s zm for every w lsw zi m claim implies that lsw is a convex function. therefore the erm rule is a problem of minimizing a convex function subject to the constraint that the solution should be in a convex set. under mild conditions such problems can be solved efficiently using generic optimization algorithms. in particular in chapter we will present a very simple algorithm for minimizing convex functions. learnability of convex learning problems we have argued that for many cases implementing the erm rule for convex learning problems can be done efficiently. but is convexity a sufficient condition for the learnability of a problem? to make the quesion more specific in vc theory we saw that halfspaces in d-dimension are learnable inefficiently. we also argued in chapter using the discretization trick that if the problem is of d parameters it is learnable with a sample complexity being a function of d. that is for a constant d the problem should be learnable. so maybe all convex learning problems over rd are learnable? example later shows that the answer is negative even when d is low. not all convex learning problems over rd are learnable. there is no contradiction to vc theory since vc theory only deals with binary classification while here we consider a wide family of problems. there is also no contradiction to the discretization trick as there we assumed that the loss function is bounded and also assumed that a representation of each parameter using a finite number of bits suffices. as we will show later under some additional restricting conditions that hold in many practical scenarios convex problems are learnable. example of linear regression even if d let h r and the loss be the squared loss y re referring to the convex learning problems choose let m m and set homogenous case. let a be any deterministic assume by way of contradiction that a is a successful pac learner for this problem. that is there exists a function m such that for every distribution d and for every if a receives a training set of size m m it should output with probability of at least a hypothesis w as such that ld w minw ldw we will define two distributions and will show that a is likely to fail on at least one of them. the first distribution is supported on two examples and where the probability mass of the first example is while the probability mass of the second example is the second distribution is supported entirely on observe that for both distributions the probability that all examples of the training set will be of the second type is at least this is trivially true for whereas for the probability of this event is e m since we assume that a is a deterministic algorithm upon receiving a training set of m examples each of which is the algorithm will output some w. now if w we will set the distribution to be hence w min w however it follows that w min w therefore such algorithm a fails on on the other hand if w then we ll set the distribution to be then we have that w while minw so a fails on in summary we have shown that for every a there exists a distribution on which a fails which implies that the problem is not pac learnable. a possible solution to this problem is to add another constraint on the hypothesis class. in addition to the convexity requirement we require that h will be bounded namely we assume that for some predefined scalar b every hypothesis w h satisfies b. boundedness and convexity alone are still not sufficient for ensuring that the problem is learnable as the following example demonstrates. example as in example consider a regression problem with the squared loss. however this time let h r be a bounded namely given s the output of a is determined. this requirement is for the sake of simplicity. a slightly more involved argument will show that nondeterministic algorithms will also fail to learn the problem. convex learning problems hypothesis class. it is easy to verify that h is convex. the argument will be the same as in example except that now the two distributions will be supported on and if the algorithm a returns w upon receiving m examples of the second type then we will set the distribution to be and have that w min w similarly if w we will set the distribution to be and have that w w min w this example shows that we need additional assumptions on the learning problem and this time the solution is in lipschitzness or smoothness of the loss function. this motivates a definition of two families of learning problems convex-lipschitz-bounded and convex-smooth-bounded which are defined later. convex-lipschitzsmooth-bounded learning problems definition learning problem a learning problem z is called convex-lipschitz-bounded with parameters b if the following holds the hypothesis class h is a convex set and for all w h we have b. for all z z the loss function z is a convex and function. example let x rd and y r. let h rd b and let the loss function be y y. this corresponds to a regression problem with the absolute-value loss where we assume that the instances are in a ball of radius and we restrict the hypotheses to be homogenous linear functions defined by a vector w whose norm is bounded by b. then the resulting problem is convex-lipschitz-bounded with parameters b. definition learning problem a learning problem z is called convex-smooth-bounded with parameters b if the following holds the hypothesis class h is a convex set and for all w h we have b. for all z z the loss function z is a convex nonnegative and function. note that we also required that the loss function is nonnegative. this is needed to ensure that the loss function is self-bounded as described in the previous section. surrogate loss functions example let x rd and y r. let h rd b and let the loss function be y this corresponds to a regression problem with the squared loss where we assume that the instances are in a ball of radius and we restrict the hypotheses to be homogenous linear functions defined by a vector w whose norm is bounded by b. then the resulting problem is convex-smooth-bounded with parameters b. we claim that these two families of learning problems are learnable. that is the properties of convexity boundedness and lipschitzness or smoothness of the loss function are sufficient for learnability. we will prove this claim in the next chapters by introducing algorithms that learn these problems successfully. surrogate loss functions as mentioned and as we will see in the next chapters convex problems can be learned effficiently. however in many cases the natural loss function is not convex and in particular implementing the erm rule is hard. as an example consider the problem of learning the hypothesis class of half spaces with respect to the loss. that is y this loss function is not convex with respect to w and indeed when trying to minimize the empirical risk with respect to this loss function we might encounter local minima exercise furthermore as discussed in chapter solving the erm problem with respect to the loss in the unrealizable case is known to be np-hard. to circumvent the hardness result one popular approach is to upper bound the nonconvex loss function by a convex surrogate loss function. as its name indicates the requirements from a convex surrogate loss are as follows it should be convex. it should upper bound the original loss. for example in the context of learning halfspaces we can define the so-called hinge loss as a convex surrogate for the loss as follows y def clearly for all w and all y y y. in addition the convexity of the hinge loss follows directly from claim hence the hinge loss satisfies the requirements of a convex surrogate loss function for the zero-one loss. an illustration of the functions and is given in the following. convex learning problems once we have defined the surrogate convex loss we can learn the problem with respect to it. the generalization requirement from a hinge loss learner will have the form lhinged min w h lhinged where lhinged can lower bound the left-hand side by which yields exy y. using the surrogate property we w h lhinged we can further rewrite the upper bound as follows min min w h w h lhinged min w h min that is the error of the learned predictor is upper bounded by three terms approximation error this is the term minw h which measures how well the hypothesis class performs on the distribution. we already elaborated on this error term in chapter estimation error this is the error that results from the fact that we only receive a training set and do not observe the distribution d. we already elaborated on this error term in chapter minw h that measures the difference between the approximation error with respect to the surrogate loss and the approximation error with respect to the original loss. the optimization error is a result of our inability to minimize the training loss with respect to the original loss. the size of this error depends on the specific distribution of the data and on the specific surrogate loss we are using. optimization error this is the term minw h lhinged summary we introduced two families of learning problems convex-lipschitz-bounded and convex-smooth-bounded. in the next two chapters we will describe two generic bibliographic remarks learning algorithms for these families. we also introduced the notion of convex surrogate loss function which enables us also to utilize the convex machinery for nonconvex problems. bibliographic remarks there are several excellent books on convex analysis and optimization vandenberghe borwein lewis bertsekas hiriart-urruty lemar echal regarding learning problems the family of convex-lipschitzbounded problems was first studied by zinkevich in the context of online learning and by shalev-shwartz shamir sridharan srebro in the context of pac learning. not a global minimum of ls. exercises construct an example showing that the loss function may suffer from local minima namely construct a training sample s for x for which there exist a vector w and some such that for any such that we have lsw the loss here is the loss. this means that w is a local minimum of ls. there exists some w such that lsw lsw. this means that w is consider the learning problem of logistic regression let h x rd b for some scalar b let y and let the loss function be defined as y exp show that the resulting learning problem is both convex-lipschitz-bounded and convexsmooth-bounded. specify the parameters of lipschitzness and smoothness. consider the problem of learning halfspaces with the hinge loss. we limit our domain to the euclidean ball with radius r. that is x r. the label set is y and the loss function is defined by y we already know that the loss function is convex. show that it is r-lipschitz. convex-lipschitz-boundedness is not sufficient for computational efficiency in the next chapter we show that from the statistical perspective all convex-lipschitz-bounded problems are learnable the agnostic pac model. however our main motivation to learn such problems resulted from the computational perspective convex optimization is often efficiently solvable. yet the goal of this exercise is to show that convexity alone is not sufficient for efficiency. we show that even for the case d there is a convex-lipschitz-bounded problem which cannot be learned by any computable learner. let the hypothesis class be h and let the example domain z be convex learning problems the set of all turing machines. define the loss function as follows. for every turing machine t z let t if t halts on the input and t if t doesn t halt on the input similarly let t if t halts on the input and t if t doesn t halt on the input finally for h let t t t show that the resulting learning problem is convex-lipschitz-bounded. show that no computable algorithm can learn the problem. regularization and stability in the previous chapter we introduced the families of convex-lipschitz-bounded and convex-smooth-bounded learning problems. in this section we show that all learning problems in these two families are learnable. for some learning problems of this type it is possible to show that uniform convergence holds hence they are learnable using the erm rule. however this is not true for all learning problems of this type. yet we will introduce another learning rule and will show that it learns all convex-lipschitz-bounded and convex-smooth-bounded learning problems. the new learning paradigm we introduce in this chapter is called regularized loss minimization or rlm for short. in rlm we minimize the sum of the empirical risk and a regularization function. intuitively the regularization function measures the complexity of hypotheses. indeed one interpretation of the regularization function is the structural risk minimization paradigm we discussed in chapter another view of regularization is as a stabilizer of the learning algorithm. an algorithm is considered stable if a slight change of its input does not change its output much. we will formally define the notion of stability we mean by slight change of input and by does not change much the output and prove its close relation to learnability. finally we will show that using the squared norm as a regularization function stabilizes all convex-lipschitz or convex-smooth learning problems. hence rlm can be used as a general learning rule for these families of learning problems. regularized loss minimization regularized loss minimization is a learning rule in which we jointly minimize the empirical risk and a regularization function. formally a regularization function is a mapping r rd r and the regularized loss minimization rule outputs a hypothesis in argmin w rw regularized loss minimization shares similarities with minimum description length algorithms and structural risk minimization chapter intuitively the complexity of hypotheses is measured by the value of the regularization func understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning regularization and stability tion and the algorithm balances between low empirical risk and simpler or less complex hypotheses. there are many possible regularization functions one can use reflecting some prior belief about the problem to the description language in minimum description length. throughout this section we will focus on one of the most simple regularization functions rw where is a scalar and the norm is the norm i this yields the learning rule as argmin w this type of regularization function is often called tikhonov regularization. as mentioned before one interpretation of equation is using structural risk minimization where the norm of w is a measure of its complexity. recall that in the previous chapter we introduced the notion of bounded hypothesis classes. therefore we can define a sequence of hypothesis classes where hi i. if the sample complexity of each hi depends on i then the rlm rule is similar to the srm rule for this sequence of nested classes. a different interpretation of regularization is as a stabilizer. in the next section we define the notion of stability and prove that stable learning rules do not overfit. but first let us demonstrate the rlm rule for linear regression with the squared loss. ridge regression applying the rlm rule with tikhonov regularization to linear regression with the squared loss we obtain the following learning rule argmin w rd m performing linear regression using equation is called ridge regression. to solve equation we compare the gradient of the objective to zero and obtain the set of linear equations mi aw b a xi i where i is the identity matrix and a b are as defined in equation namely and b yixi since a is a positive semidefinite matrix the matrix mi a has all its eigenvalues bounded below by m. hence this matrix is invertible and the solution to ridge regression becomes w mi a b. stable rules do not overfit in the next section we formally show how regularization stabilizes the algorithm and prevents overfitting. in particular the analysis presented in the next sections corollary will yield theorem let d be a distribution over x where x rd let h rd b. for any let m then applying the ridge regression algorithm with parameter satisfies e s dm min w h ldw remark the preceding theorem tells us how many examples are needed to guarantee that the expected value of the risk of the learned predictor will be bounded by the approximation error of the class plus in the usual definition of agnostic pac learning we require that the risk of the learned predictor will be bounded with probability of at least in exercise we show how an algorithm with a bounded expected risk can be used to construct an agnostic pac learner. stable rules do not overfit intuitively a learning algorithm is stable if a small change of the input to the algorithm does not change the output of the algorithm much. of course there are many ways to define what we mean by a small change of the input and what we mean by does not change the output much in this section we define a specific notion of stability and prove that under this definition stable rules do not overfit. let a be a learning algorithm let s zm be a training set of m examples and let as denote the output of a. the algorithm a suffers from overfitting if the difference between the true risk of its output ldas and the empirical risk of its output lsas is large. as mentioned in remark throughout this chapter we focus on the expectation respect to the choice of s of this quantity namely esldas lsas. we next define the notion of stability. given the training set s and an additional example let si be the training set obtained by replacing the i th example of s with namely si zi zm. in our definition of stability a small change of the input means that we feed a with si instead of with s. that is we only replace one training example. we measure the effect of this small change of the input on the output of a by comparing the loss of the hypothesis as on zi to the loss of the hypothesis asi on zi. intuitively a good learning algorithm will have zi zi since in the first term the learning algorithm does not observe the example zi while in the second term zi is indeed observed. if the preceding difference is very large we suspect that the learning algorithm might overfit. this is because the regularization and stability learning algorithm drastically changes its prediction on zi if it observes it in the training set. this is formalized in the following theorem. theorem let d be a distribution. let s zm be an i.i.d. sequence of examples and let be another i.i.d. example. let u be the uniform distribution over then for any learning algorithm e s dm lsas e u zi zi. proof since s and are both drawn i.i.d. from d we have that for every i e e s e zi. on the other hand we can write e e s si zi. combining the two equations we conclude our proof. when the right-hand side of equation is small we say that a is a stable algorithm changing a single example in the training set does not lead to a significant change. formally definition let n r be a monotonically decreasing function. we say that a learning algorithm a is on-averagereplace-one-stable with rate if for every distribution d e u zi zi theorem tells us that a learning algorithm does not overfit if and only if it is on-average-replace-one-stable. of course a learning algorithm that does not overfit is not necessarily a good learning algorithm take for example an algorithm a that always outputs the same hypothesis. a useful algorithm should find a hypothesis that on one hand fits the training set has a low empirical risk and on the other hand does not overfit. or in light of theorem the algorithm should both fit the training set and at the same time be stable. as we shall see the parameter of the rlm rule balances between fitting the training set and being stable. tikhonov regularization as a stabilizer in the previous section we saw that stable rules do not overfit. in this section we show that applying the rlm rule with tikhonov regularization leads to a stable algorithm. we will assume that the loss function is convex and that it is either lipschitz or smooth. the main property of the tikhonov regularization that we rely on is that it makes the objective of rlm strongly convex as defined in the following. tikhonov regularization as a stabilizer definition convex functions a function f is convex if for all w u and we have f w f clearly every convex function is convex. an illustration of strong convexity is given in the following figure. f f w w u the following lemma implies that the objective of rlm is con vex. in addition it underscores an important property of strong convexity. lemma the function f is convex. if f is convex and g is convex then f g is convex. if f is convex and u is a minimizer of f then for any w f f proof the first two points follow directly from the definition. to prove the last point we divide the definition of strong convexity by and rearrange terms to get that f u f f f taking the limit we obtain that the right-hand side converges to f f on the other hand the left-hand side becomes the derivative of the function g f u at since u is a minimizer of f it follows that is a minimizer of g and therefore the left-hand side of the preceding goes to zero in the limit which concludes our proof. we now turn to prove that rlm is stable. let s zm be a training set let be an additional example and let si zi zm. let a be the rlm rule namely as argmin w regularization and stability denote fsw lsw and based on lemma we know that fs is convex. relying on part of the lemma it follows that for any v fsv fsas on the other hand for any v and u and for all i we have fsv fsu lsv lsiv zi zi m in particular choosing v asi u as and using the fact that v minimizes lsi we obtain that fsasi fsas zi zi m m m combining this with equation we obtain that zi zi m m the two subsections that follow continue the stability analysis for either lipschitz or smooth loss functions. for both families of loss functions we show that rlm is stable and therefore it does not overfit. lipschitz loss if the loss function zi is then by the definition of lipschitzness zi zi similarly plugging these inequalities into equation we obtain m which yields m plugging the preceding back into equation we conclude that zi zi m since this holds for any s i we immediately obtain tikhonov regularization as a stabilizer corollary assume that the loss function is convex and then the rlm rule with the regularizer is on-average-replace-one-stable with rate m it follows theorem that e s dm lsas m smooth and nonnegative loss if the loss is and nonnegative then it is also self-bounded section we further assume that smoothness assumption we have that f f m or in other words that by the zi zi zi asi using the cauchy-schwartz inequality and equation we further obtain that zi zi by a symmetric argument it holds that plugging these inequalities into equation and rearranging terms we obtain that zi zi m m combining the preceding with the assumption yields regularization and stability combining the preceding with equation and again using the assumption yield zi zi zi zi zi m m m where in the last step we used the inequality taking expectation with respect to s i and noting that zi elsas we conclude that corollary assume that the loss function is and nonnegative. then the rlm rule with the regularizer where m satisfies zi zi elsas. note that if for all z we have z c for some scalar c then for m every s lsas lsas c. hence corollary also implies that zi zi c m controlling the fitting-stability tradeoff we can rewrite the expected risk of a learning algorithm as e e s s e lsas. s the first term reflects how well as fits the training set while the second term reflects the difference between the true and empirical risks of as. as we have shown in theorem the second term is equivalent to the stability of a. since our goal is to minimize the risk of the algorithm we need that the sum of both terms will be small. in the previous section we have bounded the stability term. we have shown that the stability term decreases as the regularization parameter increases. on the other hand the empirical risk increases with we therefore face a controlling the fitting-stability tradeoff tradeoff between fitting and overfitting. this tradeoff is quite similar to the biascomplexity tradeoff we discussed previously in the book. we now derive bounds on the empirical risk term for the rlm rule. recall fix some that the rlm rule is defined as as argminw arbitrary vector w we have lsas lsas lsw taking expectation of both sides with respect to s and noting that eslsw ldw we obtain that ldw e s plugging this into equation we obtain ldw e e s s lsas. combining the preceding with corollary we conclude corollary assume that the loss function is convex and then the rlm rule with the regularization function satisfies w e ldw s m this bound is often called an oracle inequality if we think of w as a hypothesis with low risk the bound tells us how many examples are needed so that as will be almost as good as w had we known the norm of w in practice however we usually do not know the norm of w we therefore usually tune on the basis of a validation set as described in chapter we can also easily derive a pac-like from corollary for convex lipschitz-bounded learning problems corollary let z be a convex-lipschitz-bounded learning problem m then the with parameters b. for any training set size m let rlm rule with the regularization function satisfies min e s w h ldw b m in particular for every if m esldas minw h ldw then for every distribution d the preceding corollary holds for lipschitz loss functions. if instead the loss function is smooth and nonnegative then we can combine equation with corollary to get again the bound below is on the expected risk but using exercise it can be used to derive an agnostic pac learning guarantee. regularization and stability corollary assume that the loss function is convex and nonnegative. then the rlm rule with the regularization function for m m satisfies the following for all w e s e s m for example if we choose m we obtain from the preceding that the expected true risk of as is at most twice the expected empirical risk of as. furthermore for this value of the expected empirical risk of as is at most ldw m we can also derive a learnability guarantee for convex-smooth-bounded learn ing problems based on corollary corollary let z be a convex-smooth-bounded learning problem with parameters b. assume in addition that z for all z z. for any and set then for every distribution d let m min e s w h ldw summary we introduced stability and showed that if an algorithm is stable then it does not overfit. furthermore for convex-lipschitz-bounded or convex-smooth-bounded problems the rlm rule with tikhonov regularization leads to a stable learning algorithm. we discussed how the regularization parameter controls the tradeoff between fitting and overfitting. finally we have shown that all learning problems that are from the families of convex-lipschitz-bounded and convex-smoothbounded problems are learnable using the rlm rule. the rlm paradigm is the basis for many popular learning algorithms including ridge regression we discussed in this chapter and support vector machines will be discussed in chapter in the next chapter we will present stochastic gradient descent which gives us a very practical alternative way to learn convex-lipschitz-bounded and convexsmooth-bounded problems and can also be used for efficiently implementing the rlm rule. bibliographic remarks stability is widely used in many mathematical contexts. for example the necessity of stability for so-called inverse problems to be well posed was first recognized by hadamard the idea of regularization and its relation to stability became widely known through the works of tikhonov and phillips exercises in the context of modern learning theory the use of stability can be traced back at least to the work of rogers wagner which noted that the sensitivity of a learning algorithm with regard to small changes in the sample controls the variance of the leave-one-out estimate. the authors used this observation to obtain generalization bounds for the k-nearest neighbor algorithm chapter these results were later extended to other local learning algorithms devroye gy orfi lugosi and references therein. in addition practical methods have been developed to introduce stability into learning algorithms in particular the bagging technique introduced by over the last decade stability was studied as a generic condition for learnability. see ron bousquet elisseeff kutin niyogi rakhlin mukherjee poggio mukherjee niyogi poggio rifkin our presentation follows the work of shalev-shwartz shamir srebro sridharan who showed that stability is sufficient and necessary for learning. they have also shown that all convex-lipschitz-bounded learning problems are learnable using rlm even though for some convex-lipschitz-bounded learning problems uniform convergence does not hold in a strong sense. exercises from bounded expected risk to agnostic pac learning let a be an algorithm that guarantees the following if m mh then for every distribution d it holds that e s dm min h h ldh show that for every if m mh then with probability of at least it holds that ldas minh h ldh hint observe that the random variable ldas minh h ldh is nonnegative and rely on markov s inequality. for every let mh suggest a procedure that agnostic pac learns the problem with sample complexity of mh assuming that the loss function is bounded by hint let k divide the data into k chunks where each of the first k chunks is of size examples. train the first k chunks using a. on the basis of the previous question argue that the probability that for all of these chunks we have ldas minh h ldh is at most k finally use the last chunk as a validation set. learnability without uniform convergence let b be the unit ball of regularization and stability rd let h b let z b and let z h r be defined as follows ixi this problem corresponds to an unsupervised learning task meaning that we do not try to predict the label of x. instead what we try to do is to find the center of mass of the distribution over b. however there is a twist modeled by the vectors each example is a pair where x is the instance x and indicates which features of x are active and which are turned off. a hypothesis is a vector w representing the center of mass of the distribution and the loss function is the squared euclidean distance between x and w but only with respect to the active elements of x. show that this problem is learnable using the rlm rule with a sample complexity that does not depend on d. consider a distribution d over z as follows x is fixed to be some and each element of is sampled to be either or with equal probability. show that the rate of uniform convergence of this problem grows with d. hint let m be a training set size. show that if d then there is a high probability of sampling a set of examples such that there exists some j for which j for all the examples in the training set. show that such a sample cannot be conclude that the sample complexity of uniform convergence must grow with logd. conclude that if we take d to infinity we obtain a problem that is learnable but for which the uniform convergence property does not hold. compare to the fundamental theorem of statistical learning. stability and asymptotic erm are sufficient for learnability we say that a learning rule a is an aerm empirical risk minimizer with rate if for every distribution d it holds that e s dm lsas min h h lsh we say that a learning rule a learns a class h with rate if for every distribution d it holds that e s dm prove the following ldas min h h ldh theorem if a learning algorithm a is on-average-replace-one-stable with rate and is an aerm with rate then it learns h with rate exercises strong convexity with respect to general norms throughout the section we used the norm. in this exercise we generalize some of the results to general norms. let be some arbitrary norm and let f be a strongly convex function with respect to this norm definition show that items of lemma hold for every norm. give an example of a norm for which item of lemma does not let rw be a function that is convex with respect to some hold. norm let a be an rlm rule with respect to r namely as argmin rw w assume that for every z the loss function z is with respect to the same norm namely z w v z z prove that a is on-average-replace-one-stable with rate m let q and consider the it can be shown for example shalev-shwartz that the function rw q is convex with respect to show that if q logd rw is convex with respect to the norm over rd. logd then logd stochastic gradient descent recall that the goal of learning is to minimize the risk function ldh ez z. we cannot directly minimize the risk function since it depends on the unknown distribution d. so far in the book we have discussed learning methods that depend on the empirical risk. that is we first sample a training set s and define the empirical risk function lsh. then the learner picks a hypothesis based on the value of lsh. for example the erm rule tells us to pick the hypothesis that minimizes lsh over the hypothesis class h. or in the previous chapter we discussed regularized risk minimization in which we pick a hypothesis that jointly minimizes lsh and a regularization function over h. in this chapter we describe and analyze a rather different learning approach which is called stochastic gradient descent as in chapter we will focus on the important family of convex learning problems and following the notation in that chapter we will refer to hypotheses as vectors w that come from a convex hypothesis class h. in sgd we try to minimize the risk function ldw directly using a gradient descent procedure. gradient descent is an iterative optimization procedure in which at each step we improve the solution by taking a step along the negative of the gradient of the function to be minimized at the current point. of course in our case we are minimizing the risk function and since we do not know d we also do not know the gradient of ldw. sgd circumvents this problem by allowing the optimization procedure to take a step along a random direction as long as the expected value of the direction is the negative of the gradient. and as we shall see finding a random direction whose expected value corresponds to the gradient is rather simple even though we do not know the underlying distribution d. the advantage of sgd in the context of convex learning problems over the regularized risk minimization learning rule is that sgd is an efficient algorithm that can be implemented in a few lines of code yet still enjoys the same sample complexity as the regularized risk minimization rule. the simplicity of sgd also allows us to use it in situations when it is not possible to apply methods that are based on the empirical risk but this is beyond the scope of this book. we start this chapter with the basic gradient descent algorithm and analyze its convergence rate for convex-lipschitz functions. next we introduce the notion of subgradient and show that gradient descent can be applied for nondifferentiable functions as well. the core of this chapter is section in which we describe understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning gradient descent the stochastic gradient descent algorithm along with several useful variants. we show that sgd enjoys an expected convergence rate similar to the rate of gradient descent. finally we turn to the applicability of sgd to learning problems. gradient descent before we describe the stochastic gradient descent method we would like to describe the standard gradient descent approach for minimizing a differentiable convex function f the gradient of a differentiable function f rd r at w denoted f is the vector of partial derivatives of f namely f gradient descent is an iterative algorithm. we start with an initial value of w then at each iteration we take a step in the direction of the negative of the gradient at the current point. that is the update step is f f wd wt f where is a parameter to be discussed later. intuitively since the gradient points in the direction of the greatest rate of increase of f around wt the algorithm makes a small step in the opposite direction thus decreasing the value of the function. eventually after t iterations the algorithm outputs the averaged vector w wt. the output could also be the last vector t wt or the best performing vector argmint f but taking the average turns out to be rather useful especially when we generalize gradient descent to nondifferentiable functions and to the stochastic case. another way to motivate gradient descent is by relying on taylor approximation. the gradient of f at w yields the first order taylor approximation of f around w by f f w f when f is convex this approximation lower bounds f that is f f w f therefore for w close to wt we have that f f wt f hence we can minimize the approximation of f however the approximation might become loose for w which is far away from wt. therefore we would like to minimize jointly the distance between w and wt and the approximation of f around wt. if the parameter controls the tradeoff between the two terms we obtain the update rule argmin w f wt f solving the preceding by taking the derivative with respect to w and comparing it to zero yields the same update rule as in equation stochastic gradient descent figure an illustration of the gradient descent algorithm. the function to be minimized is analysis of gd for convex-lipschitz functions to analyze the convergence rate of the gd algorithm we limit ourselves to the case of convex-lipschitz functions we have seen many problems lend themselves easily to this setting. let be any vector and let b be an upper bound on it is convenient to think of as the minimizer of f but the analysis that follows holds for every we would like to obtain an upper bound on the suboptimality of our solution wt. from the with respect to namely f w f where w definition of w and using jensen s inequality we have that t f t t t f w f f wt f f f f for every t because of the convexity of f we have that f f f combining the preceding we obtain f w f t f to bound the right-hand side we rely on the following lemma gradient descent lemma let vt be an arbitrary sequence of vectors. any algorithm with an initialization and an update rule of the form satisfies wt vt t then for every with b we have in particular for every b if for all t we have that and if we set t b t proof using algebraic manipulations the square we obtain where the last equality follows from the definition of the update rule. summing the equality over t we have the first sum on the right-hand side is a telescopic sum that collapses to plugging this in equation we have where the last equality is due to the definition this proves the first part of the lemma the second part follows by upper bounding by b by dividing by t and plugging in the value of stochastic gradient descent lemma applies to the gd algorithm with vt f as we will show later in lemma if f is then f we therefore satisfy the lemma s conditions and achieve the following corollary corollary let f be a convex function and let b f if we run the gd algorithm on f for t steps with vector w satisfies t then the output f w f b t furthermore for every to achieve f w f it suffices to run the gd algorithm for a number of iterations that satisfies t subgradients the gd algorithm requires that the function f be differentiable. we now generalize the discussion beyond differentiable functions. we will show that the gd algorithm can be applied to nondifferentiable functions by using a so-called subgradient of f at wt instead of the gradient. to motivate the definition of subgradients recall that for a convex function f the gradient at w defines the slope of a tangent that lies below f that is u f f w f an illustration is given on the left-hand side of figure the existence of a tangent that lies below f is an important property of convex functions which is in fact an alternative characterization of convexity. lemma let s be an open convex set. a function f s r is convex iff for every w s there exists v such that u s f f w the proof of this lemma can be found in many convex analysis textbooks lewis the preceding inequality leads us to the definition of subgradients. definition a vector v that satisfies equation is called a subgradient of f at w. the set of subgradients of f at w is called the differential set and denoted f an illustration of subgradients is given on the right-hand side of figure for scalar functions a subgradient of a convex function f at w is a slope of a line that touches f at w and is not above f elsewhere. subgradients f w f w f w f w u figure left the right-hand side of equation is the tangent of f at w. for a convex function the tangent lower bounds f right illustration of several subgradients of a nondifferentiable convex function. calculating subgradients how do we construct subgradients of a given convex function? if a function is differentiable at a point w then the differential set is trivial as the following claim shows. claim the gradient of f at w f if f is differentiable at w then f contains a single element example differential set of the absolute function consider the absolute value function f using claim we can easily construct the differential set for the differentiable parts of f and the only point that requires special attention is at that point it is easy to verify that the subdifferential is the set of all numbers between and hence f if x if x if x for many practical uses we do not need to calculate the whole set of subgradients at a given point as one member of this set would suffice. the following claim shows how to construct a sub-gradient for pointwise maximum functions. claim let gw maxi giw for r convex differentiable functions gr. given some w let j argmaxi giw. then gjw gw. proof since gj is convex we have that for all u gju gjw w since gw gjw and gu gju we obtain that gu gw w which concludes our proof. stochastic gradient descent example subgradient of the hinge loss recall the hinge loss function from section f for some vector x and scalar y. to calculate a subgradient of the hinge loss at some w we rely on the preceding claim and obtain that the vector v defined in the following is a subgradient of the hinge loss at w v if yx if subgradients of lipschitz functions recall that a function f a r is if for all u v a f the following lemma gives an equivalent definition using norms of subgradients. lemma let a be a convex open set and let f a r be a convex function. then f is over a iff for all w a and v f we have that proof assume that for all v f we have that since v f we have f f w bounding the right-hand side using cauchy-schwartz inequality we obtain f f w an analogous argument can show that f f hence f is now assume that f is choose some w a v f since a is open there exists such that u w belongs to a. therefore w and from the definition of the subgradient f f u on the other hand from the lipschitzness of f we have f f combining the two inequalities we conclude that subgradient descent the gradient descent algorithm can be generalized to nondifferentiable functions by using a subgradient of f at wt instead of the gradient. the analysis of the convergence rate remains unchanged simply note that equation is true for subgradients as well. stochastic gradient descent figure an illustration of the gradient descent algorithm and the stochastic gradient descent algorithm the function to be minimized is for the stochastic case the black line depicts the averaged value of w. stochastic gradient descent in stochastic gradient descent we do not require the update direction to be based exactly on the gradient. instead we allow the direction to be a random vector and only require that its expected value at each iteration will equal the gradient direction. or more generally we require that the expected value of the random vector will be a subgradient of the function at the current vector. stochastic gradient descent for minimizing f parameters scalar integer t initialize for t t choose vt at random from a distribution such that evt wt f update wt vt output w t wt an illustration of stochastic gradient descent versus gradient descent is given in figure as we will see in section in the context of learning problems it is easy to find a random vector whose expectation is a subgradient of the risk function. analysis of sgd for convex-lipschitz-bounded functions recall the bound we achieved for the gd algorithm in corollary for the stochastic case in which only the expectation of vt is in f we cannot directly apply equation however since the expected value of vt is a stochastic gradient descent subgradient of f at wt we can still derive a similar bound on the expected output of stochastic gradient descent. this is formalized in the following theorem. theorem let b let f be a convex function and let b f assume that sgd is run for t iterations with all t with probability then t assume also that for e w f b t therefore for any to achieve ef w f it suffices to run the sgd algorithm for a number of iterations that satisfies t proof let us introduce the notation to denote the sequence vt. taking expectation of equation we obtain e w f e t f since lemma holds for any sequence it applies to sgd as well. by taking expectation of the bound in the lemma we have e t b t it is left to show that e t t e f which we will hereby prove. using the linearity of the expectation we have e t t e next we recall the law of total expectation for every two random variables and a function g e e e setting and we get that e e e e once we know the value of wt is not random any more and therefore e e e e vt variants since wt only depends on and sgd requires that evtvt wt f we obtain that evtvt f thus e f e t e t t vt overall we have shown that e e e f f summing over t dividing by t and using the linearity of expectation we get that equation holds which concludes our proof. variants in this section we describe several variants of stochastic gradient descent. adding a projection step in the previous analyses of the gd and sgd algorithms we required that the norm of will be at most b which is equivalent to requiring that is in the set h b. in terms of learning this means restricting ourselves to a b-bounded hypothesis class. yet any step we take in the opposite direction of the gradient its expected direction might result in stepping out of this bound and there is even no guarantee that w satisfies it. we show in the following how to overcome this problem while maintaining the same convergence rate. the basic idea is to add a projection step namely we will now have a two-step update rule where we first subtract a subgradient from the current value of w and then project the resulting vector onto h. formally wt vt wt argminw h wt the projection step replaces the current value of w by the vector in h closest to it. clearly the projection step guarantees that wt h for all t. since h is convex this also implies that w h as required. we next show that the analysis of sgd with projections remains the same. this is based on the following lemma. lemma lemma let h be a closed convex set and let v be the projection of w onto h namely v argmin x h stochastic gradient descent then for every u h proof by the convexity of h for every we have that v v h. therefore from the optimality of v we obtain v w u rearranging we obtain w u taking the limit we get that w u therefore v v w u equipped with the preceding lemma we can easily adapt the analysis of sgd to the case in which we add projection steps on a closed and convex set. simply note that for every t therefore lemma holds when we add projection steps and hence the rest of the analysis follows directly. variable step size another variant of sgd is decreasing the step size as a function of t. that is rather than updating with a constant we use t. for instance we can set t b and achieve a bound similar to theorem the idea is that when we are closer to the minimum of the function we take our steps more carefully so as not to overshoot the minimum. t variants other averaging techniques we have set the output vector to be w wt. there are alternative approaches such as outputting wt for some random t or outputting the t average of wt over the last t iterations for some one can also take a weighted average of the last few iterates. these more sophisticated averaging schemes can improve the convergence speed in some situations such as in the case of strongly convex functions defined in the following. strongly convex functions in this section we show a variant of sgd that enjoys a faster convergence rate for problems in which the objective function is strongly convex definition of strong convexity in the previous chapter. we rely on the following claim which generalizes lemma claim have if f is convex then for every w u and v f we u f f the proof is similar to the proof of lemma and is left as an exercise. sgd for minimizing a convex function goal solve minw h f parameter t initialize for t t choose a random vector vt s.t. evtwt f set t t set wt set arg minw h wt output w t wt tvt wt theorem assume that f is convex and that let argminw h f be an optimal solution. then ef w f t logt proof let evtwt. since f is strongly convex and is in the subgradient set of f at wt we have that f f next we show that t t stochastic gradient descent since is the projection of wt therefore t onto h and h we have that taking expectation of both sides rearranging and using the assumption yield equation comparing equation and equation and summing over t we obtain e f t t. next we use the definition t t and note that the first sum on the right-hand side of the equation collapses to thus f t logt the theorem follows from the preceding by dividing by t and using jensen s inequality. remark rakhlin shamir sridharan derived a convergence rate in which the logt term is eliminated for a variant of the algorithm in which we output the average of the last t iterates w tt wt. shamir t zhang have shown that theorem holds even if we output w wt learning with sgd we have so far introduced and analyzed the sgd algorithm for general convex functions. now we shall consider its applicability to learning tasks. sgd for risk minimization recall that in learning we face the problem of minimizing the risk function ldw e z z. we have seen the method of empirical risk minimization where we minimize the empirical risk lsw as an estimate to minimizing ldw. sgd allows us to take a different approach and minimize ldw directly. since we do not know d we cannot simply calculate ldwt and minimize it with the gd method. with sgd however all we need is to find an unbiased estimate of the gradient of learning with sgd ldw that is a random vector whose conditional expected value is ldwt. we shall now see how such an estimate can be easily constructed. for simplicity let us first consider the case of differentiable loss functions. hence the risk function ld is also differentiable. the construction of the random vector vt will be as follows first sample z d. then define vt to be the gradient of the function z with respect to w at the point wt. then by the linearity of the gradient we have evtwt e z d z e z z ldwt. the gradient of the loss function z at wt is therefore an unbiased estimate of the gradient of the risk function ldwt and is easily constructed by sampling a single fresh example z d at each iteration t. the same argument holds for nondifferentiable loss functions. we simply let vt be a subgradient of z at wt. then for every u we have z z wt taking expectation on both sides with respect to z d and conditioned on the value of wt we obtain ldu ldwt z zwt wt wt it follows that evtwt is a subgradient of ldw at wt. to summarize the stochastic gradient descent framework for minimizing the risk is as follows. stochastic gradient descent for minimizing ldw parameters scalar integer t initialize for t t sample z d pick vt z update wt vt output w t wt we shall now use our analysis of sgd to obtain a sample complexity analysis for learning convex-lipschitz-bounded problems. theorem yields the following corollary consider a convex-lipschitz-bounded learning problem with parameters b. then for every if we run the sgd method for minimizing stochastic gradient descent ldw with a number of iterations number of examples t and with t then the output of sgd satisfies w h ldw e w min it is interesting to note that the required sample complexity is of the same order of magnitude as the sample complexity guarantee we derived for regularized loss minimization. in fact the sample complexity of sgd is even better than what we have derived for regularized loss minimization by a factor of analyzing sgd for convex-smooth learning problems in the previous chapter we saw that the regularized loss minimization rule also learns the class of convex-smooth-bounded learning problems. we now show that the sgd algorithm can be also used for such problems. theorem assume that for all z the loss function z is convex smooth and nonnegative. then if we run the sgd algorithm for minimizing ldw we have that for every t eld w proof recall that if a function is and nonnegative then it is selfbounded f f to analyze sgd for convex-smooth problems let us define zt the random samples of the sgd algorithm let ft zt and note that vt ftwt. for all t ft is a convex function and therefore ftwt wt summing over t and using lemma we obtain combining the preceding with the self-boundedness of ft yields wt ftwt t t ftwt. t dividing by t and rearranging we obtain next we take expectation of the two sides of the preceding equation with respect learning with sgd to zt clearly in addition using the same argument as in the proof of theorem we have that e t t ftwt e ldwt eld w. combining all we conclude our proof. as a direct corollary we obtain corollary consider a convex-smooth-bounded learning problem with parameters b. assume in addition that z for all z z. for every set then running sgd with t yields eld w min w h ldw sgd for regularized loss minimization we have shown that sgd enjoys the same worst-case sample complexity bound as regularized loss minimization. however on some distributions regularized loss minimization may yield a better solution. therefore in some cases we may want to solve the optimization problem associated with regularized loss minimization min w lsw define f since we are dealing with convex learning problems in which the loss function is convex the preceding problem is also a convex optimization problem that can be solved using sgd as well as we shall see in this section. lsw. note that f is a convex function therefore we can apply the sgd variant given in section h rd. to apply this algorithm we only need to find a way to construct an unbiased estimate of a subgradient of f at wt. this is easily done by noting that if we pick z uniformly at random from s and choose vt z then the expected value of wt vt is a subgradient of f at wt. to analyze the resulting algorithm we first rewrite the update rule we divided by for convenience. stochastic gradient descent that h rd and therefore the projection step does not matter as follows wt t wt vt vt vt t t wt t t t wt t t t wt t t vi. t vt vt if we assume that the loss function is it follows that for all t we have and therefore which yields wt theorem therefore tells us that after performing t iterations we have that ef w f t logt summary we have introduced the gradient descent and stochastic gradient descent algorithms along with several of their variants. we have analyzed their convergence rate and calculated the number of iterations that would guarantee an expected objective of at most plus the optimal objective. most importantly we have shown that by using sgd we can directly minimize the risk function. we do so by sampling a point i.i.d from d and using a subgradient of the loss of the current hypothesis wt at this point as an unbiased estimate of the gradient a subgradient of the risk function. this implies that a bound on the number of iterations also yields a sample complexity bound. finally we have also shown how to apply the sgd method to the problem of regularized risk minimization. in future chapters we show how this yields extremely simple solvers to some optimization problems associated with regularized risk minimization. bibliographic remarks sgd dates back to robbins monro it is especially effective in large scale machine learning problems. see for example le cun zhang bottou bousquet shalev-shwartz singer srebro shalev-shwartz srebro in the optimization community it was studied exercises in the context of stochastic optimization. see for example yudin nesterov nesterov nesterov nemirovski juditsky lan shapiro shapiro dentcheva ruszczy nski the bound we have derived for strongly convex function is due to hazan agarwal kale as mentioned previously improved bounds have been obtained in rakhlin et al. exercises prove claim hint extend the proof of lemma prove corollary perceptron as a subgradient descent algorithm let s ym assume that there exists w rd such that for every i we have and let be a vector that has the minimal norm among all vectors that satisfy the preceding requirement. let r maxi define a function f max i yi separates the examples in s. show that f and show that any w for which f show how to calculate a subgradient of f describe and analyze the subgradient descent algorithm for this case. compare the algorithm and the analysis to the batch perceptron algorithm given in section variable step size prove an analog of theorem for sgd with a variable step size t b t support vector machines in this chapter and the next we discuss a very useful machine learning tool the support vector machine paradigm for learning linear predictors in high dimensional feature spaces. the high dimensionality of the feature space raises both sample complexity and computational complexity challenges. the svm algorithmic paradigm tackles the sample complexity challenge by searching for large margin separators. roughly speaking a halfspace separates a training set with a large margin if all the examples are not only on the correct side of the separating hyperplane but also far away from it. restricting the algorithm to output a large margin separator can yield a small sample complexity even if the dimensionality of the feature space is high even infinite. we introduce the concept of margin and relate it to the regularized loss minimization paradigm as well as to the convergence rate of the perceptron algorithm. in the next chapter we will tackle the computational complexity challenge using the idea of kernels. margin and hard-svm let s ym be a training set of examples where each xi rd and yi we say that this training set is linearly separable if there exists a halfspace b such that yi b for all i. alternatively this condition can be rewritten as i b all halfspaces b that satisfy this condition are erm hypotheses error is zero which is the minimum possible error. for any separable training sample there are many erm halfspaces. which one of them should the learner pick? consider for example the training set described in the picture that follows. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning margin and hard-svm x x while both the dashed-black and solid-green hyperplanes separate the four examples our intuition would probably lead us to prefer the black hyperplane over the green one. one way to formalize this intuition is using the concept of margin. the margin of a hyperplane with respect to a training set is defined to be the minimal distance between a point in the training set and the hyperplane. if a hyperplane has a large margin then it will still separate the training set even if we slightly perturb each instance. we will see later on that the true error of a halfspace can be bounded in terms of the margin it has over the training sample larger the margin the smaller the error regardless of the euclidean dimension in which this halfspace resides. hard-svm is the learning rule in which we return an erm hyperplane that separates the training set with the largest possible margin. to define hard-svm formally we first express the distance between a point x to a hyperplane using the parameters defining the halfspace. claim the distance between a point x and the hyperplane defined by b where is b. proof the distance between a point x and the hyperplane is defined as b taking v x bw we have that b b and b. hence the distance is at most b. next take any other point u on the hyperplane thus b we have v v v v v v v where the last equality is because b. hence the distance support vector machines between x and u is at least the distance between x and v which concludes our proof. on the basis of the preceding claim the closest point in the training set to the separating hyperplane is mini b. therefore the hard-svm rule is argmax min i b s.t. i b whenever there is a solution to the preceding problem we are in the separable case we can write an equivalent problem as follows exercise argmax min i b. next we give another equivalent formulation of the hard-svm rule as a quadratic optimization hard-svm input ym solve argmin s.t. i b output w b the lemma that follows shows that the output of hard-svm is indeed the separating hyperplane with the largest margin. intuitively hard-svm searches for w of minimal norm among all the vectors that separate the data and for which b for all i. in other words we enforce the margin to be but now the units in which we measure the margin scale with the norm of w. therefore finding the largest margin halfspace boils down to finding w whose norm is minimal. formally lemma the output of hard-svm is a solution of equation proof let be a solution of equation and define the margin achieved by to be mini therefore for all i we have or equivalently hence the pair satisfies the conditions of the quadratic optimization a quadratic optimization problem is an optimization problem in which the objective is a convex quadratic function and the constraints are linear inequalities. margin and hard-svm problem given in equation therefore for all i it follows that w b since we obtain that w b is an optimal solution of equation the homogenous case it is often more convenient to consider homogenous halfspaces namely halfspaces that pass through the origin and are thus defined by where the bias term b is set to be zero. hard-svm for homogenous halfspaces amounts to solving s.t. i min w as we discussed in chapter we can reduce the problem of learning nonhomogenous halfspaces to the problem of learning homogenous halfspaces by adding one more feature to each instance of xi thus increasing the dimension to d note however that the optimization problem given in equation does not regularize the bias term b while if we learn a homogenous halfspace in using equation then we regularize the bias term the d component of the weight vector as well. however regularizing b usually does not make a significant difference to the sample complexity. the sample complexity of hard-svm recall that the vc-dimension of halfspaces in rd is d it follows that the sample complexity of learning halfspaces grows with the dimensionality of the problem. furthermore the fundamental theorem of learning tells us that if the number of examples is significantly smaller than d then no algorithm can learn an halfspace. this is problematic when d is very large. to overcome this problem we will make an additional assumption on the underlying data distribution. in particular we will define a separability with margin assumption and will show that if the data is separable with margin then the sample complexity is bounded from above by a function of it follows that even if the dimensionality is very large even infinite as long as the data adheres to the separability with margin assumption we can still have a small sample complexity. there is no contradiction to the lower bound given in the fundamental theorem of learning because we are now making an additional assumption on the underlying data distribution. before we formally define the separability with margin assumption there is a scaling issue we need to resolve. suppose that a training set s ym is separable with a margin namely the maximal objective value of equation is at least then for any positive scalar the training set support vector machines xm ym is separable with a margin of that is a simple scaling of the data can make it separable with an arbitrarily large margin. it follows that in order to give a meaningful definition of margin we must take into account the scale of the examples as well. one way to formalize this is using the definition that follows. definition let d be a distribution over rd we say that d is separable with a if there exists such that and such that with probability over the choice of y d we have that and similarly we say that d is separable with a using a homogenous halfspace if the preceding holds with a halfspace of the form in the advanced part of the book we will prove that the sample complexity of hard-svm depends on and is independent of the dimension d. in particular theorem in section states the following theorem let d be a distribution over rd that satisfies the with margin assumption using a homogenous halfspace. then with probability of at least over the choice of a training set of size m the error of the output of hard-svm is at most m m remark and the perceptron in section we have described and analyzed the perceptron algorithm for finding an erm hypothesis with respect to the class of halfspaces. in particular in theorem we upper bounded the number of updates the perceptron might make on a given training set. it can be shown exercise that the upper bound is exactly where is the radius of examples and is the margin. soft-svm and norm regularization the hard-svm formulation assumes that the training set is linearly separable which is a rather strong assumption. soft-svm can be viewed as a relaxation of the hard-svm rule that can be applied even if the training set is not linearly separable. the optimization problem in equation enforces the hard constraints b for all i. a natural relaxation is to allow the constraint to be violated for some of the examples in the training set. this can be modeled by introducing nonnegative slack variables m and replacing each constraint b by the constraint b i. that is i measures by how much the constraint is being violated. soft-svm jointly minimizes the norm of w to the margin and the average of i to the violations of the constraints. the tradeoff between the two soft-svm and norm regularization terms is controlled by a parameter this leads to the soft-svm optimization problem soft-svm input ym parameter solve m min wb s.t. i b i and i i output w b we can rewrite equation as a regularized loss minimization problem. recall the definition of the hinge loss b y b. given b and a training set s the averaged hinge loss on s is denoted by lhinge b. now consider the regularized loss minimization problem s lhinge s b min wb claim equation and equation are equivalent. proof fix some w b and consider the minimization over in equation fix some i. since i must be nonnegative the best assignment to i would be if b and would be b otherwise. in other words i b yi for all i and the claim follows. we therefore see that soft-svm falls into the paradigm of regularized loss minimization that we studied in the previous chapter. a soft-svm algorithm that is a solution for equation has a bias toward low norm separators. the objective function that we aim to minimize in equation penalizes not only for training errors but also for large norm. it is often more convenient to consider soft-svm for learning a homogenous halfspace where the bias term b is set to be zero which yields the following optimization problem lhinge s min w where lhinge s m support vector machines the sample complexity of soft-svm we now analyze the sample complexity of soft-svm for the case of homogenous halfspaces the output of equation in corollary we derived a generalization bound for the regularized loss minimization framework assuming that the loss function is convex and lipschitz. we have already shown that the hinge loss is convex so it is only left to analyze the lipschitzness of the hinge loss. claim let f then f is it is easy to verify that any subgradient of f at w is of the form x where proof the claim now follows from lemma corollary therefore yields the following corollary let d be a distribution over x where x consider running soft-svm on a training set s dm and let as be the solution of soft-svm. then for every u e s dm lhinged m furthermore since the hinge loss upper bounds the loss we also have e s dm lhinged m last for every b if we set then e s dm e s dm min b lhinged m we therefore see that we can control the sample complexity of learning a halfspace as a function of the norm of that halfspace independently of the euclidean dimension of the space over which the halfspace is defined. this becomes highly significant when we learn via embeddings into high dimensional feature spaces as we will consider in the next chapter. remark the condition that x will contain vectors with a bounded norm follows from the requirement that the loss function will be lipschitz. this is not just a technicality. as we discussed before separation with large margin is meaningless without imposing a restriction on the scale of the instances. indeed without a constraint on the scale we can always enlarge the margin by multiplying all instances by a large scalar. margin and norm-based bounds versus dimension the bounds we have derived for hard-svm and soft-svm do not depend on the dimension of the instance space. instead the bounds depend on the norm of the soft-svm and norm regularization examples the norm of the halfspace b equivalently the margin parameter and in the nonseparable case the bounds also depend on the minimum hinge loss of all halfspaces of norm b. in contrast the vc-dimension of the class of homogenous halfspaces is d which implies that the error of an erm hypothesis decreases as does. we now give an example in which d hence the bound given in corollary is much better than the vc bound. consider the problem of learning to classify a short text document according to its topic say whether the document is about sports or not. we first need to represent documents as vectors. one simple yet effective way is to use a bagof-words representation. that is we define a dictionary of words and set the dimension d to be the number of words in the dictionary. given a document we represent it as a vector x where xi if the i th word in the dictionary appears in the document and xi otherwise. therefore for this problem the value of will be the maximal number of distinct words in a given document. a halfspace for this problem assigns weights to words. it is natural to assume that by assigning positive and negative weights to a few dozen words we will be able to determine whether a given document is about sports or not with reasonable accuracy. therefore for this problem the value of can be set to be less than overall it is reasonable to say that the value of is smaller than on the other hand a typical size of a dictionary is much larger than for example there are more than distinct words in english. we have therefore shown a problem in which there can be an order of magnitude difference between learning a halfspace with the svm rule and learning a halfspace using the vanilla erm rule. of course it is possible to construct problems in which the svm bound will be worse than the vc bound. when we use svm we in fact introduce another form of inductive bias we prefer large margin halfspaces. while this inductive bias can significantly decrease our estimation error it can also enlarge the approximation error. the ramp loss the margin-based bounds we have derived in corollary rely on the fact that we minimize the hinge loss. as we have shown in the previous subsection the term can be much smaller than the corresponding term in the vc however the approximation error in corollary is measured with respect to the hinge loss while the approximation error in vc bounds is measured with respect to the loss. since the hinge loss upper bounds the loss the approximation error with respect to the loss will never exceed that of the hinge loss. for the loss. this follows from the fact that the loss is scale it is not possible to derive bounds that involve the estimation error term support vector machines insensitive and therefore there is no meaning to the norm of w or its margin when we measure error with the loss. however it is possible to define a loss function that on one hand it is scale sensitive and thus enjoys the estimation while on the other hand it is more similar to the loss. one option is the ramp loss defined as y y the ramp loss penalizes mistakes in the same way as the loss and does not penalize examples that are separated with margin. the difference between the ramp loss and the loss is only with respect to examples that are correctly classified but not with a significant margin. generalization bounds for the ramp loss are given in the advanced part of this book appendix the reason svm relies on the hinge loss and not on the ramp loss is that the hinge loss is convex and therefore from the computational point of view minimizing the hinge loss can be performed efficiently. in contrast the problem of minimizing the ramp loss is computationally intractable. optimality conditions and support vectors the name support vector machine stems from the fact that the solution of hard-svm is supported by is in the linear span of the examples that are exactly at distance from the separating hyperplane. these vectors are therefore called support vectors. to see this we rely on fritz john optimality conditions. theorem let be as defined in equation and let i then there exist coefficients m such that the examples i i are called support vectors. the proof of this theorem follows by applying the following lemma to equa tion i i ixi. duality lemma john suppose that argmin f w s.t. i giw f where f gm are differentiable. then there exists rm such that i i i where i duality historically many of the properties of svm have been obtained by considering the dual of equation our presentation of svm does not rely on duality. for completeness we present in the following how to derive the dual of equation we start by rewriting the problem in an equivalent form as follows. consider the function gw max rm if i otherwise we can therefore rewrite equation as min w rearranging the preceding we obtain that equation can be rewritten as the problem min w max rm now suppose that we flip the order of min and max in the above equation. this can only decrease the objective value exercise and we have min w max rm max rm min w the preceding inequality is called weak duality. it turns out that in our case strong duality also holds namely the inequality holds with equality. therefore the dual problem is max rm min w we can simplify the dual problem by noting that once is fixed the optimization support vector machines problem with respect to w is unconstrained and the objective is differentiable thus at the optimum the gradient equals zero iyixi w iyixi. this shows us that the solution must be in the linear span of the examples a fact we will use later to derive svm with kernels. plugging the preceding into equation we obtain that the dual problem can be rewritten as w max rm iyixi i jyjxj xi rearranging yields the dual problem j yi max rm i i note that the dual problem only involves inner products between instances and does not require direct access to specific elements within an instance. this property is important when implementing svm with kernels as we will discuss in the next chapter. implementing soft-svm using sgd in this section we describe a very simple algorithm for solving the optimization problem of soft-svm namely min w m we rely on the sgd framework for solving regularized loss minimization problems as described in section recall that on the basis of equation we can rewrite the update rule of sgd as t vj where vj is a subgradient of the loss function at wj on the random example chosen at iteration j. for the hinge loss given an example y we can choose vj to be if and vj y x otherwise example denoting jt vj we obtain the following procedure. summary sgd for solving soft-svm goal solve equation parameter t initialize for t t t let wt choose i uniformly at random from if set yixi else set wt output w t summary svm is an algorithm for learning halfspaces with a certain type of prior knowledge namely preference for large margin. hard-svm seeks the halfspace that separates the data perfectly with the largest margin whereas soft-svm does not assume separability of the data and allows the constraints to be violated to some extent. the sample complexity for both types of svm is different from the sample complexity of straightforward halfspace learning as it does not depend on the dimension of the domain but rather on parameters such as the maximal norms of x and w. the importance of dimension-independent sample complexity will be realized in the next chapter where we will discuss the embedding of the given domain into some high dimensional feature space as means for enriching our hypothesis class. such a procedure raises computational and sample complexity problems. the latter is solved by using svm whereas the former can be solved by using svm with kernels as we will see in the next chapter. bibliographic remarks svms have been introduced in vapnik boser guyon vapnik there are many good books on the theoretical and practical aspects of svms. for example cristianini shawe-taylor sch olkopf smola hsu chang lin steinwart christmann using sgd for solving soft-svm has been proposed in shalev-shwartz et al. support vector machines exercises show that the hard-svm rule namely b argmax min i s.t. i b is equivalent to the following formulation argmax min i b. hint define g b i b show that argmax min i show that b g b g min i b min i b margin and the perceptron consider a training set that is linearly separable with a margin and such that all the instances are within a ball of radius prove that the maximal number of updates the batch perceptron algorithm given in section will make when running on this training set is hard versus soft svm prove or refute the following claim there exists such that for every sample s of m examples which is separable by the class of homogenous halfspaces the hard-svm and the soft-svm parameter learning rules return exactly the same weight vector. weak duality prove that for any function f of two vector variables x x y y it holds that y y f y max x x max min y y min x x f y. kernel methods in the previous chapter we described the svm paradigm for learning halfspaces in high dimensional feature spaces. this enables us to enrich the expressive power of halfspaces by first mapping the data into a high dimensional feature space and then learning a linear predictor in that space. this is similar to the adaboost algorithm which learns a composition of a halfspace over base hypotheses. while this approach greatly extends the expressiveness of halfspace predictors it raises both sample complexity and computational complexity challenges. in the previous chapter we tackled the sample complexity issue using the concept of margin. in this chapter we tackle the computational complexity challenge using the method of kernels. we start the chapter by describing the idea of embedding the data into a high dimensional feature space. we then introduce the idea of kernels. a kernel is a type of a similarity measure between instances. the special property of kernel similarities is that they can be viewed as inner products in some hilbert space euclidean space of some high dimension to which the instance space is virtually embedded. we introduce the kernel trick that enables computationally efficient implementation of learning without explicitly handling the high dimensional representation of the domain instances. kernel based learning algorithms and in particular kernel-svm are very useful and popular machine learning tools. their success may be attributed both to being flexible for accommodating domain specific prior knowledge and to having a well developed set of efficient implementation algorithms. embeddings into feature spaces the expressive power of halfspaces is rather restricted for example the following training set is not separable by a halfspace. let the domain be the real line consider the domain points where the labels are for all x such that and otherwise. to make the class of halfspaces more expressive we can first map the original instance space into another space of a higher dimension and then learn a halfspace in that space. for example consider the example mentioned previously. instead of learning a halfspace in the original representation let us understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning kernel methods first define a mapping r as follows we use the term feature space to denote the range of after applying the data can be easily explained using the halfspace hx b where w and b the basic paradigm is as follows given some domain set x and a learning task choose a mapping x f for some feature space f that will usually be rn for some n the range of such a mapping can be any hilbert space including such spaces of infinite dimension as we will show later. given a sequence of labeled examples s ym create the image sequence s ym. train a linear predictor h over s. predict the label of a test point x to be h note that for every probability distribution d over x y we can readily define its image probability distribution d over f y by setting for every subset a f y d d it follows that for every predictor h over the feature space ld ldh where h is the composition of h onto the success of this learning paradigm depends on choosing a good for a given learning task that is a that will make the image of the data distribution to being linearly separable in the feature space thus making the resulting algorithm a good learner for a given task. picking such an embedding requires prior knowledge about that task. however often some generic mappings that enable us to enrich the class of halfspaces and extend its expressiveness are used. one notable example is polynomial mappings which are a generalization of the we have seen in the previous example. recall that the prediction of a standard halfspace classifier on an instance x is based on the linear mapping x we can generalize linear mappings to a polynomial mapping x px where p is a multivariate polynomial of degree k. for simplicity consider first the case in which x is dimensional. wjxj where w is the vector of coefficients of the polynomial we need to learn. we can rewrite px where r is the mapping x x xk. it follows that learning a k degree polynomial over r can be done by learning a linear mapping in the dimensional feature space. in that case px more generally a degree k multivariate polynomial from rn to r can be writ ten as px wj xji. j k this is defined for every a such that is measurable with respect to d. the kernel trick as before we can rewrite px where now rn rd is such that for every j r k the coordinate of associated with j is the xji. naturally polynomial-based classifiers yield much richer hypothesis classes than halfspaces. we have seen at the beginning of this chapter an example in which the training set in its original domain r cannot be separable by a halfspace but after the embedding x it is perfectly separable. so while the classifier is always linear in the feature space it can have highly nonlinear behavior on the original space from which instances were sampled. in general we can choose any feature mapping that maps the original instances into some hilbert the euclidean space rd is a hilbert space for any finite d. but there are also infinite dimensional hilbert spaces we shall see later on in this chapter. the bottom line of this discussion is that we can enrich the class of halfspaces by first applying a nonlinear mapping that maps the instance space into some feature space and then learning a halfspace in that feature space. however if the range of is a high dimensional space we face two problems. first the vcdimension of halfspaces in rn is n and therefore if the range of is very large we need many more samples in order to learn a halfspace in the range of second from the computational point of view performing calculations in the high dimensional space might be too costly. in fact even the representation of the vector w in the feature space can be unrealistic. the first issue can be tackled using the paradigm of large margin low norm predictors as we already discussed in the previous chapter in the context of the svm algorithm. in the following section we address the computational issue. the kernel trick we have seen that embedding the input space into some high dimensional feature space makes halfspace learning more expressive. however the computational complexity of such learning may still pose a serious hurdle computing linear separators over very high dimensional data may be computationally expensive. the common solution to this concern is kernel based learning. the term kernels is used in this context to describe inner products in the feature space. given an embedding of some domain space x into some hilbert space we define the kernel function kx one can think of k as specifying similarity between instances and of the embedding as mapping the domain set a hilbert space is a vector space with an inner product which is also complete. a space is in our case the norm is defined by the inner the reason we require complete if all cauchy sequences in the space converge. the range of to be in a hilbert space is that projections in a hilbert space are well defined. in particular if m is a linear subspace of a hilbert space then every x in the hilbert space can be written as a sum x u v where u m and for all w m we use this fact in the proof of the representer theorem given in the next section. kernel methods x into a space where these similarities are realized as inner products. it turns out that many learning algorithms for halfspaces can be carried out just on the basis of the values of the kernel function over pairs of domain points. the main advantage of such algorithms is that they implement linear separators in high dimensional feature spaces without having to specify points in that space or expressing the embedding explicitly. the remainder of this section is devoted to constructing such algorithms. in the previous chapter we saw that regularizing the norm of w yields a small sample complexity even if the dimensionality of the feature space is high. interestingly as we show later regularizing the norm of w is also helpful in overcoming the computational problem. to do so first note that all versions of the svm optimization problem we have derived in the previous chapter are instances of the following general problem min w where f rm r is an arbitrary function and r r r is a cally nondecreasing function. for example soft-svm for homogenous halfspaces can be derived from equation by letting ra and i yiai. similarly hard-svm for nonhomogenous f am m halfspaces can be derived from equation by letting ra and letting f am be if there exists b such that yiai for all i and f am otherwise. tion that lies in the span of theorem theorem assume that is a mapping from x to i a hilbert space. then there exists a vector rm such that w the following theorem shows that there exists an optimal solution of equa is an optimal solution of equation proof let be an optimal solution of equation because is an element of a hilbert space we can rewrite as i u where for all i. set w u. clearly thus since r is nondecreasing we obtain that additionally for all i we have that u hence f f w we have shown that the objective of equation at w cannot be larger than the objective at and therefore w is also an optimal solution. since i we conclude our proof. min rm f r jkxj jkxj xm the kernel trick w on the basis of the representer theorem we can optimize equation with respect to the coefficients instead of the coefficients w as follows. writing j we have that for all i j similarly j j j j j i let kx be a function that implements the kernel function with respect to the embedding instead of solving equation we can solve the equivalent problem i jkxj xi to solve the optimization problem given in equation we do not need any direct access to elements in the feature space. the only thing we should know is how to calculate inner products in the feature space or equivalently to calculate the kernel function. in fact to solve equation we solely need to know the value of the m m matrix g s.t. gij kxi xj which is often called the gram matrix. in particular specifying the preceding to the soft-svm problem given in equa tion we can rewrite the problem as min rm t g yig m where is the i th element of the vector obtained by multiplying the gram matrix g by the vector note that equation can be written as quadratic programming and hence can be solved efficiently. in the next section we describe an even simpler algorithm for solving soft-svm with kernels. once we learn the coefficients we can calculate the prediction on a new instance by jkxj x. the advantage of working with kernels rather than directly optimizing w in the feature space is that in some situations the dimension of the feature space kernel methods is extremely large while implementing the kernel function is very simple. a few examples are given in the following. example kernels the k degree polynomial kernel is defined to be kx now we will show that this is indeed a kernel function. that is we will show that there exists a mapping from the original space to some higher dimensional space for which kx for simplicity denote then we have kx j element of that j now if we define rn j xji j ji ji such that for j nk there is an xji we obtain that kx since contains all the monomials up to degree k a halfspace over the range of corresponds to a polynomial predictor of degree k over the original space. hence learning a halfspace with a k degree polynomial kernel enables us to learn polynomial predictors of degree k over the original space. note that here the complexity of implementing k is on while the dimension of the feature space is on the order of nk. example kernel let the original instance space be r and consider the mapping where for each nonnegative integer n there exists an element that equals xn. then e e n! n! e e xn e n! n! here the feature space is of infinite dimension while evaluating the kernel is very the kernel trick simple. more generally given a scalar the gaussian kernel is defined to be kx e intuitively the gaussian kernel sets the inner product in the feature space between x to be close to zero if the instances are far away from each other the original domain and close to if they are close. is a parameter that controls the scale determining what we mean by close. it is easy to verify that k implements an inner product in a space in which for any n and any monomial of order k there exists an element of that equals xji. hence we can learn any polynomial predictor over the original space by using a gaussian kernel. e n! recall that the vc-dimension of the class of all polynomial predictors is infinite exercise there is no contradiction because the sample complexity required to learn with gaussian kernels depends on the margin in the feature space which will be large if we are lucky but can in general be arbitrarily small. the gaussian kernel is also called the rbf kernel for radial basis func tions. kernels as a way to express prior knowledge as we discussed previously a feature mapping may be viewed as expanding the class of linear classifiers to a richer class to linear classifiers over the feature space. however as discussed in the book so far the suitability of any hypothesis class to a given learning task depends on the nature of that task. one can therefore think of an embedding as a way to express and utilize prior knowledge about the problem at hand. for example if we believe that positive examples can be distinguished by some ellipse we can define to be all the monomials up to order or use a degree polynomial kernel. as a more realistic example consider the task of learning to find a sequence of characters signature in a file that indicates whether it contains a virus or not. formally let xd be the set of all strings of length at most d over some alphabet set the hypothesis class that one wishes to learn is h v xd where for a string x xd hvx is iff v is a substring of x hvx otherwise. let us show how using an appropriate embedding this class can be realized by linear classifiers over the resulting feature space. consider a mapping to a space rs where s so that each coordinate of corresponds to some string v and indicates whether v is a substring of x is for every x xd is a vector in note that the dimension of this feature space is exponential in d. it is not hard to see that every member of the class h can be realized by composing a linear classifier over and moreover by such a halfspace whose norm is and that attains a margin of exercise furthermore for every x x od. so overall it is learnable using svm with a sample kernel methods complexity that is polynomial in d. however the dimension of the feature space is exponential in d so a direct implementation of svm over the feature space is problematic. luckily it is easy to calculate the inner product in the feature space the kernel function without explicitly mapping instances into the feature space. indeed kx is simply the number of common substrings of x and which can be easily calculated in time polynomial in d. this example also demonstrates how feature mapping enables us to use halfspaces for nonvectorial domains. characterizing kernel functions as we have discussed in the previous section we can think of the specification of the kernel matrix as a way to express prior knowledge. consider a given similarity function of the form k x x r. is it a valid kernel function? that is does it represent an inner product between and for some feature mapping the following lemma gives a sufficient and necessary condition. lemma a symmetric function k x x r implements an inner product in some hilbert space if and only if it is positive semidefinite namely for all xm the gram matrix gij kxi xj is a positive semidefinite matrix. proof it is trivial to see that if k implements an inner product in some hilbert space then the gram matrix is positive semidefinite. for the other direction define the space of functions over x as rx x r. for each x x let be the function x k x. define a vector space by taking all linear combinations of elements of the form k x. define an inner product on this vector space to ik xi jk j i jkxi j. i j ij this is a valid inner product since it is symmetric k is symmetric it is linear and it is positive definite is easy to see that kx x with equality only for being the zero function. clearly x k kx which concludes our proof. implementing soft-svm with kernels next we turn to solving soft-svm with kernels. while we could have designed an algorithm for solving equation there is an even simpler approach that implementing soft-svm with kernels directly tackles the soft-svm optimization problem in the feature space min w m while only using kernel evaluations. the basic observation is that the vector wt maintained by the sgd procedure we have described in section is always in the linear span of therefore rather than maintaining wt we can maintain the corresponding coefficients formally let k be the kernel function namely for all x kx we shall maintain two vectors in rm corresponding to two vectors and wt defined in the sgd procedure of section that is will be a vector such that and be such that wt j j the vectors and are updated according to the following procedure. sgd for solving soft-svm with kernels goal solve equation parameter t initialize for t t let choose i uniformly at random from for all j i set if j kxj xi set t j j i yi i else output w set i i j where t the following lemma shows that the preceding implementation is equivalent to running the sgd procedure described in section on the feature space. tion when applied on the feature space and let w lemma let w be the output of the sgd procedure described in j be the output of applying sgd with kernels. then w w. proof we will show that for every t equation holds where is the result of running the sgd procedure described in section in the feature kernel methods t this claim implies space. by the definition of that equation also holds and the proof of our lemma will follow. to prove that equation holds we use a simple inductive argument. for t the claim trivially holds. assume it holds for t then t and wt yi wt yi j yi j kxj xi. j hence the condition in the two algorithms is equivalent and if we update we have j yi j yi which concludes our proof. summary mappings from the given domain to some higher dimensional space on which a halfspace predictor is used can be highly powerful. we benefit from a rich and complex hypothesis class yet need to solve the problems of high sample and computational complexities. in chapter we discussed the adaboost algorithm which faces these challenges by using a weak learner even though we re in a very high dimensional space we have an oracle that bestows on us a single good coordinate to work with on each iteration. in this chapter we introduced a different approach the kernel trick. the idea is that in order to find a halfspace predictor in the high dimensional space we do not need to know the representation of instances in that space but rather the values of inner products between the mapped instances. calculating inner products between instances in the high dimensional space without using their representation in that space is done using kernel functions. we have also shown how the sgd algorithm can be implemented using kernels. the ideas of feature mapping and the kernel trick allow us to use the framework of halfspaces and linear predictors for nonvectorial data. we demonstrated how kernels can be used to learn predictors over the domain of strings. we presented the applicability of the kernel trick in svm. however the kernel trick can be applied in many other algorithms. a few examples are given as exercises. this chapter ends the series of chapters on linear predictors and convex problems. the next two chapters deal with completely different types of hypothesis classes. bibliographic remarks bibliographic remarks in the context of svm the kernel-trick has been introduced in boser et al. see also aizerman braverman rozonoer the observation that the kernel-trick can be applied whenever an algorithm only relies on inner products was first stated by sch olkopf smola m uller the proof of the representer theorem is given in olkopf herbrich smola williamson sch olkopf herbrich smola the conditions stated in lemma are simplification of conditions due to mercer. many useful kernel functions have been introduced in the literature for various applications. we refer the reader to sch olkopf smola exercises consider the task of finding a sequence of characters in a file as described in section show that every member of the class h can be realized by composing a linear classifier over whose norm is and that attains a margin of kernelized perceptron show how to run the perceptron algorithm while only accessing the instances via the kernel function. hint the derivation is similar to the derivation of implementing sgd with kernels. kernel ridge regression the ridge regression problem with a feature mapping is the problem of finding a vector w that minimizes the function f and then returning the predictor hx such show how to implement the ridge regression algorithm with kernels. hint the representer theorem tells us that there exists a vector rm i is a minimizer of equation let g be the gram matrix with regard to s and k. that is gij kxi xj. define g rm r by g t g tion then w where g is the i th column of g. show that if minimizes equa find a closed form expression for let n be any positive integer. for every x n define i is a minimizer of f g kx minx kernel methods prove that k is a valid kernel namely find a mapping n h where h is some hilbert space such that x n kx a supermarket manager would like to learn which of his customers have babies on the basis of their shopping carts. specifically he sampled i.i.d. customers where for customer i let xi d denote the subset of items the customer bought and let yi be the label indicating whether this customer has a baby. as prior knowledge the manager knows that there are k items such that the label is determined to be iff the customer bought at least one of these k items. of course the identity of these k items is not known there was nothing to learn. in addition according to the store regulation each customer can buy at most s items. help the manager to design a learning algorithm such that both its time complexity and its sample complexity are polynomial in s k and let x be an instance set and let be a feature mapping of x into some hilbert feature space v let k x x r be a kernel function that implements inner products in the feature space v consider the binary classification algorithm that predicts the label of an unseen instance according to the class with the closest average. formally given a training sequence s ym for every y we define iyiy cy my where my yi y. we assume that m and m are nonzero. then the algorithm outputs the following decision rule hx c otherwise. let w c c and let b show that hx b. show how to express hx on the basis of the kernel function and without accessing individual entries of or w. multiclass ranking and complex prediction problems multiclass categorization is the problem of classifying instances into one of several possible target classes. that is we are aiming at learning a predictor h x y where y is a finite set of categories. applications include for example categorizing documents according to topic is the set of documents and y is the set of possible topics or determining which object appears in a given image is the set of images and y is the set of possible objects. the centrality of the multiclass learning problem has spurred the development of various approaches for tackling the task. perhaps the most straightforward approach is a reduction from multiclass classification to binary classification. in section we discuss the most common two reductions as well as the main drawback of the reduction approach. we then turn to describe a family of linear predictors for multiclass problems. relying on the rlm and sgd frameworks from previous chapters we describe several practical algorithms for multiclass prediction. in section we show how to use the multiclass machinery for complex prediction problems in which y can be extremely large but has some structure on it. this task is often called structured output learning. in particular we demonstrate this approach for the task of recognizing handwritten words in which y is the set of all possible strings of some bounded length the size of y is exponential in the maximal length of a word. finally in section and section we discuss ranking problems in which the learner should order a set of instances according to their relevance. a typical application is ordering results of a search engine according to their relevance to the query. we describe several performance measures that are adequate for assessing the performance of ranking predictors and describe how to learn linear predictors for ranking problems efficiently. one-versus-all and all-pairs the simplest approach to tackle multiclass prediction problems is by reduction to binary classification. recall that in multiclass prediction we would like to learn a function h x y. without loss of generality let us denote y k. in the one-versus-all method one-versus-rest we train k binary clas understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning multiclass ranking and complex prediction problems sifiers each of which discriminates between one class and the rest of the classes. that is given a training set s ym where every yi is in y we construct k binary training sets sk where si in words si is the set of instances labeled if their label in s was i and otherwise. for every i we train a binary predictor hi x based on si hoping that hix should equal if and only if x belongs to class i. then given hk we construct a multiclass predictor using the rule hx argmax i hix. when more than one binary hypothesis predicts we should somehow decide which class to predict we can arbitrarily decide to break ties by taking the minimal index in argmaxi hix. a better approach can be applied whenever each hi hides additional information which can be interpreted as the confidence in the prediction y i. for example this is the case in halfspaces where the actual prediction is but we can interpret as the confidence in the prediction. in such cases we can apply the multiclass rule given in equation on the real valued predictions. a pseudocode of the one-versus-all approach is given in the following. one-versus-all input training set s ym algorithm for binary classification a foreach i y let si let hi asi output the multiclass hypothesis defined by hx argmaxi y hix another popular reduction is the all-pairs approach in which all pairs of classes are compared to each other. formally given a training set s ym where every yi is in for every i j k we construct a binary training sequence sij containing all examples from s whose label is either i or j. for each such an example we set the binary label in sij to be if the multiclass label in s is i and if the multiclass label in s is j. next we train a binary classification algorithm based on every sij to get hij. finally we construct a multiclass classifier by predicting the class that had the highest number of wins. a pseudocode of the all-pairs approach is given in the following. one-versus-all and all-pairs all-pairs input training set s ym algorithm for binary classification a foreach i j y s.t. i j initialize sij to be the empty sequence for t m if yt i add to sij if yt j add to sij let hij asij output the multiclass hypothesis defined by hx argmaxi y j y signj i hijx although reduction methods such as the one-versus-all and all-pairs are simple and easy to construct from existing algorithms their simplicity has a price. the binary learner is not aware of the fact that we are going to use its output hypotheses for constructing a multiclass predictor and this might lead to suboptimal results as illustrated in the following example. example consider a multiclass categorization problem in which the instance space is x and the label set is y suppose that instances of the different classes are located in nonintersecting balls as depicted in the following. suppose that the probability masses of classes are and respectively. consider the application of one-versus-all to this problem and assume that the binary classification algorithm used by one-versus-all is erm with respect to the hypothesis class of halfspaces. observe that for the problem of discriminating between class and the rest of the classes the optimal halfspace would be the all negative classifier. therefore the multiclass predictor constructed by one-versus-all might err on all the examples from class will be the case if the tie in the definition of hx is broken by the numerical value of the class label. in contrast if we choose hix where then the classifier defined by hx argmaxi hix perfectly predicts all the examples. we see and multiclass ranking and complex prediction problems that even though the approximation error of the class of predictors of the form hx is zero the one-versus-all approach might fail to find a good predictor from this class. linear multiclass predictors in light of the inadequacy of reduction methods in this section we study a more direct approach for learning multiclass predictors. we describe the family of linear multiclass predictors. to motivate the construction of this family recall that a linear predictor for binary classification a halfspace takes the form hx an equivalent way to express the prediction is as follows hx argmax y where yx is the vector obtained by multiplying each element of x by y. this representation leads to a natural generalization of halfspaces to multiclass problems as follows. let x y rd be a class-sensitive feature mapping. that is takes as input a pair y and maps it into a d dimensional feature vector. intuitively we can think of the elements of y as score functions that assess how well the label y fits the instance x. we will elaborate on later on. given and a vector w rd we can define a multiclass predictor h x y as follows hx argmax y y that is the prediction of h for the input x is the label that achieves the highest weighted score where weighting is according to the vector w. let w be some set of vectors in rd for example w rd b for some scalar b each pair w defines a hypothesis class of multiclass predictors h argmax y y w w. of course the immediate question which we discuss in the sequel is how to construct a good note that if y and we set y yx and w rd then h becomes the hypothesis class of homogeneous halfspace predictors for binary classification. how to construct as mentioned before we can think of the elements of y as score functions that assess how well the label y fits the instance x. naturally designing a good is similar to the problem of designing a good feature mapping we discussed in linear multiclass predictors chapter and as we will discuss in more detail in chapter two examples of useful constructions are given in the following. the multivector construction let y k and let x rn. we define x y rd where d nk as follows y ry xn rn rk yn that is y is composed of k vectors each of which is of dimension n where we set all the vectors to be the all zeros vector except the y th vector which is set to be x. it follows that we can think of w rnk as being composed of k weight vectors in rn that is w wk hence the name multivector construction. by the construction we have that and therefore the multiclass prediction becomes hx argmax y y a geometric illustration of the multiclass prediction over x is given in the following. tf-idf the previous definition of y does not incorporate any prior knowledge about the problem. we next describe an example of a feature function that does incorporate prior knowledge. let x be a set of text documents and y be a set of possible topics. let d be a size of a dictionary of words. for each word in the dictionary whose corresponding index is j let t f x be the number of times the word corresponding to j appears in the document x. this quantity is called term-frequency. additionally let df y be the number of times the word corresponding to j appears in documents in our training set that are not about topic y. this quantity is called document-frequency and measures whether word j is frequent in other topics. now define x y rd to be such that m jx y t f x log df where m is the total number of documents in our training set. the preceding quantity is called term-frequency-inverse-document-frequency or tf-idf for multiclass ranking and complex prediction problems short. intuitively jx y should be large if the word corresponding to j appears a lot in the document x but does not appear at all in documents that are not on topic y. if this is the case we tend to believe that the document x is on topic y. note that unlike the multivector construction described previously in the current construction the dimension of does not depend on the number of topics the size of y. cost-sensitive classification so far we used the zero-one loss as our performance measure of the quality of hx. that is the loss of a hypothesis h on an example y is if hx y and otherwise. in some situations it makes more sense to penalize different levels of loss for different mistakes. for example in object recognition tasks it is less severe to predict that an image of a tiger contains a cat than predicting that the image contains a whale. this can be modeled by specifying a loss function y y r where for every pair of labels y the loss of predicting the label when the correct label is y is defined to be y. we assume that y note that the zero-one loss can be easily modeled by setting y erm we have defined the hypothesis class h and specified a loss function to learn the class with respect to the loss function we can apply the erm rule with respect to this class. that is we search for a multiclass hypothesis h h parameterized by a vector w that minimizes the empirical risk with respect to lsh m yi. we now show that when w rd and we are in the realizable case then it is possible to solve the erm problem efficiently using linear programming. indeed in the realizable case we need to find a vector w rd that satisfies i yi argmax y y equivalently we need that w will satisfy the following set of linear inequalities i y y finding w that satisfies the preceding set of linear equations amounts to solving a linear program. as in the case of binary classification it is also possible to use a generalization of the perceptron algorithm for solving the erm problem. see exercise in the nonrealizable case solving the erm problem is in general computationally hard. we tackle this difficulty using the method of convex surrogate linear multiclass predictors loss functions section in particular we generalize the hinge loss to multiclass problems. generalized hinge loss recall that in binary classification the hinge loss is defined to be we now generalize the hinge loss to multiclass predictors of the form hwx argmax y recall that a surrogate convex loss should upper bound the original nonconvex loss which in our case is y. to derive an upper bound on y we first note that the definition of hwx implies that therefore y y hwx since hwx y we can upper bound the right-hand side of the preceding by y y max def y. we use the term generalized hinge loss to denote the preceding expression. as we have shown y y. furthermore equality holds whenever the score of the correct label is larger than the score of any other label by at least y namely y y. it is also immediate to see that y is a convex function with respect to w since it is a maximum over linear functions of w claim in chapter and that y is with y remark we use the name generalized hinge loss since in the binary case when y if we set y yx then the generalized hinge loss becomes the vanilla hinge loss for binary classification y geometric intuition the feature function x y rd maps each x into vectors in rd. the value of y will be zero if there exists a direction w such that when projecting the vectors onto this direction we obtain that each vector is represented by the scalar and we can rank the different points on the basis of these scalars so that the point corresponding to the correct y is top-ranked multiclass ranking and complex prediction problems for each y the difference between and is larger than the loss of predicting instead of y. the difference is also referred to as the margin section this is illustrated in the following figure w y y multiclass svm and sgd once we have defined the generalized hinge loss we obtain a convex-lipschitz learning problem and we can apply our general techniques for solving such problems. in particular the rlm technique we have studied in chapter yields the multiclass svm rule multiclass svm input ym parameters regularization parameter loss function y y r class-sensitive feature mapping x y rd solve y yi max min w rd m output the predictor hwx argmaxy we can solve the optimization problem associated with multiclass svm using generic convex optimization algorithms using the method described in section let us analyze the risk of the resulting hypothesis. the analysis seamlessly follows from our general analysis for convex-lipschitz problems given in chapter in particular applying corollary and using the fact that the generalized hinge loss upper bounds the loss we immediately obtain an analog of corollary corollary let d be a distribution over x y let x y rd and assume that for all x x and y y we have let b linear multiclass predictors on a training set s dm e consider running multiclass svm with and let hw be the output of multiclass svm. then min s dm b where l dh exy d y and lg hinge with being the generalized hinge-loss as defined in equation dhw lg hinge d hinge s dm e d d m exy y we can also apply the sgd learning framework for minimizing lg hinge as described in chapter recall claim which dealt with subgradients of max functions. in light of this claim in order to find a subgradient of the generalized hinge loss all we need to do is to find y y that achieves the maximum in the definition of the generalized hinge loss. this yields the following algorithm d sgd for multiclass learning parameters scalar integer t loss function y y r class-sensitive feature mapping x y rd initialize rd for t t sample y d set vt y y update wt vt output w t find y y y wt our general analysis of sgd given in corollary immediately implies corollary let d be a distribution over x y let x y rd and assume that for all x x and y y we have let b then for every if we run sgd for multiclass learning with a number of iterations number of examples t and with t then the output of sgd satisfies w min b hinge s dm dh w e d e s dm lg hinge d remark it is interesting to note that the risk bounds given in corollary and corollary do not depend explicitly on the size of the label set y a fact we will rely on in the next section. however the bounds may depend implicitly on the size of y via the norm of y and the fact that the bounds are meaningful only when there exists some vector u b for which lg hinge d is not excessively large. multiclass ranking and complex prediction problems structured output prediction structured output prediction problems are multiclass problems in which y is very large but is endowed with a predefined structure. the structure plays a key role in constructing efficient algorithms. to motivate structured learning problems consider the problem of optical character recognition suppose we receive an image of some handwritten word and would like to predict which word is written in the image. to simplify the setting suppose we know how to segment the image into a sequence of images each of which contains a patch of the image corresponding to a single letter. therefore x is the set of sequences of images and y is the set of sequences of letters. note that the size of y grows exponentially with the maximal length of a word. an example of an image x corresponding to the label y workable is given in the following. to tackle structure prediction we can rely on the family of linear predictors described in the previous section. in particular we need to define a reasonable loss function for the problem as well as a good class-sensitive feature mapping by good we mean a feature mapping that will lead to a low approximation error for the class of linear predictors with respect to and once we do this we can rely for example on the sgd learning algorithm defined in the previous section. however the huge size of y poses several challenges to apply the multiclass prediction we need to solve a maximization problem over y. how can we predict efficiently when y is so large? how do we train w efficiently? in particular to apply the sgd rule we again need to solve a maximization problem over y. how can we avoid overfitting? in the previous section we have already shown that the sample complexity of learning a linear multiclass predictor does not depend explicitly on the number of classes. we just need to make sure that the norm of the range of is not too large. this will take care of the overfitting problem. to tackle the computational challenges we rely on the structure of the problem and define the functions and so that calculating the maximization problems in the definition of hw and in the sgd algorithm can be performed efficiently. in the following we demonstrate one way to achieve these goals for the ocr task mentioned previously. to simplify the presentation let us assume that all the words in y are of length r and that the number of different letters in our alphabet is q. let y and be two structured output prediction words sequences of letters in y. we define the function y to be the average number of letters that are different in and y namely i. next let us define a class-sensitive feature mapping y. it will be convenient to think about x as a matrix of size n r where n is the number of pixels in each image and r is the number of images in the sequence. the j th column of x corresponds to the j th image in the sequence as a vector of gray level values of pixels. the dimension of the range of is set to be d n q r the first nq feature functions are type features and take the form y r xit that is we sum the value of the i th pixel only over the images for which y assigns the letter j. the triple index j indicates that we are dealing with feature j of type intuitively such features can capture pixels in the image whose gray level values are indicative of a certain letter. the second type of features take the form y r that is we sum the number of times the letter i follows the letter j. intuitively these features can capture rules like it is likely to see the pair qu in a word or it is unlikely to see the pair rz in a word. of course some of these features will not be very useful so the goal of the learning process is to assign weights to features by learning the vector w so that the weighted score will give us a good prediction via hwx argmax y y it is left to show how to solve the optimization problem in the definition of hwx efficiently as well as how to solve the optimization problem in the definition of y in the sgd algorithm. we can do this by applying a dynamic programming procedure. we describe the procedure for solving the maximization in the definition of hw and leave as an exercise the maximization problem in the definition of y in the sgd algorithm. to derive the dynamic programming procedure let us first observe that we can write y yt yt for an appropriate x rd and for simplicity we assume that is always equal to indeed each feature function can be written in terms of yt yt xit multiclass ranking and complex prediction problems while the feature function can be written in terms of yt yt therefore the prediction can be written as hwx argmax y y yt yt in the following we derive a dynamic programming procedure that solves every problem of the form given in equation the procedure will maintain a matrix m rqr such that ms max yt yt clearly the maximum of equals maxs msr. furthermore we can calculate m in a recursive manner ms max s this yields the following procedure dynamic programming for calculating hwx as given in equation input a matrix x rnr and a vector w initialize foreach s s for r foreach s set ms as in equation set is to be the that maximizes equation set yt argmaxs msr for r r set y iy output y yr ranking ranking is the problem of ordering a set of instances according to their relevance. a typical application is ordering results of a search engine according to their relevance to the query. another example is a system that monitors electronic transactions and should alert for possible fraudulent transactions. such a system should order transactions according to how suspicious they are. x n be the set of all sequences of instances from formally let x ranking x of arbitrary length. a ranking hypothesis h is a function that receives a sequence of instances x xr x and returns a permutation of it is more convenient to let the output of h be a vector y rr where by sorting the elements of y we obtain the permutation over we denote by the permutation over induced by y. for example for r the vector y induces the permutation that is if we sort y in an ascending order then we obtain the vector now is the position of yi in the sorted vector this notation reflects that the top-ranked instances are those that achieve the highest values in in the notation of our pac learning model the examples domain is z r rr and the hypothesis class h is some set of ranking hypotheses. we define x y x y for some function we next turn to describe loss functions for ranking. there are many possible ways to define such loss functions and here we list a few examples. in all the examples rr r. ranking loss y is zero if y and induce exactly the same ranking and y otherwise. that is y such a loss function is almost never used in practice as it does not distinguish between the case in which is almost equal to and the case in which is completely different from kendall-tau loss we count the number of pairs j that are in different order in the two permutations. this can be written as y rr i j yj r this loss function is more useful than the loss as it reflects the level of similarity between the two rankings. normalized discounted cumulative gain this measure emphasizes the correctness at the top of the list by using a monotonically nondecreasing discount function d n r. we first define a discounted cumulative gain measure y d yi. in words if we interpret yi as a score of the true relevance of item i then we take a weighted sum of the relevance of the elements while the weight of yi is determined on the basis of the position of i in assuming that all elements of y are nonnegative it is easy to verify that y gy y. we can therefore define a normalized discounted cumulative gain by the ratio ygy y and the corresponding loss function would be y y gy y gy y d yi. multiclass ranking and complex prediction problems we can easily see that y and that y whenever a typical way to define the discount function is by di if i k r otherwise where k r is a parameter. this means that we care more about elements that are ranked higher and we completely ignore elements that are not at the top-k ranked elements. the ndcg measure is often used to evaluate the performance of search engines since in such applications it makes sense completely to ignore elements that are not at the top of the ranking. once we have a hypothesis class and a ranking loss function we can learn a ranking function using the erm rule. however from the computational point of view the resulting optimization problem might be hard to solve. we next discuss how to learn linear predictors for ranking. linear predictors for ranking a natural way to define a ranking function is by projecting the instances onto some vector w and then outputting the resulting scalars as our representation of the ranking function. that is assuming that x rd for every w rd we define a ranking function xr as we discussed in chapter we can also apply a feature mapping that maps instances into some feature space and then takes the inner products with w in the feature space. for simplicity we focus on the simpler form as in equation given some w rd we can now define the hypothesis class hw w w. once we have defined this hypothesis class and have chosen a ranking loss function we can apply the erm rule as follows given a training set s xm ym where each xi yi is in rri for some ri n we xi yi. as in the case of binary classification for many loss functions this problem is computationally hard and we therefore turn to describe convex surrogate loss functions. we describe the surrogates for the kendall tau loss and for the ndcg loss. should search w w that minimizes the empirical loss a hinge loss for the kendall tau loss function we can think of the kendall tau loss as an average of losses for each pair. in particular for every j we can rewrite i j yj yj i j ranking i in our case bound as follows j xi it follows that we can use the hinge loss upper yj i j sign xi taking the average over the pairs we obtain the following surrogate convex loss for the kendall tau loss function x y rr signyi xi r the right-hand side is convex with respect to w and upper bounds the kendall tau loss. it is also a function with parameter maxij a hinge loss for the ndcg loss function the ndcg loss function depends on the predicted ranking vector rr via the permutation it induces. to derive a surrogate loss function we first make the following observation. let v be the set of all permutations of encoded as vectors namely each v v is a vector in such that for all i j we have vi vj. then exercise let us denote x v argmax v v vi i. vixi it follows that x argmax v v argmax v v argmax v v w vixi x on the basis of this observation we can use the generalized hinge loss for costsensitive multiclass classification as a surrogate loss function for the ndcg loss as follows x y x y x x y x x max v v max v v y the right-hand side is a convex function with respect to w. we can now solve the learning problem using sgd as described in section the main computational bottleneck is calculating a subgradient of the loss function which is equivalent to finding v that achieves the maximum in equation claim using the definition of the ndcg loss this is multiclass ranking and complex prediction problems equivalent to solving the problem argmin v v ivi i dvi where i and i yigy y. we can think of this problem a little bit differently by defining a matrix a rrr where aij j i dj i. now let us think about each j as a worker each i as a task and aij as the cost of assigning task i to worker j. with this view the problem of finding v becomes the problem of finding an assignment of the tasks to workers of minimal cost. this problem is called the assignment problem and can be solved efficiently. one particular algorithm is the hungarian method another way to solve the assignment problem is using linear programming. to do so let us first write the assignment problem as argmin b rrr aijbij s.t. i bij j i j bij bij a matrix b that satisfies the constraints in the preceding optimization problem is called a permutation matrix. this is because the constraints guarantee that there is at most a single entry of each row that equals and a single entry of each column that equals therefore the matrix b corresponds to the permutation v v defined by vi j for the single index j that satisfies bij the preceding optimization is still not a linear program because of the combinatorial constraint bij however as it turns out this constraint is redundant if we solve the optimization problem while simply omitting the combinatorial constraint then we are still guaranteed that there is an optimal solution that will satisfy this constraint. this is formalized later. denote mizing such that b is a permutation matrix. ij aijbij. then equation is the problem of minia matrix b rrr is called doubly stochastic if all elements of b are nonnegative the sum of each row of b is and the sum of each column of b is therefore solving equation without the constraints bij is the problem s.t. b is a doubly stochastic matrix. argmin b rrr bipartite ranking and multivariate performance measures the following claim states that every doubly stochastic matrix is a convex combination of permutation matrices. claim von neumann the set of doubly stochastic matrices in rrr is the convex hull of the set of permutation matrices in rrr. on the basis of the claim we easily obtain the following lemma there exists an optimal solution of equation that is also an optimal solution of equation write b proof let b be a solution of equation then by claim we can i ici where each ci is a permutation matrix each i and i i since all the ci are also doubly stochastic we clearly have that for every i. we claim that there is some i for which this must be true since otherwise if for every i we would have that a ici i i i which cannot hold. we have thus shown that some permutation matrix ci satisfies but since for every other permutation matrix c we have we conclude that ci is an optimal solution of both equation and equation bipartite ranking and multivariate performance measures in the previous section we described the problem of ranking. we used a vector y rr for representing an order over the elements xr. if all elements in y are different from each other then y specifies a full order over however if two elements of y attain the same value yi yj for i j then y can only specify a partial order over in such a case we say that xi and xj are of equal relevance according to y. in the extreme case y which means that each xi is either relevant or nonrelevant. this setting is often called bipartite ranking. for example in the fraud detection application mentioned in the previous section each transaction is labeled as either fraudulent or benign seemingly we can solve the bipartite ranking problem by learning a binary classifier applying it on each instance and putting the positive ones at the top of the ranked list. however this may lead to poor results as the goal of a binary learner is usually to minimize the zero-one loss some surrogate of it while the goal of a ranker might be significantly different. to illustrate this consider again the problem of fraud detection. usually most of the transactions are benign therefore a binary classifier that predicts benign on all transactions will have a zero-one error of while this is a very small number the resulting predictor is meaningless for the fraud detection application. the crux of the multiclass ranking and complex prediction problems problem stems from the inadequacy of the zero-one loss for what we are really interested in. a more adequate performance measure should take into account the predictions over the entire set of instances. for example in the previous section we have defined the ndcg loss which emphasizes the correctness of the top-ranked items. in this section we describe additional loss functions that are specifically adequate for bipartite ranking problems. as in the previous section we are given a sequence of instances x xr and we predict a ranking vector rr. the feedback vector is y we define a loss that depends on and y and depends on a threshold r. this threshold transforms the vector rr into the vector r usually the value of is set to be however as we will see we sometimes set while taking into account additional constraints on the problem. the loss functions we define in the following depend on the following num i bers true positives a yi false positives b yi false negatives c yi true negatives d yi i i i i the recall sensitivity of a prediction vector is the fraction of true a ac the precision is the fraction of correct a ab the specificity positives catches namely predictions among the positive labels we predict namely is the fraction of true negatives that our predictor catches namely d db note that as we decrease the recall increases the value when on the other hand the precision and the specificity usually decrease as we decrease therefore there is a tradeoff between precision and recall and we can control it by changing the loss functions defined in the following use various techniques for combining both the precision and recall. averaging sensitivity and specificity this measure is the average of the sensitivity and specificity namely this is also the accuracy on positive examples averaged with the accuracy on negative examples. here we set and the corresponding loss function is y a a ac d recall the score is the harmonic mean of the precision and recall its maximal value is obtained when both precision precision and recall are and its minimal value is obtained whenever one of them is if the other one is the score can be written using the numbers a b c as follows again we set and the loss function becomes y f it is like score but we attach times more importance to it can also be written as recall than to precision that is ac d db db precision recall bipartite ranking and multivariate performance measures f y f again we set and the loss function becomes recall at k we measure the recall while the prediction must contain at most k positive labels. that is we should set so that a b k. this is convenient for example in the application of a fraud detection system where a bank employee can only handle a small number of suspicious transactions. precision at k we measure the precision while the prediction must contain at least k positive labels. that is we should set so that a b k. the measures defined previously are often referred to as multivariate performance measures. note that these measures are highly different from the average abcd in the aforemenzero-one loss which in the preceding notation equals tioned example of fraud detection when of the examples are negatively labeled the zero-one loss of predicting that all the examples are negatives is in contrast the recall of such prediction is and hence the score is also which means that the corresponding loss will be bd linear predictors for bipartite ranking we next describe how to train linear predictors for bipartite ranking. as in the previous section a linear predictor for ranking is defined to be hw x the corresponding loss function is one of the multivariate performance measures described before. the loss function depends on hw x via the binary vector it induces which we denote by r as in the previous section to facilitate an efficient algorithm we derive a convex surrogate loss function on the derivation is similar to the derivation of the generalized hinge loss for the ndcg ranking loss as described in the previous section. our first observation is that for all the values of defined before there is some v such that can be rewritten as argmax v v i. this is clearly true for the case if we choose v the two measures for which is not taken to be are precision at k and recall at k. for precision at k we can take v to be the set v k containing all vectors in whose number of ones is at least k. for recall at k we can take v to be v k which is defined analogously. see exercise multiclass ranking and complex prediction problems once we have defined b as in equation we can easily derive a convex surrogate loss as follows. assuming that y v we have that x y x y max v v x y x y the right-hand side is a convex function with respect to w. we can now solve the learning problem using sgd as described in section the main computational bottleneck is calculating a subgradient of the loss function which is equivalent to finding v that achieves the maximum in equation claim in the following we describe how to find this maximizer efficiently for any performance measure that can be written as a function of the numbers a b c d given in equation and for which the set v contains all elements in for which the values of a b satisfy some constraints. for example for recall at k the set v is all vectors for which a b k. the idea is as follows. for any a b let yab vi yi a vi yi b any vector v v falls into yab for some a b furthermore if yab v is not empty for some a b then yab v yab. therefore we can search within each yab that has a nonempty intersection with v separately and then take the optimal value. the key observation is that once we are searching only within yab the value of is fixed so we only need to maximize the expression max v yab suppose the examples are sorted so that then it is easy to verify that we would like to set vi to be positive for the smallest indices i. doing this with the constraint on a b amounts to setting vi for the a top ranked positive examples and for the b top-ranked negative examples. this yields the following procedure. summary solving equation input xr yr w v assumptions is a function of a b c d v contains all vectors for which f b for some function f initialize p yi n yi sort examples so that r let ip be the indices of the positive examples let jn be the indices of the negative examples for a p c p a for b n such that f b d n b calculate using a b c d set vr s.t. via vjb and the rest of the elements of v equal if v set vi i output summary many real world supervised learning problems can be cast as learning a multiclass predictor. we started the chapter by introducing reductions of multiclass learning to binary learning. we then described and analyzed the family of linear predictors for multiclass learning. we have shown how this family can be used even if the number of classes is extremely large as long as we have an adequate structure on the problem. finally we have described ranking problems. in chapter we study the sample complexity of multiclass learning in more detail. bibliographic remarks the one-versus-all and all-pairs approach reductions have been unified under the framework of error correction output codes bakiri allwein schapire singer there are also other types of reductions such as tree-based classifiers for example beygelzimer langford ravikumar the limitations of reduction techniques have been studied multiclass ranking and complex prediction problems in et al. daniely sabato shwartz see also chapter in which we analyze the sample complexity of multiclass learning. direct approaches to multiclass learning with linear predictors have been studied in weston watkins crammer singer in particular the multivector construction is due to crammer singer collins has shown how to apply the perceptron algorithm for structured output problems. see also collins a related approach is discriminative learning of conditional random fields see lafferty mccallum pereira structured output svm has been studied in chapelle vapnik elisseeff sch olkopf taskar guestrin koller tsochantaridis hofmann joachims altun the dynamic procedure we have presented for calculating the prediction hwx in the structured output section is similar to the forward-backward variables calculated by the viterbi procedure in hmms for instance juang more generally solving the maximization problem in structured output is closely related to the problem of inference in graphical models for example koller friedman chapelle le smola proposed to learn a ranking function with respect to the ndcg loss using ideas from structured output learning. they also observed that the maximization problem in the definition of the generalized hinge loss is equivalent to the assignment problem. agarwal roth analyzed the sample complexity of bipartite ranking. joachims studied the applicability of structured output svm to bipartite ranking with multivariate performance measures. exercises consider a set s of examples in rn for which there exist vectors k such that every example y s falls within a ball centered at y whose radius is r assume also that for every i j i consider concatenating each instance by the constant and then applying the multivector construction namely y xn ry rk show that there exists a vector w such that y for every y s. hint observe that for every example y s we can write x y v for some r. now take w wk where wi i multiclass perceptron consider the following algorithm exercises multiclass batch perceptron input a training set ym a class-sensitive feature mapping x y rd initialize rd for t if i and y yi s.t. then wt yi y else output wt prove the following theorem assume that there exists such that for all i and for all y yi it holds that let r maxiy yi then the multiclass perceptron algorithm stops after at most iterations and when it stops it holds that i yi argmaxy dure for multiclass prediction. you can assume that y generalize the dynamic programming procedure given in section for solving the maximization problem given in the definition of h in the sgd procet yt for some arbitrary function prove that equation holds. show that the two definitions of as defined in equation and equation are indeed equivalent for all the multivariate performance measures. decision trees a decision tree is a predictor h x y that predicts the label associated with an instance x by traveling from a root node of a tree to a leaf. for simplicity we focus on the binary classification setting namely y but decision trees can be applied for other prediction problems as well. at each node on the root-to-leaf path the successor child is chosen on the basis of a splitting of the input space. usually the splitting is based on one of the features of x or on a predefined set of splitting rules. a leaf contains a specific label. an example of a decision tree for the papayas example in chapter is given in the following color? other pale green to pale yellow not-tasty softness? other gives slightly to palm pressure not-tasty tasty to check if a given papaya is tasty or not the decision tree first examines the color of the papaya. if this color is not in the range pale green to pale yellow then the tree immediately predicts that the papaya is not tasty without additional tests. otherwise the tree turns to examine the softness of the papaya. if the softness level of the papaya is such that it gives slightly to palm pressure the decision tree predicts that the papaya is tasty. otherwise the prediction is not-tasty. the preceding example underscores one of the main advantages of decision trees the resulting classifier is very simple to understand and interpret. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning sample complexity sample complexity a popular splitting rule at internal nodes of the tree is based on thresholding the value of a single feature. that is we move to the right or left child of the node on the basis of where i is the index of the relevant feature and r is the threshold. in such cases we can think of a decision tree as a splitting of the instance space x rd into cells where each leaf of the tree corresponds to one cell. it follows that a tree with k leaves can shatter a set of k instances. hence if we allow decision trees of arbitrary size we obtain a hypothesis class of infinite vc dimension. such an approach can easily lead to overfitting. to avoid overfitting we can rely on the minimum description length principle described in chapter and aim at learning a decision tree that on one hand fits the data well while on the other hand is not too large. for simplicity we will assume that x in other words each instance is a vector of d bits. in that case thresholding the value of a single feature corresponds to a splitting rule of the form for some i for instance we can model the papaya decision tree earlier by assuming that a papaya is parameterized by a two-dimensional bit vector x where the bit represents whether the color is pale green to pale yellow or not and the bit represents whether the softness is gives slightly to palm pressure or not. with this representation the node color? can be replaced with and the node softness? can be replaced with while this is a big simplification the algorithms and analysis we provide in the following can be extended to more general cases. with the aforementioned simplifying assumption the hypothesis class becomes finite but is still very large. in particular any classifier from to can be represented by a decision tree with leaves and depth of d exercise therefore the vc dimension of the class is which means that the number of examples we need to pac learn the hypothesis class grows with unless d is very small this is a huge number of examples. to overcome this obstacle we rely on the mdl scheme described in chapter the underlying prior knowledge is that we should prefer smaller trees over larger trees. to formalize this intuition we first need to define a description language for decision trees which is prefix free and requires fewer bits for smaller decision trees. here is one possible way a tree with n nodes will be described in n blocks each of size bits. the first n blocks encode the nodes of the tree in a depth-first order and the last block marks the end of the code. each block indicates whether the current node is an internal node of the form for some i a leaf whose value is a leaf whose value is end of the code decision trees overall there are d options hence we need bits to describe each block. assuming each internal node has two it is not hard to show that this is a prefix-free encoding of the tree and that the description length of a tree with n nodes is by theorem we have that with probability of at least over a sample of size m for every n and every decision tree h h with n nodes it holds that ldh lsh this bound performs a tradeoff on the one hand we expect larger more complex decision trees to have a smaller training risk lsh but the respective value of n will be larger. on the other hand smaller decision trees will have a smaller value of n but lsh might be larger. our hope prior knowledge is that we can find a decision tree with both low empirical risk lsh and a number of nodes n not too high. our bound indicates that such a tree will have low true risk ldh. decision tree algorithms the bound on ldh given in equation suggests a learning rule for decision trees search for a tree that minimizes the right-hand side of equation unfortunately it turns out that solving this problem is computationally consequently practical decision tree learning algorithms are based on heuristics such as a greedy approach where the tree is constructed gradually and locally optimal decisions are made at the construction of each node. such algorithms cannot guarantee to return the globally optimal decision tree but tend to work reasonably well in practice. a general framework for growing a decision tree is as follows. we start with a tree with a single leaf root and assign this leaf a label according to a majority vote among all labels over the training set. we now perform a series of iterations. on each iteration we examine the effect of splitting a single leaf. we define some gain measure that quantifies the improvement due to this split. then among all possible splits we either choose the one that maximizes the gain and perform it or choose not to split the leaf at all. in the following we provide a possible implementation. it is based on a popular decision tree algorithm known as for iterative dichotomizer we describe the algorithm for the case of binary features namely x we may assume this without loss of generality because if a decision node has only one child we can replace the node by its child without affecting the predictions of the decision tree. more precisely if then no algorithm can solve equation in time polynomial in n d and m. decision tree algorithms and therefore all splitting rules are of the form for some feature i we discuss the case of real valued features in section the algorithm works by recursive calls with the initial call being and returns a decision tree. in the pseudocode that follows we use a call to a procedure gains i which receives a training set s and an index i and evaluates the gain of a split of the tree according to the ith feature. we describe several gain measures in section a input training set s feature subset a if all examples in s are labeled by return a leaf if all examples in s are labeled by return a leaf if a return a leaf whose value majority of labels in s else let j argmaxi a gains i if all examples in s have the same label return a leaf whose value majority of labels in s else let be the tree returned by y s xj a let be the tree returned by y s xj a return the tree xj implementations of the gain measure different algorithms use different implementations of gains i. here we present three. we use the notation psf to denote the probability that an event holds with respect to the uniform distribution over s. train error the simplest definition of gain is the decrease in training error. formally let ca mina a. note that the training error before splitting on feature i is cpsy since we took a majority vote among labels. similarly the error after splitting on feature i is p s cp p s s s therefore we can define gain to be the difference between the two namely gains i cp s s cp s p s s decision trees information gain another popular gain measure that is used in the and algorithms of quinlan is the information gain. the information gain is the difference between the entropy of the label before and after the split and is achieved by replacing the function c in the previous expression by the entropy function ca a loga a a. gini index yet another definition of a gain which is used by the cart algorithm of breiman friedman olshen stone is the gini index ca a. both the information gain and the gini index are smooth and concave upper bounds of the train error. these properties can be advantageous in some situations for example kearns mansour pruning the algorithm described previously still suffers from a big problem the returned tree will usually be very large. such trees may have low empirical risk but their true risk will tend to be high both according to our theoretical analysis and in practice. one solution is to limit the number of iterations of leading to a tree with a bounded number of nodes. another common solution is to prune the tree after it is built hoping to reduce it to a much smaller tree but still with a similar empirical error. theoretically according to the bound in equation if we can make n much smaller without increasing lsh by much we are likely to get a decision tree with a smaller true risk. usually the pruning is performed by a bottom-up walk on the tree. each node might be replaced with one of its subtrees or with a leaf based on some bound or estimate of ldh example the bound in equation a pseudocode of a common template is given in the following. generic tree pruning procedure input function f m for the generalization error of a decision tree t based on a sample of size m tree t foreach node j in a bottom-up walk on t leaves to root find t which minimizes f m where t is any of the following the current tree after replacing node j with a leaf the current tree after replacing node j with a leaf the current tree after replacing node j with its left subtree. the current tree after replacing node j with its right subtree. the current tree. let t t random forests threshold-based splitting rules for real-valued features in the previous section we have described an algorithm for growing a decision tree assuming that the features are binary and the splitting rules are of the form we now extend this result to the case of real-valued features and threshold-based splitting rules namely such splitting rules yield decision stumps and we have studied them in chapter the basic idea is to reduce the problem to the case of binary features as follows. let xm be the instances of the training set. for each real-valued feature i sort the instances so that xmi. define a set of thresholds such that ji we use the convention and finally for each i and j we define the binary feature ji. once we have constructed these binary features we can run the procedure described in the previous section. it is easy to verify that for any decision tree with threshold-based splitting rules over the original real-valued features there exists a decision tree over the constructed binary features with the same training error and the same number of nodes. if the original number of real-valued features is d and the number of examples is m then the number of constructed binary features becomes dm. calculating the gain of each feature might therefore take operations. however using a more clever implementation the runtime can be reduced to odm logm. the idea is similar to the implementation of erm for decision stumps as described in section random forests as mentioned before the class of decision trees of arbitrary size has infinite vc dimension. we therefore restricted the size of the decision tree. another way to reduce the danger of overfitting is by constructing an ensemble of trees. in particular in the following we describe the method of random forests introduced by breiman a random forest is a classifier consisting of a collection of decision trees where each tree is constructed by applying an algorithm a on the training set s and an additional random vector where is sampled i.i.d. from some distribution. the prediction of the random forest is obtained by a majority vote over the predictions of the individual trees. to specify a particular random forest we need to define the algorithm a and the distribution over there are many ways to do this and here we describe one particular option. we generate as follows. first we take a random subsample from s with replacements namely we sample a new training set of size using the uniform distribution over s. second we construct a sequence where each it is a subset of of size k which is generated by sampling uniformly at random elements from all these random variables form the vector then decision trees the algorithm a grows a decision tree using the algorithm based on the sample where at each splitting stage of the algorithm the algorithm is restricted to choosing a feature that maximizes gain from the set it. intuitively if k is small this restriction may prevent overfitting. summary decision trees are very intuitive predictors. typically if a human programmer creates a predictor it will look like a decision tree. we have shown that the vc dimension of decision trees with k leaves is k and proposed the mdl paradigm for learning decision trees. the main problem with decision trees is that they are computationally hard to learn therefore we described several heuristic procedures for training them. bibliographic remarks many algorithms for learning decision trees as and have been derived by quinlan the cart algorithm is due to breiman et al. random forests were introduced by breiman for additional reading we refer the reader to tibshirani friedman rokach the proof of the hardness of training decision trees is given in hyafil rivest exercises show that any binary classifier h can be implemented as a decision tree of height at most d with internal nodes of the form for some i d. domain is conclude that the vc dimension of the class of decision trees over the of consider the following training set where x and y suppose we wish to use this training set in order to build a decision tree of depth for each input we are allowed to ask two questions of the form before deciding on the label. exercises suppose we run the algorithm up to depth we pick the root node and its children according to the algorithm but instead of keeping on with the recursion we stop and pick leaves according to the majority label in each subtree. assume that the subroutine used to measure the quality of each feature is based on the entropy function we measure the information gain and that if two features get the same score one of them is picked arbitrarily. show that the training error of the resulting decision tree is at least find a decision tree of depth that attains zero training error. nearest neighbor nearest neighbor algorithms are among the simplest of all machine learning algorithms. the idea is to memorize the training set and then to predict the label of any new instance on the basis of the labels of its closest neighbors in the training set. the rationale behind such a method is based on the assumption that the features that are used to describe the domain points are relevant to their labelings in a way that makes close-by points likely to have the same label. furthermore in some situations even when the training set is immense finding a nearest neighbor can be done extremely fast example when the training set is the entire web and distances are based on links. note that in contrast with the algorithmic paradigms that we have discussed so far like erm srm mdl or rlm that are determined by some hypothesis class h the nearest neighbor method figures out a label on any test point without searching for a predictor within some predefined class of functions. in this chapter we describe nearest neighbor methods for classification and regression problems. we analyze their performance for the simple case of binary classification and discuss the efficiency of implementing these methods. k nearest neighbors throughout the entire chapter we assume that our instance domain x is endowed with a metric function that is x x r is a function that returns the distance between any two elements of x for example if x rd then can be the euclidean distance let s ym be a sequence of training examples. for each x x let mx be a reordering of m according to their distance to x xi. that is for all i m x ix x for a number k the k-nn rule for binary classification is defined as follows understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning analysis figure an illustration of the decision boundaries of the rule. the points depicted are the sample points and the predicted label of any new point will be the label of the sample point in the center of the cell it belongs to. these cells are called a voronoi tessellation of the space. k-nn input a training sample s ym output for every point x x return the majority label among ix i k when k we have the rule hsx y a geometric illustration of the rule is given in figure for regression problems namely y r one can define the prediction to be the average target of the k nearest neighbors. that is hsx y ix. more generally for some function yk y the k-nn rule with respect k to is hsx y kx y it is easy to verify that we can cast the prediction by majority of labels classification or by the averaged target regression as in equation by an appropriate choice of the generality can lead to other rules for example if y r we can take a weighted average of the targets according to the distance from x hsx x ix x j y ix. analysis since the nn rules are such natural learning methods their generalization properties have been extensively studied. most previous results are asymptotic consistency results analyzing the performance of nn rules when the sample size m nearest neighbor goes to infinity and the rate of convergence depends on the underlying distribution. as we have argued in section this type of analysis is not satisfactory. one would like to learn from finite training samples and to understand the generalization performance as a function of the size of such finite training sets and clear prior assumptions on the data distribution. we therefore provide a finitesample analysis of the rule showing how the error decreases as a function of m and how it depends on properties of the distribution. we will also explain how the analysis can be generalized to k-nn rules for arbitrary values of k. in particular the analysis specifies the number of examples required to achieve a true error of where is the bayes optimal hypothesis assuming that the labeling rule is well behaved a sense we will define later. a generalization bound for the rule we now analyze the true error of the rule for binary classification with the loss namely y and y we also assume throughout the analysis that x and is the euclidean distance. we start by introducing some notation. let d be a distribution over x y. let dx denote the induced marginal distribution over x and let rd r be the conditional over the labels that is py recall that the bayes optimal rule is the hypothesis that minimizes ldh over all functions is we assume that the conditional probability function is c-lipschitz for some in other words this c namely for all x x assumption means that if two vectors are close to each other then their labels are likely to be the same. the following lemma applies the lipschitzness of the conditional probability function to upper bound the true error of the rule as a function of the expected distance between each test instance and its nearest neighbor in the training set. lemma let x and d be a distribution over x y for which the conditional probability function is a c-lipschitz function. let s ym be an i.i.d. sample and let hs be its corresponding hypothesis. let be the bayes optimal rule for then e s dm c s dmx x e formally py lim centered around x. bx bx y where bx is a ball of radius analysis proof since ldhs exy we obtain that esldhs is the probability to sample a training set s and an additional example y such that the label of is different from y. in other words we can first sample m unlabeled examples sx xm according to dx and an additional unlabeled example x dx then find to be the nearest neighbor of x in sx and finally sample y and y it follows that sx dmx dx e s e e sx dmx dx y p we next upper bound py for any two domain points x p y using and the assumption that is c-lipschitz we obtain that the probability is at most p y plugging this into equation we conclude that c e e e s x sx x finally the error of the bayes optimal classifier is e e x x combining the preceding two inequalities concludes our proof. the next step is to bound the expected distance between a random x and its closest element in s. we first need the following general probability lemma. the lemma bounds the probability weight of subsets that are not hit by a random sample as a function of the size of that sample. lemma let cr be a collection of subsets of some domain set x let s be a sequence of m points sampled i.i.d. according to some probability distribution d over x then ici s r m e pci e s dm nearest neighbor proof from the linearity of expectation we can rewrite s pci e s next for each i we have pci e s ici s p s pci e s s ici s e s s pcim e pci m. combining the preceding two equations we get pci e pci m r max i pci e pci m. finally by a standard calculus maxa ae ma me and this concludes the proof. equipped with the preceding lemmas we are now ready to state and prove the main result of this section an upper bound on the expected error of the learning rule. theorem let x and d be a distribution over x y for which the conditional probability function is a c-lipschitz function. let hs denote the result of applying the rule to a sample s dm. then e s dm c d m proof fix some for some integer t let r t d and let cr be the cover of the set x using boxes of length namely for every d there exists a set ci of the form j xj j jt an illustration for d t and the set corresponding to is given in the following. therefore p for each x in the same box we have and by combining lemma with the trivial bound r me x e x ici s d p get that e xs ci s e xs ici d otherwise d. d ci ici ci we analysis since the number of boxes is r we get that x e sx d d d m e m e combining the preceding with lemma we obtain that d finally setting m and noting that c e s d m e we conclude our proof. d m e m m the theorem implies that if we first fix the data-generating distribution and then let m go to infinity then the error of the rule converges to twice the bayes error. the analysis can be generalized to larger values of k showing that the expected error of the k-nn rule converges to times the error of the bayes classifier. this is formalized in theorem whose proof is left as a guided exercise. the curse of dimensionality the upper bound given in theorem grows with c lipschitz coefficient of and with d the euclidean dimension of the domain set x in fact it is easy to see that a necessary condition for the last term in theorem to be smaller than is that m c that is the size of the training set should increase exponentially with the dimension. the following theorem tells us that this is not just an artifact of our upper bound but for some distributions this amount of examples is indeed necessary for learning with the nn rule. theorem for any c and every learning rule l there exists a distribution over such that is c-lipschitz the bayes error of the distribution is but for sample sizes m the true error of the rule l is greater than proof fix any values of c and d. let gd c be the grid on with distance of between points on the grid. that is each point on the grid is of the form adc where ai is in c c. note that since any two distinct c is a points on this grid are at least apart any function gd c-lipschitz function. it follows that the set of all c-lipschitz functions over gd c contains the set of all binary valued functions over that domain. we can therefore invoke the no-free-lunch result to obtain a lower bound on the needed sample sizes for learning that class. the number of points on the grid is hence if m theorem implies the lower bound we are after. nearest neighbor the exponential dependence on the dimension is known as the curse of dimensionality. as we saw the rule might fail if the number of examples is smaller than therefore while the rule does not restrict itself to a predefined set of hypotheses it still relies on some prior knowledge its success depends on the assumption that the dimension and the lipschitz constant of the underlying distribution are not too high. efficient implementation nearest neighbor is a learning-by-memorization type of rule. it requires the entire training data set to be stored and at test time we need to scan the entire data set in order to find the neighbors. the time of applying the nn rule is therefore m. this leads to expensive computation at test time. when d is small several results from the field of computational geometry have proposed data structures that enable to apply the nn rule in time logm. however the space required by these data structures is roughly mod which makes these methods impractical for larger values of d. to overcome this problem it was suggested to improve the search method by allowing an approximate search. formally an r-approximate search procedure is guaranteed to retrieve a point within distance of at most r times the distance to the nearest neighbor. three popular approximate algorithms for nn are the kd-tree balltrees and locality-sensitive hashing we refer the reader for example to darrell indyk summary the k-nn rule is a very simple learning algorithm that relies on the assumption that things that look alike must be alike. we formalized this intuition using the lipschitzness of the conditional probability. we have shown that with a sufficiently large training set the risk of the is upper bounded by twice the risk of the bayes optimal rule. we have also derived a lower bound that shows the curse of dimensionality the required sample size might increase exponentially with the dimension. as a result nn is usually performed in practice after a dimensionality reduction preprocessing step. we discuss dimensionality reduction techniques later on in chapter bibliographic remarks cover hart gave the first analysis of showing that its risk converges to twice the bayes optimal error under mild conditions. following a lemma due to stone devroye gy orfi have shown that the k-nn rule exercises is consistent respect to the hypothesis class of all functions from rd to a good presentation of the analysis is given in the book of devroye et al. here we give a finite sample guarantee that explicitly underscores the prior assumption on the distribution. see section for a discussion on consistency results. finally gottlieb kontorovich krauthgamer derived another finite sample bound for nn that is more similar to vc bounds. exercises in this exercise we will prove the following theorem for the k-nn rule. theorem let x and d be a distribution over x y for which the conditional probability function is a c-lipschitz function. let hs denote the result of applying the k-nn rule to a sample s dm where k let be the bayes optimal hypothesis. then k e s c d k m prove the following lemma. lemma let cr be a collection of subsets of some domain set x let s be a sequence of m points sampled i.i.d. according to some probability distribution d over x then for every k e s dm ici sk pci pci pci p m s hints show that e s ici sk s k fix some i and suppose that k pci use chernoff s bound to show that p s s k p s e pci s use the inequality maxa ae ma me to show that for such i we have pci p s s k pcie pci me conclude the proof by using the fact that for the case k pci we clearly have pci p s s k pci m nearest neighbor we use the notation y p as a shorthand for y is a bernoulli random variable with expected value p. prove the following lemma lemma let k and let zk be independent bernoulli random variables with pzi pi. denote p zi. show that i pi and k k e p y p k p y p hints w.l.o.g. assume that p then py py p. let show that e p y p p p use chernoff s bound to show that where e k p h ha a a a. to conclude the proof of the lemma you can rely on the following inequality proving it for every p and k e k p k fix some p and show that p y p p y p. k conclude the proof of the theorem according to the following steps as in the proof of theorem six some and let cr be the cover of the set x using boxes of length for each x in the same box we have d. show that d otherwise pci e e s s max i p sxy ici sk hsx y j x j d bound the first summand using lemma to bound the second summand let us fix sx and x such that all the k neighbors of x in sx are at distance of at most d from x. w.l.o.g assume that the k nn are xk. denote pi and let p k i pi. use exercise to show that y e e p y p y p y exercises w.l.o.g. assume that p now use lemma to show that k y. p y p p p y p y show that p y p y p minp p min combine all the preceding to obtain that the second summand in equa tion is bounded c d. k m k d k m d k m use r to obtain that e s c set and use e c m d to conclude the proof. neural networks an artificial neural network is a model of computation inspired by the structure of neural networks in the brain. in simplified models of the brain it consists of a large number of basic computing devices that are connected to each other in a complex communication network through which the brain is able to carry out highly complex computations. artificial neural networks are formal computation constructs that are modeled after this computation paradigm. learning with neural networks was proposed in the century. it yields an effective learning paradigm and has recently been shown to achieve cuttingedge performance on several learning tasks. a neural network can be described as a directed graph whose nodes correspond to neurons and edges correspond to links between them. each neuron receives as input a weighted sum of the outputs of the neurons connected to its incoming edges. we focus on feedforward networks in which the underlying graph does not contain cycles. in the context of learning we can define a hypothesis class consisting of neural network predictors where all the hypotheses share the underlying graph structure of the network and differ in the weights over edges. as we will show in section every predictor over n variables that can be implemented in time t can also be expressed as a neural network predictor of size ot where the size of the network is the number of nodes in it. it follows that the family of hypothesis classes of neural networks of polynomial size can suffice for all practical learning tasks in which our goal is to learn predictors which can be implemented efficiently. furthermore in section we will show that the sample complexity of learning such hypothesis classes is also bounded in terms of the size of the network. hence it seems that this is the ultimate learning paradigm we would want to adapt in the sense that it both has a polynomial sample complexity and has the minimal approximation error among all hypothesis classes consisting of efficiently implementable predictors. the caveat is that the problem of training such hypothesis classes of neural network predictors is computationally hard. this will be formalized in section a widely used heuristic for training neural networks relies on the sgd framework we studied in chapter there we have shown that sgd is a successful learner if the loss function is convex. in neural networks the loss function is highly nonconvex. nevertheless we can still implement the sgd algorithm and understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning feedforward neural networks hope it will find a reasonable solution happens to be the case in several practical tasks. in section we describe how to implement sgd for neural networks. in particular the most complicated operation is the calculation of the gradient of the loss function with respect to the parameters of the network. we present the backpropagation algorithm that efficiently calculates the gradient. feedforward neural networks the idea behind neural networks is that many neurons can be joined together by communication links to carry out complex computations. it is common to describe the structure of a neural network as a graph whose nodes are the neurons and each edge in the graph links the output of some neuron to the input of another neuron. we will restrict our attention to feedforward network structures in which the underlying graph does not contain cycles. a feedforward neural network is described by a directed acyclic graph g e and a weight function over the edges w e r. nodes of the graph correspond to neurons. each single neuron is modeled as a simple scalar function r r. we will focus on three possible functions for the sign function signa the threshold function and the sigmoid function exp a which is a smooth approximation to the threshold function. we call the activation function of the neuron. each edge in the graph links the output of some neuron to the input of another neuron. the input of a neuron is obtained by taking a weighted sum of the outputs of all the neurons connected to it where the weighting is according to w. to simplify the description of the calculation performed by the network we further assume that the network is organized in layers. that is the set of nodes can be decomposed into a union of disjoint subsets v t such that every edge in e connects some node in vt to some node in vt for some t the bottom layer is called the input layer. it contains n neurons where n is the dimensionality of the input space. for every i the output of neuron i in is simply xi. the last neuron in is the constant neuron which always outputs we denote by vti the ith neuron of the tth layer and by otix the output of vti when the network is fed with the input vector x. therefore for i we have xi and for i n we have we now proceed with the calculation in a layer by layer manner. suppose we have calculated the outputs of the neurons at layer t. then we can calculate the outputs of the neurons at layer t as follows. fix some let denote the input to when the network is fed with the input vector x. then wvtr otrx r e neural networks and that is the input to is a weighted sum of the outputs of the neurons in vt that are connected to where weighting is according to w and the output of is simply the application of the activation function on its input. layers vt are often called hidden layers. the top layer vt is called the output layer. in simple prediction problems the output layer contains a single neuron whose output is the output of the network. we refer to t as the number of layers in the network or the depth of the network. the size of the network is the width of the network is maxt an illustration of a layered feedforward neural network of depth size and width is given in the following. note that there is a neuron in the hidden layer that has no incoming edges. this neuron will output the constant input layer hidden layer output layer constant output learning neural networks once we have specified a neural network by e w we obtain a function hve rvt any set of such functions can serve as a hypothesis class for learning. usually we define a hypothesis class of neural network predictors by fixing the graph e as well as the activation function and letting the hypothesis class be all functions of the form hve for some w e r. the triplet e is often called the architecture of the network. we denote the hypothesis class by hve w is a mapping from e to r. the expressive power of neural networks that is the parameters specifying a hypothesis in the hypothesis class are the weights over the edges of the network. we can now study the approximation error estimation error and optimization error of such hypothesis classes. in section we study the approximation error of hve by studying what type of functions hypotheses in hve can implement in terms of the size of the underlying graph. in section we study the estimation error of hve for the case of binary classification vt and is the sign function by analyzing its vc dimension. finally in section we show that it is computationally hard to learn the class hve even if the underlying graph is small and in section we present the most commonly used heuristic for training hve the expressive power of neural networks in this section we study the expressive power of neural networks namely what type of functions can be implemented using a neural network. more concretely we will fix some architecture v e and will study what functions hypotheses in hve can implement as a function of the size of v we start the discussion with studying which type of boolean functions functions from to can be implemented by hvesign. observe that for every computer in which real numbers are stored using b bits whenever we calculate a function f rn r on such a computer we in fact calculate a function g therefore studying which boolean functions can be implemented by hvesign can tell us which functions can be implemented on a computer that stores real numbers using b bits. we begin with a simple claim showing that without restricting the size of the network every boolean function can be implemented using a neural network of depth claim for every n there exists a graph e of depth such that hvesign contains all functions from to proof we construct a graph with n and let e be all possible edges between adjacent layers. now let f be some boolean function. we need to show that we can adjust the weights so that the network will implement f let uk be all vectors in on which f outputs observe that for every i and every x if x ui then n and if x ui then n. it follows that the function gix n equals if and only if x ui. it follows that we can adapt the weights between and so that for every i the neuron implements the function gix. next we observe that f is the disjunction of neural networks the functions gix and therefore can be written as f sign gix k which concludes our proof. the preceding claim shows that neural networks can implement any boolean function. however this is a very weak property as the size of the resulting network might be exponentially large. in the construction given at the proof of claim the number of nodes in the hidden layer is exponentially large. this is not an artifact of our proof as stated in the following theorem. theorem for every n let sn be the minimal integer such that there exists a graph e with sn such that the hypothesis class hvesign contains all the functions from to then sn is exponential in n. similar results hold for hve where is the sigmoid function. proof suppose that for some e we have that hvesign contains all functions from to it follows that it can shatter the set of m vectors in and hence the vc dimension of hvesign is on the other hand the vc dimension of hvesign is bounded by oe loge ov as we will show in the next section. this implies that which concludes our proof for the case of networks with the sign activation function. the proof for the sigmoid case is analogous. it is possible to derive a similar theorem for hve for any as remark long as we restrict the weights so that it is possible to express every weight using a number of bits which is bounded by a universal constant. we can even consider hypothesis classes where different neurons can employ different activation functions as long as the number of allowed activation functions is also finite. which functions can we express using a network of polynomial size? the preceding claim tells us that it is impossible to express all boolean functions using a network of polynomial size. on the positive side in the following we show that all boolean functions that can be calculated in time ot can also be expressed by a network of size ot theorem let t n n and for every n let fn be the set of functions that can be implemented using a turing machine using runtime of at most t then there exist constants b c r such that for every n there is a graph en of size at most c t b such that hvnensign contains fn. the proof of this theorem relies on the relation between the time complexity of programs and their circuit complexity for example sipser in a nutshell a boolean circuit is a type of network in which the individual neurons the expressive power of neural networks implement conjunctions disjunctions and negation of their inputs. circuit complexity measures the size of boolean circuits required to calculate functions. the relation between time complexity and circuit complexity can be seen intuitively as follows. we can model each step of the execution of a computer program as a simple operation on its memory state. therefore the neurons at each layer of the network will reflect the memory state of the computer at the corresponding time and the translation to the next layer of the network involves a simple calculation that can be carried out by the network. to relate boolean circuits to networks with the sign activation function we need to show that we can implement the operations of conjunction disjunction and negation using the sign activation function. clearly we can implement the negation operator using the sign activation function. the following lemma shows that the sign activation function can also implement conjunctions and disjunctions of its inputs. lemma suppose that a neuron v that implements the sign activation function has k incoming edges connecting it to neurons whose outputs are in then by adding one more edge linking a constant neuron to v and by adjusting the weights on the edges to v the output of v can implement the conjunction or the disjunction of its inputs. proof simply observe that if f is the conjunction function f ixi then it can be written as f sign similarly the disjunction function f ixi can be written as f sign k k xi xi so far we have discussed boolean functions. in exercise we show that neural networks are universal approximators. that is for every fixed precision parameter and every lipschitz function f it is possible to construct a network such that for every input x the network outputs a number between f and f however as in the case of boolean functions the size of the network here again cannot be polynomial in n. this is formalized in the following theorem whose proof is a direct corollary of theorem and is left as an exercise. theorem fix some for every n let sn be the minimal integer such that there exists a graph e with sn such that the hypothesis class hve with being the sigmoid function can approximate to within precision of every function f then sn is exponential in n. geometric intuition we next provide several geometric illustrations of functions f and show how to express them using a neural network with the sign activation function. neural networks let us start with a depth network namely a network with a single hidden layer. each neuron in the hidden layer implements a halfspace predictor. then the single neuron at the output layer applies a halfspace on top of the binary outputs of the neurons in the hidden layer. as we have shown before a halfspace can implement the conjunction function. therefore such networks contain all hypotheses which are an intersection of k halfspaces where k is the number of neurons in the hidden layer namely they can express all convex polytopes with k faces. an example of an intersection of halfspaces is given in the following. we have shown that a neuron in layer can implement a function that indicates whether x is in some convex polytope. by adding one more layer and letting the neuron in the output layer implement the disjunction of its inputs we get a network that computes the union of polytopes. an illustration of such a function is given in the following. the sample complexity of neural networks next we discuss the sample complexity of learning the class hve recall that the fundamental theorem of learning tells us that the sample complexity of learning a hypothesis class of binary classifiers depends on its vc dimension. therefore we focus on calculating the vc dimension of hypothesis classes of the form hve where the output layer of the graph contains a single neuron. we start with the sign activation function namely with hvesign. what is the vc dimension of this class? intuitively since we learn parameters the vc dimension should be order of this is indeed the case as formalized by the following theorem. theorem the vc dimension of hvesign is oe loge. the sample complexity of neural networks proof to simplify the notation throughout the proof let us denote the hypothesis class by h. recall the definition of the growth function hm from section this function measures maxc x where hc is the restriction of h to functions from c to we can naturally extend the definition for a set of functions from x to some finite set y by letting hc be the restriction of h to functions from c to y and keeping the definition of hm intact. our neural network is defined by a layered graph. let vt be the layers of the graph. fix some t by assigning different weights on the edges between vt and vt we obtain different functions from rvt let ht be the class of all possible such mappings from rvt then h can be written as a composition h ht in exercise we show that the growth function of a composition of hypothesis classes is bounded by the products of the growth functions of the individual classes. therefore hm ht in addition each ht can be written as a product of function classes ht htvt where each htj is all functions from layer t to that the jth neuron of layer t can implement. in exercise we bound product classes and this yields ht htim. let dti be the number of edges that are headed to the ith neuron of layer t. since the neuron is a homogenous halfspace hypothesis and the vc dimension of homogenous halfspaces is the dimension of their input we have by sauer s lemma that htim em hm dti ti dti overall we obtained that now assume that there are m shattered points. then we must have hm from which we obtain m logem the claim follows by lemma next we consider hve where is the sigmoid function. surprisingly it turns out that the vc dimension of hve is lower bounded by exercise that is the vc dimension is the number of tunable parameters squared. it is also possible to upper bound the vc dimension by ov but the proof is beyond the scope of this book. in any case since in practice neural networks we only consider networks in which the weights have a short representation as floating point numbers with bits by using the discretization trick we easily obtain that such networks have a vc dimension of oe even if we use the sigmoid activation function. the runtime of learning neural networks in the previous sections we have shown that the class of neural networks with an underlying graph of polynomial size can express all functions that can be implemented efficiently and that the sample complexity has a favorable dependence on the size of the network. in this section we turn to the analysis of the time complexity of training neural networks. we first show that it is np hard to implement the erm rule with respect to hvesign even for networks with a single hidden layer that contain just neurons in the hidden layer. theorem let k for every n let e be a layered graph with n input nodes k nodes at the hidden layer where one of them is the constant neuron and a single output node. then it is np hard to implement the erm rule with respect to hvesign. the proof relies on a reduction from the k-coloring problem and is left as exercise one way around the preceding hardness result could be that for the purpose of learning it may suffice to find a predictor h h with low empirical error not necessarily an exact erm. however it turns out that even the task of finding weights that result in close-to-minimal empirical error is computationally infeasible ben-david one may also wonder whether it may be possible to change the architecture of the network so as to circumvent the hardness result. that is maybe erm with respect to the original network structure is computationally hard but erm with respect to some other larger network may be implemented efficiently chapter for examples of such cases. another possibility is to use other activation functions as sigmoids or any other type of efficiently computable activation functions. there is a strong indication that all of such approaches are doomed to fail. indeed under some cryptographic assumption the problem of learning intersections of halfspaces is known to be hard even in the representation independent model of learning klivans sherstov this implies that under the same cryptographic assumption any hypothesis class which contains intersections of halfspaces cannot be learned efficiently. a widely used heuristic for training neural networks relies on the sgd framework we studied in chapter there we have shown that sgd is a successful learner if the loss function is convex. in neural networks the loss function is highly nonconvex. nevertheless we can still implement the sgd algorithm and sgd and backpropagation hope it will find a reasonable solution happens to be the case in several practical tasks. sgd and backpropagation the problem of finding a hypothesis in hve with a low risk amounts to the problem of tuning the weights over the edges. in this section we show how to apply a heuristic search for good weights using the sgd algorithm. throughout this section we assume that is the sigmoid function e a but the derivation holds for any differentiable scalar function. since e is a finite set we can think of the weight function as a vector w re. suppose the network has n input neurons and k output neurons and denote by hw rn rk the function calculated by the network if the weight function is defined by w. let us denote by y the loss of predicting hwx when the target is y y. for concreteness we will take to be the squared loss however similar derivation can be obtained for y every differentiable function. finally given a distribution d over the examples domain rn rk let ldw be the risk of the network namely ldw e d y recall the sgd algorithm for minimizing the risk function ldw. we repeat the pseudocode from chapter with a few modifications which are relevant to the neural network application because of the nonconvexity of the objective function. first while in chapter we initialized w to be the zero vector here we initialize w to be a randomly chosen vector with values close to zero. this is because an initialization with the zero vector will lead all hidden neurons to have the same weights the network is a full layered network. in addition the hope is that if we repeat the sgd procedure several times where each time we initialize the process with a new random vector one of the runs will lead to a good local minimum. second while a fixed step size is guaranteed to be good enough for convex problems here we utilize a variable step size t as defined in section because of the nonconvexity of the loss function the choice of the sequence t is more significant and it is tuned in practice by a trial and error manner. third we output the best performing vector on a validation set. in addition it is sometimes helpful to add regularization on the weights with parameter that is we try to minimize ldw gradient does not have a closed form solution. instead it is implemented using the backpropagation algorithm which will be described in the sequel. finally the neural networks sgd for neural networks parameters number of iterations step size sequence regularization parameter input layered graph e differentiable activation function r r initialize choose re at random a distribution s.t. is close enough to for i sample y d calculate gradient vi backpropagationx y w e update wi ivi wi output w is the best performing wi on a validation set backpropagation input example y weight vector w layered graph e activation function r r initialize denote layers of the graph vt where vt vtkt define wtij as the weight of we set wtij if e forward set x for t t for i kt set ati wt ot set oti backward set t ot y for t t t for i kt ti wtji output foreach edge vti e set the partial derivative to ti ot sgd and backpropagation explaining how backpropagation calculates the gradient we next explain how the backpropagation algorithm calculates the gradient of the loss function on an example y with respect to the vector w. let us first recall a few definitions from vector calculus. each element of the gradient is the partial derivative with respect to the variable in w corresponding to one of the edges of the network. recall the definition of a partial derivative. given a function f rn r the partial derivative with respect to the ith variable at w is obtained by fixing the values of wi wn which yields the scalar function g r r defined by ga f wi wi a wn and then taking the derivative of g at for a function with multiple outputs f rn rm the jacobian of f at w rn denoted jwf is the m n matrix whose i j element is the partial derivative of fi rn r w.r.t. its jth variable at w. note that if m then the jacobian matrix is the gradient of the function as a row vector. two examples of jacobian calculations which we will later use are as follows. let f aw for a rmn. then jwf a. for every n we use the notation to denote the function from rn to rn which applies the sigmoid function element-wise. that is means i it is easy to verify that for every i we have i i that j is a diagonal matrix whose i entry is i where is the derivative function of the sigmoid function namely i i we also use the notation diag to denote this matrix. the chain rule for taking the derivative of a composition of functions can be written in terms of the jacobian as follows. given two functions f rn rm and g rk rn we have that the jacobian of the composition function g rk rm at w is jwf g jgwf for example for gw aw where a rnk we have that jw g diag a. to describe the backpropagation algorithm let us first decompose v into the layers of the graph v t for every t let us write vt vtkt where kt in addition for every t denote wt a matrix which gives a weight to every potential edge between vt and if the edge exists in e then we set wtij to be the weight according to w of the edge otherwise we add a phantom edge and set its weight to be zero wtij since when calculating the partial derivative with respect to the weight of some edge we fix all other weights these additional phantom edges have no effect on the partial derivative with respect to existing edges. it follows that we can assume without loss of generality that all edges exist that is e tvt neural networks next we discuss how to calculate the partial derivatives with respect to the edges from vt to vt namely with respect to the elements in wt since we fix all other weights of the network it follows that the outputs of all the neurons in vt are fixed numbers which do not depend on the weights in wt denote the corresponding vector by ot in addition let us denote by rkt r the loss function of the subnetwork defined by layers vt vt as a function of the outputs of the neurons in vt. the input to the neurons of vt can be written as at wt and the output of the neurons of vt is ot that is for every j we have otj we obtain that the loss as a function of wt can be written as gtwt it would be convenient to rewrite this as follows. let wt rkt be the column vector obtained by concatenating the rows of wt and then taking the transpose of the resulting long vector. define by ot the kt matrix t t t ot then wt ot so we can also write gtwt wt therefore applying the chain rule we obtain that jwt j diag ot using our notation we have ot and at ot which yields jwt diag ot let us also denote t then we can further rewrite the preceding as t tkt t jwt it is left to calculate the vector t for every t. this is the gradient of at ot. we calculate this in a recursive manner. first observe that for the last layer we have that y where is the loss function. since we we obtain that y. in particular assume that y t jot y. next note that therefore by the chain rule j summary in particular t j diag in summary we can first calculate the vectors ot from the bottom of the network to its top. then we calculate the vectors t from the top of the network back to its bottom. once we have all of these vectors the partial derivatives are easily obtained using equation we have thus shown that the pseudocode of backpropagation indeed calculates the gradient. summary classes of all predictors that can be implemented in runtime of we neural networks over graphs of size sn can be used to describe hypothesis have also shown that their sample complexity depends polynomially on sn it depends on the number of edges in the network. therefore classes of neural network hypotheses seem to be an excellent choice. regrettably the problem of training the network on the basis of training data is computationally hard. we have presented the sgd framework as a heuristic approach for training neural networks and described the backpropagation algorithm which efficiently calculates the gradient of the loss function with respect to the weights over the edges. bibliographic remarks neural networks were extensively studied in the and early but with mixed empirical success. in recent years a combination of algorithmic advancements as well as increasing computational power and data size has led to a breakthrough in the effectiveness of neural networks. in particular deep networks networks of more than layers have shown very impressive practical performance on a variety of domains. a few examples include convolutional networks bengio restricted boltzmann machines osindero teh auto-encoders huang boureau lecun bengio lecun collobert weston lee grosse ranganath ng le ranzato monga devin corrado chen dean ng and sum-product networks shalev-shwartz shamir poon domingos see also and the references therein. the expressive power of neural networks and the relation to circuit complexity have been extensively studied in for the analysis of the sample complexity of neural networks we refer the reader to bartlet our proof technique of theorem is due to kakade and tewari lecture notes. neural networks klivans sherstov have shown that for any c intersections of nc halfspaces over are not efficiently pac learnable even if we allow representation independent learning. this hardness result relies on the cryptographic assumption that there is no polynomial time solution to the unique-shortestvector problem. as we have argued this implies that there cannot be an efficient algorithm for training neural networks even if we allow larger networks or other activation functions that can be implemented efficiently. the backpropagation algorithm has been introduced in rumelhart hinton williams exercises prove theorem neural networks are universal approximators let f be a function. fix some construct a neural network n with the sigmoid activation function such that for every x it holds that n hint similarly to the proof of theorem partition into small boxes. use the lipschitzness of f to show that it is approximately constant at each box. finally show that a neural network can first decide which box the input vector belongs to and then predict the averaged value of f at that box. hint for every f construct a function g such that if you can approximate g then you can express f growth function of product for i let fi be a set of functions from x to yi. define h to be the cartesian product class. that is for every and there exists h h such that hx prove that hm growth function of composition let be a set of functions from x to z and let be a set of functions from z to y. let h be the composition class. that is for every and there exists h h such that hx prove that hm vc of sigmoidal networks in this exercise we show that there is a graph e such that the vc dimension of the class of neural networks over these graphs with the sigmoid activation function is note that for every function the sigmoid activation function can approximate the threshold activation i xi up to accuracy to simplify the presentation throughout the exercise we assume that we can exactly implement the activation function i using a sigmoid activation function. fix some n. construct a network with on weights which implements a function from r to and satisfies the following property. for every x exercises if we feed the network with the real number xn then the output of the network will be x. hint denote xn and observe that is at least if xk and is at most if xk construct a network with on weights which implements a function from to such that ei for all i. that is upon receiving the input i the network outputs the vector of all zeros except at the i th neuron. let n be n real numbers such that every i is of the form ai j construct a network with on weights which imwith ai plements a function from to r and satisfies i for every i combine to obtain a network that receives i and output ai. construct a network that receives j and outputs ai j hint observe that the and function over can be calculated using weights. ai n conclude that there is a graph with on weights such that the vc di mension of the resulting hypothesis class is prove theorem hint the proof is similar to the hardness of learning intersections of halfspaces see exercise in chapter part iii additional learning models online learning in this chapter we describe a different model of learning which is called online learning. previously we studied the pac learning model in which the learner first receives a batch of training examples uses the training set to learn a hypothesis and only when learning is completed uses the learned hypothesis for predicting the label of new examples. in our papayas learning problem this means that we should first buy a bunch of papayas and taste them all. then we use all of this information to learn a prediction rule that determines the taste of new papayas. in contrast in online learning there is no separation between a training phase and a prediction phase. instead each time we buy a papaya it is first considered a test example since we should predict whether it is going to taste good. then after taking a bite from the papaya we know the true label and the same papaya can be used as a training example that can help us improve our prediction mechanism for future papayas. concretely online learning takes place in a sequence of consecutive rounds. on each online round the learner first receives an instance learner buys a papaya and knows its shape and color which form the instance. then the learner is required to predict a label the papaya tasty?. at the end of the round the learner obtains the correct label tastes the papaya and then knows whether it is tasty or not. finally the learner uses this information to improve his future predictions. to analyze online learning we follow a similar route to our study of pac learning. we start with online binary classification problems. we consider both the realizable case in which we assume as prior knowledge that all the labels are generated by some hypothesis from a given hypothesis class and the unrealizable case which corresponds to the agnostic pac learning model. in particular we present an important algorithm called weighted-majority. next we study online learning problems in which the loss function is convex. finally we present the perceptron algorithm as an example of the use of surrogate convex loss functions in the online learning model. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning online learning online classification in the realizable case online learning is performed in a sequence of consecutive rounds where at round t the learner is given an instance xt taken from an instance domain x and is required to provide its label. we denote the predicted label by pt. after predicting the label the correct label yt is revealed to the learner. the learner s goal is to make as few prediction mistakes as possible during this process. the learner tries to deduce information from previous rounds so as to improve its predictions on future rounds. clearly learning is hopeless if there is no correlation between past and present rounds. previously in the book we studied the pac model in which we assume that past and present examples are sampled i.i.d. from the same distribution source. in the online learning model we make no statistical assumptions regarding the origin of the sequence of examples. the sequence is allowed to be deterministic stochastic or even adversarially adaptive to the learner s own behavior in the case of spam e-mail filtering. naturally an adversary can make the number of prediction mistakes of our online learning algorithm arbitrarily large. for example the adversary can present the same instance on each online round wait for the learner s prediction and provide the opposite label as the correct label. to make nontrivial statements we must further restrict the problem. the realizability assumption is one possible natural restriction. in the realizable case we assume that all the labels are generated by some hypothesis x y. furthermore is taken from a hypothesis class h which is known to the learner. this is analogous to the pac learning model we studied in chapter with this restriction on the sequence the learner should make as few mistakes as possible assuming that both and the sequence of instances can be chosen by an adversary. for an online learning algorithm a we denote by mah the maximal number of mistakes a might make on a sequence of examples which is labeled by some h. we emphasize again that both and the sequence of instances can be chosen by an adversary. a bound on mah is called a mistake-bound and we will study how to design algorithms for which mah is minimal. formally definition bounds online learnability let h be a hypothesis class and let a be an online learning algorithm. given any sequence s where t is any integer and h let mas be the number of mistakes a makes on the sequence s. we denote by mah the supremum of mas over all sequences of the above form. a bound of the form mah b is called a mistake bound. we say that a hypothesis class h is online learnable if there exists an algorithm a for which mah b our goal is to study which hypothesis classes are learnable in the online model and in particular to find good learning algorithms for a given hypothesis class. remark throughout this section and the next we ignore the computa online classification in the realizable case tional aspect of learning and do not restrict the algorithms to be efficient. in section and section we study efficient online learning algorithms. to simplify the presentation we start with the case of a finite hypothesis class namely in pac learning we identified erm as a good learning algorithm in the sense that if h is learnable then it is learnable by the rule ermh. a natural learning rule for online learning is to use any online round any erm hypothesis namely any hypothesis which is consistent with all past examples. consistent input a finite hypothesis class h initialize h for t receive xt choose any h vt predict pt hxt receive true label yt update vt hxt yt the consistent algorithm maintains a set vt of all the hypotheses which are consistent with yt this set is often called the version space. it then picks any hypothesis from vt and predicts according to this hypothesis. obviously whenever consistent makes a prediction mistake at least one hypothesis is removed from vt. therefore after making m mistakes we have m since vt is always nonempty the realizability assumption it contains we have m rearranging we obtain the following corollary let h be a finite hypothesis class. the consistent algorithm enjoys the mistake bound mconsistenth it is rather easy to construct a hypothesis class and a sequence of examples on which consistent will indeed make mistakes exercise therefore we present a better algorithm in which we choose h vt in a smarter way. we shall see that this algorithm is guaranteed to make exponentially fewer mistakes. halving input a finite hypothesis class h initialize h for t receive xt predict pt argmaxr vt hxt r case of a tie predict pt receive true label yt update vt hxt yt online learning theorem let h be a finite hypothesis class. the halving algorithm enjoys the mistake bound mhalvingh proof we simply note that whenever the algorithm errs we have the name halving. therefore if m is the total number of mistakes we have m rearranging this inequality we conclude our proof. of course halving s mistake bound is much better than consistent s mistake bound. we already see that online learning is different from pac learning while in pac any erm hypothesis is good in online learning choosing an arbitrary erm hypothesis is far from being optimal. online learnability we next take a more general approach and aim at characterizing online learnability. in particular we target the following question what is the optimal online learning algorithm for a given hypothesis class h? we present a dimension of hypothesis classes that characterizes the best achievable mistake bound. this measure was proposed by nick littlestone and we therefore refer to it as ldimh. to motivate the definition of ldim it is convenient to view the online learning process as a game between two players the learner versus the environment. on round t of the game the environment picks an instance xt the learner predicts a label pt and finally the environment outputs the true label yt suppose that the environment wants to make the learner err on the first t rounds of the game. then it must output yt pt and the only question is how it should choose the instances xt in such a way that ensures that for some h we have yt for all t a strategy for an adversarial environment can be formally described as a binary tree as follows. each node of the tree is associated with an instance from x initially the environment presents to the learner the instance associated with the root of the tree. then if the learner predicts pt the environment will declare that this is a wrong prediction yt and will traverse to the right child of the current node. if the learner predicts pt then the environment will set yt and will traverse to the left child. this process will continue and at each round the environment will present the instance associated with the current node. formally consider a complete binary tree of depth t define the depth of the tree as the number of edges in a path from the root to a leaf. we have nodes in such a tree and we attach an instance to each node. let be these instances. we start from the root of the tree and set at round t we set xt vit where it is the current node. at the end of online classification in the realizable case figure an illustration of a shattered tree of depth the dashed path corresponds to the sequence of examples the tree is shattered by h where the predictions of each hypothesis in h on the instances is given in the table mark means that hjvi can be either or is unraveling the recursion we obtain it round t we go to the left child of it if yt or to the right child if yt that yj j. the preceding strategy for the environment succeeds only if for every yt there exists h h such that yt hxt for all t this leads to the following definition. definition shattered tree a shattered tree of depth d is a sequence of instances in x such that for every labeling yd there exists h h such that for all t we have hvit yt where it yj j. an illustration of a shattered tree of depth is given in figure definition s dimension ldimh is the maximal integer t such that there exists a shattered tree of depth t which is shattered by h. the definition of ldim and the discussion above immediately imply the fol lowing lemma no algorithm can have a mistake bound strictly smaller than ldimh namely for every algorithm a we have mah ldimh. proof let t ldimh and let be a sequence that satisfies the requirements in the definition of ldim. if the environment sets xt vit and yt pt for all t then the learner makes t mistakes while the definition of ldim implies that there exists a hypothesis h h such that yt hxt for all t. let us now give several examples. example let h be a finite hypothesis class. clearly any tree that is shattered by h has depth of at most therefore ldimh another way to conclude this inequality is by combining lemma with theorem example let x d and h hd where hjx iff online learning x j. then it is easy to show that ldimh while d can be arbitrarily large. therefore this example shows that ldimh can be significantly smaller than example let x and h a namely h is the class of thresholds on the interval then ldimh to see this consider the tree this tree is shattered by h. and because of the density of the reals this tree can be made arbitrarily deep. lemma states that ldimh lower bounds the mistake bound of any algorithm. interestingly there is a standard algorithm whose mistake bound matches this lower bound. the algorithm is similar to the halving algorithm. recall that the prediction of halving is made according to a majority vote of the hypotheses which are consistent with previous examples. we denoted this t vt set by vt. put another way halving partitions vt into two sets v hxt and v t vt hxt it then predicts according to the larger of the two groups. the rationale behind this prediction is that whenever halving makes a mistake it ends up with the optimal algorithm we present in the following uses the same idea but instead of predicting according to the larger class it predicts according to the class with larger ldim. standard optimal algorithm input a hypothesis class h initialize h for t receive xt for r let v predict pt argmaxr ldimv t vt hxt r t case of a tie predict pt receive true label yt update vt hxt yt the following lemma formally establishes the optimality of the preceding al gorithm. online classification in the realizable case lemma soa enjoys the mistake bound msoah ldimh. proof it suffices to prove that whenever the algorithm makes a prediction mistake we have ldimvt we prove this claim by assuming the contrary that is ldimvt. if this holds true then the definition of pt implies that ldimv ldimvt for both r and r but then we can construct a shaterred tree of depth ldimvt for the class vt which leads to the desired contradiction. t combining lemma and lemma we obtain corollary let h be any hypothesis class. then the standard optimal algorithm enjoys the mistake bound msoah ldimh and no other algorithm can have mah ldimh. comparison to vc dimension in the pac learning model learnability is characterized by the vc dimension of the class h. recall that the vc dimension of a class h is the maximal number d such that there are instances xd that are shattered by h. that is for any sequence of labels yd there exists a hypothesis h h that gives exactly this sequence of labels. the following theorem relates the vc dimension to the littlestone dimension. theorem for any class h vcdimh ldimh and there are classes for which strict inequality holds. furthermore the gap can be arbitrarily larger. proof we first prove that vcdimh ldimh. suppose vcdimh d and let xd be a shattered set. we now construct a complete binary tree of instances where all nodes at depth i are set to be xi see the following illustration now the definition of a shattered set clearly implies that we got a valid shattered tree of depth d and we conclude that vcdimh ldimh. to show that the gap can be arbitrarily large simply note that the class given in example has vc dimension of whereas its littlestone dimension is infinite. online learning online classification in the unrealizable case yt yt in the previous section we studied online learnability in the realizable case. we now consider the unrealizable case. similarly to the agnostic pac model we no longer assume that all labels are generated by some h but we require the learner to be competitive with the best fixed predictor from h. this is captured by the regret of the algorithm which measures how sorry the learner is in retrospect not to have followed the predictions of some hypothesis h h. formally the regret of an algorithm a relative to h when running on a sequence of t examples is defined as regretah t sup and the regret of the algorithm relative to a hypothesis class h is regretah t sup h h regretah t we restate the learner s goal as having the lowest possible regret relative to h. an interesting question is whether we can derive an algorithm with low regret meaning that regretah t grows sublinearly with the number of rounds t which implies that the difference between the error rate of the learner and the best hypothesis in h tends to zero as t goes to infinity. we first show that this is an impossible mission no algorithm can obtain a sublinear regret bound even if indeed consider h where is the function that always returns and is the function that always returns an adversary can make the number of mistakes of any online algorithm be equal to t by simply waiting for the learner s prediction and then providing the opposite label as the true label. in contrast for any sequence of true labels yt let b be the majority of labels in yt then the number of mistakes of hb is at most t therefore the regret of any online algorithm might be at least t t t which is not sublinear in t this impossibility result is attributed to cover to sidestep cover s impossibility result we must further restrict the power of the adversarial environment. we do so by allowing the learner to randomize his predictions. of course this by itself does not circumvent cover s impossibility result since in deriving this result we assumed nothing about the learner s strategy. to make the randomization meaningful we force the adversarial environment to decide on yt without knowing the random coins flipped by the learner on round t. the adversary can still know the learner s forecasting strategy and even the random coin flips of previous rounds but it does not know the actual value of the random coin flips used by the learner on round t. with this change of game we analyze the expected number of mistakes of the algorithm where the expectation is with respect to the learner s own randomization. that is if the learner outputs yt where p yt pt then the expected loss he pays online classification in the unrealizable case on round t is p yt yt yt. put another way instead of having the predictions of the learner being in we allow them to be in and interpret pt as the probability to predict the label on round t. with this assumption it is possible to derive a low regret algorithm. in partic ular we will prove the following theorem. theorem for every hypothesis class h there exists an algorithm for online classification whose predictions come from that enjoys the regret bound yt minlogh ldimh loget t h h t yt furthermore no algorithm can achieve an expected regret bound smaller than we will provide a constructive proof of the upper bound part of the preceding theorem. the proof of the lower bound part can be found in pal shalev-shwartz the proof of theorem relies on the weighted-majority algorithm for learning with expert advice. this algorithm is important by itself and we dedicate the next subsection to it. weighted-majority i wt with i and choosing the ith expert with probability wt weighted-majority is an algorithm for the problem of prediction with expert advice. in this online learning problem on round t the learner has to choose the advice of d given experts. we also allow the learner to randomize his choice by defining a distribution over the d experts that is picking a vector wt after the learner chooses an expert it receives a vector of costs vt where vti is the cost of following the advice of the ith expert. if the learner s predictions are randomized then its loss is defined to be the averaged cost namely i vti the algorithm assumes that the number of rounds t is given. in exercise we show how to get rid of this dependence using the doubling trick. i wt i online learning input number of experts d number of rounds t weighted-majority parameter logdt set wt wtzt where zt initialize for t i wt choose expert i at random according to pi wt receive costs of all experts vt pay cost update rule i wt i e vti i i i the following theorem is key for analyzing the regret bound of weighted majority. theorem assuming that t logd the weighted-majority algorithm enjoys the bound proof we have fact using the inequality e a a which holds for all a and the min i vti logd t i wt i zt e vti log i i e vti wt log zt log i wt i we obtain log log zt wt i i i vti vti wt i def b next note that b therefore taking log of the two sides of the inequality b e b we obtain the inequality b b which holds for all b and obtain log zt vti wt wt i i i i online classification in the unrealizable case summing this inequality over t we get logzt log zt next we lower bound zt for each i we can rewrite wt we get that i i e e log zt log t vti log max i t vti min vti. combining the preceding with equation and using the fact that logd we get that t vti and t e i t min i vti logd t t which can be rearranged as follows min i t vti logd t plugging the value of into the equation concludes our proof. proof of theorem equipped with the weighted-majority algorithm and theorem we are ready to prove theorem we start with the simpler case in which h is a finite class and let us write h hd. in this case we can refer to each hypothesis hi as an expert whose advice is to predict hixt and whose cost is vti yt. the prediction of the algorithm will therefore be i hixt and the loss is i wt pt yt i hixt yt wt i yt wt i wt now if yt then for all i hixt yt therefore the above equals to yt. if yt then for all i hixt yt and the above also i wt yt. all in all we have shown that i i yt furthermore for each yt wt i makes. applying theorem we obtain t vti is exactly the number of mistakes hypothesis hi online learning corollary let h be a finite hypothesis class. there exists an algorithm for online classification whose predictions come from that enjoys the regret bound yt logh t yt min h h next we consider the case of a general hypothesis class. previously we constructed an expert for each individual hypothesis. however if h is infinite this leads to a vacuous bound. the main idea is to construct a set of experts in a more sophisticated way. the challenge is how to define a set of experts that on one hand is not excessively large and on the other hand contains experts that give accurate predictions. we construct the set of experts so that for each hypothesis h h and every sequence of instances xt there exists at least one expert in the set which behaves exactly as h on these instances. for each l ldimh and each sequence il t we define an expert. the expert simulates the game between soa in the previous section and the environment on the sequence of instances xt assuming that soa makes a mistake precisely in rounds il. the expert is defined by the following algorithm. il input a hypothesis class h indices il initialize h for t t t vt hxt r v t receive xt for r let v define yt argmaxr ldim if case of a tie set yt t il predict yt yt else predict yt yt update v yt t note that each such expert can give us predictions at every round t while only observing the instances xt. our generic online learning algorithm is now an application of the weighted-majority algorithm with these experts. to analyze the algorithm we first note that the number of experts is l d it can be shown that when t ldimh the right-hand side of the equation is bounded by proof can be found in lemma online classification in the unrealizable case is at most the number of mistakes of the best expert logd t we will theorem tells us that the expected number of mistakes of weighted-majority next show that the number of mistakes of the best expert is at most the number of mistakes of the best hypothesis in h. the following key lemma shows that on any sequence of instances for each hypothesis h h there exists an expert with the same behavior. lemma let h be any hypothesis class with ldimh let xt be any sequence of instances. for any h h there exists l ldimh and indices il t such that when running il on the sequence xt the expert predicts hxt on each online round t t proof fix h h and the sequence xt we must construct l and the indices il. consider running soa on the input hxt soa makes at most ldimh mistakes on such input. we define l to be the number of mistakes made by soa and we define il to be the set of rounds in which soa made the mistakes. now consider the il running on the sequence xt by construction the set vt maintained by il equals the set vt maintained by soa when running on the sequence hxt the predictions of soa differ from the predictions of h if and only if the round is in il. since il predicts exactly like soa if t is not in il and the opposite of soas predictions if t is in il we conclude that the predictions of the expert are always the same as the predictions of h. the previous lemma holds in particular for the hypothesis in h that makes the least number of mistakes on the sequence of examples and we therefore obtain the following corollary let yt be a sequence of examples and let h be a hypothesis class with ldimh there exists l ldimh and indices il t such that il makes at most as many mistakes as the best h h does namely yt min h h mistakes on the sequence of examples. together with theorem the upper bound part of theorem is proven. online learning online convex optimization in chapter we studied convex learning problems and showed learnability results for these problems in the agnostic pac learning framework. in this section we show that similar learnability results hold for convex problems in the online learning framework. in particular we consider the following problem. online convex optimization loss function h z r definitions hypothesis class h domain z assumptions h is convex z z z is a convex function for t t learner predicts a vector wt h environment responds with zt z learner suffers loss zt as in the online classification problem we analyze the regret of the algorithm. recall that the regret of an online algorithm with respect to a competing hypothesis which here will be some vector h is defined as zt t zt. as before the regret of the algorithm relative to a set of competing vectors h is defined as regretah t sup h t in chapter we have shown that stochastic gradient descent solves convex learning problems in the agnostic pac model. we now show that a very similar algorithm online gradient descent solves online convex learning problems. online gradient descent parameter initialize for t t predict wt receive zt and let ft zt choose vt ftwt update wt argminw h wt wt vt the online perceptron algorithm theorem the online gradient descent algorithm enjoys the following regret bound for every h t if we further assume that ft is for all t then setting t t if we further assume that h is b-bounded and we set b t then regretah t b t t yields proof the analysis is similar to the analysis of stochastic gradient descent with projections. using the projection lemma the definition of wt and the definition of subgradients we have that for every t vt summing over t and observing that the left-hand side is a telescopic sum we obtain that rearranging the inequality and using the fact that we get that this proves the first bound in the theorem. the second bound follows from the assumption that ft is which implies that the online perceptron algorithm the perceptron is a classic online learning algorithm for binary classification with the hypothesis class of homogenous halfspaces namely h online learning w rd. in section we have presented the batch version of the perceptron which aims to solve the erm problem with respect to h. we now present an online version of the perceptron algorithm. let x rd y on round t the learner receives a vector xt rd. the learner maintains a weight vector wt rd and predicts pt then it receives yt y and pays if pt yt and otherwise. the goal of the learner is to make as few prediction mistakes as possible. in section we characterized the optimal algorithm and showed that the best achievable mistake bound depends on the littlestone dimension of the class. we show later that if d then ldimh which implies that we have no hope of making few prediction mistakes. indeed consider the tree for which etc. because of the density of the reals this tree is shattered by the subset of h which contains all hypotheses that are parametrized by w of the form w a for a we conclude that indeed ldimh to sidestep this impossibility result the perceptron algorithm relies on the technique of surrogate convex losses section this is also closely related to the notion of margin we studied in chapter a weight vector w makes a mistake on an example y whenever the sign of does not equal y. therefore we can write the loss function as follows y on rounds on which the algorithm makes a prediction mistake we shall use the hinge-loss as a surrogate convex loss function ftw the hinge-loss satisfies the two conditions ft is a convex function for all w ftw yt. in particular this holds for wt. on rounds on which the algorithm is correct we shall define ftw clearly ft is convex in this case as well. furthermore ftwt yt remark in section we used the same surrogate loss function for all the examples. in the online model we allow the surrogate to depend on the specific round. it can even depend on wt. our ability to use a round specific surrogate stems from the worst-case type of analysis we employ in online learning. let us now run the online gradient descent algorithm on the sequence of functions ft with the hypothesis class being all vectors in rd the projection step is vacuous. recall that the algorithm initializes and its update rule is wt vt for some vt ftwt. in our case if then ft is the zero the online perceptron algorithm function and we can take vt otherwise it is easy to verify that vt ytxt is in ftwt. we therefore obtain the update rule wt wt ytxt if otherwise denote by m the set of rounds in which yt. note that on round t the prediction of the perceptron can be rewritten as pt sign yi i mit this form implies that the predictions of the perceptron algorithm and the set m do not depend on the actual value of as long as we have therefore obtained the perceptron algorithm perceptron initialize for t t receive xt predict pt if wt ytxt else wt to analyze the perceptron we rely on the analysis of online gradient descent given in the previous section. in our case the subgradient of ft we use in the perceptron is vt yt xt. indeed the perceptron s update is wt vt and as discussed before this is equivalent to wt vt for every therefore theorem tells us that ftwt since ftwt is a surrogate for the loss we know ftwt denote r maxt then we obtain setting r and rearranging we obtain this inequality implies online learning theorem suppose that the perceptron algorithm runs on a sequence yt and let r maxt let m be the rounds on which the perceptron errs and let ftw m then for every r t t in particular if there exists such that for all t then proof the theorem follows from equation and the following claim given x b c r the inequality x b c. the last claim can be easily derived by analyzing the roots of the convex parabola qy by c. x c implies that x c b the last assumption of theorem is called separability with large margin chapter that is there exists that not only satisfies that the point xt lies on the correct side of the halfspace it also guarantees that xt is not too close to the decision boundary. more specifically the distance from xt to the decision boundary is at least and the bound becomes when the separability assumption does not hold the bound involves the term which measures how much the separability with margin requirement is violated. as a last remark we note that there can be cases in which there exists some that makes zero errors on the sequence but the perceptron will make many errors. indeed this is a direct consequence of the fact that ldimh the way we sidestep this impossibility result is by assuming more on the sequence of examples the bound in theorem will be meaningful only if the cumulative surrogate t is not excessively large. summary in this chapter we have studied the online learning model. many of the results we derived for the pac learning model have an analog in the online model. first we have shown that a combinatorial dimension the littlestone dimension characterizes online learnability. to show this we introduced the soa algorithm the realizable case and the weighted-majority algorithm the unrealizable case. we have also studied online convex optimization and have shown that online gradient descent is a successful online learner whenever the loss function is convex and lipschitz. finally we presented the online perceptron algorithm as a combination of online gradient descent and the concept of surrogate convex loss functions. bibliographic remarks bibliographic remarks the standard optimal algorithm was derived by the seminal work of littlestone a generalization to the nonrealizable case as well as other variants like margin-based littlestone s dimension were derived in et al. characterizations of online learnability beyond classification have been obtained in bartlett rakhlin tewari rakhlin sridharan tewari daniely et al. the weighted-majority algorithm is due to warmuth and the term online convex programming was introduced by zinkevich but this setting was introduced some years earlier by gordon the perceptron dates back to rosenblatt an analysis for the realizable case margin assumptions appears in minsky papert freund and schapire schapire presented an analysis for the unrealizable case with a squared-hinge-loss based on a reduction to the realizable case. a direct analysis for the unrealizable case with the hinge-loss was given by gentile for additional information we refer the reader to cesa-bianchi lugosi and shalev-shwartz exercises find a hypothesis class h and a sequence of examples on which consistent makes mistakes. find a hypothesis class h and a sequence of examples on which the mistake bound of the halving algorithm is tight. let d x d and let h j where hjx calculate mhalvingh derive lower and upper bounds on mhalvingh and prove that they are equal. the doubling trick in theorem the parameter depends on the time horizon t in this exercise we show how to get rid of this dependence by a simple trick. consider an algorithm that enjoys a regret bound of the form t but its parameters require the knowledge of t the doubling trick described in the following enables us to convert such an algorithm into an algorithm that does not need to know the time horizon. the idea is to divide the time into periods of increasing size and run the original algorithm on each period. the doubling trick input algorithm a whose parameters depend on the time horizon for m run a on the rounds t online learning show that if the regret of a on each period of rounds is at most then the total regret is at most t online-to-batch conversions in this exercise we demonstrate how a successful online learning algorithm can be used to derive a successful pac learner as well. consider a pac learning problem for binary classification parameterized by an instance domain x and a hypothesis class h. suppose that there exists an online learning algorithm a which enjoys a mistake bound mah consider running this algorithm on a sequence of t examples which are sampled i.i.d. from a distribution d over the instance space x and are labeled by some h. suppose that for every round t the prediction of the algorithm is based on a hypothesis ht x show that eldhr mah t where the expectation is over the random choice of the instances as well as a random choice of r according to the uniform distribution over hint use similar arguments to the ones appearing in the proof of theorem clustering clustering is one of the most widely used techniques for exploratory data analysis. across all disciplines from social sciences to biology to computer science people try to get a first intuition about their data by identifying meaningful groups among the data points. for example computational biologists cluster genes on the basis of similarities in their expression in different experiments retailers cluster customers on the basis of their customer profiles for the purpose of targeted marketing and astronomers cluster stars on the basis of their spacial proximity. the first point that one should clarify is naturally what is clustering? intuitively clustering is the task of grouping a set of objects such that similar objects end up in the same group and dissimilar objects are separated into different groups. clearly this description is quite imprecise and possibly ambiguous. quite surprisingly it is not at all clear how to come up with a more rigorous definition. there are several sources for this difficulty. one basic problem is that the two objectives mentioned in the earlier statement may in many cases contradict each other. mathematically speaking similarity proximity is not a transitive relation while cluster sharing is an equivalence relation and in particular it is a transitive relation. more concretely it may be the case that there is a long sequence of objects xm such that each xi is very similar to its two neighbors xi and but and xm are very dissimilar. if we wish to make sure that whenever two elements are similar they share the same cluster then we must put all of the elements of the sequence in the same cluster. however in that case we end up with dissimilar elements and xm sharing a cluster thus violating the second requirement. to illustrate this point further suppose that we would like to cluster the points in the following picture into two clusters. a clustering algorithm that emphasizes not separating close-by points the single linkage algorithm that will be described in section will cluster this input by separating it horizontally according to the two lines understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning clustering in contrast a clustering method that emphasizes not having far-away points share the same cluster the algorithm that will be described in section will cluster the same input by dividing it vertically into the righthand half and the left-hand half another basic problem is the lack of ground truth for clustering which is a common problem in unsupervised learning. so far in the book we have mainly dealt with supervised learning the problem of learning a classifier from labeled training data. the goal of supervised learning is clear we wish to learn a classifier which will predict the labels of future examples as accurately as possible. furthermore a supervised learner can estimate the success or the risk of its hypotheses using the labeled training data by computing the empirical loss. in contrast clustering is an unsupervised learning problem namely there are no labels that we try to predict. instead we wish to organize the data in some meaningful way. as a result there is no clear success evaluation procedure for clustering. in fact even on the basis of full knowledge of the underlying data distribution it is not clear what is the correct clustering for that data or how to evaluate a proposed clustering. consider for example the following set of points in and suppose we are required to cluster them into two clusters. we have two highly justifiable solutions clustering this phenomenon is not just artificial but occurs in real applications. a given set of objects can be clustered in various different meaningful ways. this may be due to having different implicit notions of distance similarity between objects for example clustering recordings of speech by the accent of the speaker versus clustering them by content clustering movie reviews by movie topic versus clustering them by the review sentiment clustering paintings by topic versus clustering them by style and so on. to summarize there may be several very different conceivable clustering solutions for a given data set. as a result there is a wide variety of clustering algorithms that on some input data will output very different clusterings. a clustering model clustering tasks can vary in terms of both the type of input they have and the type of outcome they are expected to compute. for concreteness we shall focus on the following common setup input a set of elements x and a distance function over it. that is a function d x x r that is symmetric satisfies dx x for all x x and often also satisfies the triangle inequality. alternatively the function could be a similarity function s x x that is symmetric and satisfies sx x for all x x additionally some clustering algorithms also require an input parameter k the number of required clusters. output a partition of the domain set x into subsets. that is c ck ci x and for all i j ci cj in some situations the clustering is soft namely the partition of x into the different clusters is probabilistic where the output is a function assigning to each domain point x x a vector pkx where pix px ci is the probability that x belongs to cluster ci. another possible output is a clustering dendrogram greek dendron tree gramma drawing which is a hierarchical tree of domain subsets having the singleton sets in its leaves and the full domain as its root. we shall discuss this formulation in more detail in the following. clustering in the following we survey some of the most popular clustering methods. in the last section of this chapter we return to the high level discussion of what is clustering. linkage-based clustering algorithms linkage-based clustering is probably the simplest and most straightforward paradigm of clustering. these algorithms proceed in a sequence of rounds. they start from the trivial clustering that has each data point as a single-point cluster. then repeatedly these algorithms merge the closest clusters of the previous clustering. consequently the number of clusters decreases with each such round. if kept going such algorithms would eventually result in the trivial clustering in which all of the domain points share one large cluster. two parameters then need to be determined to define such an algorithm clearly. first we have to decide how to measure define the distance between clusters and second we have to determine when to stop merging. recall that the input to a clustering algorithm is a between-points distance function d. there are many ways of extending d to a measure of distance between domain subsets clusters. the most common ways are single linkage clustering in which the between-clusters distance is defined by the minimum distance between members of the two clusters namely da b def mindx y x a y b average linkage clustering in which the distance between two clusters is defined to be the average distance between a point in one of the clusters and a point in the other namely da b def dx y x a y b max linkage clustering in which the distance between two clusters is defined as the maximum distance between their elements namely da b def maxdx y x a y b. the linkage-based clustering algorithms are agglomerative in the sense that they start from data that is completely fragmented and keep building larger and larger clusters as they proceed. without employing a stopping rule the outcome of such an algorithm can be described by a clustering dendrogram that is a tree of domain subsets having the singleton sets in its leaves and the full domain as its root. for example if the input is the elements x b c d e with the euclidean distance as depicted on the left then the resulting dendrogram is the one depicted on the right k-means and other cost minimization clusterings a e d c b b c d e c d e c e the single linkage algorithm is closely related to kruskal s algorithm for finding a minimal spanning tree on a weighted graph. indeed consider the full graph whose vertices are elements of x and the weight of an edge y is the distance dx y. each merge of two clusters performed by the single linkage algorithm corresponds to a choice of an edge in the aforementioned graph. it is also possible to show that the set of edges the single linkage algorithm chooses along its run forms a minimal spanning tree. if one wishes to turn a dendrogram into a partition of the space clustering one needs to employ a stopping criterion. common stopping criteria include fixed number of clusters fix some parameter k and stop merging clusters as soon as the number of clusters is k. distance upper bound fix some r r. stop merging as soon as all the between-clusters distances are larger than r. we can also set r to be maxdx y x y x for some in that case the stopping criterion is called scaled distance upper bound. k-means and other cost minimization clusterings another popular approach to clustering starts by defining a cost function over a parameterized set of possible clusterings and the goal of the clustering algorithm is to find a partitioning of minimal cost. under this paradigm the clustering task is turned into an optimization problem. the objective function is a function from pairs of an input d and a proposed clustering solution c ck to positive real numbers. given such an objective function which we denote by g the goal of a clustering algorithm is defined as finding for a given input d a clustering c so that gx d c is minimized. in order to reach that goal one has to apply some appropriate search algorithm. as it turns out most of the resulting optimization problems are np-hard and some are even np-hard to approximate. consequently when people talk about say k-means clustering they often refer to some particular common approximation algorithm rather than the cost function or the corresponding exact solution of the minimization problem. many common objective functions require the number of clusters k as a clustering parameter. in practice it is often up to the user of the clustering algorithm to choose the parameter k that is most suitable for the given clustering problem. in the following we describe some of the most common objective functions. the k-means objective function is one of the most popular clustering objectives. in k-means the data is partitioned into disjoint sets ck where each ci is represented by a centroid i. it is assumed that the input set x is embedded in some larger metric space d that x x and centroids are members of x the k-means objective function measures the squared distance between each point in x to the centroid of its cluster. the centroid of ci is defined to be x ci ici argmin x dx then the k-means objective is gk meansx d ck dx this can also be rewritten as gk meansx d ck min k x dx x ci x ci the k-means objective function is relevant for example in digital communication tasks where the members of x may be viewed as a collection of signals that have to be transmitted. while x may be a very large set of real valued vectors digital transmission allows transmitting of only a finite number of bits for each signal. one way to achieve good transmission under such constraints is to represent each member of x by a close member of some finite set k and replace the transmission of any x x by transmitting the index of the closest i. the k-means objective can be viewed as a measure of the distortion created by such a transmission representation scheme. the k-medoids objective function is similar to the k-means objective except that it requires the cluster centroids to be members of the input set. the objective function is defined by gk medoidx d ck min k x dx the k-median objective function is quite similar to the k-medoids objective except that the distortion between a data point and the centroid of its cluster is measured by distance rather than by the square of the distance gk medianx d ck min k x dx i. x ci x ci k-means and other cost minimization clusterings an example where such an objective makes sense is the facility location problem. consider the task of locating k fire stations in a city. one can model houses as data points and aim to place the stations so as to minimize the average distance between a house and its closest fire station. the previous examples can all be viewed as center-based objectives. the solution to such a clustering problem is determined by a set of cluster centers and the clustering assigns each instance to the center closest to it. more generally center-based objective is determined by choosing some monotonic function f r r and then defining some objective functions are not center based. for example the sum of in gf d ck min k x where x is either x or some superset of x cluster distances gsodx d ck x ci f i xy ci dx y and the mincut objective that we shall discuss in section are not centerbased objectives. the k-means algorithm the k-means objective function is quite popular in practical applications of clustering. however it turns out that finding the optimal k-means solution is often computationally infeasible problem is np-hard and even np-hard to approximate to within some constant. as an alternative the following simple iterative algorithm is often used so often that in many cases the term k-means clustering refers to the outcome of this algorithm rather than to the clustering that minimizes the k-means objective cost. we describe the algorithm with respect to the euclidean distance function dx y k-means input x rn number of clusters k initialize randomly choose initial centroids k repeat until convergence i set ci x i argminj ties in some arbitrary manner i update i x ci x lemma each iteration of the k-means algorithm does not increase the k-means objective function given in equation clustering proof to simplify the notation let us use the shorthand ck for the k-means objective namely ck min k rn it is convenient to define therefore we can rewrite the k-means objective as x ci x and note that argmin rn x ci x ci x ci ck x ct i k x ci c consider the update at iteration t of the k-means algorithm. let c be the previous partition let k be the new partition assigned at iteration t. using the definition of the objective as given in equation we clearly have that and let c c k i i gc c i in addition the definition of the new partition minimizes the i c k implies that it over all possible partitions ck. hence x ct i i x ct i i c using equation we have that the right-hand side of equation equals gc combining this with equation and equation we obtain that gc c which concludes our proof. k gc c k k while the preceding lemma tells us that the k-means objective is monotonically nonincreasing there is no guarantee on the number of iterations the k-means algorithm needs in order to reach convergence. furthermore there is no nontrivial lower bound on the gap between the value of the k-means objective of the algorithm s output and the minimum possible value of that objective function. in fact k-means might converge to a point which is not even a local minimum exercise to improve the results of k-means it is often recommended to repeat the procedure several times with different randomly chosen initial centroids we can choose the initial centroids to be random points from the data. spectral clustering spectral clustering often a convenient way to represent the relationships between points in a data set x xm is by a similarity graph each vertex represents a data point xi and every two vertices are connected by an edge whose weight is their similarity wij sxi xj where w rmm. for example we can set wij exp dxi where d is a distance function and is a parameter. the clustering problem can now be formulated as follows we want to find a partition of the graph such that the edges between different groups have low weights and the edges within a group have high weights. in the clustering objectives described previously the focus was on one side of our intuitive definition of clustering making sure that points in the same cluster are similar. we now present objectives that focus on the other requirement points separated into different clusters should be nonsimilar. graph cut given a graph represented by a similarity matrix w the simplest and most direct way to construct a partition of the graph is to solve the mincut problem which chooses a partition ck that minimizes the objective ck wrs. r cis ci for k the mincut problem can be solved efficiently. however in practice it often does not lead to satisfactory partitions. the problem is that in many cases the solution of mincut simply separates one individual vertex from the rest of the graph. of course this is not what we want to achieve in clustering as clusters should be reasonably large groups of points. several solutions to this problem have been suggested. the simplest solution is to normalize the cut and define the normalized mincut objective as follows ck wrs. r cis ci the preceding objective assumes smaller values if the clusters are not too small. unfortunately introducing this balancing makes the problem computationally hard to solve. spectral clustering is a way to relax the problem of minimizing ratiocut. graph laplacian and relaxed graph cuts the main mathematical object for spectral clustering is the graph laplacian matrix. there are several different definitions of graph laplacian in the literature and in the following we describe one particular definition. clustering dii definition graph laplacian the unnormalized graph laplacian is the m m matrix l d w where d is a diagonal matrix with wij. the matrix d is called the degree matrix. the following lemma underscores the relation between ratiocut and the lapla cian matrix. lemma let ck be a clustering and let h rmk be the matrix such that hij cj then the columns of h are orthonormal to each other and ck l h. proof let hk be the columns of h. the fact that these vectors are orthonormal is immediate from the definition. next by standard algebraic mai lhi and that for nipulations it can be shown that l h any vector v we have r vrvswrs s r rs s wrsvr applying this with v hi and noting that is nonzero only if r ci s ci or the other way around we obtain that rs i lhi wrs. r cis ci are orthonormal and such that each hij is either or unfortunately therefore to minimize ratiocut we can search for a matrix h whose columns this is an integer programming problem which we cannot solve efficiently. instead we relax the latter requirement and simply search an orthonormal matrix h rmk that minimizes l h. as we will see in the next chapter about pca the proof of theorem the solution to this problem is to set u to be the matrix whose columns are the eigenvectors corresponding to the k minimal eigenvalues of l. the resulting algorithm is called unnormalized spectral clustering. information bottleneck unnormalized spectral clustering unnormalized spectral clustering input w rmm number of clusters k initialize compute the unnormalized graph laplacian l let u rmk be the matrix whose columns are the eigenvectors of l corresponding to the k smallest eigenvalues let vm be the rows of u cluster the points vm using k-means output clusters ck of the k-means algorithm the spectral clustering algorithm starts with finding the matrix h of the k eigenvectors corresponding to the smallest eigenvalues of the graph laplacian matrix. it then represents points according to the rows of h. it is due to the properties of the graph laplacians that this change of representation is useful. in many situations this change of representation enables the simple k-means algorithm to detect the clusters seamlessly. intuitively if h is as defined in lemma then each point in the new representation is an indicator vector whose value is nonzero only on the element corresponding to the cluster it belongs to. information bottleneck the information bottleneck method is a clustering technique introduced by tishby pereira and bialek. it relies on notions from information theory. to illustrate the method consider the problem of clustering text documents where each document is represented as a bag-of-words namely each document is a vector x where n is the size of the dictionary and xi iff the word corresponding to index i appears in the document. given a set of m documents we can interpret the bag-of-words representation of the m documents as a joint probability over a random variable x indicating the identity of a document taking values in and a random variable y indicating the identity of a word in the dictionary taking values in with this interpretation the information bottleneck refers to the identity of a clustering as another random variable denoted c that takes values in k will be set by the method as well. once we have formulated x y c as random variables we can use tools from information theory to express a clustering objective. in particular the information bottleneck objective is ix c ic y min pcx where i is the mutual information between two random is a that is given a probability function p over the pairs c clustering parameter and the minimization is over all possible probabilistic assignments of points to clusters. intuitively we would like to achieve two contradictory goals. on one hand we would like the mutual information between the identity of the document and the identity of the cluster to be as small as possible. this reflects the fact that we would like a strong compression of the original data. on the other hand we would like high mutual information between the clustering variable and the identity of the words which reflects the goal that the relevant information about the document reflected by the words that appear in the document is retained. this generalizes the classical notion of minimal sufficient used in parametric statistics to arbitrary distributions. solving the optimization problem associated with the information bottleneck principle is hard in the general case. some of the proposed methods are similar to the em principle which we will discuss in chapter a high level view of clustering so far we have mainly listed various useful clustering tools. however some fundamental questions remain unaddressed. first and foremost what is clustering? what is it that distinguishes a clustering algorithm from any arbitrary function that takes an input space and outputs a partition of that space? are there any basic properties of clustering that are independent of any specific algorithm or task? one method for addressing such questions is via an axiomatic approach. there have been several attempts to provide an axiomatic definition of clustering. let us demonstrate this approach by presenting the attempt made by kleinberg consider a clustering function f that takes as input any finite domain x with a dissimilarity function d over its pairs and returns a partition of x consider the following three properties of such a function scale invariance for any domain set x dissimilarity function d and any the following should hold f d f d dx y def dx y. richness for any finite x and every partition c ck of x nonempty subsets there exists some dissimilarity function d over x such that f d c. ix c where the sum is over all values x can take and all pab a b pa b log papb values c can take. a sufficient statistic is a function of the data which has the property of sufficiency with respect to a statistical model and its associated unknown parameter meaning that no other statistic which can be calculated from the same sample provides any additional information as to the value of the parameter. for example if we assume that a variable is distributed normally with a unit variance and an unknown expectation then the average function is a sufficient statistic. a high level view of clustering consistency if d and are dissimilarity functions over x such that for every x y x if x y belong to the same cluster in f d then y dx y and if x y belong to different clusters in f d then y dx y then f d f a moment of reflection reveals that the scale invariance is a very natural requirement it would be odd to have the result of a clustering function depend on the units used to measure between-point distances. the richness requirement basically states that the outcome of the clustering function is fully controlled by the function d which is also a very intuitive feature. the third requirement consistency is the only requirement that refers to the basic definition of clustering we wish that similar points will be clustered together and that dissimilar points will be separated to different clusters and therefore if points that already share a cluster become more similar and points that are already separated become even less similar to each other the clustering function should have even stronger support of its previous clustering decisions. however kleinberg has shown the following impossibility result theorem there exists no function f that satisfies all the three properties scale invariance richness and consistency. proof assume by way of contradiction that some f does satisfy all three properties. pick some domain set x with at least three points. by richness there must be some such that f x x and there also exists some such that f f let r be such that for every x y x y y. let consider f by the scale invariance property of f we should have f f on the other hand since all distinct x y x reside in different clusters w.r.t. f and y y the consistency of f implies that f f this is a contradiction since we chose so that f f it is important to note that there is no single bad property among the three properties. for every pair of the the three axioms there exist natural clustering functions that satisfy the two properties in that pair can even construct such examples just by varying the stopping criteria for the single linkage clustering function. on the other hand kleinberg shows that any clustering algorithm that minimizes any center-based objective function inevitably fails the consistency property the k-sum-of-in-cluster-distances minimization clustering does satisfy consistency. the kleinberg impossibility result can be easily circumvented by varying the properties. for example if one wishes to discuss clustering functions that have a fixed number-of-clusters parameter then it is natural to replace richness by k-richness the requirement that every partition of the domain into k subsets is attainable by the clustering function. k-richness scale invariance and consistency all hold for the k-means clustering and are therefore consistent. clustering j or alternatively one can relax the consistency property. for example say that two clusterings c ck and l are compatible if for every clusters ci c and j either ci j ci or ci j is worthwhile noting that for every dendrogram every two clusterings that are obtained by trimming that dendrogram are compatible. refinement consistency is the requirement that under the assumptions of the consistency property the new clustering f is compatible with the old clustering f d. many common clustering functions satisfy this requirement as well as scale invariance and richness. furthermore one can come up with many other different properties of clustering functions that sound intuitive and desirable and are satisfied by some common clustering functions. there are many ways to interpret these results. we suggest to view it as indicating that there is no ideal clustering function. every clustering function will inevitably have some undesirable properties. the choice of a clustering function for any given task must therefore take into account the specific properties of that task. there is no generic clustering solution just as there is no classification algorithm that will learn every learnable task the no-free-lunch theorem shows. clustering just like classification prediction must take into account some prior knowledge about the specific task at hand. summary clustering is an unsupervised learning problem in which we wish to partition a set of points into meaningful subsets. we presented several clustering approaches including linkage-based algorithms the k-means family spectral clustering and the information bottleneck. we discussed the difficulty of formalizing the intuitive meaning of clustering. bibliographic remarks the k-means algorithm is sometimes named lloyd s algorithm after stuart lloyd who proposed the method in for a more complete overview of spectral clustering we refer the reader to the excellent tutorial by von luxburg the information bottleneck method was introduced by tishby pereira bialek for an additional discussion on the axiomatic approach see ackerman ben-david exercises suboptimality of k-means for every parameter t show that there exists an instance of the k-means problem for which the k-means algorithm exercises find a solution whose k-means objective is at least t opt where opt is the minimum k-means objective. k-means might not necessarily converge to a local minimum show that the k-means algorithm might converge to a point which is not a local minimum. hint suppose that k and the sample points are r suppose we initialize the k-means with the centers and suppose we break ties in the definition of ci by assigning i to be the smallest value in argminj given a metric space d where and k n we would like to find a partition of x into ck which minimizes the expression gk diamx d ck max j diamcj where diamcj cj dx use the convention diamcj if similarly to the k-means objective it is np-hard to minimize the kdiam objective. fortunately we have a very simple approximation algorithm initially we pick some x x and set x. then the algorithm iteratively sets j k j argmax x x min i dx i. finally we set i ci x i argmin j dx j. prove that the algorithm described is a algorithm. that is if we denote its output by ck and denote the optimal solution by c c k then gk diamx d ck gk diamx d c k hint consider the point other words the next center we would have chosen if we wanted k clusters. let r minj d j prove the following inequalities gk diamx d ck k r. gk diamx d c recall that a clustering function f is called center-based clustering if for some monotonic function f r r on every given input d f d is a clustering that minimizes the objective gf d ck min k x where x is either x or some superset of x f i x ci clustering prove that for every k the k-diam clustering function defined in the previous exercise is not a center-based clustering function. hint given a clustering input d with consider the effect of adding many close-by points to some not all of the members of x on either the k-diam clustering or any given center-based clustering. recall that we discussed three clustering properties scale invariance rich ness and consistency. consider the single linkage clustering algorithm. find which of the three properties is satisfied by single linkage with the fixed number of clusters fixed nonzero number stopping rule. find which of the three properties is satisfied by single linkage with the distance upper bound fixed nonzero upper bound stopping rule. show that for any pair of these properties there exists a stopping criterion for single linkage clustering under which these two axioms are satisfied. given some number k let k-richness be the following requirement for any finite x and every partition c ck of x nonempty subsets there exists some dissimilarity function d over x such that f d c. prove that for every number k there exists a clustering function that satisfies the three properties scale invariance k-richness and consistency. dimensionality reduction dimensionality reduction is the process of taking data in a high dimensional space and mapping it into a new space whose dimensionality is much smaller. this process is closely related to the concept of compression in information theory. there are several reasons to reduce the dimensionality of the data. first high dimensional data impose computational challenges. moreover in some situations high dimensionality might lead to poor generalization abilities of the learning algorithm example in nearest neighbor classifiers the sample complexity increases exponentially with the dimension see chapter finally dimensionality reduction can be used for interpretability of the data for finding meaningful structure of the data and for illustration purposes. in this chapter we describe popular methods for dimensionality reduction. in those methods the reduction is performed by applying a linear transformation to the original data. that is if the original data is in rd and we want to embed it into rn d then we would like to find a matrix w rnd that induces the mapping x w x. a natural criterion for choosing w is in a way that will enable a reasonable recovery of the original x. it is not hard to show that in general exact recovery of x from w x is impossible exercise the first method we describe is called principal component analysis in pca both the compression and the recovery are performed by linear transformations and the method finds the linear transformations for which the differences between the recovered vectors and the original vectors are minimal in the least squared sense. next we describe dimensionality reduction using random matrices w we derive an important lemma often called the johnson-lindenstrauss lemma which analyzes the distortion caused by such a random dimensionality reduction technique. last we show how one can reduce the dimension of all sparse vectors using again a random matrix. this process is known as compressed sensing. in this case the recovery process is nonlinear but can still be implemented efficiently using linear programming. we conclude by underscoring the underlying prior assumptions behind pca and compressed sensing which can help us understand the merits and pitfalls of the two methods. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning dimensionality reduction principal component analysis let xm be m vectors in rd. we would like to reduce the dimensionality of these vectors using a linear transformation. a matrix w rnd where n d induces a mapping x w x where w x rn is the lower dimensionality representation of x. then a second matrix u rdn can be used to recover each original vector x from its compressed version. that is for a compressed vector y w x where y is in the low dimensional space rn we can construct x u y so that x is the recovered version of x and resides in the original high dimensional space rd. in pca we find the compression matrix w and the recovering matrix u so that the total squared distance between the original and recovered vectors is minimal namely we aim at solving the problem argmin w rndu rdn u w to solve this problem we first show that the optimal solution takes a specific form. lemma let w be a solution to equation then the columns of u are orthonormal is the identity matrix of rn and w proof fix any u w and consider the mapping x u w x. the range of this mapping r w x x rd is an n dimensional linear subspace of rd. let v rdn be a matrix whose columns form an orthonormal basis of this subspace namely the range of v is r and v i. therefore each vector in r can be written as v y where y rn. for every x rd and y rn we have v y where we used the fact that v is the identity matrix of rn. minimizing the preceding expression with respect to y by comparing the gradient with respect to y to zero gives that y v therefore for each x we have that v v argmin x r in particular this holds for xm and therefore we can replace u w by v v and by that do not increase the objective u w v v since this holds for every u w the proof of the lemma follows. on the basis of the preceding lemma we can rewrite the optimization problem given in equation as follows argmin u u principal component analysis we further simplify the optimization problem by using the following elementary algebraic manipulations. for every x rd and a matrix u rdn such that i we have uu where the trace of a matrix is the sum of its diagonal entries. since the trace is a linear operator this allows us to rewrite equation as follows i u let a argmax u trace i the matrix a is symmetric and therefore it can be written using its spectral decomposition as a vdv where d is diagonal and v vv i. here the elements on the diagonal of d are the eigenvalues of a and the columns of v are the corresponding eigenvectors. we assume without loss of generality that ddd. since a is positive semidefinite it also holds that ddd we claim that the solution to equation is the matrix u whose columns are the n eigenvectors of a corresponding to the largest n eigenvalues. theorem let xm be arbitrary vectors in rd let a i and let un be n eigenvectors of the matrix a corresponding to the largest n eigenvalues of a. then the solution to the pca optimization problem given in equation is to set u to be the matrix whose columns are un and to set w proof let vdv be the spectral decomposition of a. fix some matrix u rdn with orthonormal columns and let b v then vb vv u it follows that and therefore also orthonormal which implies addition b i. then for every j we note that i. therefore the columns of b are ji n. in addition let b rdd be a matrix such that its first n columns are the columns of b and in ji which implies that ji it follows that djj ji. max n djj j dimensionality reduction thonormal columns it holds that it is not hard to verify exercise that the right-hand side equals to djj. we have therefore shown that for every matrix u rdn with orwe obtain that djj. on the other hand if we set u to be the matrix whose columns are the n leading eigenvectors of a objective of equation and noting objective value of equation remark the proof of theorem also tells us that the value of the dii. combining this with equation dii we obtain that the optimal djj and this concludes our proof. dii. tracea remark it is a common practice to center the examples before applying pca. that is we first calculate xi and then apply pca on the vectors this is also related to the interpretation of pca m as variance maximization exercise a more efficient solution for the case d m in some situations the original dimensionality of the data is much larger than the number of examples m. the computational complexity of calculating the pca solution as described previously is calculating eigenvalues of a plus constructing the matrix a. we now show a simple trick that enables us to calculate the pca solution more efficiently when d m. recall that the matrix a is defined to i it is convenient to rewrite a where x rmd is a matrix whose ith row is i consider the matrix b that is b rmm is the matrix whose i j element equals suppose that u is an eigenvector of b that is bu u for some r. multiplying the equality by and using the definition of b we obtain but using the definition of a we get that thus is an eigenvector of a with eigenvalue of we can therefore calculate the pca solution by calculating the eigenvalues of b instead of a. the complexity is calculating eigenvalues of b and constructing the matrix b. remark the previous discussion also implies that to calculate the pca solution we only need to know how to calculate inner products between vectors. this enables us to calculate pca implicitly even when d is very large even infinite using kernels which yields the kernel pca algorithm. implementation and demonstration a pseudocode of pca is given in the following. principal component analysis figure a set of vectors in x s and their reconstruction after dimensionality reduction to using pca circles. pca input a matrix of m examples x rmd number of components n if d a let un be the eigenvectors of a with largest eigenvalues else b let vn be the eigenvectors of b with largest eigenvalues for i n set ui output un to illustrate how pca works let us generate vectors in that approximately reside on a line namely on a one dimensional subspace of for example suppose that each example is of the form x y where x is chosen uniformly at random from and y is sampled from a gaussian distribution with mean and standard deviation of suppose we apply pca on this data. then the eigenvector corresponding to the largest eigenvalue will be close to the vector when projecting a point x y on this principal component we will obtain the scalar the reconstruction of the original vector will be in figure we depict the original versus reconstructed data. next we demonstrate the effectiveness of pca on a data set of faces. we extracted images of faces from the yale data set belhumeur kriegman each image contains pixels therefore the original dimensionality is very high. dimensionality reduction o o oo o o o x x x x xx x figure images of faces extracted from the yale data set. top-left the original images in top-right the images after dimensionality reduction to and reconstruction. middle row an enlarged version of one of the images before and after pca. bottom the images after dimensionality reduction to the different marks indicate different individuals. some images of faces are depicted on the top-left side of figure using pca we reduced the dimensionality to and reconstructed back to the original dimension which is the resulting reconstructed images are depicted on the top-right side of figure finally on the bottom of figure we depict a dimensional representation of the images. as can be seen even from a dimensional representation of the images we can still roughly separate different individuals. random projections random projections in this section we show that reducing the dimension by using a random linear transformation leads to a simple compression scheme with a surprisingly low distortion. the transformation x w x when w is a random matrix is often referred to as a random projection. in particular we provide a variant of a famous lemma due to johnson and lindenstrauss showing that random projections do not distort euclidean distances too much. let be two vectors in rd. a matrix w does not distort too much the distance between and if the ratio w is close to in other words the distances between and before and after the transformation are almost the same. to show that w is not too far away from it suffices to show that w does not distort the norm of the difference vector x therefore from now on we focus on the ratio we start with analyzing the distortion caused by applying a random projection to a single vector. lemma fix some x rd. let w rnd be a random matrix such that each wij is an independent normal random variable. then for every we have p nw e proof without loss of generality we can assume that therefore an equivalent inequality is with zero mean and variance able let wi be the ith row of w the random variable is a weighted sum of d independent normal random variables and therefore it is normally distributed j therefore the random varin distribution. the claim now follows directly from a measure concentration property of random variables stated in lemma given in section has a j the johnson-lindenstrauss lemma follows from this using a simple union bound argument. lemma lemma let q be a finite set of vectors in rd. let and n be an integer such that n dimensionality reduction then with probability of at least over a choice of a random matrix w rnd such that each element of w is distributed normally with zero mean and variance of we have n p sup x q sup x q proof combining lemma and the union bound we have that for every e let denote the right-hand side of the inequality thus we obtain that interestingly the bound given in lemma does not depend on the original dimension of x. in fact the bound holds even if x is in an infinite dimensional hilbert space. compressed sensing compressed sensing is a dimensionality reduction technique which utilizes a prior assumption that the original vector is sparse in some basis. to motivate compressed sensing consider a vector x rd that has at most s nonzero elements. that is def xi s. clearly we can compress x by representing it using s pairs. furthermore this compression is lossless we can reconstruct x exactly from the s pairs. now lets take one step forward and assume that x u where is a sparse vector s and u is a fixed orthonormal matrix. that is x has a sparse representation in another basis. it turns out that many natural vectors are least approximately sparse in some representation. in fact this assumption underlies many modern compression schemes. for example the format for image compression relies on the fact that natural images are approximately sparse in a wavelet basis. can we still compress x into roughly s numbers? well one simple way to do this is to multiply x by which yields the sparse vector and then represent by its s pairs. however this requires us first to sense x to store it and then to multiply it by this raises a very natural question why go to so much effort to acquire all the data when most of what we get will be thrown away? cannot we just directly measure the part that will not end up being thrown away? compressed sensing compressed sensing is a technique that simultaneously acquires and compresses the data. the key result is that a random linear transformation can compress x without losing information. the number of measurements needed is order of s logd. that is we roughly acquire only the important information about the signal. as we will see later the price we pay is a slower reconstruction phase. in some situations it makes sense to save time in compression even at the price of a slower reconstruction. for example a security camera should sense and compress a large amount of images while most of the time we do not need to decode the compressed data at all. furthermore in many practical applications compression by a linear transformation is advantageous because it can be performed efficiently in hardware. for example a team led by baraniuk and kelly has proposed a camera architecture that employs a digital micromirror array to perform optical calculations of a linear transformation of an image. in this case obtaining each compressed measurement is as easy as obtaining a single raw measurement. another important application of compressed sensing is medical imaging in which requiring fewer measurements translates to less radiation for the patient. informally the main premise of compressed sensing is the following three sur prising results it is possible to reconstruct any sparse signal fully if it was compressed by x w x where w is a matrix which satisfies a condition called the restricted isoperimetric property a matrix that satisfies this property is guaranteed to have a low distortion of the norm of any sparse representable vector. the reconstruction can be calculated in polynomial time by solving a linear program. a random n d matrix is likely to satisfy the rip condition provided that n is greater than an order of s logd. formally definition a matrix w rnd is s-rip if for all x s.t. s we have the first theorem establishes that rip matrices yield a lossless compression scheme for sparse vectors. it also provides a reconstruction scheme. theorem let and let w be a matrix. let x be a vector s.t. s let y w x be the compression of x and let x argmin vw vy be a reconstructed vector. then x x. dimensionality reduction proof we assume by way of contradiction that x x. since x satisfies the constraints in the optimization problem that defines x we clearly have that s. therefore and we can apply the rip inequality on the vector x x. but since w x we get that which leads to a contradiction. the reconstruction scheme given in theorem seems to be nonefficient because we need to minimize a combinatorial objective sparsity of v. quite surprisingly it turns out that we can replace the combinatorial objective with a convex objective which leads to a linear programming problem that can be solved efficiently. this is stated formally in the following theorem. theorem assume that the conditions of theorem holds and that then x argmin vw vy argmin vw vy in fact we will prove a stronger result which holds even if x is not a sparse vector. theorem let arbitrary vector and denote and let w be a matrix. let x be an xs argmin s that is xs is the vector which equals x on the s largest elements of x and equals elsewhere. let y w x be the compression of x and let argmin vw vy be the reconstructed vector. then s where note that in the special case that x xs we get an exact recovery x so theorem is a special case of theorem the proof of theorem is given in section finally the third result tells us that random matrices with n logd are likely to be rip. in fact the theorem shows that multiplying a random matrix by an orthonormal matrix also provides an rip matrix. this is important for compressing signals of the form x u where x is not sparse but is sparse. in that case if w is a random matrix and we compress using y w x then this is the same as compressing by y u and since w u is also rip we can reconstruct thus also x from y. compressed sensing theorem let u be an arbitrary fixed d d orthonormal matrix let be scalars in let s be an integer in and let n be an integer that satisfies n s let w rnd be a matrix s.t. each element of w is distributed normally with zero mean and variance of then with proabability of at least over the choice of w the matrix w u is s-rip. proofs proof of theorem we follow a proof due to candes let h x. given a vector v and a set of indices i we denote by vi the vector whose ith element is vi if i i and otherwise. the first trick we use is to partition the set of indices d into disjoint sets of size s. that is we will write tds where for all i s and we assume for simplicity that ds is an integer. we define the partition as follows. in we put the s indices corresponding to the s largest elements in absolute values of x are broken arbitrarily. let t c next will be the s indices corresponding to the s largest elements in absolute next will correspond to value of ht c the s largest elements in absolute value of ht c and we will construct in the same way. let and t c to prove the theorem we first need the following lemma which shows that rip also implies approximate orthogonality. lemma let w be an matrix. then for any two disjoint sets i j both of size at most s and for any vector u we have that ui w proof w.l.o.g. assume ui w ui w ui w but since i we get from the rip condition that ui w and that ui w which concludes our proof. we are now ready to prove the theorem. clearly c to prove the theorem we will show the following two claims ht c claim c claim s dimensionality reduction combining these two claims with equation we get that c s s and this will conclude our proof. proving claim to prove this claim we do not use the rip condition at all but only use the fact that minimizes the norm. take j for each i tj and tj we have that therefore thus s j summing this over j and using the triangle inequality we obtain that c s c next we show that c cannot be large. indeed from the definition of we have that thus using the triangle inequality we obtain that c c i i t c and since c we get that c c combining this with equation we get that s c c c which concludes the proof of claim proving claim for the second claim we use the rip condition to get that since w w h j w htj w j j j w htj we have that w w from the rip condition on inner products we obtain that for all i and j we have hti w compressed sensing since we therefore get that j combining this with equation and equation we obtain c s c rearranging the inequality gives finally using equation we get that s c but since this implies s c s c which concludes the proof of the second claim. proof of theorem to prove the theorem we follow an approach due to davenport devore wakin the idea is to combine the johnson-lindenstrauss lemma with a simple covering argument. we start with a covering property of the unit ball. lemma let there exists a finite set q rd of size such that sup min v q proof let k be an integer and let rd j i k k k s.t. xj i k. clearly we shall set q where is the unit ball of rd. since the points in are distributed evenly on the unit ball the size of q is the size of times the ratio between the volumes of the unit and balls. the volume of the ball is and the volume of is for simplicity assume that d is even and therefore e dimensionality reduction where in the last inequality we used stirling s approximation. overall we obtained that d. now lets specify k. for each x let v q be the vector whose ith element is then for each element we have that vi and thus d k to ensure that the right-hand side will be at most we shall set k plugging this value into equation we conclude that let x be a vector that can be written as x u with u being some orthonormal matrix and s. combining the earlier covering property and the jl lemma enables us to show that a random w will not distort any such x. lemma let u be an orthonormal d d matrix and let i be a set of indices of size s. let s be the span of i i where ui is the ith column of u let and n n such that n s then with probability of at least over a choice of a random matrix w rnd such that each element of w is independently distributed according to n we have sup x s it suffices to prove the lemma for all x s with we can write proof x ui where rs and ui is the matrix whose columns are i i. using lemma we know that there exists a set q of size such that sup min v q but since u is orthogonal we also have that sup min v q ui applying lemma on the set v v q we obtain that for n satisfying compressed sensing the condition given in the lemma the following holds with probability of at least sup v q this also implies that ui ui sup v q let a be the smallest number such that x s a. clearly a our goal is to show that a this follows from the fact that for any x s of unit norm there exists v q such that ui and therefore ui ui thus x s but the definition of a implies that a a this proves that for all x s we have this as well since the other side follows from ui ui the preceding lemma tells us that for x s of unit norm we have which implies that the proof of theorem follows from this by a union bound over all choices of i. dimensionality reduction pca or compressed sensing? suppose we would like to apply a dimensionality reduction technique to a given set of examples. which method should we use pca or compressed sensing? in this section we tackle this question by underscoring the underlying assumptions behind the two methods. it is helpful first to understand when each of the methods can guarantee perfect recovery. pca guarantees perfect recovery whenever the set of examples is contained in an n dimensional subspace of rd. compressed sensing guarantees perfect recovery whenever the set of examples is sparse some basis. on the basis of these observations we can describe cases in which pca will be better than compressed sensing and vice versa. as a first example suppose that the examples are the vectors of the standard basis of rd namely ed where each ei is the all zeros vector except in the ith coordinate. in this case the examples are hence compressed sensing will yield a perfect recovery whenever n on the other hand pca will lead to poor performance since the data is far from being in an n dimensional subspace as long as n d. indeed it is easy ro verify that in such a case the averaged recovery error of pca the objective of equation divided by m will be nd which is larger than whenever n we next show a case where pca is better than compressed sensing. consider m examples that are exactly on an n dimensional subspace. clearly in such a case pca will lead to perfect recovery. as to compressed sensing note that the examples are n-sparse in any orthonormal basis whose first n vectors span the subspace. therefore compressed sensing would also work if we will reduce the dimension to logd. however with exactly n dimensions compressed sensing might fail. pca has also better resilience to certain types of noise. see weiss freeman for a discussion. summary we introduced two methods for dimensionality reduction using linear transformations pca and random projections. we have shown that pca is optimal in the sense of averaged squared reconstruction error if we restrict the reconstruction procedure to be linear as well. however if we allow nonlinear reconstruction pca is not necessarily the optimal procedure. in particular for sparse data random projections can significantly outperform pca. this fact is at the heart of the compressed sensing method. bibliographic remarks bibliographic remarks pca is equivalent to best subspace approximation using singular value decomposition the svd method is described in appendix c. svd dates back to eugenio beltrami and camille jordan it has been rediscovered many times. in the statistical literature it was introduced by pearson besides pca and svd there are additional names that refer to the same idea and are being used in different scientific communities. a few examples are the eckartyoung theorem carl eckart and gale young who analyzed the method in the schmidt-mirsky theorem factor analysis and the hotelling transform. compressed sensing was introduced in donoho and in tao see also candes exercises in this exercise we show that in the general case exact recovery of a linear compression scheme is impossible. let a rnd be an arbitrary compression matrix where n d show that there exists u v rn u v such that au av. conclude that exact recovery of a linear compression scheme is impossible. let rd such that d show that max n j j j. hint take every vector such that n. let i be the minimal index for which i if i n we are done. otherwise show that we can increase i while possibly decreasing j for some j i and obtain a better solution. this will imply that the optimal solution is to set i for i n and i for i n. kernel pca in this exercise we show how pca can be used for constructing nonlinear dimensionality reduction on the basis of the kernel trick chapter let x be some instance space and let s xm be a set of points in x consider a feature mapping x v where v is some hilbert space of infinite dimension. let k x x be a kernel function that is kx kernel pca is the process of mapping the elements in s into v using and then applying pca over into rn. the output of this process is the set of reduced elements. show how this process can be done in polynomial time in terms of m and n assuming that each evaluation of k can be calculated in a constant time. in particular if your implementation requires multiplication of two matrices a and b verify that their product can be computed. similarly dimensionality reduction if an eigenvalue decomposition of some matrix c is required verify that this decomposition can be computed. an interpretation of pca as variance maximization let xm be m vectors in rd and let x be a random vector distributed according to the uniform distribution over xm. assume that ex consider the problem of finding a unit vector w rd such that the random variable has maximal variance. that is we would like to solve the problem argmax argmax m show that the solution of the problem is to set w to be the first principle vector of xm. let be the first principal component as in the previous question. now suppose we would like to find a second unit vector rd that maximizes the variance of but is also uncorrelated to that is we would like to solve argmax show that the solution to this problem is to set w to be the second principal component of xm. hint note that aw where a i constraint is equivalent to the constraint i since w is an eigenvector of a we have that the the relation between svd and pca use the svd theorem lary for providing an alternative proof of theorem random projections preserve inner products the johnson-lindenstrauss lemma tells us that a random projection preserves distances between a finite set of vectors. in this exercise you need to prove that if the set of vectors are within the unit ball then not only are the distances between any two vectors preserved but the inner product is also preserved. let q be a finite set of vectors in rd and assume that for every x q we have let and n be an integer such that n prove that with probability of at least over a choice of a random exercises matrix w rnd where each element of w is independently distributed according to n we have u w for every u v q. hint use jl to bound both and let xm be a set of vectors in rd of norm at most and assume that these vectors are linearly separable with margin of assume that d show that there exists a constant c such that if we randomly project these vectors into rn for n c then with probability of at least it holds that the projected vectors are linearly separable with margin generative models we started this book with a distribution free learning framework namely we did not impose any assumptions on the underlying distribution over the data. furthermore we followed a discriminative approach in which our goal is not to learn the underlying distribution but rather to learn an accurate predictor. in this chapter we describe a generative approach in which it is assumed that the underlying distribution over the data has a specific parametric form and our goal is to estimate the parameters of the model. this task is called parametric density estimation. the discriminative approach has the advantage of directly optimizing the quantity of interest prediction accuracy instead of learning the underlying distribution. this was phrased as follows by vladimir vapnik in his principle for solving problems using a restricted amount of information when solving a given problem try to avoid a more general problem as an intermediate step. of course if we succeed in learning the underlying distribution accurately we are considered to be experts in the sense that we can predict by using the bayes optimal classifier. the problem is that it is usually more difficult to learn the underlying distribution than to learn an accurate predictor. however in some situations it is reasonable to adopt the generative learning approach. for example sometimes it is easier to estimate the parameters of the model than to learn a discriminative predictor. additionally in some cases we do not have a specific task at hand but rather would like to model the data either for making predictions at a later time without having to retrain a predictor or for the sake of interpretability of the data. we start with a popular statistical method for estimating the parameters of the data which is called the maximum likelihood principle. next we describe two generative assumptions which greatly simplify the learning process. we also describe the em algorithm for calculating the maximum likelihood in the presence of latent variables. we conclude with a brief description of bayesian reasoning. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning maximum likelihood estimator maximum likelihood estimator let us start with a simple example. a drug company developed a new drug to treat some deadly disease. we would like to estimate the probability of survival when using the drug. to do so the drug company sampled a training set of m people and gave them the drug. let s xm denote the training set where for each i xi if the ith person survived and xi otherwise. we can model the underlying distribution using a single parameter indicating the probability of survival. we now would like to estimate the parameter on the basis of the training set s. a natural idea is to use the average number of s in s as an estimator. that is m xi. clearly es that is is an unbiased estimator of furthermore since is the average of m i.i.d. binary random variables we can use hoeffding s inequality to get that with probability of at least over the choice of s we have that m another interpretation of is as the maximum likelihood estimator as we formally explain now. we first write the probability of generating the sample s ps xm xi xi i xi xi. we define the log likelihood of s given the parameter as the log of the preceding expression ls log xm log xi xi. i i the maximum likelihood estimator is the parameter that maximizes the likelihood argmax ls next we show that in our case equation is a maximum likelihood estimator. to see this we take the derivative of ls with respect to and equate it to zero i xi xi solving the equation for we obtain the estimator given in equation generative models maximum likelihood estimation for continuous random variables let x be a continuous random variable. then for most x r we have px x and therefore the definition of likelihood as given before is trivialized. to overcome this technical problem we define the likelihood as log of the density of the probability of x at x. that is given an i.i.d. training set s xm sampled according to a density distribution p we define the likelihood of s given as ls log p logp as before the maximum likelihood estimator is a maximizer of ls with respect to as an example consider a gaussian random variable for which the density function of x is parameterized by and is defined as follows p exp we can rewrite the likelihood as ls m log to find a parameter that optimizes this we take the derivative of the likelihood w.r.t. and w.r.t. and compare it to we obtain the following two equations d d d d ls ls m m xi and m solving the preceding equations we obtain the maximum likelihood estimates note that the maximum likelihood estimate is not always an unbiased estimator. for example while is unbiased it is possible to show that the estimate of the variance is biased simplifying notation to simplify our notation we use px x in this chapter to describe both the probability that x x discrete random variables and the density of the distribution at x continuous variables. maximum likelihood estimator maximum likelihood and empirical risk minimization the maximum likelihood estimator shares some similarity with the empirical risk minimization principle which we studied extensively in previous chapters. recall that in the erm principle we have a hypothesis class h and we use the training set for choosing a hypothesis h h that minimizes the empirical risk. we now show that the maximum likelihood estimator is an erm for a particular loss function. given a parameter and an observation x we define the loss of on x as x logp that is x is the negation of the log-likelihood of the observation x assuming the data is distributed according to p this loss function is often referred to as the log-loss. on the basis of this definition it is immediate that the maximum likelihood principle is equivalent to minimizing the empirical risk with respect to the loss function given in equation that is argmin logp argmax logp assuming that the data is distributed according to a distribution p necessarily of the parametric form we employ the true risk of a parameter becomes e x x x x px logp px p x px px log hp px log drepp where dre is called the relative entropy and h is called the entropy function. the relative entropy is a divergence measure between two probabilities. for discrete variables it is always nonnegative and is equal to only if the two distributions are the same. it follows that the true risk is minimal when p p. the expression given in equation underscores how our generative assumption affects our density estimation even in the limit of infinite data. it shows that if the underlying distribution is indeed of a parametric form then by choosing the correct parameter we can make the risk be the entropy of the distribution. however if the distribution is not of the assumed parametric form even the best parameter leads to an inferior model and the suboptimality is measured by the relative entropy divergence. generalization analysis how good is the maximum likelihood estimator when we learn from a finite training set? generative models to answer this question we need to define how we assess the quality of an approximated solution of the density estimation problem. unlike discriminative learning where there is a clear notion of loss in generative learning there are various ways to define the loss of a model. on the basis of the previous subsection one natural candidate is the expected log-loss as given in equation in some situations it is easy to prove that the maximum likelihood principle guarantees low true risk as well. for example consider the problem of estimating the mean of a gaussian variable of unit variance. we saw previously that the maximum likelihood estimator is the average i xi. let be the optimal m parameter. then e x n x x e x n log p e x n e x n next we note that is the average of m gaussian variables and therefore it is also distributed normally with mean and variance from this fact we can derive bounds of the form with probability of at least we have that where depends on and on in some situations the maximum likelihood estimator clearly overfits. for example consider a bernoulli random variable x and let px as we saw previously using hoeffding s inequality we can easily derive a guarantee on that holds with high probability equation however if our goal is to obtain a small value of the expected log-loss function as defined in equation we might fail. for example assume that is nonzero but very small. then the probability that no element of a sample of size m will be is which is greater than e m. it follows that whenever m the probability that the sample is all zeros is at least and in that case the maximum likelihood rule will set but the true risk of the estimate is e x x this simple example shows that we should be careful in applying the maximum likelihood principle. to overcome overfitting we can use the variety of tools we encountered pre naive bayes viously in the book. a simple regularization technique is outlined in exercise naive bayes the naive bayes classifier is a classical demonstration of how generative assumptions and parameter estimations simplify the learning process. consider the problem of predicting a label y on the basis of a vector of features x xd where we assume that each xi is in recall that the bayes optimal classifier is hbayesx argmax y py yx x. to describe the probability function py yx x we need parameters each of which corresponds to py x for a certain value of x this implies that the number of examples we need grows exponentially with the number of features. in the naive bayes approach we make the naive generative assumption that given the label the features are independent of each other. that is px xy y pxi xiy y. with this assumption and using bayes rule the bayes optimal classifier can be further simplified hbayesx argmax y argmax y py yx x py ypx xy y argmax y py y pxi xiy y. that is now the number of parameters we need to estimate is only here the generative assumption we made reduced significantly the number of parameters we need to learn. when we also estimate the parameters using the maximum likelihood princi ple the resulting classifier is called the naive bayes classifier. linear discriminant analysis linear discriminant analysis is another demonstration of how generative assumptions simplify the learning process. as in the naive bayes classifier we consider again the problem of predicting a label y on the basis of a generative models vector of features x xd. but now the generative assumption is as follows. first we assume that py py second we assume that the conditional probability of x given y is a gaussian distribution. finally the covariance matrix of the gaussian distribution is the same for both values of the label. formally let rd and let be a covariance matrix. then the density distribution is given by px xy y exp yt y as we have shown in the previous section using bayes rule we can write hbayesx argmax y py ypx xy y. this means that we will predict hbayesx iff xy py xy log this ratio is often called the log-likelihood ratio. in our case the log-likelihood ratio becomes we can rewrite this as b where w and b t t as a result of the preceding derivation we obtain that under the aforementioned generative assumptions the bayes optimal classifier is a linear classifier. additionally one may train the classifier by estimating the parameter and from the data using for example the maximum likelihood estimator. with those estimators at hand the values of w and b can be calculated as in equation latent variables and the em algorithm in generative models we assume that the data is generated by sampling from a specific parametric distribution over our instance space x sometimes it is convenient to express this distribution using latent random variables. a natural example is a mixture of k gaussian distributions. that is x rd and we assume that each x is generated as follows. first we choose a random number in k. let y be a random variable corresponding to this choice and denote py y cy. second we choose x on the basis of the value of y according to a gaussian distribution y y px xy y exp yt latent variables and the em algorithm therefore the density of x can be written as px x py ypx xy y cy exp yt y y note that y is a hidden variable that we do not observe in our data. nevertheless we introduce y since it helps us describe a simple parametric form of the probability of x. more generally let be the parameters of the joint distribution of x and y in the preceding example consists of cy y and y for all y k. then the log-likelihood of an observation x can be written as log x log p x y y given an i.i.d. sample s xm we would like to find that maxi mizes the log-likelihood of s l log p xi log p xi log p xi y y the maximum-likelihood estimator is therefore the solution of the maximization problem argmax l argmax log p xi y y in many situations the summation inside the log makes the preceding optimization problem computationally hard. the expectation-maximization algorithm due to dempster laird and rubin is an iterative procedure for searching a maximum of l while em is not guaranteed to find the global maximum it often works reasonably well in practice. em is designed for those cases in which had we known the values of the latent variables y then the maximum likelihood optimization problem would have been tractable. more precisely define the following function over m k matrices and the set of parameters f qiy log xi y y generative models if each row of q defines a probability over the ith latent variable given x xi then we can interpret f as the expected log-likelihood of a training set ym where the expectation is with respect to the choice of each yi on the basis of the ith row of q. in the definition of f the summation is outside the log and we assume that this makes the optimization problem with respect to tractable assumption for any matrix q such that each row of q sums to the optimization problem is tractable. argmax f the intuitive idea of em is that we have a chicken and egg problem. on one hand had we known q then by our assumption the optimization problem of finding the best is tractable. on the other hand had we known the parameters we could have set qiy to be the probability of y y given that x xi. the em algorithm therefore alternates between finding given q and finding q given formally em finds a sequence of solutions where at iteration t we construct by performing two steps. expectation step set iy p yx xi. this step is called the expectation step because it yields a new probability over the latent variables which defines a new expected log-likelihood function over maximization step set to be the maximizer of the expected log likelihood where the expectation is according to argmax f by our assumption it is possible to solve this optimization problem efficiently. the initial values of and are usually chosen at random and the procedure terminates after the improvement in the likelihood value stops being significant. em as an alternate maximization algorithm to analyze the em algorithm we first view it as an alternate maximization algorithm. define the following objective function gq f qiy logqiy. latent variables and the em algorithm the second term is the sum of the entropies of the rows of q. let q q i qiy be the set of matrices whose rows define probabilities over the following lemma shows that em performs alternate maximization iterations for maximizing g. lemma the em procedure can be rewritten as argmax q q argmax gq furthermore l proof given we clearly have that argmax argmax f therefore we only need to show that for any the solution of argmaxq q gq is to set qiy p yx xi. indeed by jensen s inequality for any q q we have that gq qiy log xi y y p xi y y qiy qiy qiy log log p xi y y log xi l generative models while for qiy p yx xi we have p yx xi log xi y y p yx xi gq p yx xi log xi log xi log xi l p yx xi this shows that setting qiy p yx xi maximizes gq over q q and shows that l the preceding lemma immediately implies theorem the em procedure never decreases the log-likelihood namely for all t l l proof by the lemma we have l l em for mixture of gaussians k-means consider the case of a mixture of k gaussians in which is a triplet k k where p y cy and p xy y is as given in equation for simplicity we assume that k i where i is the identity matrix. specifying the em algorithm for this case we obtain the following expectation step for each i and y we have that p yx xi p yp xiy y where zi is a normalization factor which ensures y y p yx maximization step we need to set to be a maximizer of equation xi sums to ct y exp zi zi bayesian reasoning which in our case amounts to maximizing the following expression w.r.t. c and p yx xi logcy comparing the derivative of equation w.r.t. y to zero and rearranging terms we obtain p yx xi xi p yx xi y that is y is a weighted average of the xi where the weights are according to the probabilities calculated in the e step. to find the optimal c we need to be more careful since we must ensure that c is a probability vector. in exercise we show that the solution is p yx xi p xi cy it is interesting to compare the preceding algorithm to the k-means algorithm described in chapter in the k-means algorithm we first assign each example to a cluster according to the distance then we update each center y according to the average of the examples assigned to this cluster. in the em approach however we determine the probability that each example belongs to each cluster. then we update the centers on the basis of a weighted sum over the entire sample. for this reason the em approach for k-means is sometimes called soft k-means. bayesian reasoning the maximum likelihood estimator follows a frequentist approach. this means that we refer to the parameter as a fixed parameter and the only problem is that we do not know its value. a different approach to parameter estimation is called bayesian reasoning. in the bayesian approach our uncertainty about is also modeled using probability theory. that is we think of as a random variable as well and refer to the distribution p as a prior distribution. as its name indicates the prior distribution should be defined by the learner prior to observing the data. as an example let us consider again the drug company which developed a new drug. on the basis of past experience the statisticians at the drug company believe that whenever a drug has reached the level of clinic experiments on people it is likely to be effective. they model this prior belief by defining a density distribution on such that p if if generative models as before given a specific value of it is assumed that the conditional probability px x is known. in the drug company example x takes values in and px x x. once the prior distribution over and the conditional distribution over x given are defined we again have complete knowledge of the distribution over x. this is because we can write the probability over x as a marginal probability px x px x p x where the last equality follows from the definition of conditional probability. if is continuous we replace p with the density function and the sum becomes an integral px x p x d seemingly once we know px x a training set s xm tells us nothing as we are already experts who know the distribution over a new point x. however the bayesian view introduces dependency between s and x. this is because we now refer to as a random variable. a new point x and the previous points in s are independent only conditioned on this is different from the frequentist philosophy in which is a parameter that we might not know but since it is just a parameter of the distribution a new point x and previous points s are always independent. in the bayesian framework since x and s are not independent anymore what we would like to calculate is the probability of x given s which by the chain rule can be written as follows px xs px x sp px x the second inequality follows from the assumption that x and s are independent when we condition on using bayes rule we have ps p ps and together with the assumption that points are independent conditioned on we can write p ps ps ps px xi we therefore obtain the following expression for bayesian prediction px xs ps px x px xi getting back to our drug company example we can rewrite px xs as i xi p d px xs p summary it is interesting to note that when p is uniform we obtain that px xs xi d solving the preceding integral integration by parts we obtain i i xi m px recall that the prediction according to the maximum likelihood principle in this case is px i xi m the bayesian prediction with uniform prior is rather similar to the maximum likelihood prediction except it adds pseudoexamples to the training set thus biasing the prediction toward the uniform prior. maximum a posteriori in many situations it is difficult to find a closed form solution to the integral given in equation several numerical methods can be used to approximate this integral. another popular solution is to find a single which maximizes p the value of which maximizes p is called the maximum a posteriori estimator. once this value is found we can calculate the probability that x x given the maximum a posteriori estimator and independently on s. summary in the generative approach to machine learning we aim at modeling the distribution over the data. in particular in parametric density estimation we further assume that the underlying distribution over the data has a specific parametric form and our goal is to estimate the parameters of the model. we have described several principles for parameter estimation including maximum likelihood bayesian estimation and maximum a posteriori. we have also described several specific algorithms for implementing the maximum likelihood under different assumptions on the underlying data distribution in particular naive bayes lda and em. bibliographic remarks the maximum likelihood principle was studied by ronald fisher in the beginning of the century. bayesian statistics follow bayes rule which is named after the century english mathematician thomas bayes. there are many excellent books on the generative and bayesian approaches to machine learning. see for example koller friedman mackay murphy barber generative models exercises prove that the maximum likelihood estimator of the variance of a gaussian variable is biased. regularization for maximum likelihood consider the following regularized loss minimization m m show that the preceding objective is equivalent to the usual empirical error had we added two pseudoexamples to the training set. conclude that the regularized maximum likelihood estimator would be m xi derive a high probability bound on hint rewrite this as e e and then use the triangle inequality and hoeffding inequality. use this to bound the true risk. hint use the fact that now to relate to the relative entropy. consider a general optimization problem of the form y logcy s.t. cy cy y is a vector of nonnegative weights. verify that the m step where rk of soft k-means involves solving such an optimization problem. let show that the optimization problem is equivalent to the problem show that is a probability vector. y y min c s.t. cy cy using properties of the relative entropy conclude that is the solution to the optimization problem. max c y feature selection and generation in the beginning of the book we discussed the abstract model of learning in which the prior knowledge utilized by the learner is fully encoded by the choice of the hypothesis class. however there is another modeling choice which we have so far ignored how do we represent the instance space x for example in the papayas learning problem we proposed the hypothesis class of rectangles in the softness-color two dimensional plane. that is our first modeling choice was to represent a papaya as a two dimensional point corresponding to its softness and color. only after that did we choose the hypothesis class of rectangles as a class of mappings from the plane into the label set. the transformation from the real world object papaya into the scalar representing its softness or its color is called a feature function or a feature for short namely any measurement of the real world object can be regarded as a feature. if x is a subset of a vector space each x x is sometimes referred to as a feature vector. it is important to understand that the way we encode real world objects as an instance space x is by itself prior knowledge about the problem. furthermore even when we already have an instance space x which is represented as a subset of a vector space we might still want to change it into a different representation and apply a hypothesis class on top of it. that is we may define a hypothesis class on x by composing some class h on top of a feature function which maps x into some other vector space x we have already encountered examples of such compositions in chapter we saw that kernel-based svm learns a composition of the class of halfspaces over a feature mapping that maps each original instance in x into some hilbert space. and indeed the choice of is another form of prior knowledge we impose on the problem. in this chapter we study several methods for constructing a good feature set. we start with the problem of feature selection in which we have a large pool of features and our goal is to select a small number of features that will be used by our predictor. next we discuss feature manipulations and normalization. these include simple transformations that we apply on our original features. such transformations may decrease the sample complexity of our learning algorithm its bias or its computational complexity. last we discuss several approaches for feature learning. in these methods we try to automate the process of feature construction. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning feature selection and generation we emphasize that while there are some common techniques for feature learning one may want to try the no-free-lunch theorem implies that there is no ultimate feature learner. any feature learning algorithm might fail on some problem. in other words the success of each feature learner relies implicitly on some form of prior assumption on the data distribution. furthermore the relative quality of features highly depends on the learning algorithm we are later going to apply using these features. this is illustrated in the following example. example consider a regression problem in which x y r and the loss function is the squared loss. suppose that the underlying distribution is such that an example y is generated as follows first we sample from the uniform distribution over then we deterministically set y finally the second feature is set to be y z where z is sampled from the uniform distribution over suppose we would like to choose a single feature. intuitively the first feature should be preferred over the second feature as the target can be perfectly predicted based on the first feature alone while it cannot be perfectly predicted based on the second feature. indeed choosing the first feature would be the right choice if we are later going to apply polynomial regression of degree at least however if the learner is going to be a linear regressor then we should prefer the second feature over the first one since the optimal linear predictor based on the first feature will have a larger risk than the optimal linear predictor based on the second feature. feature selection throughout this section we assume that x rd. that is each instance is represented as a vector of d features. our goal is to learn a predictor that only relies on k d features. predictors that use only a small subset of features require a smaller memory footprint and can be applied faster. furthermore in applications such as medical diagnostics obtaining each possible feature test result can be costly therefore a predictor that uses only a small number of features is desirable even at the cost of a small degradation in performance relative to a predictor that uses more features. finally constraining the hypothesis class to use a small subset of features can reduce its estimation error and thus prevent overfitting. ideally we could have tried all subsets of k out of d features and choose the subset which leads to the best performing predictor. however such an exhaustive search is usually computationally intractable. in the following we describe three computationally feasible approaches for feature selection. while these methods cannot guarantee finding the optimal subset they often work reasonably well in practice. some of the methods come with formal guarantees on the quality of the selected subsets under certain assumptions. we do not discuss these guarantees here. feature selection filters maybe the simplest approach for feature selection is the filter method in which we assess individual features independently of other features according to some quality measure. we can then select the k features that achieve the highest score decide also on the number of features to select according to the value of their scores. many quality measures for features have been proposed in the literature. maybe the most straightforward approach is to set the score of a feature according to the error rate of a predictor that is trained solely by that feature. to illustrate this consider a linear regression problem with the squared loss. let v xmj rm be a vector designating the values of the jth feature on a training set of m examples and let y ym rm be the values of the target on the same m examples. the empirical squared loss of an erm linear predictor that uses only the jth feature would be min ab r m b where the meaning of adding a scalar b to a vector v is adding b to all coordinates of v. to solve this problem let v vi be the averaged value of the m feature and let y yi be the averaged value of the target. clearly m exercise min ab r m b min ab r m v b taking the derivative of the right-hand side objective with respect to b and comparing it to zero we obtain that b similarly solving for a we know that b yields a v y plugging this value back into the objective we obtain the value v y ranking the features according to the minimal loss they achieve is equivalent to ranking them according to the absolute value of the following score now a higher score yields a better feature v y m v y the preceding expression is known as pearson s correlation coefficient. the numerator is the empirical estimate of the covariance of the jth feature and the target value ev e vy e y while the denominator is the squared root of the empirical estimate for the variance of the jth feature ev e times the variance of the target. pearson s coefficient ranges from to where if the pearson s coefficient is either or there is a linear mapping from v to y with zero empirical risk. feature selection and generation if pearson s coefficient equals zero it means that the optimal linear function from v to y is the all-zeros function which means that v alone is useless for predicting y. however this does not mean that v is a bad feature as it might be the case that together with other features v can perfectly predict y. indeed consider a simple example in which the target is generated by the function y assume also that is generated from the uniform distribution over and z where z is also generated i.i.d. from the uniform distribution over then ey and we also have therefore for a large enough training set the first feature is likely to have a pearson s correlation coefficient that is close to zero and hence it will most probably not be selected. however no function can predict the target value well without knowing the first feature. there are many other score functions that can be used by a filter method. notable examples are estimators of the mutual information or the area under the receiver operating characteristic curve. all of these score functions suffer from similar problems to the one illustrated previously. we refer the reader to guyon elisseeff greedy selection approaches greedy selection is another popular approach for feature selection. unlike filter methods greedy selection approaches are coupled with the underlying learning algorithm. the simplest instance of greedy selection is forward greedy selection. we start with an empty set of features and then we gradually add one feature at a time to the set of selected features. given that our current set of selected features is i we go over all i i and apply the learning algorithm on the set of features i each such application yields a different predictor and we choose to add the feature that yields the predictor with the smallest risk the training set or on a validation set. this process continues until we either select k features where k is a predefined budget of allowed features or achieve an accurate enough predictor. example matching pursuit to illustrate the forward greedy selection approach we specify it to the problem of linear regression with the squared loss. let x rmd be a matrix whose rows are the m training instances. let y rm be the vector of the m labels. for every i let xi be the ith column of x. given a set i we denote by xi the matrix whose columns are i i. the forward greedy selection method starts with at iteration t we look for the feature index jt which is in argmin j min w rt feature selection then we update it it we now describe a more efficient implementation of the forward greedy selection approach for linear regression which is called orthogonal matching pursuit the idea is to keep an orthogonal basis of the features aggregated so far. let vt be a matrix whose columns form an orthonormal basis of the columns of xit. clearly min rt min w we will maintain a vector t which minimizes the right-hand side of the equation. initially we set and to be the empty vector. at round t for every j we decompose xj vj uj where vj vt t is the projection of xj onto the subspace spanned by vt and uj is the part of xj orthogonal to vt appendix c. then min min min uj vt min t min t min it follows that we should select the feature jt argmax j the rest of the update is to set vt vt ujt t t the omp procedure maintains an orthonormal basis of the selected features where in the preceding description the orthonormalization property is obtained by a procedure similar to gram-schmidt orthonormalization. in practice the gram-schmidt procedure is often numerically unstable. in the pseudocode that follows we use svd section at the end of each round to obtain an orthonormal basis in a numerically stable manner. feature selection and generation orthogonal matching pursuit input data matrix x rmd labels vector y rm budget of features t initialize for t t use svd to find an orthonormal basis v rmt of xit t set v to be the all zeros matrix foreach j it let uj xj v v let jt argmaxj update it output it more efficient greedy selection criteria let rw be the empirical risk of a vector w. at each round of the forward greedy selection method and for every possible j we should minimize rw over the vectors w whose support is it this might be time consuming. a simpler approach is to choose jt that minimizes argmin j r rwt ej min where ej is the all zeros vector except in the jth element. that is we keep the weights of the previously chosen coordinates intact and only optimize over the new variable. therefore for each j we need to solve an optimization problem over a single variable which is a much easier task than optimizing over t. an even simpler approach is to upper bound rw using a simple function and then choose the feature which leads to the largest decrease in this upper bound. for example if r is a function equation in chapter then rw ej rw rw wj minimizing the right-hand side over yields rw value into the above yields wj rw and plugging this rw ej rw wj this value is minimized if the partial derivative of rw with respect to wj is maximal. we can therefore choose jt to be the index of the largest coordinate of the gradient of rw at w. remark as a forward greedy selection procedure it is possible to interpret the adaboost algorithm from chapter as a forward greedy feature selection selection procedure with respect to the function exp yi wjhjxi rw log see exercise backward elimination another popular greedy selection approach is backward elimination. here we start with the full set of features and then we gradually remove one feature at a time from the set of features. given that our current set of selected features is i we go over all i i and apply the learning algorithm on the set of features ii. each such application yields a different predictor and we choose to remove the feature i for which the predictor obtained from i has the smallest risk the training set or on a validation set. naturally there are many possible variants of the backward elimination idea. it is also possible to combine forward and backward greedy steps. sparsity-inducing norms the problem of minimizing the empirical risk subject to a budget of k features can be written as min w lsw s.t. k wi in other words we want w to be sparse which implies that we only need to measure the features corresponding to nonzero elements of w. convex function with the norm solving this optimization problem is computationally hard davis mallat avellaneda a possible relaxation is to replace the and to solve the problem min w lsw s.t. where is a parameter. since the norm is a convex function this problem can be solved efficiently as long as the loss function is convex. a related problem is minimizing the sum of lsw plus an norm regularization term min w where is a regularization parameter. since for any there exists a such that the function is often referred to as the norm. despite the use of the norm notation is not really a norm for example it does not satisfy the positive homogeneity property of norms feature selection and generation equation and equation lead to the same solution the two problems are in some sense equivalent. the regularization often induces sparse solutions. to illustrate this let us start with the simple optimization problem min w r xw it is easy to verify exercise that the solution to this problem is the soft thresholding operator w signx def maxa that is as long as the absolute value of x is smaller where than the optimal solution will be zero. next consider a one dimensional regression problem with respect to the squared loss argmin w rm we can rewrite the problem as i m i argmin w rm for simplicity let us assume that m then the optimal solution is i and denote w xiyi m i xiyi w that is the solution will be zero unless the correlation between the feature x and the labels vector y is larger than remark unlike the norm the norm does not induce sparse solutions. indeed consider the problem above with an regularization namely argmin w rm then the optimal solution is w this solution will be nonzero even if the correlation between x and y is very small. in contrast as we have shown before when using regularization w will be nonzero only if the correlation between x and y is larger than the regularization parameter feature manipulation and normalization adding regularization to a linear regression problem with the squared loss yields the lasso algorithm defined as argmin w under some assumptions on the distribution and the regularization parameter the lasso will find sparse solutions for example yu and the references therein. another advantage of the norm is that a vector with low norm can be sparsified for example zhang srebro and the references therein. feature manipulation and normalization feature manipulations or normalization include simple transformations that we apply on each of our original features. such transformations may decrease the approximation or estimation errors of our hypothesis class or can yield a faster algorithm. similarly to the problem of feature selection here again there are no absolute good and bad transformations but rather each transformation that we apply should be related to the learning algorithm we are going to apply on the resulting feature vector as well as to our prior assumptions on the problem. to motivate normalization consider a linear regression problem with the squared loss. let x rmd be a matrix whose rows are the instance vectors and let y rm be a vector of target values. recall that ridge regression returns the vector mi m argmin w suppose that d and the underlying data distribution is as follows. first we sample y uniformly at random from then we set to be y where is sampled uniformly at random from and we set to be note that the optimal weight vector is and however the objective of ridge regression at is in contrast the objective of ridge regression at w is likely to be close to it follows that whenever the objective of ridge regression is smaller at the suboptimal solution w since typically should be at least the analysis in chapter it follows that in the aforementioned example if the number of examples is smaller than then we are likely to output a suboptimal solution. the crux of the preceding example is that the two features have completely different scales. feature normalization can overcome this problem. there are many ways to perform feature normalization and one of the simplest approaches is simply to make sure that each feature receives values between and in the preceding example if we divide each feature by the maximal value it attains feature selection and generation we will obtain that ridge regression is quite close to and y. then for the solution of moreover the generalization bounds we have derived in chapter for regularized loss minimization depend on the norm of the optimal vector and on the maximal norm of the instance therefore in the aforementioned example before we normalize the features we have that while after we normalize the features we have that the maximal norm of the instance vector remains roughly the same hence the normalization greatly improves the estimation error. feature normalization can also improve the runtime of the learning algorithm. for example in section we have shown how to use the stochastic gradient descent optimization algorithm for solving the regularized loss minimization problem. the number of iterations required by sgd to converge also depends on the norm of and on the maximal norm of therefore as before using normalization can greatly decrease the runtime of sgd. next we demonstrate in the following how a simple transformation on features such as clipping can sometime decrease the approximation error of our hypothesis class. consider again linear regression with the squared loss. let a be a large number suppose that the target y is chosen uniformly at random from and then the single feature x is set to be y with probability and set to be ay with probability that is most of the time our feature is bounded but with a very small probability it gets a very high value. then for any w the expected squared loss of w is ldw e a a solving for w we obtain that which goes to zero as a goes to infinity. therefore the objective at goes to as a goes to infinity. for example for a we will obtain next suppose we apply a clipping transformation that is we use the transformation x signx then following this transformation becomes and this simple example shows that a simple transformation can have a significant influence on the approximation error. of course it is not hard to think of examples in which the same feature transformation actually hurts performance and increases the approximation error. this is not surprising as we have already argued that feature transformations more precisely the bounds we derived in chapter for regularized loss minimization depend on and on either the lipschitzness or the smoothness of the loss function. for linear predictors and loss functions of the form y y where is convex and either or with respect to its first argument we have that is either or for example for the squared loss y first argument. is with respect to its and y feature manipulation and normalization should rely on our prior assumptions on the problem. in the aforementioned example a prior assumption that may lead us to use the clipping transformation is that features that get values larger than a predefined threshold value give us no additional useful information and therefore we can clip them to the predefined threshold. examples of feature transformations we now list several common techniques for feature transformations. usually it is helpful to combine some of these transformations centering scaling. in the following we denote by f fm rm the value of the feature f over the m training examples. also we denote by f fi the empirical m mean of the feature over all examples. centering this transformation makes the feature have zero mean by setting fi fi f unit range this transformation makes the range of each feature be formally let fmax maxi fi and fmin mini fi. then we set fi fi fmin similarly fmax fmin we can make the range of each feature be by the transformation fi of course it is easy to make the range b or b b where b is fi fmin fmax fmin a user-specified parameter. standardization this transformation makes all features have a zero mean and unit variance. f be the empirical variance of the feature. formally let m then we set fi fi f clipping this transformation clips high or low values of the feature. for example fi signfi maxbfi where b is a user-specified parameter. sigmoidal transformation as its name indicates this transformation applies a sigmoid function on the feature. for example fi fi where b is a user-specified parameter. this transformation can be thought of as a soft version of clipping it has a small effect on values close to zero and behaves similarly to clipping on values far away from zero. feature selection and generation logarithmic transformation the transformation is fi logbfi where b is a user-specified parameter. this is widely used when the feature is a counting feature. for example suppose that the feature represents the number of appearances of a certain word in a text document. then the difference between zero occurrences of the word and a single occurrence is much more important than the difference between occurrences and occurrences. remark in the aforementioned transformations each feature is transformed on the basis of the values it obtains on the training set independently of other features values. in some situations we would like to set the parameter of the transformation on the basis of other features as well. a notable example is a transformation in which one applies a scaling to the features so that the empirical average of some norm of the instances becomes feature learning so far we have discussed feature selection and manipulations. in these cases we start with a predefined vector space rd representing our features. then we select a subset of features selection or transform individual features transformation. in this section we describe feature learning in which we start with some instance space x and would like to learn a function x rd which maps instances in x into a representation as d-dimensional feature vectors. the idea of feature learning is to automate the process of finding a good representation of the input space. as mentioned before the no-free-lunch theorem tells us that we must incorporate some prior knowledge on the data distribution in order to build a good feature representation. in this section we present a few feature learning approaches and demonstrate conditions on the underlying data distribution in which these methods can be useful. throughout the book we have already seen several useful feature constructions. for example in the context of polynomial regression we have mapped the original instances into the vector space of all their monomials section in chapter after performing this mapping we trained a linear predictor on top of the constructed features. automation of this process would be to learn a transformation x rd such that the composition of the class of linear predictors on top of yields a good hypothesis class for the task at hand. in the following we describe a technique of feature construction called dictio nary learning. dictionary learning using auto-encoders the motivation of dictionary learning stems from a commonly used representation of documents as a bag-of-words given a dictionary of words d wk where each wi is a string representing a word in the dictionary feature learning and given a document pd where each pi is a word in the document we represent the document as a vector x where xi is if wi pj for some j and xi otherwise. it was empirically observed in many text processing tasks that linear predictors are quite powerful when applied on this representation. intuitively we can think of each word as a feature that measures some aspect of the document. given labeled examples topics of the documents a learning algorithm searches for a linear predictor that weights these features so that a right combination of appearances of words is indicative of the label. while in text processing there is a natural meaning to words and to the dictionary in other applications we do not have such an intuitive representation of an instance. for example consider the computer vision application of object recognition. here the instance is an image and the goal is to recognize which object appears in the image. applying a linear predictor on the pixel-based representation of the image does not yield a good classifier. what we would like to have is a mapping that would take the pixel-based representation of the image and would output a bag of visual words representing the content of the image. for example a visual word can be there is an eye in the image. if we had such representation we could have applied a linear predictor on top of this representation to train a classifier for say face recognition. our question is therefore how can we learn a dictionary of visual words such that a bag-ofwords representation of an image would be helpful for predicting which object appears in the image? a first naive approach for dictionary learning relies on a clustering algorithm chapter suppose that we learn a function c x k where cx is the cluster to which x belongs. then we can think of the clusters as words and of instances as documents where a document x is mapped to the vector where is if and only if x belongs to the ith cluster. now it is straightforward to see that applying a linear predictor on is equivalent to assigning the same target value to all instances that belong to the same cluster. furthermore if the clustering is based on distances from a class center k-means then a linear predictor on yields a piece-wise constant predictor on x. find a pair of functions such that the reconstruction both the k-means and pca approaches can be regarded as special cases of a more general approach for dictionary learning which is called auto-encoders. in an auto-encoder we learn a pair of functions an encoder function rd rk and a decoder function rk rd. the goal of the learning process is to i is small. of course we can trivially set k d and both to be the identity mapping which yields a perfect reconstruction. we therefore must restrict and in some way. in pca we constrain k d and further restrict and to be linear functions. in k-means k is not restricted to be smaller than d but now and rely on k centroids k and returns an indicator vector feature selection and generation in that indicates the closest centroid to x while takes as input an indicator vector and returns the centroid representing this vector. an important property of the k-means construction which is key in allowing k to be larger than d is that maps instances into sparse vectors. in fact in k-means only a single coordinate of is nonzero. an immediate extension of the k-means construction is therefore to restrict the range of to be vectors with at most s nonzero elements where s is a small integer. in particular let and be functions that depend on k. the function maps an instance vector x to a vector rk where should have at most s nonzero elements. vi i. as before our goal is to have a the function is defined to be small reconstruction error and therefore we can define argmin v s.t. s where vj note that when s and we further restrict then we obtain the k-means encoding function that is is the indicator vector of the centroid closest to x. for larger values of s the optimization problem in the preceding definition of becomes computationally difficult. therefore in practice we sometime use regularization instead of the sparsity constraint and define to be argmin v ror where is a regularization parameter. anyway the dictionary learning problem is now to find the vectors k such that the reconstruction is as small as possible. even if is defined using the regularization this is still a computationally hard problem to the k-means problem. however several heuristic search algorithms may give reasonably good solutions. these algorithms are beyond the scope of this book. summary many machine learning algorithms take the feature representation of instances for granted. yet the choice of representation requires careful attention. we discussed approaches for feature selection introducing filters greedy selection algorithms and sparsity-inducing norms. next we presented several examples for feature transformations and demonstrated their usefulness. last we discussed feature learning and in particular dictionary learning. we have shown that feature selection manipulation and learning all depend on some prior knowledge on the data. bibliographic remarks bibliographic remarks guyon elisseeff surveyed several feature selection procedures including many types of filters. forward greedy selection procedures for minimizing a convex objective subject to a polyhedron constraint date back to the frank-wolfe algorithm wolfe the relation to boosting has been studied by several authors including liao ratsch warmuth glocer vishwanathan shalev-shwartz singer matching pursuit has been studied in the signal processing community zhang several papers analyzed greedy selection methods under various conditions. see for example shalevshwartz zhang srebro and the references therein. the use of the as a surrogate for sparsity has a long history tibshirani and the references therein and much work has been done on understanding the relationship between the and sparsity. it is also closely related to compressed sensing chapter the ability to sparsify low norm predictors dates back to maurey in section we also show that low norm can be used to bound the estimation error of our predictor. feature learning and dictionary learning have been extensively studied recently in the context of deep neural networks. see for example bengio hinton et al. ranzato et al. collobert weston lee et al. le et al. bengio and the references therein. exercises prove the equality given in equation hint let a b be minimizers of the left-hand side. find a b such that the objective value of the right-hand side is smaller than that of the left-hand side. do the same for the other direction. show that equation is the solution of equation adaboost as a forward greedy selection algorithm recall the adaboost algorithm from chapter in this section we give another interpretation of adaboost as a forward greedy selection algorithm. given a set of m instances xm and a hypothesis class h of finite vc dimension show that there exist d and hd such that for every h h there exists i with hixj hxj for every j let rw be as defined in equation given some w define fw to be the function fw wihi feature selection and generation let d be the distribution over defined by exp yifwxi di z where z is a normalization factor that ensures that d is a probability vector. show that diyihjxi. rw furthermore denoting wj show that rw conclude that if then hint use the proof of theorem rw wj wj show that the update of adaboost guarantees rwt part iv advanced theory rademacher complexities in chapter we have shown that uniform convergence is a sufficient condition for learnability. in this chapter we study the rademacher complexity which measures the rate of uniform convergence. we will provide generalization bounds based on this measure. the rademacher complexity recall the definition of an sample from chapter repeated here for convenience. definition sample a training set s is called domain z hypothesis class h loss function and distribution d if lsh sup h h we have shown that if s is an representative sample then the erm rule is namely ldermhs minh h ldh to simplify our notation let us denote f def h def z h h and given f f we define ldf e z df lsf m f we define the representativeness of s with respect to f as the largest gap between the true error of a function f and its empirical error namely repdf s def sup f f lsf now suppose we would like to estimate the representativeness of s using the sample s only. one simple idea is to split s into two disjoint sets s refer to as a validation set and to as a training set. we can then estimate the representativeness of s by sup f f understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning rademacher complexities this can be written more compactly by defining m to be a vector such that i and i then if we further assume that then equation can be rewritten as m sup f f if the rademacher complexity measure captures this idea by considering the expectation of the above with respect to a random choice of formally let f s be the set of all possible evaluations a function f f can achieve on a sample s namely f s f f f. let the variables in be distributed i.i.d. according to p i p i then the rademacher complexity of f with respect to s is defined as follows rf s def m e if more generally given a set of vectors a rm we define sup f f ra def m e sup a a iai the following lemma bounds the expected value of the representativeness of s by twice the expected rademacher complexity. lemma e s dm proof let ldf therefore for every f f we have repdf s e s dm rf s. m be another i.i.d. sample. clearly for all f f ldf lsf e lsf e lsf taking supremum over f f of both sides and using the fact that the supremum of expectation is smaller than expectation of the supremum we obtain sup f f lsf sup f f e taking expectation over s on both sides we obtain e s sup f f lsf sup f f lsf e lsf lsf i f sup f f sup f f e m e the rademacher complexity next we note that for each j zj and replace them without affecting the expectation j are i.i.d. variables. therefore we can j f i f j i f let j be a random variable such that p j p j from equation we obtain that f jf e e e j e f f f f sup sup sup sup f f f f sup f f sup f f i e of equation of equation j f i f j f e i f if sup f f i f if repeating this for all j we obtain that i f e finally if i f sup f f if i sup f f i i and since the probability of is the same as the probability of the right-hand side of equation can be bounded by i sup f f m e if s i i sup f f m e if s e s. s the lemma immediately yields that in expectation the erm rule finds a hypothesis which is close to the optimal hypothesis in h. theorem we have e s dm lsermhs e s dm furthermore for any h h s. e s dm e s dm h s. rademacher complexities furthermore if argminh ldh then for each with probability of at least over the choice of s we have ldermhs dm h proof the first inequality follows directly from lemma the second inequality follows because for any fixed e e s s the third inequality follows from the previous inequality by relying on markov s inequality that the random variable ldermhs is nonnegative. next we derive bounds similar to the bounds in theorem with a better dependence on the confidence parameter to do so we first introduce the following bounded differences concentration inequality. lemma s inequality let v be some set and let f v m r be a function of m variables such that for some c for all i and for all xm i v we have xm f xi i xm c. let xm be m independent random variables taking values in v then with probability of at least we have xm ef xm c on the basis of the mcdiarmid inequality we can derive generalization bounds with a better dependence on the confidence parameter. theorem assume that for all z and h h we have that z c. then with probability of at least for all h h ldh lsh e dm h c m in particular this holds for h ermhs. with probability of at least for all h h ldh lsh h s c m in particular this holds for h ermhs. for any with probability of at least ldermhs h s c ln m the rademacher complexity proof first note that the random variable repdf s suph h lsh satisfies the bounded differences condition of lemma with a constant combining the bounds in lemma with lemma we obtain that with probability of at least e m h c repdf s e repdf s c the first inequality of the theorem follows from the definition of repdf s. for the second inequality we note that the random variable h s also satisfies the bounded differences condition of lemma with a constant therefore the second inequality follows from the first inequality lemma and the union bound. finally for the last inequality denote hs ermhs and note that m ldhs ldhs lshs lshs lshs the first summand on the right-hand side is bounded by the second inequality of the theorem. for the second summand we use the fact that does not depend on s hence by using hoeffding s inequality we obtain that with probaility of at least c combining this with the union bound we conclude our proof. the preceding theorem tells us that if the quantity h s is small then it is possible to learn the class h using the erm rule. it is important to emphasize that the last two bounds given in the theorem depend on the specific training set s. that is we use s both for learning a hypothesis from h as well as for estimating the quality of it. this type of bound is called a data-dependent bound. rademacher calculus let us now discuss some properties of the rademacher complexity measure. these properties will help us in deriving some simple bounds on h s for specific cases of interest. the following lemma is immediate from the definition. lemma for any a rm scalar c r and vector rm we have rc a a a ra. the following lemma tells us that the convex hull of a has the same complexity as a. rademacher complexities lemma let a be a subset of rm and let n j aj a j then ra. proof the main idea follows from the fact that for any vector v we have jaj n sup therefore jvj max j vj. jaj i i iaj i m e sup sup e j sup aj sup iai e sup a a m ra and we conclude our proof. the next lemma due to massart states that the rademacher complexity of a finite set grows logarithmically with the size of the set. lemma lemma let a an be a finite set of vectors in rm. define a n ai. then ra max a a logn m proof based on lemma we can assume without loss of generality that a let and let an. we upper bound the rademacher complexity as follows a max e log e e max a a a e log a e i iai log log jensen s inequality where the last equality occurs because the rademacher variables are independent. next using lemma we have that for all ai r expai exp ai e iai i e i the rademacher complexity and therefore log a exp i a max log a since ra max a log we obtain from the equation that ra loga maxa setting loga maxa a and rearranging terms we conclude our m proof. the following lemma shows that composing a with a lipschitz function does not blow up the rademacher complexity. the proof is due to kakade and tewari. lemma lemma for each i let i r r be a lipschitz function namely for all r we have i i for a rm let denote the vector mym. let a a a. then r a ra. proof for simplicity we prove the lemma for the case the case will follow by defining and then using lemma let ai ai iai am a a. clearly it suffices to prove that for any set a and all i we have rai ra. without loss of generality we will prove the latter claim for i and to simplify notation we omit the subscript from we have iai iai iai e e sup a sup a a e m e m e m sup a a sup a sup a iai sup a a i iai iai i where in the last inequality we used the assumption that is lipschitz. next we note that the absolute value on in the preceding expression can rademacher complexities be omitted since both a and are from the same set a and the rest of the expression in the supremum is not affected by replacing a and therefore i e m sup a iai but using the same equalities as in equation it is easy to see that the right-hand side of equation exactly equals m ra which concludes our proof. rademacher complexity of linear classes in this section we analyze the rademacher complexity of linear classes. to simplify the derivation we first define the following two classes the following lemma bounds the rademacher complexity of we allow the xi to be vectors in any hilbert space infinite dimensional and the bound does not depend on the dimensionality of the hilbert space. this property becomes useful when analyzing kernel methods. lemma let s xm be vectors in a hilbert space. define s then s maxi m proof using cauchy-schwartz inequality we know that for any vectors w v we have therefore s e e e e sup a s iai sup sup e ixi next using jensen s inequality we have that e ixi e ixi generalization bounds for svm finally since the variables m are independent we have e i e ij i e i j e m max i combining this with equation and equation we conclude our proof. next we bound the rademacher complexity of s. lemma let s xm be vectors in rn. then proof using holder s inequality we know that for any vectors w v we have therefore s max i m s e e e sup a s iai sup sup e for each j let vj xmj rm. note that let v vn vn. the right-hand side of equation is m rv using massart lemma we have that m maxi rv max i which concludes our proof. generalization bounds for svm in this section we use rademacher complexity to derive generalization bounds for generalized linear predictors with euclidean norm constraint. we will show how this leads to generalization bounds for hard-svm and soft-svm. rademacher complexities we shall consider the following general constraint-based formulation. let h b be our hypothesis class and let z x y be the examples domain. assume that the loss function h z r is of the form y y where r y r is such that for all y y the scalar function a y is for example the hinge-loss function y can be written as in equation using y ya and note that is for all y another example is the absolute loss function y y which can be written as in equation using y y which is also for all y r. the following theorem bounds the generalization error of all predictors in h using their empirical error. theorem suppose that d is a distribution over x y such that with probability we have that r. let h b and let h z r be a loss function of the form given in equation such that for all y y a y is a function and such that maxa brbr y c. then for any with probability of at least over the choice of an i.i.d. sample of size m w h ldw lsw br m c m proof let f y y w h. we will show that with probability rf s br m and then the theorem will follow from theorem indeed the set f s can be written as f s ym w h and the bound on rf s follows directly by combining lemma lemma and the assumption that r with probability we next derive a generalization bound for hard-svm based on the previous theorem. for simplicity we do not allow a bias term and consider the hard-svm problem s.t. i argmin w theorem consider a distribution d over x such that there exists some vector with pxy and such that r with probability let ws be the output of equation then with probability of at least over the choice of s dm we have that dy r p m r m generalization bounds for svm proof throughout the proof let the loss function be the ramp loss section note that the range of the ramp loss is and that it is a function. since the ramp loss upper bounds the zero-one loss we have that p dy ldws. let b and consider the set h b. by the definition of hard-svm and our assumption on the distribution we have that ws h with probability and that lsws therefore using theorem we have that ldws lsws m m remark theorem implies that the sample complexity of hard-svm grows like using a more delicate analysis and the separability assumption it is possible to improve the bound to an order of the bound in the preceding theorem depends on which is unknown. in the following we derive a bound that depends on the norm of the output of svm hence it can be calculated from the training set itself. the proof is similar to the derivation of bounds for structure risk minimization theorem assume that the conditions of theorem hold. then with probability of at least over the choice of s dm we have that dy p m ln m proof for any integer i let bi hi bi and let i fix i then using theorem we have that with probability of at least i m i m w hi ldw lsw applying the union bound and i we obtain that with probability of at least this holds for all i. therefore for all w if we let i then w hi bi and ldw lsw m therefore i i lsw m m m in particular it holds for ws which concludes our proof. rademacher complexities remark note that all the bounds we have derived do not depend on the dimension of w. this property is utilized when learning svm with kernels where the dimension of w can be extremely large. generalization bounds for predictors with low norm in the previous section we derived generalization bounds for linear predictors with an constraint. in this section we consider the following general constraint formulation. let h b be our hypothesis class and let z x y be the examples domain. assume that the loss function h z r is of the same form as in equation with r y r being w.r.t. its first argument. the following theorem bounds the generalization error of all predictors in h using their empirical error. theorem suppose that d is a distribution over x y such that with probability we have that r. let h rd b and let h z r be a loss function of the form given in equation such that for all y y a y is an function and such that maxa brbr y c. then for any with probability of at least over the choice of an i.i.d. sample of size m m w h ldw lsw br m c proof the proof is identical to the proof of theorem while relying on lemma instead of relying on lemma it is interesting to compare the two bounds given in theorem and theorem apart from the extra logd factor that appears in theorem both bounds look similar. however the parameters b r have different meanings in the two bounds. in theorem the parameter b imposes an constraint on w and the parameter r captures a low assumption on the instances. in contrast in theorem the parameter b imposes an constraint on w is stronger than an constraint while the parameter r captures a low assumption on the instance is weaker than a low assumption. therefore the choice of the constraint should depend on our prior knowledge of the set of instances and on prior assumptions on good predictors. bibliographic remarks the use of rademacher complexity for bounding the uniform convergence is due to panchenko bartlett mendelson bartlett mendelson for additional reading see for example boucheron bousquet lugosi bartlett bousquet mendelson bibliographic remarks our proof of the concentration lemma is due to kakade and tewari lecture notes. kakade sridharan tewari gave a unified framework for deriving bounds on the rademacher complexity of linear classes with respect to different assumptions on the norms. covering numbers in this chapter we describe another way to measure the complexity of sets which is called covering numbers. covering definition let a rm be a set of vectors. we say that a is r-covered by a set with respect to the euclidean metric if for all a a there exists with r. we define by n a the cardinality of the smallest that r-covers a. example suppose that a rm let c maxa a and as sume that a lies in a d-dimensional subspace of rm. then n a drd. to see this let vd be an orthonormal basis of the subspace. then any ivi with c. let r and consider the set a a can be written as a i c c c c given a a s.t. a ivi i i d then r and therefore is an r-cover of a. hence ivi with c there exists such that d. i i r d n a choose r properties the following lemma is immediate from the definition. lemma for any a rm scalar c and vector rm we have r n a a a n a. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning from covering to rademacher complexity via chaining next we derive a contraction principle. lemma for each i let i r r be a function namely for all r we have i i for a rm let denote the vector mam. let a a a. then n r a n a. proof define b a. let be an r-cover of a and define then for all a a there exists with r. so iai hence is an r-cover of b. i i from covering to rademacher complexity via chaining the following lemma bounds the rademacher complexity of a based on the covering numbers n a. this technique is called chaining and is attributed to dudley. lemma let c min a maxa a then for any integer m ra c m m c m logn k a. proof let a be a minimizer of the objective function given in the definition of c. on the basis of lemma we can analyze the rademacher complexity assuming that a consider the set and note that it is a c-cover of a. let bm be sets such that each bk corresponds to a minimal k-cover of a. let a argmaxa if there is more than one maximizer choose one in an arbitrary way and if a maximizer does not exist choose a such that a is close enough to the supremum. note that a is a function of for each k let bk be the nearest neighbor of a in bk bk is also a function of using the triangle inequality bk a bk c k c k. for each k define the set bk a bk bk c k. covering numbers we can now write ra m m e a bm a bm bk bk e m sup a bk m m and bm c m the first summand is at most since m m additionally by massart lemma c m c k m e sup a bk therefore logn k m ra c m m k a m c k logn k a. as a corollary we obtain the following lemma assume that there are such that for any k we have logn k a k. then ra m k proof the bound follows from lemma by taking m and noting that k and such that c maxa a we have shown that n a fore for any k example consider a set a which lies in a d dimensional subspace of rm there d r logn k a d log d d d k d d d k. d logd m hence lemma yields ra m d d d o bibliographic remarks bibliographic remarks the chaining technique is due to dudley for an extensive study of covering numbers as well as other complexity measures that can be used to bound the rate of uniform convergence we refer the reader to bartlet proof of the fundamental theorem of learning theory in this chapter we prove theorem from chapter we remind the reader the conditions of the theorem which will hold throughout this chapter h is a hypothesis class of functions from a domain x to the loss function is the loss and vcdimh d we shall prove the upper bound for both the realizable and agnostic cases and shall prove the lower bound for the agnostic case. the lower bound for the realizable case is left as an exercise. the upper bound for the agnostic case for the upper bound we need to prove that there exists c such that h is agnostic pac learnable with sample complexity mh c d we will prove the slightly looser bound mh c d logd the tighter bound in the theorem statement requires a more involved proof in which a more careful analysis of the rademacher complexity using a technique called chaining should be used. this is beyond the scope of this book. to prove equation it suffices to show that applying the erm with a sample size m loged yields an for h. we prove this result on the basis of theorem let ym be a classification training set. recall that the sauershelah lemma tells us that if vcdimh d then log hxm h h e m d denote a h h. this clearly implies that e m d understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning the lower bound for the agnostic case combining this with lemma we obtain the following bound on the rademacher complexity ra logemd m using theorem we obtain that with probability of at least for every h h we have that ldh lsh logemd m m repeating the previous argument for minus the zero-one loss and applying the union bound we obtain that with probability of at least for every h h it holds that lsh logemd m m logemd m to ensure that this is smaller than we need m logm loged using lemma a sufficient condition for the inequality to hold is that m log loged the lower bound for the agnostic case here we prove that there exists c such that h is agnostic pac learnable with sample complexity mh c d we will prove the lower bound in two parts. first we will show that m and second we will show that for every we have that m these two bounds will conclude the proof. showing that m and any we have that m we first show that for any to do so we show that for m h is not learnable. choose one example that is shattered by h. that is let c be an example such proof of the fundamental theorem of learning theory that there are h h h for which hc and h define two distributions d and d such that for b we have dbx y if x c otherwise. that is all the distribution mass is concentrated on two examples and where the probability of b is and the probability of b is b let a be an arbitrary algorithm. any training set sampled from db has the form s ym. therefore it is fully characterized by the vector y ym upon receiving a training set s the algorithm a returns a hypothesis h x since the error of a w.r.t. db only depends on hc we can think of a as a mapping from into therefore we denote by ay the value in corresponding to the prediction of hc where h is the hypothesis that a outputs upon receiving the training set s ym. note that for any hypothesis h we have ldb hcb in particular the bayes optimal hypothesis is hb and ldb ldb ayb if ay b otherwise. fix a. for b let y b ay b. the distribution db induces a probability pb over hence p ldb dby b y denote n yi and n n note that for any y n we have py p and for any y n we have p py. the lower bound for the agnostic case therefore p p max b max b y y y y n p ldb p y n p exp py y n y n next note that y n p y n y n p y n py and both values are the probability that a binomial random variable will have value greater than using lemma this probability is lower bounded by exp where we used the assumption that it follows that if m then there exists b such that p ldb where the last inequality follows by standard algebraic manipulations. this concludes our proof. let and note that showing that m we shall now prove that for every we will construct a family of distributions as follows. first let c cd be a set of d instances which are shattered by h. second for each vector bd define a distribution db such that we have that m d if i x ci otherwise. dbx y that is to sample an example according to db we first sample an element ci c uniformly at random and then set the label to be bi with probability or bi with probability it is easy to verify that the bayes optimal predictor for db is the hypothesis proof of the fundamental theorem of learning theory h h such that hci bi for all i and its error is any other function f x it is easy to verify that in addition for ldb f bi d f bi d therefore ldb min h h ldb f bi d next fix some learning algorithm a. as in the proof of the no-free-lunch theorem we have that max dbb e s dm b ldb min h h ldb ldb min asci bi h h ldb d e dbb u e s dm b e dbb u e s dm b d e dbb u e s dm b d where the first equality follows from equation in addition using the definition of db to sample s db we can first sample jm u set xr cji and finally sample yr such that pyr bji let us simplify the notation and use y b to denote sampling according to py b therefore the right-hand side of equation equals e j u e b u e ryr bjr we now proceed in two steps. first we show that among all learning algorithms a the one which minimizes equation hence also equation is the maximum-likelihood learning rule denoted am l. formally for each i am lsci is the majority vote among the set r xr ci. second we lower bound equation for am l. lemma among all algorithms equation is minimized for a being the maximum-likelihood algorithm am l defined as i am lsci sign yr proof fix some j note that given j and y the training set s is fully determined. therefore we can write aj y instead of as. let us also fix i denote b i the sequence bi bm. also for any rxrci the lower bound for the agnostic case y let yi denote the elements of y corresponding to indices for which jr i and let y i be the rest of the elements of y. we have b u ryr bjr e e b i u bi y e y i p i yi bi e b i u p ib i p the sum within the parentheses is minimized when aj yci is the maximizer of p over bi which is exactly the maximum-likelihood rule. repeating the same argument for all i we conclude our proof. fix i. for every j let nij jt i be the number of instances in which the instance is ci. for the maximum-likelihood rule we have that the quantity e b u e ryr bjr is exactly the probability that a binomial random variable will be larger than using lemma and the assumption we have that e p we have thus shown that d e j u b u e e ryr bjr e e j u e j u where in the last inequality we used the inequality e a a. since the square root function is concave we can apply jensen s inequality to obtain that the above is lower bounded by j u nij e proof of the fundamental theorem of learning theory as long as m d this term would be larger than in summary we have shown that if m d then for any algorithm there exists a distribution such that e ldas min h h ldh s dm minh h ldh and note that finally let equation therefore using lemma we get that pldas min h h ldh p e choosing we conclude that if m d we will have ldas minh h ldh then with probability of at least the upper bound for the realizable case here we prove that there exists c such that h is pac learnable with sample complexity mh c d we do so by showing that for m c d erm rule. we prove this claim based on the notion of definition let x be a domain. s x is an for h with respect to a distribution d over x if h is learnable using the h h dh h s theorem let h with vcdimh d. fix and let m log log then with probability of at least over a choice of s dm we have that s is an for h. proof let b x m h hdh h s be the set of sets which are not we need to bound ps b. define t x m h hdh h s h the upper bound for the realizable case claim ps b ps t proof of claim since s and t are chosen independently we can write e e ps t e s dm t dm note that t implies s b and therefore b which gives ps t e s dm e s dm e t dm b e t dm b fix some s. then either b or s b and then hs such that dhs and s it follows that a sufficient condition for t is that hs therefore whenever s b we have e t dm p t dm hs but since we now assume s b we know that dhs therefore hs is a binomial random variable with parameters of success for a single try and m of tries. chernoff s inequality implies pt hs m thus pt hs combining all the preceding we conclude the proof of claim pt hs m pt hs m m e e m e e d claim ps t e proof of claim to simplify notation let and for a sequence a let xm. using the definition of we get that pa e e a a max h h max h h a a now let us define by ha the effective number of different hypotheses on a namely ha a h h it follows that pa e e a a max h ha h ha a a let j m. for any j j and a define aj xjm. since the elements of a are chosen i.i.d. we have that for any j j and any function f it holds that ea proof of the fundamental theorem of learning theory ea aj. since this holds for any j it also holds for the expectation of j chosen at random from j. in particular it holds for the function f h ha a we therefore obtain that pa e a e a e j j h ha a h ha a e j j now fix some a s.t. a then ej is the probability that when choosing m balls from a bag with at least red balls we will never choose a red ball. this probability is at most e we therefore get that pa e a h ha e e e a using the definition of the growth function we conclude the proof of claim completing the proof by sauer s lemma we know that combining this with the two claims we obtain that ps b e we would like the right-hand side of the inequality to be at most that is e rearranging we obtain the requirement m logm using lemma a sufficient condition for the preceding to hold is that a sufficient condition for this is that d m log log log m log log and this concludes our proof. the upper bound for the realizable case from to pac learnability theorem let h be a hypothesis class over x with vcdimh d. let d be a distribution over x and let c h be a target hypothesis. fix and let m be as defined in theorem then with probability of at least over a choice of m i.i.d. instances from x with labels according to c we have that any erm hypothesis has a true error of at most proof define the class hc h h h where h c h. it is note that ldh c. therefore for any h h with ldh we have that c s which implies that h cannot be an erm hypothesis which easy to verify that if some a x is shattered by h then it is also shattered by hc and vice versa. hence vcdimh vcdimhc. therefore using theorem we know that with probability of at least the sample s is an for hc. concludes our proof. multiclass learnability in chapter we have introduced the problem of multiclass categorization in which the goal is to learn a predictor h x in this chapter we address pac learnability of multiclass predictors with respect to the loss. as in chapter the main goal of this chapter is to characterize which classes of multiclass hypotheses are learnable in the ticlass pac model. quantify the sample complexity of such hypothesis classes. in view of the fundamental theorem of learning theory it is natural to seek a generalization of the vc dimension to multiclass hypothesis classes. in section we show such a generalization called the natarajan dimension and state a generalization of the fundamental theorem based on the natarajan dimension. then we demonstrate how to calculate the natarajan dimension of several important hypothesis classes. recall that the main message of the fundamental theorem of learning theory is that a hypothesis class of binary classifiers is learnable respect to the loss if and only if it has the uniform convergence property and then it is learnable by any erm learner. in chapter exercise we have shown that this equivalence breaks down for a certain convex learning problem. the last section of this chapter is devoted to showing that the equivalence between learnability and uniform convergence breaks down even in multiclass problems with the loss which are very similar to binary classification. indeed we construct a hypothesis class which is learnable by a specific erm learner but for which other erm learners might fail and the uniform convergence property does not hold. the natarajan dimension in this section we define the natarajan dimension which is a generalization of the vc dimension to classes of multiclass predictors. throughout this section let h be a hypothesis class of multiclass predictors namely each h h is a function from x to understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning the multiclass fundamental theorem to define the natarajan dimension we first generalize the definition of shat tering. definition version we say that a set c x is shattered by h if there exist two functions c such that for every x c for every b c there exists a function h h such that x b hx and x c b hx definition dimension the natarajan dimension of h denoted ndimh is the maximal size of a shattered set c x it is not hard to see that in the case that there are exactly two classes ndimh vcdimh. therefore the natarajan dimension generalizes the vc dimension. we next show that the natarajan dimension allows us to generalize the fundamental theorem of statistical learning from binary classification to multiclass classification. the multiclass fundamental theorem theorem multiclass fundamental theorem there exist absolute constants such that the following holds. for every hypothesis class h of functions from x to such that the natarajan dimension of h is d we have h has the uniform convergence property with sample complexity d log d much h is agnostic pac learnable with sample complexity d mh d log d kd h is pac learnable realizability with sample complexity d mh on the proof of theorem the lower bounds in theorem can be deduced by a reduction from the binary fundamental theorem exercise the upper bounds in theorem can be proved along the same lines of the proof of the fundamental theorem for binary classification given in chapter exercise the sole ingredient of that proof that should be modified in a nonstraightforward manner is sauer s lemma. it applies only to binary classes and therefore must be replaced. an appropriate substitute is natarajan s lemma multiclass learnability lemma the proof of natarajan s lemma shares the same spirit of the proof of sauer s lemma and is left as an exercise exercise calculating the natarajan dimension in this section we show how to calculate estimate the natarajan dimension of several popular classes some of which were studied in chapter as these calculations indicate the natarajan dimension is often proportional to the number of parameters required to define a hypothesis. one-versus-all based classes in chapter we have seen two reductions of multiclass categorization to binary classification one-versus-all and all-pairs. in this section we calculate the natarajan dimension of the one-versus-all method. recall that in one-versus-all we train for each label a binary classifier that distinguishes between that label and the rest of the labels. this naturally suggests considering multiclass hypothesis classes of the following form. let hbin be a binary hypothesis class. for every h hk define t h x by t hx argmax i hix. if there are two labels that maximize hix we choose the smaller one. also let hovak bin h h what should be the natarajan dimension of hovak intuitively to specify a hypothesis in hbin we need d vcdimhbin parameters. to specify a hypothesis in hovak we need to specify k hypotheses in hbin. therefore kd parameters should suffice. the following lemma establishes this intuition. bin bin lemma if d vcdimhbin then ndimhovak bin log proof let c x be a shattered set. by the definition of shattering multiclass hypotheses is determined by using k hypothe ses from hbin. on the other hand each hypothesis in hovak bin bin c bin c calculating the natarajan dimension by sauer s lemma we conclude that bin c the proof follows by taking the logarithm and applying lemma how tight is lemma it is not hard to see that for some classes ndimhovak bin can be much smaller than dk exercise however there are several natural binary classes hbin halfspaces for which ndimhovak exercise bin general multiclass-to-binary reductions the same reasoning used to establish lemma can be used to upper bound the natarajan dimension of more general multiclass-to-binary reductions. these reductions train several binary classifiers on the data. then given a new instance they predict its label by using some rule that takes into account the labels predicted by the binary classifiers. these reductions include one-versusall and all-pairs. suppose that such a method trains l binary classifiers from a binary class hbin and r is the rule that determines the label according to the predictions of the binary classifiers. the hypothesis class corresponding to this method can be defined as follows. for every h hl define r h x by finally let r hx hlx. hr bin h h similarly to lemma it can be proven that lemma if d vcdimhbin then ndimhr bin l d log d the proof is left as exercise linear multiclass predictors next we consider the class of linear multiclass predictors section let x rd be some class-sensitive feature mapping and let h x argmax i w rd each hypothesis in h is determined by d parameters namely a vector w rd. therefore we would expect that the natarajan dimension would be upper bounded by d. indeed multiclass learnability theorem ndimh d proof let c x be a shattered set and let c be the two functions that witness the shattering. we need to show that d. for every x c let we claim that the set def x c consists of elements is one to one and is shattered by the binary hypothesis class of homogeneous linear separators on rd h w rd. since vcdimh d it will follow that d as required. to establish our claim it is enough to show that indeed given a subset b c by the definition of shattering there exists hb h for which x b hbx and x c b hbx let wb rd be a vector that defines hb. we have that for every x b similarly for every x c b it follows that the hypothesis gb h defined by the same w rd label the points in by and the points in b by since this holds for every b c we obtain that and which concludes our proof. the theorem is tight in the sense that there are mappings for which ndimh for example this is true for the multivector construction section and the bibliographic remarks at the end of this chapter. we therefore conclude corollary let x rn and let x rnk be the class sensitive feature mapping for the multi-vector construction y ry rk yn rn xn let h be as defined in equation then the natarajan dimension of h satisfies ndimh kn. on good and bad erms in this section we present an example of a hypothesis class with the property that not all erms for the class are equally successful. furthermore if we allow an infinite number of labels we will also obtain an example of a class that is on good and bad erms learnable by some erm but other erms will fail to learn it. clearly this also implies that the class is learnable but it does not have the uniform convergence property. for simplicity we consider only the realizable case. the class we consider is defined as follows. the instance space x will be any finite or countable set. let pf be the collection of all finite and cofinite subsets of x is for each a pf either a or x a must be finite. instead of the label set is y pf where is some special label. for every a pf define ha x y by hax a x a x a finally the hypothesis class we take is h a pf let a be some erm algorithm for h. assume that a operates on a sample labeled by ha h. since ha is the only hypothesis in h that might return the label a if a observes the label a it knows that the learned hypothesis is ha and as an erm must return it that in this case the error of the returned hypothesis is therefore to specify an erm we should only specify the hypothesis it returns upon receiving a sample of the form s we consider two erms the first agood is defined by agoods h that is it outputs the hypothesis which predicts for every x x the second erm abad is defined by abads the following claim shows that the sample complexity of abad is about larger than the sample complexity of agood. this establishes a gap between different erms. if x is infinite we even obtain a learnable class that is not learnable by every erm. examples sampled according to d and labeled by claim let d a distribution over x and ha h. let s be an i.i.d. sample consisting of m ha. then with probability of at least the hypothesis returned by agood will have an error of at most there exists a constant a such that for every a there exists a distribution d over x and ha h such that the following holds. the hypothesis returned by abad upon receiving a sample of size m sampled according to d and labeled by ha will have error with probability e multiclass learnability proof let d be a distribution over x and suppose that the correct labeling is ha. for any sample agood returns either h or ha. if it returns ha then its true error is zero. thus it returns a hypothesis with error only if all the m examples in the sample are from x a while the error of h ldh pda is assume m then the probability of the latter event is no more than e this establishes item next we prove item we restrict the proof to the case that d log the proof for infinite x is similar. suppose that x xd let a be small enough such that e for every a and fix some a. define a distribution on x by setting and for all i d pxi d suppose that the correct hypothesis is h and let the sample size be m. clearly the hypothesis returned by abad will err on all the examples from x which are not in the sample. by chernoff s bound if m d then with probability e examples from x thus the returned hypothesis will have error the sample will include no more than d the conclusion of the example presented is that in multiclass classification the sample complexity of different erms may differ. are there good erms for every hypothesis class? the following conjecture asserts that the answer is yes. conjecture the realizable sample complexity of every hypothesis class h is x ndimh mh o we emphasize that the o notation may hide only poly-log factors of and ndimh but no factor of k. bibliographic remarks the natarajan dimension is due to natarajan that paper also established the natarajan lemma and the generalization of the fundamental theorem. generalizations and sharper versions of the natarajan lemma are studied in haussler long ben-david cesa-bianchi haussler long defined a large family of notions of dimensions all of which generalize the vc dimension and may be used to estimate the sample complexity of multiclass classification. the calculation of the natarajan dimension presented here together with calculation of other classes can be found in daniely et al. the example of good and bad erms as well as conjecture are from daniely et al. exercises exercises let d k show that there exists a binary hypothesis hbin of vc dimension d such that ndimhovak d. bin prove lemma prove natarajan s lemma. hint fix some x for i j denote by hij all the functions f x that can be extended to a function in h both by defining f i and by defining f j. show that and use induction. adapt the proof of the binary fundamental theorem and natarajan s lemma to prove that for some universal constant c and for every hypothesis class of natarajan dimension d the agnostic sample complexity of h is d kd mh c prove that for some universal constant c and for every hypothesis class of natarajan dimension d the agnostic sample complexity of h is mh c d hint deduce it from the binary fundamental theorem. let h be the binary hypothesis class of halfspaces in rd. the goal of this exercise is to prove that ndimhovak let hdiscrete be the class of all functions f for which there exists some such that for every j i f j while i f j show that hdiscrete can be realized by h. that is show that there exists show that ndimhovak discrete a mapping rd such that hdiscrete h h hint you can take j to be the vector whose jth coordinate is whose last coordinate is i and the rest are zeros. conclude that ndimhovak compression bounds throughout the book we have tried to characterize the notion of learnability using different approaches. at first we have shown that the uniform convergence property of a hypothesis class guarantees successful learning. later on we introduced the notion of stability and have shown that stable algorithms are guaranteed to be good learners. yet there are other properties which may be sufficient for learning and in this chapter and its sequel we will introduce two approaches to this issue compression bounds and the pac-bayes approach. in this chapter we study compression bounds. roughly speaking we shall see that if a learning algorithm can express the output hypothesis using a small subset of the training set then the error of the hypothesis on the rest of the examples estimates its true error. in other words an algorithm that can compress its output is a good learner. compression bounds to motivate the results let us first consider the following learning protocol. first we sample a sequence of k examples denoted t on the basis of these examples we construct a hypothesis denoted ht now we would like to estimate the performance of ht so we sample a fresh sequence of m k examples denoted v and calculate the error of ht on v since v and t are independent we immediately get the following from bernstein s inequality lemma lemma assume that the range of the loss function is then p ldht lv to derive this bound all we needed was independence between t and v therefore we can redefine the protocol as follows. first we agree on a sequence of k indices i ik then we sample a sequence of m examples s zm. now define t si zik and define v to be the rest of the examples in s. note that this protocol is equivalent to the protocol we defined before hence lemma still holds. applying a union bound over the choice of the sequence of indices we obtain the following theorem. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning compression bounds theorem let k be an integer and let b z k h be a mapping from sequences of k examples to the hypothesis class. let m be a training set size and let a z m h be a learning rule that receives a training sequence s of size m and returns a hypothesis such that as zik for some ik let v j ik be the set of examples which were not selected for defining as. then with probability of at least over the choice of s we have ldas lv lv logm m logm m p proof for any i let hi zik let n m k. combining lemma with the union bound we have i s.t. ldhi lv p ldhi lv n n n n i mk denote mk using the assumption k which implies that n m k the above implies that with probability of at least we have that ldas lv lv logm m logm m which concludes our proof. as a direct corollary we obtain corollary assuming the conditions of theorem and further assuming that lv then with probability of at least over the choice of s we have ldas logm m these results motivate the following definition scheme let h be a hypothesis class of definition functions from x to y and let k be an integer. we say that h has a compression scheme of size k if the following holds for all m there exists a z m and b z k h such that for all h h if we feed any training set of the form hxm into a and then feed hxik into b where ik is the output of a then the output of b denoted satisfies it is possible to generalize the definition for unrealizable sequences as follows. compression bounds definition scheme for unrealizable sequences let h be a hypothesis class of functions from x to y and let k be an integer. we say that h has a compression scheme of size k if the following holds for all m there exists a z m and b z k h such that for all h h if we feed any training set of the form ym into a and then feed yik into b where ik is the output of a then the output of b denoted satisfies lsh. the following lemma shows that the existence of a compression scheme for the realizable case also implies the existence of a compression scheme for the unrealizable case. lemma let h be a hypothesis class for binary classification and assume it has a compression scheme of size k in the realizable case. then it has a compression scheme of size k for the unrealizable case as well. proof consider the following scheme first find an erm hypothesis and denote it by h. then discard all the examples on which h errs. now apply the realizable compression scheme on the examples that have not been removed. the output of the realizable compression scheme denoted must be correct on the examples that have not been removed. since h errs on the removed examples it follows that the error of cannot be larger than the error of h hence is also an erm hypothesis. examples in the examples that follows we present compression schemes for several hypothesis classes for binary classification. in light of lemma we focus on the realizable case. therefore to show that a certain hypothesis class has a compression scheme it is necessary to show that there exist a b and k for which axis aligned rectangles note that this is an uncountable infinite class. we show that there is a simple compression scheme. consider the algorithm a that works as follows for each dimension choose the two positive examples with extremal values at this dimension. define b to be the function that returns the minimal enclosing rectangle. then for k we have that in the realizable case lsbas halfspaces let x rd and consider the class of homogenous halfspaces w rd. examples a compression scheme w.l.o.g. assume all labels are positive replace xi by yixi. the compression scheme we propose is as follows. first a finds the vector w which is in the convex hull of xm and has minimal norm. then it represents it as a convex combination of d points in the sample will be shown later that this is always possible. the output of a are these d points. the algorithm b receives these d points and set w to be the point in their convex hull of minimal norm. next we prove that this indeed is a compression sceme. since the data is linearly separable the convex hull of xm does not contain the origin. consider the point w in this convex hull closest to the origin. is a unique point which is the euclidean projection of the origin onto this convex hull. we claim that w separates the to see this assume by contradiction that for some i. take xi for then is also in the convex hull and which leads to a contradiction. we have thus shown that w is also an erm. finally since w is in the convex hull of the examples we can apply caratheodory s theorem to obtain that w is also in the convex hull of a subset of d points of the polygon. furthermore the minimality of w implies that w must be on a face of the polygon and this implies it can be represented as a convex combination of d points. it remains to show that w is also the projection onto the polygon defined by the d points. but this must be true on one hand the smaller polygon is a subset of the larger one hence the projection onto the smaller cannot be smaller in norm. on the other hand w itself is a valid solution. the uniqueness of projection concludes our proof. separating polynomials let x rd and consider the class x signpx where p is a degree r polynomial. it can be shown that w is the direction of the max-margin solution. compression bounds note that px can be rewritten as where the elements of are all the monomials of x up to degree r. therefore the problem of constructing a compression scheme for px reduces to the problem of constructing a compression scheme for halfspaces in where odr. separation with margin suppose that a training set is separated with margin the perceptron algorithm guarantees to make at most updates before converging to a solution that makes no mistakes on the entire training set. hence we have a compression scheme of size k bibliographic remarks compression schemes and their relation to learning were introduced by littlestone warmuth as we have shown if a class has a compression scheme then it is learnable. for binary classification problems it follows from the fundamental theorem of learning that the class has a finite vc dimension. the other direction namely whether every hypothesis class of finite vc dimension has a compression scheme of finite size is an open problem posed by manfred warmuth and is still open also floyd warmuth ben-david litman livni simon pac-bayes the minimum description length and occam s razor principles allow a potentially very large hypothesis class but define a hierarchy over hypotheses and prefer to choose hypotheses that appear higher in the hierarchy. in this chapter we describe the pac-bayesian approach that further generalizes this idea. in the pac-bayesian approach one expresses the prior knowledge by defining prior distribution over the hypothesis class. pac-bayes bounds as in the mdl paradigm we define a hierarchy over hypotheses in our class h. now the hierarchy takes the form of a prior distribution over h. that is we assign a probability density if h is continuous p for each h h and refer to p as the prior score of h. following the bayesian reasoning approach the output of the learning algorithm is not necessarily a single hypothesis. instead the learning process defines a posterior probability over h which we denote by q. in the context of a supervised learning problem where h contains functions from x to y one can think of q as defining a randomized prediction rule as follows. whenever we get a new instance x we randomly pick a hypothesis h h according to q and predict hx. we define the loss of q on an example z to be z def e h q z. by the linearity of expectation the generalization loss and training loss of q can be written as ldq def e h q and lsq def e h q the following theorem tells us that the difference between the generalization loss and the empirical loss of a posterior q is bounded by an expression that depends on the kullback-leibler divergence between q and the prior distribution p the kullback-leibler is a natural measure of the distance between two distributions. the theorem suggests that if we would like to minimize the generalization loss of q we should jointly minimize both the empirical loss of q and the kullback-leibler distance between q and the prior distribution. we will understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning pac-bayes later show how in some cases this idea leads to the regularized risk minimization principle. theorem let d be an arbitrary distribution over an example domain z. let h be a hypothesis class and let h z be a loss function. let p be a prior distribution over h and let then with probability of at least over the choice of an i.i.d. training set s zm sampled according to d for all distributions q over h such that depend on s we have ldq lsq dqp ln m where dqp def e h q is the kullback-leibler divergence. proof for any function f using markov s inequality p e esef s e p s let ldh lsh. we will apply equation with the function f sup q e h q dqp we now turn to bound esef the main trick is to upper bound f by using an expression that does not depend on q but rather depends on the prior probability p to do so fix some s and note that from the definition of dqp we get that for all q e h q dqp e h q ln e h q ln e h p p p where the inequality follows from jensen s inequality and the concavity of the log function. therefore e e s s e h p the advantage of the expression on the right-hand side stems from the fact that we can switch the order of expectations p is a prior that does not depend on s which yields e e h p s e s bibliographic remarks next we claim that for all h we have hoeffding s inequality tells us that m. to do so recall that e p s this implies that equation and plugging into equation we get m exercise combining this with p s m e denote the right-hand side of the above thus lnm and we therefore obtain that with probability of at least we have that for all q e h q dqp lnm rearranging the inequality and using jensen s inequality again function is convex we conclude that e h q e h q lnm dqp remark the pac-bayes bound leads to the following learning rule given a prior p return a posterior q that minimizes the function lsq dqp ln m this rule is similar to the regularized risk minimization principle. that is we jointly minimize the empirical loss of q on the sample and the kullback-leibler distance between q and p bibliographic remarks pac-bayes bounds were first introduced by mcallester see also mcallester seeger langford shawe-taylor langford exercises let x be a random variable that satisfies px e m. prove that pac-bayes suppose that h is a finite hypothesis class set the prior to be uniform over h and set the posterior to be qhs for some hs and qh for all other h h. show that ldhs lsh lnh lnm compare to the bounds we derived using uniform convergence. derive a bound similar to the occam bound given in chapter using the pac-bayes bound appendix a technical lemmas lemma let a then x loga x a logx. it follows that a necessary condition for the inequality x a logx to hold is that x loga. e the inequality x a logx holds unconproof first note that for a e. ditionally and therefore the claim is trivial. from now on assume that a consider the function f x a logx. the derivative is ax. thus for x a the derivative is positive and the function increases. in addition f loga loga a loga loga a loga a loga a loga a loga. since a loga for all a the proof follows. lemma let a and b then x x a logxb. it suffices to prove that x implies that both x proof logx and x since we assume a we clearly have that x in addition since b we have that x which using lemma implies that x logx. this concludes our proof. lemma let x be a random variable and r be a scalar and assume that there exists a such that for all t we have px t then ex a. we have that ex is at with the assumption in the lemma we get that ex proof for all i denote ti a i. since ti is monotonically increasing ti px ti combining this ie the proof now follows from the inequalities ie ie xe dx lemma let x be a random variable and r be a scalar and assume that there exists a and b e such that for all t we have px t e then ex understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning technical lemmas proof for all i denote ti a since ti is monotonically increasing we have that ex ti px ti using the assumption in the lemma we have ti px ti a b logb e logb logb a b a b a b a b xe dx dy ye dy logb a bb a. combining the preceding inequalities we conclude our proof. lemma let m d be two positive integers such that d m then k e m d proof we prove the claim by induction. for d the left-hand side equals m while the right-hand side equals em hence the claim is true. assume that the claim holds for d and let us prove it for d by the induction assumption we have k d e m e m em d d d m mm d d d d e m e technical lemmas d dded d using stirling s approximation we further have that d d d e m e m e m e m e m e m e m d d d d d d e d m d dd d d d m e m e m e m e m em em em m d d d d d d d d d d d d e where in the last inequality we used the assumption that d m on the other hand which proves our inductive argument. lemma for all a r we have ea e a proof observe that therefore and ea ea e a an n! n! observing that n! for every n we conclude our proof. appendix b measure concentration let zm be an i.i.d. sequence of random variables and let be their mean. the strong law of large numbers states that when m tends to infinity the empirical average zi converges to the expected value with probability m measure concentration inequalities quantify the deviation of the empirical average from the expectation when m is finite. markov s inequality a we start with an inequality which is called markov s inequality. let z be a nonnegative random variable. the expectation of z can be written as follows ez pz xdx. a since pz x is monotonically nonincreasing we obtain a ez pz xdx pz adx a pz a. rearranging the inequality yields markov s inequality a pz a ez a for random variables that take value in we can derive from markov s inequality the following. lemma let z be a random variable that takes values in assume that ez then for any a pz a a a this also implies that for every a pz a a a a. proof let y z. then y is a nonnegative random variable with ey ez applying markov s inequality on y we obtain pz a z a py a ey a a understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning chebyshev s inequality therefore pz a a a a chebyshev s inequality applying markov s inequality on the random variable we obtain chebyshev s inequality a pz ez a pz varz where varz ez is the variance of z. consider the random variable m zi. since zm are i.i.d. it is easy to verify that var zi m m applying chebyshev s inequality we obtain the following lemma let zm be a sequence of i.i.d. random variables and assume that and then for any with probability of at least we have m m zi a m p zi m m proof applying chebyshev s inequality we obtain that for all a the proof follows by denoting the right-hand side and solving for a. the deviation between the empirical average and the mean given previously decreases polynomially with m. it is possible to obtain a significantly faster decrease. in the sections that follow we derive bounds that decrease exponentially fast. chernoff s bounds pi and pzi pi. let p let zm be independent bernoulli variables where for every i pzi zi. using the pi and let z measure concentration monotonicity of the exponent function and markov s inequality we have that for every t pz petz eetz next etzi i eetzi i zi e piet eetz i piet epiet i i i i e eet by independence using x ex combining the above with equation and choosing t we obtain i pzi pi and pzi pi. let p lemma let zm be independent bernoulli variables where for every zi. pi and let z then for any pz e h p where h using the inequality ha we obtain lemma using the notation of lemma we also have pz e p for the other direction we apply similar calculations pz p z pe tz e ee tz e hoeffding s inequality and e tzi ee tz ee ee tzi i zi e pie t i i i by independence epie t using x ex i ee t setting t yields pz it is easy to verify that h h and hence e p p e ph lemma using the notation of lemma we also have pz e ph e ph e p hoeffding s inequality lemma s inequality let zm be a sequence of i.i.d. zi. assume that e z and pa random variables and let z zi b for every i. then for any m m zi p m proof denote xi zi ezi and x i xi. using the monotonicity of the exponent function and markov s inequality we have that for every and m p x pe x e e ee x using the independence assumption we also have ee x e e xim ee xim. by hoeffding s lemma later for every i we have i i ee xim e measure concentration therefore p x e i setting we obtain e e p x e applying the same arguments on the variable x we obtain that p x e the theorem follows by applying the union bound on the two cases. lemma s lemma let x be a random variable that takes values in the interval b and such that ex then for every ee x e proof since f e x is a convex function we have that for every and x b f f setting b x b a yields e x b x b a taking the expectation we obtain that e a x a b a e b. ee x b ex b a e a ex a b a e b b b a e a a b a e b where we used the fact that ex denote h a p a b a and lh hp p peh. then the expression on the right-hand side of the above can be rewritten as elh. therefore to conclude our proof it suffices to show that lh this follows from taylor s theorem using the facts and for all h. bennet s and bernstein s inequalities bennet s and bernsein s inequalities are similar to chernoff s bounds but they hold for any sequence of independent random variables. we state the inequalities without proof which can be found for example in cesa-bianchi lugosi lemma s inequality let zm be independent random variables with zero mean and assume that zi with probability let m ez i bennet s and bernstein s inequalities then for all where zi p e m m ha a a a. by using the inequality ha it is possible to derive the following lemma s inequality let zm be i.i.d. random variables with a zero mean. if for all i pzi m then for all t p zi t exp e z j m application bernstein s inequality can be used to interpolate between the rate we derived for pac learning in the realizable case chapter and the rate we derived for the unrealizable case chapter lemma let h z be a loss function. let d be an arbitrary distribution over z. fix some h. then for any we have p s dm p s dm lsh ldh ldh lsh m m m m proof define random variables m s.t. i zi ldh. note that e i and that e i zi zi ldh where in the last inequality we used the fact that zi and thus zi. applying bernsein s inequality over the i s yields p i t exp exp e j m ldh def measure concentration solving for t yields m ldh t m ldh m ldh i i lsh ldh it follows that with probability of at least since m t m ldh lsh ldh ldh m which proves the first inequality. the second part of the lemma follows in a similar way. slud s inequality let x be a p binomial variable. that is x bility that a normal variable will be greater than or equal the zi where each zi is with probability p and with probability p. assume that p slud s inequality tells us that px is lower bounded by the proba following lemma follows by standard tail bounds for the normal distribution. lemma let x be a p binomial variable and assume that p then exp px concentration of variables let xk be k independent normally distributed random variables. that is for all i xi n the distribution of the random variable x is called x square and the distribution of the random variable z x k k square with k degrees of freedom. clearly ex is called i and ez k. the following lemma states that x k is concentrated around its mean. lemma let z i k. then for all we have pz e and for all we have pz e concentration of variables finally for all proof let us write z p z i where xi n to prove both bounds we use chernoff s bounding method. for the first inequality we first bound for all a ee x we have that where will be specified later. since e a a x ee x ex ex and ex and the fact that using the well known equalities ex a e a we obtain that ee x p z now applying chernoff s bounding method we get that e e z e e x e k e k choose we obtain the first inequality stated in the lemma. for the second inequality we use a known closed form expression for the moment generating function of a k distributed random variable e e z pz e on the basis of the equation and using chernoff s bounding method we have e e ek e where the last inequality occurs because a e a. setting is in by our assumption we obtain the second inequality stated in the lemma. finally the last inequality follows from the first two inequalities and the union bound. appendix c linear algebra basic definitions in this chapter we only deal with linear algebra over finite dimensional euclidean spaces. we refer to vectors as column vectors. given two d dimensional vectors u v rd their inner product is uivi. the euclidean norm the norm is we also use the norm and the norm maxi a subspace of rd is a subset of rd which is closed under addition and scalar multiplication. the span of a set of vectors uk is the subspace containing all vectors of the form iui where for all i i r. a set of vectors u uk is independent if for every i ui is not in the span of ui uk. we say that u spans a subspace v if v is the span of the vectors in u we say that u is a basis of v if it is both independent and spans v. the dimension of v is the size of a basis of v it can be verified that all bases of v have the same size. we say that u is an orthogonal set if for all i j we say that u is an orthonormal set if it is orthogonal and if for every i given a matrix a rnd the range of a is the span of its columns and the null space of a is the subspace of all vectors that satisfy au the rank of a is the dimension of its range. the transpose of a matrix a denoted is the matrix whose j entry equals the i entry of a. we say that a is symmetric if a understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning eigenvalues and eigenvectors eigenvalues and eigenvectors let a rdd be a matrix. a non-zero vector u is an eigenvector of a with a corresponding eigenvalue if au u. ui is an eigenvector of a. furthermore a can be written as a if a rdd is a symmetric matrix of theorem decomposition rank k then there exists an orthonormal basis of rd ud such that each i where each i is the eigenvalue corresponding to the eigenvector ui. this can be written equivalently as a u where the columns of u are the vectors ud and d is a diagonal matrix with dii i and for i j dij finally the number of i which are nonzero is the rank of the matrix the eigenvectors which correspond to the nonzero eigenvalues span the range of a and the eigenvectors which correspond to zero eigenvalues span the null space of a. positive definite matrices a symmetric matrix a rdd is positive definite if all its eigenvalues are positive. a is positive semidefinite if all its eigenvalues are nonnegative. theorem let a rdd be a symmetric matrix. then the following are equivalent definitions of positive semidefiniteness of a all the eigenvalues of a are nonnegative. for every vector u there exists a matrix b such that a singular value decomposition let a rmn be a matrix of rank r. when m n the eigenvalue decomposition given in theorem cannot be applied. we will describe another decomposition of a which is called singular value decomposition or svd for short. unit vectors v rn and u rm are called right and left singular vectors of a with corresponding singular value if av u and v. we first show that if we can find r orthonormal singular vectors with positive singular values then we can decompose a u dv with the columns of u and v containing the left and right singular vectors and d being a diagonal r r matrix with the singular values on its diagonal. linear algebra lemma let a rmn be a matrix of rank r. assume that vr is an orthonormal set of right singular vectors of a ur is an orthonormal set of corresponding left singular vectors of a and r are the corresponding singular values. then a i it follows that if u is a matrix whose columns are the ui s v is a matrix whose columns are the vi s and d is a diagonal matrix with dii i then a u dv adding the vectors vn. define b proof any right singular vector of a must be in the range of the singular value will have to be zero. therefore vr is an orthonormal basis of the range of a. let us complete it to an orthonormal basis of rn by i it suffices to prove that for all i avi bvi. clearly if i r then avi and bvi as well. for i r we have bvi j vi iui avi where the last equality follows from the definition. the next lemma relates the singular values of a to the eigenvalues of and lemma v u are right and left singular vectors of a with singular value iff v is an eigenvector of with corresponding eigenvalue and u is an eigenvector of with corresponding eigenvalue proof suppose that is a singular value of a with v rn being the corresponding right singular vector. then similarly av for the other direction if is an eigenvalue of with v being the corresponding eigenvector then because is positive semidefinite. let u then u and av av v v. singular value decomposition finally we show that if a has rank r then it has r orthonormal singular vectors. lemma let a rmn with rank r. define the following vectors argmax v argmax v vr argmax v ir then vr is an orthonormal set of right singular vectors of a. proof first note that since the rank of a is r the range of a is a subspace of dimension r and therefore it is easy to verify that for all i r let w rnn be an orthonormal matrix obtained by the eigenvalue decomposition of namely w dw with d being a diagonal matrix with we will show that vr are eigenvectors of that correspond to nonzero eigenvalues and hence using lemma it follows that these are also right singular vectors of a. the proof is by induction. for the basis of the induction note that any unit vector v can be written as v w x for x w and note that therefore dw iixi therefore max max iixi the solution of the right-hand side is to set x which implies that is the first eigenvector of since it follows that as required. for the induction step assume that the claim holds for some t r then any v which is orthogonal to vt can be written as v w x with all the first t elements of x being zero. it follows that max i max iixi the solution of the right-hand side is the all zeros vector except this implies that is the column of w finally since it follows that as required. this concludes our proof. linear algebra corollary svd theorem let a rmn with rank r. then a u dv where d is an r r matrix with nonzero singular values of a and the columns of u v are orthonormal left and right singular vectors of a. furtherii is an eigenvalue of the ith column of v is the cormore for all i responding eigenvector of and the ith column of u is the corresponding eigenvector of notes understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning references abernethy j. bartlett p. l. rakhlin a. tewari a. optimal strategies and minimax lower bounds for online convex games in proceedings of the nineteenth annual conference on computational learning theory ackerman m. ben-david s. measures of clustering quality a working set of axioms for clustering in proceedings of neural information processing systems pp. agarwal s. roth d. learnability of bipartite ranking functions in pro ceedings of the annual conference on learning theory pp. agmon s. the relaxation method for linear inequalities canadian journal of mathematics aizerman m. a. braverman e. m. rozonoer l. i. theoretical foundations of the potential function method in pattern recognition learning automation and remote control allwein e. l. schapire r. singer y. reducing multiclass to binary a unifying approach for margin classifiers journal of machine learning research alon n. ben-david s. cesa-bianchi n. haussler d. scale-sensitive dimen sions uniform convergence and learnability journal of the acm anthony m. bartlet p. neural network learning theoretical foundations cambridge university press. baraniuk r. davenport m. devore r. wakin m. a simple proof of the restricted isometry property for random matrices constructive approximation barber d. bayesian reasoning and machine learning cambridge university press. bartlett p. bousquet o. mendelson s. local rademacher complexities annals of statistics bartlett p. l. ben-david s. hardness results for neural network approxi mation problems theor. comput. sci. bartlett p. l. long p. m. williamson r. c. fat-shattering and the learnability of real-valued functions in proceedings of the seventh annual conference on computational learning theory acm pp. bartlett p. l. mendelson s. rademacher and gaussian complexities risk bounds and structural results in annual conference on computational learning theory colt vol. springer berlin pp. understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning references bartlett p. l. mendelson s. rademacher and gaussian complexities risk bounds and structural results journal of machine learning research ben-david s. cesa-bianchi n. haussler d. long p. characterizations of learnability for classes of n-valued functions journal of computer and system sciences ben-david s. eiron n. long p. on the difficulty of approximately maxi mizing agreements journal of computer and system sciences ben-david s. litman a. combinatorial variability of vapnik-chervonenkis classes with applications to sample compression schemes discrete applied mathematics ben-david s. pal d. shalev-shwartz s. agnostic online learning in con ference on learning theory ben-david s. simon h. efficient learning of linear perceptrons advances in neural information processing systems pp. bengio y. learning deep architectures for ai foundations and trends in machine learning bengio y. lecun y. scaling learning algorithms towards ai large-scale kernel machines bertsekas d. nonlinear programming athena scientific. beygelzimer a. langford j. ravikumar p. multiclass classification with filter trees preprint june birkhoff g. three observations on linear algebra revi. univ. nac. tucuman ser a bishop c. m. pattern recognition and machine learning vol. springer new york. blum l. shub m. smale s. on a theory of computation and complexity over the real numbers np-completeness recursive functions and universal machines am. math. soc blumer a. ehrenfeucht a. haussler d. warmuth m. k. occam s razor information processing letters blumer a. ehrenfeucht a. haussler d. warmuth m. k. learnability and the vapnik-chervonenkis dimension journal of the association for computing machinery borwein j. lewis a. convex analysis and nonlinear optimization springer. boser b. e. guyon i. m. vapnik v. n. a training algorithm for optimal margin classifiers in conference on learning theory pp. bottou l. bousquet o. the tradeoffs of large scale learning in nips pp. boucheron s. bousquet o. lugosi g. theory of classification a survey of recent advances esaim probability and statistics bousquet o. concentration inequalities and empirical processes theory ap plied to the analysis of learning algorithms phd thesis ecole polytechnique. bousquet o. elisseeff a. stability and generalization journal of machine learning research boyd s. vandenberghe l. convex optimization cambridge university press. references breiman l. bias variance and arcing classifiers technical report statis tics department university of california at berkeley. breiman l. random forests machine learning breiman l. friedman j. h. olshen r. a. stone c. j. classification and regression trees wadsworth brooks. candes e. the restricted isometry property and its implications for com pressed sensing comptes rendus mathematique candes e. j. compressive sampling in proc. of the int. congress of math. madrid spain candes e. tao t. decoding by linear programming ieee trans. on information theory cesa-bianchi n. lugosi g. prediction learning and games cambridge university press. chang h. s. weiss y. freeman w. t. informative sensing arxiv preprint chapelle o. le q. smola a. large margin optimization of ranking mea sures in nips workshop machine learning for web search collins m. discriminative reranking for natural language parsing in machine learning collins m. discriminative training methods for hidden markov models theory and experiments with perceptron algorithms in conference on empirical methods in natural language processing collobert r. weston j. a unified architecture for natural language processing deep neural networks with multitask learning in international conference on machine learning cortes c. vapnik v. support-vector networks machine learning cover t. behavior of sequential predictors of binary sequences trans. prague conf. information theory statistical decision functions random processes pp. cover t. hart p. nearest neighbor pattern classification information theory ieee transactions on crammer k. singer y. on the algorithmic implementation of multiclass kernel-based vector machines journal of machine learning research cristianini n. shawe-taylor j. an introduction to support vector machines cambridge university press. daniely a. sabato s. ben-david s. shalev-shwartz s. multiclass learn ability and the erm principle in conference on learning theory daniely a. sabato s. shwartz s. s. multiclass learning approaches a theoretical comparison with implications in nips davis g. mallat s. avellaneda m. greedy adaptive approximation jour nal of constructive approximation devroye l. gy orfi l. nonparametric density estimation the l s view wiley. devroye l. gy orfi l. lugosi g. a probabilistic theory of pattern recog nition springer. references dietterich t. g. bakiri g. solving multiclass learning problems via error correcting output codes journal of artificial intelligence research donoho d. l. compressed sensing information theory ieee transactions on dudley r. gine e. zinn j. uniform and universal glivenko-cantelli classes journal of theoretical probability dudley r. m. universal donsker classes and metric entropy annals of prob ability fisher r. a. on the mathematical foundations of theoretical statistics philosophical transactions of the royal society of london. series a containing papers of a mathematical or physical character floyd s. space-bounded learning and the vapnik-chervonenkis dimension in conference on learning theory pp. floyd s. warmuth m. sample compression learnability and the vapnik chervonenkis dimension machine learning frank m. wolfe p. an algorithm for quadratic programming naval res. logist. quart. freund y. schapire r. a decision-theoretic generalization of on-line learning and an application to boosting in european conference on computational learning theory springer-verlag pp. freund y. schapire r. e. large margin classification using the perceptron algorithm machine learning garcia j. koelling r. relation of cue to consequence in avoidance learning foundations of animal behavior classic papers with commentaries gentile c. the robustness of the p-norm algorithms machine learning georghiades a. belhumeur p. kriegman d. from few to many illumination cone models for face recognition under variable lighting and pose ieee trans. pattern anal. mach. intelligence gordon g. regret bounds for prediction problems in conference on learning theory gottlieb l.-a. kontorovich l. krauthgamer r. efficient classification for metric data in conference on learning theory pp. guyon i. elisseeff a. an introduction to variable and feature selection journal of machine learning research special issue on variable and feature selection hadamard j. sur les problemes aux d eriv ees partielles et leur signification physique princeton university bulletin hastie t. tibshirani r. friedman j. the elements of statistical learning springer. haussler d. decision theoretic generalizations of the pac model for neural net and other learning applications information and computation haussler d. long p. m. a generalization of sauer s lemma journal of combinatorial theory series a hazan e. agarwal a. kale s. logarithmic regret algorithms for online convex optimization machine learning references hinton g. e. osindero s. teh y.-w. a fast learning algorithm for deep belief nets neural computation hiriart-urruty j.-b. lemar echal c. convex analysis and minimization al gorithms part fundamentals vol. springer. hsu c.-w. chang c.-c. lin c.-j. a practical guide to support vector classification hyafil l. rivest r. l. constructing optimal binary decision trees is np complete information processing letters joachims t. a support vector method for multivariate performance measures in proceedings of the international conference on machine learning kakade s. sridharan k. tewari a. on the complexity of linear prediction risk bounds margin bounds and regularization in nips karp r. m. reducibility among combinatorial problems springer. kearns m. j. schapire r. e. sellie l. m. toward efficient agnostic learn ing machine learning kearns m. mansour y. on the boosting ability of top-down decision tree learning algorithms in acm symposium on the theory of computing kearns m. ron d. algorithmic stability and sanity-check bounds for leave one-out cross-validation neural computation kearns m. valiant l. g. learning boolean formulae or finite automata is as hard as factoring technical report harvard university aiken computation laboratory. kearns m. vazirani u. an introduction to computational learning theory mit press. kleinberg j. an impossibility theorem for clustering advances in neural information processing systems pp. klivans a. r. sherstov a. a. cryptographic hardness for learning intersec tions of halfspaces in focs koller d. friedman n. probabilistic graphical models principles and tech niques mit press. koltchinskii v. panchenko d. rademacher processes and bounding the risk of function learning in high dimensional probability ii springer pp. kuhn h. w. the hungarian method for the assignment problem naval re search logistics quarterly kutin s. niyogi p. almost-everywhere algorithmic stability and generalization error in proceedings of the conference in uncertainty in artificial intelligence pp. lafferty j. mccallum a. pereira f. conditional random fields probabilistic models for segmenting and labeling sequence data in international conference on machine learning pp. langford j. tutorial on practical prediction theory for classification journal of machine learning research langford j. shawe-taylor j. pac-bayes margins in nips pp. le cun l. large scale online learning. in advances in neural information processing systems proceedings of the conference vol. mit press p. references le q. v. ranzato m.-a. monga r. devin m. corrado g. chen k. dean j. ng a. y. building high-level features using large scale unsupervised learning in international conference on machine learning lecun y. bengio y. convolutional networks for images speech and time series the mit press pp. lee h. grosse r. ranganath r. ng a. convolutional deep belief networks for scalable unsupervised learning of hierarchical representations in international conference on machine learning littlestone n. learning quickly when irrelevant attributes abound a new linear-threshold algorithm machine learning littlestone n. warmuth m. relating data compression and learnability. unpublished manuscript. littlestone n. warmuth m. k. the weighted majority algorithm infor mation and computation livni r. shalev-shwartz s. shamir o. a provably efficient algorithm for training deep networks arxiv preprint livni r. simon p. honest compressions and their application to compression schemes in conference on learning theory mackay d. j. information theory inference and learning algorithms cambridge university press. mallat s. zhang z. matching pursuits with time-frequency dictionaries ieee transactions on signal processing mcallester d. a. some pac-bayesian theorems in conference on learning theory mcallester d. a. pac-bayesian model averaging in conference on learning theory pp. mcallester d. a. simplified pac-bayesian margin bounds. in conference on learning theory pp. minsky m. papert s. perceptrons an introduction to computational ge ometry the mit press. mukherjee s. niyogi p. poggio t. rifkin r. learning theory stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization advances in computational mathematics murata n. a statistical study of on-line learning online learning and neural networks. cambridge university press cambridge uk murphy k. p. machine learning a probabilistic perspective the mit press. natarajan b. sparse approximate solutions to linear systems siam j. com puting natarajan b. k. on learning sets and functions mach. learn. nemirovski a. juditsky a. lan g. shapiro a. robust stochastic approximation approach to stochastic programming siam journal on optimization nemirovski a. yudin d. problem complexity and method efficiency in opti mization nauka publishers moscow. nesterov y. primal-dual subgradient methods for convex problems technical report center for operations research and econometrics catholic university of louvain references nesterov y. nesterov i. introductory lectures on convex optimization a basic course vol. springer netherlands. novikoff a. b. j. on convergence proofs on perceptrons in proceedings of the symposium on the mathematical theory of automata vol. xii pp. parberry i. circuit complexity and neural networks the mit press. pearson k. on lines and planes of closest fit to systems of points in space the london edinburgh and dublin philosophical magazine and journal of science phillips d. l. a technique for the numerical solution of certain integral equa tions of the first kind journal of the acm pisier g. remarques sur un r esultat non publi e de b. maurey pitt l. valiant l. computational limitations on learning from examples journal of the association for computing machinery poon h. domingos p. sum-product networks a new deep architecture in conference on uncertainty in artificial intelligence quinlan j. r. induction of decision trees machine learning quinlan j. r. programs for machine learning morgan kaufmann. rabiner l. juang b. an introduction to hidden markov models ieee assp magazine rakhlin a. shamir o. sridharan k. making gradient descent optimal for strongly convex stochastic optimization in international conference on machine learning rakhlin a. sridharan k. tewari a. online learning random averages combinatorial parameters and learnability in nips rakhlin s. mukherjee s. poggio t. stability results in learning theory analysis and applications ranzato m. huang f. boureau y. lecun y. unsupervised learning of invariant feature hierarchies with applications to object recognition in computer vision and pattern recognition cvpr ieee conference on ieee pp. rissanen j. modeling by shortest data description automatica rissanen j. a universal prior for integers and estimation by minimum descrip tion length the annals of statistics robbins h. monro s. a stochastic approximation method the annals of mathematical statistics pp. rogers w. wagner t. a finite sample distribution-free performance bound for local discrimination rules the annals of statistics rokach l. data mining with decision trees theory and applications vol. world scientific. rosenblatt f. the perceptron a probabilistic model for information storage in and organization in the brain psychological review neurocomputing press rumelhart d. e. hinton g. e. williams r. j. learning internal representations by error propagation in d. e. rumelhart j. l. mcclelland eds parallel distributed processing explorations in the microstructure of cognition mit press chapter pp. references sankaran j. k. a note on resolving infeasibility in linear programs by con straint relaxation operations research letters sauer n. on the density of families of sets journal of combinatorial theory series a schapire r. the strength of weak learnability machine learning schapire r. e. freund y. boosting foundations and algorithms mit press. sch olkopf b. herbrich r. smola a. a generalized representer theorem in computational learning theory pp. sch olkopf b. herbrich r. smola a. williamson r. a generalized repre senter theorem in neurocolt sch olkopf b. smola a. j. learning with kernels support vector machines regularization optimization and beyond mit press. sch olkopf b. smola a. m uller k.-r. nonlinear component analysis as a kernel eigenvalue problem neural computation seeger m. pac-bayesian generalisation error bounds for gaussian process clas sification the journal of machine learning research shakhnarovich g. darrell t. indyk p. nearest-neighbor methods in learning and vision theory and practice mit press. shalev-shwartz s. online learning theory algorithms and applications phd thesis the hebrew university. shalev-shwartz s. online learning and online convex optimization founda tions and trends in machine learning shalev-shwartz s. shamir o. srebro n. sridharan k. learnability stability and uniform convergence the journal of machine learning research shalev-shwartz s. shamir o. sridharan k. learning kernel-based halfs paces with the zero-one loss in conference on learning theory shalev-shwartz s. shamir o. sridharan k. srebro n. stochastic convex optimization in conference on learning theory shalev-shwartz s. singer y. on the equivalence of weak learnability and linear separability new relaxations and efficient boosting algorithms in proceedings of the nineteenth annual conference on computational learning theory shalev-shwartz s. singer y. srebro n. pegasos primal estimated subgradient solver for svm in international conference on machine learning pp. shalev-shwartz s. srebro n. svm optimization inverse dependence on training set size in international conference on machine learning pp. shalev-shwartz s. zhang t. srebro n. trading accuracy for sparsity in optimization problems with sparsity constraints siam journal on optimization shamir o. zhang t. stochastic gradient descent for non-smooth optimization convergence results and optimal averaging schemes in international conference on machine learning shapiro a. dentcheva d. ruszczy nski a. lectures on stochastic programming modeling and theory vol. society for industrial and applied mathematics. references shelah s. a combinatorial problem stability and order for models and theories in infinitary languages pac. j. math sipser m. introduction to the theory of computation thomson course tech nology. slud e. v. distribution inequalities for the binomial law the annals of probability steinwart i. christmann a. support vector machines springerverlag new york. stone c. consistent nonparametric regression the annals of statistics taskar b. guestrin c. koller d. max-margin markov networks in nips tibshirani r. regression shrinkage and selection via the lasso j. royal. statist. soc b. tikhonov a. n. on the stability of inverse problems dolk. akad. nauk sssr tishby n. pereira f. bialek w. the information bottleneck method in the th allerton conference on communication control and computing tsochantaridis i. hofmann t. joachims t. altun y. support vector machine learning for interdependent and structured output spaces in proceedings of the twenty-first international conference on machine learning valiant l. g. a theory of the learnable communications of the acm vapnik v. principles of risk minimization for learning theory in j. e. moody s. j. hanson r. p. lippmann eds advances in neural information processing systems morgan kaufmann pp. vapnik v. the nature of statistical learning theory springer. vapnik v. n. estimation of dependences based on empirical data springer verlag. vapnik v. n. statistical learning theory wiley. vapnik v. n. chervonenkis a. y. on the uniform convergence of relative frequencies of events to their probabilities theory of probability and its applications vapnik v. n. chervonenkis a. y. theory of pattern recognition nauka moscow. russian. von luxburg u. a tutorial on spectral clustering statistics and computing von neumann j. zur theorie der gesellschaftsspiele the theory of parlor games math. ann. von neumann j. a certain zero-sum two-person game equivalent to the opti mal assignment problem contributions to the theory of games vovk v. g. aggregating strategies in conference on learning theory pp. warmuth m. glocer k. vishwanathan s. entropy regularized lpboost in algorithmic learning theory warmuth m. liao j. ratsch g. totally corrective boosting algorithms that maximize the margin in proceedings of the international conference on machine learning references weston j. chapelle o. vapnik v. elisseeff a. sch olkopf b. kernel dependency estimation in advances in neural information processing systems pp. weston j. watkins c. support vector machines for multi-class pattern recognition in proceedings of the seventh european symposium on artificial neural networks wolpert d. h. macready w. g. no free lunch theorems for optimization evolutionary computation ieee transactions on zhang t. solving large scale linear prediction problems using stochastic gradient descent algorithms in proceedings of the twenty-first international conference on machine learning zhao p. yu b. on model selection consistency of lasso journal of machine learning research zinkevich m. online convex programming and generalized infinitesimal gradi ent ascent in international conference on machine learning index dnf norm accuracy activation function adaboost all-pairs approximation error auto-encoders backpropagation backward elimination bag-of-words base hypothesis bayes optimal bayes rule bayesian reasoning bennet s inequality bernstein s inequality bias bias-complexity tradeoff boolean conjunctions boosting boosting the confidence boundedness cart chaining chebyshev s inequality chernoff bounds class-sensitive feature mapping classifier clustering spectral compressed sensing compression bounds compression scheme computational complexity confidence consistency consistent contraction lemma convex function set strongly convex convex-lipschitz-bounded learning convex-smooth-bounded learning covering numbers curse of dimensionality decision stumps decision trees dendrogram dictionary learning differential set dimensionality reduction discretization trick discriminative distribution free domain domain of examples doubly stochastic matrix duality strong duality weak duality dudley classes efficient computable em empirical error empirical risk empirical risk minimization see erm entropy relative entropy epigraph erm error decomposition estimation error expectation-maximization see em face recognition see viola-jones feasible feature feature learning feature normalization feature selection feature space feature transformations filters understanding machine learning by shai shalev-shwartz and shai ben-david published by cambridge university press. personal use only. not for distribution. do not post. please link to httpwww.cs.huji.ac.ilshaisunderstandingmachinelearning index forward greedy selection frequentist gain gd see gradient descent generalization error generative models gini index glivenko-cantelli gradient gradient descent gram matrix growth function halfspace homogenous non-separable separable halving hidden layers hilbert space hoeffding s inequality hold out hypothesis hypothesis class i.i.d. improper see representation independent inductive bias see bias information bottleneck information gain instance instance space integral image johnson-lindenstrauss lemma k-means soft k-means k-median k-medoids kendall tau kernel pca kernels gaussian kernel kernel trick polynomial kernel rbf kernel label lasso generalization bounds latent variables lda ldim learning curves least squares likelihood ratio linear discriminant analysis see lda linear predictor homogenous linear programming linear regression linkage lipschitzness sub-gradient littlestone dimension see ldim local minimum logistic regression loss loss function loss absolute value loss convex loss generalized hinge-loss hinge loss lipschitz loss log-loss logistic loss ramp loss smooth loss square loss surrogate loss margin markov s inequality massart lemma max linkage maximum a-posteriori maximum likelihood mcdiarmid s inequality mdl measure concentration minimum description length see mdl mistake bound mixture of gaussians model selection multiclass cost-sensitive linear predictors multi-vector perceptron reductions sgd svm multivariate performance measures naive bayes natarajan dimension ndcg nearest neighbor k-nn neural networks feedforward networks layered networks sgd no-free-lunch non-uniform learning index sample complexity sauer s lemma self-boundedness sensitivity sgd shattering single linkage singular value decomposition see svd slud s inequality smoothness soa sparsity-inducing norms specificity spectral clustering srm stability stochastic gradient descent see sgd strong learning structural risk minimization see srm structured output prediction sub-gradient support vector machines see svm svd svm duality generalization bounds hard-svm homogenous kernel trick soft-svm support vectors target set term-frequency tf-idf training error training set true error underfitting uniform convergence union bound unsupervised learning validation cross validation train-validation-test split vapnik-chervonenkis dimension see vc dimension vc dimension version space viola-jones weak learning weighted-majority normalized discounted cumulative gain see ndcg occam s razor omp one-vs-all one-vs-rest see one-vs-all one-vs.-all online convex optimization online gradient descent online learning optimization error oracle inequality orthogonal matching pursuit see omp overfitting pac agnostic pac agnostic pac for general loss pac-bayes parametric density estimation pca pearson s correlation coefficient perceptron kernelized perceptron multiclass online permutation matrix polynomial regression precision predictor prefix free language principal component analysis see pca prior knowledge probably approximately correct see pac projection projection lemma proper pruning rademacher complexity random forests random projections ranking bipartite realizability recall regression regularization tikhonov regularized loss minimization see rlm representation independent representative sample representer theorem ridge regression kernel ridge regression rip risk rlm