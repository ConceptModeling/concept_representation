computer vision algorithms and applications richard szeliski september draft springer this electronic draft is for non-commercial personal use only and may not be posted or re-distributed in any form. please refer interested readers to the book s web site at httpszeliski.orgbook. this book is dedicated to my parents zdzis aw and jadwiga and my family lyn anne and stephen. introduction what is computer vision? a brief history book overview sample syllabus notation image formation geometric primitives and transformations photometric image formation the digital camera image processing point operators linear filtering more neighborhood operators fourier transforms pyramids and wavelets geometric transformations global optimization feature detection and matching points and patches edges lines segmentation mean shift and mode finding normalized cuts active contours split and merge graph cuts and energy-based methods feature-based alignment and feature-based alignment pose estimation geometric intrinsic calibration structure from motion triangulation two-frame structure from motion factorization bundle adjustment constrained structure and motion n dense motion estimation translational alignment parametric motion spline-based motion optical flow layered motion image stitching motion models global alignment compositing computational photography photometric calibration high dynamic range imaging super-resolution and blur removal image matting and compositing texture analysis and synthesis stereo correspondence epipolar geometry sparse correspondence dense correspondence local methods global optimization multi-view stereo reconstruction shape from x active rangefinding surface representations point-based representations volumetric representations model-based reconstruction image-based rendering recovering texture maps and albedos view interpolation layered depth images light fields and lumigraphs environment mattes video-based rendering recognition instance recognition category recognition object detection face recognition context and scene understanding recognition databases and test sets preface the seeds for this book were first planted in when steve seitz at the university of washington invited me to co-teach a course called computer vision for computer graphics at that time computer vision techniques were increasingly being used in computer graphics to create image-based models of real-world objects to create visual effects and to merge realworld imagery using computational photography techniques. our decision to focus on the applications of computer vision to fun problems such as image stitching and photo-based modeling from personal photos seemed to resonate well with our students. since that time a similar syllabus and project-oriented course structure has been used to teach general computer vision courses both at the university of washington and at stanford. latter was a course i co-taught with david fleet in similar curricula have been adopted at a number of other universities and also incorporated into more specialized courses on computational photography. ideas on how to use this book in your own course please see table in section this book also reflects my years experience doing computer vision research in corporate research labs mostly at digital equipment corporation s cambridge research lab and at microsoft research. in pursuing my work i have mostly focused on problems and solution techniques that have practical real-world applications and that work well in practice. thus this book has more emphasis on basic techniques that work under real-world conditions and less on more esoteric mathematics that has intrinsic elegance but less practical applicability. this book is suitable for teaching a senior-level undergraduate course in computer vision to students in both computer science and electrical engineering. i prefer students to have either an image processing or a computer graphics course as a prerequisite so that they can spend less time learning general background mathematics and more time studying computer vision techniques. the book is also suitable for teaching graduate-level courses in computer vision delving into the more demanding application and algorithmic areas and as a general reference to fundamental techniques and the recent research literature. to this end i have attempted wherever possible to at least cite the newest research in each sub-field even if the viii computer vision algorithms and applications draft technical details are too complex to cover in the book itself. in teaching our courses we have found it useful for the students to attempt a number of small implementation projects which often build on one another in order to get them used to working with real-world images and the challenges that these present. the students are then asked to choose an individual topic for each of their small-group final projects. these projects even turn into conference papers! the exercises at the end of each chapter contain numerous suggestions for smaller mid-term projects as well as more open-ended problems whose solutions are still active research topics. wherever possible i encourage students to try their algorithms on their own personal photographs since this better motivates them often leads to creative variants on the problems and better acquaints them with the variety and complexity of real-world imagery. in formulating and solving computer vision problems i have often found it useful to draw inspiration from three high-level approaches scientific build detailed models of the image formation process and develop mathematical techniques to invert these in order to recover the quantities of interest necessary making simplifying assumption to make the mathematics more tractable. statistical use probabilistic models to quantify the prior likelihood of your unknowns and the noisy measurement processes that produce the input images then infer the best possible estimates of your desired quantities and analyze their resulting uncertainties. the inference algorithms used are often closely related to the optimization techniques used to invert the image formation processes. engineering develop techniques that are simple to describe and implement but that are also known to work well in practice. test these techniques to understand their limitation and failure modes as well as their expected computational costs performance. these three approaches build on each other and are used throughout the book. my personal research and development philosophy hence the exercises in the book have a strong emphasis on testing algorithms. it s too easy in computer vision to develop an algorithm that does something plausible on a few images rather than something correct. the best way to validate your algorithms is to use a three-part strategy. first test your algorithm on clean synthetic data for which the exact results are known. second add noise to the data and evaluate how the performance degrades as a function of noise level. finally test the algorithm on real-world data preferably drawn from a wide variety of sources such as photos found on the web. only then can you truly know if your algorithm can deal with real-world complexity i.e. images that do not fit some simplified model or assumptions. preface ix in order to help students in this process this books comes with a large amount of supplementary material which can be found on the book s web site httpszeliski.orgbook. this material which is described in appendix c includes pointers to commonly used data sets for the problems which can be found on the web pointers to software libraries which can help students get started with basic tasks such as readingwriting images or creating and manipulating images slide sets corresponding to the material covered in this book a bibtex bibliography of the papers cited in this book. the latter two resources may be of more interest to instructors and researchers publishing new papers in this field but they will probably come in handy even with regular students. some of the software libraries contain implementations of a wide variety of computer vision algorithms which can enable you to tackle more ambitious projects your instructor s consent. acknowledgements i would like to gratefully acknowledge all of the people whose passion for research and inquiry as well as encouragement have helped me write this book. steve zucker at mcgill university first introduced me to computer vision taught all of his students to question and debate research results and techniques and encouraged me to pursue a graduate career in this area. takeo kanade and geoff hinton my ph. d. thesis advisors at carnegie mellon university taught me the fundamentals of good research writing and presentation. they fired up my interest in visual processing modeling and statistical methods while larry matthies introduced me to kalman filtering and stereo matching. demetri terzopoulos was my mentor at my first industrial research job and taught me the ropes of successful publishing. yvan leclerc and pascal fua colleagues from my brief interlude at sri international gave me new perspectives on alternative approaches to computer vision. during my six years of research at digital equipment corporation s cambridge research lab i was fortunate to work with a great set of colleagues including ingrid carlbom gudrun klinker keith waters richard weiss st ephane lavall ee and sing bing kang as well as to supervise the first of a long string of outstanding summer interns including david tonnesen sing bing kang james coughlan and harry shum. this is also where i began my long-term collaboration with daniel scharstein now at middlebury college. x computer vision algorithms and applications draft at microsoft research i ve had the outstanding fortune to work with some of the world s best researchers in computer vision and computer graphics including michael cohen hugues hoppe stephen gortler steve shafer matthew turk harry shum anandan phil torr antonio criminisi georg petschnigg kentaro toyama ramin zabih shai avidan sing bing kang matt uyttendaele patrice simard larry zitnick richard hartley simon winder drew steedly chris pal nebojsa jojic patrick baudisch dani lischinski matthew brown simon baker michael goesele eric stollnitz david nist er blaise aguera y arcas sudipta sinha johannes kopf neel joshi and krishnan ramnath. i was also lucky to have as interns such great students as polina golland simon baker mei han arno sch odl ron dror ashley eden jinxiang chai rahul swaminathan yanghai tsin sam hasinoff anat levin matthew brown eric bennett vaibhav vaish jan-michael frahm james diebel ce liu josef sivic grant schindler colin zheng neel joshi sudipta sinha zeev farbman rahul garg tim cho yekeun jeong richard roberts varsha hedau and dilip krishnan. while working at microsoft i ve also had the opportunity to collaborate with wonderful colleagues at the university of washington where i hold an affiliate professor appointment. i m indebted to tony derose and david salesin who first encouraged me to get involved with the research going on at uw my long-time collaborators brian curless steve seitz maneesh agrawala sameer agarwal and yasu furukawa as well as the students i have had the privilege to supervise and interact with including fr ederic pighin yung-yu chuang doug zongker colin zheng aseem agarwala dan goldman noah snavely rahul garg and ryan kaminsky. as i mentioned at the beginning of this preface this book owes its inception to the vision course that steve seitz invited me to co-teach as well as to steve s encouragement course notes and editorial input. i m also grateful to the many other computer vision researchers who have given me so many constructive suggestions about the book including sing bing kang who was my informal book editor vladimir kolmogorov who contributed appendix on linear programming techniques for mrf inference daniel scharstein richard hartley simon baker noah snavely bill freeman svetlana lazebnik matthew turk jitendra malik alyosha efros michael black brian curless sameer agarwal li zhang deva ramanan olga veksler yuri boykov carsten rother phil torr bill triggs bruce maxwell jana ko seck a eero simoncelli aaron hertzmann antonio torralba tomaso poggio theo pavlidis baba vemuri nando de freitas chuck dyer song yi falk schubert roman pflugfelder marshall tappen james coughlan sammy rogmans klaus strobel shanmuganathan andreas siebert yongjun wu fred pighin juan cockburn ronald mallet tim soper georgios evangelidis dwight fowler itzik bayaz daniel o connor and srikrishna bhat. shena deuchers did a fantastic job copy-editing the book and suggesting many useful improvements and wayne wheeler and simon rees at springer were most helpful throughout the whole book publishing process. keith price s annotated computer vision bibliography was invaluable in preface xi tracking down references and finding related work. if you have any suggestions for improving the book please send me an e-mail as i would like to keep the book as accurate informative and timely as possible. lastly this book would not have been possible or worthwhile without the incredible supi dedicate this book to my parents zdzis aw and port and encouragement of my family. jadwiga whose love generosity and accomplishments have always inspired me to my sister basia for her lifelong friendship and especially to lyn anne and stephen whose daily encouragement in all matters this book project makes it all worthwhile. lake wenatchee august xii computer vision algorithms and applications draft contents preface contents introduction what is computer vision? a brief history book overview sample syllabus a note on notation additional reading geometric primitives transformations transformations rotations to projections lens distortions photometric image formation image formation geometric primitives and transformations lighting reflectance and shading optics the digital camera sampling and aliasing color compression vii xiii xiv computer vision algorithms and applications draft additional reading exercises image processing point operators linear filtering more neighborhood operators fourier transforms pixel transforms separable filtering non-linear filtering morphology distance transforms connected components fourier transform pairs color transforms compositing and matting histogram equalization application tonal adjustment examples of linear filtering band-pass and steerable filters two-dimensional fourier transforms wiener filtering application sharpening blur and noise removal decimation multi-resolution representations wavelets application image blending mesh-based warping application feature-based morphing regularization markov random fields application image restoration parametric transformations pyramids and wavelets interpolation geometric transformations global optimization contents additional reading exercises edges points and patches feature detection and matching feature detectors feature descriptors feature matching feature tracking application performance-driven animation edge detection edge linking application edge editing and enhancement lines hough transforms vanishing points application rectangle detection successive approximation additional reading exercises segmentation active contours snakes scissors dynamic snakes and condensation level sets application contour tracking and rotoscoping watershed region splitting clustering region merging clustering graph-based segmentation k-means and mixtures of gaussians mean shift probabilistic aggregation split and merge mean shift and mode finding xv xvi computer vision algorithms and applications draft normalized cuts graph cuts and energy-based methods application medical image segmentation additional reading exercises feature-based alignment pose estimation iterative algorithms alignment iterative algorithms alignment using least squares and feature-based alignment application panography robust least squares and ransac calibration patterns vanishing points application single view metrology rotational motion radial distortion linear algorithms application augmented reality geometric intrinsic calibration additional reading exercises structure from motion triangulation two-frame structure from motion application view morphing projective reconstruction self-calibration perspective and projective factorization application sparse model extraction exploiting sparsity application match move and augmented reality bundle adjustment factorization contents uncertainty and ambiguities application reconstruction from internet photos constrained structure and motion plane-based techniques line-based techniques additional reading exercises dense motion estimation translational alignment parametric motion spline-based motion optical flow fourier-based alignment incremental refinement hierarchical motion estimation application video stabilization learned motion models application medical image registration application frame interpolation transparent layers and reflections multi-frame motion estimation application video denoising application de-interlacing layered motion additional reading exercises xvii image stitching motion models planar perspective motion application whiteboard and document scanning rotational panoramas gap closing application video summarization and compression cylindrical and spherical coordinates bundle adjustment global alignment xviii computer vision algorithms and applications draft parallax removal compositing recognizing panoramas direct vs. feature-based alignment pixel selection and weighting choosing a compositing surface application photomontage blending additional reading exercises computational photography photometric calibration super-resolution and blur removal high dynamic range imaging tone mapping application flash photography color image demosaicing application colorization radiometric response function noise level estimation vignetting optical blur response estimation application hole filling and inpainting application non-photorealistic rendering image matting and compositing blue screen matting natural image matting optimization-based matting smoke shadow and flash matting video matting texture analysis and synthesis additional reading exercises stereo correspondence epipolar geometry rectification contents local methods global optimization dense correspondence plane sweep sparse correspondence similarity measures curves and profiles sub-pixel estimation and uncertainty application stereo-based head tracking dynamic programming segmentation-based techniques application z-keying and background replacement volumetric and surface reconstruction shape from silhouettes additional reading exercises multi-view stereo reconstruction shape from x active rangefinding surface representations range data merging application digital heritage shape from shading and photometric stereo shape from texture shape from focus implicit surfaces and level sets architecture heads and faces application facial animation whole body modeling and tracking surface interpolation surface simplification geometry images model-based reconstruction point-based representations volumetric representations xix xx computer vision algorithms and applications draft recovering texture maps and albedos estimating brdfs application photography additional reading exercises image-based rendering view interpolation layered depth images light fields and lumigraphs view-dependent texture maps application photo tourism impostors sprites and layers unstructured lumigraph surface light fields application concentric mosaics higher-dimensional light fields the modeling to rendering continuum video-based animation video textures application animating pictures video application video-based walkthroughs environment mattes video-based rendering additional reading exercises recognition object detection face detection pedestrian detection face recognition eigenfaces active appearance and shape models application personal photo collections geometric alignment instance recognition contents large databases application location recognition category recognition bag of words part-based models recognition with segmentation application intelligent photo editing learning and large image collections application image search recognition databases and test sets additional reading exercises context and scene understanding conclusion a linear algebra and numerical techniques matrix decompositions linear least squares singular value decomposition eigenvalue decomposition qr factorization cholesky factorization total least squares non-linear least squares direct sparse matrix techniques variable reordering conjugate gradient preconditioning multigrid iterative techniques b bayesian modeling and inference estimation theory likelihood for multivariate gaussian noise maximum likelihood estimation and least squares robust statistics prior models and bayesian inference markov random fields xxi xxii computer vision algorithms and applications draft gradient descent and simulated annealing dynamic programming belief propagation graph cuts linear programming uncertainty estimation analysis c supplementary material data sets software slides and lectures bibliography references index chapter introduction what is computer vision? a brief history book overview sample syllabus a note on notation additional reading figure the human visual system has no problem interpreting the subtle variations in translucency and shading in this photograph and correctly segmenting the object from its background. computer vision algorithms and applications draft figure some examples of computer vision algorithms and applications. structure from motion algorithms can reconstruct a sparse point model of a large complex scene from hundreds of partially overlapping photographs seitz and szeliski acm. stereo matching algorithms can build a detailed model of a building fac ade from hundreds of differently exposed photographs taken from the internet snavely curless et al. ieee. person tracking algorithms can track a person walking in front of a cluttered background black and fleet springer. face detection algorithms coupled with color-based clothing and hair detection algorithms can locate and recognize the individuals in this image zitnick and szeliski springer. what is computer vision? what is computer vision? as humans we perceive the three-dimensional structure of the world around us with apparent ease. think of how vivid the three-dimensional percept is when you look at a vase of flowers sitting on the table next to you. you can tell the shape and translucency of each petal through the subtle patterns of light and shading that play across its surface and effortlessly segment each flower from the background of the scene looking at a framed group portrait you can easily count name all of the people in the picture and even guess at their emotions from their facial appearance. perceptual psychologists have spent decades trying to understand how the visual system works and even though they can devise optical to tease apart some of its principles a complete solution to this puzzle remains elusive palmer livingstone researchers in computer vision have been developing in parallel mathematical techniques for recovering the three-dimensional shape and appearance of objects in imagery. we now have reliable techniques for accurately computing a partial model of an environment from thousands of partially overlapping photographs given a large enough set of views of a particular object or fac ade we can create accurate dense surface models using stereo matching we can track a person moving against a complex background we can even with moderate success attempt to find and name all of the people in a photograph using a combination of face clothing and hair detection and recognition however despite all of these advances the dream of having a computer interpret an image at the same level as a two-year old example counting all of the animals in a picture remains elusive. why is vision so difficult? in part it is because vision is an inverse problem in which we seek to recover some unknowns given insufficient information to fully specify the solution. we must therefore resort to physics-based and probabilistic models to disambiguate between potential solutions. however modeling the visual world in all of its rich complexity is far more difficult than say modeling the vocal tract that produces spoken sounds. the forward models that we use in computer vision are usually developed in physics optics and sensor design and in computer graphics. both of these fields model how objects move and animate how light reflects off their surfaces is scattered by the atmosphere refracted through camera lenses human eyes and finally projected onto a flat curved image plane. while computer graphics are not yet perfect fully computeranimated movie with human characters has yet succeeded at crossing the uncanny that separates real humans from android robots and computer-animated humans in limited httpwww.michaelbach.deotsze muelue the term uncanny valley was originally coined by roboticist masahiro mori as applied to robotics it is also commonly applied to computer-animated films such as final fantasy and polar express computer vision algorithms and applications draft figure some common optical illusions and what they might tell us about the visual system the classic m uller-lyer illusion where the length of the two horizontal lines appear different probably due to the imagined perspective effects. the white square b in the shadow and the black square a in the light actually have the same absolute intensity value. the percept is due to brightness constancy the visual system s attempt to discount illumination when interpreting colors. image courtesy of ted adelson httpweb.mit.edupersci peopleadelsoncheckershadow illusion.html. a variation of the hermann grid illusion courtesy of hany farid httpwww.cs.dartmouth.edu faridillusionshermann.html. as you move your eyes over the figure gray spots appear at the intersections. count the red xs in the left half of the figure. now count them in the right half. is it significantly harder? the explanation has to do with a pop-out effect which tells us about the operations of parallel perception and integration pathways in the brain. xxxxxxxoxoxoxxxxxxxxxxoxxxoxxxxxxxxoxxoxxoxxxxxxxxxoxooxxxxxxxxoxxoxxxxxxxxxxxoxxxoxxxxxxxxoxxoxxoxxxxxxxxoxxxoxxxxxxxxxxxooxxxxxxxxxxoxxxox what is computer vision? domains such as rendering a still scene composed of everyday objects or animating extinct creatures such as dinosaurs the illusion of reality is perfect. in computer vision we are trying to do the inverse i.e. to describe the world that we see in one or more images and to reconstruct its properties such as shape illumination and color distributions. it is amazing that humans and animals do this so effortlessly while computer vision algorithms are so error prone. people who have not worked in the field often underestimate the difficulty of the problem. at work often ask me for software to find and name all the people in photos so they can get on with the more interesting work. this misperception that vision should be easy dates back to the early days of artificial intelligence section when it was initially believed that the cognitive proving and planning parts of intelligence were intrinsically more difficult than the perceptual components the good news is that computer vision is being used today in a wide variety of real-world applications which include optical character recognition reading handwritten postal codes on letters and automatic number plate recognition machine inspection rapid parts inspection for quality assurance using stereo vision with specialized illumination to measure tolerances on aircraft wings or auto body parts or looking for defects in steel castings using x-ray vision retail object recognition for automated checkout lanes model building fully automated construction of models from aerial photographs used in systems such as bing maps medical imaging registering pre-operative and intra-operative imagery or performing long-term studies of people s brain morphology as they age automotive safety detecting unexpected obstacles such as pedestrians on the street under conditions where active vision techniques such as radar or lidar do not work well see also miller campbell huttenlocher et al. montemerlo becker bhat et al. urmson anhalt bagnell et al. for examples of fully automated driving match move merging computer-generated imagery with live action footage by tracking feature points in the source video to estimate the camera motion and shape of the environment. such techniques are widely used in hollywood in movies such as jurassic park roble and zafar they also require the use of precise matting to insert new elements between foreground and background elements agarwala curless et al. computer vision algorithms and applications draft figure some industrial applications of computer vision optical character recognition httpyann.lecun.comexdblenet mechanical inspection httpwww.cognitens. com retail httpwww.evoretail.com medical imaging httpwww.clarontech.com automotive safety httpwww.mobileye.com surveillance and traffic monitoring http courtesy of honeywell international inc. what is computer vision? motion capture using retro-reflective markers viewed from multiple cameras or other vision-based techniques to capture actors for computer animation surveillance monitoring for intruders analyzing highway traffic and monitoring pools for drowning victims fingerprint recognition and biometrics for automatic access authentication as well as forensic applications. david lowe s web site of industrial vision applications vision.html lists many other interesting industrial applications of computer vision. while the above applications are all extremely important they mostly pertain to fairly specialized kinds of imagery and narrow domains. in this book we focus more on broader consumer-level applications such as fun things you can do with your own personal photographs and video. these include stitching turning overlapping photos into a single seamlessly stitched panorama as described in chapter exposure bracketing merging multiple exposures taken under challenging lighting conditions sunlight and shadows into a single perfectly exposed image as described in section morphing turning a picture of one of your friends into another using a seamless morph transition modeling converting one or more snapshots into a model of the object or person you are photographing as described in section video match move and stabilization inserting pictures or models into your videos by automatically tracking nearby reference points section or using motion estimates to remove shake from your videos section photo-based walkthroughs navigating a large collection of photographs such as the interior of your house by flying between different photos in sections and face detection for improved camera focusing as well as more relevant image searching section visual authentication automatically logging family members onto your home computer as they sit down in front of the webcam section computer vision algorithms and applications draft figure some consumer applications of computer vision image stitching merging different views and shum acm exposure bracketing merging different exposures morphing blending between two photographs darsa costa et al. morgan kaufmann turning a collection of photographs into a model steedly szeliski et al. acm. what is computer vision? the great thing about these applications is that they are already familiar to most students they are at least technologies that students can immediately appreciate and use with their own personal media. since computer vision is a challenging topic given the wide range of mathematics being and the intrinsically difficult nature of the problems being solved having fun and relevant problems to work on can be highly motivating and inspiring. the other major reason why this book has a strong focus on applications is that they can be used to formulate and constrain the potentially open-ended problems endemic in vision. for example if someone comes to me and asks for a good edge detector my first question is usually to ask why? what kind of problem are they trying to solve and why do they believe that edge detection is an important component? if they are trying to locate faces i usually point out that most successful face detectors use a combination of skin color detection and simple blob features section they do not rely on edge detection. if they are trying to match door and window edges in a building for the purpose of reconstruction i tell them that edges are a fine idea but it is better to tune the edge detector for long edges sections and and link them together into straight lines with common vanishing points before matching section thus it is better to think back from the problem at hand to suitable techniques rather than to grab the first technique that you may have heard of. this kind of working back from problems to solutions is typical of an engineering approach to the study of vision and reflects my own background in the field. first i come up with a detailed problem definition and decide on the constraints and specifications for the problem. then i try to find out which techniques are known to work implement a few of these evaluate their performance and finally make a selection. in order for this process to work it is important to have realistic test data both synthetic which can be used to verify correctness and analyze noise sensitivity and real-world data typical of the way the system will finally be used. however this book is not just an engineering text source of recipes. it also takes a scientific approach to basic vision problems. here i try to come up with the best possible models of the physics of the system at hand how the scene is created how light interacts with the scene and atmospheric effects and how the sensors work including sources of noise and uncertainty. the task is then to try to invert the acquisition process to come up with the best possible description of the scene. the book often uses a statistical approach to formulating and solving computer vision problems. where appropriate probability distributions are used to model the scene and the noisy image acquisition process. the association of prior distributions with unknowns is often for a fun student project on this topic see the photobook project at httpwww.cc.gatech.edudvfxvideos these techniques include physics euclidean and projective geometry statistics and optimization. they make computer vision a fascinating field to study and a great way to learn techniques widely applicable in other fields. computer vision algorithms and applications draft a robot trying to estimate the distance to an obstacle called bayesian modeling b. it is possible to associate a risk or loss function with mis-estimating the answer and to set up your inference algorithm to minimize the expected risk. it is usually safer to underestimate than to overestimate. with statistical techniques it often helps to gather lots of training data from which to learn probabilistic models. finally statistical approaches enable you to use proven inference techniques to estimate the best answer distribution of answers and to quantify the uncertainty in the resulting estimates. because so much of computer vision involves the solution of inverse problems or the estimation of unknown quantities my book also has a heavy emphasis on algorithms especially those that are known to work well in practice. for many vision problems it is all too easy to come up with a mathematical description of the problem that either does not match realistic real-world conditions or does not lend itself to the stable estimation of the unknowns. what we need are algorithms that are both robust to noise and deviation from our models and reasonably efficient in terms of run-time resources and space. in this book i go into these issues in detail using bayesian techniques where applicable to ensure robustness and efficient search minimization and linear system solving algorithms to ensure efficiency. most of the algorithms described in this book are at a high level being mostly a list of steps that have to be filled in by students or by reading more detailed descriptions elsewhere. in fact many of the algorithms are sketched out in the exercises. now that i ve described the goals of this book and the frameworks that i use i devote the rest of this chapter to two additional topics. section is a brief synopsis of the history of computer vision. it can easily be skipped by those who want to get to the meat of the new material in this book and do not care as much about who invented what when. the second is an overview of the book s contents section which is useful reading for everyone who intends to make a study of this topic to jump in partway since it describes chapter inter-dependencies. this outline is also useful for instructors looking to structure one or more courses around this topic as it provides sample curricula based on the book s contents. a brief history in this section i provide a brief personal synopsis of the main developments in computer vision over the last years at least those that i find personally interesting and which appear to have stood the test of time. readers not interested in the provenance of various ideas and the evolution of this field should skip ahead to the book overview in section a brief history figure a rough timeline of some of the most active topics of research in computer vision. when computer vision first started out in the early it was viewed as the visual perception component of an ambitious agenda to mimic human intelligence and to endow robots with intelligent behavior. at the time it was believed by some of the early pioneers of artificial intelligence and robotics places such as mit stanford and cmu that solving the visual input problem would be an easy step along the path to solving more difficult problems such as higher-level reasoning and planning. according to one well-known story in marvin minsky at mit asked his undergraduate student gerald jay sussman to spend the summer linking a camera to a computer and getting the computer to describe what it saw p. we now know that the problem is slightly more difficult than what distinguished computer vision from the already existing field of digital image processing and pfaltz rosenfeld and kak was a desire to recover the three-dimensional structure of the world from images and to use this as a stepping stone towards full scene understanding. winston and hanson and riseman provide two nice collections of classic papers from this early period. early attempts at scene understanding involved extracting edges and then inferring the structure of an object or a blocks world from the topological structure of the lines several line labeling algorithms were developed at that time clowes waltz rosenfeld hummel and zucker kanade nalwa gives a nice review of this area. the topic of edge detection was also boden cites as the original source. the actual vision memo was authored by seymour papert and involved a whole cohort of students. to see how far robotic vision has come in the last four decades have a look at the towel-folding robot at cusumano-towner lei et al. digital image processingblocks world line cylinderspictorial structuresstereo correspondenceintrinsic imagesoptical flowstructure from pyramidsscale-space processingshape from shading texture and focusphysically-based random fieldskalman range data processingprojective visiongraph cutsparticle filteringenergy-based recognition and detectionsubspace methodsimage-based modeling and renderingtexture synthesis and inpaintingcomputational recognitionmrf inference algorithmscategory recognitionlearning computer vision algorithms and applications draft figure some early examples of computer vision algorithms line labeling addison-wesley pictorial structures and elschlager ieee articulated body model david marr intrinsic images and tenenbaum ieee stereo correspondence david marr optical flow and enkelmann ieee. an active area of research a nice survey of contemporaneous work can be found in three-dimensional modeling of non-polyhedral objects was also being studied baker one popular approach used generalized cylinders i.e. solids of revolution and swept closed curves and binford nevatia and binford often arranged into parts marr fischler and elschlager called such elastic arrangements of parts pictorial structures this is currently one of the favored approaches being used in object recognition section and felzenszwalb and huttenlocher a qualitative approach to understanding intensities and shading variations and explaining them by the effects of image formation phenomena such as surface orientation and shadows was championed by barrow and tenenbaum in their paper on intrinsic images along with the related sketch ideas of marr this approach is again seeing a bit of a revival in the work of tappen freeman and adelson more quantitative approaches to computer vision were also developed at the time including the first of many feature-based stereo correspondence algorithms in robotics and computer animation these linked-part graphs are often called kinematic chains. a brief history marr and poggio moravec marr and poggio mayhew and frisby baker barnard and fischler ohta and kanade grimson pollard mayhew and frisby prazdny and intensity-based optical flow algorithms and schunck huang lucas and kanade nagel the early work in simultaneously recovering structure and camera motion chapter also began around this time longuet-higgins a lot of the philosophy of how vision was believed to work at the time is summarized in david marr s in particular marr introduced his notion of the three levels of description of a information processing system. these three levels very loosely paraphrased according to my own interpretation are computational theory what is the goal of the computation and what are the constraints that are known or can be brought to bear on the problem? representations and algorithms how are the input output and intermediate information represented and which algorithms are used to calculate the desired result? hardware implementation how are the representations and algorithms mapped onto actual hardware e.g. a biological vision system or a specialized piece of silicon? conversely how can hardware constraints be used to guide the choice of representation and algorithm? with the increasing use of graphics chips and many-core architectures for computer vision section this question is again becoming quite relevant. as i mentioned earlier in this introduction it is my conviction that a careful analysis of the problem specification and known constraints from image formation and priors scientific and statistical approaches must be married with efficient and robust algorithms engineering approach to design successful vision algorithms. thus it seems that marr s philosophy is as good a guide to framing and solving problems in our field today as it was years ago. in the a lot of attention was focused on more sophisticated mathematical techniques for performing quantitative image and scene analysis. image pyramids section started being widely used to perform tasks such as image blending and coarse-to-fine correspondence search burt and adelson rosenfeld quam anandan continuous versions of pyramids using the concept of scale-space processing were also developed witkin terzopoulos and kass lindeberg in the late wavelets section started displacing or augmenting regular image pyramids in some applications more recent developments in visual perception theory are covered in livingstone computer vision algorithms and applications draft figure examples of computer vision algorithms from the pyramid blending and adelson acm shape from shading and adelson ieee edge detection and adelson ieee physically based models and witkin ieee regularizationbased surface reconstruction ieee range data acquisition and merging masuda oishi et al. springer. simoncelli and hingorani mallat simoncelli and adelson simoncelli freeman adelson et al. the use of stereo as a quantitative shape cue was extended by a wide variety of shapefrom-x techniques including shape from shading section and horn pentland blake zimmerman and knowles horn and brooks photometric stereo section and woodham shape from texture section and witkin pentland malik and rosenholtz and shape from focus section and nayar watanabe and noguchi horn has a nice discussion of most of these techniques. research into better edge and contour detection section was also active during this period nalwa and binford including the introduction of dynamically evolving contour trackers such as snakes witkin and terzopoulos as well as three-dimensional physically based models witkin and kass kass witkin and terzopoulos terzopoulos and fleischer terzopoulos witkin and kass researchers noticed that a lot of the stereo flow shape-from-x and edge detection al a brief history gorithms could be unified or at least described using the same mathematical framework if they were posed as variational optimization problems section and made more robust using regularization section and terzopoulos poggio torre and koch terzopoulos blake and zisserman bertero poggio and torre terzopoulos around the same time geman and geman pointed out that such problems could equally well be formulated using discrete markov random field models section which enabled the use of better search and optimization algorithms such as simulated annealing. online variants of mrf algorithms that modeled and updated uncertainties using the kalman filter were introduced a little later and graefe matthies kanade and szeliski szeliski attempts were also made to map both regularized and mrf algorithms onto parallel hardware and koch poggio little gamble et al. fischler firschein barnard et al. the book by fischler and firschein contains a nice collection of articles focusing on all of these topics flow regularization mrfs and even higher-level vision. three-dimensional range data processing merging modeling and recognition see figure continued being actively explored during this decade and binford besl and jain faugeras and hebert curless and levoy the compilation by kanade contains a lot of the interesting papers in this area. while a lot of the previously mentioned topics continued to be explored a few of them became significantly more active. a burst of activity in using projective invariants for recognition and zisserman evolved into a concerted effort to solve the structure from motion problem chapter a lot of the initial activity was directed at projective reconstructions which did not require knowledge of camera calibration hartley gupta and chang hartley faugeras and luong hartley and zisserman simultaneously factorization techniques were developed to solve efficiently problems for which orthographic camera approximations were applicable and kanade poelman and kanade anandan and irani and then later extended to the perspective case and horaud triggs eventually the field started using full global optimization section and taylor kriegman and anandan szeliski and kang azarbayejani and pentland which was later recognized as being the same as the bundle adjustment techniques traditionally used in photogrammetry mclauchlan hartley et al. fully automated modeling systems were built using such techniques torr and zisserman schaffalitzky and zisserman brown and lowe snavely seitz and szeliski work begun in the on using detailed measurements of color and intensity combined computer vision algorithms and applications draft figure examples of computer vision algorithms from the factorization-based structure from motion and kanade springer dense stereo matching veksler and zabih multi-view reconstruction and dyer springer face tracking xiao and baker image segmentation fowlkes chung et al. springer face recognition and pentland with accurate physical models of radiance transport and color image formation created its own subfield known as physics-based vision. a good survey of the field can be found in the threevolume collection on this topic shafer and healey healey and shafer shafer healey and wolff optical flow methods chapter continued to be improved and enkelmann bolles baker and marimont horn and weldon jr. anandan bergen anandan hanna et al. black and anandan bruhn weickert and schn orr papenberg bruhn brox et al. with barron fleet and beauchemin baker black lewis et al. being good surveys. similarly a lot of progress was made on dense stereo correspondence algorithms chapter okutomi and kanade boykov veksler and zabih birchfield and tomasi boykov veksler and zabih and the survey and comparison in scharstein and szeliski with the biggest breakthrough being perhaps global optimization using graph cut techniques veksler and zabih a brief history multi-view stereo algorithms that produce complete surfaces section were also an active topic of research and dyer kutulakos and seitz that continues to be active today curless diebel et al. techniques for producing volumetric descriptions from binary silhouettes section continued to be developed srivasan liang and hackwood szeliski laurentini along with techniques based on tracking and reconstructing smooth occluding contours section and cipolla and blake vaillant and faugeras zheng boyer and berger szeliski and weiss cipolla and giblin tracking algorithms also improved a lot including contour tracking using active contours section such as snakes witkin and terzopoulos particle filters and isard and level sets sethian and vemuri as well as intensity-based techniques and kanade shi and tomasi rehg and kanade often applied to tracking faces taylor and cootes matthews and baker matthews xiao and baker and whole bodies black and fleet hilton fua and ronfard moeslund hilton and kr uger image segmentation chapter a topic which has been active since the earliest days of computer vision and fennema horowitz and pavlidis riseman and arbib rosenfeld and davis haralick and shapiro pavlidis and liow was also an active topic of research producing techniques based on minimum energy and shah and minimum description length normalized cuts and malik and mean shift and meer statistical learning techniques started appearing first in the application of principal component eigenface analysis to face recognition section and turk and pentland and linear dynamical systems for curve tracking section and blake and isard perhaps the most notable development in computer vision during this decade was the increased interaction with computer graphics and szeliski especially in the cross-disciplinary area of image-based modeling and rendering chapter the idea of manipulating real-world imagery directly to create new animations first came to prominence with image morphing techniques section and beier and neely and was later applied to view interpolation and williams seitz and dyer panoramic image stitching chapter and mann and picard chen szeliski szeliski and shum szeliski and full light-field rendering section and gortler grzeszczuk szeliski et al. levoy and hanrahan shade gortler he et al. at the same time image-based modeling techniques for automatically creating realistic models from collections of images were also being introduced torr and zisserman debevec taylor and malik taylor debevec and malik computer vision algorithms and applications draft figure recent examples of computer vision algorithms image-based rendering grzeszczuk szeliski et al. image-based modeling taylor and malik acm interactive tone mapping farbman uyttendaele et al. texture synthesis and freeman feature-based recognition perona and zisserman region-based recognition ren efros et al. ieee. this past decade has continued to see a deepening interplay between the vision and graphics fields. in particular many of the topics introduced under the rubric of image-based rendering such as image stitching chapter light-field capture and rendering section and high dynamic range image capture through exposure bracketing section and mann and picard debevec and malik were re-christened as computational photography chapter to acknowledge the increased use of such techniques in everyday digital photography. for example the rapid adoption of exposure bracketing to create high dynamic range images necessitated the development of tone mapping algorithms section to convert such images back to displayable results lischinski and werman durand and dorsey reinhard stark shirley et al. lischinski farbman uyttendaele et al. in addition to merging multiple exposures techniques were developed to merge flash images with non-flash counterparts and durand petschnigg agrawala hoppe et al. and to interactively or automatically select different regions from overlapping images book overview dontcheva agrawala et al. texture synthesis section quilting and leung efros and freeman kwatra sch odl essa et al. and inpainting sapiro caselles et al. bertalmio vese sapiro et al. criminisi p erez and toyama are additional topics that can be classified as computational photography techniques since they re-combine input image samples to produce new photographs. a second notable trend during this past decade has been the emergence of feature-based techniques with learning for object recognition section and ponce hebert schmid et al. some of the notable papers in this area include the constellation model of fergus perona and zisserman and the pictorial structures of felzenszwalb and huttenlocher feature-based techniques also dominate other recognition tasks such as scene recognition marszalek lazebnik et al. and panorama and location recognition and lowe schindler brown and szeliski and while interest point features tend to dominate current research some groups are pursuing recognition based on contours malik and puzicha and region segmentation ren efros et al. another significant trend from this past decade has been the development of more efficient algorithms for complex global optimization problems sections and and szeliski zabih scharstein et al. blake kohli and rother while this trend began with work on graph cuts veksler and zabih kohli and torr a lot of progress has also been made in message passing algorithms such as loopy belief propagation freeman and weiss kumar and torr the final trend which now dominates a lot of the visual recognition research in our community is the application of sophisticated machine learning techniques to computer vision problems section and freeman perona and sch olkopf this trend coincides with the increased availability of immense quantities of partially labelled data on the internet which makes it more feasible to learn object categories without the use of careful human supervision. book overview in the final part of this introduction i give a brief tour of the material in this book as well as a few notes on notation and some additional general references. since computer vision is such a broad field it is possible to study certain aspects of it e.g. geometric image formation and structure recovery without engaging other parts e.g. the modeling of reflectance and shading. some of the chapters in this book are only loosely coupled with others and it is not strictly necessary to read all of the material in sequence. computer vision algorithms and applications draft figure relationship between images geometry and photometry as well as a taxonomy of the topics covered in this book. topics are roughly positioned along the left right axis depending on whether they are more closely related to image-based geometry-based or appearance-based representations and on the vertical axis by increasing level of abstraction. the whole figure should be taken with a large grain of salt as there are many additional subtle connections between topics not illustrated here. images shapephotometry appearancevisiongraphicsimage geometric image photometric image samplingand image feature feature-based structure from computational stereo shape texture image-based book overview figure shows a rough layout of the contents of this book. since computer vision involves going from images to a structural description of the scene computer graphics the converse i have positioned the chapters horizontally in terms of which major component they address in addition to vertically according to their dependence. going from left to right we see the major column headings as images are in nature geometry encompasses descriptions and photometry encompasses object appearance. alternative labeling for these latter two could also be shape and appearance see e.g. chapter and kang szeliski and anandan going from top to bottom we see increasing levels of modeling and abstraction as well as techniques that build on previously developed algorithms. of course this taxonomy should be taken with a large grain of salt as the processing and dependencies in this diagram are not strictly sequential and subtle additional dependencies and relationships also exist some recognition techniques make use of information. the placement of topics along the horizontal axis should also be taken lightly as most vision algorithms involve mapping between at least two different interspersed throughout the book are sample applications which relate the algorithms and mathematical material being presented in various chapters to useful real-world applications. many of these applications are also presented in the exercises sections so that students can write their own. at the end of each section i provide a set of exercises that the students can use to implement test and refine the algorithms and techniques presented in each section. some of the exercises are suitable as written homework assignments others as shorter one-week projects and still others as open-ended research problems that make for challenging final projects. motivated students who implement a reasonable subset of these exercises will by the end of the book have a computer vision software library that can be used for a variety of interesting tasks and projects. as a reference book i try wherever possible to discuss which techniques and algorithms work well in practice as well as providing up-to-date pointers to the latest research results in the areas that i cover. the exercises can be used to build up your own personal library of selftested and validated vision algorithms which is more worthwhile in the long term you have the time than simply pulling algorithms out of a library whose performance you do not really understand. the book begins in chapter with a review of the image formation processes that create the images that we see and capture. understanding this process is fundamental if you want to take a scientific approach to computer vision. students who are eager to just start implementing algorithms courses that have limited time can skip ahead to the for an interesting comparison with what is known about the human visual system e.g. the largely parallel what and where pathways see some textbooks on human perception livingstone computer vision algorithms and applications draft image formation image processing features segmentation structure from motion motion stitching computational photography stereo shape image-based rendering recognition figure a pictorial summary of the chapter contents. sources brown szeliski and winder comaniciu and meer snavely seitz and szeliski nagel and enkelmann szeliski and shum debevec and malik gortler grzeszczuk szeliski et al. viola and jones see the figures in the respective chapters for copyright information. n book overview next chapter and dip into this material later. in chapter we break down image formation into three major components. geometric image formation deals with points lines and planes and how these are mapped onto images using projective geometry and other models radial lens distortion. photometric image formation covers radiometry which describes how light interacts with surfaces in the world and optics which projects light onto the sensor plane. finally section covers how sensors work including topics such as sampling and aliasing color sensing and in-camera compression. chapter covers image processing which is needed in almost all computer vision applications. this includes topics such as linear and non-linear filtering the fourier transform image pyramids and wavelets geometric transformations such as image warping and global optimization techniques such as regularization and markov random fields while most of this material is covered in courses and textbooks on image processing the use of optimization techniques is more typically associated with computer vision mrfs are now being widely used in image processing as well. the section on mrfs is also the first introduction to the use of bayesian inference techniques which are covered at a more abstract level in appendix b. chapter also presents applications such as seamless image blending and image restoration. in chapter we cover feature detection and matching. a lot of current reconstruction and recognition techniques are built on extracting and matching feature points so this is a fundamental technique required by many subsequent chapters and we also cover edge and straight line detection in sections and chapter covers region segmentation techniques including active contour detection and tracking segmentation techniques include top-down and bottom-up techniques mean shift techniques that find modes of clusters and various graphbased segmentation approaches. all of these techniques are essential building blocks that are widely used in a variety of applications including performance-driven animation interactive image editing and recognition. in chapter we cover geometric alignment and camera calibration. we introduce the basic techniques of feature-based alignment in section and show how this problem can be solved using either linear or non-linear least squares depending on the motion involved. we also introduce additional concepts such as uncertainty weighting and robust regression which are essential to making real-world systems work. feature-based alignment is then used as a building block for pose estimation calibration in section and camera calibration in section chapter also describes applications of these techniques to photo alignment for flip-book animations pose estimation from a hand-held camera and single-view reconstruction of building models. chapter covers the topic of structure from motion which involves the simultaneous recovery of camera motion and scene structure from a collection of tracked fea computer vision algorithms and applications draft tures. this chapter begins with the easier problem of point triangulation which is the reconstruction of points from matched features when the camera positions are known. it then describes two-frame structure from motion for which algebraic techniques exist as well as robust sampling techniques such as ransac that can discount erroneous feature matches. the second half of chapter describes techniques for multi-frame structure from motion including factorization bundle adjustment and constrained motion and structure models it also presents applications in view morphing sparse model construction and match move. in chapter we go back to a topic that deals directly with image intensities opposed to feature tracks namely dense intensity-based motion estimation flow. we start with the simplest possible motion models translational motion and cover topics such as hierarchical motion estimation fourier-based techniques and iterative refinement. we then present parametric motion models which can be used to compensate for camera rotation and zooming as well as affine or planar perspective motion this is then generalized to spline-based motion models and finally to general per-pixel optical flow including layered and learned motion models applications of these techniques include automated morphing frame interpolation motion and motion-based user interfaces. chapter is devoted to image stitching i.e. the construction of large panoramas and composites. while stitching is just one example of computation photography chapter there is enough depth here to warrant a separate chapter. we start by discussing various possible motion models including planar motion and pure camera rotation. we then discuss global alignment which is a special case of general bundle adjustment and then present panorama recognition i.e. techniques for automatically discovering which images actually form overlapping panoramas. finally we cover the topics of image compositing and blending which involve both selecting which pixels from which images to use and blending them together so as to disguise exposure differences. image stitching is a wonderful application that ties together most of the material covered in earlier parts of this book. it also makes for a good mid-term course project that can build on previously developed techniques such as image warping and feature detection and matching. chapter also presents more specialized variants of stitching such as whiteboard and document scanning video summarization panography full spherical panoramas and interactive photomontage for blending repeated action shots together. chapter presents additional examples of computational photography which is the process of creating new images from one or more input photographs often based on the careful modeling and calibration of the image formation process computational photography techniques include merging multiple exposures to create high dynamic range images increasing image resolution through blur removal and super-resolution book overview tion and image editing and compositing operations we also cover the topics of texture analysis synthesis and inpainting filling in section as well as non-photorealistic rendering in chapter we turn to the issue of stereo correspondence which can be thought of as a special case of motion estimation where the camera positions are already known this additional knowledge enables stereo algorithms to search over a much smaller space of correspondences and in many cases to produce dense depth estimates that can be converted into visible surface models we also cover multi-view stereo algorithms that build a true surface representation instead of just a single depth map applications of stereo matching include head and gaze tracking as well as depth-based background replacement chapter covers additional shape and appearance modeling techniques. these include classic shape-from-x techniques such as shape from shading shape from texture and shape from focus as well as shape from smooth occluding contours and silhouettes an alternative to all of these passive computer vision techniques is to use active rangefinding i.e. to project patterned light onto scenes and recover the geometry through triangulation. processing all of these representations often involves interpolating or simplifying the geometry or using alternative representations such as surface point sets the collection of techniques for going from one or more images to partial or full models is often called image-based modeling or photography. section examines three more specialized application areas faces and human bodies which can use model-based reconstruction to fit parameterized models to the sensed data. section examines the topic of appearance modeling i.e. techniques for estimating the texture maps albedos or even sometimes complete bi-directional reflectance distribution functions that describe the appearance of surfaces. in chapter we discuss the large number of image-based rendering techniques that have been developed in the last two decades including simpler techniques such as view interpolation layered depth images and sprites and layers as well as the more general framework of light fields and lumigraphs and higher-order fields such as environment mattes applications of these techniques include navigating collections of photographs using photo tourism and viewing models as object movies. in chapter we also discuss video-based rendering which is the temporal extension of image-based rendering. the topics we cover include video-based animation periodic video turned into video textures and video constructed from multiple video streams applications of these techniques include video denoising morphing and tours based on video. computer vision algorithms and applications draft week material project chapter image formation chapter image processing chapter feature detection and matching chapter feature-based alignment chapter image stitching chapter dense motion estimation chapter structure from motion chapter recognition chapter computational photography chapter stereo correspondence chapter reconstruction chapter image-based rendering final project presentations pp fp table sample syllabi for and courses. the weeks in parentheses are not used in the shorter version. and are two early-term mini-projects pp is when the final project proposals are due and fp is the final project presentations. chapter describes different approaches to recognition. it begins with techniques for detecting and recognizing faces and then looks at techniques for finding and recognizing particular objects recognition in section next we cover the most difficult variant of recognition namely the recognition of broad categories such as cars motorcycles horses and other animals and the role that scene context plays in recognition to support the book s use as a textbook the appendices and associated web site contain more detailed mathematical topics and additional material. appendix a covers linear algebra and numerical techniques including matrix algebra least squares and iterative techniques. appendix b covers bayesian estimation theory including maximum likelihood estimation robust statistics markov random fields and uncertainty modeling. appendix c describes the supplementary material available to complement this book including images and data sets pointers to software course slides and an on-line bibliography. sample syllabus teaching all of the material covered in this book in a single quarter or semester course is a herculean task and likely one not worth attempting. it is better to simply pick and choose a note on notation topics related to the lecturer s preferred emphasis and tailored to the set of mini-projects envisioned for the students. steve seitz and i have successfully used a syllabus similar to the one shown in table the parenthesized weeks as both an undergraduate and a graduate-level course in computer vision. the undergraduate tends to go lighter on the mathematics and takes more time reviewing basics while the graduate-level dives more deeply into techniques and assumes the students already have a decent grounding in either vision or related mathematical techniques. also the introduction to computer vision course at which uses a similar curriculum. related courses have also been taught on the topics of and computational when steve and i teach the course we prefer to give the students several small programming projects early in the course rather than focusing on written homework or quizzes. with a suitable choice of topics it is possible for these projects to build on each other. for example introducing feature matching early on can be used in a second assignment to do image alignment and stitching. alternatively direct flow techniques can be used to do the alignment and more focus can be put on either graph cut seam selection or multi-resolution blending techniques. we also ask the students to propose a final project provide a set of suggested topics for those who need ideas by the middle of the course and reserve the last week of the class for student presentations. with any luck some of these final projects can actually turn into conference submissions! no matter how you decide to structure the course or how you choose to use this book i encourage you to try at least a few small programming tasks to get a good feel for how vision techniques work and when they do not. better yet pick topics that are fun and can be used on your own photographs and try to push your creative boundaries to come up with surprising results. a note on notation for better or worse the notation found in computer vision and multi-view geometry textbooks tends to vary all over the map hartley and zisserman girod greiner and niemann faugeras and luong forsyth and ponce in this book i use the convention i first learned in my high school physics class later multi-variate computer vision algorithms and applications draft calculus and computer graphics courses which is that vectors v are lower case bold matrices m are upper case bold and scalars s are mixed case italic. unless otherwise noted vectors operate as column vectors i.e. they post-multiply matrices m v although they are sometimes written as comma-separated parenthesized lists x y instead of bracketed column vectors x yt some commonly used matrices are r for rotations k for calibration matrices and i for the identity matrix. homogeneous coordinates are denoted with a tilde over the vector e.g. x x y w wx y w x in p the cross product operator in matrix form is denoted by additional reading this book attempts to be self-contained so that students can implement the basic assignments and algorithms described here without the need for outside references. however it does presuppose a general familiarity with basic concepts in linear algebra and numerical techniques which are reviewed in appendix a and image processing which is reviewed in chapter students who want to delve more deeply into these topics can look in and van loan for matrix algebra and for linear algebra. in image processing there are a number of popular textbooks including gomes and velho j ahne pratt russ burger and burge gonzales and woods for computer graphics popular texts include van dam feiner et al. watt with providing a more in-depth look at image formation and rendering. for statistics and machine learning chris bishop s book is a wonderful and comprehensive introduction with a wealth of exercises. students may also want to look in other textbooks on computer vision for material that we do not cover here as well as for additional project ideas and brown faugeras nalwa trucco and verri forsyth and ponce there is however no substitute for reading the latest research literature both for the latest ideas and techniques and for the most up-to-date references to related in this book i have attempted to cite the most recent work in each field so that students can read them directly and use them as inspiration for their own work. browsing the last few years conference proceedings from the major vision and graphics conferences such as cvpr eccv iccv and siggraph will provide a wealth of new ideas. the tutorials offered at these conferences for which slides or notes are often available on-line are also an invaluable resource. for a comprehensive bibliography and taxonomy of computer vision research keith price s annotated com puter vision bibliography httpwww.visionbib.combibliographycontents.html is an invaluable resource. chapter image formation geometric primitives transformations transformations rotations to projections lens distortions photometric image formation geometric primitives and transformations lighting reflectance and shading optics the digital camera sampling and aliasing color compression additional reading exercises computer vision algorithms and applications draft figure a few components of the image formation process perspective projection light scattering when hitting a surface lens optics bayer color filter array. geometric primitives and transformations before we can intelligently analyze and manipulate images we need to establish a vocabulary for describing the geometry of a scene. we also need to understand the image formation process that produced a particular image given a set of lighting conditions scene geometry surface properties and camera optics. in this chapter we present a simplified model of such an image formation process. section introduces the basic geometric primitives used throughout the book lines and planes and the geometric transformations that project these quantities into image features section describes how lighting surface properties and camera optics interact in order to produce the color values that fall onto the image sensor. section describes how continuous color images are turned into discrete digital samples inside the image sensor and how to avoid at least characterize sampling deficiencies such as aliasing. the material covered in this chapter is but a brief summary of a very rich and deep set of topics traditionally covered in a number of separate fields. a more thorough introduction to the geometry of points lines planes and projections can be found in textbooks on multi-view geometry and zisserman faugeras and luong and computer graphics van dam feiner et al. the image formation process is traditionally taught as part of a computer graphics curriculum van dam feiner et al. glassner watt shirley but it is also studied in physics-based computer vision shafer and healey the behavior of camera lens systems is studied in optics oller hecht ray two good books on color theory are and stiles healey and shafer with providing a more fun and informal introduction to the topic of color perception. topics relating to sampling and aliasing are covered in textbooks on signal and image processing j ahne oppenheim and schafer oppenheim schafer and buck pratt russ burger and burge gonzales and woods a note to students if you have already studied computer graphics you may want to skim the material in section although the sections on projective depth and object-centered projection near the end of section may be new to you. similarly physics students well as computer graphics students will mostly be familiar with section finally students with a good background in image processing will already be familiar with sampling issues as well as some of the material in chapter geometric primitives and transformations in this section we introduce the basic and primitives used in this textbook namely points lines and planes. we also describe how features are projected into features. computer vision algorithms and applications draft more detailed descriptions of these topics with a gentler and more intuitive introduction can be found in textbooks on multiple-view geometry and zisserman faugeras and luong geometric primitives geometric primitives form the basic building blocks used to describe three-dimensional shapes. in this section we introduce points lines and planes. later sections of the book discuss curves and surfaces and volumes points. points coordinates in an image can be denoted using a pair of values x y or alternatively x x y stated in the introduction we use the notation to denote column vectors. points can also be represented using homogeneous coordinates x x y w p where vectors that differ only by scale are considered to be equivalent. p is called the projective space. a homogeneous vector x can be converted back into an inhomogeneous vector x by dividing through by the last element w i.e. x x y w wx y w x where x y is the augmented vector. homogeneous points whose last element is w are called ideal points or points at infinity and do not have an equivalent inhomogeneous representation. lines. the corresponding line equation is lines can also be represented using homogeneous coordinates l b c. x l ax by c we can normalize the line equation vector so that l nx ny d n d with in this case n is the normal vector perpendicular to the line and d is its distance to the origin one exception to this normalization is the line at infinity l which includes all points at infinity. we can also express n as a function of rotation angle n nx ny sin this representation is commonly used in the hough transform line-finding geometric primitives and transformations figure line equation and plane equation expressed in terms of the normal n and distance to the origin d. algorithm which is discussed in section the combination d is also known as polar coordinates. when using homogeneous coordinates we can compute the intersection of two lines as x where is the cross product operator. similarly the line joining two points can be written as l when trying to fit an intersection point to multiple lines or conversely a line to multiple points least squares techniques and appendix can be used as discussed in exercise conics. there are other algebraic curves that can be expressed with simple polynomial homogeneous equations. for example the conic sections called because they arise as the intersection of a plane and a cone can be written using a quadric equation xt q x quadric equations play useful roles in the study of multi-view geometry and camera calibration and zisserman faugeras and luong but are not used extensively in this book. points. point coordinates in three dimensions can be written using inhomogeneous coordinates x y z or homogeneous coordinates x x y z w p as before it is sometimes useful to denote a point using the augmented vector x y z with x w x. yxd nlzxdnmy computer vision algorithms and applications draft figure line equation r q. planes. planes can also be represented as homogeneous coordinates m b c d with a corresponding plane equation x m ax by cz d we can also normalize the plane equation as m nx ny nz d n d with in this case n is the normal vector perpendicular to the plane and d is its distance to the origin as with the case of lines the plane at infinity m which contains all the points at infinity cannot be normalized it does not have a unique normal or a finite distance. we can express n as a function of two angles n cos sin cos sin i.e. using spherical coordinates but these are less commonly used than polar coordinates since they do not uniformly sample the space of possible normal vectors. lines. lines in are less elegant than either lines in or planes in one possible representation is to use two points on the line q. any other point on the line can be expressed as a linear combination of these two points r q as shown in figure if we restrict we get the line segment joining p and q. if we use homogeneous coordinates we can write the line as a special case of this is when the second point is at infinity i.e. q dx dy dz d here we see that d is the direction of the line. we can then re-write the inhomogeneous line equation as r p q. r p d. zx q geometric primitives and transformations a disadvantage of the endpoint representation for lines is that it has too many degrees of freedom i.e. six for each endpoint instead of the four degrees that a line truly has. however if we fix the two points on the line to lie in specific planes we obtain a representation with four degrees of freedom. for example if we are representing nearly vertical lines then z and z form two suitable planes i.e. the y coordinates in both planes provide the four coordinates describing the line. this kind of two-plane parameterization is used in the light field and lumigraph image-based rendering systems described in chapter to represent the collection of rays seen by a camera as it moves in front of an object. the two-endpoint representation is also useful for representing line segments even when their exact endpoints cannot be seen guessed at. if we wish to represent all possible lines without bias towards any particular orientation we can use pl ucker coordinates and zisserman chapter faugeras and luong chapter these coordinates are the six independent non-zero entries in the skew symmetric matrix l p qt q pt where p and q are any two points on the line. this representation has only four degrees of freedom since l is homogeneous and also satisfies detl which results in a quadratic constraint on the pl ucker coordinates. in practice the minimal representation is not essential for most applications. an adequate model of lines can be obtained by estimating their direction may be known ahead of time e.g. for architecture and some point within the visible portion of the line section or by using the two endpoints since lines are most often visible as finite line segments. however if you are interested in more details about the topic of minimal line parameterizations f orstner discusses various ways to infer and model lines in projective geometry as well as how to estimate the uncertainty in such fitted models. quadrics. the analog of a conic section is a quadric surface xt q x and zisserman chapter again while quadric surfaces are useful in the study of multi-view geometry and can also serve as useful modeling primitives ellipsoids cylinders we do not study them in great detail in this book. transformations having defined our basic primitives we can now turn our attention to how they can be transformed. the simplest transformations occur in the plane and are illustrated in figure computer vision algorithms and applications draft figure basic set of planar transformations. translation. translations can be written as x t or where i is the identity matrix or i i t x x t where is the zero vector. using a matrix results in a more compact notation whereas using a full-rank matrix can be obtained from the matrix by appending a row makes it possible to chain transformations using matrix multiplication. note that in any equation where an augmented vector such as x appears on both sides it can always be replaced with a full homogeneous vector x. where rotation translation. this transformation is also known as rigid body motion or the euclidean transformation euclidean distances are preserved. it can be written as rx t or r t x r cos sin cos is an orthonormal rotation matrix with rrt i and scaled rotation. also known as the similarity transform this transformation can be expressed as srx t where s is an arbitrary scale factor. it can also be written as sin sr t x a b a b tx ty x where we no longer require that the similarity transform preserves angles between lines. yxsimilarityeuclideanaffineprojectivetranslation geometric primitives and transformations affine. the affine transformation is written as a x where a is an arbitrary matrix i.e. x. parallel lines remain parallel under affine transformations. projective. this transformation also known as a perspective transform or homography operates on homogeneous coordinates h x where h is an arbitrary matrix. note that h is homogeneous i.e. it is only defined up to a scale and that two h matrices that differ only by scale are equivalent. the resulting homogeneous coordinate must be normalized in order to obtain an inhomogeneous result x i.e. and perspective transformations preserve straight lines they remain straight after the transformation. hierarchy of transformations. the preceding set of transformations are illustrated in figure and summarized in table the easiest way to think of them is as a set of restricted matrices operating on homogeneous coordinate vectors. hartley and zisserman contains a more detailed description of the hierarchy of planar transformations. the above transformations form a nested set of groups i.e. they are closed under composition and have an inverse that is a member of the same group. will be important later when applying these transformations to images in section each group is a subset of the more complex group below it. co-vectors. while the above transformations can be used to transform points in a plane can they also be used directly to transform a line equation? consider the homogeneous equation l x if we transform hx we obtain h x h t x l x i.e. h t l. thus the action of a projective transformation on a co-vector such as a line or normal can be represented by the transposed inverse of the matrix which is equivalent to the adjoint of h since projective transformation matrices are homogeneous. jim computer vision algorithms and applications draft transformation matrix dof preserves icon translation rigid similarity affine projective i t r t sr t a h orientation lengths angles ss ss s s straight lines parallelism table hierarchy of coordinate transformations. each transformation also preserves the properties listed in the rows below it i.e. similarity preserves not only angles but also parallelism and straight lines. the matrices are extended with a third row to form a full matrix for homogeneous coordinate transformations. blinn describes chapters and the ins and outs of notating and manipulating co-vectors. while the above transformations are the ones we use most extensively a number of addi tional transformations are sometimes used. stretchsquash. this transformation changes the aspect ratio of an image sxx tx syy ty and is a restricted form of an affine transformation. unfortunately it does not nest cleanly with the groups listed in table planar surface flow. this eight-parameter transformation bergen anandan hanna et al. girod greiner and niemann arises when a planar surface undergoes a small motion. it can thus be thought of as a small motion approximation to a full homography. its main attraction is that it is linear in the motion parameters ak which are often the quantities being estimated. geometric primitives and transformations transformation matrix dof preserves icon translation rigid similarity affine projective i t r t sr t a h orientation angles lengths ss ss s s straight lines parallelism table hierarchy of coordinate transformations. each transformation also preserves the properties listed in the rows below it i.e. similarity preserves not only angles but also parallelism and straight lines. the matrices are extended with a fourth row to form a full matrix for homogeneous coordinate transformations. the mnemonic icons are drawn in but are meant to suggest transformations occurring in a full cube. bilinear interpolant. this eight-parameter transform can be used to interpolate the deformation due to the motion of the four corner points of a square. fact it can interpolate the motion of any four non-collinear points. while the deformation is linear in the motion parameters it does not generally preserve straight lines lines parallel to the square axes. however it is often quite useful e.g. in the interpolation of sparse grids using splines transformations the set of three-dimensional coordinate transformations is very similar to that available for transformations and is summarized in table as in these transformations form a nested set of groups. hartley and zisserman section give a more detailed description of this hierarchy. translation. translations can be written as x t or i t x computer vision algorithms and applications draft where i is the identity matrix and is the zero vector. rotation translation. also known as rigid body motion or the euclidean transformation it can be written as rx t or r t x where r is a orthonormal rotation matrix with rrt i and note that sometimes it is more convenient to describe a rigid motion using rx c rx rc where c is the center of rotation the camera center. compactly parameterizing a rotation is a non-trivial task which we describe in more detail below. scaled rotation. the similarity transform can be expressed as srx t where s is an arbitrary scale factor. it can also be written as this transformation preserves angles between lines and planes. sr t x. affine. the affine transform is written as a x where a is an arbitrary matrix i.e. x. parallel lines and planes remain parallel under affine transformations. projective. this transformation variously known as a perspective transform homography or collineation operates on homogeneous coordinates h x where h is an arbitrary homogeneous matrix. as in the resulting homogeneous coordinate must be normalized in order to obtain an inhomogeneous result x. perspective transformations preserve straight lines they remain straight after the transformation. geometric primitives and transformations figure rotation around an axis n by an angle rotations the biggest difference between and coordinate transformations is that the parameterization of the rotation matrix r is not as straightforward but several possibilities exist. euler angles a rotation matrix can be formed as the product of three rotations around three cardinal axes e.g. x y and z or x y and x. this is generally a bad idea as the result depends on the order in which the transforms are applied. what is worse it is not always possible to move smoothly in the parameter space i.e. sometimes one or more of the euler angles change dramatically in response to a small change in for these reasons we do not even give the formula for euler angles in this book interested readers can look in other textbooks or technical reports diebel note that in some applications if the rotations are known to be a set of uni-axial transforms they can always be represented using an explicit set of rigid transformations. axisangle twist a rotation can be represented by a rotation axis n and an angle or equivalently by a vector n. figure shows how we can compute the equivalent rotation. first we project the vector v onto the axis n to obtain n n v n nt which is the component of v that is not affected by the rotation. next we compute the perpendicular residual of v from n in robotics this is sometimes referred to as gimbal lock. v v n nt vv nv v v u u n nz ny nz nx ny nx computer vision algorithms and applications draft we can rotate this vector by using the cross product v n v n v where n is the matrix form of the cross product operator with the vector n nx ny nz note that rotating this vector by another is equivalent to taking the cross product again v n v v v and hence v v v v we can now compute the in-plane component of the rotated vector u as u cos v sin v n cos putting all these terms together we obtain the final rotated vector as u u sin n cos we can therefore write the rotation matrix corresponding to a rotation by around an axis n as r n i sin n cos which is known as rodriguez s formula the product of the axis n and angle n x y z is a minimal representation for a rotation. rotations through common angles such as multiples of can be represented exactly converted to exact matrices if is stored in degrees. unfortunately this representation is not unique since we can always add a multiple of radians to and get the same rotation matrix. as well n and n represent the same rotation. however for small rotations corrections to rotations this is an excellent choice. in particular for small or instantaneous rotations and expressed in radians rodriguez s formula simplifies to r i sin n i n z y z x y x geometric primitives and transformations which gives a nice linearized relationship between the rotation parameters and r. we can also write r v v which is handy when we want to compute the derivative of rv with respect to rv t z y z x y x another way to derive a rotation through a finite angle is called the exponential twist li and sastry a rotation by an angle is equivalent to k rotations through in the limit as k we obtain r n lim k n exp k if we expand the matrix exponential as a taylor series the identity k and again assuming is in radians nk exp i n n i i sin n cos which yields the familiar rodriguez s formula. unit quaternions the unit quaternion representation is closely related to the angleaxis representation. a unit quaternion is a unit length whose components can be written as q qy qz qw or q y z w for short. unit quaternions live on the unit sphere and antipodal sign quaternions q and q represent the same rotation other than this ambiguity covering the unit quaternion representation of a rotation is unique. furthermore the representation is continuous i.e. as rotation matrices vary continuously one can find a continuous quaternion representation although the path on the quaternion sphere may wrap all the way around before returning to the origin qo for these and other reasons given below quaternions are a very popular representation for pose and for pose interpolation in computer graphics quaternions can be derived from the axisangle representation through the formula q w n cos computer vision algorithms and applications draft figure unit quaternions live on the unit sphere this figure shows a smooth trajectory through the three quaternions and the antipodal point to namely represents the same rotation as where n and are the rotation axis and angle. using the trigonometric identities sin sin rodriguez s formula can be converted to cos and cos r n i sin n cos i this suggests a quick way to rotate a vector v by a quaternion using a series of cross products scalings and additions. to obtain a formula for rq as a function of y z w recall that we thus obtain z z y y x x and xy xz xy yz xz yz rq zw yw zw xw yw xw the diagonal terms can be made more symmetrical by replacing with etc. the nicest aspect of unit quaternions is that there is a simple algebra for composing rotations expressed as unit quaternions. given two quaternions and the quaternion multiply operator is defined as zxw q geometric primitives and transformations with the property that note that quaternion multiplication is not commutative just as rotations and matrix multiplications are not. taking the inverse of a quaternion is easy just flip the sign of v or w not both!. can verify this has the desired effect of transposing the r matrix in thus we can also define quaternion division as this is useful when the incremental rotation between two rotations is desired. in particular if we want to determine a rotation that is partway between two given rotations we can compute the incremental rotation take a fraction of the angle and compute the new rotation. this procedure is called spherical linear interpolation or slerp for short and is given in algorithm note that shoemake presents two formulas other than the one given here. the first exponentiates qr by alpha before multiplying the original quaternion while the second treats the quaternions as on a sphere and uses q r sin sin sin where cos and the dot product is directly between the quaternion all of these formulas give comparable results although care should be taken when and are close together which is why i prefer to use an arctangent to establish the rotation angle. which rotation representation is better? the choice of representation for rotations depends partly on the application. the axisangle representation is minimal and hence does not require any additional constraints on the parameters need to re-normalize after each update. if the angle is expressed in degrees it is easier to understand the pose twist around x-axis and also easier to express exact rotations. when the angle is in radians the derivatives of r with respect to can easily be computed quaternions on the other hand are better if you want to keep track of a smoothly moving camera since there are no discontinuities in the representation. it is also easier to interpolate between rotations and to chain rigid transformations li and sastry bregler and malik my usual preference is to use quaternions but to update their estimates using an incre mental rotation as described in section computer vision algorithms and applications draft procedure qr wr if wr then qr qr r tan nr n r nr cos q return q algorithm spherical linear interpolation the axis and total angle are first computed from the quaternion ratio. computation can be lifted outside an inner loop that generates a set of interpolated position for animation. an incremental quaternion is then computed and multiplied by the starting rotation quaternion. to projections now that we know how to represent and geometric primitives and how to transform them spatially we need to specify how primitives are projected onto the image plane. we can do this using a linear to projection matrix. the simplest model is orthography which requires no division to get the final result. the more commonly used model is perspective since this more accurately models the behavior of real cameras. orthography and para-perspective an orthographic projection simply drops the z component of the three-dimensional coordinate p to obtain the point x. this section we use p to denote points and x to denote points. this can be written as if we are using homogeneous coordinates we can write x p. x p geometric primitives and transformations view orthography scaled orthography para-perspective perspective object-centered figure commonly used projection models view of world orthography scaled orthography para-perspective perspective object-centered. each diagram shows a top-down view of the projection. note how parallel lines on the ground plane and box sides remain parallel in the non-perspective projections. computer vision algorithms and applications draft i.e. we drop the z component but keep the w component. orthography is an approximate model for long focal length lenses and objects whose depth is shallow relative to their distance to the camera and hanson it is exact only for telecentric lenses and nayar in practice world coordinates may measure dimensions in meters need to be scaled to fit onto an image sensor measured in millimeters but ultimately measured in pixels. for this reason scaled orthography is actually more commonly used x p. this model is equivalent to first projecting the world points onto a local fronto-parallel image plane and then scaling this image using regular perspective projection. the scaling can be the same for all parts of the scene or it can be different for objects that are being modeled independently more importantly the scaling can vary from frame to frame when estimating structure from motion which can better model the scale change that occurs as an object approaches the camera. scaled orthography is a popular model for reconstructing the shape of objects far away from the camera since it greatly simplifies certain computations. for example pose orientation can be estimated using simple least squares under orthography structure and motion can simultaneously be estimated using factorization value decomposition as discussed in section and kanade a closely related projection model is para-perspective poelman and kanade in this model object points are again first projected onto a local reference parallel to the image plane. however rather than being projected orthogonally to this plane they are projected parallel to the line of sight to the object center this is followed by the usual projection onto the final image plane which again amounts to a scaling. the combination of these two projections is therefore affine and can be written as x p. note how parallel lines in remain parallel after projection in figure d. para-perspective provides a more accurate projection model than scaled orthography without incurring the added complexity of per-pixel perspective division which invalidates traditional factorization methods and kanade perspective the most commonly used projection in computer graphics and computer vision is true perspective here points are projected onto the image plane by dividing them geometric primitives and transformations by their z component. using inhomogeneous coordinates this can be written as in homogeneous coordinates the projection has a simple linear form xz yz x pzp x p i.e. we drop the w component of p. thus after projection it is not possible to recover the distance of the point from the image which makes sense for a imaging sensor. a form often seen in computer graphics systems is a two-step projection that first projects coordinates into normalized device coordinates in the range y z and then rescales these coordinates to integer pixel coordinates using a viewport transformation opengl-arb the perspective projection is then represented using a matrix p x zfarzrange znearzfarzrange where znear and zfar are the near and far z clipping planes and zrange zfar znear. note that the first two rows are actually scaled by the focal length and the aspect ratio so that visible rays are mapped to y z the reason for keeping the third row rather than dropping it is that visibility operations such as z-buffering require a depth for every graphical element that is being rendered. if we set znear zfar and switch the sign of the third row the third element of the normalized screen vector becomes the inverse depth i.e. the disparity and kanade this can be quite convenient in many cases since for cameras moving around outdoors the inverse depth to the camera is often a more well-conditioned parameterization than direct distance. while a regular image sensor has no way of measuring distance to a surface point range sensors and stereo matching algorithms can compute such values. it is then convenient to be able to map from a sensor-based depth or disparity value d directly back to a location using the inverse of a matrix we can do this if we represent perspective projection using a full-rank matrix as in computer vision algorithms and applications draft figure projection of a camera-centered point pc onto the sensor planes at location p. oc is the camera center point cs is the origin of the sensor plane coordinate system and sx and sy are the pixel spacings. camera intrinsics once we have projected a point through an ideal pinhole using a projection matrix we must still transform the resulting coordinates according to the pixel sensor spacing and the relative position of the sensor plane to the origin. figure shows an illustration of the geometry involved. in this section we first present a mapping from pixel coordinates to rays using a sensor homography m s since this is easier to explain in terms of physically measurable quantities. we then relate these quantities to the more commonly used camera intrinsic matrix k which is used to map camera-centered points pc to pixel coordinates xs. image sensors return pixel values indexed by integer pixel coordinates ys often with the coordinates starting at the upper-left corner of the image and moving down and to the right. convention is not obeyed by all imaging libraries but the adjustment for other coordinate systems is straightforward. to map pixel centers to coordinates we first scale the ys values by the pixel spacings sy expressed in microns for solid-state sensors and then describe the orientation of the sensor array relative to the camera projection center oc with an origin cs and a rotation rs the combined to projection can then be written as p rs cs the first two columns of the matrix m s are the vectors corresponding to unit steps in the image pixel array along the xs and ys directions while the third column is the image array origin cs. sx sy xs ys m s xs. zcxccsycxsyssxsypcpoc geometric primitives and transformations the matrix m s is parameterized by eight unknowns the three parameters describing the rotation rs the three parameters describing the translation cs and the two scale factors sy. note that we ignore here the possibility of skew between the two axes on the image plane since solid-state manufacturing techniques render this negligible. in practice unless we have accurate external knowledge of the sensor spacing or sensor orientation there are only seven degrees of freedom since the distance of the sensor from the origin cannot be teased apart from the sensor spacing based on external image measurement alone. however estimating a camera model m s with the required seven degrees of freedom where the first two columns are orthogonal after an appropriate re-scaling is impractical so most practitioners assume a general homogeneous matrix form. the relationship between the pixel center p and the camera-centered point pc is given by an unknown scaling s p spc. we can therefore write the complete projection between pc and a homogeneous version of the pixel address xs as xs m s pc kpc. the matrix k is called the calibration matrix and describes the camera intrinsics opposed to the camera s orientation in space which are called the extrinsics. from the above discussion we see that k has seven degrees of freedom in theory and eight degrees of freedom full dimensionality of a homogeneous matrix in practice. why then do most textbooks on computer vision and multi-view geometry hartley and zisserman faugeras and luong treat k as an upper-triangular matrix with five degrees of freedom? while this is usually not made explicit in these books it is because we cannot recover the full k matrix based on external measurement alone. when calibrating a camera based on external points or other measurements we end up estimating the intrinsic and extrinsic t camera parameters simultaneously using a series of measurements where pw are known world coordinates and xs r t pw p pw p krt is known as the camera matrix. inspecting this equation we see that we can post-multiply and still end up with a valid calibration. thus it k by and pre-multiply by rt is impossible based on image measurements alone to know the true orientation of the sensor and the true camera intrinsics. the choice of an upper-triangular form for k seems to be conventional. given a full camera matrix p krt we can compute an upper-triangular k matrix using qr computer vision algorithms and applications draft figure simplified camera intrinsics showing the focal length f and the optical center cy. the image width and height are w and h. factorization and van loan the unfortunate clash of terminologies in matrix algebra textbooks r represents an upper-triangular of the diagonal matrix in computer vision r is an orthogonal rotation. there are several ways to write the upper-triangular form of k. one possibility is k fx s fy cx cy k k f s af cx cy f f cx cy which uses independent focal lengths fx and fy for the sensor x and y dimensions. the entry s encodes any possible skew between the sensor axes due to the sensor not being mounted perpendicular to the optical axis and cy denotes the optical center expressed in pixel coordinates. another possibility is where the aspect ratio a has been made explicit and a common focal length f is used. in practice for many applications an even simpler form can be obtained by setting a and s often setting the origin at roughly the center of the image e.g. cy where w and h are the image height and width can result in a perfectly usable camera model with a single unknown i.e. the focal length f. geometric primitives and transformations figure central projection showing the relationship between the and coordinates p and x as well as the relationship between the focal length f image width w and the field of view figure shows how these quantities can be visualized as part of a simplified imaging model. note that now we have placed the image plane in front of the nodal point center of the lens. the sense of the y axis has also been flipped to get a coordinate system compatible with the way that most imaging libraries treat the vertical coordinate. certain graphics libraries such as use a left-handed coordinate system which can lead to some confusion. a note on focal lengths the issue of how to express focal lengths is one that often causes confusion in implementing computer vision algorithms and discussing their results. this is because the focal length depends on the units used to measure pixels. if we number pixel coordinates using integer values say w h the focal length f and camera center cy in can be expressed as pixel values. how do these quantities relate to the more familiar focal lengths used by photographers? figure illustrates the relationship between the focal length f the sensor width w and the field of view which obey the formula tan w or f w for conventional film cameras w and hence f is also expressed in millimeters. since we work with digital images it is more convenient to express w in pixels so that the focal length f can be used directly in the calibration matrix k as in another possibility is to scale the pixel coordinates so that they go from along the longer image dimension and a a along the shorter axis where a is the image aspect ratio opposed to the sensor cell aspect ratio introduced earlier. this can be computer vision algorithms and applications draft accomplished using modified normalized device coordinates w and hs where s maxw h. this has the advantage that the focal length f and optical center cy become independent of the image resolution which can be useful when using multi-resolution image-processing algorithms such as image pyramids the use of s instead of w also makes the focal length the same for landscape and portrait pictures as is the case in photography. some computer graphics textbooks and systems normalized device coordinates go from which requires the use of two different focal lengths to describe the camera intrinsics opengl-arb setting s w in we obtain the simpler relationship f tan the conversion between the various focal length representations is straightforward e.g. to go from a unitless f to one expressed in pixels multiply by while to convert from an f expressed in pixels to the equivalent focal length multiply by camera matrix now that we have shown how to parameterize the calibration matrix k we can put the camera intrinsics and extrinsics together to obtain a single camera matrix it is sometimes preferable to use an invertible matrix which can be obtained by not dropping the last row in the p matrix p r t p k r t ke where e is a rigid-body transformation and k is the full-rank calibration matrix. the camera matrix p can be used to map directly from world coordinates pw yw zw to screen coordinates disparity xs ys d xs p pw where indicates equality up to scale. note that after multiplication by p the vector is divided by the third element of the vector to obtain the normalized form xs ys d. to make the conversion truly accurate after a downsampling step in a pyramid floating point values of w and h would have to be maintained since they can become non-integral if they are ever odd at a larger resolution in the pyramid. geometric primitives and transformations figure regular disparity depth and projective depth from a reference plane. plane plus parallax depth in general when using the matrix p we have the freedom to remap the last row to whatever suits our purpose than just being the standard interpretation of disparity as inverse depth. let us re-write the last row of p as where we then have the equation d z pw where z pw rz c is the distance of pw from the camera center c along the optical axis z thus we can interpret d as the projective disparity or projective depth of a scene point pw from the reference plane pw and coughlan szeliski and golland shade gortler he et al. baker szeliski and anandan projective depth is also sometimes called parallax in reconstruction algorithms that use the term plane plus parallax anandan and hanna sawhney setting and i.e. putting the reference plane at infinity results in the more standard d version of disparity and kanade another way to see this is to invert the p matrix so that we can map pixels plus disparity directly back to points pw p xs. in general we can choose p to have whatever form is convenient i.e. to sample space using an arbitrary projection. this can come in particularly handy when setting up multi-view stereo reconstruction algorithms since it allows us to sweep a series of planes through space with a variable sampling that best matches the sensed image motions szeliski and golland saito and kanade cxsysdzimage inverse depthxwywzwdzcxsysdzimage projective depthxwywzwzplaneparallax computer vision algorithms and applications draft figure a point is projected into two images relationship between the point coordinate y z and the projected point y d planar homography induced by points all lying on a common plane p mapping from one camera to another what happens when we take two images of a scene from different camera positions or orientations using the full rank camera matrix p ke from we can write the projection from world to screen coordinates as p assuming that we know the z-buffer or disparity value for a pixel in one image we can compute the point location p using p e and then project it into another image yielding k k p p m unfortunately we do not usually have access to the depth coordinates of pixels in a regular photographic image. however for a planar scene as discussed above in we can replace the last row of p in with a general plane equation p that maps points on the plane to values thus if we set we can ignore the last column of m in and also its last row since we do not care about the final z-buffer depth. the mapping equation thus reduces to h where h is a general homography matrix and and are now homogeneous coordinates justifies the use of the homography as a general alignment model for mosaics of planar scenes and picard szeliski p geometric primitives and transformations the other special case where we do not need to know depth to perform inter-camera mapping is when the camera is undergoing pure rotation i.e. when in this case we can write k which again can be represented with a homography. if we assume that the calibration matrices have known aspect ratios and centers of projection this homography can be parameterized by the rotation amount and the two unknown focal lengths. this particular formulation is commonly used in image-stitching applications object-centered projection when working with long focal length lenses it often becomes difficult to reliably estimate the focal length from image measurements alone. this is because the focal length and the distance to the object are highly correlated and it becomes difficult to tease these two effects apart. for example the change in scale of an object viewed through a zoom telephoto lens can either be due to a zoom change or a motion towards the user. effect was put to dramatic use in some of alfred hitchcock s film vertigo where the simultaneous change of zoom and camera motion produces a disquieting effect. this ambiguity becomes clearer if we write out the projection equation corresponding to the simple calibration matrix k xs f ys f rx p tx rz p tz ry p ty rz p tz cx cy where rx ry and rz are the three rows of r. if the distance to the object center tz size of the object the denominator is approximately tz and the overall scale of the projected object depends on the ratio of f to tz. it therefore becomes difficult to disentangle these two quantities. to see this more clearly let z t z equations as and s zf. we can then re-write the above xs s ys s rx p tx zrz p ry p ty zrz p cx cy and kang pighin hecker lischinski et al. the scale of the projection s can be reliably estimated if we are looking at a known object the coordinates p computer vision algorithms and applications draft are known. the inverse distance z is now mostly decoupled from the estimates of s and can be estimated from the amount of foreshortening as the object rotates. furthermore as the lens becomes longer i.e. the projection model becomes orthographic there is no need to replace a perspective imaging model with an orthographic one since the same equation can be used with z opposed to f and tz both going to infinity. this allows us to form a natural link between orthographic reconstruction techniques such as factorization and their projectiveperspective counterparts lens distortions the above imaging models all assume that cameras obey a linear projection model where straight lines in the world result in straight lines in the image. follows as a natural consequence of linear matrix operations being applied to homogeneous coordinates. unfortunately many wide-angle lenses have noticeable radial distortion which manifests itself as a visible curvature in the projection of straight lines. section for a more detailed discussion of lens optics including chromatic aberration. unless this distortion is taken into account it becomes impossible to create highly accurate photorealistic reconstructions. for example image mosaics constructed without taking radial distortion into account will often exhibit blurring due to the mis-registration of corresponding features before pixel blending fortunately compensating for radial distortion is not that difficult in practice. for most lenses a simple quartic model of distortion can produce good results. let yc be the pixel coordinates obtained after perspective division but before scaling by focal length f and shifting by the optical center cy i.e. xc yc rx p tx rz p tz ry p ty rz p tz the radial distortion model says that coordinates in the observed images are displaced away distortion or towards distortion the image center by an amount proportional to their radial distance the simplest radial distortion models use low-order polynomials e.g. xc yc c c c c anamorphic lenses which are widely used in feature film production do not follow this radial distortion model. instead they can be thought of to a first approximation as inducing different vertical and horizontal scalings i.e. non-square pixels. geometric primitives and transformations figure radial lens distortions barrel pincushion and fisheye. the fisheye image spans almost from side-to-side. c c where radial distortion step the final pixel coordinates can be computed using c and and are called the radial distortion after the xs f cx ys f cy. a variety of techniques can be used to estimate the radial distortion parameters for a given lens as discussed in section sometimes the above simplified model does not model the true distortions produced by complex lenses accurately enough at very wide angles. a more complete analytic model also includes tangential distortions and decentering distortions but these distortions are not covered in this book. fisheye lenses require a model that differs from traditional polynomial models of radial distortion. fisheye lenses behave to a first approximation as equi-distance projectors of angles away from the optical axis and turkowski which is the same as the polar projection described by equations xiong and turkowski describe how this model can be extended with the addition of an extra quadratic correction in and how the unknown parameters of projection scaling factor s etc. can be estimated from a set of overlapping fisheye images using a direct non-linear minimization algorithm. for even larger less regular distortions a parametric distortion model using splines may be necessary if the lens does not have a single center of projection it sometimes the relationship between xc and xc is expressed the other way around i.e. xc c c this is convenient if we map image pixels into rays by dividing through by f. we can then undistort the rays and have true rays in space. computer vision algorithms and applications draft may become necessary to model the line opposed to direction corresponding to each pixel separately thorpe and kanade champleboux lavall ee sautot et al. grossberg and nayar sturm and ramalingam tardif sturm trudeau et al. some of these techniques are described in more detail in section which discusses how to calibrate lens distortions. there is one subtle issue associated with the simple radial distortion model that is often glossed over. we have introduced a non-linearity between the perspective projection and final sensor array projection steps. therefore we cannot in general post-multiply an arbitrary matrix k with a rotation to put it into upper-triangular form and absorb this into the global rotation. however this situation is not as bad as it may at first appear. for many applications keeping the simplified diagonal form of is still an adequate model. furthermore if we correct radial and other distortions to an accuracy where straight lines are preserved we have essentially converted the sensor back into a linear imager and the previous decomposition still applies. photometric image formation in modeling the image formation process we have described how geometric features in the world are projected into features in an image. however images are not composed of features. instead they are made up of discrete color or intensity values. where do these values come from? how do they relate to the lighting in the environment surface properties and geometry camera optics and sensor properties in this section we develop a set of models to describe these interactions and formulate a generative process of image formation. a more detailed treatment of these topics can be found in other textbooks on computer graphics and image synthesis weyrich lawrence lensch et al. foley van dam feiner et al. watt cohen and wallace sillion and puech lighting images cannot exist without light. to produce an image the scene must be illuminated with one or more light sources. modalities such as fluorescent microscopy and x-ray tomography do not fit this model but we do not deal with them in this book. light sources can generally be divided into point and area light sources. a point light source originates at a single location in space a small light bulb potentially at infinity the sun. that for some applications such as modeling soft shadows the sun may have to be treated as an area light source. in addition to its location a point light source has an intensity and a color spectrum i.e. a distribution over photometric image formation figure a simplified model of photometric image formation. light is emitted by one or more light sources and is then reflected from an object s surface. a portion of this light is directed towards the camera. this simplified model ignores multiple reflections which often occur in real-world scenes. wavelengths l the intensity of a light source falls off with the square of the distance between the source and the object being lit because the same light is being spread over a larger area. a light source may also have a directional falloff but we ignore this in our simplified model. area light sources are more complicated. a simple area light source such as a fluorescent ceiling light fixture with a diffuser can be modeled as a finite rectangular area emitting light equally in all directions and wallace sillion and puech glassner when the distribution is strongly directional a four-dimensional lightfield can be used instead a more complex light distribution that approximates say the incident illumination on an object sitting in an outdoor courtyard can often be represented using an environment map called a reflection map and newell this representation maps incident light directions v to color values wavelengths l v and is equivalent to assuming that all light sources are at infinity. environment maps can be represented as a collection of cubical faces as a single longitude latitude map and newell or as the image of a reflecting sphere a convenient way to get a rough model of a real-world environment map is to take an image of a reflective mirrored sphere and to unwrap this image onto the desired environment map watt gives a nice discussion of environment mapping including the formulas needed to map directions to pixels for the three most commonly used representations. nsurfacelight sourceimage planesensor planeoptics computer vision algorithms and applications draft light scatters when it hits a surface. figure the bidirectional reflectance distribution function f i i r r is parameterized by the angles that the incident vi and reflected vr light ray directions make with the local surface coordinate frame dx dy n. reflectance and shading when light hits an object s surface it is scattered and reflected many different models have been developed to describe this interaction. in this section we first describe the most general form the bidirectional reflectance distribution function and then look at some more specialized models including the diffuse specular and phong shading models. we also discuss how these models can be used to compute the global illumination corresponding to a scene. the bidirectional reflectance distribution function the most general model of light scattering is the bidirectional reflectance distribution function relative to some local coordinate frame on the surface the brdf is a fourdimensional function that describes how much of each wavelength arriving at an incident direction vi is emitted in a reflected direction vr the function can be written in terms of the angles of the incident and reflected directions relative to the surface frame as fr i i r r the brdf is reciprocal i.e. because of the physics of light transport you can interchange the roles of vi and vr and still get the same answer is sometimes called helmholtz reciprocity. actually even more general models of light transport exist including some that model spatial variation along the surface sub-surface scattering and atmospheric effects see section rushmeier and sillion weyrich lawrence lensch et al. nvidxnvrdy i i r r photometric image formation most surfaces are isotropic i.e. there are no preferred directions on the surface as far exceptions are anisotropic surfaces such as brushed as light transport is concerned. aluminum where the reflectance depends on the light orientation relative to the direction of the scratches. for an isotropic material we can simplify the brdf to fr i r r i or fr vi vr n since the quantities i r and r i can be computed from the directions vi vr and n. to calculate the amount of light exiting a surface point p in a direction vr under a given lighting condition we integrate the product of the incoming light li vi with the brdf authors call this step a convolution. taking into account the foreshortening factor cos i we obtain where lr vr li vi vi vr n cos i d vi cos i cos i. if the light sources are discrete finite number of point light sources we can replace the integral with a summation lr vr li vi vr n cos i. brdfs for a given surface can be obtained through physical modeling and sparrow cook and torrance glassner heuristic modeling or through empirical observation westin arvo and torrance dana van ginneken nayar et al. dorsey rushmeier and sillion weyrich lawrence lensch et al. typical brdfs can often be split into their diffuse and specular components as described below. diffuse reflection the diffuse component known as lambertian or matte reflection scatters light uniformly in all directions and is the phenomenon we most normally associate with shading e.g. the smooth variation of intensity with surface normal that is seen when observing a statue diffuse reflection also often imparts a strong body color to the light since it is caused by selective absorption and re-emission of light inside the object s material glassner see for a database of some empirically sampled brdfs. computer vision algorithms and applications draft figure this close-up of a statue shows both diffuse shading and specular highlight reflection as well as darkening in the grooves and creases due to reduced light visibility and interreflections. courtesy of the caltech vision lab httpwww. vision.caltech.eduarchive.html. while light is scattered uniformly in all directions i.e. the brdf is constant fd vi vr n fd the amount of light depends on the angle between the incident light direction and the surface normal i. this is because the surface area exposed to a given amount of light becomes larger at oblique angles becoming completely self-shadowed as the outgoing surface normal points away from the light about how you orient yourself towards the sun or fireplace to get maximum warmth and how a flashlight projected obliquely against a wall is less bright than one pointing directly at it. the shading equation for diffuse reflection can thus be written as ld vr where specular reflection li vi n li cos i vi n vi n. the second major component of a typical brdf is specular or highlight reflection which depends strongly on the direction of the outgoing light. consider light reflecting off a mirrored surface incident light rays are reflected in a direction that is rotated by around the surface normal n. using the same notation as in equations photometric image formation figure the diminution of returned light caused by foreshortening depends on vi n the cosine of the angle between the incident light direction vi and the surface normal n. mirror reflection the incident light ray direction vi is reflected onto the specular direction si around the surface normal n. we can compute the specular reflection direction si as si v n nt ivi. the amount of light reflected in a given direction vr thus depends on the angle s cos vr si between the view direction vr and the specular direction si. for example the phong model uses a power of the cosine of the angle fs s ks coske s while the torrance and sparrow micro-facet model uses a gaussian fs s ks exp s s. larger exponents ke inverse gaussian widths cs correspond to more specular surfaces with distinct highlights while smaller exponents better model materials with softer gloss. phong shading phong combined the diffuse and specular components of reflection with another term which he called the ambient illumination. this term accounts for the fact that objects are generally illuminated not only by point light sources but also by a general diffuse illumination corresponding to inter-reflection the walls in a room or distant sources such as the vi n vi n n n nv v computer vision algorithms and applications draft figure cross-section through a phong shading model brdf for a fixed incident illumination direction component values as a function of angle away from surface normal polar plot. the value of the phong exponent ke is indicated by the exp labels and the light source is at an angle of away from the normal. blue sky. in the phong model the ambient term does not depend on surface orientation but depends on the color of both the ambient illumination la and the object ka fa ka putting all of these terms together we arrive at the phong shading model lr vr ka kd li vr sike. figure shows a typical set of phong shading model components as a function of the angle away from the surface normal a plane containing both the lighting direction and the viewer. li vi n ks typically the ambient and diffuse reflection color distributions ka and kd are the same since they are both due to sub-surface scattering reflection inside the surface material the specular reflection distribution ks is often uniform since it is caused by interface reflections that do not change the light color. exception to this are metallic materials such as copper as opposed to the more common dielectric materials such as plastics. the ambient illumination la often has a different color cast from the direct light sources li e.g. it may be blue for a sunny outdoor scene or yellow for an interior lit with candles or incandescent lights. presence of ambient sky illumination in shadowed areas is what often causes shadows to appear bluer than the corresponding lit portions of a scene. note also that the diffuse component of the phong model of any shading model depends on the angle of the incoming light source vi while the specular component depends on the relative angle between the viewer vr and the specular reflection direction si itself depends on the incoming light direction vi and the surface normal n. photometric image formation the phong shading model has been superseded in terms of physical accuracy by a number of more recently developed models in computer graphics including the model developed by cook and torrance based on the original micro-facet model of torrance and sparrow until recently most computer graphics hardware implemented the phong model but the recent advent of programmable pixel shaders makes the use of more complex models feasible. di-chromatic reflection model the torrance and sparrow model of reflection also forms the basis of shafer s di-chromatic reflection model which states that the apparent color of a uniform material lit from a single source depends on the sum of two terms lr vr li vr vi n lb vr vi n ci vr vi n cb vr vi n i.e. the radiance of the light reflected at the interface li and the radiance reflected at the surface body lb. each of these in turn is a simple product between a relative power spectrum c which depends only on wavelength and a magnitude m vr vi n which depends only on geometry. model can easily be derived from a generalized version of phong s model by assuming a single light source and no ambient illumination and re-arranging terms. the di-chromatic model has been successfully used in computer vision to segment specular colored objects with large variations in shading and more recently has inspired local two-color models for applications such bayer pattern demosaicing uyttendaele zitnick et al. global illumination tracing and radiosity the simple shading model presented thus far assumes that light rays leave the light sources bounce off surfaces visible to the camera thereby changing in intensity or color and arrive at the camera. in reality light sources can be shadowed by occluders and rays can bounce multiple times around a scene while making their trip from a light source to the camera. two methods have traditionally been used to model such effects. if the scene is mostly specular classic example being scenes made of glass objects and mirrored or highly polished balls the preferred approach is ray tracing or path tracing akeninem oller and haines shirley which follows individual rays from the camera across multiple bounces towards the light sources vice versa. if the scene is composed mostly of uniform albedo simple geometry illuminators and surfaces radiosity illumination techniques are preferred and wallace sillion and puech glassner computer vision algorithms and applications draft combinations of the two techniques have also been developed cohen and greenberg as well as more general light transport techniques for simulating effects such as the caustics cast by rippling water. the basic ray tracing algorithm associates a light ray with each pixel in the camera image and finds its intersection with the nearest surface. a primary contribution can then be computed using the simple shading equations presented previously equation for all light sources that are visible for that surface element. alternative technique for computing which surfaces are illuminated by a light source is to compute a shadow map or shadow buffer i.e. a rendering of the scene from the light source s perspective and then compare the depth of pixels being rendered with the map akenine-m oller and haines additional secondary rays can then be cast along the specular direction towards other objects in the scene keeping track of any attenuation or color change that the specular reflection induces. radiosity works by associating lightness values with rectangular surface areas in the scene area light sources. the amount of light interchanged between any two visible areas in the scene can be captured as a form factor which depends on their relative orientation and surface reflectance properties as well as the fall-off as light is distributed over a larger effective sphere the further away it is and wallace sillion and puech glassner a large linear system can then be set up to solve for the final lightness of each area patch using the light sources as the forcing function hand side. once the system has been solved the scene can be rendered from any desired point of view. under certain circumstances it is possible to recover the global illumination in a scene from photographs using computer vision techniques debevec malik et al. the basic radiosity algorithm does not take into account certain near field effects such as the darkening inside corners and scratches or the limited ambient illumination caused by partial shadowing from other surfaces. such effects have been exploited in a number of computer vision algorithms ikeuchi and kanade langer and zucker while all of these global illumination effects can have a strong effect on the appearance of a scene and hence its interpretation they are not covered in more detail in this book. see section for a discussion of recovering brdfs from real scenes and objects. optics once the light from a scene reaches the camera it must still pass through the lens before reaching the sensor film or digital silicon. for many applications it suffices to treat the lens as an ideal pinhole that simply projects all rays through a common center of projection and however if we want to deal with issues such as focus exposure vignetting and aber photometric image formation figure a thin lens of focal length f focuses the light from a plane a distance zo in front of the lens at a distance zi behind the lens where f if the focal plane zo gray line next to c is moved forward the images are no longer in focus and the circle of confusion c thick line segments depends on the distance of the image plane motion zi relative to the lens aperture diameter d. the field of view depends on the ratio between the sensor width w and the focal length f more precisely the focusing distance zi which is usually quite close to f. zi ration we need to develop a more sophisticated model which is where the study of optics comes in oller hecht ray figure shows a diagram of the most basic lens model i.e. the thin lens composed of a single piece of glass with very low equal curvature on both sides. according to the lens law can be derived using simple geometric arguments on light ray refraction the relationship between the distance to an object zo and the distance behind the lens at which a focused image is formed zi can be expressed as zi zo f where f is called the focal length of the lens. if we let zo i.e. we adjust the lens the image plane so that objects at infinity are in focus we get zi f which is why we can think of a lens of focal length f as being equivalent a first approximation to a pinhole a distance f from the focal plane whose field of view is given by if the focal plane is moved away from its proper in-focus setting of zi by twisting the focus ring on the lens objects at zo are no longer in focus as shown by the gray plane in figure the amount of mis-focus is measured by the circle of confusion c as short thick blue line segments on the gray the equation for the circle of confusion can be derived using similar triangles it depends on the distance of travel in the focal plane zi relative to the original focus distance zi and the diameter of the aperture d exercise if the aperture is not completely circular e.g. if it is caused by a hexagonal diaphragm it is sometimes possible to see this effect in the actual blur function fergus durand et al. joshi szeliski and kriegman or in the glints that are seen when shooting into the sun. mmf mf.o.v.c zipd computer vision algorithms and applications draft figure regular and zoom lens depth of field indicators. the allowable depth variation in the scene that limits the circle of confusion to an acceptable number is commonly called the depth of field and is a function of both the focus distance and the aperture as shown diagrammatically by many lens markings since this depth of field depends on the aperture diameter d we also have to know how this varies with the commonly displayed f-number which is usually denoted as f or n and is defined as f n f d where the focal length f and the aperture diameter d are measured in the same unit millimeters. the usual way to write the f-number is to replace the in f with the actual number i.e. f f f f we can say n etc. an easy way to interpret these numbers is to notice that dividing the focal length by the f-number gives us the diameter d so these are just formulas for the aperture notice that the usual progression for f-numbers is in full stops which are multiples of since this corresponds to doubling the area of the entrance pupil each time a smaller f-number is selected. doubling is also called changing the exposure by one exposure value or ev. it has the same effect on the amount of light reaching the sensor as doubling the exposure duration e.g. from to see exercise now that you know how to convert between f-numbers and aperture diameters you can construct your own plots for the depth of field as a function of focal length f circle of confusion c and focus distance zo as explained in exercise and see how well these match what you observe on actual lenses such as those shown in figure of course real lenses are not infinitely thin and therefore suffer from geometric aberrations unless compound elements are used to correct for them. the classic five seidel aberrations which arise when using third-order optics include spherical aberration coma astigmatism curvature of field and distortion oller hecht ray this also explains why with zoom lenses the f-number varies with the current zoom length setting. photometric image formation figure in a lens subject to chromatic aberration light at different wavelengths the red and blur arrows is focused with a different focal length and hence a different depth resulting in both a geometric displacement and a loss of focus. chromatic aberration because the index of refraction for glass varies slightly as a function of wavelength simple lenses suffer from chromatic aberration which is the tendency for light of different colors to focus at slightly different distances hence also with slightly different magnification factors as shown in figure the wavelength-dependent magnification factor i.e. the transverse chromatic aberration can be modeled as a per-color radial distortion and hence calibrated using the techniques described in section the wavelength-dependent blur caused by longitudinal chromatic aberration can be calibrated using techniques described in section unfortunately the blur induced by longitudinal aberration can be harder to undo as higher frequencies can get strongly attenuated and hence hard to recover. in order to reduce chromatic and other kinds of aberrations most photographic lenses today are compound lenses made of different glass elements different coatings. such lenses can no longer be modeled as having a single nodal point p through which all of the rays must pass approximating the lens with a pinhole model. instead these lenses have both a front nodal point through which the rays enter the lens and a rear nodal point through which they leave on their way to the sensor. in practice only the location of the front nodal point is of interest when performing careful camera calibration e.g. when determining the point around which to rotate to capture a parallax-free panorama section not all lenses however can be modeled as having a single nodal point. in particular very wide-angle lenses such as fisheye lenses and certain catadioptric imaging systems consisting of lenses and curved mirrors and nayar do not have a single point through which all of the acquired light rays pass. in such cases it is preferable to explicitly construct a mapping function table between pixel coordinates and rays in space thorpe and kanade champleboux lavall ee sautot et al. zi computer vision algorithms and applications draft figure the amount of light hitting a pixel of surface area i depends on the square of the ratio of the aperture diameter d to the focal length f as well as the fourth power of the off-axis angle cosine grossberg and nayar sturm and ramalingam tardif sturm trudeau et al. as mentioned in section vignetting another property of real-world lenses is vignetting which is the tendency for the brightness of the image to fall off towards the edge of the image. two kinds of phenomena usually contribute to this effect the first is called natural vignetting and is due to the foreshortening in the object surface projected pixel and lens aperture as shown in figure consider the light leaving the object surface patch of size o located at an off-axis angle because this patch is foreshortened with respect to the camera lens the amount of light reaching the lens is reduced by a factor cos the amount of light reaching the lens is also subject to the usual fall-off in this case the distance ro zo cos the actual area of the aperture through which the light passes is foreshortened by an additional factor cos i.e. the aperture as seen from point o is an ellipse of dimensions d d cos putting all of these factors together we see that the amount of light leaving o and passing through the aperture on its way to the image pixel located at i is proportional to o cos o d cos o o since triangles op q and ip j are similar the projected areas of of the object surface o and image pixel i are in the same ratio as zo zi putting these together we obtain the final relationship between the amount of light reaching o i o i id o pjioqro the digital camera pixel i and the aperture diameter d the focusing distance zi f and the off-axis angle o o i i i d which is called the fundamental radiometric relation between the scene radiance l and the light e reaching the pixel sensor e l d nalwa hecht ray notice in this equation how the amount of light depends on the pixel surface area is why the smaller sensors in point-and-shoot cameras are so much noisier than digital single lens reflex cameras the inverse square of the f-stop n f and the fourth power of the off-axis fall-off which is the natural vignetting term. the other major kind of vignetting called mechanical vignetting is caused by the internal occlusion of rays near the periphery of lens elements in a compound lens and cannot easily be described mathematically without performing a full ray-tracing of the actual lens however unlike natural vignetting mechanical vignetting can be decreased by reducing the camera aperture the f-number. it can also be calibrated with natural vignetting using special devices such as integrating spheres uniformly illuminated targets or camera rotation as discussed in section the digital camera after starting from one or more light sources reflecting off one or more surfaces in the world and passing through the camera s optics light finally reaches the imaging sensor. how are the photons arriving at this sensor converted into the digital g b values that we observe when we look at a digital image? in this section we develop a simple model that accounts for the most important effects such as exposure and shutter speed nonlinear mappings sampling and aliasing and noise. figure which is based on camera models developed by healey and kondepudy tsin ramesh and kanade liu szeliski kang et al. shows a simple version of the processing stages that occur in modern digital cameras. chakrabarti scharstein and zickler developed a sophisticated model that is an even better match to the processing performed in today s cameras. there are some empirical models that work well in practice and weiss zheng lin and kang computer vision algorithms and applications draft figure image sensing pipeline showing the various sources of noise as well as typical digital post-processing steps. light falling on an imaging sensor is usually picked up by an active sensing area integrated for the duration of the exposure expressed as the shutter speed in a fraction of and then passed to a set of sense amplifiers the two main kinds a second e.g. of sensor used in digital still and video cameras today are charge-coupled device and complementary metal oxide on silicon in a ccd photons are accumulated in each active well during the exposure time. then in a transfer phase the charges are transferred from well to well in a kind of bucket brigade until they are deposited at the sense amplifiers which amplify the signal and pass it to an analog-to-digital converter older ccd sensors were prone to blooming when charges from one over-exposed pixel spilled into adjacent ones but most newer ccds have anti-blooming technology troughs into which the excess charge can spill. in cmos the photons hitting the sensor directly affect the conductivity gain of a photodetector which can be selectively gated to control exposure duration and locally amplified before being read out using a multiplexing scheme. traditionally ccd sensors outperformed cmos in quality sensitive applications such as digital slrs while cmos was better for low-power applications but today cmos is used in most digital cameras. the main factors affecting the performance of a digital image sensor are the shutter speed sampling pitch fill factor chip size analog gain sensor noise and the resolution quality in digital still cameras a complete frame is captured and then read out sequentially at once. however if video is being captured a rolling shutter which exposes and transfers each line separately is often used. in older video cameras the even fields were scanned first followed by the odd fields in a process that is called interlacing. the digital camera of the analog-to-digital converter. many of the actual values for these parameters can be read from the exif tags embedded with digital images. while others can be obtained from the camera manufacturers specification sheets or from camera review or calibration web shutter speed. the shutter speed time directly controls the amount of light reaching the sensor and hence determines if images are under- or over-exposed. bright scenes where a large aperture or slow shutter speed are desired to get a shallow depth of field or motion blur neutral density filters are sometimes used by photographers. for dynamic scenes the shutter speed also determines the amount of motion blur in the resulting picture. usually a higher shutter speed motion blur makes subsequent analysis easier section for techniques to remove such blur. however when video is being captured for display some motion blur may be desirable to avoid stroboscopic effects. sampling pitch. the sampling pitch is the physical spacing between adjacent sensor cells on the imaging chip. a sensor with a smaller sampling pitch has a higher sampling density and hence provides a higher resolution terms of pixels for a given active chip area. however a smaller pitch also means that each sensor has a smaller area and cannot accumulate as many photons this makes it not as light sensitive and more prone to noise. fill factor. the fill factor is the active sensing area size as a fraction of the theoretically available sensing area product of the horizontal and vertical sampling pitches. higher fill factors are usually preferable as they result in more light capture and less aliasing section however this must be balanced with the need to place additional electronics between the active sense areas. the fill factor of a camera can be determined empirically using a photometric camera calibration process section chip size. video and point-and-shoot cameras have traditionally used small chip areas while digital slr cameras try to come closer to the traditional size inch to of a film when overall device size is not important having a larger chip size is preferable since each sensor cell can be more photo-sensitive. compact cameras a smaller chip means that all of the optics can be shrunk down proportionately. however httpwww.clarkvision.comimagedetaildigital.sensor.performance.summary these numbers refer to the tube diameter of the old vidicon tubes used in video cameras dpreview.comlearn?glossarycamera systemsensor sizes the sensor on the canon camera actually measures i.e. a sixth of the size side of a full-frame dslr sensor. when a dslr chip does not fill the full frame it results in a multiplier effect on the lens focal length. for example a chip that is only the dimension of a frame will make a lens image the same angular extent as a lens as demonstrated in computer vision algorithms and applications draft larger chips are more expensive to produce not only because fewer chips can be packed into each wafer but also because the probability of a chip defect goes up linearly with the chip area. analog gain. before analog-to-digital conversion the sensed signal is usually boosted by a sense amplifier. in video cameras the gain on these amplifiers was traditionally controlled by automatic gain control logic which would adjust these values to obtain a good overall exposure. in newer digital still cameras the user now has some additional control over this gain through the iso setting which is typically expressed in iso standard units such as or since the automated exposure control in most cameras also adjusts the aperture and shutter speed setting the iso manually removes one degree of freedom from the camera s control just as manually specifying aperture and shutter speed does. in theory a higher gain allows the camera to perform better under low light conditions motion blur due to long exposure times when the aperture is already maxed out. in practice however higher iso settings usually amplify the sensor noise. sensor noise. throughout the whole sensing process noise is added from various sources which may include fixed pattern noise dark current noise shot noise amplifier noise and quantization noise and kondepudy tsin ramesh and kanade the final amount of noise present in a sampled image depends on all of these quantities as well as the incoming light by the scene radiance and aperture the exposure time and the sensor gain. also for low light conditions where the noise is due to low photon counts a poisson model of noise may be more appropriate than a gaussian model. as discussed in more detail in section liu szeliski kang et al. use this model along with an empirical database of camera response functions obtained by grossberg and nayar to estimate the noise level function for a given image which predicts the overall noise variance at a given pixel as a function of its brightness separate nlf is estimated for each color channel. an alternative approach when you have access to the camera before taking pictures is to pre-calibrate the nlf by taking repeated shots of a scene containing a variety of colors and luminances such as the macbeth color chart shown in figure marcus and davidson estimating the variance be sure to throw away or downweight pixels with large gradients as small shifts between exposures will affect the sensed values at such pixels. unfortunately the precalibration process may have to be repeated for different exposure times and gain settings because of the complex interactions occurring within the sensing system. in practice most computer vision algorithms such as image denoising edge detection and stereo matching all benefit from at least a rudimentary estimate of the noise level. barring the ability to pre-calibrate the camera or to take repeated shots of the same scene the simplest the digital camera approach is to look for regions of near-constant value and to estimate the noise variance in such regions szeliski kang et al. adc resolution. the final step in the analog processing chain occurring within an imaging sensor is the analog to digital conversion while a variety of techniques can be used to implement this process the two quantities of interest are the resolution of this process many bits it yields and its noise level many of these bits are useful in practice. for most cameras the number of bits quoted bits for compressed jpeg images and a nominal bits for the raw formats provided by some dslrs exceeds the actual number of usable bits. the best way to tell is to simply calibrate the noise of a given sensor e.g. by taking repeated shots of the same scene and plotting the estimated noise as a function of brightness digital post-processing. once the irradiance values arriving at the sensor have been converted to digital bits most cameras perform a variety of digital signal processing operations to enhance the image before compressing and storing the pixel values. these include color filter array demosaicing white point setting and mapping of the luminance values through a gamma function to increase the perceived dynamic range of the signal. we cover these topics in section but before we do we return to the topic of aliasing which was mentioned in connection with sensor array fill factors. sampling and aliasing what happens when a field of light impinging on the image sensor falls onto the active sense areas in the imaging chip? the photons arriving at each active cell are integrated and then digitized. however if the fill factor on the chip is small and the signal is not otherwise band-limited visually unpleasing aliasing can occur. to explore the phenomenon of aliasing let us first look at a one-dimensional signal in which we have two sine waves one at a frequency of f and the other at f if we sample these two signals at a frequency of f we see that they produce the same samples in black and so we say that they are why is this a bad effect? in essence we can no longer reconstruct the original signal since we do not know which of the two original frequencies was present. in fact shannon s sampling theorem shows that the minimum sampling and schafer oppenheim schafer and buck rate required to reconstruct a signal an alias is an alternate name for someone so the sampled signal corresponds to two different aliases. computer vision algorithms and applications draft figure aliasing of a one-dimensional signal the blue sine wave at f and the red sine wave at f have the same digital samples when sampled at f even after convolution with a fill factor box filter the two signals while no longer of the same magnitude are still aliased in the sense that the sampled red signal looks like an inverted lower magnitude version of the blue signal. image on the right is scaled up for better visibility. the actual sine magnitudes are and of their original values. from its instantaneous samples must be at least twice the highest fs the maximum frequency in a signal is known as the nyquist frequency and the inverse of the minimum sampling frequency rs is known as the nyquist rate. however you may ask since an imaging chip actually averages the light field over a finite area are the results on point sampling still applicable? averaging over the sensor area does tend to attenuate some of the higher frequencies. however even if the fill factor is as in the right image of figure frequencies above the nyquist limit the sampling frequency still produce an aliased signal although with a smaller magnitude than the corresponding band-limited signals. a more convincing argument as to why aliasing is bad can be seen by downsampling a signal using a poor quality filter such as a box filter. figure shows a highfrequency chirp image called because the frequencies increase over time along with the results of sampling it with a fill-factor area sensor a fill-factor sensor and a highquality filter. additional examples of downsampling filters can be found in section and figure the best way to predict the amount of aliasing that an imaging system even an image processing algorithm will produce is to estimate the point spread function which represents the response of a particular pixel sensor to an ideal point light source. the psf is a combination of the blur induced by the optical system and the finite integration area of a chip the actual theorem states that fs must be at least twice the signal bandwidth but since we are not dealing with modulated signals such as radio waves during image capture the maximum frequency suffices. imaging chips usually interpose an optical anti-aliasing filter just before the imaging chip to reduce or control the amount of aliasing. the digital camera figure aliasing of a two-dimensional signal original full-resolution image downsampled with a fill factor box filter downsampled with a fill factor box filter downsampled with a high-quality filter. notice how the higher frequencies are aliased into visible frequencies with the lower quality filters while the filter completely removes these higher frequencies. if we know the blur function of the lens and the fill factor area shape and spacing for the imaging chip optionally the response of the anti-aliasing filter we can convolve these described in section to obtain the psf. figure shows the one-dimensional cross-section of a psf for a lens whose blur function is assumed to be a disc of a radius equal to the pixel spacing s plus a sensing chip whose horizontal fill factor is taking the fourier transform of this psf we obtain the modulation transfer function from which we can estimate the amount of aliasing as the area of the fourier magnitude outside the f fs nyquist if we de-focus the lens so that the blur function has a radius of we see that the amount of aliasing decreases significantly but so does the amount of image detail closer to f fs. under laboratory conditions the psf can be estimated pixel precision by looking at a point light source such as a pin hole in a black piece of cardboard lit from behind. however this psf actual image of the pin hole is only accurate to a pixel resolution and while it can model larger blur as blur caused by defocus it cannot model the sub-pixel shape of the psf and predict the amount of aliasing. an alternative technique described in section is to look at a calibration pattern one consisting of slanted step edges park and narayanswamy williams and burns joshi szeliski and kriegman whose ideal appearance can be re-synthesized to sub-pixel precision. in addition to occurring during image acquisition aliasing can also be introduced in various image processing operations such as resampling upsampling and downsampling. sections and discuss these issues and show how careful selection of filters can reduce the complex fourier transform of the psf is actually called the optical transfer function its magnitude is called the modulation transfer function and its phase is called the phase transfer function computer vision algorithms and applications draft figure sample point spread functions the diameter of the blur disc in is equal to half the pixel spacing while the diameter in is twice the pixel spacing. the horizontal fill factor of the sensing chip is and is shown in brown. the convolution of these two kernels gives the point spread function shown in green. the fourier response of the psf mtf is plotted in and the area above the nyquist frequency where aliasing occurs is shown in red. the amount of aliasing that operations inject. color in section we saw how lighting and surface reflections are functions of wavelength. when the incoming light hits the imaging sensor light from different parts of the spectrum is somehow integrated into the discrete red green and blue color values that we see in a digital image. how does this process work and how can we analyze and manipulate color values? you probably recall from your childhood days the magical process of mixing paint colors to obtain new ones. you may recall that blueyellow makes green redblue makes purple and redgreen makes brown. if you revisited this topic at a later age you may have learned that the proper subtractive primaries are actually cyan light blue-green magenta and yellow although black is also often used in four-color printing you ever subsequently took any painting classes you learned that colors can have even the digital camera figure primary and secondary colors additive colors red green and blue can be mixed to produce cyan magenta yellow and white subtractive colors cyan magenta and yellow can be mixed to produce red green blue and black. more fanciful names such as alizarin crimson cerulean blue and chartreuse. the subtractive colors are called subtractive because pigments in the paint absorb certain wavelengths in the color spectrum. later on you may have learned about the additive primary colors green and blue and how they can be added a slide projector or on a computer monitor to produce cyan magenta yellow white and all the other colors we typically see on our tv sets and monitors through what process is it possible for two different colors such as red and green to interact to produce a third color like yellow? are the wavelengths somehow mixed up to produce a new wavelength? you probably know that the correct answer has nothing to do with physically mixing wavelengths. instead the existence of three primaries is a result of the tri-stimulus trichromatic nature of the human visual system since we have three different kinds of cone each of which responds selectively to a different portion of the color spectrum wyszecki and stiles fairchild reinhard ward pattanaik et al. livingstone note that for machine vision applications such as remote sensing and terrain classification it is preferable to use many more wavelengths. similarly surveillance applications can often benefit from sensing in the near-infrared range. cie rgb and xyz to test and quantify the tri-chromatic theory of perception we can attempt to reproduce all monochromatic wavelength colors as a mixture of three suitably chosen primaries. see also mark fairchild s web page httpwww.cis.rit.edufairchildwhyiscolorbooks links.html. computer vision algorithms and applications draft figure standard cie color matching functions r g b color spectra obtained from matching pure colors to the and primaries x y z color matching functions which are linear combinations of the r g b spectra. wavelength light can be obtained using either a prism or specially manufactured color filters. in the the commission internationale d eclairage standardized the rgb representation by performing such color matching experiments using the primary colors of red wavelength green and blue figure shows the results of performing these experiments with a standard observer i.e. averaging perceptual results over a large number of subjects. you will notice that for certain pure spectra in the blue green range a negative amount of red light has to be added i.e. a certain amount of red has to be added to the color being matched in order to get a color match. these results also provided a simple explanation for the existence of metamers which are colors with different spectra that are perceptually indistinguishable. note that two fabrics or paint colors that are metamers under one light may no longer be so under different lighting. because of the problem associated with mixing negative light the cie also developed a new color space called xyz which contains all of the pure spectral colors within its positive octant. also maps the y axis to the luminance i.e. perceived relative brightness and maps pure white to a diagonal vector. the transformation from rgb to xyz is given by x y z r g b while the official definition of the cie xyz standard has the matrix normalized so that the y value corresponding to pure red is a more commonly used form is to omit the leading the digital camera figure cie chromaticity diagram showing colors and their corresponding y values. pure spectral colors are arranged around the outside of the curve. fraction so that the second row adds up to one i.e. the rgb triplet maps to a y value of linearly blending the r g b curves in figure according to we obtain the resulting x y z curves shown in figure notice how all three spectra matching functions now have only positive values and how the y curve matches that of the luminance perceived by humans. if we divide the xyz values by the sum of xyz we obtain the chromaticity coordi nates x x x y z y y x y z z z x y z which sum up to the chromaticity coordinates discard the absolute intensity of a given color sample and just represent its pure color. if we sweep the monochromatic color parameter in figure from to we obtain the familiar chromaticity diagram shown in figure this figure shows the y value for every color value perceivable by most humans. course the cmyk reproduction process in this book does not actually span the whole gamut of perceivable colors. the outer curved rim represents where all of the pure monochromatic color values map in y space while the lower straight line which connects the two endpoints is known as the purple line. a convenient representation for color values when we want to tease apart luminance and chromaticity is therefore yxy plus the two most distinctive chrominance components. lab color space while the xyz color space has many convenient properties including the ability to separate luminance from chrominance it does not actually predict how well humans perceive differences in color or luminance. computer vision algorithms and applications draft because the response of the human visual system is roughly logarithmic can perceive relative luminance differences of about the cie defined a non-linear re-mapping of the xyz space called lab sometimes called cielab where differences in luminance or chrominance are more perceptually the l component of lightness is defined as l y where yn is the luminance value for nominal white and ft t else is a finite-slope approximation to the cube root with the resulting scale roughly measures equal amounts of lightness perceptibility. in a similar fashion the a and b components are defined as a x y and b y z where again yn zn is the measured white point. figure k show the lab representation for a sample color image. color cameras while the preceding discussion tells us how we can uniquely describe the perceived tristimulus description of any color distribution it does not tell us how rgb still and video cameras actually work. do they just measure the amount of light at the nominal wavelengths of red green and blue do color monitors just emit exactly these wavelengths and if so how can they emit negative red light to reproduce colors in the cyan range? in fact the design of rgb video cameras has historically been based around the availability of colored phosphors that go into television sets. when standard-definition color television was invented a mapping was defined between the rgb values that would drive the three color guns in the cathode ray tube and the xyz values that unambiguously define perceived color standard was called itu-r with the advent of hdtv and newer monitors a new standard called itu-r was created which specifies the xyz another perceptually motivated color space called luv was developed and standardized simultaneously the digital camera values of each of the color primaries x y z in practice each color camera integrates light according to the spectral response function of its red green and blue sensors r l g l b l where l is the incoming spectrum of light at a given pixel and sg sb are the red green and blue spectral sensitivities of the corresponding sensors. can we tell what spectral sensitivities the cameras actually have? unless the camera manufacturer provides us with this data or we observe the response of the camera to a whole spectrum of monochromatic lights these sensitivities are not specified by a standard such as instead all that matters is that the tri-stimulus values for a given color produce the specified rgb values. the manufacturer is free to use sensors with sensitivities that do not match the standard xyz definitions so long as they can later be converted a linear transform to the standard colors. similarly while tv and computer monitors are supposed to produce rgb values as specified by equation there is no reason that they cannot use digital logic to transform the incoming rgb values into different signals to drive each of the color channels. properly calibrated monitors make this information available to software applications that perform color management so that colors in real life on the screen and on the printer all match as closely as possible. color filter arrays while early color tv cameras used three vidicons to perform their sensing and later cameras used three separate rgb sensing chips most of today s digital still and video cameras cameras use a color filter array where alternating sensors are covered by different colored a newer chip design by foveon stacks the red green and blue sensors beneath each other but it has not yet gained widespread adoption. computer vision algorithms and applications draft figure bayer rgb pattern color filter array layout interpolated pixel values with unknown values shown as lower case. the most commonly used pattern in color cameras today is the bayer pattern which places green filters over half of the sensors a checkerboard pattern and red and blue filters over the remaining ones the reason that there are twice as many green filters as red and blue is because the luminance signal is mostly determined by green values and the visual system is much more sensitive to high frequency detail in luminance than in chrominance fact that is exploited in color image compression see section the process of interpolating the missing color values so that we have valid rgb values for all the pixels is known as demosaicing and is covered in detail in section similarly color lcd monitors typically use alternating stripes of red green and blue filters placed in front of each liquid crystal active area to simulate the experience of a full color display. as before because the visual system has higher resolution in luminance than chrominance it is possible to digitally pre-filter rgb monochrome images to enhance the perception of crispness blinn dresevic et al. platt color balance before encoding the sensed rgb values most cameras perform some kind of color balancing operation in an attempt to move the white point of a given image closer to pure white rgb values. if the color system and the illumination are the same system uses the daylight illuminant as its reference white the change may be minimal. however if the illuminant is strongly colored such as incandescent indoor lighting generally results in a yellow or orange hue the compensation can be quite significant. a simple way to perform color correction is to multiply each of the rgb values by a different factor to apply a diagonal matrix transform to the rgb color space. more complicated transforms which are sometimes the result of mapping to xyz space and back the digital camera figure gamma compression the relationship between the input signal luminance y and the transmitted signal y is given by y y at the receiver the signal y is exponentiated by the factor y y noise introduced during transmission is squashed in the dark regions which corresponds to the more noise-sensitive region of the visual system. actually perform a color twist i.e. they use a general color transform exercise has you explore some of these issues. gamma in the early days of black and white television the phosphors in the crt used to display the tv signal responded non-linearly to their input voltage. the relationship between the voltage and the resulting brightness was characterized by a number called gamma since the formula was roughly b v with a of about to compensate for this effect the electronics in the tv camera would pre-map the sensed luminance y through an inverse gamma y y with a typical value of the mapping of the signal through this non-linearity before transmission had a beneficial side effect noise added during transmission these were analog days! would be reduced applying the gamma at the receiver in the darker regions of the signal where it was more visible that our visual system is roughly sensitive to relative differences in luminance. those of you old enough to remember the early days of color television will naturally think of the hue adjustment knob on the television set which could produce truly bizarre results. a related technique called companding was the basis of the dolby noise reduction systems used with audio tapes. yy y y yy y quantization noisevisible noise computer vision algorithms and applications draft when color television was invented it was decided to separately pass the red green and blue signals through the same gamma non-linearity before combining them for encoding. today even though we no longer have analog noise in our transmission systems signals are still quantized during compression section so applying inverse gamma to sensed values is still useful. unfortunately for both computer vision and computer graphics the presence of gamma in images is often problematic. for example the proper simulation of radiometric phenomena such as shading section and equation occurs in a linear radiance space. once all of the computations have been performed the appropriate gamma should be applied before display. unfortunately many computer graphics systems as shading models operate directly on rgb values and display these values directly. newer color imaging standards such as the scrgb use a linear space which makes this less of a problem in computer vision the situation can be even more daunting. the accurate determination of surface normals using a technique such as photometric stereo or even a simpler operation such as accurate image deblurring require that the measurements be in a linear space of intensities. therefore it is imperative when performing detailed quantitative computations such as these to first undo the gamma and the per-image color re-balancing in the sensed color values. chakrabarti scharstein and zickler develop a sophisticated model that is a good match to the processing performed by today s digital cameras they also provide a database of color images you can use for your own for other vision applications however such as feature detection or the matching of signals in stereo and motion estimation this linearization step is often not necessary. in fact determining whether it is necessary to undo gamma can take some careful thinking e.g. in the case of compensating for exposure variations in image stitching exercise if all of these processing steps sound confusing to model they are. exercise has you try to tease apart some of these phenomena using empirical investigation i.e. taking pictures of color charts and comparing the raw and jpeg compressed color values. other color spaces while rgb and xyz are the primary color spaces used to describe the spectral content hence tri-stimulus response of color signals a variety of other representations have been developed both in video and still image coding and in computer graphics. the earliest color representation developed for video transmission was the yiq standard developed for ntsc video in north america and the closely related yuv standard developed for pal in europe. in both of these cases it was desired to have a luma channel y called httpvision.middlebury.educolor. the digital camera since it only roughly mimics true luminance that would be comparable to the regular blackand-white tv signal along with two lower frequency chroma channels. in both systems the y signal more appropriately the y luma signal since it is gamma compressed is obtained from y where r g b is the triplet of gamma-compressed color components. when using the newer color definitions for hdtv in the formula is y the uv components are derived from scaled versions of y and y namely u y and v y in whereas the iq components are the uv components rotated through an angle of composite and pal video the chroma signals were then low-pass filtered horizontally before being modulated and superimposed on top of the y luma signal. backward compatibility was achieved by having older black-and-white tv sets effectively ignore the high-frequency chroma signal of slow electronics or at worst superimposing it as a high-frequency pattern on top of the main signal. while these conversions were important in the early days of computer vision when frame grabbers would directly digitize the composite tv signal today all digital video and still image compression standards are based on the newer ycbcr conversion. ycbcr is closely related to yuv cb and cr signals carry the blue and red color difference signals and have more useful mnemonics than uv but uses different scale factors to fit within the eight-bit range available with digital signals. for video the y signal is re-scaled to fit within the range of values while the cb and cr signals are scaled to fit within and velho fairchild for still images the jpeg standard uses the full eight-bit range with no reserved values y cb cr where the r g b values are the eight-bit gamma-compressed color components the actual rgb values we obtain when we open up or display a jpeg image. for most applications this formula is not that important since your image reading software will directly computer vision algorithms and applications draft provide you with the eight-bit gamma-compressed r g b values. however if you are trying to do careful image deblocking this information may be useful. another color space you may come across is hue saturation value which is a projection of the rgb color cube onto a non-linear chroma angle a radial saturation percentage and a luminance-inspired value. in more detail value is defined as either the mean or maximum color value saturation is defined as scaled distance from the diagonal and hue is defined as the direction around a color wheel exact formulas are described by hall foley van dam feiner et al. such a decomposition is quite natural in graphics applications such as color picking approximates the munsell chart for color description. figure n shows an hsv representation of a sample color image where saturation is encoded using a gray scale darker and hue is depicted as a color. if you want your computer vision algorithm to only affect the value of an image and not its saturation or hue a simpler solution is to use either the y xy chromaticity coordinates defined in or the even simpler color ratios r r r g b g g r g b b b r g b h. after manipulating the luma e.g. through the process of histogram equalization you can multiply each color ratio by the ratio of the new to old luma to obtain an adjusted rgb triplet. while all of these color systems may sound confusing in the end it often may not matter that much which one you use. poynton in his color faq httpwww.poynton.com colorfaq.html notes that the perceptually motivated lab system is qualitatively similar to the gamma-compressed r g b system we mostly deal with since both have a fractional power scaling approximates a logarithmic response between the actual intensity values and the numbers being manipulated. as in all cases think carefully about what you are trying to accomplish before deciding on a technique to compression the last stage in a camera s processing pipeline is usually some form of image compression you are using a lossless compression scheme such as camera raw or png. all color video and image compression algorithms start by converting the signal into ycbcr some closely related variant so that they can compress the luminance signal with higher fidelity than the chrominance signal. that the human visual system has poorer if you are at a loss for questions at a conference you can always ask why the speaker did not use a perceptual color space such as lab. conversely if they did use lab you can ask if they have any concrete evidence that this works better than regular colors. the digital camera rgb r g b rgb r g b l a b h s v figure color space transformations d rgb h rgb. k lab n hsv. note that the rgb lab and hsv values are all re-scaled to fit the dynamic range of the printed page. frequency response to color than to luminance changes. in video it is common to subsample cb and cr by a factor of two horizontally with still images the subsampling occurs both horizontally and vertically. once the luminance and chrominance images have been appropriately subsampled and separated into individual images they are then passed to a block transform stage. the most common technique used here is the discrete cosine transform which is a real-valued variant of the discrete fourier transform section the dct is a reasonable approximation to the karhunen loeve or eigenvalue decomposition of natural image patches i.e. the decomposition that simultaneously packs the most energy into the first coefficients and diagonalizes the joint covariance matrix among the pixels transform coefficients computer vision algorithms and applications draft figure image compressed with jpeg at three quality settings. note how the amount of block artifact and high-frequency aliasing mosquito noise increases from left to right. statistically independent. both mpeg and jpeg use dct transforms le gall although newer variants use smaller blocks or alternative transformations such as wavelets and marcellin and lapped transforms are now used. after transform coding the coefficient values are quantized into a set of small integer values that can be coded using a variable bit length scheme such as a huffman code or an arithmetic code dc frequency coefficients are also adaptively predicted from the previous block s dc values. the term dc comes from direct current i.e. the non-sinusoidal or non-alternating part of a signal. the step size in the quantization is the main variable controlled by the quality setting on the jpeg file with video it is also usual to perform block-based motion compensation i.e. to encode the difference between each block and a predicted set of pixel values obtained from a shifted block in the previous frame. exception is the motion-jpeg scheme used in older dv camcorders which is nothing more than a series of individually jpeg compressed image frames. while basic mpeg uses motion compensation blocks with integer motion values gall newer standards use adaptively sized block sub-pixel motions and the ability to reference blocks from older frames. in order to recover more gracefully from failures and to allow for random access to the video stream predicted p frames are interleaved among independently coded i frames. b frames are also sometimes used. the quality of a compression algorithm is usually reported using its peak signal-to-noise ratio which is derived from the average mean square error m se where ix is the original uncompressed image and ix is its compressed counterpart or equivalently the root mean square error error which is defined as rm s m se. additional reading the psnr is defined as p sn r i max m se imax rm s where imax is the maximum signal extent e.g. for eight-bit images. while this is just a high-level sketch of how image compression works it is useful to understand so that the artifacts introduced by such techniques can be compensated for in various computer vision applications. additional reading as we mentioned at the beginning of this chapter it provides but a brief summary of a very rich and deep set of topics traditionally covered in a number of separate fields. a more thorough introduction to the geometry of points lines planes and projections can be found in textbooks on multi-view geometry and zisserman faugeras and luong and computer graphics van dam feiner et al. watt opengl-arb topics covered in more depth include higher-order primitives such as quadrics conics and cubics as well as three-view and multi-view geometry. the image formation process is traditionally taught as part of a computer graphics curriculum van dam feiner et al. glassner watt shirley but it is also studied in physics-based computer vision shafer and healey the behavior of camera lens systems is studied in optics oller hecht ray some good books on color theory have been written by healey and shafer wyszecki and stiles fairchild with livingstone providing a more fun and informal introduction to the topic of color perception. mark fairchild s page of color books and lists many other sources. topics relating to sampling and aliasing are covered in textbooks on signal and image processing j ahne oppenheim and schafer oppenheim schafer and buck pratt russ burger and burge gonzales and woods exercises a note to students this chapter is relatively light on exercises since it contains mostly background material and not that many usable techniques. if you really want to understand httpwww.cis.rit.edufairchildwhyiscolorbooks links.html. computer vision algorithms and applications draft multi-view geometry in a thorough way i encourage you to read and do the exercises provided by hartley and zisserman similarly if you want some exercises related to the image formation process glassner s book is full of challenging problems. ex least squares intersection point and line fitting advanced equation shows how the intersection of two lines can be expressed as their cross product assuming the lines are expressed as homogeneous coordinates. if you are given more than two lines and want to find a point x that minimizes the sum of squared distances to each line d x how can you compute this quantity? write the dot product as xt li and turn the squared quantity into a quadratic form xt a x. to fit a line to a bunch of points you can compute the centroid of the points as well as the covariance matrix of the points around this mean. show that the line passing through the centroid along the major axis of the covariance ellipsoid eigenvector minimizes the sum of squared distances to the points. these two approaches are fundamentally different even though projective duality tells us that points and lines are interchangeable. why are these two algorithms so apparently different? are they actually minimizing different objectives? ex transform editor write a program that lets you interactively create a set of rectangles and then modify their pose transform. you should implement the following steps open an empty window canvas shift drag to create a new rectangle. select the deformation mode model translation rigid similarity affine or perspective. drag any corner of the outline to change its transformation. this exercise should be built on a set of pixel coordinate and transformation classes either implemented by yourself or from a software library. persistence of the created representation and load should also be supported each rectangle save its transformation. exercises ex viewer write a simple viewer for points lines and polygons. import a set of point and line commands as well as a viewing transform. interactively modify the object or camera transform. this viewer can be an extension of the one you created in simply replace the viewing transformations with their equivalents. add a z-buffer to do hidden surface removal for polygons. use a drawing package and just write the viewer control. ex focus distance and depth of field figure out how the focus distance and depth of field indicators on a lens are determined. compute and plot the focus distance zo as a function of the distance traveled from the focal length zi f zi for a lens of focal length f does this explain the hyperbolic progression of focus distances you see on a typical lens compute the depth of field and maximum focus distances for a given focus setting zo as a function of the circle of confusion diameter c it a fraction of the sensor width the focal length f and the f-stop number n relates to the aperture diameter d. does this explain the usual depth of field markings on a lens that bracket the in-focus marker as in figure now consider a zoom lens with a varying focal length f. assume that as you zoom the lens stays in focus i.e. the distance from the rear nodal point to the sensor plane zi adjusts itself automatically for a fixed focus distance zo. how do the depth of field indicators vary as a function of focal length? can you reproduce a two-dimensional plot that mimics the curved depth of field lines seen on the lens in figure ex f-numbers and shutter speeds list the common f-numbers and shutter speeds that your camera provides. on older model slrs they are visible on the lens and shutter speed dials. on newer cameras you have to look at the electronic viewfinder lcd screenindicator as you manually adjust exposures. do these form geometric progressions if so what are the ratios? how do these relate to exposure values if your camera has shutter speeds of exactly a factor of two apart or a factor of apart? and do you think that these two speeds are how accurate do you think these numbers are? can you devise some way to measure exactly how the aperture affects how much light reaches the sensor and what the exact exposure times actually are? computer vision algorithms and applications draft ex noise level calibration estimate the amount of noise in your camera by taking repeated shots of a scene with the camera mounted on a tripod. a remote shutter release is a good investment if you own a dslr. alternatively take a scene with constant color regions as a color checker chart and estimate the variance by fitting a smooth function to each color region and then taking differences from the predicted function. plot your estimated variance as a function of level for each of your color channels separately. change the iso setting on your camera if you cannot do that reduce the overall light in your scene off lights draw the curtains wait until dusk. does the amount of noise vary a lot with isogain? compare your camera to another one at a different price point or year of make. is there evidence to suggest that you get what you pay for does the quality of digital cameras seem to be improving over time? ex gamma correction in image stitching here s a relatively simple puzzle. assume you are given two images that are part of a panorama that you want to stitch chapter the two images were taken with different exposures so you want to adjust the rgb values so that they match along the seam line. is it necessary to undo the gamma in the color values in order to achieve this? ex skin color detection devise a simple skin color detector and fleck jones and rehg vezhnevets sazonov and andreeva kakumanu makrogiannis and bourbakis based on chromaticity or other color properties. take a variety of photographs of people and calculate the xy chromaticity values for each pixel. crop the photos or otherwise indicate with a painting tool which pixels are likely to be skin face and arms. calculate a color distribution for these pixels. you can use something as simple as a mean and covariance measure or as complicated as a mean-shift segmentation algorithm section you can optionally use non-skin pixels to model the background distribution. use your computed distribution to find the skin regions in an image. one easy way to visualize this is to paint all non-skin pixels a given color such as white or black. how sensitive is your algorithm to color balance lighting? exercises does a simpler chromaticity measurement such as a color ratio work just as well? ex white point balancing tricky a common or post-processing technique for performing white point adjustment is to take a picture of a white piece of paper and to adjust the rgb values of an image to make this a neutral color. describe how you would adjust the rgb values in an image given a sample white color of gw bw to make this color neutral changing the exposure too much. does your transformation involve a simple scaling of the rgb values or do you need a full color twist matrix something else? convert your rgb values to xyz. does the appropriate correction now only depend on the xy xy values? if so when you convert back to rgb space do you need a full color twist matrix to achieve the same effect? if you used pure diagonal scaling in the direct rgb mode but end up with a twist if you work in xyz space how do you explain this apparent dichotomy? which approach is correct? is it possible that neither approach is actually correct? if you want to find out what your camera actually does continue on to the next exercise. ex in-camera color processing challenging if your camera supports a raw pixel mode take a pair of raw and jpeg images and see if you can infer what the camera is doing when it converts the raw pixel values to the final color-corrected and gamma-compressed eight-bit jpeg pixel values. deduce the pattern in your color filter array from the correspondence between colocated raw and color-mapped pixel values. use a color checker chart at this stage if it makes your life easier. you may find it helpful to split the raw image into four separate images even and odd columns and rows and to treat each of these new images as a virtual sensor. evaluate the quality of the demosaicing algorithm by taking pictures of challenging scenes which contain strong color edges as those shown in in section if you can take the same exact picture after changing the color balance values in your camera compare how these settings affect this processing. compare your results against those presented by chakrabarti scharstein and zickler or use the data available in their database of color httpvision.middlebury.educolor. computer vision algorithms and applications draft chapter image processing point operators linear filtering separable filtering pixel transforms color transforms compositing and matting histogram equalization application tonal adjustment examples of linear filtering band-pass and steerable filters non-linear filtering morphology distance transforms connected components more neighborhood operators fourier transform pairs fourier transforms two-dimensional fourier transforms wiener filtering application sharpening blur and noise removal mesh-based warping application feature-based morphing interpolation decimation multi-resolution representations wavelets application image blending regularization markov random fields application image restoration parametric transformations pyramids and wavelets geometric transformations global optimization additional reading exercises computer vision algorithms and applications draft figure some common image processing operations original image increased contrast change in hue posterized colors blurred rotated. point operators now that we have seen how images are formed through the interaction of scene elements lighting and camera optics and sensors let us look at the first stage in most computer vision applications namely the use of image processing to preprocess the image and convert it into a form suitable for further analysis. examples of such operations include exposure correction and color balancing the reduction of image noise increasing sharpness or straightening the image by rotating it while some may consider image processing to be outside the purview of computer vision most computer vision applications such as computational photography and even recognition require care in designing the image processing stages in order to achieve acceptable results. in this chapter we review standard image processing operators that map pixel values from one image to another. image processing is often taught in electrical engineering departments as a follow-on course to an introductory course in signal processing and schafer oppenheim schafer and buck there are several popular textbooks for image processing gomes and velho j ahne pratt russ burger and burge gonzales and woods we begin this chapter with the simplest kind of image transforms namely those that manipulate each pixel independently of its neighbors such transforms are often called point operators or point processes. next we examine neighborhood operators where each new pixel s value depends on a small number of neighboring input values and a convenient tool to analyze sometimes accelerate such neighborhood operations is the fourier transform which we cover in section neighborhood operators can be cascaded to form image pyramids and wavelets which are useful for analyzing images at a variety of resolutions and for accelerating certain operations another important class of global operators are geometric transformations such as rotations shears and perspective deformations finally we introduce global optimization approaches to image processing which involve the minimization of an energy functional or equivalently optimal estimation using bayesian markov random field models point operators the simplest kinds of image processing transforms are point operators where each output pixel s value depends on only the corresponding input pixel value potentially some globally collected information or parameters. examples of such operators include brightness and contrast adjustments as well as color correction and transformations. in the image processing literature such operations are also known as point processes we begin this section with a quick review of simple point operators such as brightness computer vision algorithms and applications draft figure some local image processing operations original image along with its three color histograms brightness increased offset b contrast increased gain a gamma linearized full histogram equalization partial histogram equalization. point operators figure visualizing image data original image cropped portion and scanline plot using an image inspection tool grid of numbers surface plot. for figures the image was first converted to grayscale. scaling and image addition. next we discuss how colors in images can be manipulated. we then present image compositing and matting operations which play an important role in computational photography and computer graphics applications. finally we describe the more global process of histogram equalization. we close with an example application that manipulates tonal values and contrast to improve image appearance. pixel transforms a general image processing operator is a function that takes one or more input images and produces an output image. in the continuous domain this can be denoted as gx hfx or gx fnx where x is in the d-dimensional domain of the functions d for images and the functions f and g operate over some range which can either be scalar or vector-valued e.g. for color images or motion. for discrete images the domain consists of a finite number of pixel locations x j and we can write gi j hfi j. figure shows how an image can be represented either by its color as a grid of numbers or as a two-dimensional function plot. two commonly used point processes are multiplication and addition with a constant gx afx b. the parameters a and b are often called the gain and bias parameters sometimes these parameters are said to control contrast and brightness respectively the an image s luminance characteristics can also be summarized by its key luminanance and range uyttendaele deussen et al. computer vision algorithms and applications draft bias and gain parameters can also be spatially varying gx axfx bx e.g. when simulating the graded density filter used by photographers to selectively darken the sky or when modeling vignetting in an optical system. multiplicative gain global and spatially varying is a linear operation since it obeys the superposition principle will have more to say about linear shift invariant operators in section operators such as image squaring is often used to get a local estimate of the energy in a bandpass filtered signal see section are not linear. another commonly used dyadic operator is the linear blend operator gx by varying from this operator can be used to perform a temporal cross-dissolve between two images or videos as seen in slide shows and film production or as a component of image morphing algorithms one highly used non-linear transform that is often applied to images before further processing is gamma correction which is used to remove the non-linear mapping between input radiance and quantized pixel values to invert the gamma mapping applied by the sensor we can use gx where a gamma value of is a reasonable fit for most digital cameras. color transforms while color images can be treated as arbitrary vector-valued functions or collections of independent bands it usually makes sense to think about them as highly correlated signals with strong connections to the image formation process sensor design and human perception consider for example brightening a picture by adding a constant value to all three channels as shown in figure can you tell if this achieves the desired effect of making the image look brighter? can you see any undesirable side-effects or artifacts? in fact adding the same value to each color channel not only increases the apparent intensity of each pixel it can also affect the pixel s hue and saturation. how can we define and manipulate such quantities in order to achieve the desired perceptual effects? point operators figure image matting and compositing curless salesin et al. ieee source image extracted foreground object f alpha matte shown in grayscale new composite c. as discussed in section chromaticity coordinates or even simpler color ratios can first be computed and then used after manipulating brightening the luminance y to re-compute a valid rgb image with the same hue and saturation. figure i shows some color ratio images multiplied by the middle gray value for better visualization. similarly color balancing to compensate for incandescent lighting can be performed either by multiplying each channel with a different scale factor or by the more complex process of mapping to xyz color space changing the nominal white point and mapping back to rgb which can be written down using a linear color twist transform matrix. exercises and have you explore some of these issues. another fun project best attempted after you have mastered the rest of the material in this chapter is to take a picture with a rainbow in it and enhance the strength of the rainbow compositing and matting in many photo editing and visual effects applications it is often desirable to cut a foreground object out of one scene and put it on top of a different background the process of extracting the object from the original image is often called matting and blinn while the process of inserting it into another image visible artifacts is called compositing and duff blinn the intermediate representation used for the foreground object between these two stages is called an alpha-matted color image c. in addition to the three color rgb channels an alpha-matted image contains a fourth alpha channel a that describes the relative amount of opacity or fractional coverage at each pixel and the opacity is the opposite of the transparency. pixels within the object are fully opaque while pixels fully outside the object are transparent pixels on the boundary of the object vary smoothly between these two extremes which hides the perceptual visible jaggies computer vision algorithms and applications draft b f c figure compositing equation c f the images are taken from a close-up of the region of the hair in the upper right part of the lion in figure that occur if only binary opacities are used. to composite a new foreground image on top of an old image the over operator first proposed by porter and duff and then studied extensively by blinn is used this operator attenuates the influence of the background image b by a factor and then adds in the color opacity values corresponding to the foreground layer f as shown in figure c f. in many situations it is convenient to represent the foreground colors in pre-multiplied form i.e. to store manipulate the f values directly. as blinn shows the pre-multiplied rgba representation is preferred for several reasons including the ability to blur or resample rotate alpha-matted images without any additional complications treating each rgba band independently. however when matting using local color consistency and tomasi chuang curless salesin et al. the pure unmultiplied foreground colors f are used since these remain constant vary slowly in the vicinity of the object edge. the over operation is not the only kind of compositing operation that can be used. porter and duff describe a number of additional operations that can be useful in photo editing and visual effects applications. in this book we concern ourselves with only one additional commonly occurring case see exercise when light reflects off clean transparent glass the light passing through the glass and the light reflecting off the glass are simply added together this model is useful in the analysis of transparent motion and anandan szeliski avidan and anandan which occurs when such scenes are observed from a moving camera section the actual process of matting i.e. recovering the foreground background and alpha matte values from one or more images has a rich history which we study in section point operators figure an example of light reflecting off the transparent glass of a picture frame and anandan elsevier. you can clearly see the woman s portrait inside the picture frame superimposed with the reflection of a man s face off the glass. smith and blinn have a nice survey of traditional blue-screen matting techniques while toyama krumm brumitt et al. review difference matting. more recently there has been a lot of activity in computational photography relating to natural image matting and tomasi chuang curless salesin et al. wang and cohen which attempts to extract the mattes from a single natural image or from extended video sequences agarwala curless et al. all of these techniques are described in more detail in section histogram equalization while the brightness and gain controls described in section can improve the appearance of an image how can we automatically determine their best values? one approach might be to look at the darkest and brightest pixel values in an image and map them to pure black and pure white. another approach might be to find the average value in the image push it towards middle gray and expand the range so that it more closely fills the displayable values uyttendaele deussen et al. how can we visualize the set of lightness values in an image in order to test some of these heuristics? the answer is to plot the histogram of the individual color channels and luminance values as shown in figure from this distribution we can compute relevant statistics such as the minimum maximum and average intensity values. notice that the image in figure has both an excess of dark values and light values but that the mid-range values are largely under-populated. would it not be better if we could simultaneously brighten some the histogram is simply the count of the number of pixels at each gray level value. for an eight-bit image an accumulation table with entries is needed. for higher bit depths a table with the appropriate number of entries fewer than the full number of gray levels should be used. computer vision algorithms and applications draft figure histogram analysis and equalization original image color channel and intensity histograms cumulative distribution functions equalization functions full histogram equalization partial histogram equalization. dark values and darken some light values while still using the full extent of the available dynamic range? can you think of a mapping that might do this? one popular answer to this question is to perform histogram equalization i.e. to find an intensity mapping function fi such that the resulting histogram is flat. the trick to finding such a mapping is the same one that people use to generate random samples from a probability density function which is to first compute the cumulative distribution function shown in figure think of the original histogram hi as the distribution of grades in a class after some exam. how can we map a particular grade to its corresponding percentile so that students at the percentile range scored better than of their classmates? the answer is to integrate the distribution hi to obtain the cumulative distribution ci ci n hi ci n hi where n is the number of pixels in the image or students in the class. for any given grade or intensity we can look up its corresponding percentile ci and determine the final value that pixel should take. when working with eight-bit pixel values the i and c axes are rescaled from point operators figure locally adaptive histogram equalization original image block histogram equalization full locally adaptive equalization. figure shows the result of applying fi ci to the original image. as we can see the resulting histogram is flat so is the resulting image is flat in the sense of a lack of contrast and being muddy looking. one way to compensate for this is to only partially compensate for the histogram unevenness e.g. by using a mapping function fi ci which is a linear blend between the cumulative distribution function and the identity transform straight line. as you can see in figure the resulting image maintains more of its original grayscale distribution while having a more appealing balance. another potential problem with histogram equalization in general image brightening is that noise in dark regions can be amplified and become more visible. exercise suggests some possible ways to mitigate this as well as alternative techniques to maintain contrast and punch in the original images rushmeier and piatko stark locally adaptive histogram equalization while global histogram equalization can be useful for some images it might be preferable to apply different kinds of equalization in different regions. consider for example the image in figure which has a wide range of luminance values. instead of computing a single curve what if we were to subdivide the image into m m pixel blocks and perform separate histogram equalization in each sub-block? as you can see in figure the resulting image exhibits a lot of blocking artifacts i.e. intensity discontinuities at block boundaries. one way to eliminate blocking artifacts is to use a moving window i.e. to recompute the histogram for every m m block centered at each pixel. this process can be quite slow operations per pixel although with clever programming only the histogram entries corresponding to the pixels entering and leaving the block a raster scan across the image need to be updated operations per pixel. note that this operation is an example of the non-linear neighborhood operations we study in more detail in section a more efficient approach is to compute non-overlapped block-based equalization functions as before but to then smoothly interpolate the transfer functions as we move between computer vision algorithms and applications draft figure local histogram interpolation using relative t coordinates block-based histograms with block centers shown as circles corner-based spline histograms. pixels are located on grid intersections. the black square pixel s transfer function is interpolated from the four adjacent lookup tables arrows using the computed t values. block boundaries are shown as dashed lines. blocks. this technique is known as adaptive histogram equalization and its contrastlimited version is known as clahe amburn austin et al. the weighting function for a given pixel j can be computed as a function of its horizontal and vertical position t within a block as shown in figure to blend the four lookup functions a bilinear blending function fsti can be used. section for higher-order generalizations of such spline functions. note that instead of blending the four lookup tables for each output pixel would be quite slow we can instead blend the results of mapping a given pixel through the four neighboring lookups. a variant on this algorithm is to place the lookup tables at the corners of each m m block figure and exercise in addition to blending four lookups to compute the final value we can also distribute each input pixel into four adjacent lookup tables during the histogram accumulation phase that the gray arrows in figure point both ways i.e. hklii j wi j k l where wi j k l is the bilinear weighting function between pixel j and lookup table l. this is an example of soft histogramming which is used in a variety of other applica algorithm is implemented in the matlab adapthist function. tsts linear filtering tions including the construction of sift feature descriptors and vocabulary trees application tonal adjustment one of the most widely used applications of point-wise image processing operators is the manipulation of contrast or tone in photographs to make them look either more attractive or more interpretable. you can get a good sense of the range of operations possible by opening up any photo manipulation tool and trying out a variety of contrast brightness and color manipulation options as shown in figures and exercises and have you implement some of these operations in order to become familiar with basic image processing operators. more sophisticated techniques for tonal adjustment ward pattanaik et al. bae paris and durand are described in the section on high dynamic range tone mapping linear filtering locally adaptive histogram equalization is an example of a neighborhood operator or local operator which uses a collection of pixel values in the vicinity of a given pixel to determine its final output value in addition to performing local tone adjustment neighborhood operators can be used to filter images in order to add soft blur sharpen details accentuate edges or remove noise d. in this section we look at linear filtering operators which involve weighted combinations of pixels in small neighborhoods. in section we look at non-linear operators such as morphological filters and distance transforms. the most commonly used type of neighborhood operator is a linear filter in which an output pixel s value is determined as a weighted sum of input pixel values gi j fi k j lhk l. the entries in the weight kernel or mask hk l are often called the filter coefficients. the above correlation operator can be more compactly notated as g f h. a common variant on this formula is gi j fi k j lhk l fk lhi k j l computer vision algorithms and applications draft figure neighborhood filtering the image on the left is convolved with the filter in the middle to yield the image on the right. the light blue pixels indicate the source neighborhood for the light green destination pixel. where the sign of the offsets in f has been reversed. this is called the convolution operator g f h and h is then called the impulse response the reason for this name is that the kernel function h convolved with an impulse signal j image that is everywhere except at the origin reproduces itself h h whereas correlation produces the reflected signal. this yourself to verify that it is so. in fact equation can be interpreted as the superposition of shifted impulse response functions hi k j l multiplied by the input pixel values fk l. convolution has additional nice properties e.g. it is both commutative and associative. as well the fourier transform of two convolved images is the product of their individual fourier transforms both correlation and convolution are linear shift-invariant operators which obey both the superposition principle h h h and the shift invariance principle gi j fi k j l gi j fi k j l which means that shifting a signal commutes with applying the operator stands for the lsi operator. another way to think of shift invariance is that the operator behaves the same everywhere the continuous version of convolution can be written as gx f uhudu. linear filtering figure some neighborhood operations original image blurred sharpened smoothed with edge-preserving filter binary image dilated distance transform connected components. for the dilation and connected components black pixels are assumed to be active i.e. to have a value of in equations computer vision algorithms and applications draft figure one-dimensional signal convolution as a sparse matrix-vector multiply g hf. occasionally a shift-variant version of correlation or convolution may be used e.g. gi j fi k j lhk l i j where hk l i j is the convolution kernel at pixel j. for example such a spatially varying kernel can be used to model blur in an image due to variable depth-dependent defocus. correlation and convolution can both be written as a matrix-vector multiply if we first convert the two-dimensional images fi j and gi j into raster-ordered vectors f and g g hf where the h matrix contains the convolution kernels. figure shows how a one-dimensional convolution can be represented in matrix-vector form. padding effects the astute reader will notice that the matrix multiply shown in figure suffers from boundary effects i.e. the results of filtering the image in this form will lead to a darkening of the corner pixels. this is because the original image is effectively being padded with values wherever the convolution kernel extends beyond the original image boundaries. to compensate for this a number of alternative padding or extension modes have been developed zero set all pixels outside the source image to good choice for alpha-matted cutout images constant color set all pixels outside the source image to a specified border value clamp or clamp to edge repeat edge pixels indefinitely wrap or tile loop around the image in a toroidal configuration linear filtering zero wrap clamp mirror blurred zero normalized zero blurred clamp blurred mirror figure border padding row and the results of blurring the padded image row. the normalized zero image is the result of dividing the blurred zeropadded rgba image by its corresponding soft alpha value. mirror reflect pixels across the image edge extend extend the signal by subtracting the mirrored version of the signal from the edge pixel value. in the computer graphics literature oller and haines p. these mechanisms are known as the wrapping mode or texture addressing mode the formulas for each of these modes are left to the reader figure shows the effects of padding an image with each of the above mechanisms and then blurring the resulting padded image. as you can see zero padding darkens the edges clamp padding propagates border values inward mirror padding preserves colors near the borders. extension padding shown keeps the border pixels fixed blur. an alternative to padding is to blur the zero-padded rgba image and to then divide the resulting image by its alpha value to remove the darkening effect. the results can be quite good as seen in the normalized zero image in figure separable filtering the process of performing a convolution requires k operations per pixel where k is the size or height of the convolution kernel e.g. the box filter in fig computer vision algorithms and applications draft k box k bilinear gaussian sobel corner figure separable linear filters for each image we show the filter kernel the corresponding horizontal kernel and the filtered image the filtered sobel and corner images are signed scaled up by and respectively and added to a gray offset before display. ure in many cases this operation can be significantly sped up by first performing a one-dimensional horizontal convolution followed by a one-dimensional vertical convolution requires a total of operations per pixel. a convolution kernel for which this is possible is said to be separable. it is easy to show that the two-dimensional kernel k corresponding to successive convolution with a horizontal kernel h and a vertical kernel v is the outer product of the two kernels k vht figure for some examples. because of the increased efficiency the design of convolution kernels for computer vision applications is often influenced by their separability. how can we tell if a given kernel k is indeed separable? this can often be done by inspection or by looking at the analytic form of the kernel and adelson a more direct method is to treat the kernel as a matrix k and to take its singular value decomposition iuivt i k appendix for the definition of the svd. if only the first singular value is non-zero the kernel is separable and and provide the vertical and horizontal linear filtering kernels for example the laplacian of gaussian kernel and can be implemented as the sum of two separable filters buxton and buxton what if your kernel is not separable and yet you still want a faster way to implement it? perona who first made the link between kernel separability and svd suggests using more terms in the series i.e. summing up a number of separable convolutions. whether this is worth doing or not depends on the relative sizes of k and the number of significant singular values as well as other considerations such as cache coherency and memory locality. examples of linear filtering now that we have described the process for performing linear filtering let us examine a number of frequently used filters. the simplest filter to implement is the moving average or box filter which simply averages the pixel values in a k k window. this is equivalent to convolving the image with a kernel of all ones and then scaling for large kernels a more efficient implementation is to slide a moving window across each scanline a separable filter while adding the newest pixel and subtracting the oldest pixel from the running sum. this is related to the concept of summed area tables which we describe shortly. a smoother image can be obtained by separably convolving the image with a piecewise linear tent function known as a bartlett filter. figure shows a version of this filter which is called the bilinear kernel since it is the outer product of two linear splines section convolving the linear tent function with itself yields the cubic approximating spline which is called the gaussian kernel in burt and adelson s laplacian pyramid representation note that approximate gaussian kernels can also be obtained by iterated convolution with box filters in applications where the filters really need to be rotationally symmetric carefully tuned versions of sampled gaussians should be used and adelson the kernels we just discussed are all examples of blurring or low-pass kernels they pass through the lower frequencies while attenuating higher frequencies. how good are they at doing this? in section we use frequency-space fourier analysis to examine the exact frequency response of these filters. we also introduce the sinc xx filter which performs ideal low-pass filtering. in practice smoothing kernels are often used to reduce high-frequency noise. we have much more to say about using variants on smoothing to remove noise later sections and surprisingly smoothing kernels can also be used to sharpen images using a process called computer vision algorithms and applications draft unsharp masking. since blurring the image reduces high frequencies adding some of the difference between the original and the blurred image makes it sharper gsharp f hblur f. in fact before the advent of digital photography this was the standard way to sharpen images in the darkroom create a blurred positive negative from the original negative by misfocusing then overlay the two negatives before printing the final image which corresponds to this is no longer a linear filter but it still works well. gunsharp hblur f. linear filtering can also be used as a pre-processing stage to edge extraction and interest point detection algorithms. figure shows a simple edge extractor called the sobel operator which is a separable combination of a horizontal central difference called because the horizontal derivative is centered on the pixel and a vertical tent filter smooth the results. as you can see in the image below the kernel this filter effectively emphasizes horizontal edges. the simple corner detector looks for simultaneous horizontal and vertical second derivatives. as you can see however it responds not only to the corners of the square but also along diagonal edges. better corner detectors or at least interest point detectors that are more rotationally invariant are described in section band-pass and steerable filters the sobel and corner operators are simple examples of band-pass and oriented filters. more sophisticated kernels can be created by first smoothing the image with a area gaussian filter and then taking the first or second derivatives witkin freeman and adelson such filters are known collectively as band-pass filters since they filter out both low and high frequencies. the second derivative of a two-dimensional image is known as the laplacian operator. blurring an image with a gaussian and then taking its laplacian is equivalent to convolving directly with the laplacian of gaussian filter y gx y gx y e linear filtering figure second-order steerable filter ieee original image of einstein orientation map computed from the second-order oriented energy original image with oriented structures enhanced. which has certain nice scale-space properties witkin terzopoulos and kass the five-point laplacian is just a compact approximation to this more sophisticated filter. likewise the sobel operator is a simple approximation to a directional or oriented filter which can obtained by smoothing with a gaussian some other filter and then taking a directional derivative u u which is obtained by taking the dot product between the gradient field and a unit direction u sin u f ug f ug f. the smoothed directional derivative filter g u ugx vgy u g x v g y where u v is an example of a steerable filter since the value of an image convolved with g u can be computed by first convolving with the pair of filters gy and then steering the filter locally by multiplying this gradient field with a unit vector u and adelson the advantage of this approach is that a whole family of filters can be evaluated with very little cost. how about steering a directional second derivative filter u ug u which is the result of taking a directional derivative and then taking the directional derivative again? for example gxx is the second directional derivative in the x direction. at first glance it would appear that the steering trick will not work since for every direction u we need to compute a different first directional derivative. somewhat surprisingly freeman and adelson showed that for directional gaussian derivatives it is possible computer vision algorithms and applications draft figure fourth-order steerable filter and adelson ieee test image containing bars and step edges at different orientations average oriented energy dominant orientation oriented energy as a function of angle plot. to steer any order of derivative with a relatively small number of basis functions. for example only three basis functions are required for the second-order directional derivative g u u furthermore each of the basis filters while not itself necessarily separable can be computed using a linear combination of a small number of separable filters and adelson this remarkable result makes it possible to construct directional derivative filters of increasingly greater directional selectivity i.e. filters that only respond to edges that have strong local consistency in orientation furthermore higher order steerable filters can respond to potentially more than a single edge orientation at a given location and they can respond to both bar edges lines and the classic step edges in order to do this however full hilbert transform pairs need to be used for second-order and higher filters as described in and adelson steerable filters are often used to construct both feature descriptors and edge detectors while the filters developed by freeman and adelson are best suited for detecting linear structures more recent work by koethe shows how a combined boundary tensor can be used to encode both edge and junction corner features. exercise has you implement such steerable filters and apply them to finding both edge and corner features. summed area table image if an image is going to be repeatedly convolved with different box filters especially filters of different sizes at different locations you can precompute the summed area table linear filtering figure summed area tables original image summed area table computation of area sum. each value in the summed area table si j is computed recursively from its three adjacent neighbors area sums s are computed by combining the four values at the rectangle corners positive values are shown in bold and negative values in italics. which is just the running sum of all the pixel values from the origin si j fk l. this can be efficiently computed using a recursive algorithm si j si j si j si j fi j. the image si j is also often called an integral image figure and can actually be computed using only two additions per pixel if separate row sums are used and jones to find the summed area inside a rectangle we simply combine four samples from the summed area table a potential disadvantage of summed area tables is that they require log m log n extra bits in the accumulation image compared to the original image where m and n are the image width and height. extensions of summed area tables can also be used to approximate other convolution kernels section contains a review. in computer vision summed area tables have been used in face detection and jones to compute simple multi-scale low-level features. such features which consist of adjacent rectangles of positive and negative values are also known as boxlets bottou s s s computer vision algorithms and applications draft haffner et al. in principle summed area tables could also be used to compute the sums in the sum of squared differences stereo and motion algorithms in practice separable moving average filters are usually preferred yoshida oda et al. unless many different window shapes and sizes are being considered recursive filtering the incremental formula for the summed area is an example of a recursive filter i.e. one whose values depends on previous filter outputs. in the signal processing literature such filters are known as infinite impulse response since the output of the filter to an impulse non-zero value goes on forever. for example for a summed area table an impulse generates an infinite rectangle of below and to the right of the impulse. the filters we have previously studied in this chapter which involve the image with a finite extent kernel are known as finite impulse response two-dimensional iir filters and recursive formulas are sometimes used to compute quantities that involve large area interactions such as two-dimensional distance functions and connected components more commonly however iir filters are used inside one-dimensional separable filtering stages to compute large-extent smoothing kernels such as efficient approximations to gaussians and edge filters nielsen florack and deriche pyramid-based algorithms can also be used to perform such large-area smoothing computations. more neighborhood operators as we have just seen linear filters can perform a wide variety of image transformations. however non-linear filters such as edge-preserving median or bilateral filters can sometimes perform even better. other examples of neighborhood operators include morphological operators that operate on binary images as well as semi-global operators that compute distance transforms and find connected components in binary images h. non-linear filtering the filters we have looked at so far have all been linear i.e. their response to a sum of two signals is the same as the sum of the individual responses. this is equivalent to saying that each output pixel is a weighted summation of some number of input pixels linear filters are easier to compose and are amenable to frequency response analysis in many cases however better performance can be obtained by using a non-linear combination of neighboring pixels. consider for example the image in figure where the more neighborhood operators figure median and bilateral filtering original image with gaussian noise gaussian filtered median filtered bilaterally filtered original image with shot noise gaussian filtered median filtered bilaterally filtered. note that the bilateral filter fails to remove the shot noise because the noisy pixels are too different from their neighbors. figure median and bilateral filtering median pixel selected mean pixels domain filter along edge are pixel distances range filter. median domain filterd range filter computer vision algorithms and applications draft noise rather than being gaussian is shot noise i.e. it occasionally has very large values. in this case regular blurring with a gaussian filter fails to remove the noisy pixels and instead turns them into softer still visible spots median filtering a better filter to use in this case is the median filter which selects the median value from each pixel s neighborhood median values can be computed in expected linear time using a randomized select algorithm and incremental variants have also been developed by tomasi and manduchi and bovik section since the shot noise value usually lies well outside the true values in the neighborhood the median filter is able to filter away such bad pixels one downside of the median filter in addition to its moderate computational cost is that since it selects only one input pixel value to replace each output pixel it is not as efficient at averaging away regular gaussian noise hampel ronchetti rousseeuw et al. stewart a better choice may be the mean and redner p. which averages together all of the pixels except for the fraction that are the smallest and the largest another possibility is to compute a weighted median in which each pixel is used a number of times depending on its distance from the center. this turns out to be equivalent to minimizing the weighted objective function wk lfi k j l gi jp where gi j is the desired output value and p for the weighted median. the value p is the usual weighted mean which is equivalent to correlation after normalizing by the sum of the weights section and shapiro section the weighted mean also has deep connections to other methods in robust statistics appendix such as influence functions hampel ronchetti rousseeuw et al. non-linear smoothing has another perhaps even more important property especially since shot noise is rare in today s cameras. such filtering is more edge preserving i.e. it has less tendency to soften edges while filtering away high-frequency noise. consider the noisy image in figure in order to remove most of the noise the gaussian filter is forced to smooth away high-frequency detail which is most noticeable near strong edges. median filtering does better but as mentioned before does not do as good a job at smoothing away from discontinuities. see and manduchi for some additional references to edge-preserving smoothing techniques. more neighborhood operators while we could try to use the mean or weighted median these techniques still have a tendency to round sharp corners since the majority of pixels in the smoothing area come from the background distribution. bilateral filtering what if we were to combine the idea of a weighted filter kernel with a better version of outlier rejection? what if instead of rejecting a fixed percentage we simply reject a soft way pixels whose values differ too much from the central pixel value? this is the essential idea in bilateral filtering which was first popularized in the computer vision community by tomasi and manduchi chen paris and durand and paris kornprobst tumblin et al. cite similar earlier work and weule smith and brady as well as the wealth of subsequent applications in computer vision and computational photography. in the bilateral filter the output pixel value depends on a weighted combination of neigh boring pixel values the weighting coefficient wi j k l depends on the product of a domain kernel gi j fk lwi j k l wi j k l di j k l d and a data-dependent range kernel ri j k l j fk r when multiplied together these yield the data-dependent bilateral weight function wi j k l d j fk r figure shows an example of the bilateral filtering of a noisy step edge. note how the domain kernel is the usual gaussian the range kernel measures appearance similarity to the center pixel and the bilateral filter kernel is a product of these two. notice that the range filter uses the vector distance between the center and the neighboring pixel. this is important in color images since an edge in any one of the color bands signals a change in material and hence the need to downweight a pixel s tomasi and manduchi show that using the vector distance opposed to filtering each color band separately reduces color fringing effects. they also recommend taking the color difference in the more perceptually uniform cielab color space section computer vision algorithms and applications draft figure bilateral filtering and dorsey acm noisy step edge input domain filter range filter to center pixel value bilateral filter filtered step edge output distance between pixels. since bilateral filtering is quite slow compared to regular separable filtering a number of acceleration techniques have been developed and dorsey paris and durand chen paris and durand paris kornprobst tumblin et al. unfortunately these techniques tend to use more memory than regular filtering and are hence not directly applicable to filtering full-color images. iterated adaptive smoothing and anisotropic diffusion bilateral other filters can also be applied in an iterative fashion especially if an appearance more like a cartoon is desired and manduchi when iterated filtering is applied a much smaller neighborhood can often be used. consider for example using only the four nearest neighbors i.e. restricting i j in observe that di j k l d e d i j i j more neighborhood operators we can thus re-write as f j f j f lri j k l ri j k l f j ri j k lf l f j where r ri j k l l are the neighbors of j and we have made the iterative nature of the filtering explicit. as barash notes is the same as the discrete anisotropic diffusion equation first proposed by perona and malik since its original introduction anisotropic diffusion has been extended and applied to a wide range of problems florack and deriche black sapiro marimont et al. weickert ter haar romeny and viergever weickert it has also been shown to be closely related to other adaptive smoothing techniques chen and medioni barash barash and comaniciu as well as bayesian regularization with a non-linear smoothness term that can be derived from image statistics black and haussecker in its general form the range kernel ri j k l j fk which is usually called the gain or edge-stopping function or diffusion coefficient can be any monotonically increasing function with as x black sapiro marimont et al. show how anisotropic diffusion is equivalent to minimizing a robust penalty function on the image gradients which we discuss in sections and scharr black and haussecker show how the edge-stopping function can be derived in a principled manner from local image statistics. they also extend the diffusion neighborhood from to which allows them to create a diffusion operator that is both rotationally invariant and incorporates information about the eigenvalues of the local structure tensor. note that without a bias term towards the original image anisotropic diffusion and iterative adaptive smoothing converge to a constant image. unless a small number of iterations is used for speed it is usually preferable to formulate the smoothing problem as a joint minimization of a smoothness term and a data fidelity term as discussed in sections and and by scharr black and haussecker which introduce such a bias in a principled manner. morphology while non-linear filters are often used to enhance grayscale and color images they are also used extensively to process binary images. such images often occur after a thresholding the r factor is not present in anisotropic diffusion but becomes negligible as computer vision algorithms and applications draft figure binary image morphology original image dilation erosion majority opening closing. the structuring element for all examples is a square. the effects of majority are a subtle rounding of sharp corners. opening fails to eliminate the dot since it is not wide enough. operation t if f t else e.g. converting a scanned grayscale document into a binary image for further processing such as optical character recognition. the most common binary image operations are called morphological operations since they change the shape of the underlying binary objects and wilson chapter to perform such an operation we first convolve the binary image with a binary structuring element and then select a binary output value depending on the thresholded result of the convolution. is not the usual way in which these operations are described but i find it a nice simple way to unify the processes. the structuring element can be any shape from a simple box filter to more complicated disc structures. it can even correspond to a particular shape that is being sought for in the image. figure shows a close-up of the convolution of a binary image f with a struc turing element s and the resulting images for the operations described below. let c f s be the integer-valued count of the number of inside each structuring element as it is scanned over the image and s be the size of the structuring element of pixels. the standard operations used in binary morphology include dilation dilatef s erosion erodef s s majority majf s opening openf s dilateerodef s s more neighborhood operators closing closef s erodedilatef s s. as we can see from figure dilation grows objects consisting of while erosion shrinks them. the opening and closing operations tend to leave large regions and smooth boundaries unaffected while removing small objects or holes and smoothing boundaries. while we will not use mathematical morphology much in the rest of this book it is a handy tool to have around whenever you need to clean up some thresholded images. you can find additional details on morphology in other textbooks on computer vision and image processing and shapiro section section and wilson section as well as articles and books specifically on this topic serra and vincent yuille vincent and geiger soille distance transforms the distance transform is useful in quickly precomputing the distance to a curve or set of points using a two-pass raster algorithm and pfaltz danielsson borgefors paglieroni breu gil kirkpatrick et al. felzenszwalb and huttenlocher fabbri costa torelli et al. it has many applications including level sets fast chamfer matching image alignment klanderman and rucklidge feathering in image stitching and blending and nearest point alignment the distance transform di j of a binary image bi j is defined as follows. let dk l be some distance metric between pixel offsets. two commonly used metrics include the city block or manhattan distance and the euclidean distance the distance transform is then defined as l l di j min di k j l i.e. it is the distance to the nearest background pixel whose value is the city block distance transform can be efficiently computed using a forward and backward pass of a simple raster-scan algorithm as shown in figure during the forward pass each non-zero pixel in b is replaced by the minimum of the distance of its north or west neighbor. during the backward pass the same occurs except that the minimum is both over the current value d and the distance of the south and east neighbors computer vision algorithms and applications draft figure city block distance transform original binary image top to bottom raster sweep green values are used to compute the orange value bottom to top raster sweep green values are merged with old orange value final distance transform. efficiently computing the euclidean distance transform is more complicated. here just keeping the minimum scalar distance to the boundary during the two passes is not sufficient. instead a vector-valued distance consisting of both the x and y coordinates of the distance to the boundary must be kept and compared using the squared distance rule. as well larger search regions need to be used to obtain reasonable results. rather than explaining the algorithm borgefors in more detail we leave it as an exercise for the motivated reader figure shows a distance transform computed from a binary image. notice how the values grow away from the black regions and form ridges in the white area of the original image. because of this linear growth from the starting boundary pixels the distance transform is also sometimes known as the grassfire transform since it describes the time at which a fire starting inside the black region would consume any given pixel or a chamfer because it resembles similar shapes used in woodworking and industrial design. the ridges in the distance transform become the skeleton medial axis transform of the region where the transform is computed and consist of pixels that are of equal distance to two more boundaries and kimia sebastian and kimia a useful extension of the basic distance transform is the signed distance transform which computes distances to boundary pixels for all the pixels ee and szeliski the simplest way to create this is to compute the distance transforms for both the original binary image and its complement and to negate one of them before combining. because such distance fields tend to be smooth it is possible to store them more compactly minimal loss in relative accuracy using a spline defined over a quadtree or octree data structure ee and szeliski szeliski and lavall ee frisken perry rockwood et al. such precomputed signed distance transforms can be extremely useful in efficiently aligning and merging curves and surfaces klanderman and rucklidge more neighborhood operators figure connected component computation original grayscale image horizontal runs connected by vertical edges blue runs are pseudocolored with unique colors inherited from parent nodes re-coloring after merging adjacent segments. szeliski and lavall ee curless and levoy especially if the vectorial version of the distance transform i.e. a pointer from each pixel or voxel to the nearest boundary or surface element is stored and interpolated. signed distance fields are also an essential component of level set evolution where they are called characteristic functions. connected components another useful semi-global image operation is finding connected components which are defined as regions of adjacent pixels that have the same input value label. the remainder of this section consider pixels to be adjacent if they are immediate neighbors and they have the same input value. connected components can be used in a variety of applications such as finding individual letters in a scanned document or finding objects cells in a thresholded image and computing their area statistics. consider the grayscale image in figure there are four connected components in this figure the outermost set of white pixels the large ring of gray pixels the white enclosed region and the single gray pixel. these are shown pseudocolored in figure as pink green blue and brown. to compute the connected components of an image we first split the image into horizontal runs of adjacent pixels and then color the runs with unique labels re-using the labels of vertically adjacent runs whenever possible. in a second phase adjacent runs of different colors are then merged. while this description is a little sketchy it should be enough to enable a motivated student to implement this algorithm haralick and shapiro section give a much longer description of various connected component algorithms including ones computer vision algorithms and applications draft that avoid the creation of a potentially large re-coloring table. well-debugged connected component algorithms are also available in most image processing libraries. once a binary or multi-valued image has been segmented into its connected components it is often useful to compute the area statistics for each individual region r. such statistics include the area of pixels the perimeter of boundary pixels the centroid x and y values the second moments m r y y x x y y x x from which the major and minor axis orientation and lengths can be computed using eigenvalue these statistics can then be used for further processing e.g. for sorting the regions by the area size consider the largest regions first or for preliminary matching of regions in different images. fourier transforms in section we mentioned that fourier analysis could be used to analyze the frequency characteristics of various filters. in this section we explain both how fourier analysis lets us determine these characteristics equivalently the frequency content of an image and how using the fast fourier transform lets us perform large-kernel convolutions in time that is independent of the kernel s size. more comprehensive introductions to fourier transforms are provided by bracewell glassner oppenheim and schafer oppenheim schafer and buck how can we analyze what a given filter does to high medium and low frequencies? the answer is to simply pass a sinusoid of known frequency through the filter and to observe by how much it is attenuated. let sx f x i sin x i moments can also be computed using green s theorem applied to the boundary pixels and albregtsen fourier transforms figure the fourier transform as the response of a filter hx to an input sinusoid sx ej x yielding an output sinusoid ox hx sx aej x be the input sinusoid whose frequency is f angular frequency is f and phase is i. note that in this section we use the variables x and y to denote the spatial coordinates of an image rather than i and j as in the previous sections. this is both because the letters i and j are used for the imaginary number usage depends on whether you are reading complex variables or electrical engineering literature and because it is clearer how to distinguish the horizontal and vertical components in frequency space. in this section we use the letter j for the imaginary number since that is the form more commonly found in the signal processing literature oppenheim and schafer oppenheim schafer and buck if we convolve the sinusoidal signal sx with a filter whose impulse response is hx we get another sinusoid of the same frequency but different magnitude a and phase o ox hx sx a sin x o as shown in figure to see that this is the case remember that a convolution can be expressed as a weighted summation of shifted input signals and that the summation of a bunch of shifted sinusoids of the same frequency is just a single sinusoid at that the new magnitude a is called the gain or magnitude of the filter while the phase difference o i is called the shift or phase. in fact a more compact notation is to use the complex-valued sinusoid sx ej x cos x j sin x. in that case we can simply write ox hx sx aej x if h is a general transform additional harmonic frequencies are introduced. this was traditionally the bane of audiophiles who insisted on equipment with no harmonic distortion. now that digital audio has introduced pure distortion-free sound some audiophiles are buying retro tube amplifiers or digital signal processors that simulate such distortions because of their warmer sound sxoxhxsoxxa computer vision algorithms and applications draft the fourier transform is simply a tabulation of the magnitude and phase response at each frequency h f aej i.e. it is the response to a complex sinusoid of frequency passed through the filter hx. the fourier transform pair is also often written as hx f h unfortunately does not give an actual formula for computing the fourier transform. instead it gives a recipe i.e. convolve the filter with a sinusoid observe the magnitude and phase shift repeat. fortunately closed form equations for the fourier transform exist both in the continuous domain h hxe j xdx and in the discrete domain hk n n hxe j kx n where n is the length of the signal or region of analysis. these formulas apply both to filters such as hx and to signals or images such as sx or gx. the discrete form of the fourier transform is known as the discrete fourier transform note that while can be evaluated for any value of k it only makes sense this is because larger values of k alias with lower for values in the range k n frequencies and hence provide no additional information as explained in the discussion on aliasing in section n at face value the dft takes on operations to evaluate. fortunately there exists a faster algorithm called the fast fourier transform which requires only on n operations oppenheim schafer and buck we do not explain the details of the algorithm here except to say that it involves a series of n stages where each stage performs small transforms multiplications with known coefficients followed by some semi-global permutations. will often see the term butterfly applied to these stages because of the pictorial shape of the signal processing graphs involved. implementations for the fft can be found in most numerical and signal processing libraries. now that we have defined the fourier transform what are some of its properties and how can they be used? table lists a number of useful properties which we describe in a little more detail below fourier transforms property signal transform f j f superposition shift reversal convolution correlation multiplication differentiation domain scaling real images fx f x fx hx fx hx fxhx fax f f f h fx f f f j f parseval s theorem table some useful properties of fourier transforms. the original transform pair is f ffx. superposition the fourier transform of a sum of signals is the sum of their fourier transforms. thus the fourier transform is a linear operator. shift the fourier transform of a shifted signal is the transform of the original signal multiplied by a linear phase shift sinusoid. reversal the fourier transform of a reversed signal is the complex conjugate of the signal s transform. convolution the fourier transform of a pair of convolved signals is the product of their transforms. correlation the fourier transform of a correlation is the product of the first transform times the complex conjugate of the second one. multiplication the fourier transform of the product of two signals is the convolution of their transforms. differentiation the fourier transform of the derivative of a signal is that signal s transform multiplied by the frequency. in other words differentiation linearly emphasizes higher frequencies. domain scaling the fourier transform of a stretched signal is the equivalently compressed scaled version of the original transform and vice versa. computer vision algorithms and applications draft real images the fourier transform of a real-valued signal is symmetric around the origin. this fact can be used to save space and to double the speed of image ffts by packing alternating scanlines into the real and imaginary parts of the signal being transformed. parseval s theorem the energy of squared values of a signal is the same as the energy of its fourier transform. all of these properties are relatively straightforward to prove exercise and they will come in handy later in the book e.g. when designing optimum wiener filters or performing fast image correlations fourier transform pairs now that we have these properties in place let us look at the fourier transform pairs of some commonly occurring filters and signals as listed in table in more detail these pairs are as follows impulse the impulse response has a constant frequency transform. shifted impulse the shifted impulse has unit magnitude and linear phase. box filter the box average filter boxx if else has a sinc fourier transform sinc sin which has an infinite number of side lobes. conversely the sinc filter is an ideal lowpass filter. for a non-unit box the width of the box a and the spacing of the zero crossings in the sinc are inversely proportional. tent the piecewise linear tent function tentx has a fourier transform. gaussian the area gaussian of width gx e has a height gaussian of width as its fourier transform. fourier transforms name impulse shifted impulse box filter tent gaussian laplacian of gaussian gabor unsharp mask windowed sinc signal transform u boxxa tentxa gx e j u asinca g g cos gx rcosxaw sincxa g figure table some useful fourier transform pairs the dashed line in the fourier transform of the shifted impulse indicates its phase. all other transforms have zero phase are real-valued. note that the figures are not necessarily drawn to scale but are drawn to illustrate the general shape and characteristics of the filter or its response. in particular the laplacian of gaussian is drawn inverted because it resembles more a mexican hat as it is sometimes called. computer vision algorithms and applications draft laplacian of gaussian the second derivative of a gaussian of width logx has a band-pass response of as its fourier transform. gabor the even gabor function which is the product of a cosine of frequency and a gaussian of width has as its transform the sum of the two gaussians of width centered at the odd gabor function which uses a sine is the difference of two such gaussians. gabor functions are often used for oriented and band-pass filtering since they can be more frequency selective than gaussian derivatives. unsharp mask the unsharp mask introduced in has as its transform a unit response with a slight boost at higher frequencies. windowed sinc the windowed sinc function shown in table has a response function that approximates an ideal low-pass filter better and better as additional side lobes are added is increased. figure shows the shapes of these such filters along with their fourier transforms. for these examples we use a one-lobe raised cosine rcosx cos xboxx also known as the hann window as the windowing function. wolberg and oppenheim schafer and buck discuss additional windowing functions which include the lanczos window the positive first lobe of a sinc function. we can also compute the fourier transforms for the small discrete kernels shown in figure table notice how the moving average filters do not uniformly dampen higher frequencies and hence can lead to ringing artifacts. the binomial filter and velho used as the gaussian in burt and adelson s laplacian pyramid section does a decent job of separating the high and low frequencies but still leaves a fair amount of high-frequency detail which can lead to aliasing after downsampling. the sobel edge detector at first linearly accentuates frequencies but then decays at higher frequencies and hence has trouble detecting fine-scale edges e.g. adjacent black and white columns. we look at additional examples of small kernel fourier transforms in section where we study better kernels for pre-filtering before decimation reduction. fourier transforms name kernel transform plot cos cos cos linear cos binomial cos sobel sin corner cos table fourier transforms of the separable kernels shown in figure computer vision algorithms and applications draft two-dimensional fourier transforms the formulas and insights we have developed for one-dimensional signals and their transforms translate directly to two-dimensional images. here instead of just specifying a horizontal or vertical frequency x or y we can create an oriented sinusoid of frequency x y sx y sin xx yy. the corresponding two-dimensional fourier transforms are then h x y hx ye j xx yydx dy and in the discrete domain hkx ky m n m n hx ye kxxky y m n where m and n are the width and height of the image. all of the fourier transform properties from table carry over to two dimensions if we replace the scalar variables x and a with their vector counterparts x y x y and a ay and use vector inner products instead of multiplications. wiener filtering while the fourier transform is a useful tool for analyzing the frequency characteristics of a filter kernel or image it can also be used to analyze the frequency spectrum of a whole class of images. a simple model for images is to assume that they are random noise fields whose expected magnitude at each frequency is given by this power spectrum ps x y i.e. x ps x y where the angle brackets denote the expected value of a random to generate such an image we simply create a random gaussian noise image s x y where each pixel is a zero-mean of variance ps x y and then take its inverse fft. the observation that signal spectra capture a first-order description of spatial statistics is widely used in signal and image processing. in particular assuming that an image is a the notation e is also commonly used. we set the dc constant component at to the mean grey level. see algorithm in appendix for code to generate gaussian noise. fourier transforms sample from a correlated gaussian random noise field combined with a statistical model of the measurement process yields an optimum restoration filter known as the wiener to derive the wiener filter we analyze each frequency component of a signal s fourier transform independently. the noisy image formation process can be written as ox y sx y nx y where sx y is the image we are trying to recover nx y is the additive noise signal and ox y is the observed noisy image. because of the linearity of the fourier transform we can write o x y s x y n x y where each quantity in the above equation is the fourier transform of the corresponding image. at each frequency x y we know from our image spectrum that the unknown transform component s x y has a prior distribution which is a zero-mean gaussian with variance ps x y. we also have noisy measurement o x y whose variance is pn x y i.e. the power spectrum of the noise which is usually assumed to be constant n. pn x y according to bayes rule the posterior estimate of s can be written as pso posps po where po posps is a normalizing constant used to make the pso distribution proper to the prior distribution ps is given by ps e where is the expected mean at that frequency everywhere except at the origin and the measurement distribution p is given by ps e taking the negative logarithm of both sides of and setting for simplicity we get log pso log pos log ps c n s c wiener is pronounced veener since in german the w is pronounced v remember that next time you order wiener schnitzel computer vision algorithms and applications draft figure one-dimensional wiener filter power spectrum of signal psf noise level and wiener filter transform w wiener filter spatial kernel. which is the negative posterior log likelihood. the minimum of this quantity is easy to compute sopt the quantity p n p n p s o ps ps pn o w x y nps x y pnps o. is the fourier transform of the optimum wiener filter needed to remove the noise from an image whose power spectrum is ps x y. notice that this filter has the right qualitative properties i.e. for low frequencies where n it has unit gain whereas for high frequencies it attenuates the noise by a factor ps n. figure shows the one-dimensional transform w and the corresponding filter ps kernel wx for the commonly assumed case of p f exercise has you compare the wiener filter as a denoising algorithm to hand-tuned gaussian smoothing. the methodology given above for deriving the wiener filter can easily be extended to the case where the observed image is a noisy blurred version of the original image ox y bx y sx y nx y where bx y is the known blur kernel. rather than deriving the corresponding wiener filter we leave it as an exercise which also encourages you to compare your de-blurring results with unsharp masking and na ve inverse filtering. more sophisticated algorithms for blur removal are discussed in sections and discrete cosine transform the discrete cosine transform is a variant of the fourier transform particularly wellsuited to compressing images in a block-wise fashion. the one-dimensional dct is computed by taking the dot product of each n-wide block of pixels with a set of cosines of pnw fourier transforms figure discrete cosine transform basis functions the first dc constant basis is the horizontal blue line the second is the brown half-cycle waveform etc. these bases are widely used in image and video compression standards such as jpeg. n fi n different frequencies f where k is the coefficient index and the offset is used to make the basis coefficients symmetric some of the discrete cosine basis functions are shown in figure as you can see the first basis function straight blue line encodes the average dc value in the block of pixels while the second encodes a slightly curvy version of the slope. in turns out that the dct is a good approximation to the optimal karhunen loeve decomposition of natural image statistics over small patches which can be obtained by performing a principal component analysis of images as described in section the kltransform de-correlates the signal optimally the signal is described by its spectrum and thus theoretically leads to optimal compression. the two-dimensional version of the dct is defined similarly f l n n n n fi j. like the fast fourier transform the dct can be implemented separably i.e. first computing the dct of each line in the block and then computing the dct of each resulting column. like the fft each of the dcts can also be computed in on log n time. as we mentioned in section the dct is widely used in today s image and video compression algorithms although it is slowly being supplanted by wavelet algorithms and adelson as discussed in section and overlapped variants of the dct which are used in the new jpeg xr these computer vision algorithms and applications draft newer algorithms suffer less from the blocking artifacts edge-aligned discontinuities that result from the pixels in each block being transformed and quantized independently. see exercise for ideas on how to remove blocking artifacts from compressed jpeg images. application sharpening blur and noise removal another common application of image processing is the enhancement of images through the use of sharpening and noise removal operations which require some kind of neighborhood processing. traditionally these kinds of operation were performed using linear filtering sections and section today it is more common to use non-linear filters such as the weighted median or bilateral filter anisotropic diffusion or non-local means coll and morel variational methods especially those using non-quadratic norms such as the norm is called total variation are also often used. figure shows some examples of linear and non-linear filters being used to remove noise. when measuring the effectiveness of image denoising algorithms it is common to report the results as a peak signal-to-noise ratio measurement where ix is the original image and ix is the image after denoising this is for the case where the noisy image has been synthetically generated so that the clean image is known. a better way to measure the quality is to use a perceptually based similarity metric such as the structural similarity index bovik sheikh et al. wang bovik and simoncelli exercises and have you implement some of these operations and compare their effectiveness. more sophisticated techniques for blur removal and the related task of super-resolution are discussed in section pyramids and wavelets so far in this chapter all of the image transformations we have studied produce output images of the same size as the inputs. often however we may wish to change the resolution of an image before proceeding further. for example we may need to interpolate a small image to make its resolution match that of the output printer or computer screen. alternatively we may want to reduce the size of an image to speed up the execution of an algorithm or to save on storage space or transmission time. sometimes we do not even know what the appropriate resolution for the image should be. consider for example the task of finding a face in an image since we do not know the scale at which the face will appear we need to generate a whole pyramid pyramids and wavelets of differently sized images and scan each one for possible faces. visual systems also operate on a hierarchy of scales such a pyramid can also be very helpful in accelerating the search for an object by first finding a smaller instance of that object at a coarser level of the pyramid and then looking for the full resolution object only in the vicinity of coarse-level detections finally image pyramids are extremely useful for performing multi-scale editing operations such as blending images while maintaining details. in this section we first discuss good filters for changing image resolution i.e. upsampling section and downsampling section we then present the concept of multi-resolution pyramids which can be used to create a complete hierarchy of differently sized images and to enable a variety of applications a closely related concept is that of wavelets which are a special kind of pyramid with higher frequency selectivity and other useful properties finally we present a useful application of pyramids namely the blending of different images in a way that hides the seams between the image boundaries interpolation in order to interpolate upsample an image to a higher resolution we need to select some interpolation kernel with which to convolve the image gi j fk lhi rk j rl. this formula is related to the discrete convolution formula except that we replace k and l in h with rk and rl where r is the upsampling rate. figure shows how to think of this process as the superposition of sample weighted interpolation kernels one centered at each input sample k. an alternative mental model is shown in figure where the kernel is centered at the output pixel value i two forms are equivalent. the latter form is sometimes called the polyphase filter form since the kernel values hi can be stored as r separate kernels each of which is selected for convolution with the input samples depending on the phase of i relative to the upsampled grid. what kinds of kernel make good interpolators? the answer depends on the application and the computation time involved. any of the smoothing kernels shown in tables and can be used after appropriate the linear interpolator to the tent kernel produces interpolating piecewise linear curves which result in unappealing creases when applied to images the cubic b-spline whose discrete sampling appears as the binomial kernel in table is an approximating kernel interpolated the smoothing kernels in table have a unit area. to turn them into interpolating kernels we simply scale them up by the interpolation rate r. computer vision algorithms and applications draft figure signal interpolation gi fkhi rk weighted summation of input values polyphase filter interpretation. image does not pass through the input data points that produces soft images with reduced high-frequency detail. the equation for the cubic b-spline is easiest to derive by convolving the tent function b-spline with itself. while most graphics cards use the bilinear kernel combined with a mipmap see section most photo editing packages use bicubic interpolation. the cubic interpolant is a c piecewise-cubic spline term spline is synonymous with piecewise-polynomial whose equation is ax if if otherwise hx where a specifies the derivative at x kenyon and troxel the value of a is often set to since this best matches the frequency characteristics of a sinc function it also introduces a small amount of sharpening which can be visually appealing. unfortunately this choice does not linearly interpolate straight lines ramps so some visible ringing may occur. a better choice for large amounts of interpolation is probably a which produces a quadratic reproducing spline it interpolates linear and quadratic functions exactly section figure shows the a and a cubic interpolating kernel along with their fourier transforms figure and c shows them being applied to two-dimensional interpolation. splines have long been used for function and data value interpolation because of the ability to precisely specify derivatives at control points and efficient incremental algorithms for their evaluation beatty and barsky farin splines are widely used in geometric modeling and computer-aided design applications although they have the term spline comes from the draughtsman s workshop where it was the name of a flexible piece of wood or metal used to draw smooth curves. pyramids and wavelets figure two-dimensional image interpolation bilinear bicubic bicubic windowed sinc taps. figure some windowed sinc functions and their log fourier transforms raisedcosine windowed sinc in blue cubic interpolators and a in green and purple and tent function in brown. they are often used to perform high-accuracy low-pass filtering operations. computer vision algorithms and applications draft started being displaced by subdivision surfaces schr oder and sweldens peters and reif in computer vision splines are often used for elastic image deformations motion estimation and surface interpolation in fact it is possible to carry out most image processing operations by representing images as splines and manipulating them in a multi-resolution framework the highest quality interpolator is generally believed to be the windowed sinc function because it both preserves details in the lower resolution image and avoids aliasing. is also possible to construct a c piecewise-cubic approximation to the windowed sinc by matching its derivatives at zero crossing and ito however some people object to the excessive ringing that can be introduced by the windowed sinc and to the repetitive nature of the ringing frequencies figure for this reason some photographers prefer to repeatedly interpolate images by a small fractional amount tends to de-correlate the original pixel grid with the final image. additional possibilities include using the bilateral filter as an interpolator cohen lischinski et al. using global optimization or hallucinating details decimation while interpolation can be used to increase the resolution of an image decimation is required to reduce the to perform decimation we first convolve the image with a low-pass filter avoid aliasing and then keep every rth sample. in practice we usually only evaluate the convolution at every rth sample gi j fk lhri k rj l as shown in figure note that the smoothing kernel hk l in this case is often a stretched and re-scaled version of an interpolation kernel. alternatively we can write gi j fk lhi kr j lr and keep the same kernel hk l for both interpolation and decimation. one commonly used decimation filter is the binomial filter introduced by burt and adelson as shown in table this kernel does a decent job of separating the high and low frequencies but still leaves a fair amount of high-frequency detail which can lead to aliasing after downsampling. however for applications such as image blending later in this section this aliasing is of little concern. the term decimation has a gruesome etymology relating to the practice of killing every tenth soldier in it is generally used in signal processing to mean any downsampling or rate a roman unit guilty of cowardice. reduction operation. pyramids and wavelets figure signal decimation the original samples are convolved with a low-pass filter before being downsampled. if however the downsampled images will be displayed directly to the user or perhaps blended with other resolutions in mip-mapping section a higher-quality filter is desired. for high downsampling rates the windowed sinc pre-filter is a good choice however for small downsampling rates e.g. r more careful filter design is required. table shows a number of commonly used r downsampling filters while fig ure shows their corresponding frequency responses. these filters include the linear filter gives a relatively poor response the binomial filter cuts off a lot of frequencies but is useful for computer vision analysis pyramids the cubic filters from the a filter has a sharper fall-off than the a filter linear binomial cubic a cubic a windowed sinc jpeg table filter coefficients for decimation. these filters are of odd length are symmetric and are normalized to have unit dc gain up to see figure for their associated frequency responses. computer vision algorithms and applications draft figure frequency response for some decimation filters. the cubic a filter has the sharpest fall-off but also a bit of ringing the wavelet analysis filters and jpeg while useful for compression have more aliasing. a cosine-windowed sinc function the filter of simoncelli and adelson is used for wavelet denoising and aliases a fair amount that the original filter coefficients are normalized to gain so they can be self-inverting the analysis filter from jpeg and marcellin please see the original papers for the full-precision values of some of these coefficients. multi-resolution representations now that we have described interpolation and decimation algorithms we can build a complete image pyramid as we mentioned before pyramids can be used to accelerate coarse-to-fine search algorithms to look for objects or patterns at different scales and to perform multi-resolution blending operations. they are also widely used in computer graphics hardware and software to perform fractional-level decimation using the mip-map which we cover in section the best known probably most widely used pyramid in computer vision is burt and adelson s laplacian pyramid. to construct the pyramid we first blur and subsample the original image by a factor of two and store this in the next level of the pyramid because adjacent levels in the pyramid are related by a sampling rate r this kind of pyramid is known as an octave pyramid. burt and adelson originally proposed a pyramids and wavelets figure a traditional image pyramid each level has half the resolution and height and hence a quarter of the pixels of its parent level. five-tap kernel of the form with b and c in practice a which results in the familiar binomial kernel b a b c c which is particularly easy to implement using shifts and adds. was important in the days when multipliers were expensive. the reason they call their resulting pyramid a gaussian pyramid is that repeated convolutions of the binomial kernel converge to a to compute the laplacian pyramid burt and adelson first interpolate a lower resolution image to obtain a reconstructed low-pass version of the original image they then subtract this low-pass version from the original to yield the band-pass laplacian image which can be stored away for further processing. the resulting pyramid has perfect reconstruction i.e. the laplacian images plus the base-level gaussian in figure are sufficient to exactly reconstruct the original image. figure shows the same computation in one dimension as a signal processing diagram which completely captures the computations being performed during the analysis and re-synthesis stages. burt and adelson also describe a variant on the laplacian pyramid where the low-pass image is taken from the original blurred image rather than the reconstructed pyramid the output of the l box directly to the subtraction in figure this variant has less then again this is true for any smoothing kernel finemediumcoarsel computer vision algorithms and applications draft figure the gaussian pyramid shown as a signal processing diagram the analysis and re-synthesis stages are shown as using similar computations. the white circles indicate zero values inserted by the upsampling operation. notice how the reconstruction filter coefficients are twice the analysis coefficients. the computation is shown as flowing down the page regardless of whether we are going from coarse to fine or vice versa. aliasing since it avoids one downsampling and upsampling round-trip but it is not selfinverting since the laplacian images are no longer adequate to reproduce the original image. as with the gaussian pyramid the term laplacian is a bit of a misnomer since their band-pass images are really differences of gaussians or dogs dogi g i g i g i. a laplacian of gaussian we saw in is actually its second derivative logi i i where is the laplacian of a function. figure shows how the differences of gaussian and laplacians of gaussian look in both space and frequency. laplacians of gaussian have elegant mathematical properties which have been widely studied in the scale-space community witkin terzopoulos and kass lindeberg nielsen florack and deriche and can be used for a variety of applications including edge detection and hildreth perona and malik stereo matching terzopoulos and kass and image enhancement florack and deriche a less widely used variant is half-octave pyramids shown in figure these were first introduced to the vision community by crowley and stern who call them difference of low-pass transforms. because of the small scale change between adja pyramids and wavelets figure the laplacian pyramid the conceptual flow of images through processing stages images are high-pass and low-pass filtered and the low-pass filtered images are processed in the next stage of the pyramid. during reconstruction the interpolated image and the filtered high-pass image are added back together. the q box indicates quantization or some other pyramid processing e.g. noise removal by coring small wavelet values to the actual computation of the high-pass filter involves first interpolating the downsampled low-pass image and then subtracting it. this results in perfect reconstruction when q is the identity. the high-pass band-pass images are typically called laplacian images while the low-pass images are called gaussian images. hl computer vision algorithms and applications draft space frequency low-pass lower-pass figure the difference of two low-pass filters results in a band-pass filter. the dashed blue lines show the close fit to a half-octave laplacian of gaussian. cent levels the authors claim that coarse-to-fine algorithms perform better. in the imageprocessing community half-octave pyramids combined with checkerboard sampling grids are known as quincunx sampling van de ville and unser in detecting multiscale features it is often common to use half-octave or even quarter-octave pyramids triggs however in this case the subsampling only occurs at every octave level i.e. the image is repeatedly blurred with wider gaussians until a full octave of resolution change has been achieved wavelets while pyramids are used extensively in computer vision applications some people use wavelet decompositions as an alternative. wavelets are filters that localize a signal in both space and frequency the gabor filter in table and are defined over a hierarchy of scales. wavelets provide a smooth way to decompose a signal into frequency components without blocking and are closely related to pyramids. wavelets were originally developed in the applied math and signal processing communities and were introduced to the computer vision community by mallat strang simoncelli and adelson rioul and vetterli chui meyer all provide nice introductions to the subject along with historical reviews while chui provides a more comprehensive review and survey of applications. sweldens describes the more recent lifting approach to wavelets that we discuss shortly. wavelets are widely used in the computer graphics community to perform multi-resolution geometric processing derose and salesin and have also been used in computer vision for similar applications pentland gortler and cohen yaou and chang lai and vemuri szeliski as well as for multi-scale oriented filtering freeman adelson et al. and denoising strela pyramids and wavelets figure multiresolution pyramids pyramid with half-octave sampling levels are colored gray for clarity. wavelet pyramid each wavelet level stores of the original pixels the horizontal vertical and mixed gradients so that the total number of wavelet coefficients and original pixels is the same. wainwright et al. since both image pyramids and wavelets decompose an image into multi-resolution descriptions that are localized in both space and frequency how do they differ? the usual answer is that traditional pyramids are overcomplete i.e. they use more pixels than the original image to represent the decomposition whereas wavelets provide a tight frame i.e. they keep the size of the decomposition the same as the image however some wavelet families are in fact overcomplete in order to provide better shiftability or steering in orientation freeman adelson et al. a better distinction therefore might be that wavelets are more orientation selective than regular band-pass pyramids. how are two-dimensional wavelets constructed? figure shows a high-level diagram of one stage of the coarse-to-fine construction pipeline alongside the complementary re-construction stage. in this diagram the high-pass filter followed by decimation keeps of the original pixels while of the low-frequency coefficients are passed on to the next stage for further analysis. in practice the filtering is usually broken down into two separable sub-stages as shown in figure the resulting three wavelet images are sometimes called the high high high low and low high images. the high low and low high images accentuate the horizontal and vertical edges and gradients while the high high image contains the less frequently occurring mixed derivatives. how are the high-pass h and low-pass l filters shown in figure chosen and how can the corresponding reconstruction filters i and f be computed? can filters be designed finemediumcoarsel computer vision algorithms and applications draft figure two-dimensional wavelet decomposition high-level diagram showing the low-pass and high-pass transforms as single boxes separable implementation which involves first performing the wavelet transform horizontally and then vertically. the i and f boxes are the interpolation and filtering boxes required to re-synthesize the image from its wavelet components. that all have finite impulse responses? this topic has been the main subject of study in the wavelet community for over two decades. the answer depends largely on the intended application e.g. whether the wavelets are being used for compression image analysis finding or denoising. simoncelli and adelson show table some good oddlength quadrature mirror filter coefficients that seem to work well in practice. since the design of wavelet filters is such a tricky art is there perhaps a better way? indeed a simpler procedure is to split the signal into its even and odd components and then perform trivially reversible filtering operations on each sequence to produce what are called lifted wavelets and sweldens gives a wonderfully understandable introduction to the lifting scheme for second-generation wavelets followed by a comprehensive review as figure demonstrates rather than first filtering the whole input sequence hl pyramids and wavelets figure one-dimensional wavelet transform usual high-pass low-pass filters followed by odd and even downsampling lifted version which first selects the odd and even subsequences and then applies a low-pass prediction stage l and a high-pass correction stage c in an easily reversible manner. with high-pass and low-pass filters and then keeping the odd and even sub-sequences the lifting scheme first splits the sequence into its even and odd sub-components. filtering the even sequence with a low-pass filter l and subtracting the result from the even sequence is trivially reversible simply perform the same filtering and then add the result back in. furthermore this operation can be performed in place resulting in significant space savings. the same applies to filtering the even sequence with the correction filter c which is used to ensure that the even sequence is low-pass. a series of such lifting steps can be used to create more complex filter responses with low computational cost and guaranteed reversibility. this process can perhaps be more easily understood by considering the signal processing diagram in figure during analysis the average of the even values is subtracted from the odd value to obtain a high-pass wavelet coefficient. however the even samples still contain an aliased sample of the low-frequency signal. to compensate for this a small amount of the high-pass wavelet is added back to the even sequence so that it is properly low-pass filtered. is easy to show that the effective low-pass filter is which is in hl computer vision algorithms and applications draft figure lifted transform shown as a signal processing diagram the analysis stage first predicts the odd value from its even neighbors stores the difference wavelet and then compensates the coarser even value by adding in a fraction of the wavelet. the synthesis stage simply reverses the flow of computation and the signs of some of the filters and operations. the light blue lines show what happens if we use four taps for the prediction and correction instead of just two. deed a low-pass filter. during synthesis the same operations are reversed with a judicious change in sign. of course we need not restrict ourselves to two-tap filters. figure shows as light blue arrows additional filter coefficients that could optionally be added to the lifting scheme without affecting its reversibility. in fact the low-pass and high-pass filtering operations can be interchanged e.g. we could use a five-tap cubic low-pass filter on the odd sequence center value first followed by a four-tap cubic low-pass predictor to estimate the wavelet although i have not seen this scheme written down. lifted wavelets are called second-generation wavelets because they can easily adapt to non-regular sampling topologies e.g. those that arise in computer graphics applications such as multi-resolution surface manipulation oder and sweldens it also turns out that lifted weighted wavelets i.e. wavelets whose coefficients adapt to the underlying problem being solved can be extremely effective for low-level image manipulation tasks and also for preconditioning the kinds of sparse linear systems that arise in the optimizationbased approaches to vision algorithms that we discuss in section an alternative to the widely used separable approach to wavelet construction which decomposes each level into horizontal vertical and cross sub-bands is to use a representation that is more rotationally symmetric and orientationally selective and also avoids the aliasing inherent in sampling signals below their nyquist simoncelli freeman adelson et al. introduce such a representation which they call a pyramidal radial frequency such aliasing can often be seen as the signal content moving between bands as the original signal is slowly shifted. pyramids and wavelets figure steerable shiftable multiscale transforms freeman adelson et al. ieee radial multi-scale frequency domain decomposition original image a set of four steerable filters the radial multi-scale wavelet decomposition. implementation of shiftable multi-scale transforms or more succinctly steerable pyramids. their representation is not only overcomplete eliminates the aliasing problem but is also orientationally selective and has identical analysis and synthesis basis functions i.e. it is self-inverting just like regular wavelets. as a result this makes steerable pyramids a much more useful basis for the structural analysis and matching tasks commonly used in computer vision. figure shows how such a decomposition looks in frequency space. instead of recursively dividing the frequency domain into squares which results in checkerboard high frequencies radial arcs are used instead. figure illustrates the resulting pyramid sub-bands. even through the representation is overcomplete i.e. there are more wavelet coefficients than input pixels the additional frequency and orientation selectivity makes this representation preferable for tasks such as texture analysis and synthesis and simoncelli and image denoising strela wainwright et al. lyu and simoncelli computer vision algorithms and applications draft figure laplacian pyramid blending and adelson acm original image of apple original image of orange regular splice pyramid blend. application image blending one of the most engaging and fun applications of the laplacian pyramid presented in section is the creation of blended composite images as shown in figure and adelson while splicing the apple and orange images together along the midline produces a noticeable cut splining them together burt and adelson called their procedure creates a beautiful illusion of a truly hybrid fruit. the key to their approach is that the low-frequency color variations between the red apple and the orange are smoothly blended while the higher-frequency textures on each fruit are blended more quickly to avoid ghosting effects when two textures are overlaid. to create the blended image each source image is first decomposed into its own laplacian pyramid left and middle columns. each band is then multiplied by a smooth weighting function whose extent is proportional to the pyramid level. the simplest and most general way to create these weights is to take a binary mask image and to construct a gaussian pyramid from this mask. each laplacian pyramid image is then pyramids and wavelets figure laplacian pyramid blending details and adelson acm. the first three rows show the high medium and low frequency parts of the laplacian pyramid from levels and the left and middle columns show the original apple and orange images weighted by the smooth interpolation functions while the right column shows the averaged contributions. computer vision algorithms and applications draft figure laplacian pyramid blend of two images of arbitrary shape and adelson acm first input image second input image region mask blended image. multiplied by its corresponding gaussian mask and the sum of these two weighted pyramids is then used to construct the final image right column. figure shows that this process can be applied to arbitrary mask images with surprising results. it is also straightforward to extend the pyramid blend to an arbitrary number of images whose pixel provenance is indicated by an integer-valued label image exercise this is particularly useful in image stitching and compositing applications where the exposures may vary between different images as described in section geometric transformations in the previous sections we saw how interpolation and decimation could be used to change the resolution of an image. in this section we look at how to perform more general transformations such as image rotations or general warps. in contrast to the point processes we saw in section where the function applied to an image transforms the range of the image gx hfx geometric transformations figure image warping involves modifying the domain of an image function rather than its range. figure basic set of geometric image transformations. here we look at functions that transform the domain gx fhx figure we begin by studying the global parametric transformation first introduced in section a transformation is called parametric because it is controlled by a small number of parameters. we then turn our attention to more local general deformations such as those defined on meshes finally we show how image warps can be combined with cross-dissolves to create interesting morphs animations in section for readers interested in more details on these topics there is an excellent survey by heckbert as well as very accessible textbooks by wolberg gomes darsa costa et al. and akenine-m oller and haines note that heckbert s survey is on texture mapping which is how the computer graphics community refers to the topic of warping images onto surfaces. parametric transformations parametric transformations apply a global deformation to an image where the behavior of the transformation is controlled by a small number of parameters. figure shows a few ex fxhffghghhgxfxgxyxsimilarityeuclideanaffineprojectivetranslation computer vision algorithms and applications draft transformation matrix dof preserves icon translation rigid similarity affine projective i t r t sr t a h orientation lengths angles ss ss s s straight lines parallelism table hierarchy of coordinate transformations. each transformation also preserves the properties listed in the rows below it i.e. similarity preserves not only angles but also parallelism and straight lines. the matrices are extended with a third row to form a full matrix for homogeneous coordinate transformations. amples of such transformations which are based on the geometric transformations shown in figure the formulas for these transformations were originally given in table and are reproduced here in table for ease of reference. in general given a transformation specified by a formula hx and a source image fx how do we compute the values of the pixels in the new image gx as given in think about this for a minute before proceeding and see if you can figure it out. if you are like most people you will come up with an algorithm that looks something like algorithm this process is called forward warping or forward mapping and is shown in figure can you think of any problems with this approach? procedure forwardwarpf h out g for every pixel x in fx compute the destination location hx. copy the pixel fx to algorithm forward warping algorithm for transforming an image fx into an image through the parametric transform hx. geometric transformations figure forward warping algorithm a pixel fx is copied to its corresponding location hx in image detail of the source and destination pixel locations. in fact this approach suffers from several limitations. the process of copying a pixel fx to a location in g is not well defined when has a non-integer value. what do we do in such a case? what would you do? you can round the value of to the nearest integer coordinate and copy the pixel there but the resulting image has severe aliasing and pixels that jump around a lot when animating the transformation. you can also distribute the value among its four nearest neighbors in a weighted fashion keeping track of the per-pixel weights and normalizing at the end. this technique is called splatting and is sometimes used for volume rendering in the graphics community and whitted levoy westover rusinkiewicz and levoy unfortunately it suffers from both moderate amounts of aliasing and a fair amount of blur of high-resolution detail. the second major problem with forward warping is the appearance of cracks and holes especially when magnifying an image. filling such holes with their nearby neighbors can lead to further aliasing and blurring. what can we do instead? a preferable solution is to use inverse warping where each pixel in the destination image is sampled from the original image fx how does this differ from the forward warping algorithm? for one thing since is defined for all pixels in we no longer have holes. more importantly resampling an image at non-integer locations is a well-studied problem image interpolation see section and high-quality filters that control aliasing can be used. where does the function come from? quite often it can simply be computed as the inverse of hx. in fact all of the parametric transforms listed in table have closed form solutions for the inverse transform simply take the inverse of the matrix specifying the transform. in other cases it is preferable to formulate the problem of image warping as that of resampling a source image fx given a mapping x from destination pixels to source pixels x. for example in optical flow we estimate the flow field as the fxgx x x computer vision algorithms and applications draft procedure inversewarpf h out g for every pixel in compute the source location x resample fx at location x and copy to algorithm inverse warping algorithm for creating an image from an image fx using the parametric transform hx. figure inverse warping algorithm a pixel is sampled from its corresponding location x in image fx detail of the source and destination pixel locations. location of the source pixel which produced the current pixel whose flow is being estimated as opposed to computing the destination pixel to which it is going. similarly when correcting for radial distortion we calibrate the lens by computing for each pixel in the final image the corresponding pixel location in the original image. what kinds of interpolation filter are suitable for the resampling process? any of the filters we studied in section can be used including nearest neighbor bilinear bicubic and windowed sinc functions. while bilinear is often used for speed inside the inner loop of a patch-tracking algorithm see section bicubic and windowed sinc are preferable where visual quality is important. to compute the value of fx at a non-integer location x we simply apply our usual fir resampling filter gx y fk lhx k y l where y are the sub-pixel coordinate values and hx y is some interpolating or smoothing kernel. recall from section that when decimation is being performed the smoothing kernel is stretched and re-scaled according to the downsampling rate r. unfortunately for a general image transformation the resampling rate r is not well defined. consider a transformation that stretches the x dimensions while squashing fxgx xhx fxgx xhx geometric transformations figure anisotropic texture filtering jacobian of transform a and the induced horizontal and vertical resampling rates elliptical footprint of an ewa smoothing kernel anisotropic filtering using multiple samples along the major axis. image pixels lie at line intersections. the y dimensions. the resampling kernel should be performing regular interpolation along the x dimension and smoothing anti-alias the blurred image in the y direction. this gets even more complicated for the case of general affine or perspective transforms. what can we do? fortunately fourier analysis can help. the two-dimensional general ization of the one-dimensional domain scaling law given in table is gax t f. for all of the transforms in table except perspective the matrix a is already defined. for perspective transformations the matrix a is the linearized derivative of the perspective transformation i.e. the local affine approximation to the stretching induced by the projection wolberg gomes darsa costa et al. akeninem oller and haines to prevent aliasing we need to pre-filter the image fx with a filter whose frequency response is the projection of the final desired spectrum through the a t transform winder and uyttendaele in general non-zoom transforms this filter is nonseparable and hence is very slow to compute. therefore a number of approximations to this filter are used in practice include mip-mapping elliptically weighted gaussian averaging and anisotropic filtering oller and haines mip-mapping mip-mapping was first proposed by williams as a means to rapidly pre-filter images being used for texture mapping in computer graphics. a is a standard image the term mip stands for multi in parvo meaning many in one xyx y xyx y xyx y ay yay xax xax yabcmajor axisminor axis computer vision algorithms and applications draft pyramid where each level is pre-filtered with a high-quality filter rather than a poorer quality approximation such as burt and adelson s five-tap binomial. to resample an image from a mip-map a scalar estimate of the resampling rate r is first computed. for example r can be the maximum of the absolute values in a suppresses aliasing or it can be the minimum reduces blurring. akenine-m oller and haines discuss these issues in more detail. once a resampling rate has been specified a fractional pyramid level is computed using the base logarithm l r. one simple solution is to resample the texture from the next higher or lower pyramid level depending on whether it is preferable to reduce aliasing or blur. a better solution is to resample both images and blend them linearly using the fractional component of l. since most mip-map implementations use bilinear resampling within each level this approach is usually called trilinear mip-mapping. computer graphics rendering apis such as opengl and have parameters that can be used to select which variant of mip-mapping of the sampling rate r computation should be used depending on the desired tradeoff between speed and quality. exercise has you examine some of these tradeoffs in more detail. elliptical weighted average the elliptical weighted average filter invented by greene and heckbert is based on the observation that the affine mapping x defines a skewed two-dimensional coordinate system in the vicinity of each source pixel x for every destination pixel the ellipsoidal projection of a small pixel grid in onto x is computed this is then used to filter the source image gx with a gaussian whose inverse covariance matrix is this ellipsoid. despite its reputation as a high-quality filter oller and haines we have found in our work winder and uyttendaele that because a gaussian kernel is used the technique suffers simultaneously from both blurring and aliasing compared to higher-quality filters. the ewa is also quite slow although faster variants based on mipmapping have been proposed winder and uyttendaele provide some additional references. anisotropic filtering an alternative approach to filtering oriented textures which is sometimes implemented in graphics hardware is to use anisotropic filtering akenine-m oller and haines in this approach several samples at different resolutions levels in the mip-map are combined along the major axis of the ewa gaussian geometric transformations figure one-dimensional signal resampling winder and uyttendaele original sampled signal fi interpolated signal warped signal filtered signal sampled signal the corresponding spectra are shown below the signals with the aliased portions shown in red. multi-pass transforms the optimal approach to warping images without excessive blurring or aliasing is to adaptively pre-filter the source image at each pixel using an ideal low-pass filter i.e. an oriented skewed sinc or low-order cubic approximation figure shows how this works in one dimension. the signal is first interpolated to a continuous waveform low-pass filtered to below the new nyquist rate and then re-sampled to the final desired resolution. in practice the interpolation and decimation steps are concatenated into a single polyphase digital filtering operation winder and uyttendaele for parametric transforms the oriented two-dimensional filtering and resampling operations can be approximated using a series of one-dimensional resampling and shearing transforms and smith heckbert wolberg gomes darsa costa et al. szeliski winder and uyttendaele the advantage of using a series of onedimensional transforms is that they are much more efficient terms of basic arithmetic operations than large non-separable two-dimensional filter kernels. in order to prevent aliasing however it may be necessary to upsample in the opposite direction before applying a shearing transformation winder and uyttendaele figure shows this process for a rotation where a vertical upsampling stage is added before the horizontal shearing upsampling stage. the upper image shows the appearance of the letter being rotated while the lower image shows its corresponding fourier transform. computer vision algorithms and applications draft figure four-pass rotation winder and uyttendaele original pixel grid image and its fourier transform vertical upsampling horizontal shear and upsampling vertical shear and downsampling horizontal downsampling. the general affine case looks similar except that the first two stages perform general resampling. mesh-based warping while parametric transforms specified by a small number of global parameters have many uses local deformations with more degrees of freedom are often required. consider for example changing the appearance of a face from a frown to a smile what is needed in this case is to curve the corners of the mouth upwards while leaving the rest of the face to perform such a transformation different amounts of motion are required in different parts of the image. figure shows some of the commonly used approaches. the first approach shown in figure b is to specify a sparse set of corresponding points. the displacement of these points can then be interpolated to a dense displacement field using a variety of techniques one possibility is to triangulate the set of points in one image berg cheong van kreveld et al. litwinowicz and williams buck finkelstein jacobs et al. and to use an affine motion model specified by the three triangle vertices inside each triangle. if the destination rowland and perrett pighin hecker lischinski et al. blanz and vetter leyvand cohen or dror et al. show more sophisticated examples of changing facial expression and appearance. vertical shear downsampleabcdvertical upsamplehorizontal shear upsamplehorizontal downsamplee geometric transformations figure image warping alternatives darsa costa et al. morgan kaufmann sparse control points deformation grid denser set of control point correspondences oriented line correspondences uniform quadrilateral grid. image is triangulated according to the new vertex locations an inverse warping algorithm can be used. if the source image is triangulated and used as a texture map computer graphics rendering algorithms can be used to draw the new image care must be taken along triangle edges to avoid potential aliasing. alternative methods for interpolating a sparse set of displacements include moving nearby quadrilateral mesh vertices as shown in figure using variational minimizing interpolants such as regularization and williams see section or using locally weighted basis function combinations of displacements for additional scattered data interpolation techniques. if quadrilateral meshes are used it may be desirable to interpolate displacements down to individual pixel values using a smooth interpolant such as a quadratic b-spline lee wolberg chwa et al. in some cases e.g. if a dense depth map has been estimated for an image gortler he et al. we only know the forward displacement for each pixel. as mentioned before drawing source pixels at their destination location i.e. forward warping suffers from several potential problems including aliasing and the appearance of small cracks. an alternative technique in this case is to forward warp the displacement field depth map to note that the block-based motion models used by many video compression standards gall can be thought of as a displacement field. computer vision algorithms and applications draft figure line-based image warping and neely acm distance computation and position transfer rendering algorithm two intermediate warps used for morphing. its new location fill small holes in the resulting map and then use inverse warping to perform the resampling gortler he et al. the reason that this generally works better than forward warping is that displacement fields tend to be much smoother than images so the aliasing introduced during the forward warping of the displacement field is much less noticeable. a second approach to specifying displacements for local deformations is to use corresponding oriented line segments and neely as shown in figures and pixels along each line segment are transferred from source to destination exactly as specified and other pixels are warped using a smooth interpolation of these displacements. each line segment correspondence specifies a translation rotation and scaling i.e. a similarity transform for pixels in its vicinity as shown in figure line segments influence the overall displacement of the image using a weighting function that depends on the minimum distance to the line segment in figure if u else the shorter of the two distances to p and q. for each pixel x the target location for each line correspondence is computed along with a weight that depends on the distance and the line segment length the weighted average of all target locations then becomes the final destination location. note that while beier and neely describe this algorithm as a forward warp an equivalent algorithm can be written by sequencing through the destination pixels. the resulting warps are not identical because line lengths or distances to lines may be different. exercise has you implement the beier neely warp and compare it to a number of other local deformation methods. yet another way of specifying correspondences in order to create image warps is to use snakes combined with b-splines wolberg chwa et al. this technique is used in apple s shake software and is popular in the medical imaging community. geometric transformations figure image morphing darsa costa et al. morgan kaufmann. top row if the two images are just blended visible ghosting results. bottom row both images are first warped to the same intermediate location halfway towards the other image and the resulting warped images are then blended resulting in a seamless morph. one final possibility for specifying displacement fields is to use a mesh specifically adapted to the underlying image content as shown in figure specifying such meshes by hand can involve a fair amount of work gomes darsa costa et al. describe an interactive system for doing this. once the two meshes have been specified intermediate warps can be generated using linear interpolation and the displacements at mesh nodes can be interpolated using splines. application feature-based morphing while warps can be used to change the appearance of or to animate a single image even more powerful effects can be obtained by warping and blending two or more images using a process now commonly known as morphing and neely lee wolberg chwa et al. gomes darsa costa et al. figure shows the essence of image morphing. instead of simply cross-dissolving between two images which leads to ghosting as shown in the top row each image is warped toward the other image before blending as shown in the bottom row. if the correspondences have been set up well any of the techniques shown in figure corresponding features are aligned and no ghosting results. the above process is repeated for each intermediate frame being generated during a computer vision algorithms and applications draft morph using different blends amounts of deformation at each interval. let t be the time parameter that describes the sequence of interpolated frames. the weighting functions for the two warped images in the blend go as t and t. conversely the amount of motion that image undergoes at time t is t of the total amount of motion that is specified by the correspondences. however some care must be taken in defining what it means to partially warp an image towards a destination especially if the desired motion is far from linear gao wang et al. exercise has you implement a morphing algorithm and test it out under such challenging conditions. global optimization so far in this chapter we have covered a large number of image processing operators that take as input one or more images and produce some filtered or transformed version of these images. in many applications it is more useful to first formulate the goals of the desired transformation using some optimization criterion and then find or infer the solution that best meets this criterion. in this final section we present two different closely related variants on this idea. the first which is often called regularization or variational methods constructs a continuous global energy function that describes the desired characteristics of the solution and then finds a minimum energy solution using sparse linear systems or related iterative techniques. the second formulates the problem using bayesian statistics modeling both the noisy measurement process that produced the input images as well as prior assumptions about the solution space which are often encoded using a markov random field examples of such problems include surface interpolation from scattered data image denoising and the restoration of missing regions and the segmentation of images into foreground and background regions regularization the theory of regularization was first developed by statisticians trying to fit models to data that severely underconstrained the solution space and arsenin engl hanke and neubauer consider for example finding a smooth surface that passes through near a set of measured data points such a problem is described as illposed because many possible surfaces can fit this data. since small changes in the input can sometimes lead to large changes in the fit if we use polynomial interpolation such problems are also often ill-conditioned. since we are trying to recover the unknown function fx y from which the data point dxi yi were sampled such problems are also often called global optimization figure a simple surface interpolation problem nine data points of various height scattered on a grid second-order controlled-continuity thin-plate spline interpolator with a tear along its left edge and a crease along its right springer. inverse problems. many computer vision tasks can be viewed as inverse problems since we are trying to recover a full description of the world from a limited set of images. in order to quantify what it means to find a smooth solution we can define a norm on the solution space. for one-dimensional functions fx we can integrate the squared first derivative of the function or perhaps integrate the squared second derivative f f xx dx xxx dx. we use subscripts to denote differentiation. such energy measures are examples of functionals which are operators that map functions to scalar values. they are also often called variational methods because they measure the variation in a function. in two dimensions for images flow fields or surfaces the corresponding smooth ness functionals are and where the mixed f f xx y f y y dx dy fx dx dy xxx y xyx y f yyx y dx dy xy term is needed to make the measure rotationally invariant the first derivative norm is often called the membrane since interpolating a set of data points using this measure results in a tent-like structure. fact this formula is a smalldeflection approximation to the surface area which is what soap bubbles minimize. the for a problem like noise removal a continuous version of this measure can be used yi ed ed y dx dx dy. computer vision algorithms and applications draft second-order norm is called the thin-plate spline since it approximates the behavior of thin plates flexible steel under small deformations. a blend of the two is called the thinplate spline under tension versions of these formulas where each derivative term is multiplied by a local weighting function are called controlled-continuity splines figure shows a simple example of a controlled-continuity interpolator fit to nine scattered data points. in practice it is more common to find first-order smoothness terms used with images and flow fields and second-order smoothness associated with surfaces in addition to the smoothness term regularization also requires a data term data penalty. for scattered data interpolation the data term measures the distance between the function fx y and a set of data points di dxi yi to obtain a global energy that can be minimized the two energy terms are usually added together e ed es where es is the smoothness penalty or some weighted blend and is the regularization parameter which controls how smooth the solution should be. in order to find the minimum of this continuous problem the function fx y is usually first discretized on a regular the most principled way to perform this discretization is to use finite element analysis i.e. to approximate the function with a piecewise continuous spline and then perform the analytic integration fortunately for both the first-order and second-order smoothness functionals the judicious selection of appropriate finite elements results in particularly simple discrete forms the corresponding discrete smoothness energy functions become sxi jfi j fi j gxi syi jfi j fi j gyi and h cxi jfi j j fi the alternative of using kernel basis functions centered on the data points and kender nielson is discussed in more detail in section global optimization jfi j fi j fi j fi cyi jfi j j fi j where h is the size of the finite element grid. the h factor is only important if the energy is being discretized at a variety of resolutions as in coarse-to-fine or multigrid techniques. the optional smoothness weights sxi j and syi j control the location of horizontal and vertical tears weaknesses in the surface. for other problems such as colorization lischinski and weiss and interactive tone mapping farbman uyttendaele et al. they control the smoothness in the interpolated chroma or exposure field and are often set inversely proportional to the local luminance gradient strength. for second-order problems the crease variables cxi j cmi j and cyi j control the locations of creases in the surface szeliski the data values gxi j and gyi j are gradient data terms used by algorithms such as photometric stereo hdr tone mapping lischinski and werman poisson blending erez gangnet and blake and gradient-domain blending zomet peleg et al. they are set to zero when just discretizing the conventional first-order smoothness functional the two-dimensional discrete data energy is written as ed wi jfi j di where the local weights wi j control how strongly the data constraint is enforced. these values are set to zero where there is no data and can be set to the inverse variance of the data measurements when there is data discussed by szeliski and in section the total energy of the discretized problem can now be written as a quadratic form e ed es xt ax b c where x fm n is called the state the sparse symmetric positive-definite matrix a is called the hessian since it encodes the second derivative of the energy for the one-dimensional first-order problem a is tridiagonal for the two-dimensional first-order problem it is multi-banded with five nonzero entries per row. we call b the weighted data vector. minimizing the above quadratic form is equivalent to solving the sparse linear system we use x instead of f because this is the more common form in the numerical analysis literature and ax b van loan in numerical analysis a is called the coefficient matrix in finite element analysis it is called the stiffness matrix. computer vision algorithms and applications draft figure graphical model interpretation of first-order regularization. the white circles are the unknowns fi j while the dark circles are the input data di j. in the resistive grid interpretation the d and f values encode input and output voltages and the black squares denote resistors whose conductance is set to sxi j syi j and wi j. in the spring-mass system analogy the circles denote elevations and the black squares denote springs. the same graphical model can be used to depict a first-order markov random field which can be done using a variety of sparse matrix techniques such as multigrid henson and mccormick and hierarchical preconditioners as described in appendix while regularization was first introduced to the vision community by poggio torre and koch and terzopoulos for problems such as surface interpolation it was quickly adopted by other vision researchers for such varied problems as edge detection optical flow and shape from shading torre and koch horn and brooks terzopoulos bertero poggio and torre brox bruhn papenberg et al. poggio torre and koch also showed how the discrete energy defined by equations could be implemented in a resistive grid as shown in figure in computational photography regularization and its variants are commonly used to solve problems such as high-dynamic range tone mapping lischinski and werman lischinski farbman uyttendaele et al. poisson and gradient-domain blending erez gangnet and blake levin zomet peleg et al. agarwala dontcheva agrawala et al. colorization lischinski and weiss and natural image matting lischinski and weiss robust regularization while regularization is most commonly formulated using quadratic norms with the squared derivatives in and squared differences in it can global optimization also be formulated using non-quadratic robust penalty functions for example can be generalized to sxi j j fi j syi j j fi j where is some monotonically increasing penalty function. for example the family of norms is called p-norms. when p the resulting smoothness terms become more piecewise continuous than totally smooth which can better model the discontinuous nature of images flow fields and surfaces. an early example of robust regularization is the graduated non-convexity algorithm introduced by blake and zisserman here the norms on the data and derivatives are clamped to a maximum value v because the resulting problem is highly non-convex has many local minima a continuation method is proposed where a quadratic norm is convex is gradually replaced by the non-convex robust norm and georg the same time terzopoulos was also using continuation to infer the tear and crease variables in his surface interpolation problems. today it is more common to use the norm which is often called total variation osher and shen tschumperl e and deriche tschumperl e kaftory schechner and zeevi other norms for which the influence more quickly decays to zero are presented by black and rangarajan black sapiro marimont et al. and discussed in appendix even more recently hyper-laplacian norms with p have gained popularity based on the observation that the log-likelihood distribution of image derivatives follows a p slope and is therefore a hyper-laplacian distribution levin and weiss weiss and freeman krishnan and fergus such norms have an even stronger tendency to prefer large discontinuities over small ones. see the related discussion in section while least squares regularized problems using norms can be solved using linear systems other p-norms require different iterative techniques such as iteratively reweighted least squares levenberg marquardt or alternation between local non-linear subproblems and global quadratic regularization and fergus such techniques are discussed in section and appendices and computer vision algorithms and applications draft markov random fields as we have just seen regularization which involves the minimization of energy functionals defined over continuous functions can be used to formulate and solve a variety of low-level computer vision problems. an alternative technique is to formulate a bayesian model which separately models the noisy image formation process as well as assuming a statistical prior model over the solution space. in this section we look at priors based on markov random fields whose log-likelihood can be described using local neighborhood interaction penalty terms and snell geman and geman marroquin mitter and poggio li szeliski zabih scharstein et al. the use of bayesian modeling has several potential advantages over regularization also appendix b. the ability to model measurement processes statistically enables us to extract the maximum information possible from each measurement rather than just guessing what weighting to give the data. similarly the parameters of the prior distribution can often be learned by observing samples from the class we are modeling and black tappen li and huttenlocher furthermore because our model is probabilistic it is possible to estimate principle complete probability distributions over the unknowns being recovered and in particular to model the uncertainty in the solution which can be useful in latter processing stages. finally markov random field models can be defined over discrete variables such as image labels the variables have no proper ordering for which regularization does not apply. recall from in section see appendix that according to bayes rule the posterior distribution for a given set of measurements y pyx combined with a prior px over the unknowns x is given by pxy pyxpx py where py pyxpx is a normalizing constant used to make the pxy distribution proper to taking the negative logarithm of both sides of we get log pxy log pyx log px c which is the negative posterior log likelihood. to find the most likely a posteriori or map solution x given some measurements y we simply minimize this negative log likelihood which can also be thought of as an energy ex y edx y epx. drop the constant c because its value does not matter during energy minimization. the first term edx y is the data energy or data penalty it measures the negative log likelihood global optimization that the data were observed given the unknown state x. the second term epx is the prior energy it plays a role analogous to the smoothness energy in regularization. note that the map estimate may not always be desirable since it selects the peak in the posterior distribution rather than some more stable statistic see the discussion in appendix and by levin weiss durand et al. for image processing applications the unknowns x are the set of output pixels and the data are the simplest case the input pixels x fm n y dm n as shown in figure for a markov random field the probability px is a gibbs or boltzmann distribution whose negative log likelihood to the hammersley clifford theorem can be written as a sum of pairwise interaction potentials epx n vijklfi j fk l where n j denotes the neighbors of pixel j. in fact the general version of the theorem says that the energy may have to be evaluated over a larger set of cliques which depend on the order of the markov random field and snell geman and geman bishop kohli ladick y and torr kohli kumar and torr the most commonly used neighborhood in markov random field modeling is the neighborhood where each pixel in the field fi j interacts only with its immediate neighbors. the model in figure which we previously used in figure to illustrate the discrete version of first-order regularization shows an mrf. the sxi j and syi j black boxes denote arbitrary interaction potentials between adjacent nodes in the random field and the wi j denote the data penalty functions. these square nodes can also be interpreted as factors in a factor graph version of the graphical model which is another name for interaction potentials. speaking the factors are probability functions whose product is the posterior distribution. as we will see in there is a close relationship between these interaction potentials and the discretized versions of regularized image restoration problems. thus to a first approximation we can view energy minimization being performed when solving a regularized problem and the maximum a posteriori inference being performed in an mrf as equivalent. while neighborhoods are most commonly used in some applications even higher order neighborhoods perform better at tasks such as image segmentation because computer vision algorithms and applications draft figure graphical model for an neighborhood markov random field. blue edges are added for an neighborhood. the white circles are the unknowns fi j while the dark circles are the input data di j. the sxi j and syi j black boxes denote arbitrary interaction potentials between adjacent nodes in the random field and the wi j denote the data penalty functions. the same graphical model can be used to depict a discrete version of a first-order regularization problem they can better model discontinuities at different orientations and kolmogorov rother kohli feng et al. kohli ladick y and torr kohli kumar and torr binary mrfs the simplest possible example of a markov random field is a binary field. examples of such fields include and white scanned document images as well as images segmented into foreground and background regions. to denoise a scanned image we set the data penalty to reflect the agreement between the scanned and final images edi j w j di j and the smoothness penalty to reflect the agreement between neighboring pixels epi j exi j eyi j s j fi j s j fi j once we have formulated the energy how do we minimize it? the simplest approach is to perform gradient descent flipping one state at a time if it produces a lower energy. this approach is known as contextual classification and f oglein iterated conditional modes or highest confidence first and brown if the pixel with the largest energy decrease is selected first. unfortunately these downhill methods tend to get easily stuck in local minima. an alternative approach is to add some randomness to the process which is known as stochastic global optimization gradient descent rosenbluth rosenbluth et al. geman and geman when the amount of noise is decreased over time this technique is known as simulated annealing gelatt and vecchi carnevali coletti and patarnello wolberg and pavlidis swendsen and wang and was first popularized in computer vision by geman and geman and later applied to stereo matching by barnard among others. even this technique however does not perform that well veksler and zabih for binary images a much better technique introduced to the computer vision community by boykov veksler and zabih is to re-formulate the energy minimization as a max-flowmin-cut graph optimization problem porteous and seheult this technique has informally come to be known as graph cuts in the computer vision community and kolmogorov for simple energy functions e.g. those where the penalty for non-identical neighboring pixels is a constant this algorithm is guaranteed to produce the global minimum. kolmogorov and zabih formally characterize the class of binary energy potentials conditions for which these results hold while newer work by komodakis tziritas and paragios and rother kolmogorov lempitsky et al. provide good algorithms for the cases when they do not. in addition to the above mentioned techniques a number of other optimization approaches have been developed for mrf energy minimization such as belief propagation and dynamic programming one-dimensional problems. these are discussed in more detail in appendix as well as the comparative survey paper by szeliski zabih scharstein et al. ordinal-valued mrfs in addition to binary images markov random fields can be applied to ordinal-valued labels such as grayscale images or depth maps. the term ordinal indicates that the labels have an implied ordering e.g. that higher values are lighter pixels. in the next section we look at unordered labels such as source image labels for image compositing. in many cases it is common to extend the binary data and smoothness prior terms as edi j wi j dfi j di j and epi j sxi j pfi j fi j syi j pfi j fi j which are robust generalizations of the quadratic penalty terms and first introduced in as before the wi j sxi j and syi j weights can be used to locally control the data weighting and the horizontal and vertical smoothness. instead of computer vision algorithms and applications draft figure grayscale image denoising and inpainting original image image corrupted by noise and with missing data bar image restored using loopy belief propagation image restored using expansion move graph cuts. images are from httpvision.middlebury.edumrfresults zabih scharstein et al. using a quadratic penalty however a general monotonically increasing penalty function is used. functions can be used for the data and smoothness terms. for example p can be a hyper-laplacian penalty pd p which better encodes the distribution of gradients edges in an image than either a quadratic or linear variation levin and weiss use such a penalty to separate a transmitted and reflected image by encouraging gradients to lie in one or the other image but not both. more recently levin fergus durand et al. use the hyper-laplacian as a prior for image deconvolution and krishnan and fergus develop a faster algorithm for solving such problems. for the data penalty d can be quadratic model gaussian noise or the log of a contaminated gaussian when p is a quadratic function the resulting markov random field is called a gaussian markov random field and its minimum can be found by sparse linear system solving when the weighting functions are uniform the gmrf becomes a special case of wiener filtering allowing the weighting functions to depend on the input image special kind of conditional random field which we describe below enables quite sophisticated image processing algorithms to be performed including colorization lischinski and weiss interactive tone mapping farbman uyttendaele et al. natural image matting lischinski and weiss and image restoration liu freeman et al. note that unlike a quadratic penalty the sum of the horizontal and vertical derivative p-norms is not rotationally invariant. a better approach may be to locally estimate the gradient direction and to impose different norms on the perpendicular and parallel components which roth and black call a steerable random field. global optimization standard move initial labeling figure multi-level graph optimization from veksler and zabih ieee initial problem configuration the standard move only changes one pixel the optimally exchanges all and pixels the move optimally selects among current pixel values and the label. when d or p are non-quadratic functions gradient descent techniques such as nonlinear least squares or iteratively re-weighted least squares can sometimes be used however if the search space has lots of local minima as is the case for stereo matching boykov veksler and zabih more sophisticated techniques are required. the extension of graph cut techniques to multi-valued problems was first proposed by boykov veksler and zabih in their paper they develop two different algorithms called the swap move and the expansion move which iterate among a series of binary labeling sub-problems to find a good solution note that a global solution is generally not achievable as the problem is provably np-hard for general energy functions. because both these algorithms use a binary mrf optimization inside their inner loop they are subject to the kind of constraints on the energy functions that occur in the binary labeling case and zabih appendix discusses these algorithms in more detail along with some more recently developed approaches to this problem. another mrf inference technique is belief propagation while belief propagation was originally developed for inference over trees where it is exact it has more recently been applied to graphs with loops such as markov random fields pasztor and carmichael yedidia freeman and weiss in fact some of the better performing stereo-matching algorithms use loopy belief propagation to perform their inference zheng and shum lbp is discussed in more detail in appendix as well as the comparative survey paper on mrf optimization zabih scharstein et al. figure shows an example of image denoising and inpainting filling using a non-quadratic energy function mrf. the original image has been corrupted by noise and a portion of the data has been removed black bar. in this case the loopy computer vision algorithms and applications draft figure graphical model for a markov random field with a more complex measurement model. the additional colored edges show how combinations of unknown values in a sharp image produce the measured values noisy blurred image. the resulting graphical model is still a classic mrf and is just as easy to sample from but some inference algorithms those based on graph cuts may not be applicable because of the increased network complexity since state changes during the inference become more entangled and the posterior mrf has much larger cliques. belief propagation algorithm computes a slightly lower energy and also a smoother image than the alpha-expansion graph cut algorithm. of course the above formula for the smoothness term epi j just shows the simplest case. in more recent work roth and black propose a field of experts model which sums up a large number of exponentiated local filter outputs to arrive at the smoothness penalty. weiss and freeman analyze this approach and compare it to the simpler hyper-laplacian model of natural image statistics. lyu and simoncelli use gaussian scale mixtures to construct an inhomogeneous multi-scale mrf with one exponential gmrf modulating the variance of another gaussian mrf. it is also possible to extend the measurement model to make the sampled input pixels correspond to blends of unknown image pixels as in figure this is the commonly occurring case when trying to de-blur an image. while this kind of a model is still a traditional generative markov random field finding an optimal solution can be difficult because the clique sizes get larger. in such situations gradient descent techniques such as iteratively reweighted least squares can be used zitnick szeliski et al. exercise has you explore some of these issues. global optimization figure an unordered label mrf dontcheva agrawala et al. acm strokes in each of the source images on the left are used as constraints on an mrf optimization which is solved using graph cuts. the resulting multi-valued label field is shown as a color overlay in the middle image and the final composite is shown on the right. unordered labels another case with multi-valued labels where markov random fields are often applied are unordered labels i.e. labels where there is no semantic meaning to the numerical difference between the values of two labels. for example if we are classifying terrain from aerial imagery it makes no sense to take the numeric difference between the labels assigned to forest field water and pavement. in fact the adjacencies of these various kinds of terrain each have different likelihoods so it makes more sense to use a prior of the form epi j sxi jv j li j syi jv j li j where v is a general compatibility or potential function. that we have also replaced fi j with li j to make it clearer that these are labels rather than discrete function samples. an alternative way to write this prior energy veksler and zabih szeliski zabih scharstein et al. is ep n vpqlp lq where the q are neighboring pixels and a spatially varying potential function vpq is evaluated for each neighboring pair. an important application of unordered mrf labeling is seam finding in image compositing agarwala dontcheva agrawala et al. figure which is explained in more detail in section here the compatibility vpqlp lq measures the quality of the visual appearance that would result from placing a pixel p from image lp next to a pixel q from image lq. as with most mrfs we assume that vpql l i.e. it is perfectly fine to choose contiguous pixels from the same image. for different labels however computer vision algorithms and applications draft figure image segmentation and funka-lea springer the user draws a few red strokes in the foreground object and a few blue ones in the background. the system computes color distributions for the foreground and background and solves a binary mrf. the smoothness weights are modulated by the intensity gradients which makes this a conditional random field the compatibility vpqlp lq may depend on the values of the underlying pixels ilpp and ilqq. consider for example where one image is all sky blue i.e. b while the other image has a transition from sky blue b to forest green g. in this case colors agree while colors disagree. conditional random fields in a classic bayesian model pxy pyxpx the prior distribution px is independent of the observations y. sometimes however it is useful to modify our prior assumptions say about the smoothness of the field we are trying to estimate in response to the sensed data. whether this makes sense from a probability viewpoint is something we discuss once we have explained the new model. consider the interactive image segmentation problem shown in figure and funka-lea in this application the user draws foreground and background strokes and the system then solves a binary mrf labeling problem to estimate the extent of the foreground object. in addition to minimizing a data term which measures the pointwise similarity between pixel colors and the inferred region distributions the mrf pqpq global optimization figure graphical model for a conditional random field the additional green edges show how combinations of sensed data influence the smoothness in the underlying mrf prior model i.e. sxi j and syi j in depend on adjacent di j values. these additional links enable the smoothness to depend on the input data. however they make sampling from this mrf more complex. is modified so that the smoothness terms sxx y and syx y in figure and depend on the magnitude of the gradient between adjacent since the smoothness term now depends on the data bayes rule no longer applies. instead we use a direct model for the posterior distribution pxy whose negative log likelihood can be written as exy edx y esx y vpxp y n vpqxp xq y using the notation introduced in the resulting probability distribution is called a conditional random field and was first introduced to the computer vision field by kumar and hebert based on earlier work in text modeling by lafferty mccallum and pereira figure shows a graphical model where the smoothness terms depend on the data values. in this particular model each smoothness term depends only on its adjacent pair of data values i.e. terms are of the form vpqxp xq yp yq in the idea of modifying smoothness terms in response to input data is not new. for example boykov and jolly used this idea for interactive segmentation as shown in figure and it is now widely used in image segmentation rother an alternative formulation that also uses detected edges to modulate the smoothness of a depth or motion field and hence to integrate multiple lower level vision modules is presented by poggio gamble and little computer vision algorithms and applications draft figure graphical model for a discriminative random field the additional green edges show how combinations of sensed data e.g. di j influence the data term for fi j. the generative model is therefore more complex i.e. we cannot just apply a simple function to the unknown variables and add noise. brown et al. rother kolmogorov and blake denoising liu freeman et al. and object recognition and shotton shotton winn rother et al. in stereo matching the idea of encouraging disparity discontinuities to coincide with intensity edges goes back even further to the early days of optimization and mrf-based algorithms gamble and little fua bobick and intille boykov veksler and zabih and is discussed in more detail in in addition to using smoothness terms that adapt to the input data kumar and hebert also compute a neighborhood function over the input data for each vpxp y term as illustrated in figure instead of using the classic unary mrf data term vpxp yp shown in figure because such neighborhood functions can be thought of as discriminant functions term widely used in machine learning they call the resulting graphical model a discriminative random field in their paper kumar and hebert show that drfs outperform similar crfs on a number of applications such as structure detection and binary image denoising. here again one could argue that previous stereo correspondence algorithms also look at a neighborhood of input data either explicitly because they compute correlation measures cross blake et al. as data terms or implicitly because even pixel-wise disparity costs look at several pixels in either the left or right image boykov veksler and zabih kumar and hebert call the unary potentials vpxp y association potentials and the pairwise potentials vpqxp yq y interaction potentials. global optimization figure structure detection results using an mrf and a drf and hebert springer. what then are the advantages and disadvantages of using conditional or discriminative random fields instead of mrfs? classic bayesian inference assumes that the prior distribution of the data is independent of the measurements. this makes a lot of sense if you see a pair of sixes when you first throw a pair of dice it would be unwise to assume that they will always show up thereafter. however if after playing for a long time you detect a statistically significant bias you may want to adjust your prior. what crfs do in essence is to select or modify the prior model based on observed data. this can be viewed as making a partial inference over additional hidden variables or correlations between the unknowns a label depth or clean image and the knowns images. in some cases the crf approach makes a lot of sense and is in fact the only plausible way to proceed. for example in grayscale image colorization lischinski and weiss the best way to transfer the continuity information from the input grayscale image to the unknown color image is to modify local smoothness constraints. similarly for simultaneous segmentation and recognition and shotton shotton winn rother et al. it makes a lot of sense to permit strong color edges to influence the semantic image label continuities. in other cases such as image denoising the situation is more subtle. using a nonquadratic smoothness term as in plays a qualitatively similar role to setting the smoothness based on local gradient information in a gaussian mrf liu freeman et al. more recent work tanaka and okutomi use a larger neighborhood and full covariance matrix on a related gaussian mrf. the advantage of gaussian mrfs when the smoothness can be correctly inferred is that the resulting quadratic energy can be minimized in a single step. however for situations where the discontinuities are not self-evident in the input data such as for piecewise-smooth sparse data interpolation and zisserman terzopoulos classic robust smoothness energy minimiza computer vision algorithms and applications draft tion may be preferable. thus as with most computer vision algorithms a careful analysis of the problem at hand and desired robustness and computation constraints may be required to choose the best technique. perhaps the biggest advantage of crfs and drfs as argued by kumar and hebert tappen liu freeman et al. and blake rother brown et al. is that learning the model parameters is sometimes easier. while learning parameters in mrfs and their variants is not a topic that we cover in this book interested readers can find more details in recently published articles and hebert roth and black tappen liu freeman et al. tappen li and huttenlocher application image restoration in section we saw how two-dimensional linear and non-linear filters can be used to remove noise or enhance sharpness in images. sometimes however images are degraded by larger problems such as scratches and blotches in this case bayesian methods such as mrfs which can model spatially varying per-pixel measurement noise can be used instead. an alternative is to use hole filling or inpainting techniques sapiro caselles et al. bertalmio vese sapiro et al. criminisi p erez and toyama as discussed in sections and figure shows an example of image denoising and inpainting filling using a markov random field. the original image has been corrupted by noise and a portion of the data has been removed. in this case the loopy belief propagation algorithm computes a slightly lower energy and also a smoother image than the alpha-expansion graph cut algorithm. additional reading if you are interested in exploring the topic of image processing in more depth some popular textbooks have been written by lim crane gomes and velho j ahne pratt russ burger and burge gonzales and woods the pre-eminent conference and journal in this field are the ieee conference on image processsing and the ieee transactions on image processing. for image compositing operators the seminal reference is by porter and duff while blinn provides a more detailed tutorial. for image compositing smith and blinn were the first to bring this topic to the attention of the graphics community while wang and cohen provide a recent in-depth survey. in the realm of linear filtering freeman and adelson provide a great introduction to separable and steerable oriented band-pass filters while perona shows how to additional reading approximate any filter as a sum of separable components. the literature on non-linear filtering is quite wide and varied it includes such topics as bilateral filtering and manduchi durand and dorsey paris and durand chen paris and durand paris kornprobst tumblin et al. related iterative algorithms chen and medioni nielsen florack and deriche black sapiro marimont et al. weickert ter haar romeny and viergever weickert barash scharr black and haussecker barash and comaniciu and variational approaches osher and shen tschumperl e and deriche tschumperl e kaftory schechner and zeevi good references to image morphology include and shapiro section bovik section ritter and wilson section serra serra and vincent yuille vincent and geiger soille the classic papers for image pyramids and pyramid blending are by burt and adelson wavelets were first introduced to the computer vision community by mallat and good tutorial and review papers and books are available simoncelli and adelson rioul and vetterli chui meyer sweldens wavelets are widely used in the computer graphics community to perform multi-resolution geometric processing derose and salesin and have been used in computer vision for similar applications pentland gortler and cohen yaou and chang lai and vemuri szeliski as well as for multi-scale oriented filtering freeman adelson et al. and denoising strela wainwright et al. while image pyramids are usually constructed using linear filtering operators some recent work has started investigating non-linear filters since these can better preserve details and other salient features. some representative papers in the computer vision literature are by gluckman lyu and simoncelli and in computational photography by bae paris and durand farbman fattal lischinski et al. fattal high-quality algorithms for image warping and resampling are covered both in the image processing literature dodgson gomes darsa costa et al. szeliski winder and uyttendaele and in computer graphics heckbert barkans akenine-m oller and haines where they go under the name of texture mapping. combination of image warping and image blending techniques are used to enable morphing between images which is covered in a series of seminal papers and books and neely gomes darsa costa et al. the regularization approach to computer vision problems was first introduced to the vision community by poggio torre and koch and terzopoulos and continues to be a popular framework for formulating and solving low-level vision problems computer vision algorithms and applications draft black and jepson nielsen florack and deriche nordstr om brox bruhn papenberg et al. levin lischinski and weiss more detailed mathematical treatment and additional applications can be found in the applied mathematics and statistics literature and arsenin engl hanke and neubauer the literature on markov random fields is truly immense with publications in related fields such as optimization and control theory of which few vision practitioners are even aware. a good guide to the latest techniques is the book edited by blake kohli and rother other recent articles that contain nice literature reviews or experimental comparisons include and funka-lea szeliski zabih scharstein et al. kumar veksler and torr the seminal paper on markov random fields is the work of geman and geman who introduced this formalism to computer vision researchers and also introduced the notion of line processes additional binary variables that control whether smoothness penalties are enforced or not. black and rangarajan showed how independent line processes could be replaced with robust pairwise potentials boykov veksler and zabih developed iterative binary graph cut algorithms for optimizing multi-label mrfs kolmogorov and zabih characterized the class of binary energy potentials required for these techniques to work and freeman pasztor and carmichael popularized the use of loopy belief propagation for mrf inference. many more additional references can be found in sections and and appendix exercises ex color balance write a simple application to change the color balance of an image by multiplying each color value by a different user-specified constant. if you want to get fancy you can make this application interactive with sliders. do you get different results if you take out the gamma transformation before or after doing the multiplication? why or why not? take the same picture with your digital camera using different color balance settings cameras control the color balance from one of the menus. can you recover what the color balance ratios are between the different settings? you may need to put your camera on a tripod and align the images manually or automatically to make this work. alternatively use a color checker chart as discussed in sections and if you have access to the raw image for the camera perform the demosaicing yourself or downsample the image resolution to get a true rgb image. does exercises your camera perform a simple linear mapping between raw values and the colorbalanced values in a jpeg? some high-end cameras have a rawjpeg mode which makes this comparison much easier. can you think of any reason why you might want to perform a color twist tion on the images? see also exercise for some related ideas. ex compositing and reflections section describes the process of compositing an alpha-matted image on top of another. answer the following questions and optionally validate them experimentally most captured images have gamma correction applied to them. does this invalidate the basic compositing equation if so how should it be fixed? the additive reflection model may have limitations. what happens if the glass is tinted especially to a non-gray hue? how about if the glass is dirty or smudged? how could you model wavy glass or other kinds of refractive objects? ex blue screen matting set up a blue or green background e.g. by buying a large piece of colored posterboard. take a picture of the empty background and then of the background with a new object in front of it. pull the matte using the difference between each colored pixel and its assumed corresponding background pixel using one of the techniques described in section or by smith and blinn ex difference keying implement a difference keying algorithm section krumm brumitt et al. consisting of the following steps compute the mean and variance median and robust variance at each pixel in an empty video sequence. for each new frame classify each pixel as foreground or background the back ground pixels to compute the alpha channel and composite over a new background. clean up the image using morphology label the connected components compute their centroids and track them from frame to frame. use this to build a people counter ex photo effects write a variety of photo enhancement or effects filters contrast solarization etc. which ones are useful sensible corrections and which ones are more creative unusual images? computer vision algorithms and applications draft ex histogram equalization compute the gray level histogram for an image and equalize it so that the tones look better the image is less sensitive to exposure settings. you may want to use the following steps convert the color image to luminance compute the histogram the cumulative distribution and the compensation transfer function try to increase the punch in the image by ensuring that a certain fraction of pixels are mapped to pure black and white. limit the local gain in the transfer function. one way to do this is to limit fi i or while performing the accumulation keeping any unaccumulated values in reserve ll let you figure out the exact details. compensate the luminance channel through the lookup table and re-generate the color image using color ratios color values that are clipped in the original image i.e. have one or more saturated color channels may appear unnatural when remapped to a non-clipped value. extend your algorithm to handle this case in some useful way. ex local histogram equalization compute the gray level histograms for each patch but add to vertices based on distance spline. build on exercise computation. distribute values to adjacent vertices convert to cdf functions. use low-pass filtering of cdfs. interpolate adjacent cdfs for final lookup. ex padding for neighborhood operations write down the formulas for computing the padded pixel values fi j as a function of the original pixel values fk l and the image width and height n for each of the padding modes shown in figure for example for replication fi j fk l k minm i l minn j you may want to use the min max mod and absolute value operators in addition to the regular arithmetic operators. exercises describe in more detail the advantages and disadvantages of these various modes. check what your graphics card does by drawing a texture-mapped rectangle where the texture coordinates lie beyond the range and using different texture clamping modes. ex separable filters implement convolution with a separable kernel. the input should be a grayscale or color image along with the horizontal and vertical kernels. make sure you support the padding mechanisms developed in the previous exercise. you will need this functionality for some of the later exercises. if you already have access to separable filtering in an image processing package you are using as ipl skip this exercise. use pietro perona s technique to approximate convolution as a sum of a number of separable kernels. let the user specify the number of kernels and report back some sensible metric of the approximation fidelity. ex discrete gaussian filters discuss the following issues with implementing a discrete gaussian filter if you just sample the equation of a continuous gaussian filter at discrete locations will you get the desired properties e.g. will the coefficients sum up to similarly if you sample a derivative of a gaussian do the samples sum up to or have vanishing higher-order moments? would it be preferable to take the original signal interpolate it with a sinc blur with a continuous gaussian then pre-filter with a sinc before re-sampling? is there a simpler way to do this in the frequency domain? would it make more sense to produce a gaussian frequency response in the fourier domain and to then take an inverse fft to obtain a discrete filter? how does truncation of the filter change its frequency response? does it introduce any additional artifacts? are the resulting two-dimensional filters as rotationally invariant as their continuous analogs? is there some way to improve this? in fact can any discrete or non-separable filter be truly rotationally invariant? ex sharpening blur and noise removal implement some softening sharpening and non-linear diffusion sharpening or noise removal filters such as gaussian median and bilateral as discussed in section take blurry or noisy images in low light is a good way to get both and try to improve their appearance and legibility. computer vision algorithms and applications draft ex steerable filters implement freeman and adelson s steerable filter algorithm. the input should be a grayscale or color image and the output should be a multi-banded image consisting of and the coefficients for the filters can be found in the paper by freeman and adelson test the various order filters on a number of images of your choice and see if you can reliably find corner and intersection features. these filters will be quite useful later to detect elongated structures such as lines ex distance transform implement some algorithms for city block and euclidean distance transforms. can you do it without peeking at the literature borgefors if so what problems did you come across and resolve? later on you can use the distance functions you compute to perform feathering during image stitching ex connected components implement one of the connected component algorithms from section or section from haralick and shapiro s book and discuss its computational complexity. threshold or quantize an image to obtain a variety of input labels and then compute the area statistics for the regions that you find. use the connected components that you have found to track or match regions in differ ent images or video frames. ex fourier transform prove the properties of the fourier transform listed in table and derive the formulas for the fourier transforms listed in tables and these exercises are very useful if you want to become comfortable working with fourier transforms which is a very useful skill when analyzing and designing the behavior and efficiency of many computer vision algorithms. ex wiener filtering estimate the frequency spectrum of your personal photo collection and use it to perform wiener filtering on a few images with varying degrees of noise. collect a few hundred of your images by re-scaling them to fit within a window and cropping them. take their fourier transforms throw away the phase information and average together all of the spectra. pick two of your favorite images and add varying amounts of gaussian noise n gray levels. exercises for each combination of image and noise determine by eye which width of a gaussian blurring filter s gives the best denoised result. you will have to make a subjective decision between sharpness and noise. compute the wiener filtered version of all the noised images and compare them against your hand-tuned gaussian-smoothed images. do your image spectra have a lot of energy concentrated along the horizontal and vertical axes and fy can you think of an explanation for this? does rotating your image samples by move this energy to the diagonals? if not could it be due to edge effects in the fourier transform? can you suggest some techniques for reducing such effects? ex deblurring using wiener filtering use wiener filtering to deblur some images. modify the wiener filter derivation to incorporate blur discuss the resulting wiener filter in terms of its noise suppression and frequency boosting characteristics. assuming that the blur kernel is gaussian and the image spectrum follows an inverse frequency law compute the frequency response of the wiener filter and compare it to the unsharp mask. synthetically blur two of your sample images with gaussian blur kernels of different radii add noise and then perform wiener filtering. repeat the above experiment with a pillbox blurring kernel which is characteristic of a finite aperture lens compare these results to gaussian blur kernels sure to inspect your frequency plots. it has been suggested that regular apertures are anathema to de-blurring because they introduce zeros in the sensed frequency spectrum raskar agrawal et al. show that this is indeed an issue if no prior model is assumed for the signal i.e. p if a reasonable power spectrum is assumed is this still a problem we still get banding or ringing artifacts? s ex high-quality image resampling implement several of the low-pass filters presented in section and also the discussion of the windowed sinc shown in table and figure feel free to implement other filters unser apply your filters to continuously resize an image both magnifying and minifying it compare the resulting animations for several filters. use both a computer vision algorithms and applications draft figure sample images for testing the quality of resampling algorithms a synthetic chirp and some high-frequency images from the image compression community. synthetic chirp image and natural images with lots of high-frequency detail you may find it helpful to write a simple visualization program that continuously plays the animations for two or more filters at once and that let you blink between different results. discuss the merits and deficiencies of each filter as well as its tradeoff between speed and quality. ex pyramids construct an image pyramid. the inputs should be a grayscale or color image a separable filter kernel and the number of desired levels. implement at least the following kernels block filtering burt and adelson s binomial kernel and adelson a high-quality seven- or nine-tap filter. compare the visual quality of the various decimation filters. also shift your input image by to pixels and compare the resulting decimated size image sequence. ex pyramid blending write a program that takes as input two color images and a binary mask image and produces the laplacian pyramid blend of the two images. construct the laplacian pyramid for each image. construct the gaussian pyramid for the two mask images input image and its complement. these particular images are available on the book s web site. exercises multiply each laplacian image by its corresponding mask and sum the images figure reconstruct the final image from the blended laplacian pyramid. generalize your algorithm to input n images and a label image with values n value can be reserved for no input discuss whether the weighted summation stage needs to keep track of the total weight for renormalization or whether the math just works out. use your algorithm either to blend two differently exposed image avoid under- and over-exposed regions or to make a creative blend of two different scenes. ex wavelet construction and applications implement one of the wavelet families described in section or by simoncelli and adelson as well as the basic laplacian pyramid apply the resulting representations to one of the following two tasks compression compute the entropy in each band for the different wavelet implementations assuming a given quantization level gray level to keep the rounding error acceptable. quantize the wavelet coefficients and reconstruct the original images. which technique performs better? and adelson or any of the multitude of wavelet compression papers for some typical results. denoising. after computing the wavelets suppress small values using coring i.e. set small values to zero using a piecewise linear or other c function. compare the results of your denoising using different wavelet and pyramid representations. ex parametric image warping write the code to do affine and perspective image warps bilinear as well. try a variety of interpolants and report on their visual quality. in particular discuss the following in a mip-map selecting only the coarser level adjacent to the computed fractional level will produce a blurrier image while selecting the finer level will lead to aliasing. explain why this is so and discuss whether blending an aliased and a blurred image mip-mapping is a good idea. when the ratio of the horizontal and vertical resampling rates becomes very different the mip-map performs even worse. suggest some approaches to reduce such problems. ex local image warping open an image and deform its appearance in one of the following ways computer vision algorithms and applications draft click on a number of pixels and move them to new locations. interpolate the resulting sparse displacement field to obtain a dense motion field and draw a number of lines in the image. move the endpoints of the lines to specify their new positions and use the beier neely interpolation algorithm and neely discussed in section to get a dense motion field. overlay a spline control grid and move one grid point at a time select the level of the deformation. have a dense per-pixel flow field and use a soft paintbrush to design a horizontal and vertical velocity field. prove whether the beier neely warp does or does not reduce to a sparse point-based deformation as the line segments become shorter to points. ex forward warping given a displacement field from the previous exercise write a forward warping algorithm write a forward warper using splatting either nearest neighbor or soft accumulation write a two-pass algorithm which forward warps the displacement field fills in small holes and then uses inverse warping gortler he et al. compare the quality of these two algorithms. ex feature-based morphing extend the warping code you wrote in exercise to import two different images and specify correspondences line or mesh-based between the two images. create a morph by partially warping the images towards each other and cross-dissolving try using your morphing algorithm to perform an image rotation and discuss whether it behaves the way you want it to. ex image editor extend the program you wrote in exercise to import images and let you create a collage of pictures. you should implement the following steps open up a new image a separate window. exercises figure there is a faint image of a rainbow visible in the right hand side of this picture. can you think of a way to enhance it shift drag to crop a subregion select whole image. paste into the current canvas. select the deformation mode model translation rigid similarity affine or perspective. drag any corner of the outline to change its transformation. change the relative ordering of the images and which image is currently being manipulated. the user should see the composition of the various images pieces on top of each other. this exercise should be built on the image transformation classes supported in the software library. persistence of the created representation and load should also be supported each image save its transformation. ex texture-mapped viewer extend the viewer you created in exercise to include texture-mapped polygon rendering. augment each polygon with v w coordinates into an image. ex image denoising implement at least two of the various image denoising techniques described in this chapter and compare them on both synthetically noised image sequences and real-world sequences. does the performance of the algorithm depend on the correct choice of noise level estimate? can you draw any conclusions as to which techniques work better? computer vision algorithms and applications draft ex rainbow enhancer challenging take a picture containing a rainbow such as figure and enhance the strength of the rainbow. draw an arc in the image delineating the extent of the rainbow. fit an additive rainbow function why it is additive to this arc is best to work with linearized pixel values using the spectrum as the cross section and estimating the width of the arc and the amount of color being added. this is the trickiest part of the problem as you need to tease apart the rainbow pattern and the natural image hiding behind it. amplify the rainbow signal and add it back into the image re-applying the gamma function if necessary to produce the final image. ex image deblocking challenging now that you have some good techniques to distinguish signal from noise develop a technique to remove the blocking artifacts that occur with jpeg at high compression settings your technique can be as simple as looking for unexpected edges along block boundaries to looking at the quantization step as a projection of a convex region of the transform coefficient space onto the corresponding quantized values. does the knowledge of the compression factor which is available in the jpeg header information help you perform better deblocking? because the quantization occurs in the dct transformed ycbcr space it may be preferable to perform the analysis in this space. on the other hand image priors make more sense in an rgb space do they?. decide how you will approach this dichotomy and discuss your choice. while you are at it since the ycbcr conversion is followed by a chrominance subsampling stage the dct see if you can restore some of the lost high-frequency chrominance signal using one of the better restoration techniques discussed in this chapter. if your camera has a raw jpeg mode how close can you come to the noise-free true pixel values? suggestion may not be that useful since cameras generally use reasonably high quality settings for their raw jpeg models. ex inference in de-blurring challenging write down the graphical model corresponding to figure for a non-blind image deblurring problem i.e. one where the blur kernel is known ahead of time. what kind of efficient inference algorithms can you think of for solving such problems? chapter feature detection and matching edges points and patches feature detectors feature descriptors feature matching feature tracking application performance-driven animation edge detection edge linking application edge editing and enhancement lines hough transforms vanishing points application rectangle detection additional reading exercises successive approximation computer vision algorithms and applications draft figure a variety of feature detectors and descriptors can be used to analyze describe and match images point-like interest operators szeliski and winder ieee region-like interest operators chum urban et al. elsevier edges and goldberg ieee straight lines steedly szeliski et al. acm. points and patches feature detection and matching are an essential component of many computer vision applications. consider the two pairs of images shown in figure for the first pair we may wish to align the two images so that they can be seamlessly stitched into a composite mosaic for the second pair we may wish to establish a dense set of correspondences so that a model can be constructed or an in-between view can be generated in either case what kinds of features should you detect and then match in order to establish such an alignment or set of correspondences? think about this for a few moments before reading on. the first kind of feature that you may notice are specific locations in the images such as mountain peaks building corners doorways or interestingly shaped patches of snow. these kinds of localized feature are often called keypoint features or interest points even corners and are often described by the appearance of patches of pixels surrounding the point location another class of important features are edges e.g. the profile of mountains against the sky these kinds of features can be matched based on their orientation and local appearance profiles and can also be good indicators of object boundaries and occlusion events in image sequences. edges can be grouped into longer curves and straight line segments which can be directly matched or analyzed to find vanishing points and hence internal and external camera parameters in this chapter we describe some practical approaches to detecting such features and also discuss how feature correspondences can be established across different images. point features are now used in such a wide variety of applications that it is good practice to read and implement some of the algorithms from edges and lines provide information that is complementary to both keypoint and region-based descriptors and are well-suited to describing object boundaries and man-made objects. these alternative descriptors while extremely useful can be skipped in a short introductory course. points and patches point features can be used to find a sparse set of corresponding locations in different images often as a pre-cursor to computing camera pose which is a prerequisite for computing a denser set of correspondences using stereo matching such correspondences can also be used to align different images e.g. when stitching image mosaics or performing video stabilization they are also used extensively to perform object instance and category recognition and a key advantage of keypoints is that they permit matching even in the presence of clutter and large scale and orientation changes. feature-based correspondence techniques have been used since the early days of stereo computer vision algorithms and applications draft figure two pairs of images to be matched. what kinds of feature might one use to establish a set of correspondences between these images? matching moravec hannah and have more recently gained popularity for image-stitching applications faugeras and deriche brown and lowe as well as fully automated modeling torr and zisserman schaffalitzky and zisserman brown and lowe snavely seitz and szeliski there are two main approaches to finding feature points and their correspondences. the first is to find features in one image that can be accurately tracked using a local search technique such as correlation or least squares the second is to independently detect features in all the images under consideration and then match features based on their local appearance the former approach is more suitable when images are taken from nearby viewpoints or in rapid succession video sequences while the latter is more suitable when a large amount of motion or appearance change is expected e.g. in stitching together panoramas and lowe establishing correspondences in wide baseline stereo and zisserman or performing object recognition perona and zisserman in this section we split the keypoint detection and matching pipeline into four separate stages. during the feature detection stage each image is searched for locations that are likely to match well in other images. at the feature description stage each region around detected keypoint locations is converted into a more compact and stable descriptor that can be matched against other descriptors. the points and patches figure image pairs with extracted patches below. notice how some patches can be localized or matched with higher accuracy than others. feature matching stage efficiently searches for likely matching candidates in other images. the feature tracking stage is an alternative to the third stage that only searches a small neighborhood around each detected feature and is therefore more suitable for video processing. a wonderful example of all of these stages can be found in david lowe s paper which describes the development and refinement of his scale invariant feature transform comprehensive descriptions of alternative techniques can be found in a series of survey and evaluation papers covering both feature detection mohr and bauckhage mikolajczyk tuytelaars schmid et al. tuytelaars and mikolajczyk and feature descriptors and schmid shi and tomasi and triggs also provide nice reviews of feature detection techniques. feature detectors how can we find image locations where we can reliably find correspondences with other images i.e. what are good features to track and tomasi triggs look again at the image pair shown in figure and at the three sample patches to see how well they might be matched or tracked. as you may notice textureless patches are nearly impossible to localize. patches with large contrast changes are easier to localize although straight line segments at a single orientation suffer from the aperture problem and schunck lucas and kanade anandan i.e. it is only possible to align the patches along the direction normal to the edge direction patches with computer vision algorithms and applications draft figure aperture problems for different image patches stable corner-like flow classic aperture problem illusion textureless region. the two images and are overlaid. the red vector u indicates the displacement between the patch centers and the wxi weighting function window is shown as a dark circle. gradients in at least two different orientations are the easiest to localize as shown schematically in figure these intuitions can be formalized by looking at the simplest possible matching criterion for comparing two image patches i.e. their summed square difference ewssdu u where and are the two images being compared u v is the displacement vector wx is a spatially varying weighting window function and the summation i is over all the pixels in the patch. note that this is the same formulation we later use to estimate motion between complete images when performing feature detection we do not know which other image locations the feature will end up being matched against. therefore we can only compute how stable this metric is with respect to small variations in position u by comparing an image patch against itself which is known as an auto-correlation function or surface eac u u note how the auto-correlation surface for the textured flower bed and the red cross in the lower right quadrant of figure exhibits a strong minimum indicating that it can be well localized. the correlation surface corresponding to the roof edge has a strong ambiguity along one direction while the correlation surface corresponding to the cloud region has no stable minimum. strictly speaking a correlation is the product of two patches i m using the term here in a more qualitative sense. the weighted sum of squared differences is often called an ssd surface xxixiuui points and patches figure three auto-correlation surfaces eac u shown as both grayscale images and surface plots the original image is marked with three red crosses to denote where the auto-correlation surfaces were computed this patch is from the flower bed unique minimum this patch is from the roof edge aperture problem and this patch is from the cloud good peak. each grid point in figures b d is one value of u. computer vision algorithms and applications draft using a taylor series expansion of the image function u u and kanade shi and tomasi we can approximate the auto-correlation surface as eac u u u wxi ut a u where x y is the image gradient at xi. this gradient can be computed using a variety of techniques mohr and bauckhage the classic harris detector and stephens uses a filter but more modern variants mohr and bauckhage triggs convolve the image with horizontal and vertical derivatives of a gaussian with the auto-correlation matrix a can be written as a w i x ixiy y ixiy i where we have replaced the weighted summations with discrete convolutions with the weighting kernel w. this matrix can be interpreted as a tensor image where the outer products of the gradients i are convolved with a weighting function w to provide a per-pixel estimate of the local shape of the auto-correlation function. as first shown by anandan and further discussed in section and the inverse of the matrix a provides a lower bound on the uncertainty in the location of a matching patch. it is therefore a useful indicator of which patches can be reliably matched. the easiest way to visualize and reason about this uncertainty is to perform an eigenvalue analysis of the auto-correlation matrix a which produces two eigenvalues and two eigenvector directions since the larger uncertainty depends on the smaller eigenvalue i.e. it makes sense to find maxima in the smaller eigenvalue to locate good features to track and tomasi f orstner harris. while anandan and lucas and kanade were the first to analyze the uncertainty structure of the auto-correlation matrix they did so in the context of associating certainties with optic flow measurements. f orstner and harris and stephens points and patches figure uncertainty ellipse corresponding to an eigenvalue analysis of the autocorrelation matrix a. were the first to propose using local maxima in rotationally invariant scalar measures derived from the auto-correlation matrix to locate keypoints for the purpose of sparse feature matching. mohr and bauckhage triggs give more detailed historical reviews of feature detection algorithms. both of these techniques also proposed using a gaussian weighting window instead of the previously used square patches which makes the detector response insensitive to in-plane image rotations. the minimum eigenvalue and tomasi is not the only quantity that can be used to find keypoints. a simpler quantity proposed by harris and stephens is deta with unlike eigenvalue analysis this quantity does not require the use of square roots and yet is still rotationally invariant and also downweights edge-like features where triggs suggests using the quantity with which also reduces the response at edges where aliasing errors sometimes inflate the smaller eigenvalue. he also shows how the basic hessian can be extended to parametric motions to detect points that are also accurately localizable in scale and rotation. brown szeliski and winder on the other hand use the harmonic mean det a tr a which is a smoother function in the region where figure shows isocontours of the various interest point operators from which we can see how the two eigenvalues are blended to determine the final interest value. computer vision algorithms and applications draft figure isocontours of popular keypoint detection functions szeliski and winder each detector looks for points where the eigenvalues of a w i i t are both large. compute the horizontal and vertical derivatives of the image ix and iy by con volving the original image with derivatives of gaussians compute the three images corresponding to the outer products of these gradients. matrix a is symmetric so only three entries are needed. convolve each of these images with a larger gaussian. compute a scalar interest measure using one of the formulas discussed above. find local maxima above a certain threshold and report them as detected feature point locations. algorithm outline of a basic feature detection algorithm. points and patches figure interest operator responses sample image harris response and dog response. the circle sizes and colors indicate the scale at which each interest point was detected. notice how the two detectors tend to respond at complementary locations. the steps in the basic auto-correlation-based keypoint detector are summarized in algorithm figure shows the resulting interest operator responses for the classic harris detector as well as the difference of gaussian detector discussed below. adaptive non-maximal suppression while most feature detectors simply look for local maxima in the interest function this can lead to an uneven distribution of feature points across the image e.g. points will be denser in regions of higher contrast. to mitigate this problem brown szeliski and winder only detect features that are both local maxima and whose response value is significantly greater than that of all of its neighbors within a radius r d. they devise an efficient way to associate suppression radii with all local maxima by first sorting them by their response strength and then creating a second list sorted by decreasing suppression radius szeliski and winder figure shows a qualitative comparison of selecting the top n features and using anms. measuring repeatability. given the large number of feature detectors that have been developed in computer vision how can we decide which ones to use? schmid mohr and bauckhage were the first to propose measuring the repeatability of feature detectors which they define as the frequency with which keypoints detected in one image are found within pixels of the corresponding location in a transformed image. in their paper they transform their planar images by applying rotations scale changes illumination changes viewpoint changes and adding noise. they also measure the information content available at each detected feature point which they define as the entropy of a set of rotationally invariant local grayscale descriptors. among the techniques they survey they find that the improved derivative version of the harris operator with d of the derivative gaussian and i of the integration gaussian works best. computer vision algorithms and applications draft strongest strongest anms r anms r figure adaptive non-maximal suppression szeliski and winder ieee the upper two images show the strongest and interest points while the lower two images show the interest points selected with adaptive non-maximal suppression along with the corresponding suppression radius r. note how the latter features have a much more uniform spatial distribution across the image. scale invariance in many situations detecting features at the finest stable scale possible may not be appropriate. for example when matching images with little high frequency detail clouds fine-scale features may not exist. one solution to the problem is to extract features at a variety of scales e.g. by performing the same operations at multiple resolutions in a pyramid and then matching features at the same level. this kind of approach is suitable when the images being matched do not undergo large scale changes e.g. when matching successive aerial images taken from an airplane or stitching panoramas taken with a fixed-focal-length camera. figure shows the output of one such approach the multi-scale oriented patch detector of brown szeliski and winder for which responses at five different scales are shown. however for most object recognition applications the scale of the object in the image points and patches figure multi-scale oriented patches extracted at five pyramid levels szeliski and winder ieee. the boxes show the feature orientation and the region from which the descriptor vectors are sampled. is unknown. instead of extracting features at many different scales and then matching all of them it is more efficient to extract features that are stable in both location and scale mikolajczyk and schmid early investigations into scale selection were performed by lindeberg who first proposed using extrema in the laplacian of gaussian function as interest point locations. based on this work lowe proposed computing a set of sub-octave difference of gaussian filters looking for maxima in the resulting structure and then computing a sub-pixel spacescale location using a quadratic fit and lowe the number of sub-octave levels was determined after careful empirical investigation to be three which corresponds to a quarter-octave pyramid which is the same as used by triggs as with the harris operator pixels where there is strong asymmetry in the local curvature of the indicator function this case the dog are rejected. this is implemented by first computing the local hessian of the difference image d h dxx dxy dxy dyy and then rejecting keypoints for which deth computer vision algorithms and applications draft figure scale-space feature detection using a sub-octave difference of gaussian pyramid springer adjacent levels of a sub-octave gaussian pyramid are subtracted to produce difference of gaussian images extrema and minima in the resulting volume are detected by comparing a pixel to its neighbors. while lowe s scale invariant feature transform performs well in practice it is not based on the same theoretical foundation of maximum spatial stability as the auto-correlationbased detectors. fact its detection locations are often complementary to those produced by such techniques and can therefore be used in conjunction with these other approaches. in order to add a scale selection mechanism to the harris corner detector mikolajczyk and schmid evaluate the laplacian of gaussian function at each detected harris point a multi-scale pyramid and keep only those points for which the laplacian is extremal or smaller than both its coarser and finer-level values. an optional iterative refinement for both scale and position is also proposed and evaluated. additional examples of scale invariant region detectors are discussed by mikolajczyk tuytelaars schmid et al. tuytelaars and mikolajczyk rotational invariance and orientation estimation in addition to dealing with scale changes most image matching and object recognition algorithms need to deal with least in-plane image rotation. one way to deal with this problem is to design descriptors that are rotationally invariant and mohr but such descriptors have poor discriminability i.e. they map different looking patches to the same descriptor. scale octavescalenextoctavegaussiandifference ofgaussian ratherthanthemoreusualt g g and g gxyk gxy andthereforegxyk gxy points and patches figure a dominant orientation estimate can be computed by creating a histogram of all the gradient orientations by their magnitudes or after thresholding out small gradients and then finding the significant peaks in this distribution springer. a better method is to estimate a dominant orientation at each detected keypoint. once the local orientation and scale of a keypoint have been estimated a scaled and oriented patch around the detected point can be extracted and used to form a feature descriptor and the simplest possible orientation estimate is the average gradient within a region around the keypoint. if a gaussian weighting function is used szeliski and winder this average gradient is equivalent to a first-order steerable filter i.e. it can be computed using an image convolution with the horizontal and vertical derivatives of gaussian filter and adelson in order to make this estimate more reliable it is usually preferable to use a larger aggregation window kernel size than detection window szeliski and winder the orientations of the square boxes shown in figure were computed using this technique. sometimes however the averaged gradient in a region can be small and therefore an unreliable indicator of orientation. a more reliable technique is to look at the histogram of orientations computed around the keypoint. lowe computes a histogram of edge orientations weighted by both gradient magnitude and gaussian distance to the center finds all peaks within of the global maximum and then computes a more accurate orientation estimate using a three-bin parabolic fit affine invariance while scale and rotation invariance are highly desirable for many applications such as wide baseline stereo matching and zisserman schaffalitzky and zisserman or location recognition philbin sivic et al. full affine invariance is preferred. computer vision algorithms and applications draft figure affine region detectors used to match two images taken from dramatically different viewpoints and schmid springer. a a figure affine normalization using the second moment matrices as described by mikolajczyk tuytelaars schmid et al. springer. after image coordinates are transformed using the matrices a they are related by a pure rotation r which can be estimated using a dominant orientation technique. and a affine-invariant detectors not only respond at consistent locations after scale and orientation changes they also respond consistently across affine deformations such as perspective foreshortening in fact for a small enough patch any continuous image warping can be well approximated by an affine deformation. to introduce affine invariance several authors have proposed fitting an ellipse to the autocorrelation or hessian matrix eigenvalue analysis and then using the principal axes and ratios of this fit as the affine coordinate frame and garding baumberg mikolajczyk and schmid mikolajczyk tuytelaars schmid et al. tuytelaars and mikolajczyk figure shows how the square root of the moment matrix can be used to transform local patches into a frame which is similar up to rotation. another important affine invariant region detector is the maximally stable extremal region detector developed by matas chum urban et al. to detect msers binary regions are computed by thresholding the image at all possible gray levels technique therefore only works for grayscale images. this operation can be performed efficiently by first sorting all pixels by gray value and then incrementally adding pixels to each connected component as the threshold is changed er and stew enius as the threshold is changed the area of each component is monitored regions whose rate of change of area with respect to the threshold is minimal are defined as maximally stable. such regions points and patches figure maximally stable extremal regions extracted and matched from a number of images chum urban et al. elsevier. figure feature matching how can we extract local descriptors that are invariant to inter-image variations and yet still discriminative enough to establish correct correspondences? are therefore invariant to both affine geometric and photometric bias-gain or smooth monotonic transformations if desired an affine coordinate frame can be fit to each detected region using its moment matrix. the area of feature point detectors continues to be very active with papers appearing every year at major computer vision conferences and shah koethe carneiro and jepson kenney zuliani and manjunath bay tuytelaars and van gool platel balmachnova florack et al. rosten and drummond mikolajczyk tuytelaars schmid et al. survey a number of popular affine region detectors and provide experimental comparisons of their invariance to common image transformations such as scaling rotations noise and blur. these experimental results code and pointers to the surveyed papers can be found on their web site at httpwww.robots.ox.ac.uk vggresearchaffine. of course keypoints are not the only features that can be used for registering images. zoghlami faugeras and deriche use line segments as well as point-like features to estimate homographies between pairs of images whereas bartoli coquerelle and sturm use line segments with local correspondences along the edges to extract structure and motion. tuytelaars and van gool use affine invariant regions to detect correspondences for wide baseline stereo matching whereas kadir zisserman and brady detect salient regions where patch entropy and its rate of change with scale are locally maximal. corso and hager use a related technique to fit oriented gaussian kernels to homogeneous regions. more details on techniques for finding and matching curves lines and regions can be found later in this chapter. computer vision algorithms and applications draft figure mops descriptors are formed using an sampling of bias and gain normalized intensity values with a sample spacing of five pixels relative to the detection scale szeliski and winder ieee. this low frequency sampling gives the features some robustness to interest point location error and is achieved by sampling at a higher pyramid level than the detection scale. feature descriptors after detecting features we must match them i.e. we must determine which features come from corresponding locations in different images. in some situations e.g. for video sequences and tomasi or for stereo pairs that have been rectified deriche faugeras et al. loop and zhang scharstein and szeliski the local motion around each feature point may be mostly translational. in this case simple error metrics such as the sum of squared differences or normalized cross-correlation described in section can be used to directly compare the intensities in small patches around each feature point. comparative study by mikolajczyk and schmid discussed below uses cross-correlation. because feature points may not be exactly located a more accurate matching score can be computed by performing incremental motion refinement as described in section but this can be time consuming and can sometimes even decrease performance szeliski and winder in most cases however the local appearance of features will change in orientation and scale and sometimes even undergo affine deformations. extracting a local scale orientation or affine frame estimate and then using this to resample the patch before forming the feature descriptor is thus usually preferable even after compensating for these changes the local appearance of image patches will usually still vary from image to image. how can we make image descriptors more invariant to such changes while still preserving discriminability between different patches mikolajczyk and schmid review some recently developed view-invariant local image descriptors and experimentally compare their performance. below we describe a few of these descriptors in more detail. points and patches bias and gain normalization for tasks that do not exhibit large amounts of foreshortening such as image stitching simple normalized intensity patches perform reasonably well and are simple to implement szeliski and winder in order to compensate for slight inaccuracies in the feature point detector orientation and scale these multi-scale oriented patches are sampled at a spacing of five pixels relative to the detection scale using a coarser level of the image pyramid to avoid aliasing. to compensate for affine photometric variations exposure changes or bias and gain patch intensities are re-scaled so that their mean is zero and their variance is one. scale invariant feature transform sift features are formed by computing the gradient at each pixel in a window around the detected keypoint using the appropriate level of the gaussian pyramid at which the keypoint was detected. the gradient magnitudes are downweighted by a gaussian fall-off function as a blue circle in in order to reduce the influence of gradients far from the center as these are more affected by small misregistrations. in each quadrant a gradient orientation histogram is formed by adding the weighted gradient value to one of eight orientation histogram bins. to reduce the effects of location and dominant orientation misestimation each of the original weighted gradient magnitudes is softly added to histogram bins using trilinear interpolation. softly distributing values to adjacent histogram bins is generally a good idea in any application where histograms are being computed e.g. for hough transforms or local histogram equalization the resulting non-negative values form a raw version of the sift descriptor vector. to reduce the effects of contrast or gain variations are already removed by the gradient the vector is normalized to unit length. to further make the descriptor robust to other photometric variations values are clipped to and the resulting vector is once again renormalized to unit length. pca-sift. ke and sukthankar propose a simpler way to compute descriptors inspired by sift it computes the x and y derivatives over a patch and then reduces the resulting vector to using principal component analysis and appendix another popular variant of sift is surf tuytelaars and van gool which uses box filters to approximate the derivatives and integrals used in sift. gradient location-orientation histogram this descriptor developed by mikolajczyk and schmid is a variant on sift that uses a log-polar binning structure instead of the four quadrants used by lowe the spatial bins are of radius computer vision algorithms and applications draft image gradients keypoint descriptor figure a schematic representation of lowe s scale invariant feature transform gradient orientations and magnitudes are computed at each pixel and weighted by a gaussian fall-off function circle. a weighted gradient orientation histogram is then computed in each subregion using trilinear interpolation. while this figure shows an pixel patch and a descriptor array lowe s actual implementation uses patches and a array of eight-bin histograms. and with eight angular bins for the central region for a total of spatial bins and orientation bins. the histogram is then projected onto a descriptor using pca trained on a large database. in their evaluation mikolajczyk and schmid found that gloh which has the best performance overall outperforms sift by a small margin. steerable filters. steerable filters are combinations of derivative of gaussian filters that permit the rapid computation of even and odd and anti-symmetric edge-like and corner-like features at all possible orientations and adelson because they use reasonably broad gaussians they too are somewhat insensitive to localization and orientation errors. performance of local descriptors. among the local descriptors that mikolajczyk and schmid compared they found that gloh performed best followed closely by sift figure they also present results for many other descriptors not covered in this book. the field of feature descriptors continues to evolve rapidly with some of the newer techniques looking at local color information de weijer and schmid abdel-hakim and farag winder and brown develop a multi-stage framework for feature descriptor computation that subsumes both sift and gloh and also allows them to learn optimal parameters for newer descriptors that outperform previous hand-tuned points and patches image gradients keypoint descriptor figure the gradient location-orientation histogram descriptor uses log-polar bins instead of square bins to compute orientation histograms and schmid descriptors. hua brown and winder extend this work by learning lower-dimensional projections of higher-dimensional descriptors that have the best discriminative power. both of these papers use a database of real-world image patches obtained by sampling images at locations that were reliably matched using a robust structure-from-motion algorithm applied to internet photo collections seitz and szeliski goesele snavely curless et al. in concurrent work tola lepetit and fua developed a similar daisy descriptor for dense stereo matching and optimized its parameters based on ground truth stereo data. while these techniques construct feature detectors that optimize for repeatability across all object classes it is also possible to develop class- or instance-specific feature detectors that maximize discriminability from other classes learned-miller and malik feature matching once we have extracted features and their descriptors from two or more images the next step is to establish some preliminary feature matches between these images. in this section we divide this problem into two separate components. the first is to select a matching strategy which determines which correspondences are passed on to the next stage for further processing. the second is to devise efficient data structures and algorithms to perform this matching as quickly as possible. the discussion of related techniques in section computer vision algorithms and applications draft figure spatial summation blocks for sift gloh and some newly developed feature descriptors and brown ieee the parameters for the new features e.g. their gaussian weights are learned from a training database of matched real-world image patches obtained from robust structure from motion applied to internet photo collections brown and winder matching strategy and error rates determining which feature matches are reasonable to process further depends on the context in which the matching is being performed. say we are given two images that overlap to a fair amount for image stitching as in figure or for tracking objects in a video. we know that most features in one image are likely to match the other image although some may not match because they are occluded or their appearance has changed too much. on the other hand if we are trying to recognize how many known objects appear in a cluttered scene most of the features may not match. furthermore a large number of potentially matching objects must be searched which requires more efficient strategies as described below. to begin with we assume that the feature descriptors have been designed so that euclidean magnitude distances in feature space can be directly used for ranking potential matches. if it turns out that certain parameters in a descriptor are more reliable than others it is usually preferable to re-scale these axes ahead of time e.g. by determining how much they vary when compared against other known good matches brown and winder a more general process which involves transforming feature vectors into a new scaled basis is called whitening and is discussed in more detail in the context of eigenface-based face recognition given a euclidean distance metric the simplest matching strategy is to set a threshold distance and to return all matches from other images within this threshold. setting the threshold too high results in too many false positives i.e. incorrect matches being returned. setting the threshold too low results in too many false negatives i.e. too many correct matches being missed we can quantify the performance of a matching algorithm at a particular threshold by points and patches figure recognizing objects in a cluttered scene springer. two of the training images in the database are shown on the left. these are matched to the cluttered scene in the middle using sift features shown as small squares in the right image. the affine warp of each recognized database image onto the scene is shown as a larger parallelogram in the right image. figure false positives and negatives the black digits and are features being matched against a database of features in other images. at the current threshold setting solid circles the green is a true positive match the blue is a false negative to match and the red is a false positive match. if we set the threshold higher dashed circles the blue becomes a true positive but the brown becomes an additional false positive. computer vision algorithms and applications draft table the number of matches correctly and incorrectly estimated by a feature matching algorithm showing the number of true positives false positives false negatives and true negatives the columns sum up to the actual number of positives and negatives while the rows sum up to the predicted number of positives and negatives the formulas for the true positive rate the false positive rate the positive predictive value and the accuracy are given in the text. first counting the number of true and false matches and match failures using the following definitions tp true positives i.e. number of correct matches fn false negatives matches that were not correctly detected fp false positives proposed matches that are incorrect tn true negatives non-matches that were correctly rejected. table shows a sample confusion matrix table containing such numbers. we can convert these numbers into unit rates by defining the following quantities true positive rate false positive rate tpr tp tpfn tp p fpr fp fptn fp n positive predictive value accuracy ppv tp tpfp tp p acc tptn pn predicted matchestp non-matchesfn matchestrue non-matches points and patches figure roc curve and its related rates the roc curve plots the true positive rate against the false positive rate for a particular combination of feature extraction and matching algorithms. ideally the true positive rate should be close to while the false positive rate is close to the area under the roc curve is often used as a single measure of algorithm performance. alternatively the equal error rate is sometimes used. the distribution of positives and negatives as a function of interfeature distance d. as the threshold is increased the number of true positives and false positives increases. in the information retrieval document retrieval literature and ribeironeto manning raghavan and sch utze the term precision many returned documents are relevant is used instead of ppv and recall fraction of relevant documents was found is used instead of tpr. any particular matching strategy a particular threshold or parameter setting can be rated by the tpr and fpr numbers ideally the true positive rate will be close to and the false positive rate close to as we vary the matching threshold we obtain a family of such points which are collectively known as the receiver operating characteristic curve the closer this curve lies to the upper left corner i.e. the larger the area under the curve the better its performance. figure shows how we can plot the number of matches and non-matches as a function of inter-feature distance d. these curves can then be used to plot an roc curve the roc curve can also be used to calculate the mean average precision which is the average precision as you vary the threshold to select the best results then the two top results etc. the problem with using a fixed threshold is that it is difficult to set the useful range false positive ratetrue positive error raterandom chancetpfpfntn d computer vision algorithms and applications draft figure fixed threshold nearest neighbor and nearest neighbor distance ratio matching. at a fixed distance threshold circles descriptor da fails to match db and dd incorrectly matches dc and de. if we pick the nearest neighbor da correctly matches db but dd incorrectly matches dc. using nearest neighbor distance ratio matching the small nndr correctly matches da with db and the large nndr correctly rejects matches for dd. of thresholds can vary a lot as we move to different parts of the feature space mikolajczyk and schmid a better strategy in such cases is to simply match the nearest neighbor in feature space. since some features may have no matches they may be part of background clutter in object recognition or they may be occluded in the other image a threshold is still used to reduce the number of false positives. ideally this threshold itself will adapt to different regions of the feature space. if sufficient training data is available brown and winder it is sometimes possible to learn different thresholds for different features. often however we are simply given a collection of images to match e.g. when stitching images or constructing models from unordered photo collections and lowe snavely seitz and szeliski in this case a useful heuristic can be to compare the nearest neighbor distance to that of the second nearest neighbor preferably taken from an image that is known not to match the target a different object in the database and lowe lowe we can define this nearest neighbor distance ratio and schmid as nndr db dc where and are the nearest and second nearest neighbor distances da is the target descriptor and db and dc are its closest two neighbors the effects of using these three different matching strategies for the feature descriptors evaluated by mikolajczyk and schmid are shown in figure as you can see the nearest neighbor and nndr strategies produce improved roc curves. points and patches figure performance of the feature descriptors evaluated by mikolajczyk and schmid ieee shown for three matching strategies fixed threshold nearest neighbor nearest neighbor distance ratio note how the ordering of the algorithms does not change that much but the overall performance varies significantly between the different matching strategies. precisioncorrect momentscross correlationsteerable filterscomplex filtersdifferential invariantsglohsiftpca siftshape contextspinhes lap precisioncorrect momentscross correlationsteerable filterscomplex filtersdifferential invariantsglohsiftpca siftshape contextspinhes lap precisioncorrect momentscross correlationsteerable filterscomplex filtersdifferential invariantsglohsiftpca siftshape contextspinhes lap computer vision algorithms and applications draft figure the three haar wavelet coefficients used for hashing the mops descriptor devised by brown szeliski and winder are computed by summing each normalized patch over the light and dark gray regions and taking their difference. efficient matching once we have decided on a matching strategy we still need to search efficiently for potential candidates. the simplest way to find all corresponding feature points is to compare all features against all other features in each pair of potentially matching images. unfortunately this is quadratic in the number of extracted features which makes it impractical for most applications. a better approach is to devise an indexing structure such as a multi-dimensional search tree or a hash table to rapidly search for features near a given feature. such indexing structures can either be built for each image independently is useful if we want to only consider certain potential matches e.g. searching for a particular object or globally for all the images in a given database which can potentially be faster since it removes the need to iterate over each image. for extremely large databases of images or more even more efficient structures based on ideas from document retrieval vocabulary trees er and stew enius can be used one of the simpler techniques to implement is multi-dimensional hashing which maps descriptors into fixed size buckets based on some function applied to each descriptor vector. at matching time each new feature is hashed into a bucket and a search of nearby buckets is used to return potential candidates which can then be sorted or graded to determine which are valid matches. a simple example of hashing is the haar wavelets used by brown szeliski and winder in their mops paper. during the matching structure construction each scaled oriented and normalized mops patch is converted into a three-element index by performing sums over different quadrants of the patch the resulting three values are normalized by their expected standard deviations and then mapped to the two b nearest bins. the three-dimensional indices formed by concatenating the three quantized values are used to index the bins where the feature is stored at query time only the primary indices are used so only a single three-dimensional bin needs to points and patches figure k-d tree and best bin first search and lowe ieee the spatial arrangement of the axis-aligned cutting planes is shown using dashed lines. individual data points are shown as small diamonds. the same subdivision can be represented as a tree where each interior node represents an axis-aligned cutting plane the top node cuts along dimension at value and each leaf node is a data point. during a bbf search a query point by first looks in its containing bin and then in its nearest adjacent bin rather than its closest neighbor in the tree be examined. the coefficients in the bin can then be used to select k approximate nearest neighbors for further processing as computing the nndr. a more complex but more widely applicable version of hashing is called locality sensitive hashing which uses unions of independently computed hashing functions to index the features indyk and motwani shakhnarovich darrell and indyk shakhnarovich viola and darrell extend this technique to be more sensitive to the distribution of points in parameter space which they call parameter-sensitive hashing. even more recent work converts high-dimensional descriptor vectors into binary codes that can be compared using hamming distances weiss and fergus weiss torralba and fergus or that can accommodate arbitrary kernel functions and grauman raginsky and lazebnik another widely used class of indexing structures are multi-dimensional search trees. the best known of these are k-d trees also often written as kd-trees which divide the multidimensional feature space along alternating axis-aligned hyperplanes choosing the threshold along each axis so as to maximize some criterion such as the search tree balance figure shows an example of a two-dimensional k-d tree. here eight different data points a h are shown as small diamonds arranged on a two-dimensional plane. the k-d tree computer vision algorithms and applications draft recursively splits this plane along axis-aligned or vertical cutting planes. each split can be denoted using the dimension number and split value the splits are arranged so as to try to balance the tree i.e. to keep its maximum depth as small as possible. at query time a classic k-d tree search first locates the query point in its appropriate bin and then searches nearby leaves in the tree b until it can guarantee that the nearest neighbor has been found. the best bin first search and lowe searches bins in order of their spatial proximity to the query point and is therefore usually more efficient. many additional data structures have been developed over the years for solving nearest neighbor problems mount netanyahu et al. liang liu xu et al. hjaltason and samet for example nene and nayar developed a technique they call slicing that uses a series of binary searches on the point list sorted along different dimensions to efficiently cull down a list of candidate points that lie within a hypercube of the query point. grauman and darrell reweight the matches at different levels of an indexing tree which allows their technique to be less sensitive to discretization errors in the tree construction. nist er and stew enius use a metric tree which compares feature descriptors to a small number of prototypes at each level in a hierarchy. the resulting quantized visual words can then be used with classical information retrieval relevance techniques to quickly winnow down a set of potential candidates from a database of millions of images muja and lowe compare a number of these approaches introduce a new one of their own search on hierarchical k-means trees and conclude that multiple randomized k-d trees often provide the best performance. despite all of this promising work the rapid computation of image feature correspondences remains a challenging open research problem. feature match verification and densification once we have some hypothetical matches we can often use geometric alignment to verify which matches are inliers and which ones are outliers. for example if we expect the whole image to be translated or rotated in the matching view we can fit a global geometric transform and keep only those feature matches that are sufficiently close to this estimated transformation. the process of selecting a small set of seed matches and then verifying a larger set is often called random sampling or ransac once an initial set of correspondences has been established some systems look for additional matches e.g. by looking for additional correspondences along epipolar lines or in the vicinity of estimated locations based on the global transform. these topics are discussed further in sections and points and patches feature tracking an alternative to independently finding features in all candidate images and then matching them is to find a set of likely feature locations in a first image and to then search for their corresponding locations in subsequent images. this kind of detect then track approach is more widely used for video tracking applications where the expected amount of motion and appearance deformation between adjacent frames is expected to be small. the process of selecting good features to track is closely related to selecting good features for more general recognition applications. in practice regions containing high gradients in both directions i.e. which have high eigenvalues in the auto-correlation matrix provide stable locations at which to find correspondences and tomasi in subsequent frames searching for locations where the corresponding patch has low squared difference often works well enough. however if the images are undergoing brightness change explicitly compensating for such variations or using normalized cross-correlation may be preferable. if the search range is large it is also often more efficient to use a hierarchical search strategy which uses matches in lower-resolution images to provide better initial guesses and hence speed up the search alternatives to this strategy involve learning what the appearance of the patch being tracked should be and then searching for it in the vicinity of its predicted position jurie and dhome williams blake and cipolla these topics are all covered in more detail in section if features are being tracked over longer image sequences their appearance can undergo larger changes. you then have to decide whether to continue matching against the originally detected patch or to re-sample each subsequent frame at the matching location. the former strategy is prone to failure as the original patch can undergo appearance changes such as foreshortening. the latter runs the risk of the feature drifting from its original location to some other location in the image and tomasi small misregistration errors compound to create a markov random walk which leads to larger drift over time. a preferable solution is to compare the original patch to later image locations using an affine motion model shi and tomasi first compare patches in neighboring frames using a translational model and then use the location estimates produced by this step to initialize an affine registration between the patch in the current frame and the base frame where a feature was first detected in their system features are only detected infrequently i.e. only in regions where tracking has failed. in the usual case an area around the current predicted location of the feature is searched with an incremental registration algorithm the resulting tracker is often called the kanade lucas tomasi tracker. computer vision algorithms and applications draft figure feature tracking using an affine motion model and tomasi ieee top row image patch around the tracked feature location. bottom row image patch after warping back toward the first frame using an affine deformation. even though the speed sign gets larger from frame to frame the affine transformation maintains a good resemblance between the original and subsequent tracked frames. since their original work on feature tracking shi and tomasi s approach has generated a string of interesting follow-on papers and applications. beardsley torr and zisserman use extended feature tracking combined with structure from motion to incrementally build up sparse models from video sequences. kang szeliski and shum tie together the corners of adjacent gridded patches to provide some additional stability to the tracking at the cost of poorer handling of occlusions. tommasini fusiello trucco et al. provide a better spurious match rejection criterion for the basic shi and tomasi algorithm collins and liu provide improved mechanisms for feature selection and dealing with larger appearance changes over time and shafique and shah develop algorithms for feature matching association for videos with large numbers of moving objects or points. yilmaz javed and shah and lepetit and fua survey the larger field of object tracking which includes not only feature-based techniques but also alternative techniques based on contour and region one of the newest developments in feature tracking is the use of learning algorithms to build special-purpose recognizers to rapidly search for matching features anywhere in an image pilet and fua hinterstoisser benhimane navab et al. rogez rihan ramalingam et al. ozuysal calonder lepetit et al. by taking the time to train classifiers on sample patches and their affine deformations extremely fast and reliable feature detectors can be constructed which enables much faster motions to be supported coupling such features to deformable models lepetit and fua or structure-from-motion algorithms and murray can result in even higher stability. see also my previous comment on earlier work in learning-based tracking jurie and dhome williams blake and cipolla points and patches figure real-time head tracking using the fast trained classifiers of lepetit pilet and fua ieee. application performance-driven animation one of the most compelling applications of fast feature tracking is performance-driven animation i.e. the interactive deformation of a graphics model based on tracking a user s motions litwinowicz and williams lepetit pilet and fua buck finkelstein jacobs et al. present a system that tracks a user s facial expressions and head motions and then uses them to morph among a series of hand-drawn sketches. an animator first extracts the eye and mouth regions of each sketch and draws control lines over each image at run time a face-tracking system determines the current location of these features the animation system decides which input images to morph based on nearest neighbor feature appearance matching and triangular barycentric interpolation. it also computes the global location and orientation of the head from the tracked features. the resulting morphed eye and mouth regions are then composited back into the overall head model to yield a frame of hand-drawn animation in more recent work barnes jacobs sanders et al. watch users animate paper cutouts on a desk and then turn the resulting motions and drawings into seamless animations. computer vision algorithms and applications draft figure performance-driven hand-drawn animation finkelstein jacobs et al. acm eye and mouth portions of hand-drawn sketch with their overlaid control lines an input video frame with the tracked features overlaid a different input video frame along with its corresponding hand-drawn animation. edges while interest points are useful for finding image locations that can be accurately matched in edge points are far more plentiful and often carry important semantic associations. for example the boundaries of objects which also correspond to occlusion events in are usually delineated by visible contours. other kinds of edges correspond to shadow boundaries or crease edges where surface orientation changes rapidly. isolated edge points can also be grouped into longer curves or contours as well as straight line segments it is interesting that even young children have no difficulty in recognizing familiar objects or animals from such simple line drawings. edge detection given an image how can we find the salient edges? consider the color images in figure if someone asked you to point out the most salient or strongest edges or the object boundaries fowlkes and malik arbel aez maire fowlkes et al. which ones would you trace? how closely do your perceptions match the edge images shown in figure qualitatively edges occur at boundaries between regions of different color intensity or texture. unfortunately segmenting an image into coherent regions is a difficult task which we address in chapter often it is preferable to detect edges using only purely local information. under such conditions a reasonable approach is to define an edge as a location of rapid edges figure human boundary detection fowlkes and malik ieee. the darkness of the edges corresponds to how many human subjects marked an object boundary at that location. intensity think of an image as a height field. on such a surface edges occur at locations of steep slopes or equivalently in regions of closely packed contour lines a topographic map. a mathematical way to define the slope and direction of a surface is through its gradient jx ix i x i y the local gradient vector j points in the direction of steepest ascent in the intensity function. its magnitude is an indication of the slope or strength of the variation while its orientation points in a direction perpendicular to the local contour. unfortunately taking image derivatives accentuates high frequencies and hence amplifies noise since the proportion of noise to signal is larger at high frequencies. it is therefore prudent to smooth the image with a low-pass filter prior to computing the gradient. because we would like the response of our edge detector to be independent of orientation a circularly symmetric smoothing filter is desirable. as we saw in section the gaussian is the only separable circularly symmetric filter and so it is used in most edge detection algorithms. canny discusses alternative filters and a number of researcher review alternative edge detection algorithms and compare their performance nalwa and binford nalwa deriche freeman and adelson nalwa heath sarkar sanocki et al. crane ritter and wilson bowyer kranenburg and dougherty arbel aez maire fowlkes et al. because differentiation is a linear operation it commutes with other linear filtering oper we defer the topic of edge detection in color images. computer vision algorithms and applications draft ations. the gradient of the smoothed image can therefore be written as j ix g ix i.e. we can convolve the image with the horizontal and vertical derivatives of the gaussian kernel function g g x g y x y parameter indicates the width of the gaussian. this is the same computation that is performed by freeman and adelson s first-order steerable filter which we already covered in section for many applications however we wish to thin such a continuous gradient image to only return isolated edges i.e. as single pixels at discrete locations along the edge contours. this can be achieved by looking for maxima in the edge strength magnitude in a direction perpendicular to the edge orientation i.e. along the gradient direction. finding this maximum corresponds to taking a directional derivative of the strength field in the direction of the gradient and then looking for zero crossings. the desired directional derivative is equivalent to the dot product between a second gradient operator and the results of the first s j ix. the gradient operator dot product with the gradient is called the laplacian. the convolution kernel is therefore called the laplacian of gaussian kernel and hildreth this kernel can be split into two separable parts g g buxton and buxton which allows for a much more efficient implementation using separable filtering in practice it is quite common to replace the laplacian of gaussian convolution with a difference of gaussian computation since the kernel shapes are qualitatively similar this is especially convenient if a laplacian pyramid has already been recall that burt and adelson s laplacian pyramid actually computed differences of gaussian-filtered levels. edges in fact it is not strictly necessary to take differences between adjacent levels when computing the edge field. think about what a zero crossing in a generalized difference of gaussians image represents. the finer kernel gaussian is a noise-reduced version of the original image. the coarser kernel gaussian is an estimate of the average intensity over a larger region. thus whenever the dog image changes sign this corresponds to the blurred image going from relatively darker to relatively lighter as compared to the average intensity in that neighborhood. once we have computed the sign function sx we must find its zero crossings and convert these into edge elements an easy way to detect and represent zero crossings is to look for adjacent pixel locations xi and xj where the sign changes value i.e. the sub-pixel location of this crossing can be obtained by computing the x-intercept of the line connecting sxi and sxj xz xisxj xjsxi sxj sxi the orientation and strength of such edgels can be obtained by linearly interpolating the gradient values computed on the original pixel grid. an alternative edgel representation can be obtained by linking adjacent edgels on the dual grid to form edgels that live inside each square formed by four adjacent pixels in the original pixel the advantage of this representation is that the edgels now live on a grid offset by half a pixel from the original pixel grid and are thus easier to store and access. as before the orientations and strengths of the edges can be computed by interpolating the gradient field or estimating these values from the difference of gaussian image exercise in applications where the accuracy of the edge orientation is more important higher-order steerable filters can be used and adelson section such filters are more selective for more elongated edges and also have the possibility of better modeling curve intersections because they can represent multiple orientations at the same pixel their disadvantage is that they are more expensive to compute and the directional derivative of the edge strength does not have a simple closed form computer vision algorithms and applications draft figure scale selection for edge detection and zucker ieee original image c cannyderiche edge detector tuned to the finer and coarser scales minimum reliable scale for gradient estimation minimum reliable scale for second derivative estimation final detected edges. scale selection and blur estimation as we mentioned before the derivative laplacian and difference of gaussian filters all require the selection of a spatial scale parameter if we are only interested in detecting sharp edges the width of the filter can be determined from image noise characteristics elder and zucker however if we want to detect edges that occur at different resolutions c a scale-space approach that detects and then selects edges at different scales may be necessary lindeberg nielsen florack and deriche elder and zucker present a principled approach to solving this problem. given a known image noise level their technique computes for every pixel the minimum scale at which an edge can be reliably detected their approach first computes this algorithm is a version of the marching cubes isosurface extraction algorithm and cline in fact the edge orientation can have a ambiguity for bar edges which makes the computation of zero crossings in the derivative more tricky. edges gradients densely over an image by selecting among gradient estimates computed at different scales based on their gradient magnitudes. it then performs a similar estimate of minimum scale for directed second derivatives and uses zero crossings of this latter quantity to robustly select edges f. as an optional final step the blur width of each edge can be computed from the distance between extrema in the second derivative response minus the width of the gaussian filter. color edge detection while most edge detection techniques have been developed for grayscale images color images can provide additional information. for example noticeable edges between iso-luminant colors that have the same luminance are useful cues but fail to be detected by grayscale edge operators. one simple approach is to combine the outputs of grayscale detectors run on each color band however some care must be taken. for example if we simply sum up the gradients in each of the color bands the signed gradients may actually cancel each other! for example a pure red-to-green edge. we could also detect edges independently in each band and then take the union of these but this might lead to thickened or doubled edges that are hard to link. a better approach is to compute the oriented energy in each band and burr perona and malik e.g. using a second-order steerable filter and adelson and then sum up the orientation-weighted energies and find their joint best orientation. unfortunately the directional derivative of this energy may not have a closed form solution in the case of signed first-order steerable filters so a simple zero crossing-based strategy cannot be used. however the technique described by elder and zucker can be used to compute these zero crossings numerically instead. an alternative approach is to estimate local color statistics in regions around each pixel and tomasi martin fowlkes and malik this has the advantage that more sophisticated techniques color histograms can be used to compare regional statistics and that additional measures such as texture can also be considered. figure shows the output of such detectors. of course many other approaches have been developed for detecting color edges dating back to early work by nevatia ruzon and tomasi and gevers van de weijer and stokman provide good reviews of these approaches which include ideas such as fusing outputs from multiple channels using multidimensional gradients and vector-based instead of using the raw rgb space a more perceptually uniform color space such as lab section can be used instead. when trying to match human performance fowlkes and malik this makes sense. however in terms of the physics of the underlying image formation and sensing it may be a questionable strategy. computer vision algorithms and applications draft methods. combining edge feature cues if the goal of edge detection is to match human boundary detection performance kranenburg and dougherty martin fowlkes and malik arbel aez maire fowlkes et al. as opposed to simply finding stable features for matching even better detectors can be constructed by combining multiple low-level cues such as brightness color and texture. martin fowlkes and malik describe a system that combines brightness color and texture edges to produce state-of-the-art performance on a database of hand-segmented natural color images fowlkes tal et al. first they construct and separate oriented half-disc detectors for measuring significant differences in brightness color and b channels summed responses and texture filter bank responses from the work of malik belongie leung et al. some of the responses are then sharpened using a soft non-maximal suppression technique. finally the outputs of the three detectors are combined using a variety of machine-learning techniques from which logistic regression is found to have the best tradeoff between speed space and accuracy the resulting system figure for some examples is shown to outperform previously developed techniques. maire arbelaez fowlkes et al. improve on these results by combining the detector based on local appearance with a spectral dein more recent work arbel aez maire fowlkes et al. tector and malik build a hierarchical segmentation on top of this edge detector using a variant of the watershed algorithm. edge linking while isolated edges can be useful for a variety of applications such as line detection and sparse stereo matching they become even more useful when linked into continuous contours. if the edges have been detected using zero crossings of some function linking them up is straightforward since adjacent edgels share common endpoints. linking the edgels into chains involves picking up an unlinked edgel and following its neighbors in both directions. either a sorted list of edgels first by x coordinates and then by y coordinates for example or a array can be used to accelerate the neighbor finding. if edges were not detected using zero crossings finding the continuation of an edgel can be tricky. in this case comparing the orientation optionally phase of adjacent edgels can be used for the training uses labeled images and testing is performed on a different set of images. edges figure combined brightness color texture boundary detector fowlkes and malik ieee. successive rows show the outputs of the brightness gradient color gradient texture gradient and combined detectors. the final row shows human-labeled boundaries derived from a database of hand-segmented images fowlkes tal et al. computer vision algorithms and applications draft figure chain code representation of a grid-aligned linked edge chain. the code is represented as a series of direction codes e.g which can further be compressed using predictive and run-length coding. disambiguation. ideas from connected component computation can also sometimes be used to make the edge linking process even faster exercise once the edgels have been linked into chains we can apply an optional thresholding with hysteresis to remove low-strength contour segments the basic idea of hysteresis is to set two different thresholds and allow a curve being tracked above the higher threshold to dip in strength down to the lower threshold. linked edgel lists can be encoded more compactly using a variety of alternative representations. a chain code encodes a list of connected points lying on an grid using a three-bit code corresponding to the eight cardinal directions ne e se s sw w nw between a point and its successor while this representation is more compact than the original edgel list if predictive variable-length coding is used it is not very suitable for further processing. a more useful representation is the arc length parameterization of a contour xs where s denotes the arc length along a curve. consider the linked set of edgels shown in figure we start at one point dot at in figure and plot it at coordinate s the next point at gets plotted at s and the next point at gets plotted at s i.e. we increment s by the length of each edge segment. the resulting plot can be resampled on a regular integral s grid before further processing. the advantage of the arc-length parameterization is that it makes matching and processing smoothing operations much easier. consider the two curves describing similar shapes shown in figure to compare the curves we first subtract the average values xs from each descriptor. next we rescale each descriptor so that s goes from to instead of to s i.e. we divide xs by s. finally we take the fourier transform of each n edges figure arc-length parameterization of a contour discrete points along the contour are first transcribed as y pairs along the arc length s. this curve can then be regularly re-sampled or converted into alternative fourier representations. figure matching two contours using their arc-length parameterization. if both curves are normalized to unit length s and centered around their centroid they will have the same descriptor up to an overall temporal shift to different starting points for s and a phase shift to rotation. computer vision algorithms and applications draft figure curve smoothing with a gaussian kernel ieee without a shrinkage correction term with a shrinkage correction term. figure changing the character of a curve without affecting its sweep and salesin acm higher frequency wavelets can be replaced with exemplars from a style library to effect different local appearances. normalized descriptor treating each x y value as a complex number. if the original curves are the same to an unknown scale and rotation the resulting fourier transforms should differ only by a scale change in magnitude plus a constant complex phase shift due to rotation and a linear phase shift in the domain due to different starting points for s exercise arc-length parameterization can also be used to smooth curves in order to remove digitization noise. however if we just apply a regular smoothing filter the curve tends to shrink on itself lowe and taubin describe techniques that compensate for this shrinkage by adding an offset term based on second derivative estimates or a larger smoothing kernel an alternative approach based on selectively modifying different frequencies in a wavelet decomposition is presented by finkelstein and salesin in addition to controlling shrinkage without affecting its sweep wavelets allow the character of a curve to be interactively modified as shown in figure the evolution of curves as they are smoothed and simplified is related to grassfire edges figure image editing in the contour domain and goldberg ieee and original images and extracted edges to be deleted are marked in white and reconstructed edited images. tance transforms and region skeletons and kimia and can be used to recognize objects based on their contour shape and kimia more local descriptors of curve shape such as shape contexts malik and puzicha can also be used for recognition and are potentially more robust to missing parts due to occlusions. the field of contour detection and linking continues to evolve rapidly and now includes techniques for global contour grouping boundary completion and junction detection arbelaez fowlkes et al. as well as grouping contours into likely regions aez maire fowlkes et al. and wide-baseline correspondence and soatto application edge editing and enhancement while edges can serve as components for object recognition or features for matching they can also be used directly for image editing. in fact if the edge magnitude and blur estimate are kept along with each edge a visually similar image can be reconstructed from this information based on this principle elder and goldberg propose a system for image editing in the contour domain their system allows users to selectively remove edges corresponding to unwanted features such as specularities shadows or distracting visual elements. after reconstructing the image from the remaining edges the undesirable visual features have been removed computer vision algorithms and applications draft figure approximating a curve in black as a polyline or b-spline original curve and a polyline approximation shown in red successive approximation by recursively finding points furthest away from the current approximation smooth interpolating spline shown in dark blue fit to the polyline vertices. another potential application is to enhance perceptually salient edges while simplifying the underlying image to produce a cartoon-like or pen-and-ink stylized image and santella this application is discussed in more detail in section lines while edges and general curves are suitable for describing the contours of natural objects the man-made world is full of straight lines. detecting and matching these lines can be useful in a variety of applications including architectural modeling pose estimation in urban environments and the analysis of printed document layouts. in this section we present some techniques for extracting piecewise linear descriptions from the curves computed in the previous section. we begin with some algorithms for approximating a curve as a piecewise-linear polyline. we then describe the hough transform which can be used to group edgels into line segments even across gaps and occlusions. finally we describe how lines with common vanishing points can be grouped together. these vanishing points can be used to calibrate a camera and to determine its orientation relative to a rectahedral scene as described in section successive approximation as we saw in section describing a curve as a series of locations xi xsi provides a general representation suitable for matching and further processing. in many applications however it is preferable to approximate such a curve with a simpler representation e.g. as a piecewise-linear polyline or as a b-spline curve as shown in figure many techniques have been developed over the years to perform this approximation which is also known as line simplification. one of the oldest and simplest is the one proposed lines figure original hough transform each point votes for a complete family of potential lines ri xi cos yi sin each pencil of lines sweeps out a sinusoid in their intersection provides the desired line equation. by ramer and douglas and peucker who recursively subdivide the curve at the point furthest away from the line joining the two endpoints the current coarse polyline approximation as shown in figure hershberger and snoeyink provide a more efficient implementation and also cite some of the other related work in this area. once the line simplification has been computed it can be used to approximate the original curve. if a smoother representation or visualization is desired either approximating or interpolating splines or curves can be used and and ito bartels beatty and barsky farin as shown in figure hough transforms while curve approximation with polylines can often lead to successful line extraction lines in the real world are sometimes broken up into disconnected components or made up of many collinear line segments. in many cases it is desirable to group such collinear segments into extended lines. at a further processing stage in section we can then group such lines into collections with common vanishing points. the hough transform named after its original inventor is a well-known technique for having edges vote for plausible line locations and hart ballard illingworth and kittler in its original formulation each edge point votes for all possible lines passing through it and lines corresponding to high accumulator or bin values are examined for potential line unless the points on a line are truly punctate a better approach my experience is to use the local orientation information at each edgel to vote for a single accumulator cell as described below. a hybrid strategy the hough transform can also be generalized to look for other geometric features such as circles but we do not cover such extensions in this book. iri computer vision algorithms and applications draft figure oriented hough transform an edgel re-parameterized in polar coordinates with ni i sin i and ri ni xi accumulator array showing the votes for the three edgels marked in red green and blue. figure line equation expressed in terms of the normal n and distance to the origin d. where each edgel votes for a number of possible orientation or location pairs centered around the estimate orientation may be desirable in some cases. before we can vote for line hypotheses we must first choose a suitable representation. figure from figure shows the normal-distance n d parameterization for a line. since lines are made up of edge segments we adopt the convention that the line normal n points in the same direction has the same sign as the image gradient jx ix to obtain a minimal two-parameter representation for lines we convert the normal vector into an angle tan nynx as shown in figure the range of possible d values is assuming that we are using normalized pixel coordinates that lie in the number of bins to use along each axis depends on the accuracy of the position and orientation estimate available at each edgel and the expected line density and is best set experimentally with some test runs on sample imagery. given the line parameterization the hough transform proceeds as shown in algorithm iri nl lines procedure houghx y clear the accumulator array. for each detected edgel at location y and orientation tan nynx compute the value of d x nx y ny and increment the accumulator corresponding to d. find the peaks in the accumulator corresponding to lines. optionally re-fit the lines to the constituent edgels. algorithm outline of a hough transform algorithm based on oriented edge segments. note that the original formulation of the hough transform which assumed no knowledge of the edgel orientation has an additional loop inside step that iterates over all possible values of and increments a whole series of accumulators. there are a lot of details in getting the hough transform to work well but these are best worked out by writing an implementation and testing it out on sample data. exercise describes some of these steps in more detail including using edge segment lengths or strengths during the voting process keeping a list of constituent edgels in the accumulator array for easier post-processing and optionally combining edges of different polarity into the same line segments. an alternative to the polar d representation for lines is to use the full m n d line equation projected onto the unit sphere. while the sphere can be parameterized using spherical coordinates m cos sin cos sin this does not uniformly sample the sphere and still requires the use of trigonometry. an alternative representation can be obtained by using a cube map i.e. projecting m onto the face of a unit cube to compute the cube map coordinate of a vector m first find the largest value component of m i.e. m maxnxnyd and use this to select one of the six cube faces. divide the remaining two coordinates by m and use these as indices into the cube face. while this avoids the use of trigonometry it does require some decision logic. one advantage of using the cube map first pointed out by tuytelaars van gool and proesmans is that all of the lines passing through a point correspond to line segments computer vision algorithms and applications draft figure cube map representation for line equations and vanishing points a cube map surrounding the unit sphere projecting the half-cube onto three subspaces van gool and proesmans ieee. on the cube faces which is useful if the original voting variant of the hough transform is being used. in their work they represent the line equation as ax b y which does not treat the x and y axes symmetrically. note that if we restrict d by ignoring the polarity of the edge orientation sign we can use a half-cube instead which can be represented using only three cube faces as shown in figure van gool and proesmans ransac-based line detection. another alternative to the hough transform is the random sample consensus algorithm described in more detail in section in brief ransac randomly chooses pairs of edgels to form a line hypothesis and then tests how many other edgels fall onto this line. the edge orientations are accurate enough a single edgel can produce this hypothesis. lines with sufficiently large numbers of inliers edgels are then selected as the desired line segments. an advantage of ransac is that no accumulator array is needed and so the algorithm can be more space efficient and potentially less prone to the choice of bin size. the disadvantage is that many more hypotheses may need to be generated and tested than those obtained by finding peaks in the accumulator array. in general there is no clear consensus on which line estimation technique performs best. it is therefore a good idea to think carefully about the problem at hand and to implement several approaches approximation hough and ransac to determine the one that works best for your application. vanishing points in many scenes structurally important lines have the same vanishing point because they are parallel in examples of such lines are horizontal and vertical building edges zebra crossings railway tracks the edges of furniture such as tables and dressers and of course the ubiquitous calibration pattern finding the vanishing points common to such lines figure real-world vanishing points architecture steedly szeliski et al. furniture cu s k wildenauer and ko seck a ieee and calibration patterns line sets can help refine their position in the image and in certain cases help determine the intrinsic and extrinsic orientation of the camera over the years a large number of techniques have been developed for finding vanishing points including and mohr collins and weiss brillaut-o mahoney mclean and kotturi becker and bove shufelt tuytelaars van gool and proesmans schaffalitzky and zisserman antone and teller rother ko seck a and zhang pflugfelder tardif see some of the more recent papers for additional references. in this section we present a simple hough technique based on having line pairs vote for potential vanishing point locations followed by a robust least squares fitting stage. for alternative approaches please see some of the more recent papers listed above. the first stage in my vanishing point detection algorithm uses a hough transform to accumulate votes for likely vanishing point candidates. as with line fitting one possible approach is to have each line vote for all possible vanishing point directions either using a cube map van gool and proesmans antone and teller or a gaussian sphere and weiss optionally using knowledge about the uncertainty in the vanishing point location to perform a weighted vote and weiss brillaut-o mahoney shufelt my preferred approach is to use pairs of detected line segments to form candidate vanishing point locations. let mi and mj be the norm line equations for a pair of line segments and li and lj be their corresponding segment lengths. the location of the corresponding vanishing point hypothesis can be computed as and the corresponding weight set to vij mi mj wij computer vision algorithms and applications draft figure triple product of the line segments endpoints and and the vanishing point v. the area a is proportional to the perpendicular distance and the distance between the other endpoint and the vanishing point. this has the desirable effect of downweighting line segments and short line segments. the hough space itself can either be represented using spherical coordinates or as a cube map once the hough accumulator space has been populated peaks can be detected in a manner similar to that previously discussed for line detection. given a set of candidate line segments that voted for a vanishing point which can optionally be kept as a list at each hough accumulator cell i then use a robust least squares fit to estimate a more accurate location for each vanishing point. consider the relationship between the two line segment endpoints and the vanishing point v as shown in figure the area a of the triangle given by these three points which is the magnitude of their triple product ai v is proportional to the perpendicular distance between each endpoint and the line through v and the other endpoint as well as the distance between and v. assuming that the accuracy of a fitted line segment is proportional to its endpoint accuracy this therefore serves as an optimal metric for how well a vanishing point fits a set of extracted lines section and pflugfelder section a robustified least squares estimate for the vanishing point can therefore be written as wiaimimt v vt m v e where mi is the segment line equation weighted by its length li and wi is the influence of each robustified measurement on the final error notice how this metric is closely related to the original formula for the pairwise weighted hough transform accumulation step. the final desired value for v is computed as the least eigenvector of m. additional reading while the technique described above proceeds in two discrete stages better results may be obtained by alternating between assigning lines to vanishing points and refitting the vanishing point locations and teller ko seck a and zhang pflugfelder the results of detecting individual vanishing points can also be made more robust by simultaneously searching for pairs or triplets of mutually orthogonal vanishing points antone and teller rother sinha steedly szeliski et al. some results of such vanishing point detection algorithms can be seen in figure application rectangle detection once sets of mutually orthogonal vanishing points have been detected it now becomes possible to search for rectangular structures in the image over the last decade a variety of techniques have been developed to find such rectangles primarily focused on architectural scenes seck a and zhang han and zhu shaw and barnes mi cu s k wildenauer and ko seck a schindler krishnamurthy lublinerman et al. after detecting orthogonal vanishing directions ko seck a and zhang refine the fitted line equations search for corners near line intersections and then verify rectangle hypotheses by rectifying the corresponding patches and looking for a preponderance of horizontal and vertical edges b. in follow-on work mi cu s k wildenauer and ko seck a use a markov random field to disambiguate between potentially overlapping rectangle hypotheses. they also use a plane sweep algorithm to match rectangles between different views f. a different approach is proposed by han and zhu who use a grammar of potential rectangle shapes and nesting structures rectangles and vanishing points to infer the most likely assignment of line segments to rectangles additional reading one of the seminal papers on feature detection description and matching is by lowe comprehensive surveys and evaluations of such techniques have been made by schmid mohr and bauckhage mikolajczyk and schmid mikolajczyk tuytelaars schmid et al. tuytelaars and mikolajczyk while shi and tomasi and triggs also provide nice reviews. in the area of feature detectors tuytelaars schmid et al. in addition to such classic approaches as f orstner harris orstner harris and stephens and difference of gaussians lowe maximally stable extremal regions are widely used for applications that require affine invariance chum computer vision algorithms and applications draft figure rectangle detection indoor corridor and building exterior with grouped facades seck a and zhang elsevier grammar-based recognition and zhu ieee f rectangle matching using a plane sweep algorithm cu s k wildenauer and ko seck a ieee. urban et al. nist er and stew enius more recent interest point detectors are discussed by xiao and shah koethe carneiro and jepson kenney zuliani and manjunath bay tuytelaars and van gool platel balmachnova florack et al. rosten and drummond as well as techniques based on line matching faugeras and deriche bartoli coquerelle and sturm and region detection zisserman and brady matas chum urban et al. tuytelaars and van gool corso and hager a variety of local feature descriptors matching heuristics are surveyed and compared by mikolajczyk and schmid more recent publications in this area include those by van de weijer and schmid abdel-hakim and farag winder and brown hua brown and winder techniques for efficiently matching features include k-d trees and lowe lowe muja and lowe pyramid matching kernels and darrell metric trees er and stew enius and a variety of multi-dimensional hashing techniques viola and darrell torralba weiss and fergus weiss torralba and fergus kulis and exercises grauman raginsky and lazebnik the classic reference on feature detection and tracking is and tomasi more recent work in this field has focused on learning better matching functions for specific features jurie and dhome williams blake and cipolla lepetit and fua lepetit pilet and fua hinterstoisser benhimane navab et al. rogez rihan ramalingam et al. ozuysal calonder lepetit et al. a highly cited and widely used edge detector is the one developed by canny alternative edge detectors as well as experimental comparisons can be found in publications by nalwa and binford nalwa deriche freeman and adelson nalwa heath sarkar sanocki et al. crane ritter and wilson bowyer kranenburg and dougherty arbel aez maire fowlkes et al. the topic of scale selection in edge detection is nicely treated by elder and zucker while approaches to color and texture edge detection can be found in and tomasi martin fowlkes and malik gevers van de weijer and stokman edge detectors have also recently been combined with region segmentation techniques to further improve the detection of semantically salient boundaries arbelaez fowlkes et al. arbel aez maire fowlkes et al. edges linked into contours can be smoothed and manipulated for artistic effect finkelstein and salesin taubin and used for recognition malik and puzicha tek and kimia sebastian and kimia an early well-regarded paper on straight line extraction in images was written by burns hanson and riseman more recent techniques often combine line detection with vanishing point detection and mohr collins and weiss brillaut-o mahoney mclean and kotturi becker and bove shufelt tuytelaars van gool and proesmans schaffalitzky and zisserman antone and teller rother ko seck a and zhang pflugfelder sinha steedly szeliski et al. tardif exercises ex interest point detector their performance your own or with a classmate s detector. implement one or more keypoint detectors and compare possible detectors laplacian or difference of gaussian f orstner harris hessian different formula variants given in computer vision algorithms and applications draft orientedsteerable filter looking for either second-order high second response or two edges in a window as discussed in section other detectors are described by mikolajczyk tuytelaars schmid et al. tuytelaars and mikolajczyk additional optional steps could include compute the detections on a sub-octave pyramid and find maxima. find local orientation estimates using steerable filter responses or a gradient histogram ming method. implement non-maximal suppression such as the adaptive technique of brown szeliski and winder vary the window shape and size and aggregation. to test for repeatability download the code from httpwww.robots.ox.ac.uk vggresearch affine tuytelaars schmid et al. tuytelaars and mikolajczyk or simply rotate or shear your own test images. a domain you may want to use later e.g. for outdoor stitching. be sure to measure and report the stability of your scale and orientation estimates. ex interest point descriptor implement one or more descriptors to local scale and orientation and compare their performance your own or with a classmate s detector. some possible descriptors include contrast-normalized patches szeliski and winder sift gloh and schmid daisy and brown tola lepetit and fua other detectors are described by mikolajczyk and schmid ex roc curve computation given a pair of curves plotting the number of matching and non-matching features as a function of euclidean distance d as shown in figure derive an algorithm for plotting a roc curve in particular let td be the distribution of true matches and fd be the distribution of non-matches. write down the equations for the roc i.e. tprfpr and the auc. plot the cumulative distributions t td and f fd and see if these help you derive the tpr and fpr at a given threshold exercises ex feature matcher after extracting features from a collection of overlapping or distorted match them up by their descriptors either using nearest neighbor matching or a more efficient matching strategy such as a k-d tree. see whether you can improve the accuracy of your matches using techniques such as the nearest neighbor distance ratio. ex feature tracker instead of finding feature points independently in multiple images and then matching them find features in the first image of a video or image sequence and then re-locate the corresponding points in the next frames using either search and gradient descent and tomasi or learned feature detectors pilet and fua fossati dimitrijevic lepetit et al. when the number of tracked points drops below a threshold or new regions in the image become visible find additional points to track. winnow out incorrect matches by estimating a homography or fundamental matrix refine the accuracy of your matches using the iterative registration algorithm described in section and exercise ex facial feature tracker apply your feature tracker to tracking points on a person s face either manually initialized to interesting locations such as eye corners or automatically initialized at interest points. match features between two people and use these features to perform image morphing ex edge detector mance to that of your classmates detectors or code downloaded from the internet. implement an edge detector of your choice. compare its perfor a simple but well-performing sub-pixel edge detector can be created as follows blur the input image a little b g ix. construct a gaussian pyramid p pyramidb subtract an interpolated coarser-level pyramid image from the original resolution blurred image httpwww.robots.ox.ac.uk vggresearchaffine. sx b p.interpolatedlevell. computer vision algorithms and applications draft struct sedgel float float x y float n_x n_y float theta float length float strength edgel endpoints crossing sub-pixel edge position orientation as normal vector orientation as angle length of edgel strength of edgel magnitude struct sline public sedgel float line_length length of line from ellipsoid float sigma float r estimated std. dev. of edgel noise line equation x n_y y n_x r figure a potential c structure for edgel and line elements. for each quad of pixels j j j j count the number of zero crossings along the four edges. when there are exactly two zero crossings compute their locations using and store these edgel endpoints along with the midpoint in the edgel structure for each edgel compute the local gradient by taking the horizontal and vertical differ ences between the values of s along the zero crossing edges. store the magnitude of this gradient as the edge strength and either its orientation or that of the segment joining the edgel endpoints as the edge orientation. add the edgel to a list of edgels or store it in a array of edgels by pixel coordinates. figure shows a possible representation for each computed edgel. ex edge linking and thresholding link up the edges computed in the previous exercise into chains and optionally perform thresholding with hysteresis. the steps may include store the edgels either in a array an integer image with indices into the edgel list or pre-sort the edgel list first by x coordinates and then y coordinates for faster neighbor finding. exercises pick up an edgel from the list of unlinked edgels and find its neighbors in both directions until no neighbor is found or a closed contour is obtained. flag edgels as linked as you visit them and push them onto your list of linked edgels. alternatively generalize a previously developed connected component algorithm ercise to perform the linking in just two raster passes. perform hysteresis-based thresholding use two thresholds hi and lo for the edge strength. a candidate edgel is considered an edge if either its strength is above the hi threshold or its strength is above the lo threshold and it is connected to a previously detected edge. link together contours that have small gaps but whose endpoints have sim ilar orientations. find junctions between adjacent contours e.g. using some of the ideas references from maire arbelaez fowlkes et al. ex contour matching convert a closed contour edgel list into its arc-length parameterization and use this to match object outlines. the steps may include walk along the contour and create a list of yi si triplets using the arc-length formula si resample this list onto a regular set of yj j samples using linear interpolation of each segment. compute the average values of x and y i.e. x and y and subtract them from your sampled curve points. resample the original yi si piecewise-linear function onto a length-independent set of samples say j a length which is a power of two makes subsequent fourier transforms more convenient. compute the fourier transform of the curve treating each y pair as a complex number. to compare two curves fit a linear equation to the phase difference between the two curves. phase wraps around at also you may wish to weight samples by their fourier spectrum magnitude see section computer vision algorithms and applications draft prove that the constant phase component corresponds to the temporal shift in s while the linear component corresponds to rotation. of course feel free to try any other curve descriptor and matching technique from the computer vision literature and kimia sebastian and kimia ex jigsaw puzzle solver challenging write a program to automatically solve a jigsaw puzzle from a set of scanned puzzle pieces. your software may include the following components scan the pieces face up or face down on a flatbed scanner with a distinctively colored background. scan in the box top to use as a low-resolution reference image. use color-based thresholding to isolate the pieces. extract the contour of each piece using edge finding and linking. re-represent each contour using an arc-length or some other re-parameterization. break up the contours into meaningful matchable pieces. this hard? associate color values with each contour to help in the matching. match pieces to the reference image using some rotationally invariant fea ture descriptors. solve a global optimization or search problem to snap pieces together and place them in the correct location relative to the reference image. test your algorithm on a succession of more difficult puzzles and compare your results with those of others. ex successive approximation line detector douglas and peucker to convert a hand-drawn curve linked edge image into a small set of polylines. implement a line simplification algorithm re-render this curve using either an approximating or interpolating spline or bezier curve and ito bartels beatty and barsky farin ex hough transform line detector in images implement a hough transform for finding lines exercises create an accumulator array of the appropriate user-specified size and clear it. the user can specify the spacing in degrees between orientation bins and in pixels between distance bins. the array can be allocated as integer simple counts floating point weighted counts or as an array of vectors for keeping back pointers to the constituent edges. for each detected edgel at location y and orientation tan nynx compute the value of d xnx yny and increment the accumulator corresponding to d. weight the vote of each edge by its length exercise or the strength of its gradient. smooth the scalar accumulator array by adding in values from its immediate neighbors. this can help counteract the discretization effect of voting for only a single bin see exercise find the largest peaks maxima in the accumulator corresponding to lines. for each peak re-fit the lines to the constituent edgels using total least squares use the original edgel lengths or strength weights to weight the least squares fit as well as the agreement between the hypothesized line orientation and the edgel orientation. determine whether these heuristics help increase the accuracy of the fit. after fitting each peak zero-out or eliminate that peak and its adjacent bins in the array and move on to the next largest peak. test out your hough transform on a variety of images taken indoors and outdoors as well as checkerboard calibration patterns. for checkerboard patterns you can modify your hough transform by collapsing antipodal bins d with d to find lines that do not care about polarity changes. can you think of examples in real-world images where this might be desirable as well? ex line fitting uncertainty estimate the uncertainty in your line fit using uncertainty analysis. after determining which edgels belong to the line segment either successive approximation or hough transform re-fit the line segment using total least squares huffel and vandewalle van huffel and lemmerling i.e. find the mean or centroid of the edgels and then use eigenvalue analysis to find the dominant orientation. computer vision algorithms and applications draft compute the perpendicular errors to the line and robustly estimate the variance of the fitting noise using an estimator such as mad re-fit the line parameters by throwing away outliers or using a robust norm or influence function. estimate the error in the perpendicular location of the line segment and its orientation. ex vanishing points compute the vanishing points in an image using one of the techniques described in section and optionally refine the original line equations associated with each vanishing point. your results can be used later to track a target or reconstruct architecture ex vanishing point uncertainty perform an uncertainty analysis on your estimated vanishing points. you will need to decide how to represent your vanishing point e.g. homogeneous coordinates on a sphere to handle vanishing points near infinity. see the discussion of bingham distributions by collins and weiss for some ideas. chapter segmentation split and merge active contours snakes scissors dynamic snakes and condensation level sets application contour tracking and rotoscoping watershed region splitting clustering region merging clustering graph-based segmentation probabilistic aggregation mean shift and mode finding k-means and mixtures of gaussians mean shift normalized cuts graph cuts and energy-based methods application medical image segmentation additional reading exercises computer vision algorithms and applications draft figure some popular image segmentation techniques active contours and blake springer level sets rousson and deriche springer graph-based merging and huttenlocher springer mean shift and meer ieee texture and intervening contour-based normalized cuts belongie leung et al. springer binary mrf solved using graph cuts and funka-lea springer. segmentation image segmentation is the task of finding groups of pixels that go together in statistics this problem is known as cluster analysis and is a widely studied area with hundreds of different algorithms and dubes kaufman and rousseeuw jain duin and mao jain topchy law et al. in computer vision image segmentation is one of the oldest and most widely studied problems and fennema pavlidis riseman and arbib ohlander price and reddy rosenfeld and davis haralick and shapiro early techniques tend to use region splitting or merging and fennema horowitz and pavlidis ohlander price and reddy pavlidis and liow which correspond to divisive and agglomerative algorithms in the clustering literature topchy law et al. more recent algorithms often optimize some global criterion such as intra-region consistency and inter-region boundary lengths or dissimilarity mumford and shah shi and malik comaniciu and meer felzenszwalb and huttenlocher cremers rousson and deriche we have already seen examples of image segmentation in sections and in this chapter we review some additional techniques that have been developed for image segmentation. these include algorithms based on active contours and level sets region splitting and merging mean shift finding normalized cuts based on pixel similarity metrics and binary markov random fields solved using graph cuts figure shows some examples of these techniques applied to different images. since the literature on image segmentation is so vast a good way to get a handle on some of the better performing algorithms is to look at experimental comparisons on human-labeled databases aez maire fowlkes et al. the best known of these is the berkeley segmentation dataset and fowlkes tal et al. which consists of images from a corel image dataset that were hand-labeled by human subjects. many of the more recent image segmentation algorithms report comparative results on this database. for example unnikrishnan pantofaru and hebert propose new metrics for comparing such algorithms. estrada and jepson compare four well-known segmentation algorithms on the berkeley data set and conclude that while their own se-mincut algorithm jepson and chennubhotla algorithm outperforms the others by a small margin there still exists a wide gap between automated and human segmentation a new database of foreground and background segmentations used by alpert galun basri et al. is also httpwww.eecs.berkeley.eduresearchprojectscsvisiongroupingsegbench an interesting observation about their roc plots is that automated techniques cluster tightly along similar curves but human performance is all over the map. httpwww.wisdom.weizmann.ac.il visionseg evaluation dbindex.html computer vision algorithms and applications draft active contours while lines vanishing points and rectangles are commonplace in the man-made world curves corresponding to object boundaries are even more common especially in the natural environment. in this section we describe three related approaches to locating such boundary curves in images. the first originally called snakes by its inventors witkin and terzopoulos is an energy-minimizing two-dimensional spline curve that evolves towards image features such as strong edges. the second intelligent scissors and barrett allow the user to sketch in real time a curve that clings to object boundaries. finally level set techniques evolve the curve as the zeroset of a characteristic function which allows them to easily change topology and incorporate region-based statistics. all three of these are examples of active contours and isard mortensen since these boundary detectors iteratively move towards their final solution under the combination of image and optional user-guidance forces. snakes snakes are a two-dimensional generalization of the energy-minimizing splines first introduced in section eint ds where s is the arc-length along the curve fs ys and and are firstand second-order continuity weighting functions analogous to the sx y and cx y terms introduced in we can discretize this energy by sampling the initial curve position evenly along its length to obtain eint fi where h is the step size which can be neglected if we resample the curve along its arc-length after each iteration. in addition to this internal spline energy a snake simultaneously minimizes external image-based and constraint-based potentials. the image-based potentials are the sum of several terms eimage wlineeline wedgeeedge wtermeterm active contours figure snakes witkin and terzopoulos springer the snake pit for interactively controlling shape lip tracking. where the line term attracts the snake to dark ridges the edge term attracts it to strong gradients and the term term attracts it to line terminations. in practice most systems only use the edge term which can either be directly proportional to the image gradients or to a smoothed version of the image laplacian eedge eedge people also sometimes extract edges and then use a distance map to the edges as an alternative to these two originally proposed potentials. in interactive applications a variety of user-placed constraints can also be added e.g. attractive forces towards anchor points di espring as well as repulsive volcano forces as the snakes evolve by minimizing their energy they often wiggle and slither which accounts for their popular name. figure shows snakes being used to track a person s lips. because regular snakes have a tendency to shrink it is usually better to initialize them by drawing the snake outside the object of interest to be tracked. alternatively an expansion ballooning force can be added to the dynamics and cohen essentially moving each point outwards along its normal. to efficiently solve the sparse linear system arising from snake energy minimization a sparse direct solver can be used since the linear system is essentially snake evolution is usually implemented as an alternation between this linear sys a closed snake has a toeplitz matrix form which can still be factored and solved in on time. computer vision algorithms and applications draft figure elastic net the open squares indicate the cities and the closed squares linked by straight line segments are the tour points. the blue circles indicate the approximate extent of the attraction force of each city which is reduced over time. under the bayesian interpretation of the elastic net the blue circles correspond to one standard deviation of the circular gaussian that generates each city from some unknown tour point. tem solution and the linearization of non-linear constraints such as edge energy. a more direct way to find a global energy minimum is to use dynamic programming weymouth and jain williams and shah but this is not often used in practice since it has been superseded by even more efficient or interactive algorithms such as intelligent scissors and grabcut elastic nets and slippery springs an interesting variant on snakes first proposed by durbin and willshaw and later re-formulated in an energy-minimizing framework by durbin szeliski and yuille is the elastic net formulation of the traveling salesman problem recall that in a tsp the salesman must visit each city once while minimizing the total distance traversed. a snake that is constrained to pass through each city could solve this problem any optimality guarantees but it is impossible to tell ahead of time which snake control point should be associated with each city. instead of having a fixed constraint between snake nodes and cities as in a city is assumed to pass near some point along the tour in a probabilistic interpretation each city is generated as a mixture of gaussians centered at each tour point pdj pij with pij e ij where is the standard deviation of the gaussian and dij active contours is the euclidean distance between a tour point fi and a city location dj. the corresponding data fitting energy log likelihood is eslippery log pdj e this energy derives its name from the fact that unlike a regular spring which couples a given snake point to a given constraint this alternative energy defines a slippery spring that allows the association between constraints and curve points to evolve over time note that this is a soft variant of the popular iterated closest point data constraint that is often used in fitting or aligning surfaces to data points or to each other and mckay zhang to compute a good solution to the tsp the slippery spring data association energy is combined with a regular first-order internal smoothness energy to define the cost of a tour. the tour fs is initialized as a small circle around the mean of the city points and is progressively lowered for large values the tour tries to stay near the centroid of the points but as decreases each city pulls more and more strongly on its closest tour points szeliski and yuille in the limit as each city is guaranteed to capture at least one tour point and the tours between subsequent cites become straight lines. splines and shape priors while snakes can be very good at capturing the fine and irregular detail in many real-world contours they sometimes exhibit too many degrees of freedom making it more likely that they can get trapped in local minima during their evolution. one solution to this problem is to control the snake with fewer degrees of freedom through the use of b-spline approximations saint-marc and medioni cipolla and blake the resulting b-snake can be written as fs bksxk f bx or in discrete form as with f f t f t b bksn and x xt xt computer vision algorithms and applications draft figure point distribution model for a set of resistors cooper taylor et al. elsevier set of input resistor shapes assignment of control points to the boundary distribution plot of point locations first mode of variation in the ensemble shapes. if the object being tracked or recognized has large variations in location scale or orientation these can be modeled as an additional transformation on the control points e.g. srxk t which can be estimated at the same time as the values of the control points. alternatively separate detection and alignment stages can be run to first localize and orient the objects of interest cooper taylor et al. in a b-snake because the snake is controlled by fewer degrees of freedom there is less need for the internal smoothness forces used with the original snakes although these can still be derived and implemented using finite element analysis i.e. taking derivatives and integrals of the b-spline basis functions bathe in practice it is more common to estimate a set of shape priors on the typical distribution of the control points cooper taylor et al. consider the set of resistor if we describe each contour with the set of control points shapes shown in figure shown in figure we can plot the distribution of each point in a scatter plot as shown in figure one potential way of describing this distribution would be by the location xk and covariance ck of each individual point xk. these could then be turned into a quadratic penalty energy on the point location elocxk xkt c k xk. in practice however the variation in point locations is usually highly correlated. a preferable approach is to estimate the joint covariance of all the points simultaneously. first concatenate all of the point locations into a single vector x e.g. by interleaving the x and y locations of each point. the distribution of these vectors across all training active contours figure active shape model the effect of varying the first four shape parameters for a set of faces taylor lanitis et al. ieee searching for the strongest gradient along the normal to each control point cooper taylor et al. elsevier. examples can be described with a mean x and a covariance c p xxp xt where xp are the p training examples. using eigenvalue analysis which is also known as principal component analysis the covariance matrix can be written as c diag k t in most cases the likely appearance of the points can be modeled using only a few eigenvectors with the largest eigenvalues. the resulting point distribution model taylor lanitis et al. cootes cooper taylor et al. can be written as x x b where b is an m k element shape parameter vector and are the first m columns of to constrain the shape parameters to reasonable values we can use a quadratic penalty of the form alternatively the range of allowable bm values can be limited to some range e.g. m cooper taylor et al. alternative approaches for deriving a set of shape vectors are reviewed by isard and blake eshape m. bt diag m b computer vision algorithms and applications draft varying the individual shape parameters bm over the range m m can give a good indication of the expected variation in appearance as shown in figure another example this time related to face contours is shown in figure in order to align a point distribution model with an image each control point searches in a direction normal to the contour to find the most likely corresponding image edge point these individual measurements can be combined with priors on the shape parameters if desired position scale and orientation parameters to estimate a new set of parameters. the resulting active shape model can be iteratively minimized to fit images to non-rigidly deforming objects such as medical images or body parts such as hands cooper taylor et al. the asm can also be combined with a pca analysis of the underlying gray-level distribution to create an active appearance model edwards and taylor which we discuss in more detail in section dynamic snakes and condensation in many applications of active contours the object of interest is being tracked from frame to frame as it deforms and evolves. in this case it makes sense to use estimates from the previous frame to predict and constrain the new estimates. one way to do this is to use kalman filtering which results in a formulation called kalman snakes and szeliski blake curwen and zisserman the kalman filter is based on a linear dynamic model of shape parameter evolution xt axt wt where xt and xt are the current and previous state variables a is the linear transition matrix and w is a noise vector which is often modeled as a gaussian the matrices a and the noise covariance can be learned ahead of time by observing typical sequences of the object being tracked and isard the qualitative behavior of the kalman filter can be seen in figure the linear dynamic model causes a deterministic change in the previous estimate while the process noise causes a stochastic diffusion that increases the system entropy of certainty. new measurements from the current frame restore some of the certainty in the updated estimate. in many situations however such as when tracking in clutter a better estimate for the contour can be obtained if we remove the assumptions that the distribution are gaussian which is what the kalman filter requires. in this case a general multi-modal distribution is propagated as shown in figure in order to model such multi-modal distributions isard and blake introduced the use of particle filtering to the computer vision alternatives to modeling multi-modal distributions include mixtures of gaussians and multiple active contours figure probability density propagation and blake springer. at the beginning of each estimation step the probability density is updated according to the linear dynamic model drift and its certainty is reduced due to process noise diffusion. new measurements introduce additional information that helps refine the current estimate. the kalman filter models the distributions as uni-modal i.e. using a mean and covariance. some applications require more general multi-modal distributions. computer vision algorithms and applications draft figure factored sampling using particle filter in the condensation algorithm and blake springer each density distribution is represented using a superposition of weighted particles the drift-diffusion-measurement cycle implemented using random sampling perturbation and re-weighting stages. active contours figure head tracking using condensation and blake springer sample set representation of head estimate distribution multiple measurements at each control vertex location multi-hypothesis tracking over time. particle filtering techniques represent a probability distribution using a collection of weighted point samples de freitas doucet et al. bishop koller and friedman to update the locations of the samples according to the linear dynamics drift the centers of the samples are updated according to and multiple samples are generated for each point these are then perturbed to account for the stochastic diffusion i.e. their locations are moved by random vectors taken from the distribution of finally the weights of these samples are multiplied by the measurement probability density i.e. we take each sample and measure its likelihood given the current measurements. because the point samples represent and propagate conditional estimates of the multi-modal density isard and blake dubbed their algorithm conditional density propagation or condensation. figure shows what a factored sample of a head tracker might look like drawing a red b-spline contour for each of subset of the particles being tracked. figure shows why the measurement density itself is often multi-modal the locations of the edges perpendicular to the spline curve can have multiple local maxima due to background clutter. finally figure shows the temporal evolution of the conditional density coordinate of the head and shoulder tracker centroid as it tracks several people over time. hypothesis tracking and fortmann cham and rehg note that because of the structure of these steps non-linear dynamics and non-gaussian noise can be used. computer vision algorithms and applications draft figure intelligent scissors as the mouse traces the white path the scissors follow the orange path along the object boundary green curves show intermediate positions and barrett acm regular scissors can sometimes jump to a stronger boundary after training to the previous segment similar edge profiles are preferred and barrett elsevier. scissors active contours allow a user to roughly specify a boundary of interest and have the system evolve the contour towards a more accurate location as well as track it over time. the results of this curve evolution however may be unpredictable and may require additional user-based hints to achieve the desired result. an alternative approach is to have the system optimize the contour in real time as the user is drawing the intelligent scissors system developed by mortensen and barrett does just that. as the user draws a rough outline white curve in figure the system computes and draws a better curve that clings to high-contrast edges orange curve. to compute the optimal curve path the image is first pre-processed to associate low costs with edges between neighboring horizontal vertical and diagonal i.e. neighbors that are likely to be boundary elements. their system uses a combination of zerocrossing gradient magnitudes and gradient orientations to compute these costs. next as the user traces a rough curve the system continuously recomputes the lowestcost path between the starting seed point and the current mouse location using dijkstra s algorithm a breadth-first dynamic programming algorithm that terminates at the current target location. in order to keep the system from jumping around unpredictably the system will freeze the curve to date the seed point after a period of inactivity. to prevent the live wire from jumping onto adjacent higher-contrast contours the system also learns the intensity active contours figure level set evolution for a geodesic active contour. the embedding function is updated based on the curvature of the underlying surface modulated by the edgespeed function gi as well as the gradient of gi thereby attracting it to strong edges. profile under the current optimized curve and uses this to preferentially keep the wire moving along the same a similar looking boundary c. several extensions have been proposed to the basic algorithm which works remarkably well even in its original form. mortensen and barrett use tobogganing which is a simple form of watershed region segmentation to pre-segment the image into regions whose boundaries become candidates for optimized curve paths. the resulting region boundaries are turned into a much smaller graph where nodes are located wherever three or four regions meet. the dijkstra algorithm is then run on this reduced graph resulting in much faster often more stable performance. another extension to intelligent scissors is to use a probabilistic framework that takes into account the current trajectory of the boundary resulting in a system called jetstream erez blake and gangnet instead of re-computing an optimal curve at each time instant a simpler system can be developed by simply snapping the current mouse position to the nearest likely boundary point applications of these boundary extraction techniques to image cutting and pasting are presented in section level sets a limitation of active contours based on parametric curves of the form fs e.g. snakes bsnakes and condensation is that it is challenging to change the topology of the curve as it evolves. and terzopoulos describe one approach to doing this. furthermore if the shape changes dramatically curve reparameterization may also be required. an alternative representation for such closed contours is to use a level set where the zerocrossings of a characteristic signed distance function define the curve. gi computer vision algorithms and applications draft level sets evolve to fit and track objects of interest by modifying the underlying embedding function name for this function y instead of the curve fs sethian and vemuri sethian sapiro osher and paragios to reduce the amount of computation required only a small strip around the locations of the current zero-crossing needs to updated at each step which results in what are called fast marching methods an example of an evolution equation is the geodesic active contour proposed by caselles kimmel and sapiro and yezzi kichenassamy kumar et al. d dt gi gi where gi is a generalized version of the snake edge potential to get an intuitive sense of the curve s behavior assume that the embedding function is a signed distance function away from the curve in which case the first term in equation moves the curve in the direction of its curvature i.e. it acts to straighten the curve under the influence of the modulation function gi. the second term moves the curve down the gradient of gi encouraging the curve to migrate towards minima of gi. while this level-set formulation can readily change topology it is still susceptible to local minima since it is based on local measurements such as image gradients. an alternative approach is to re-cast the problem in a segmentation framework where the energy measures the consistency of the image statistics color texture motion inside and outside the segmented regions rousson and deriche rousson and paragios houhou thiran and bresson these approaches build on earlier energy-based segmentation frameworks introduced by leclerc mumford and shah and chan and vese which are discussed in more detail in section examples of such level-set segmentations are shown in figure which shows the evolution of the level sets from a series of distributed circles towards the final binary segmentation. for more information on level sets and their applications please see the collection of papers edited by osher and paragios as well as the series of workshops on variational and level set methods in computer vision faugeras chan et al. and special issues on scale space and variational methods in computer vision and sgallari application contour tracking and rotoscoping active contours can be used in a wide variety of object-tracking applications and isard yilmaz javed and shah for example they can be used to track facial features active contours figure level set segmentation rousson and deriche springer grayscale image segmentation and color image segmentation. uni-variate and multi-variate gaussians are used to model the foreground and background pixel distributions. the initial circles evolve towards an accurate segmentation of foreground and background adapting their topology as they evolve. for performance-driven animation and waters lee terzopoulos and waters parke and waters bregler covell and slaney they can also be used to track heads and people as shown in figure as well as moving vehicles and deriche additional applications include medical image segmentation where contours can be tracked from slice to slice in computerized tomography medical imagery and taylor or over time as in ultrasound scans. an interesting application that is closer to computer animation and visual effects is rotoscoping which uses the tracked contours to deform a set of hand-drawn animations to modify or replace the original video agarwala hertzmann seitz et al. present a system based on tracking hand-drawn b-spline contours drawn at selected keyframes using a combination of geometric and appearance-based criteria they also provide an excellent review of previous rotoscoping and image-based contour-tracking systems. the term comes from a device rotoscope that projected frames of a live-action film underneath an acetate so that artists could draw animations directly over the actors shapes. computer vision algorithms and applications draft figure keyframe-based rotoscoping hertzmann seitz et al. acm original frames rotoscoped contours re-colored blouse rotoscoped hand-drawn animation. additional applications of rotoscoping contour detection and segmentation such as cutting and pasting objects from one photograph into another are presented in section split and merge as mentioned in the introduction to this chapter the simplest possible technique for segmenting a grayscale image is to select a threshold and then compute connected components unfortunately a single threshold is rarely sufficient for the whole image because of lighting and intra-object statistical variations. in this section we describe a number of algorithms that proceed either by recursively splitting the whole image into pieces based on region statistics or conversely merging pixels and regions together in a hierarchical fashion. it is also possible to combine both splitting and merging by starting with a medium-grain segmentation a quadtree representation and then allowing both merging and splitting operations and pavlidis pavlidis and liow watershed a technique related to thresholding since it operates on a grayscale image is watershed computation and soille this technique segments an image into several catchment basins which are the regions of an image as a height field or landscape where split and merge figure locally constrained watershed segmentation ieee original confocal microscopy image with marked seeds segments standard watershed segmentation locally constrained watershed segmentation. rain would flow into the same lake. an efficient way to compute such regions is to start flooding the landscape at all of the local minima and to label ridges wherever differently evolving components meet. the whole algorithm can be implemented using a priority queue of pixels and breadth-first search and soille since images rarely have dark regions separated by lighter ridges watershed segmentation is usually applied to a smoothed version of the gradient magnitude image which also makes it usable with color images. as an alternative the maximum oriented energy in a steerable filter and adelson can be used as the basis of the oriented watershed transform developed by arbel aez maire fowlkes et al. such techniques end up finding smooth regions separated by visible gradient boundaries. since such boundaries are what active contours usually follow active contour algorithms and barrett li sun tang et al. often precompute such a segmentation using either the watershed or the related tobogganing technique unfortunately watershed segmentation associates a unique region with each local minimum which can lead to over-segmentation. watershed segmentation is therefore often used as part of an interactive system where the user first marks seed locations a click or a short stroke that correspond to the centers of different desired components. figure shows the results of running the watershed algorithm with some manually placed markers on a confocal microscopy image. it also shows the result for an improved version of watershed that uses local morphology to smooth out and optimize the boundaries separating the regions a related algorithm can be used to compute maximally stable extremal regions efficiently tion er and stew enius computer vision algorithms and applications draft region splitting clustering splitting the image into successively finer regions is one of the oldest techniques in computer vision. ohlander price and reddy present such a technique which first computes a histogram for the whole image and then finds a threshold that best separates the large peaks in the histogram. this process is repeated until regions are either fairly uniform or below a certain size. more recent splitting algorithms often optimize some metric of intra-region similarity and inter-region dissimilarity. these are covered in sections and region merging clustering region merging techniques also date back to the beginnings of computer vision. brice and fennema use a dual grid for representing boundaries between pixels and merge regions based on their relative boundary lengths and the strength of the visible edges at these boundaries. in data clustering algorithms can link clusters together based on the distance between their closest points clustering their farthest points clustering or something in between topchy law et al. kamvar klein and manning provide a probabilistic interpretation of these algorithms and show how additional models can be incorporated within this framework. a very simple version of pixel-based merging combines adjacent regions whose average color difference is below a threshold or whose regions are too small. segmenting the image into such superpixels ren efros et al. which are not semantically meaningful can be a useful pre-processing stage to make higher-level algorithms such as stereo matching kang uyttendaele et al. taguchi wilburn and zitnick optic flow jojic and kang brox bregler and malik and recognition ren efros et al. mori gu lim arbelaez et al. lim arbel aez gu et al. both faster and more robust. graph-based segmentation while many merging algorithms simply apply a fixed rule that groups pixels and regions together felzenszwalb and huttenlocher present a merging algorithm that uses relative dissimilarities between regions to determine which ones should be merged it produces an algorithm that provably optimizes a global grouping metric. they start with a pixel-topixel dissimilarity measure we that measures for example intensity differences between neighbors. they can use the joint feature space distances introduced by comaniciu and meer which we discuss in section split and merge figure graph-based merging segmentation and huttenlocher springer input grayscale image that is successfully segmented into three regions even though the variation inside the smaller rectangle is larger than the variation across the middle edge input grayscale image resulting segmentation using an pixel neighborhood. for any region r its internal difference is defined as the largest edge weight in the re gion s minimum spanning tree intr min e m st we. for any two adjacent regions with at least one edge connecting their vertices the difference between these regions is defined as the minimum weight edge connecting the two regions dif min we. their algorithm merges any two adjacent regions whose difference is smaller than the minimum internal difference of these two regions where is a heuristic region penalty that felzenszwalb and huttenlocher set to kr but which can be set to any application-specific measure of region goodness. by merging regions in decreasing order of the edges separating them can be efficiently evaluated using a variant of kruskal s minimum spanning tree algorithm they provably produce segmentations that are neither too fine exist regions that could have been merged nor too coarse are regions that could be split without being mergeable. for fixed-size pixel neighborhoods the running time for this algorithm is on log n where n is the number of image pixels which makes it one of the fastest segmentation algorithms and durand figure shows two examples of images segmented using their technique. computer vision algorithms and applications draft figure coarse to fine node aggregation in segmentation by weighted aggregation galun sharon et al. macmillan publishers ltd original gray-level pixel grid inter-pixel couplings where thicker lines indicate stronger couplings after one level of coarsening where each original pixel is strongly coupled to one of the coarse-level nodes after two levels of coarsening. probabilistic aggregation alpert galun basri et al. develop a probabilistic merging algorithm based on two cues namely gray-level similarity and texture similarity. the gray-level similarity between regions ri and rj is based on the minimal external difference from other neighboring regions local min i j where and rk. this is compared to the average intensity difference i mink ik and ik is the difference in average intensities between regions ri local i j where i ik ik and ik is the boundary length between regions ri and rk. the texture similarity is defined using relative differences between histogram bins of simple oriented sobel filter responses. the pairwise statistics local and local are used to compute the likelihoods pij that two regions should be merged. the paper by alpert galun basri et al. for more details. merging proceeds in a hierarchical fashion inspired by algebraic multigrid techniques briggs henson and mccormick and previously used by alpert galun basri et al. in their segmentation by weighted aggregation algorithm galun sharon et al. which we discuss in section a subset of the nodes c v that are strongly coupled to all of the original nodes are used to define the problem at a coarser scale where strong coupling is defined as c pij v pij mean shift and mode finding with usually set to the intensity and texture similarity statistics for the coarser nodes are recursively computed using weighted averaging where the relative strengths between coarse- and fine-level nodes are based on their merge probabilities pij. this allows the algorithm to run in essentially on time using the same kind of hierarchical aggregation operations that are used in pyramid-based filtering or preconditioning algorithms. after a segmentation has been identified at a coarser level the exact memberships of each pixel are computed by propagating coarse-level assignments to their finer-level children galun sharon et al. alpert galun basri et al. figure shows the segmentations produced by this algorithm compared to other popular segmentation algorithms. mean shift and mode finding mean-shift and mode finding techniques such as k-means and mixtures of gaussians model the feature vectors associated with each pixel color and position as samples from an unknown probability density function and then try to find clusters in this distribution. consider the color image shown in figure how would you segment this image based on color alone? figure shows the distribution of pixels in luv space which is equivalent to what a vision algorithm that ignores spatial location would see. to make the visualization simpler let us only consider the lu coordinates as shown in figure how many obvious clusters do you see? how would you go about finding these clusters? the k-means and mixtures of gaussians techniques use a parametric model of the density function to answer this question i.e. they assume the density is the superposition of a small number of simpler distributions gaussians whose locations and shape can be estimated. mean shift on the other hand smoothes the distribution and finds its peaks as well as the regions of feature space that correspond to each peak. since a complete density is being modeled this approach is called non-parametric let us look at these techniques in more detail. k-means and mixtures of gaussians while k-means implicitly models the probability density as a superposition of spherically symmetric distributions it does not require any probabilistic reasoning or modeling instead the algorithm is given the number of clusters k it is supposed to find it then iteratively updates the cluster center location based on the samples that are closest to each center. the algorithm can be initialized by randomly sampling k centers from the input feature vectors. techniques have also been developed for splitting or merging cluster centers computer vision algorithms and applications draft figure mean-shift image segmentation and meer ieee input color image pixels plotted in luv space lu space distribution clustered results after mean-shift procedures corresponding trajectories with peaks marked as red dots. mean shift and mode finding based on their statistics and for accelerating the process of finding the nearest mean center in mixtures of gaussians each cluster center is augmented by a covariance matrix whose values are re-estimated from the corresponding samples. instead of using nearest neighbors to associate input samples with cluster centers a mahalanobis distance is used dxi k k k kt k k where xi are the input samples k are the cluster centers and k are their covariance estimates. samples can be associated with the nearest cluster center hard assignment of membership or can be softly assigned to several nearby clusters. this latter more commonly used approach corresponds to iteratively re-estimating the parameters for a mixture of gaussians density function px k k k k n k k where k are the mixing coefficients k and k are the gaussian means and covariances and k is the normal distribution n k k e dx k k to iteratively compute local maximum likely estimate for the unknown mixture parameters k k k the expectation maximization algorithm laird and rubin proceeds in two alternating stages the expectation stage step estimates the responsibilities zik zi k n k k with zik which are the estimates of how likely a sample xi was generated from the kth gaussian cluster. the maximization stage step updates the parameter values k k k nk n zikxi zikxi kxi kt computer vision algorithms and applications draft where nk zik. is an estimate of the number of sample points assigned to each cluster. bishop has a wonderful exposition of both mixture of gaussians estimation and the more general topic of expectation maximization. in the context of image segmentation ma derksen hong et al. present a nice review of segmentation using mixtures of gaussians and develop their own extension based on minimum description length coding which they show produces good results on the berkeley segmentation database. mean shift while k-means and mixtures of gaussians use a parametric form to model the probability density function being segmented mean shift implicitly models this distribution using a smooth continuous non-parametric model. the key to mean shift is a technique for efficiently finding peaks in this high-dimensional data distribution without ever computing the complete function explicitly and hostetler cheng comaniciu and meer consider once again the data points shown in figure which can be thought of as having been drawn from some probability density function. if we could compute this density function as visualized in figure we could find its major peaks and identify regions of the input space that climb to the same peak as being part of the same region. this is the inverse of the watershed algorithm described in section which climbs downhill to find basins of attraction. the first question then is how to estimate the density function given a sparse set of samples. one of the simplest approaches is to just smooth the data e.g. by convolving it with a fixed kernel of width h fx kx xi where xi are the input samples and kr is the kernel function parzen this approach is known as kernel density estimation or the parzen window technique hart and stork section bishop section once we have computed fx as shown in figures and we can find its local maxima using gradient ascent or some other optimization technique. in this simplified formula a euclidean metric is used. we discuss a little later how to generalize this to non-uniform or oriented metrics. note also that this distribution may not be proper i.e. integrate to since we are looking for maxima in the density this does not matter. mean shift and mode finding figure one-dimensional visualization of the kernel density estimate its derivative and a mean shift. the kernel density estimate fx is obtained by convolving the sparse set of input samples xi with the kernel function kx. the derivative of this function can be obtained by convolving the inputs with the derivative kernel gx. estimating the local displacement vectors around a current estimate xk results in the mean-shift vector mxk which in a multi-dimensional setting point in the same direction as the function gradient fxk. the red dots indicate local maxima in fx to which the mean shifts converge. the problem with this brute force approach is that for higher dimensions it becomes computationally prohibitive to evaluate fx over the complete search instead mean shift uses a variant of what is known in the optimization literature as multiple restart gradient descent. starting at some guess for a local maximum yk which can be a random input data point xi mean shift computes the gradient of the density estimate fx at yk and takes an uphill step in that direction the gradient of fx is given by fx xgx xi gr where and is the first derivative of kr. we can re-write the gradient of the density function as where the vector gx mx fx mx xigx xi gx xi x is called the mean shift since it is the difference between the weighted mean of the neighbors xi around x and the current value of x. even for one dimension if the space is extremely sparse it may be inefficient. xf computer vision algorithms and applications draft in the mean-shift procedure the current estimate of the mode yk at iteration k is replaced by its locally weighted mean yk myk xigyk xi gyk xi comaniciu and meer prove that this algorithm converges to a local maximum of fx under reasonably weak conditions on the kernel kr i.e. that it is monotonically decreasing. this convergence is not guaranteed for regular gradient descent unless appropriate step size control is used. the two kernels that comaniciu and meer studied are the epanechnikov kernel which is a radial generalization of a bilinear kernel and the gaussian kernel ker r kn the corresponding derivative kernels gr are a unit ball and another gaussian respectively. using the epanechnikov kernel converges in a finite number of steps while the gaussian kernel has a smoother trajectory produces better results but converges very slowly near a mode the simplest way to apply mean shift is to start a separate mean-shift mode estimate y at every input point xi and to iterate for a fixed number of steps or until the mean-shift magnitude is below a threshold. a faster approach is to randomly subsample the input points xi and to keep track of each point s temporal evolution. the remaining points can then be classified based on the nearest evolution path and meer paris and durand review a number of other more efficient implementations of mean shift including their own approach which is based on using an efficient low-resolution estimate of the complete multi-dimensional space of fx along with its stationary points. the color-based segmentation shown in figure only looks at pixel colors when determining the best clustering. it may therefore cluster together small isolated pixels that happen to have the same color which may not correspond to a semantically meaningful segmentation of the image. better results can usually be obtained by clustering in the joint domain of color and location. in this approach the spatial coordinates of the image xs y which are called the spatial domain are concatenated with the color values xr which are known as the range domain and mean-shift clustering is applied in this five-dimensional space xj. since location and color may have different scales the kernels are adjusted accordingly i.e. we use a kernel of the form kxj r s mean shift and mode finding figure mean-shift color image segmentation with parameters hr m and meer ieee. where separate parameters hs and hr are used to control the spatial and range bandwidths of the filter kernels. figure shows an example of mean-shift clustering in the joint domain with parameters hr m where spatial regions containing less than m pixels are eliminated. the form of the joint domain filter kernel is reminiscent of the bilateral filter kernel discussed in section the difference between mean shift and bilateral filtering however is that in mean shift the spatial coordinates of each pixel are adjusted along with its color values so that the pixel migrates more quickly towards other pixels with similar colors and can therefore later be used for clustering and segmentation. determining the best bandwidth parameters h to use with mean shift remains something of an art although a number of approaches have been explored. these include optimizing the bias variance tradeoff looking for parameter ranges where the number of clusters varies slowly optimizing some external clustering criterion or using top-down domain knowledge and meer it is also possible to change the orientation of the kernel in joint parameter space for applications such as spatio-temporal segmentations thiesson xu et al. mean shift has been applied to a number of different problems in computer vision including face tracking shape extraction and texture segmentation and meer and more recently in stereo matching and quan non-photorealistic rendering and santella and video editing bhat colburn et al. paris and durand provide a nice review of such applications as well as techniques for more efficiently solving the mean-shift equations and producing hierarchical segmentations. computer vision algorithms and applications draft a a assoca a cutb a b sum assoca v b sum cuta b assoca v assocb b assocb v assocb v figure sample weighted graph and its normalized cut a small sample graph and its smallest normalized cut tabular form of the associations and cuts for this graph. the assoc and cut entries are computed as area sums of the associated weight matrix w normalizing the table entries by the row or column sums produces normalized associations and cuts n assoc and n cut. normalized cuts while bottom-up merging techniques aggregate regions into coherent wholes and mean-shift techniques try to find clusters of similar pixels using mode finding the normalized cuts technique introduced by shi and malik examines the affinities between nearby pixels and tries to separate groups that are connected by weak affinities. consider the simple graph shown in figure the pixels in group a are all strongly connected with high affinities shown as thick red lines as are the pixels in group b. the connections between these two groups shown as thinner blue lines are much weaker. a normalized cut between the two groups shown as a dashed line separates them into two clusters. the cut between two groups a and b is defined as the sum of all the weights being cut cuta b aj b wij where the weights between two pixels regions i and j measure their similarity. using a minimum cut as a segmentation criterion however does not result in reasonable clusters since the smallest cuts usually involve isolating a single pixel. a better measure of segmentation is the normalized cut which is defined as ncuta b cuta b assoca v cuta b assocb v where assoca a aj a wij is the association of all the weights within a cluster and assoca v assoca a cuta b is the sum of all the weights associated aaaabbb normalized cuts with nodes in a. figure shows how the cuts and associations can be thought of as area sums in the weight matrix w where the entries of the matrix have been arranged so that the nodes in a come first and the nodes in b come second. figure shows an actual weight matrix for which these area sums can be computed. dividing each of these areas by the corresponding row sum rightmost column of figure results in the normalized cut and association values. these normalized values better reflect the fitness of a particular segmentation since they look for collections of edges that are weak relative to all of the edges both inside and emanating from a particular region. unfortunately computing the optimal normalized cut is np-complete. instead shi and malik suggest computing a real-valued assignment of nodes to groups. let x be the indicator vector where xi iff i a and xi iff i b. let d w be the row sums of the symmetric matrix w and d diagd be the corresponding diagonal matrix. shi and malik show that minimizing the normalized cut over all possible indicator vectors x is equivalent to minimizing min y yt w yt dy where y x is a vector consisting of all and bs such that y d minimizing this rayleigh quotient is equivalent to solving the generalized eigenvalue system which can be turned into a regular eigenvalue problem w dy nz z where n d d is the normalized affinity matrix and z because these eigenvectors can be interpreted as the large modes of vibration in a spring-mass system normalized cuts is an example of a spectral method for image segmentation. extending an idea originally proposed by scott and longuet-higgins weiss suggests normalizing the affinity matrix and then using the top k eigenvectors to reconstitute a q matrix. other papers have extended the basic normalized cuts framework by modifying the affinity matrix in different ways finding better discrete solutions to the minimization problem or applying multi-scale techniques a and shi ng jordan and weiss yu and shi cour b en ezit and shi tolliver and miller figure shows the second smallest eigenvector corresponding to the weight matrix shown in figure the rows have been permuted to separate the two groups of variables that belong to the different components of this eigenvector. after this real-valued vector is computed the variables corresponding to positive and negative computer vision algorithms and applications draft figure sample weight table and its second smallest eigenvector and malik ieee sample weight matrix w eigenvector corresponding to the second smallest eigenvalue of the generalized eigenvalue problem w dy. eigenvector values are associated with the two cut components. this process can be further repeated to hierarchically subdivide an image as shown in figure the original algorithm proposed by shi and malik used spatial position and image feature differences to compute the pixel-wise affinities wij i f f s for pixels within a radius r where f is a feature vector that consists of intensities colors or oriented filter histograms. how is the negative exponential of the joint feature space distance in subsequent work malik belongie leung et al. look for intervening contours between pixels i and j and define an intervening contour weight ij max wic x lij pconx where lij is the image line joining pixels i and j and pconx is the probability of an intervening contour perpendicular to this line which is defined as the negative exponential of the oriented energy in the perpendicular direction. they multiply these weights with a textonbased texture similarity metric and use an initial over-segmentation based purely on local pixel-wise features to re-estimate intervening contours and texture statistics in a region-based manner. figure shows the results of running this improved algorithm on a number of test images. because it requires the solution of large sparse eigenvalue problems normalized cuts can be quite slow. sharon galun sharon et al. present a way to accelerate the computation of the normalized cuts using an approach inspired by algebraic multigrid normalized cuts figure normalized cuts segmentation and malik ieee the input image and the components returned by the normalized cuts algorithm. figure comparative segmentation results galun basri et al. ieee. our method refers to the probabilistic bottom-up merging algorithm developed by alpert et al. computer vision algorithms and applications draft briggs henson and mccormick to coarsen the original problem they select a smaller number of variables such that the remaining fine-level variables are strongly coupled to at least one coarse-level variable. figure shows this process schematically while gives the definition for strong coupling except that in this case the original weights wij in the normalized cut are used instead of merge probabilities pij. once a set of coarse variables has been selected an inter-level interpolation matrix with elements similar to the left hand side of is used to define a reduced version of the normalized cuts problem. in addition to computing the weight matrix using interpolation-based coarsening additional region statistics are used to modulate the weights. after a normalized cut has been computed at the coarsest level of analysis the membership values of finer-level nodes are computed by interpolating parent values and mapping values within of and to pure boolean values. an example of the segmentation produced by weighted aggregation is shown in figure along with the most recent probabilistic bottom-up merging algorithm by alpert galun basri et al. which was described in section in even more recent work wang and oliensis show how to estimate statistics over segmentations mean region size directly from the affinity graph. they use this to produce segmentations that are more central with respect to other possible segmentations. graph cuts and energy-based methods a common theme in image segmentation algorithms is the desire to group pixels that have similar appearance and to have the boundaries between pixels in different regions be of short length and across visible discontinuities. if we restrict the boundary measurements to be between immediate neighbors and compute region membership statistics by summing over pixels we can formulate this as a classic pixel-based energy function using either a variational formulation see section or as a binary markov random field examples of the continuous approach include and shah chan and vese zhu and yuille tabb and ahuja along with the level set approaches discussed in section an early example of a discrete labeling problem that combines both region-based and boundary-based energy terms is the work of leclerc who used minimum description length coding to derive the energy function being minimized. boykov and funka-lea present a wonderful survey of various energy-based techniques for binary object segmentation some of which we discuss below. as we saw in section the energy corresponding to a segmentation problem can be graph cuts and energy-based methods written equations and as where the region term ef eri j ebi j eri j esii j rfi j is the negative log likelihood that pixel intensity color ii j is consistent with the statistics of region rfi j and the boundary term ebi j sxi j j fi j syi j j fi j measures the inconsistency between neighbors modulated by local horizontal and vertical smoothness terms sxi j and syi j. region statistics can be something as simple as the mean gray level or color in which case esi k alternatively they can be more complex such as region intensity histograms and jolly or color gaussian mixture models kolmogorov and blake for smoothness terms it is common to make the strength of the smoothness sxi j inversely proportional to the local edge strength veksler and zabih originally energy-based segmentation problems were optimized using iterative gradient descent techniques which were slow and prone to getting trapped in local minima. boykov and jolly were the first to apply the binary mrf optimization algorithm developed by greig porteous and seheult to binary object segmentation. in this approach the user first delineates pixels in the background and foreground regions using a few strokes of an image brush these pixels then become the seeds that tie nodes in the s t graph to the source and sink labels s and t seed pixels can also be used to estimate foreground and background region statistics or color histograms. the capacities of the other edges in the graph are derived from the region and boundary energy terms i.e. pixels that are more compatible with the foreground or background region get stronger connections to the respective source or sink adjacent pixels with greater smoothness also get stronger links. once the minimum-cutmaximum-flow problem has been solved using a polynomial time algorithm and tarjan boykov and kolmogorov pixels on either side of the computed cut are labeled according to the source or sink to which they remain connected while graph cuts is just one of several known techniques for mrf energy minimization it is still the one most commonly used for solving binary mrf problems. computer vision algorithms and applications draft figure graph cuts for region segmentation and jolly ieee the energy function is encoded as a maximum flow problem the minimum cut determines the region boundary. figure grabcut image segmentation kolmogorov and blake acm the user draws a bounding box in red the algorithm guesses color distributions for the object and background and performs a binary segmentation the process is repeated with better region statistics. the basic binary segmentation algorithm of boykov and jolly has been extended in a number of directions. the grabcut system of rother kolmogorov and blake iteratively re-estimates the region statistics which are modeled as a mixtures of gaussians in color space. this allows their system to operate given minimal user input such as a single bounding box the background color model is initialized from a strip of pixels around the box outline. foreground color model is initialized from the interior pixels but quickly converges to a better estimate of the object. the user can also place additional strokes to refine the segmentation as the solution progresses. in more recent work cui yang wen et al. use color and edge models derived from previous segmentations of similar objects to improve the local models used in grabcut. another major extension to the original binary segmentation formulation is the addition of objectterminal terminalbackground pqrwvstbackground object terminalterminalpqrwvstcut graph cuts and energy-based methods figure segmentation with a directed graph cut and funka-lea springer directed graph image with seed points the undirected graph incorrectly continues the boundary along the bright object the directed graph correctly segments the light gray region from its darker surround. directed edges which allows boundary regions to be oriented e.g. to prefer light to dark transitions or vice versa and boykov figure shows an example where the directed graph cut correctly segments the light gray liver from its dark gray surround. the same approach can be used to measure the flux exiting a region i.e. the signed gradient projected normal to the region boundary. combining oriented graphs with larger neighborhoods enables approximating continuous problems such as those traditionally solved using level sets in the globally optimal graph cut framework and kolmogorov kolmogorov and boykov even more recent developments in graph cut-based segmentation techniques include the addition of connectivity priors to force the foreground to be in a single piece kolmogorov and rother and shape priors to use knowledge about an object s shape during the segmentation process and boykov lempitsky blake and rother while optimizing the binary mrf energy requires the use of combinatorial optimization techniques such as maximum flow an approximate solution can be obtained by converting the binary energy terms into quadratic energy terms defined over a continuous random field which then becomes a classical membrane-based regularization problem the resulting quadratic energy function can then be solved using standard linear system solvers although if speed is an issue you should use multigrid or one of its variants once the continuous solution has been computed it can be thresholded at to yield a binary segmentation. the continuous optimization problem can also be interpreted as computing the prob object seedtopixelpthatwasnotpreviouslyassignedanyseed.weneedtochangethecostsfortwot-linksatpt-linkinitialcostnewcostps rp bkg rp obj object seedatpixelpweincreasethet-linksweightsaccordingtothetablet-linkinitialcostaddnewcostps rp bkg rp obj rp obj rp bkg background computer vision algorithms and applications draft ability at each pixel that a random walker starting at that pixel ends up at one of the labeled seed pixels which is also equivalent to computing the potential in a resistive grid where the resistors are equal to the edge weights sinop and grady k-way segmentations can also be computed by iterating through the seed labels using a binary problem with one label set to and all the others set to to compute the relative membership probabilities for each pixel. in follow-on work grady and ali use a precomputation of the eigenvectors of the linear system to make the solution with a novel set of seeds faster which is related to the laplacian matting problem presented in section acha and lischinski couprie grady najman et al. relate the random walker to watersheds and other segmentation techniques. singaraju grady and vidal add directededge constraints in order to support flux which makes the energy piecewise quadratic and hence not solvable as a single linear system. the random walker algorithm can also be used to solve the mumford shah segmentation problem and alvino and to compute fast multigrid solutions a nice review of these techniques is given by singaraju grady sinop et al. an even faster way to compute a continuous approximate segmentation is to compute weighted geodesic distances between the and seed regions and sapiro which can also be used to estimate soft alpha mattes a related approach by criminisi sharp and blake can be used to find fast approximate solutions to general binary markov random field optimization problems. application medical image segmentation one of the most promising applications of image segmentation is in the medical imaging domain where it can be used to segment anatomical tissues for later quantitative analysis. figure shows a binary graph cut with directed edges being used to segment the liver tissue gray from its surrounding bone and muscle gray tissue. figure shows the segmentation of bones in a computed x-ray tomography volume. without the powerful optimization techniques available in today s image segmentation algorithms such processing used to require much more laborious manual tracing of individual x-ray slices. the fields of medical image segmentation and terzopoulos and medical image registration and unser are rich research fields with their own specialized conferences such as medical imaging computing and computer assisted intervention and journals such as medical image analysis and ieee transactions on medical imaging. these can be great sources of references and ideas for research in this area. additional reading figure volumetric medical image segmentation using graph cuts and funka-lea springer computed tomography slice with some seeds recovered volumetric bone model a voxel grid. additional reading the topic of image segmentation is closely related to clustering techniques which are treated in a number of monographs and review articles and dubes kaufman and rousseeuw jain duin and mao jain topchy law et al. some early segmentation techniques include those describerd by brice and fennema pavlidis riseman and arbib ohlander price and reddy rosenfeld and davis haralick and shapiro while examples of newer techniques are developed by leclerc mumford and shah shi and malik felzenszwalb and huttenlocher arbel aez maire fowlkes et al. provide a good review of automatic segmentation techniques and also compare their performance on the berkeley segmentation dataset and benchmark fowlkes tal et al. additional comparison papers and databases include those by unnikrishnan pantofaru and hebert alpert galun basri et al. estrada and jepson the topic of active contours has a long history beginning with the seminal work on snakes and other energy-minimizing variational methods witkin and terzopoulos cootes cooper taylor et al. blake and isard continuing through techniques such as intelligent scissors and barrett p erez blake and gangnet and culminating in level sets sethian and vemuri caselles kimmel and sapiro sethian paragios and deriche sapiro osher and paragios paragios faugeras chan et al. cremers rousson and deriche rousson and paragios paragios and sgallari which are currently the most widely httpwww.eecs.berkeley.eduresearchprojectscsvisiongroupingsegbench. computer vision algorithms and applications draft used active contour methods. techniques for segmenting images based on local pixel similarities combined with aggregation or splitting methods include watersheds and soille beare arbel aez maire fowlkes et al. region splitting price and reddy region merging and fennema pavlidis and liow jain topchy law et al. as well as graph-based and probabilistic multi-scale approaches and huttenlocher alpert galun basri et al. mean-shift algorithms which find modes in a density function representation of the pixels are presented by comaniciu and meer paris and durand parametric mixtures of gaussians can also be used to represent and segment such pixel densities ma derksen hong et al. the seminal work on spectral methods for image segmentation is the normalized cut algorithm of shi and malik related work includes that by weiss meil a and shi malik belongie leung et al. ng jordan and weiss yu and shi cour b en ezit and shi sharon galun sharon et al. tolliver and miller wang and oliensis continuous-energy-based approaches to interactive segmentation include leclerc mumford and shah chan and vese zhu and yuille tabb and ahuja discrete variants of such problems are usually optimized using binary graph cuts or other combinatorial energy minimization methods and jolly boykov and kolmogorov rother kolmogorov and blake kolmogorov and boykov cui yang wen et al. vicente kolmogorov and rother lempitsky and boykov lempitsky blake and rother although continuous optimization techniques followed by thresholding can also be used grady and ali singaraju grady and vidal criminisi sharp and blake grady bai and sapiro couprie grady najman et al. boykov and funka-lea present a good survey of various energy-based techniques for binary object segmentation. exercises ex snake evolution prove that in the absence of external forces a snake will always shrink to a small circle and eventually a single point regardless of whether first- or secondorder smoothness some combination is used. if you can show that the evolution of the xs and ys components are indepen dent you can analyze the case more easily. ex snake tracker implement a snake-based contour tracker exercises decide whether to use a large number of contour points or a smaller number interpo lated with a b-spline. define your internal smoothness energy function and decide what image-based attrac tive forces to use. at each iteration set up the banded linear system of equations energy func tion and solve it using banded cholesky factorization ex intelligent scissors implement the intelligent scissors interactive segmentation algorithm and barrett and design a graphical user interface to let you draw such curves over an image and use them for segmentation. ex region segmentation implement one of the region segmentation algorithms described in this chapter. some popular segmentation algorithms include k-means mixtures of gaussians mean shift and exercise normalized cuts similarity graph-based segmentation binary markov random fields solved using graph cuts apply your region segmentation to a video sequence and use it to track moving regions from frame to frame. alternatively test out your segmentation algorithm on the berkeley segmentation database fowlkes tal et al. ex mean shift develop a mean-shift segmentation algorithm for color images and meer convert your image to lab space or keep the original rgb colors and augment them with the pixel y locations. for every pixel a b x y compute the weighted mean of its neighbors using either a unit ball kernel or finite-radius gaussian or some other kernel of your choosing. weight the color and spatial scales differently e.g. using values of hr m as shown in figure computer vision algorithms and applications draft replace the current value with this weighted mean and iterate until either the motion is below a threshold or a finite number of steps has been taken. cluster all final values that are within a threshold i.e. find the connected components. since each pixel is associated with a final mean-shift value this results in an image segmentation i.e. each pixel is labeled with its final component. use a random subset of the pixels as starting points and find which component each unlabeled pixel belongs to either by finding its nearest neighbor or by iterating the mean shift until it finds a neighboring track of mean-shift values. describe the data structures you use to make this efficient. mean shift divides the kernel density function estimate by the local weighting to obtain a step size that is guaranteed to converge but may be slow. use an alternative step size estimation algorithm from the optimization literature to see if you can make the algorithm converge faster. chapter feature-based alignment pose estimation alignment iterative algorithms iterative algorithms alignment using least squares and feature-based alignment application panography robust least squares and ransac calibration patterns vanishing points application single view metrology rotational motion radial distortion linear algorithms application augmented reality geometric intrinsic calibration additional reading exercises computer vision algorithms and applications draft figure geometric alignment and calibration geometric alignment of images for stitching and shum acm a two-dimensional calibration target ieee calibration from vanishing points scene with easy-tofind lines and vanishing directions reid and zisserman springer. and feature-based alignment figure basic set of planar transformations once we have extracted features from images the next stage in many vision algorithms is to match these features across different images an important component of this matching is to verify whether the set of matching features is geometrically consistent e.g. whether the feature displacements can be described by a simple or geometric transformation. the computed motions can then be used in other applications such as image stitching or augmented reality in this chapter we look at the topic of geometric image registration i.e. the computation of and transformations that map features in one image to another one special case of this problem is pose estimation which is determining a camera s position relative to a known object or scene another case is the computation of a camera s intrinsic calibration which consists of the internal parameters such as focal length and radial distortion in chapter we look at the related problems of how to estimate point structure from matches and how to simultaneously estimate geometry and camera motion from motion. and feature-based alignment feature-based alignment is the problem of estimating the motion between two or more sets of matched or points. in this section we restrict ourselves to global parametric transformations such as those described in section and shown in table and figure or higher order transformation for curved surfaces and toelg can stewart roysam et al. applications to non-rigid or elastic deformations szeliski and lavall ee torresani hertzmann and bregler are examined in sections and yxsimilarityeuclideanaffineprojectivetranslation computer vision algorithms and applications draft transform matrix parameters p jacobian j translation euclidean similarity affine projective tx tx b s c tx ty ty c s a b a ty ty tx ty ty ty a b ty c x s y s x c y x x y x y x y y section table jacobians of the coordinate transformations fx p shown in table where we have re-parameterized the motions so that they are identity for p alignment using least squares given a set of matched feature points and a planar parametric of the form fx p how can we produce the best estimate of the motion parameters p? the usual way to do this is to use least squares i.e. to minimize the sum of squared residuals where els p ri fxi p is the residual between the measured location and its corresponding current predicted location fxi p. appendix for more on least squares and appendix for a statistical justification. for examples of non-planar parametric models such as quadrics see the work of shashua and toelg shashua and wexler and feature-based alignment many of the motion models presented in section and table i.e. translation similarity and affine have a linear relationship between the amount of motion x x and the unknown parameters p x x jxp where j f p is the jacobian of the transformation f with respect to the motion parameters p table in this case a simple linear regression least squares problem can be formulated as ells j t p pt ap b c. j t the minimum can be found by solving the symmetric positive definite system of normal where ap b a j t is called the hessian and b j t xi. for the case of pure translation the result ing equations have a particularly simple form i.e. the translation is the average translation between corresponding points or equivalently the translation of the point centroids. uncertainty weighting. the above least squares formulation assumes that all feature points are matched with the same accuracy. this is often not the case since certain points may fall into more textured regions than others. if we associate a scalar variance estimate i with each correspondence we can minimize the weighted least squares problem ewls i as shown in section a covariance estimate for patch-based matching can be obtained by multiplying the inverse of the patch hessian ai with the per-pixel noise covariance for poorly conditioned problems it is better to use qr decomposition on the set of linear equations jxip xi instead of the normal equations orck golub and van loan however such conditions rarely arise in image registration. problems where each measurement can have a different variance or certainty are called heteroscedastic models. computer vision algorithms and applications draft figure a simple panograph consisting of three images automatically aligned with a translational model and then averaged together. n weighting each squared residual by its inverse covariance is called the information matrix we obtain i n ai ecwls i i rt i ri n rt i airi. application panography one of the simplest most fun applications of image alignment is a special form of image stitching called panography. in a panograph images are translated and optionally rotated and scaled before being blended with simple averaging this process mimics the photographic collages created by artist david hockney although his compositions use an opaque overlay model being created out of regular photographs. in most of the examples seen on the web the images are aligned by hand for best artistic however it is also possible to use feature matching and alignment techniques to perform the registration automatically zhang and nayar zelnik-manor and perona consider a simple translational model. we want all the corresponding features in different images to line up as best as possible. let tj be the location of the jth image coordinate frame in the global composite frame and xij be the location of the ith matched feature in the jth image. in order to align the images we wish to minimize the least squares error epls xij httpwww.flickr.comgroupspanography. and feature-based alignment where xi is the consensus position of feature i in the global coordinate frame. alternative approach is to register each pair of overlapping images separately and then compute a consensus location for each frame see exercise the above least squares problem is indeterminate can add a constant offset to all the frame and point locations tj and xi. to fix this either pick one frame as being at the origin or add a constraint to make the average frame offsets be the formulas for adding rotation and scale transformations are straightforward and are left as an exercise see if you can create some collages that you would be happy to share with others on the web. iterative algorithms while linear least squares is the simplest method for estimating parameters most problems in computer vision do not have a simple linear relationship between the measurements and the unknowns. in this case the resulting problem is called non-linear least squares or non-linear regression. consider for example the problem of estimating a rigid euclidean transformation plus rotation between two sets of points. if we parameterize this transformation by the translation amount ty and the rotation angle as in table the jacobian of this transformation given in table depends on the current value of notice how in table we have re-parameterized the motion matrices so that they are always the identity at the origin p which makes it easier to initialize the motion parameters. to minimize the non-linear least squares problem we iteratively find an update p to the current parameter estimate p by minimizing p p p p enls p j t p j t pt a p pt b c where the hessian a is the same as equation and the right hand side vector b j t the hessian a is not the true hessian derivative of the non-linear least squares problem instead it is the approximate hessian which neglects second higher order derivatives of f p p. computer vision algorithms and applications draft is now a jacobian-weighted sum of residual vectors. this makes intuitive sense as the parameters are pulled in the direction of the prediction error with a strength proportional to the jacobian. once a and b have been computed we solve for p using diaga p b and update the parameter vector p p p accordingly. the parameter is an additional damping parameter used to ensure that the system takes a downhill step in energy error and is an essential component of the levenberg marquardt algorithm in more detail in appendix in many applications it can be set to if the system is successfully converging. for the case of our translationrotation we end up with a set of normal equations in the unknowns tx ty an initial guess for ty can be obtained by fitting a four-parameter similarity transform in ty c s and then setting tan an alternative approach is to estimate the translation parameters using the centroids of the points and to then estimate the rotation angle using polar coordinates for the other motion models the derivatives in table are all fairly straightforward except for the projective motion which arises in image-stitching applications these equations can be re-written from in their new parametric form as and the jacobian is therefore j f p x y x y where d is the denominator in which depends on the current parameter settings do and an initial guess for the eight unknowns can be obtained by multiplying both sides of the equations in through by the denominator which yields the linear set of equations x y x y x y however this is not optimal from a statistical point of view since the denominator d which was used to multiply each equation can vary quite a bit from point to hartley and zisserman call this strategy of forming linear equations from rational equations the direct and feature-based alignment one way to compensate for this is to reweight each equation by the inverse of the current estimate of the denominator d x y x y x y while this may at first seem to be the exact same set of equations as because least squares is being used to solve the over-determined set of equations the weightings do matter and produce a different set of normal equations that performs better in practice. the most principled way to do the estimation however is to directly minimize the squared residual equations using the gauss newton approximation i.e. performing a firstorder taylor series expansion in p as shown in which yields the set of equations x y x y while these look similar to they differ in two important respects. first the left hand side consists of unweighted prediction errors rather than point displacements and the solution vector is a perturbation to the parameter vector p. second the quantities inside j involve predicted feature locations instead of sensed feature locations both of these differences are subtle and yet they lead to an algorithm that when combined with proper checking for downhill steps in the levenberg marquardt algorithm will converge to a local minimum. note that iterating equations is not guaranteed to converge since it is not minimizing a well-defined energy function. equation is analogous to the additive algorithm for direct intensity-based registration since the change to the full transformation is being computed. if we prepend an incremental homography to the current homography instead i.e. we use a compositional algorithm in section we get d p and the above formula simplifies to y x y x x y xy xy where we have replaced with y for conciseness. how this results in the same jacobian as linear transform but that term is more commonly associated with pose estimation note also that our definition of the hij parameters differs from that used in their book since we define hii to be the difference from unity and we do not leave as a free parameter which means that we cannot handle certain extreme homographies. computer vision algorithms and applications draft robust least squares and ransac while regular least squares is the method of choice for measurements where the noise follows a normal distribution more robust versions of least squares are required when there are outliers among the correspondences there almost always are. in this case it is preferable to use an m-estimator hampel ronchetti rousseeuw et al. black and rangarajan stewart which involves applying a robust penalty function to the residuals erls p instead of squaring them. we can take the derivative of this function with respect to p and set it to p rt i ri p where is the derivative of and is called the influence function. if we introduce a weight function wr we observe that finding the stationary point of using is equivalent to minimizing the iteratively reweighted least squares problem eirls where the play the same local weighting role as in the irls algorithm alternates between computing the influence functions and solving the resulting weighted least squares problem fixed w values. other incremental robust least squares algorithms can be found in the work of sawhney and ayer black and anandan black and rangarajan baker gross ishikawa et al. and textbooks and tutorials on robust statistics hampel ronchetti rousseeuw et al. rousseeuw and leroy stewart i while m-estimators can definitely help reduce the influence of outliers in some cases starting with too many outliers will prevent irls other gradient descent algorithms from converging to the global optimum. a better approach is often to find a starting set of inlier correspondences i.e. points that are consistent with a dominant motion two widely used approaches to this problem are called random sample consensus or ransac for short and bolles and least median of squares both techniques start by selecting random a subset of k correspondences which is for pixel-based alignment methods hierarchical techniques are often used to lock onto the dominant motion in a scene. and feature-based alignment then used to compute an initial estimate for p. the residuals of the full set of correspondences are then computed as where are the estimated locations and are the sensed feature point locations. ri p the ransac technique then counts the number of inliers that are within of their predicted location i.e. whose value is application dependent but is often around pixels. least median of squares finds the median value of the values. the random selection process is repeated s times and the sample set with the largest number of inliers with the smallest median residual is kept as the final solution. either the initial parameter guess p or the full set of computed inliers is then passed on to the next data fitting stage. when the number of measurements is quite large it may be preferable to only score a subset of the measurements in an initial round that selects the most plausible hypotheses for additional scoring and selection. this modification of ransac which can significantly speed up its performance is called preemptive ransac er in another variant on ransac called prosac sample consensus random samples are initially added from the most confident matches thereby speeding up the process of finding a likely good set of inliers and matas to ensure that the random sampling has a good chance of finding a true set of inliers a sufficient number of trials s must be tried. let p be the probability that any given correspondence is valid and p be the total probability of success after s trials. the likelihood in one trial that all k random samples are inliers is pk. therefore the likelihood that s such trials will all fail is and the required minimum number of trials is p pks s p pk stewart gives examples of the required number of trials s to attain a probability of success. as you can see from table the number of trials grows quickly with the number of sample points used. this provides a strong incentive to use the minimum number of sample points k possible for any given trial which is how ransac is normally used in practice. uncertainty modeling in addition to robustly computing a good alignment some applications require the computation of uncertainty appendix for linear problems this estimate can be obtained computer vision algorithms and applications draft k p s table number of trials s to attain a probability of success by inverting the hessian matrix and multiplying it by the feature position noise these have not already been used to weight the individual measurements as in equations and in statistics the hessian which is the inverse covariance is sometimes called the information matrix when the problem involves non-linear least squares the inverse of the hessian matrix provides the cramer rao lower bound on the covariance matrix i.e. it provides the minimum amount of covariance in a given solution which can actually have a wider spread longer tails if the energy flattens out away from the local minimum where the optimal solution is found. alignment instead of aligning sets of image features many computer vision applications require the alignment of points. in the case where the transformations are linear in the motion parameters e.g. for translation similarity and affine regular least squares can be used. the case of rigid motion rxi which arises more frequently and is often called the absolute orientation problem requires slightly different techniques. if only scalar weightings are being used opposed to full per-point anisotropic covariance estimates the weighted centroids of the two point clouds c and can be used to estimate the translation t we are then left with the problem of estimating the rotation between two sets of points xi xi c and that are both centered at the origin. one commonly used technique is called the orthogonal procrustes algorithm and van loan p. and involves computing the singular value decomposition of when full covariances are used they are transformed by the rotation and so a closed-form solution for transla tion is not possible. pose estimation the correlation matrix c xt u v t the rotation matrix is then obtained as r u v t this for yourself when r x. another technique is the absolute orientation algorithm for estimating the unit quaternion corresponding to the rotation matrix r which involves forming a matrix from the entries in c and then finding the eigenvector associated with its largest positive eigenvalue. lorusso eggert and fisher experimentally compare these two techniques to two additional techniques proposed in the literature but find that the difference in accuracy is negligible below the effects of measurement noise. in situations where these closed-form algorithms are not applicable e.g. when full covariances are being used or when the alignment is part of some larger optimization the incremental rotation update introduced in section which is parameterized by an instantaneous rotation vector can be used section for an application to image stitching. in some situations e.g. when merging range data maps the correspondence between data points is not known a priori. in this case iterative algorithms that start by matching nearby points and then update the most likely correspondence can be used and mckay zhang szeliski and lavall ee gold rangarajan lu et al. david dementhon duraiswami et al. li and hartley enqvist josephson and kahl these techniques are discussed in more detail in section pose estimation a particular instance of feature-based alignment which occurs very often is estimating an object s pose from a set of point projections. this pose estimation problem is also known as extrinsic calibration as opposed to the intrinsic calibration of internal camera parameters such as focal length which we discuss in section the problem of recovering pose from three correspondences which is the minimal amount of information necessary is known as the with extensions to larger numbers of points collectively known as pnp lee ottenberg et al. quan and lan moreno-noguer lepetit and fua in this section we look at some of the techniques that have been developed to solve such problems starting with the direct linear transform which recovers a camera matrix followed by other linear algorithms and then looking at statistically optimal iterative algorithms. computer vision algorithms and applications draft linear algorithms the simplest way to recover the pose of the camera is to form a set of linear equations analogous to those used for motion estimation from the camera matrix form of perspective projection xi yi where yi are the measured feature locations and yi zi are the known feature locations as with this system of equations can be solved in a linear fashion for the unknowns in the camera matrix p by multiplying the denominator on both sides of the the resulting algorithm is called the direct linear transform and is commonly attributed to sutherland a more in-depth discussion refer to the work of hartley and zisserman in order to compute the unknowns in p at least six correspondences between and locations must be known. as with the case of estimating homographies more accurate results for the entries in p can be obtained by directly minimizing the set of equations using non-linear least squares with a small number of iterations. once the entries in p have been recovered it is possible to recover both the intrinsic calibration matrix k and the rigid transformation t by observing from equation that p krt. since k is by convention upper-triangular the discussion in section both k and r can be obtained from the front sub-matrix of p using rq factorization and van loan in most applications however we have some prior knowledge about the intrinsic calibration matrix k e.g. that the pixels are square the skew is very small and the optical center is near the center of the image such constraints can be incorporated into a non-linear minimization of the parameters in k and t as described in section in the case where the camera is already calibrated i.e. the matrix k is known we can perform pose estimation using as few as three points and bolles haralick lee ottenberg et al. quan and lan the basic observation that these linear pnp n-point algorithms employ is that the visual angle between any because p is unknown up to a scale we can either fix one of the entries e.g. or find the smallest singular vector of the set of linear equations. note the unfortunate clash of terminologies in matrix algebra textbooks r represents an upper-triangular matrix in computer vision r is an orthogonal rotation. pose estimation figure pose estimation by the direct linear transform and by measuring visual angles and distances between pairs of points. pair of points xi and xj must be the same as the angle between their corresponding points pi and pj given a set of corresponding and points xi pi where the xi are unit directions obtained by transforming pixel measurements xi to unit norm directions xi through the inverse calibration matrix k xi n k the unknowns are the distances di from the camera origin c to the points pi where the cosine law for triangle pi pj gives us pi di xi c fijdi dj i j ij where and cij cos ij xi xj ij we can take any triplet of constraints fik fjk and eliminate the dj and dk using sylvester resultants little and o shea to obtain a quartic equation in i i i i i i given five or more correspondences we can generate estimate svd for the values of triplets to obtain a linear i and lan estimates for i i i pi ijc computer vision algorithms and applications draft i can computed as ratios of successive obtain a final estimate of i hence di. i estimates and these can be averaged to i once the individual estimates of the di distances have been computed we can generate a structure consisting of the scaled point directions di xi which can then be aligned with the point cloud using absolute orientation to obtained the desired pose estimate. quan and lan give accuracy results for this and other techniques which use fewer points but require more complicated algebraic manipulations. the paper by moreno-noguer lepetit and fua reviews more recent alternatives and also gives a lower complexity algorithm that typically produces more accurate results. unfortunately because minimal pnp solutions can be quite noise sensitive and also suffer from bas-relief ambiguities depth reversals it is often preferable to use the linear six-point algorithm to guess an initial pose and then optimize this estimate using the iterative technique described in section an alternative pose estimation algorithm involves starting with a scaled orthographic projection model and then iteratively refining this initial estimate using a more accurate perspective projection model and davis the attraction of this model as stated in the paper s title is that it can be implemented in lines of code iterative algorithms the most accurate flexible way to estimate pose is to directly minimize the squared robust reprojection error for the points as a function of the unknown pose parameters in t and optionally k using non-linear least squares bogart gleicher and witkin we can write the projection equations as xi fpi r t k and iteratively minimize the robustified linearized reprojection errors enlp f r r f t t f k k where ri xi xi is the current residual vector error in predicted position and the partial derivatives are with respect to the unknown pose parameters translation and optionally calibration. note that if full covariance estimates are available for the feature locations the above squared norm can be weighted by the inverse point covariance matrix as in equation an easier to understand implement version of the above non-linear regression problem can be constructed by re-writing the projection equations as a concatenation of simpler steps each of which transforms a homogeneous coordinate pi by a simple transformation pose estimation figure a set of chained transforms for projecting a point pi to a measurement xi through a series of transformations f each of which is controlled by its own set of parameters. the dashed lines indicate the flow of information as partial derivatives are computed during a backward pass. such as translation rotation or perspective division the resulting projection equations can be written as f tpi cj pi cj f qj rqj f xi f k kk note that in these equations we have indexed the camera centers cj and camera rotation quaternions qj by an index j in case more than one pose of the calibration object is being used also section we are also using the camera center cj instead of the world translation tj since this is a more natural parameter to estimate. the advantage of this chained set of transformations is that each one has a simple partial derivative with respect both to its parameters and to its input. thus once the predicted value of xi has been computed based on the point location pi and the current values of the pose parameters qj k we can obtain all of the required partial derivatives using the chain rule ri pk ri yk yk pk where pk indicates one of the parameter vectors that is being optimized. same trick is used in neural networks as part of the backpropagation algorithm the one special case in this formulation that can be considerably simplified is the computation of the rotation update. instead of directly computing the derivatives of the rotation matrix rq as a function of the unit quaternion entries you can prepend the incremental rotation matrix r given in equation to the current rotation matrix and compute the fcx kxkfpx pzfrx rxqjftx computer vision algorithms and applications draft figure the videomouse can sense six degrees of freedom relative to a specially printed mouse pad using its embedded camera sinclair hanson et al. acm top view of the mouse view of the mouse showing the curved base for rocking moving the mouse pad with the other hand extends the interaction capabilities the resulting movement seen on the screen. partial derivative of the transform with respect to these parameters which results in a simple cross product of the backward chaining partial derivative and the outgoing vector application augmented reality a widely used application of pose estimation is augmented reality where virtual images or annotations are superimposed on top of a live video feed either through the use of seethrough glasses head-mounted display or on a regular computer or mobile device screen baillot behringer et al. haller billinghurst and thomas in some applications a special pattern printed on cards or in a book is tracked to perform the augmentation billinghurst poupyrev et al. billinghurst kato and poupyrev for a desktop application a grid of dots printed on a mouse pad can be tracked by a camera embedded in an augmented mouse to give the user control of a full six degrees of freedom over their position and orientation in a space sinclair hanson et al. as shown in figure sometimes the scene itself provides a convenient object to track such as the rectangle defining a desktop used in through-the-lens camera control and witkin in outdoor locations such as film sets it is more common to place special markers such as brightly colored balls in the scene to make it easier to find and track them in older applications surveying techniques were used to determine the locations of these balls before filming. today it is more common to apply structure-from-motion directly to the film footage itself rapid pose estimation is also central to tracking the position and orientation of the handheld remote controls used in nintendo s wii game systems. a high-speed camera embedded in the remote control is used to track the locations of the infrared leds in the bar that geometric intrinsic calibration is mounted on the tv monitor. pose estimation is then used to infer the remote control s location and orientation at very high frame rates. the wii system can be extended to a variety of other user interaction applications by mounting the bar on a hand-held device as described by johnny exercises and have you implement two different tracking and pose estimation systems for augmented-reality applications. the first system tracks the outline of a rectangular object such as a book cover or magazine page and the second has you track the pose of a hand-held rubik s cube. geometric intrinsic calibration as described above in equations the computation of the internal camera calibration parameters can occur simultaneously with the estimation of the pose of the camera with respect to a known calibration target. this indeed is the classic approach to camera calibration used in both the photogrammetry and the computer vision communities. in this section we look at alternative formulations may not involve the full solution of a non-linear regression problem the use of alternative calibration targets and the estimation of the non-linear part of camera optics such as radial calibration patterns the use of a calibration pattern or set of markers is one of the more reliable ways to estimate a camera s intrinsic parameters. in photogrammetry it is common to set up a camera in a large field looking at distant calibration targets whose exact location has been precomputed using surveying equipment atkinson kraus in this case the translational component of the pose becomes irrelevant and only the camera rotation and intrinsic parameters need to be recovered. if a smaller calibration rig needs to be used e.g. for indoor robotics applications or for mobile robots that carry their own calibration target it is best if the calibration object can span as much of the workspace as possible as planar targets often fail to accurately predict the components of the pose that lie far away from the plane. a good way to determine if the calibration has been successfully performed is to estimate the covariance in the parameters and then project points from various points in the workspace into the image in order to estimate their positional uncertainty. httpjohnnylee.netprojectswii. in some applications you can use the exif tags associated with a jpeg image to obtain a rough estimate of a camera s focal length but this technique should be used with caution as the results are often inaccurate. computer vision algorithms and applications draft figure calibrating a lens by drawing straight lines on cardboard wenger tchou et al. acm an image taken by the video camera showing a hand holding a metal ruler whose right edge appears vertical in the image the set of lines drawn on the cardboard converging on the front nodal point of projection of the lens and indicating the horizontal field of view. an alternative method for estimating the focal length and center of projection of a lens is to place the camera on a large flat piece of cardboard and use a long metal ruler to draw lines on the cardboard that appear vertical in the image as shown in figure wenger tchou et al. such lines lie on planes that are parallel to the vertical axis of the camera sensor and also pass through the lens front nodal point. the location of the nodal point vertically onto the cardboard plane and the horizontal field of view from lines that graze the left and right edges of the visible image can be recovered by intersecting these lines and measuring their angular extent if no calibration pattern is available it is also possible to perform calibration simultaneously with structure and pose recovery and which is known as selfcalibration luong and maybank hartley and zisserman moons van gool and vergauwen however such an approach requires a large amount of imagery to be accurate. planar calibration patterns when a finite workspace is being used and accurate machining and motion control platforms are available a good way to perform calibration is to move a planar calibration target in a controlled fashion through the workspace volume. this approach is sometimes called the nplanes calibration approach thorpe and kanade champleboux lavall ee szeliski et al. grossberg and nayar and has the advantage that each camera pixel can be mapped to a unique ray in space which takes care of both linear effects modeled geometric intrinsic calibration figure calibration patterns a three-dimensional target and lan ieee a two-dimensional target ieee. note that radial distortion needs to be removed from such images before the feature points can be used for calibration. by the calibration matrix k and non-linear effects such as radial distortion a less cumbersome but also less accurate calibration can be obtained by waving a planar calibration pattern in front of a camera in this case the pattern s pose has principle to be recovered in conjunction with the intrinsics. in this technique each input image is used to compute a separate homography h mapping the plane s calibration points yi into image coordinates yi xi xi yi t xi yi hpi where the ri are the first two columns of r and indicates equality up to scale. from these zhang shows how to form linear constraints on the nine entries in the b k t k matrix from which the calibration matrix k can be recovered using a matrix square root and inversion. matrix b is known as the image of the absolute conic in projective geometry and is commonly used for camera calibration and zisserman section if only the focal length is being recovered the even simpler approach of using vanishing points can be used instead. vanishing points a common case for calibration that occurs often in practice is when the camera is looking at a man-made scene with strong extended rectahedral objects such as boxes or room walls. in this case we can intersect the lines corresponding to parallel lines to compute their vanishing points as described in section and use these to determine the intrinsic and extrinsic calibration parameters and torre becker and bove liebowitz computer vision algorithms and applications draft figure calibration from vanishing points any pair of finite vanishing points xi xj can be used to estimate the focal length the orthocenter of the vanishing point triangle gives the optical center of the image c. and zisserman cipolla drummond and robertson antone and teller criminisi reid and zisserman hartley and zisserman pflugfelder let us assume that we have detected two or more orthogonal vanishing points all of which are finite i.e. they are not obtained from lines that appear to be parallel in the image plane let us also assume a simplified form for the calibration matrix k where only the focal length is unknown is often safe for rough modeling to assume that the optical center is at the center of the image that the aspect ratio is and that there is no skew. in this case the projection equation for the vanishing points can be written as rpi ri xi xi cx yi cy f where pi corresponds to one of the cardinal directions or and ri is the ith column of the rotation matrix r. from the orthogonality between columns of the rotation matrix we have ri rj cxxj cy cyyj cy f from which we can obtain an estimate for f note that the accuracy of this estimate increases as the vanishing points move closer to the center of the image. in other words it is best to tilt the calibration pattern a decent amount around the axis as in figure once the focal length f has been determined the individual columns of r can be estimated by normalizing the left hand side of and taking cross products. alternatively an svd of the initial r estimate which is a variant on orthogonal procrustes can be used. if all three vanishing points are visible and finite in the same image it is also possible to estimate the optical center as the orthocenter of the triangle formed by the three vanishing points and torre hartley and zisserman section geometric intrinsic calibration figure single view metrology reid and zisserman springer input image showing the three coordinate axes computed from the two horizontal vanishing points can be determined from the sidings on the shed a new view of the reconstruction. in practice however it is more accurate to re-estimate any unknown intrinsic calibration parameters using non-linear least squares application single view metrology a fun application of vanishing point estimation and camera calibration is the single view metrology system developed by criminisi reid and zisserman their system allows people to interactively measure heights and other dimensions as well as to build piecewiseplanar models as shown in figure the first step in their system is to identify two orthogonal vanishing points on the ground plane and the vanishing point for the vertical direction which can be done by drawing some parallel sets of lines in the image. automated techniques such as those discussed in section or by schaffalitzky and zisserman could be used. the user then marks a few dimensions in the image such as the height of a reference object and the system can automatically compute the height of another object. walls and other planar impostors can also be sketched and reconstructed. in the formulation originally developed by criminisi reid and zisserman the system produces an affine reconstruction i.e. one that is only known up to a set of independent scaling factors along each axis. a potentially more useful system can be constructed by assuming that the camera is calibrated up to an unknown focal length which can be recovered from orthogonal vanishing directions as we just described in section once this is done the user can indicate an origin on the ground plane and another point a known distance away. from this points on the ground plane can be directly projected into and computer vision algorithms and applications draft figure four images taken with a hand-held camera registered using a rotation motion model which can be used to estimate the focal length of the camera and shum acm. points above the ground plane when paired with their ground plane projections can also be recovered. a fully metric reconstruction of the scene then becomes possible. exercise has you implement such a system and then use it to model some simple scenes. section describes other potentially multi-view approaches to architectural reconstruction including an interactive piecewise-planar modeling system that uses vanishing points to establish line directions and plane normals steedly szeliski et al. rotational motion when no calibration targets or known structures are available but you can rotate the camera around its front nodal point equivalently work in a large open environment where all objects are distant the camera can be calibrated from a set of overlapping images by assuming that it is undergoing pure rotational motion as shown in figure hartley hartley hayman de agapito et al. de agapito hayman and reid kang and weiss shum and szeliski frahm and koch when a full motion is used to perform this calibration a very accurate estimate of the focal length f can be obtained as the accuracy in this estimate is proportional to the total number of pixels in the resulting cylindrical panorama shum and szeliski to use this technique we first compute the homographies h ij between all overlapping pairs of images as explained in equations then we use the observation first made in equation and explored in more detail in section that each homography is related to the inter-camera rotation rij through the calibration matrices geometric intrinsic calibration ki and kj h ij kirir j k j kirijk j the simplest way to obtain the calibration is to use the simplified form of the calibration matrix where we assume that the pixels are square and the optical center lies at the center of the image i.e. kk diagfk fk number the pixel coordinates accordingly i.e. place pixel y at the center of the image. we can then rewrite equation as k h f f f where hij are the elements of h using the orthonormality properties of the rotation matrix and the fact that the right hand side of is known only up to a scale we obtain f f and from this we can compute estimates for of f f or f if if that the equations originally given by szeliski and shum are erroneous the correct equations are given by shum and szeliski if neither of these conditions holds we can also take the dot products between the first second row and the third one. similar results can be obtained for as well by analyzing the columns of h if the focal length is the same for both images we can take the geometric mean of and as the estimated focal length f when multiple estimates of f are available e.g. from different homographies the median value can be used as the final estimate. a more general estimate of k can be obtained in the case of a fixedparameter camera ki k using the technique of hartley observe from that rij k h ijk and r t ij we obtain k h ijk kt h t ij kt h t ij k t from which we get ij k t equating rij r t h ijkkt h t ij computer vision algorithms and applications draft this provides us with some homogeneous linear constraints on the entries in a kkt which is known as the dual of the image of the absolute conic hartley and zisserman that when we estimate a homography we can only recover it up to an unknown scale. given a sufficient number of independent homography estimates h ij we can recover a to a scale using either svd or eigenvalue analysis and then recover k through cholesky decomposition extensions to the cases of temporally varying calibration parameters and non-stationary cameras are discussed by hartley hayman de agapito et al. and de agapito hayman and reid the quality of the intrinsic camera parameters can be greatly increased by constructing a full panorama since mis-estimating the focal length will result in a gap excessive overlap when the first image in the sequence is stitched to itself the resulting mis-alignment can be used to improve the estimate of the focal length and to re-adjust the rotation estimates as described in section rotating the camera by around its optic axis and re-shooting the panorama is a good way to check for aspect ratio and skew pixel problems as is generating a full hemi-spherical panorama when there is sufficient texture. ultimately however the most accurate estimate of the calibration parameters radial distortion can be obtained using a full simultaneous non-linear minimization of the intrinsic and extrinsic parameters as described in section radial distortion when images are taken with wide-angle lenses it is often necessary to model lens distortions such as radial distortion. as discussed in section the radial distortion model says that coordinates in the observed images are displaced away from distortion or towards distortion the image center by an amount proportional to their radial distance b. the simplest radial distortion models use low-order polynomials equation x y where and and are called the radial distortion parameters slama a variety of techniques can be used to estimate the radial distortion parameters for a given one of the simplest and most useful is to take an image of a scene with a lot sometimes the relationship between x and x is expressed the other way around i.e. using primed coordinates on the right-hand side x this is convenient if we map image pixels into rays and then undistort the rays to obtain rays in space i.e. if we are using inverse warping. some of today s digital cameras are starting to remove radial distortion using software in the camera itself. additional reading of straight lines especially lines aligned with and near the edges of the image. the radial distortion parameters can then be adjusted until all of the lines in the image are straight which is commonly called the plumb-line method kang el-melegy and farag exercise gives some more details on how to implement such a technique. another approach is to use several overlapping images and to combine the estimation of the radial distortion parameters with the image alignment process i.e. by extending the pipeline used for stitching in section sawhney and kumar use a hierarchy of motion models affine projective in a coarse-to-fine strategy coupled with a quadratic radial distortion correction term. they use direct minimization to compute the alignment. stein uses a feature-based approach combined with a general motion model quadratic radial distortion which requires more matches than a parallax-free rotational panorama but is potentially more general. more recent approaches sometimes simultaneously compute both the unknown intrinsic parameters and the radial distortion coefficients which may include higher-order terms or more complex rational or non-parametric forms and fitzgibbon sturm thirthala and pollefeys barreto and daniilidis hartley and kang steele and jaynes tardif sturm trudeau et al. when a known calibration target is being used the radial distortion estimation can be folded into the estimation of the other intrinsic and extrinsic parameters hartley and kang tardif sturm trudeau et al. this can be viewed as adding another stage to the general non-linear minimization pipeline shown in figure between the intrinsic parameter multiplication box f c and the perspective division box f p. exercise on more details for the case of a planar calibration target. of course as discussed in section more general models of lens distortion such as fisheye and non-central projection may sometimes be required. while the parameterization of such lenses may be more complicated the general approach of either using calibration rigs with known positions or self-calibration through the use of multiple overlapping images of a scene can both be used and kang tardif sturm and roy the same techniques used to calibrate for radial distortion can also be used to reduce the amount of chromatic aberration by separately calibrating each color channel and then warping the channels to put them back into alignment additional reading hartley and zisserman provide a wonderful introduction to the topics of feature-based alignment and optimal motion estimation as well as an in-depth discussion of camera calibration and pose estimation techniques. computer vision algorithms and applications draft techniques for robust estimation are discussed in more detail in appendix and in monographs and review articles on this topic hampel ronchetti rousseeuw et al. rousseeuw and leroy black and rangarajan stewart the most commonly used robust initialization technique in computer vision is random sample consensus and bolles which has spawned a series of more efficient variants er chum and matas the topic of registering point data sets is called absolute orientation and pose estimation eggert and fisher a variety of techniques has been developed for simultaneously computing point correspondences and their corresponding rigid transformations and mckay zhang szeliski and lavall ee gold rangarajan lu et al. david dementhon duraiswami et al. li and hartley enqvist josephson and kahl camera calibration was first studied in photogrammetry slama atkinson kraus but it has also been widely studied in computer vision gremban thorpe and kanade champleboux lavall ee szeliski et al. zhang grossberg and nayar vanishing points observed either from rectahedral calibration objects or man-made architecture are often used to perform rudimentary calibration and torre becker and bove liebowitz and zisserman cipolla drummond and robertson antone and teller criminisi reid and zisserman hartley and zisserman pflugfelder performing camera calibration without using known targets is known as self-calibration and is discussed in textbooks and surveys on structure from motion luong and maybank hartley and zisserman moons van gool and vergauwen one popular subset of such techniques uses pure rotational motion hartley hartley hayman de agapito et al. de agapito hayman and reid kang and weiss shum and szeliski frahm and koch exercises ex feature-based image alignment for flip-book animations take a set of photos of an action scene or portrait in motor-drive continuous shooting mode and align them to make a composite or flip-book animation. extract features and feature descriptors using some of the techniques described in sec tions match your features using nearest neighbor matching with a nearest neighbor distance ratio test exercises compute an optimal translation and rotation between the first image and all subsequent images using least squares with optional ransac for robustness resample all of the images onto the first image s coordinate frame using either bilinear or bicubic resampling and optionally crop them to their common area. convert the resulting images into an animated gif software available from the web or optionally implement cross-dissolves to turn them into a slo-mo video. combine this technique with feature-based morphing. ex panography create the kind of panograph discussed in section and commonly found on the web. take a series of interesting overlapping photos. use the feature detector descriptor and matcher developed in exercises existing software to match features among the images. turn each connected component of matching features into a track i.e. assign a unique index i to each track discarding any tracks that are inconsistent two different features in the same image. compute a global translation for each image using equation since your matches probably contain errors turn the above least square metric into a robust metric and re-solve your system using iteratively reweighted least squares. compute the size of the resulting composite canvas and resample each image into its final position on the canvas. track of bounding boxes will make this more efficient. average all of the images or choose some kind of ordering and implement translucent over compositing extend your parametric motion model to include rotations and scale i.e. the similarity transform given in table discuss how you could handle the case of translations and rotations only scale. write a simple tool to let the user adjust the ordering and opacity and add or remove images. computer vision algorithms and applications draft write down a different least squares problem that involves pairwise matching of images. discuss why this might be better or worse than the global matching formula given in ex rigideuclidean matching several alternative approaches are given in section for estimating a rigid alignment. implement the various alternatives and compare their accuracy on synthetic data i.e. random point clouds with noisy feature positions. one approach is to estimate the translations from the centroids and then estimate rotation in polar coordinates. do you need to weight the angles obtained from a polar decomposition in some way to get the statistically correct estimate? how can you modify your techniques to take into account either scalar or full two-dimensional point covariance weightings do all of the previously developed shortcuts still work or does full weighting require iterative optimization? ex match moveaugmented reality replace a picture in a magazine or a book with a different image or video. with a webcam take a picture of a magazine or book page. outline a figure or picture on the page with a rectangle i.e. draw over the four sides as they appear in the image. match features in this area with each new image frame. replace the original image with an advertising insert warping the new image with the appropriate homography. try your approach on a clip from a sporting event indoor or outdoor soccer to implement a billboard replacement. ex joystick track a rubik s cube to implement a joystickmouse control. get out an old rubik s cube get one from your parents. write a program to detect the center of each colored square. group these centers into lines and then find the vanishing points for each face. estimate the rotation angle and focal length from the vanishing points. exercises estimate the full pose translation by finding one or more grids and recovering the plane s full equation from this known homography using the technique developed by zhang alternatively since you already know the rotation simply estimate the unknown translation from the known corner points on the cube and their measured locations using either linear or non-linear least squares. use the rotation and position to control a vrml or game viewer. ex rotation-based calibration take an outdoor or indoor sequence from a rotating camera with very little parallax and use it to calibrate the focal length of your camera using the techniques described in section or sections take out any radial distortion in the images using one of the techniques from exer cises or using parameters supplied for a given camera by your instructor. detect and match feature points across neighboring frames and chain them into feature tracks. compute homographies between overlapping frames and use equations to get an estimate of the focal length. compute a full panorama and update your focal length estimate to close the gap perform a complete bundle adjustment in the rotation matrices and focal length to obtain the highest quality estimate ex target-based calibration use a three-dimensional target to calibrate your camera. construct a three-dimensional calibration pattern with known locations. it is not easy to get high accuracy unless you use a machine shop but you can get close using heavy plywood and printed patterns. find the corners e.g using a line finder and intersecting the lines. implement one of the iterative calibration and pose estimation algorithms described in tsai bogart gleicher and witkin or the system described in section take many pictures at different distances and orientations relative to the calibration target and report on both your re-projection errors and accuracy. do the latter you may need to use simulated data. computer vision algorithms and applications draft ex calibration accuracy compare the three calibration techniques rotationbased and one approach is to have a different student implement each one and to compare the results. another approach is to use synthetic data potentially re-using the software you developed for exercise the advantage of using synthetic data is that you know the ground truth for the calibration and pose parameters you can easily run lots of experiments and you can synthetically vary the noise in your measurements. here are some possible guidelines for constructing your test sets assume a medium-wide focal length field of view. for the plane-based technique generate a grid target and project it at different inclinations. for a target create an inner cube corner and position it so that it fills most of field of view. for the rotation technique scatter points uniformly on a sphere until you get a similar number of points as for other techniques. before comparing your techniques predict which one will be the most accurate your results by the square root of the number of points used. add varying amounts of noise to your measurements and describe the noise sensitivity of your various techniques. ex single view metrology implement a system to measure dimensions and reconstruct a model from a single image of a man-made scene using visible vanishing directions reid and zisserman find the three orthogonal vanishing points from parallel lines and use them to establish the three coordinate axes matrix r of the camera relative to the scene. if two of the vanishing points are finite at infinity use them to compute the focal length assuming a known optical center. otherwise find some other way to calibrate your camera you could use some of the techniques described by schaffalitzky and zisserman click on a ground plane point to establish your origin and click on a point a known distance away to establish the scene scale. this lets you compute the translation t between the camera and the scene. as an alternative click on a pair of points one on the ground plane and one above it and use the known height to establish the scene scale. exercises write a user interface that lets you click on ground plane points to recover their locations. you already know the camera matrix so knowledge of a point s z value is sufficient to recover its location. click on pairs of points on the ground plane one above it to measure vertical heights. extend your system to let you draw quadrilaterals in the scene that correspond to axisaligned rectangles in the world using some of the techniques described by sinha steedly szeliski et al. export your rectangles to a vrml or file. warp the pixels enclosed by the quadrilateral using the correct homography to produce a texture map for each planar polygon. ex radial distortion with plumb lines mine the radial distortion parameters. implement a plumb-line algorithm to deter take some images of scenes with lots of straight lines e.g. hallways in your home or office and try to get some of the lines as close to the edges of the image as possible. extract the edges and link them into curves as described in section and exer cise fit quadratic or elliptic curves to the linked edges using a generalization of the successive line approximation algorithm described in section and exercise and keep the curves that fit this form well. for each curved segment fit a straight line and minimize the perpendicular distance between the curve and the line while adjusting the radial distortion parameters. alternate between re-fitting the straight line and adjusting the radial distortion param eters until convergence. ex radial distortion with a calibration target use a grid calibration target to determine the radial distortion parameters. print out a planar calibration target mount it on a stiff board and get it to fill your field of view. detect the squares lines or dots in your calibration target. estimate the homography mapping the target to the camera from the central portion of the image that does not have any radial distortion. httpmeshlab.sf.net. computer vision algorithms and applications draft predict the positions of the remaining targets and use the differences between the ob served and predicted positions to estimate the radial distortion. fit a general spline model severe distortion instead of the quartic dis tortion model. extend your technique to calibrate a fisheye lens. ex chromatic aberration use the radial distortion estimates for each color channel computed in the previous exercise to clean up wide-angle lens images by warping all of the channels into alignment. straighten out the images at the same time. can you think of any reasons why this warping strategy may not always work? chapter structure from motion triangulation two-frame structure from motion factorization bundle adjustment application view morphing projective reconstruction self-calibration perspective and projective factorization application sparse model extraction exploiting sparsity application match move and augmented reality uncertainty and ambiguities application reconstruction from internet photos line-based techniques constrained structure and motion plane-based techniques additional reading exercises computer vision algorithms and applications draft figure structure from motion systems d orthographic factorization and kanade springer f line matching and zisserman ieee k incremental structure from motion seitz and szeliski reconstruction of trafalgar square seitz and szeliski reconstruction of the great wall of china seitz and szeliski reconstruction of the old town square prague seitz and szeliski acm. triangulation in the previous chapter we saw how and point sets could be aligned and how such alignments could be used to estimate both a camera s pose and its internal calibration parameters. in this chapter we look at the converse problem of estimating the locations of points from multiple images given only a sparse set of correspondences between image features. while this process often involves simultaneously estimating both geometry and camera pose it is commonly known as structure from motion the topics of projective geometry and structure from motion are extremely rich and some excellent textbooks and surveys have been written on them and luong hartley and zisserman moons van gool and vergauwen this chapter skips over a lot of the richer material available in these books such as the trifocal tensor and algebraic techniques for full self-calibration and concentrates instead on the basics that we have found useful in large-scale image-based reconstruction problems seitz and szeliski we begin with a brief discussion of triangulation which is the problem of estimating a point s location when it is seen from multiple cameras. next we look at the two-frame structure from motion problem which involves the determination of the epipolar geometry between two cameras and which can also be used to recover certain information about the camera intrinsics using self-calibration section looks at factorization approaches to simultaneously estimating structure and motion from large numbers of point tracks using orthographic approximations to the projection model. we then develop a more general and useful approach to structure from motion namely the simultaneous bundle adjustment of all the camera and structure parameters we also look at special cases that arise when there are higher-level structures such as lines and planes in the scene triangulation the problem of determining a point s position from a set of corresponding image locations and known camera positions is known as triangulation. this problem is the converse of the pose estimation problem we studied in section one of the simplest ways to solve this problem is to find the point p that lies closest to all of the rays corresponding to the matching feature locations observed by cameras j kjrjtj where tj rjcj and cj is the jth camera center as you can see in figure these rays originate at cj in a direction vj n j xj. the nearest point to p on this ray which we denote as qj minimizes the distance j k dj vj computer vision algorithms and applications draft figure point triangulation by finding the point p that lies nearest to all of the optical rays cj dj vj. which has a minimum at dj vj cj. hence qj cj vj vt j cj cj in the notation of equation and the squared distance between p and qj is j vj vt j cj the optimal value for p which lies closest to all of the rays can be computed as a regular least squares problem by summing over all the j and finding the optimal value of p p j j vj vt vj vt an alternative formulation which is more statistically optimal and which can produce significantly better estimates if some of the cameras are closer to the point than others is to minimize the residual in the measurement equations x pj xj pj x pj pj x pj yj pj pj x pj y pj y pj y pj y pj z pj z pj z pj z pj w w w w where yj are the measured feature locations and in camera matrix p j are the known entries as with equations and this set of non-linear equations can be converted into a linear least squares problem by multiplying both sides of the denominator. note that if pj two-frame structure from motion we use homogeneous coordinates p y z w the resulting set of equations is homogeneous and is best solved as a singular value decomposition or eigenvalue problem for the smallest singular vector or eigenvector. if we set w we can use regular linear least squares but the resulting system may be singular or poorly conditioned i.e. if all of the viewing rays are parallel as occurs for points far away from the camera. for this reason it is generally preferable to parameterize points using homogeneous coordinates especially if we know that there are likely to be points at greatly varying distances from the cameras. of course minimizing the set of observations using nonlinear least squares as described in and is preferable to using linear least squares regardless of the representation chosen. for the case of two observations it turns out that the location of the point p that exactly minimizes the true reprojection error can be computed using the solution of degree six equations and sturm another problem to watch out for with triangulation is the issue of chirality i.e. ensuring that the reconstructed points lie in front of all the cameras while this cannot always be guaranteed a useful heuristic is to take the points that lie behind the cameras because their rays are diverging figure where the rays were pointing away from each other and to place them on the plane at infinity by setting their w values to two-frame structure from motion so far in our study of reconstruction we have always assumed that either the point positions or the camera poses are known in advance. in this section we take our first look at structure from motion which is the simultaneous recovery of structure and pose from image correspondences. consider figure which shows a point p being viewed from two cameras whose relative position can be encoded by a rotation r and a translation t. since we do not know anything about the camera positions without loss of generality we can set the first camera at the origin and at a canonical orientation i. now notice that the observed location of point p in the first image is mapped into the second image by the transformation t t where xj k sides with t in order to annihilate it on the right hand side j xj are the ray direction vectors. taking the cross product of both the cross-product operator was introduced in r computer vision algorithms and applications draft figure epipolar geometry the vectors t p and p are co-planar and define the basic epipolar constraint expressed in terms of the pixel measurements and taking the dot product of both sides with yields xt r xt since the right hand side is a triple product with two identical entries. way to say this is that the cross product matrix is skew symmetric and returns when pre- and post-multiplied by the same vector. we therefore arrive at the basic epipolar constraint xt e where e r is called the essential matrix an alternative way to derive the epipolar constraint is to notice that in order for the cameras to be oriented so that the rays and intersect in at point p the vectors connecting the two camera centers r t and the rays corresponding to pixels and namely r j xj must be co-planar. this requires that the triple product r r t r xt notice that the essential matrix e maps a point in image into a line e in image since xt all such lines must pass through the second epipole which is therefore defined as the left singular vector of e with a singular value or equivalently the projection of the vector t into image the dual of these r epipolar planep two-frame structure from motion relationships gives us the epipolar line in the first image as et and as the zerovalue right singular vector of e. given this fundamental relationship how can we use it to recover the camera motion encoded in the essential matrix e? if we have n corresponding measurements we can form n homogeneous equations in the nine elements of e where xij yij this can be written more compactly as xt e zi e zi f where indicates an element-wise multiplication and summation of matrix elements and zi and e given n and f are the rasterized forms of the zi xt such equations we can compute an estimate to scale for the entries in e using an svd. in the presence of noisy measurements how close is this estimate to being statistically optimal? if you look at the entries in you can see that some entries are the products of image measurements such as and others are direct image measurements even the identity. if the measurements have comparable noise the terms that are products of measurements have their noise amplified by the other element in the product which can lead to very poor scaling e.g. an inordinately large influence of points with large coordinates away from the image center. in order to counteract this trend hartley suggests that the point coordinates should be translated and scaled so that their centroid lies at the origin and their variance is unity i.e. xi sxi x yi sxi y i such xi yi once the essential matrix e has been computed from the transformed coordinates where xij t j xij the original essential matrix e can be recovered as i where n is the number of e t et we use f instead of e to denote the rasterized form of e to avoid confusion with the epipoles ej. more precisely hartley suggests scaling the points so that the average distance from the origin is equal but the heuristic of unit variance is faster to compute not require per-point square roots and should to yield comparable improvements. computer vision algorithms and applications draft in his paper hartley compares the improvement due to his re-normalization strategy to alternative distance measures proposed by others such as zhang and concludes that his simple re-normalization in most cases is as effective as better than alternative techniques. torr and fitzgibbon recommend a variant on this algorithm where the norm of the upper sub-matrix of e is set to and show that it has even better stability with respect to coordinate transformations. once an estimate for the essential matrix e has been recovered the direction of the translation vector t can be estimated. note that the absolute distance between the two cameras can never be recovered from pure image measurements alone regardless of how many cameras or points are used. knowledge about absolute camera and point positions or distances often called ground control points in photogrammetry is always required to establish the final scale position and orientation. to estimate this direction t observe that under ideal noise-free conditions the essential e this singularity shows up as a singular value of when t matrix e is singular i.e. t an svd of e is performed vt vt vt e t r u v t t when e is computed from noisy measurements the singular vector associated with the smallest singular value gives us t. other two singular values should be similar but are not in general equal to because e is only computed up to an unknown scale. because e is rank-deficient it turns out that we actually only need seven correspondences of the form of equation instead of eight to estimate this matrix torr and murray hartley and zisserman advantage of using fewer correspondences inside a ransac robust fitting stage is that fewer random samples need to be generated. from this set of seven homogeneous equations we can stack into a matrix for svd analysis we can find two independent vectors say f and f such that zi f j these two vectors can be converted back into matrices and which span the solution space for e to find the correct value of we observe that e has a zero determinant since it is rank deficient and hence det this gives us a cubic equation in which has either one or three solutions substituting these values into to obtain e we can test this essential matrix against other unused feature correspondences to select the correct one. two-frame structure from motion once t has been recovered how can we estimate the corresponding rotation matrix r? recall that the cross-product operator t projects a vector onto a set of orthogonal basis vectors that include t zeros out the t component and rotates the other two by t st t where t from equations and we get e t r st r u v t st st t t from which we can conclude that s u. recall that for a noise-free essential matrix z and hence u t r v t and unfortunately we only know both e and t up to a sign. furthermore the matrices u and v are not guaranteed to be rotations can flip both their signs and still get a valid svd. for this reason we have to generate all four possible rotation matrices r u rt v t r u rt v t and keep the two whose determinant to disambiguate between the remaining pair of potential rotations which form a twisted pair and zisserman p. we need to pair them with both possible signs of the translation direction t and select the combination for which the largest number of points is seen in front of both the property that points must lie in front of the camera i.e. at a positive distance along the viewing rays emanating from the camera is known as chirality in addition to determining the signs of the rotation and translation as described above the chirality of the distances of the points in a reconstruction can be used inside a ransac procedure with the reprojection errors to distinguish between likely and unlikely chirality can also be used to transform projective reconstructions and into quasi-affine reconstructions the normalized eight-point algorithm described above is not the only way to estimate the camera motion from correspondences. variants include using seven points in the noise-free case a single point suffices. it is safer however to test all or a sufficient subset of points downweighting the ones that lie close to the plane at infinity for which it is easy to get depth reversals. note that as points get further away from a camera i.e. closer toward the plane at infinity errors in chirality become more likely. computer vision algorithms and applications draft figure pure translational camera motion results in visual motion where all the points move towards away from a common focus of expansion e. they therefore satisfy the triple product condition e e while enforcing the rank two constraint in e and a five-point algorithm that requires finding the roots of a degree polynomial er since such algorithms use fewer points to compute their estimates they are less sensitive to outliers when used as part of a random sampling strategy. pure translation rotation in the case where we know the rotation we can pre-rotate the points in the second image to match the viewing direction of the first. the resulting set of points all move towards away from the focus of expansion as shown in figure the resulting essential matrix e is the noise-free case skew symmetric and so can be estimated more directly by setting eij eji and eii in two points with non-zero parallax now suffice to estimate the foe. a more direct derivation of the foe estimate can be obtained by minimizing the triple product which is equivalent to finding the null space for the set of equations note that as in the eight-point algorithm it is advisable to normalize the points to have unit variance before computing this estimate. in situations where a large number of points at infinity are available e.g. when shooting outdoor scenes or when the camera motion is small compared to distant objects this suggests an alternative ransac strategy for estimating the camera motion. first pick a pair of points to estimate a rotation hoping that both of the points lie at infinity far from the fans of star trek and star wars will recognize this as the jump to hyperdrive visual effect. two-frame structure from motion camera. then compute the foe and check whether the residual error is small agreement with this rotation hypothesis and whether the motions towards or away from the epipole are all in the same direction very small motions which may be noise-contaminated. pure rotation the case of pure rotation results in a degenerate estimate of the essential matrix e and of the translation direction t. consider first the case of the rotation matrix being known. the estimates for the foe will be degenerate since and hence is degenerate. a similar argument shows that the equations for the essential matrix are also rankdeficient. this suggests that it might be prudent before computing a full essential matrix to first compute a rotation estimate r using potentially with just a small number of points and then compute the residuals after rotating the points before proceeding with a full e computation. projective reconstruction in many cases such as when trying to build a model from internet or legacy photos taken by unknown cameras without any exif tags we do not know ahead of time the intrinsic calibration parameters associated with the input images. in such situations we can still estimate a two-frame reconstruction although the true metric structure may not be available e.g. orthogonal lines or planes in the world may not end up being reconstructed as orthogonal. consider the derivations we used to estimate the essential matrix e in the uncalibrated case we do not know the calibration matrices kj so we cannot use the normalized ray directions xj k j xj. instead we have access only to the image coordinates xj and so the essential matrix becomes xt e xt f k t ek xt where f k t ek h is called the fundamental matrix hartley gupta and chang hartley and zisserman like the essential matrix the fundamental matrix is principle rank two f h u v t vt vt et computer vision algorithms and applications draft its smallest left singular vector indicates the epipole in the image and its smallest right singular vector is the homography h in which in principle should equal h k t rk cannot be uniquely recovered from f since any homography of the form h evt results in the same f matrix. that annihilates any multiple of e. any one of these valid homographies h maps some plane in the scene from one image to the other. it is not possible to tell in advance which one it is without either selecting four or more co-planar correspondences to compute h as part of the f estimation process a manner analogous to guessing a rotation for e or mapping all points in one image through h and seeing which ones line up with their corresponding locations in the in order to create a projective reconstruction of the scene we can pick any valid homography h that satisfies equation for example following a technique analogous to equations we get f h st h u v t and hence h u rt v t where is the singular value matrix with the smallest value replaced by a reasonable alternative the middle we can then form a pair of camera matrices p and p he from which a projective reconstruction of the scene can be computed using triangulation while the projective reconstruction may not be useful in practice it can often be upgraded to an affine or metric reconstruction as detailed below. even without this step however the fundamental matrix f can be very useful in finding additional correspondences as they must all lie on corresponding epipolar lines i.e. any feature in image must have its correspondence lying on the associated epipolar line f in image assuming that the point motions are due to a rigid transformation. this process is sometimes referred to as plane plus parallax anandan and hanna sawhney hartley and zisserman p. recommend using h f and vi eville which places the camera on the plane at infinity. two-frame structure from motion self-calibration the results of structure from motion computation are much more useful intelligible if a metric reconstruction is obtained i.e. one in which parallel lines are parallel orthogonal walls are at right angles and the reconstructed model is a scaled version of reality. over the years a large number of self-calibration auto-calibration techniques have been developed for converting a projective reconstruction into a metric one which is equivalent to recovering the unknown calibration matrices kj associated with each image and zisserman moons van gool and vergauwen in situations where certain additional information is known about the scene different methods may be employed. for example if there are parallel lines in the scene having several lines converge on the same vanishing point is good evidence three or more vanishing points which are the images of points at infinity can be used to establish the homography for the plane at infinity from which focal lengths and rotations can be recovered. if two or more finite orthogonal vanishing points have been observed the single-image calibration method based on vanishing points can be used instead. in the absence of such external information it is not possible to recover a fully parameterized independent calibration matrix kj for each image from correspondences alone. to see this consider the set of all camera matrices p j kjrjtj projecting world coordinates pi yi zi wi into screen coordinates xij p jpi. now consider transforming the scene through an arbitrary projective transformation h yielding a new model consisting of points hpi. post-multiplying each p j matrix by h still produces the same screen coordinates and a new set calibration matrices can be computed by applying rq decomposition to the new camera matrix p p j h for this reason all self-calibration methods assume some restricted form of the calibration matrix either by setting or equating some of their elements or by assuming that they do not vary over time. while most of the techniques discussed by hartley and zisserman moons van gool and vergauwen require three or more frames in this section we present a simple technique that can recover the focal lengths of both images from the fundamental matrix f in a two-frame reconstruction and zisserman p. to accomplish this we assume that the camera has zero skew a known aspect ratio set to and a known optical center as in equation how reasonable is this assumption in practice? the answer as with many questions is it depends if absolute metric accuracy is required as in photogrammetry applications it is imperative to pre-calibrate the cameras using one of the techniques from section and to use ground control points to pin down the reconstruction. if instead we simply wish to reconstruct the world for visualization or image-based rendering applications as in the photo tourism system of snavely seitz and szeliski this assumption is quite reasonable in practice. computer vision algorithms and applications draft most cameras today have square pixels and an optical center near the middle of the image and are much more likely to deviate from a simple camera model due to radial distortion which should be compensated for whenever possible. the biggest problems occur when images have been cropped off-center in which case the optical center will no longer be in the middle or when perspective pictures have been taken of a different picture in which case a general camera matrix becomes given these caveats the two-frame focal length estimation algorithm based on the kruppa equations developed by hartley and zisserman p. proceeds as follows. take the left and right singular vectors of the fundamental matrix f and their associated singular values and form the following set of equations ut ut ut where the two matrices dj kjkt j diagf j f j f j f j encode the unknown focal lengths. for simplicity let us rewrite each of the numerators and denominators in as ut i jvt i aij bijf i cij dijf notice that each of these is affine plus constant in either f can cross-multiply these equations to obtain quadratic equations in f be solved. also the work by bougnoux for some alternative formulations. hence we or f j which can readily an alternative solution technique is to observe that we have a set of three equations related by an unknown scalar i.e. and hence hartley personal communication july these can readily be solved to yield f how well does this approach work in practice? there are certain degenerate configurations such as when there is no rotation or when the optical axes intersect when it does not work at all. such a situation you can vary the focal lengths of the cameras and obtain in photo tourism our system registered photographs of an information sign outside notre dame with real pictures of the cathedral. factorization a deeper or shallower reconstruction which is an example of a bas-relief ambiguity hartley and zisserman recommend using techniques based on three or more frames. however if you find two images for which the estimates of are well conditioned they can be used to initialize a more complete bundle adjustment of all the parameters an alternative which is often used in systems such as photo tourism is to use camera exif tags or generic default values to initialize focal length estimates and refine them as part of bundle adjustment. f application view morphing an interesting application of basic two-frame structure from motion is view morphing known as view interpolation see section which can be used to generate a smooth animation from one view of a scene to another and williams seitz and dyer to create such a transition you must first smoothly interpolate the camera matrices i.e. the camera positions orientations and focal lengths. while simple linear interpolation can be used rotations as quaternions a more pleasing effect is obtained by easing in and easing out the camera parameters e.g. using a raised cosine as well as moving the camera along a more circular trajectory seitz and szeliski to generate in-between frames either a full set of correspondences needs to be established or models must be created for each reference view. section describes several widely used approaches to this problem. one of the simplest is to just triangulate the set of matched feature points in each image e.g. using delaunay triangulation. as the points are re-projected into their intermediate views pixels can be mapped from their original source images to their new views using affine or projective mapping and shum the final image is then composited using a linear blend of the two reference images as with usual morphing factorization when processing video sequences we often get extended feature tracks from which it is possible to recover the structure and motion using a process called factorization. consider the tracks generated by a rotating ping pong ball which has been marked with dots to make its shape and motion more discernable we can readily see from the shape of the tracks that the moving object must be a sphere but how can we infer this mathematically? it turns out that under orthography or related models we discuss below the shape and motion can be recovered simultaneously using a singular value decomposition and computer vision algorithms and applications draft figure reconstruction of a rotating ping pong ball using factorization and kanade springer sample image with tracked features overlaid subsampled feature motion stream two views of the reconstructed model. kanade consider the orthographic and weak perspective projection models introduced in equations since the last row is always there is no perspective division and we can write where xji is the location of the ith point in the jth frame p j is the upper portion of the projection matrix p j and pi yi zi is the augmented point let us assume now that every point i is visible in every frame j. we can take the xji p j pi centroid of the projected point locations xji in frame j xj n xji p j n pi p j c where c x y z is the augmented centroid of the point cloud. since world coordinate frames in structure from motion are always arbitrary i.e. we cannot recover true locations without ground control points measurements we can place the origin of the world at the centroid of the points i.e x y z so that c we see from this that the centroid of the points in each frame xj directly gives us the last element of p j. let xji xji xj be the point locations after their image centroid has been sub tracted. we can now write xji m jpi in this section we index the point positions as xji instead of xij since this is the convention adopted by factorization papers and kanade and is consistent with the factorization given in factorization where m j is the upper portion of the projection matrix p j and pi yi zi. we can concatenate all of these measurement equations into one large matrix m m j m m pi m s. x xjn xji xm xm i xm n x is called the measurement matrix and m and s are the motion and structure matrices respectively and kanade because the motion matrix m is and the structure matrix s is n an svd applied to x has only three non-zero singular values. in the case where the measurements in x are noisy svd returns the rank-three factorization of x that is the closest to x in a least squares sense and kanade golub and van loan hartley and zisserman it would be nice if the svd of x u v t directly returned the matrices m and s but it does not. instead we can write the relationship x u v t qq v t and set m u q and s q v t how can we recover the values of the matrix q? this depends on the motion model being used. in the case of orthographic projection the entries in m j are the first two rows of rotation matrices rj so we have ut ut ut where uk are the rows of the matrix u. this gives us a large set of equations for the entries in the matrix qqt from which the matrix q can be recovered using a matrix square root if we have scaled orthography i.e. m j sjrj the first and third equations are equal to sj and can be set equal to each other. note that even once q has been recovered there still exists a bas-relief ambiguity i.e. we can never be sure if the object is rotating left to right or if its depth reversed version is moving the other way. can be seen in the classic rotating necker cube visual illusion. tomasi and kanade first take the square root of and distribute this to u and v but there is no particular reason to do this. computer vision algorithms and applications draft additional cues such as the appearance and disappearance of points or perspective effects both of which are discussed below can be used to remove this ambiguity. for motion models other than pure orthography e.g. for scaled orthography or paraperspective the approach above must be extended in the appropriate manner. such techniques are relatively straightforward to derive from first principles more details can be found in papers that extend the basic factorization approach to these more flexible models and kanade additional extensions of the original factorization algorithm include multi-body rigid motion and kanade sequential updates to the factorization and kanade the addition of lines and planes and kanade and re-scaling the measurements to incorporate individual location uncertainties and irani a disadvantage of factorization approaches is that they require a complete set of tracks i.e. each point must be visible in each frame in order for the factorization approach to work. tomasi and kanade deal with this problem by first applying factorization to smaller denser subsets and then using known camera or point estimates to hallucinate additional missing values which allows them to incrementally incorporate more features and cameras. huynh hartley and heyden extend this approach to view missing data as special cases of outliers. buchanan and fitzgibbon develop fast iterative algorithms for performing large matrix factorizations with missing data. the general topic of principal component analysis with missing data also appears in other computer vision problems ikeuchi and reddy de la torre and black gross matthews and baker torresani hertzmann and bregler vidal ma and sastry perspective and projective factorization another disadvantage of regular factorization is that it cannot deal with perspective cameras. one way to get around this problem is to perform an initial affine orthographic reconstruction and to then correct for the perspective effects in an iterative manner and horaud observe that the object-centered projection model rxj pi txj jrzj pi ryj pi tyj jrzj pi xji sj yji sj differs from the scaled orthographic projection model by the inclusion of the denominator terms jrzj assuming that the optical center cy lies at and that pixels are square. factorization if we knew the correct values of j t zj and the structure and motion parameters rj and pi we could cross-multiply the left hand side point measurements xji and yji by the denominator and get corrected values for which the bilinear projection model is exact. in practice after an initial reconstruction the values of j can be estimated independently for each frame by comparing reconstructed and sensed point positions. third row of the rotation matrix rzj is always available as the cross-product of the first two rows. note that since the j are determined from the image measurements the cameras do not have to be pre-calibrated i.e. their focal lengths can be recovered from fj sj j. once the j have been estimated the feature locations can then be corrected before applying another round of factorization. note that because of the initial depth reversal ambiguity both reconstructions have to be tried while calculating j. incorrect reconstruction will result in a negative j which is not physically meaningful. christy and horaud report that their algorithm usually converges in three to five iterations with the majority of the time spent in the svd computation. an alternative approach which does not assume partially calibrated cameras optical center square pixels and zero skew is to perform a fully projective factorization and triggs triggs in this case the inclusion of the third row of the camera matrix in is equivalent to multiplying each reconstructed measurement xji m jpi by its inverse depth ji d ji or equivalently multiplying each measured position by its projective depth dji m s. in the original paper by sturm and triggs the projective depths dji are obtained from two-frame reconstructions while in later work oliensis and hartley they are initialized to dji and updated after each iteration. oliensis and hartley present an update formula that is guaranteed to converge to a fixed point. none of these authors suggest actually estimating the third row of p j as part of the projective depth computations. in any case it is unclear when a fully projective reconstruction would be preferable to a partially calibrated one especially if they are being used to initialize a full bundle adjustment of all the parameters. one of the attractions of factorization methods is that they provide a closed form called a linear method to initialize iterative techniques such as bundle adjustment. an alternative initialization technique is to estimate the homographies corresponding to some x dji xji djn xjn dm xm dm i xm i dm n xm n computer vision algorithms and applications draft figure teacup model reconstructed from a video sequence and kanade springer first frame of video last frame of video side view of model top view of model. common plane seen by all the cameras and carlsson in a calibrated camera setting this can correspond to estimating consistent rotations for all of the cameras for example using matched vanishing points and teller once these have been recovered the camera positions can then be obtained by solving a linear system and teller rother and carlsson rother application sparse model extraction once a multi-view reconstruction of the scene has been estimated it then becomes possible to create a texture-mapped model of the object and to look at it from new directions. the first step is to create a denser model than the sparse point cloud that structure from motion produces. one alternative is to run dense multi-view stereo alternatively a simpler technique such as triangulation can be used as shown in figure in which reconstructed points are triangulated to produce a surface mesh. in order to create a more realistic model a texture map can be extracted for each triangle face. the equations to map points on the surface of a triangle to a image are straightforward just pass the local coordinates on the triangle through the camera projection matrix to obtain a homography perspective projection. when multiple source images are available as is usually the case in multi-view reconstruction either the closest and most fronto-parallel image can be used or multiple images can be blended in to deal with view-dependent foreshortening kang szeliski et al. or to obtain super-resolved results and cremers another alternative is to create a separate texture map from each reference camera and to blend between them during rendering which is known as view-dependent texture mapping taylor and malik debevec yu and borshukov bundle adjustment figure a set of chained transforms for projecting a point pi into a measurement xij through a series of transformations f each of which is controlled by its own set of parameters. the dashed lines indicate the flow of information as partial derivatives are computed during a backward pass. the formula for the radial distortion function is f rdx bundle adjustment as we have mentioned several times before the most accurate way to recover structure and motion is to perform robust non-linear minimization of the measurement errors which is commonly known in the photogrammetry now computer vision communities as bundle triggs mclauchlan hartley et al. provide an excellent overview of this topic including its historical development pointers to the photogrammetry literature atkinson kraus and subtle issues with gauge ambiguities. the topic is also treated in depth in textbooks and surveys on multi-view geometry and luong hartley and zisserman moons van gool and vergauwen we have already introduced the elements of bundle adjustment in our discussion on iterative pose estimation i.e. equations and figure the biggest difference between these formulas and full bundle adjustment is that our feature location measurements xij now depend not only on the point index i but also on the camera pose index j xij fpi rj cj kj and that the point positions pi are also being simultaneously updated. in addition it is common to add a stage for radial distortion parameter estimation f rdx if the cameras being used have not been pre-calibrated as shown in figure the term bundle refers to the bundles of rays connecting camera centers to points and the term adjustment refers to the iterative minimization of re-projection error. alternative terms for this in the vision community include optimal motion estimation ahuja and huang and non-linear least squares kriegman and anandan szeliski and kang fcx kxfjfpx pzfrx rjxqjftx j ijxijxij computer vision algorithms and applications draft while most of the boxes in figure have previously been explained the leftmost box has not. this box performs a robust comparison of the predicted and measured locations xij and xij after re-scaling by the measurement noise covariance ij. in more detail this operation can be written as rij xij xij ij ij rt ij rij eij ij where the corresponding jacobians derivatives can be written as eij ij ij xij ij ij rij. the advantage of the chained representation introduced above is that it not only makes the computations of the partial derivatives and jacobians simpler but it can also be adapted to any camera configuration. consider for example a pair of cameras mounted on a robot that is moving around in the world as shown in figure by replacing the rightmost two transformations in figure with the transformations shown in figure we can simultaneously recover the position of the robot at each time and the calibration of each camera with respect to the rig in addition to the structure of the world. exploiting sparsity large bundle adjustment problems such as those involving reconstructing scenes from thousands of internet photographs seitz and szeliski agarwal snavely simon et al. agarwal furukawa snavely et al. snavely simon goesele et al. can require solving non-linear least squares problems with millions of measurements matches and tens of thousands of unknown parameters point positions and camera poses. unless some care is taken these kinds of problem can become intractable since the solution of dense least squares problems is cubic in the number of unknowns. fortunately structure from motion is a bipartite problem in structure and motion. each feature point xij in a given image depends on one point position pi and one camera pose cj. this is illustrated in figure where each circle indicates a point each square d indicates a camera and lines indicate which points are visible in which cameras features. if the values for all the points are known or fixed the equations for all the cameras become independent and vice versa. bundle adjustment figure a camera rig and its associated transform chain. as the mobile rig moves around in the world its pose with respect to the world at time t is captured by t. t cr j. a point with world each camera s pose with respect to the rig is captured by j cc coordinates pw i and then through the rest of the i camera-specific chain as shown in figure is first transformed into rig coordinates pr figure bipartite graph for a toy structure from motion problem and its associated jacobian j and hessian a. numbers indicate points and letters indicate cameras. the dashed arcs and light blue squares indicate the fill-in that occurs when the structure variables are eliminated. piwpirrtrctrrjccjcyxfrx rjcxqjcftx rtrxqtrftx computer vision algorithms and applications draft if we order the structure variables before the motion variables in the hessian matrix a hence also the right hand side vector b we obtain a structure for the hessian shown in figure when such a system is solved using sparse cholesky factorization appendix orck golub and van loan the fill-in occurs in the smaller motion hessian acc and kang triggs mclauchlan hartley et al. hartley and zisserman lourakis and argyros engels stew enius and nist er some recent papers by od and astr om jeong nist er steedly et al. and snavely seitz et al. explore the use of iterative gradient techniques for the solution of bundle adjustment problems. in more detail the reduced motion hessian is computed using the schur complement acc at pca pp apc where app is the point hessian top left block of figure apc is the point-camera hessian top right block and acc and are the motion hessians before and after the point variable elimination bottom right block of figure notice that has a non-zero entry between two cameras if they see any point in common. this is indicated with dashed arcs in figure and light blue squares in figure whenever there are global parameters present in the reconstruction algorithm such as camera intrinsics that are common to all of the cameras or camera rig calibration parameters such as those shown in figure they should be ordered last along the right and bottom edges of a in order to reduce fill-in. engels stew enius and nist er provide a nice recipe for sparse bundle adjustment including all the steps needed to initialize the iterations as well as typical computation times for a system that uses a fixed number of backward-looking frames in a real-time setting. they also recommend using homogeneous coordinates for the structure parameters pi which is a good idea since it avoids numerical instabilities for points near infinity. bundle adjustment is now the standard method of choice for most structure-from-motion problems and is commonly applied to problems with hundreds of weakly calibrated images and tens of thousands of points e.g. in systems such as photosynth. larger problems are commonly solved in photogrammetry and aerial imagery but these are usually carefully calibrated and make use of surveyed ground control points. however as the problems become larger it becomes impractical to re-solve full bundle adjustment problems at each iteration. one approach to dealing with this problem is to use an incremental algorithm where new cameras are added over time. makes particular sense if the data is being acquired from this ordering is preferable when there are fewer cameras than points which is the usual case. the exception is when we are tracking a small number of points through many video frames in which case this ordering should be reversed. bundle adjustment a video camera or moving vehicle er naroditsky and bergen pollefeys nist er frahm et al. a kalman filter can be used to incrementally update estimates as new information is acquired. unfortunately such sequential updating is only statistically optimal for linear least squares problems. for non-linear problems such as structure from motion an extended kalman filter which linearizes measurement and update equations around the current estimate needs to be used vi eville and faugeras to overcome this limitation several passes can be made through the data and pentland because points disappear from view old cameras become irrelevant a variable state dimension filter can be used to adjust the set of state variables over time for example by keeping only cameras and point tracks seen in the last k frames a more flexible approach to using a fixed number of frames is to propagate corrections backwards through points and cameras until the changes on parameters are below a threshold and essa variants of these techniques including methods that use a fixed window for bundle adjustment stew enius and nist er or select keyframes for doing full bundle adjustment and murray are now commonly used in real-time tracking and augmented-reality applications as discussed in section when maximum accuracy is required it is still preferable to perform a full bundle adjustment over all the frames. in order to control the resulting computational complexity one approach is to lock together subsets of frames into locally rigid configurations and to optimize the relative positions of these cluster essa and dellaert a different approach is to select a smaller number of frames to form a skeletal set that still spans the whole dataset and produces reconstructions of comparable accuracy seitz and szeliski we describe this latter technique in more detail in section where we discuss applications of structure from motion to large image sets. while bundle adjustment and other robust non-linear least squares techniques are the methods of choice for most structure-from-motion problems they suffer from initialization problems i.e. they can get stuck in local energy minima if not started sufficiently close to the global optimum. many systems try to mitigate this by being conservative in what reconstruction they perform early on and which cameras and points they add to the solution an alternative however is to re-formulate the problem using a norm that supports the computation of global optima. kahl and hartley describe techniques for using l norms in geometric reconstruction problems. the advantage of such norms is that globally optimal solutions can be efficiently computed using second-order cone programming the disadvantage is that l norms are particularly sensitive to outliers and so must be combined with good outlier rejection techniques before they can be used. computer vision algorithms and applications draft application match move and augmented reality one of the neatest applications of structure from motion is to estimate the motion of a video or film camera along with the geometry of a scene in order to superimpose graphics or computer-generated images on the scene. in the visual effects industry this is known as the match move problem since the motion of the synthetic camera used to render the graphics must be matched to that of the real-world camera. for very small motions or motions involving pure camera rotations one or two tracked points can suffice to compute the necessary visual motion. for planar surfaces moving in four points are needed to compute the homography which can then be used to insert planar overlays e.g. to replace the contents of advertising billboards during sporting events. the general version of this problem requires the estimation of the full camera pose along with the focal length of the lens and potentially its radial distortion parameters when the structure of the scene is known ahead of time pose estimation techniques such as view correlation or through-the-lens camera control and witkin can be used as described in section for more complex scenes it is usually preferable to recover the structure simultaneously with the camera motion using structure-from-motion techniques. the trick with using such techniques is that in order to prevent any visible jitter between the synthetic graphics and the actual scene features must be tracked to very high accuracy and ample feature tracks must be available in the vicinity of the insertion location. some of today s best known match move software packages such as the boujou package from which won an emmy award in originated in structure-from-motion research in the computer vision community and zisserman closely related to the match move problem is robotics navigation where a robot must estimate its location relative to its environment while simultaneously avoiding any dangerous obstacles. this problem is often known as simultaneous localization and mapping burgard and fox or visual odometry and szeliski nist er naroditsky and bergen maimone cheng and matthies early versions of such algorithms used range-sensing techniques such as ultrasound laser range finders or stereo matching to estimate local geometry which could then be fused into a model. newer techniques can perform the same task based purely on visual feature tracking sometimes not even requiring a stereo camera rig reid molton et al. another closely related application is augmented reality where objects are inserted into a video feed in real time often to annotate or help users understand a scene baillot behringer et al. while traditional systems require prior knowledge about the scene or object being visually tracked and drummond newer systems can bundle adjustment figure augmented reality darth vader and a horde of ewoks battle it out on a table-top recovered using real-time keyframe-based structure from motion and murray ieee a virtual teapot is fixed to the top of a real-world coffee cup whose pose is re-recognized at each time frame and lowe springer. simultaneously build up a model of the environment and then track it so that graphics can be superimposed. klein and murray describe a parallel tracking and mapping system which simultaneously applies full bundle adjustment to keyframes selected from a video stream while performing robust real-time pose estimation on intermediate frames. figure shows an example of their system in use. once an initial scene has been reconstructed a dominant plane is estimated this case the table-top and animated characters are virtually inserted. klein and murray extend their previous system to handle even faster camera motion by adding edge features which can still be detected even when interest points become too blurred. they also use a direct rotation estimation algorithm for even faster motions. instead of modeling the whole scene as one rigid reference frame gordon and lowe first build a model of an individual object using feature matching and structure from motion. once the system has been initialized for every new frame they find the object and its pose using a instance recognition algorithm and then superimpose a graphical object onto that model as shown in figure while reliably tracking such objects and environments is now a well-solved problem determining which pixels should be occluded by foreground scene elements still remains an open problem agarwala curless et al. wang and cohen computer vision algorithms and applications draft uncertainty and ambiguities because structure from motion involves the estimation of so many highly coupled parameters often with no known ground truth components the estimates produced by structure from motion algorithms can often exhibit large amounts of uncertainty and kang an example of this is the classic bas-relief ambiguity which makes it hard to simultaneously estimate the depth of a scene and the amount of camera motion as mentioned before a unique coordinate frame and scale for a reconstructed scene cannot be recovered from monocular visual measurements alone. a stereo rig is used the scale can be recovered if we know the distance between the cameras. this seven-degree-of-freedom gauge ambiguity makes it tricky to compute the covariance matrix associated with a reconstruction mclauchlan hartley et al. kanatani and morris a simple way to compute a covariance matrix that ignores the gauge freedom is to throw away the seven smallest eigenvalues of the information matrix covariance whose values are equivalent to the problem hessian a up to noise scaling section and appendix after we do this the resulting matrix can be inverted to obtain an estimate of the parameter covariance. szeliski and kang use this approach to visualize the largest directions of variation in typical structure from motion problems. not surprisingly they find that the gauge freedoms the greatest uncertainties for problems such as observing an object from a small number of nearby viewpoints are in the depths of the structure relative to the extent of the camera it is also possible to estimate local or marginal uncertainties for individual parameters which corresponds simply to taking block sub-matrices from the full covariance matrix. under certain conditions such as when the camera poses are relatively certain compared to point locations such uncertainty estimates can be meaningful. however in many cases individual uncertainty measures can mask the extent to which reconstruction errors are correlated which is why looking at the first few modes of greatest joint variation can be helpful. the other way in which gauge ambiguities affect structure from motion and in particular bundle adjustment is that they make the system hessian matrix a rank-deficient and hence impossible to invert. a number of techniques have been proposed to mitigate this problem mclauchlan hartley et al. bartoli in practice however it appears that simply adding a small amount of the hessian diagonal diaga to the hessian a itself as is done in the levenberg marquardt non-linear least squares algorithm usually bas-relief refers to a kind of sculpture in which objects often on ornamental friezes are sculpted with less depth than they actually occupy. when lit from above by sunlight they appear to have true depth because of the ambiguity between relative depth and the angle of the illuminant a good way to minimize the amount of such ambiguities is to use wide field of view cameras and teller levin and szeliski bundle adjustment works well. application reconstruction from internet photos the most widely used application of structure from motion is in the reconstruction of objects and scenes from video sequences and collections of images and van gool the last decade has seen an explosion of techniques for performing this task automatically without the need for any manual correspondence or pre-surveyed ground control points. a lot of these techniques assume that the scene is taken with the same camera and hence the images all have the same intrinsics and zisserman koch pollefeys and van gool schaffalitzky and zisserman tuytelaars and van gool pollefeys nist er frahm et al. moons van gool and vergauwen many of these techniques take the results of the sparse feature matching and structure from motion computation and then compute dense surface models using multi-view stereo techniques pollefeys and van gool pollefeys and van gool pollefeys nist er frahm et al. moons van gool and vergauwen the latest innovation in this space has been the application of structure from motion and multi-view stereo techniques to thousands of images taken from the internet where very little is known about the cameras taking the photographs seitz and szeliski before the structure from motion computation can begin it is first necessary to establish sparse correspondences between different pairs of images and to then link such correspondences into feature tracks which associate individual image features with global points. because the on comparison of all pairs of images can be very slow a number of techniques have been developed in the recognition community to make this process faster er and stew enius philbin chum sivic et al. li wu zach et al. chum philbin and zisserman chum and matas to begin the reconstruction process it is important to to select a good pair of images where there are both a large number of consistent matches lower the likelihood of incorrect correspondences and a significant amount of out-of-plane to ensure that a stable reconstruction can be obtained seitz and szeliski the exif tags associated with the photographs can be used to get good initial estimates for camera focal lengths although this is not always strictly necessary since these parameters are re-adjusted as part of the bundle adjustment process. once an initial pair has been reconstructed the pose of cameras that see a sufficient number of the resulting points can be estimated and the complete set of cameras and feature correspondences can be used to perform another round of bundle adjustment. fig a simple way to compute this is to robustly fit a homography to the correspondences and measure reprojection errors. computer vision algorithms and applications draft figure incremental structure from motion seitz and szeliski acm starting with an initial two-frame reconstruction of trevi fountain batches of images are added using pose estimation and their positions with the model are refined using bundle adjustment. ure shows the progression of the incremental bundle adjustment algorithm where sets of cameras are added after each successive round of bundle adjustment while figure shows some additional results. an alternative to this kind of seed and grow approach is to first reconstruct triplets of images and then hierarchically merge triplets into larger collections as described by fitzgibbon and zisserman unfortunately as the incremental structure from motion algorithm continues to add more cameras and points it can become extremely slow. the direct solution of a dense system of on equations for the camera pose updates can take on time while structure from motion problems are rarely dense scenes such as city squares have a high percentage of cameras that see points in common. re-running the bundle adjustment algorithm after every few camera additions results in a quartic scaling of the run time with the number of images in the dataset. one approach to solving this problem is to select a smaller number of images for the original scene reconstruction and to fold in the remaining images at the very end. snavely seitz and szeliski develop an algorithm for computing such a skeletal set of images which is guaranteed to produce a reconstruction whose error is within a bounded factor of the optimal reconstruction accuracy. their algorithm first evaluates all pairwise uncertainties covariances between overlapping images and then chains them together to estimate a lower bound for the relative uncertainty of any distant pair. the skeletal set is constructed so that the maximal uncertainty between any pair grows by no more than a constant factor. figure shows an example of the skeletal set computed for images of the pantheon in rome. as you can see even though the skeletal set contains just a fraction of the original images the shapes of the skeletal set and full bundle adjusted reconstructions are virtually indistinguishable. the ability to automatically reconstruct models from large unstructured image collections has opened a wide variety of additional applications including the ability to automat bundle adjustment figure reconstructions produced by the incremental structure from motion algorithm developed by snavely seitz and szeliski acm cameras and point cloud from trafalgar square cameras and points overlaid on an image from the great wall of china overhead view of a reconstruction of the old town square in prague registered to an aerial photograph. figure large scale structure from motion using skeletal sets seitz and szeliski ieee original match graph for images skeletal set containing images top-down view of scene reconstructed from the skeletal set reconstruction after adding in the remaining images using pose estimation final bundle adjusted reconstruction which is almost identical. computer vision algorithms and applications draft ically find and label locations and regions of interest snavely and seitz simon and seitz gammeter bossard quack et al. and to cluster large image collections so that they can be automatically labeled wu zach et al. quack leibe and van gool some of these application are discussed in more detail in section constrained structure and motion the most general algorithms for structure from motion make no prior assumptions about the objects or scenes that they are reconstructing. in many cases however the scene contains higher-level geometric primitives such as lines and planes. these can provide information complementary to interest points and also serve as useful building blocks for modeling and visualization. furthermore these primitives are often arranged in particular relationships i.e. many lines and planes are either parallel or orthogonal to each other. this is particularly true of architectural scenes and models which we study in more detail in section sometimes instead of exploiting regularity in the scene structure it is possible to take advantage of a constrained motion model. for example if the object of interest is rotating on a turntable i.e. around a fixed but unknown axis specialized techniques can be used to recover this motion cross and zisserman in other situations the camera itself may be moving in a fixed arc around some center of rotation and he specialized capture setups such as mobile stereo camera rigs or moving vehicles equipped with multiple fixed cameras can also take advantage of the knowledge that individual cameras are fixed with respect to the capture rig as shown in figure line-based techniques it is well known that pairwise epipolar geometry cannot be recovered from line matches alone even if the cameras are calibrated. to see this think of projecting the set of lines in each image into a set of planes in space. you can move the two cameras around into any configuration you like and still obtain a valid reconstruction for lines. when lines are visible in three or more views the trifocal tensor can be used to transfer lines from one pair of images to another and zisserman the trifocal tensor can also be computed on the basis of line matches alone. schmid and zisserman describe a widely used technique for matching lines based on the average of pixel correlation scores evaluated at all pixels along their because of mechanical compliance and jitter it may be prudent to allow for a small amount of individual camera rotation around a nominal position. constrained structure and motion figure two images of a toy house along with their matched line segments and zisserman springer. common line segment in their system the epipolar geometry is assumed to be known e.g. computed from point matches. for wide baselines all possible homographies corresponding to planes passing through the line are used to warp pixels and the maximum correlation score is used. for triplets of images the trifocal tensor is used to verify that the lines are in geometric correspondence before evaluating the correlations between line segments. figure shows the results of using their system. bartoli and sturm describe a complete system for extending three view relations tensors computed from manual line correspondences to a full bundle adjustment of all the line and camera parameters. the key to their approach is to use the pl ucker coordinates to parameterize lines and to directly minimize reprojection errors. it is also possible to represent line segments by their endpoints and to measure either the reprojection error perpendicular to the detected line segments in each image or the errors using an elongated uncertainty ellipse aligned with the line segment direction and kang instead of reconstructing lines bay ferrari and van gool use ransac to group lines into likely coplanar subsets. four lines are chosen at random to compute a homography which is then verified for these and other plausible line segment matches by evaluating color histogram-based correlation scores. the intersection points of lines belonging to the same plane are then used as virtual measurements to estimate the epipolar geometry which is more accurate than using the homographies directly. an alternative to grouping lines into coplanar subsets is to group lines by parallelism. whenever three or more lines share a common vanishing point there is a good likelihood that they are parallel in by finding multiple vanishing points in an image and establishing correspondences between such vanishing points in different images the relative rotations between the various images often the camera intrinsics can be directly estimated because lines often occur at depth or orientation discontinuities it may be preferable to compute correlation scores to match color histograms ferrari and van gool separately on each side of the line. computer vision algorithms and applications draft shum han and szeliski describe a modeling system which first constructs calibrated panoramas from multiple images and then has the user draw vertical and horizontal lines in the image to demarcate the boundaries of planar regions. the lines are initially used to establish an absolute rotation for each panorama and are later used with the inferred vertices and planes to infer a structure which can be recovered up to scale from one or more images a fully automated approach to line-based structure from motion is presented vy werner and zisserman in their system they first find lines and group them by common vanishing points in each image the vanishing points are then used to calibrate the camera i.e. to performa a metric upgrade lines corresponding to common vanishing points are then matched using both appearance and zisserman and trifocal tensors. the resulting set of lines color coded by common vanishing directions orientations is shown in figure these lines are then used to infer planes and a block-structured model for the scene as described in more detail in section plane-based techniques in scenes that are rich in planar structures e.g. in architecture and certain kinds of manufactured objects such as furniture it is possible to directly estimate homographies between different planes using either feature-based or intensity-based methods. in principle this information can be used to simultaneously infer the camera poses and the plane equations i.e. to compute plane-based structure from motion. luong and faugeras show how a fundamental matrix can be directly computed from two or more homographies using algebraic manipulations and least squares. unfortunately this approach often performs poorly since the algebraic errors do not correspond to meaningful reprojection errors and torr a better approach is to hallucinate virtual point correspondences within the areas from which each homography was computed and to feed them into a standard structure from motion algorithm and torr an even better approach is to use full bundle adjustment with explicit plane equations as well as additional constraints to force reconstructed co-planar features to lie exactly on their corresponding planes. principled way to do this is to establish a coordinate frame for each plane e.g. at one of the feature points and to use in-plane parameterizations for the other points. the system developed by shum han and szeliski shows an example of such an approach where the directions of lines and normals for planes in the scene are pre-specified by the user. additional reading additional reading the topic of structure from motion is extensively covered in books and review articles on multi-view geometry and luong hartley and zisserman moons van gool and vergauwen for two-frame reconstruction hartley wrote a highly cited paper on the eight-point algorithm for computing an essential or fundamental matrix with reasonable point normalization. when the cameras are calibrated the five-point algorithm of nist er can be used in conjunction with ransac to obtain initial reconstructions from the minimum number of points. when the cameras are uncalibrated various self-calibration techniques can be found in work by hartley and zisserman moons van gool and vergauwen i only briefly mention one of the simplest techniques the kruppa equations in applications where points are being tracked from frame to frame factorization techniques based on either orthographic camera models and kanade poelman and kanade costeira and kanade morita and kanade morris and kanade anandan and irani or projective extensions and horaud sturm and triggs triggs oliensis and hartley can be used. triggs mclauchlan hartley et al. provide a good tutorial and survey on bundle adjustment while lourakis and argyros and engels stew enius and nist er provide tips on implementation and effective practices. bundle adjustment is also covered in textbooks and surveys on multi-view geometry and luong hartley and zisserman moons van gool and vergauwen techniques for handling larger problems are described by snavely seitz and szeliski agarwal snavely simon et al. jeong nist er steedly et al. agarwal snavely seitz et al. while bundle adjustment is often called as an inner loop inside incremental reconstruction algorithms seitz and szeliski hierarchical and zisserman farenzena fusiello and gherardi and global and carlsson martinec and pajdla approaches for initialization are also possible and perhaps even preferable. as structure from motion starts being applied to dynamic scenes the topic of non-rigid structure from motion hertzmann and bregler which we do not cover in this book will become more important. exercises ex triangulation use the calibration pattern you built and tested in exercise to test your triangulation accuracy. as an alternative generate synthetic points and cameras and add noise to the point measurements. computer vision algorithms and applications draft assume that you know the camera pose i.e. the camera matrices. use the distance to rays or linearized versions of equations to compute an initial set of locations. compare these to your known ground truth locations. use iterative non-linear minimization to improve your initial estimates and report on the improvement in accuracy. use the technique described by hartley and sturm to perform two frame triangulation. see if any of the failure modes reported by hartley and sturm or hartley occur in practice. ex essential and fundamental matrix implement the two-frame e and f matrix estimation techniques presented in section with suitable re-scaling for better noise immunity. use the data from exercise to validate your algorithms and to report on their accu racy. implement one of the improved f or e estimation algorithms e.g. using renormalization torr and fitzgibbon hartley and zisserman ransac and murray least media squares or the five-point algorithm developed by nist er ex view morphing and interpolation implement automatic view morphing i.e. compute two-frame structure from motion and then use these results to generate a smooth animation from one image to the next decide how to represent your scene e.g. compute a delaunay triangulation of the matched point and decide what to do with the triangles near the border. try fitting a plane to the scene e.g. behind most of the points. compute your in-between camera positions and orientations. warp each triangle to its new location preferably using the correct perspective projec tion and shum if you have a denser model from stereo decide what to do at the cracks for a non-rigid scene e.g. two pictures of a face with different expressions not all of your matched points will obey the epipolar geometry. decide how to handle them to achieve the best effect. exercises ex factorization implement the factorization algorithm described in section using point tracks you computed in exercise implement uncertainty rescaling and irani and comment on whether this improves your results. implement one of the perspective improvements to factorization discussed in section and horaud sturm and triggs triggs does this produce significantly lower reprojection errors? can you upgrade this reconstruction to a metric one? ex bundle adjuster it really is not. implement a full bundle adjuster. this may sound daunting but devise the internal data structures and external file representations to hold your camera parameters orientation and focal length point locations or homogeneous and point tracks and point identifier as well as locations. use some other technique such as factorization to initialize the point and camera locations from your tracks a subset of points that appears in all frames. implement the code corresponding to the forward transformations in figure i.e. for each point measurement take the corresponding point map it through the camera transformations perspective projection and focal length scaling and compare it to the point measurement to get a residual error. take the residual error and compute its derivatives with respect to all the unknown motion and structure parameters using backward chaining as shown e.g. in figure and equation this gives you the sparse jacobian j used in equations and equation use a sparse least squares or linear system solver e.g. matlab sparsesuite or sparskit appendix and to solve the corresponding linearized system adding a small amount of diagonal preconditioning as in levenberg marquardt. update your parameters make sure your rotation matrices are still orthonormal by re-computing them from your quaternions and continue iterating while monitoring your residual error. use the schur complement trick to reduce the size of the system being solved mclauchlan hartley et al. hartley and zisserman lourakis and argyros engels stew enius and nist er computer vision algorithms and applications draft implement your own iterative sparse solver e.g. conjugate gradient and compare its performance to a direct method. make your bundle adjuster robust to outliers or try adding some of the other improvements discussed in stew enius and nist er can you think of any other ways to make your algorithm even faster or more robust? ex match move and augmented reality use the results of the previous exercise to superimpose a rendered model on top of video. see section for more details and ideas. check for how locked down the objects are. ex line-based reconstruction augment the previously developed bundle adjuster to include lines possibly with known orientations. optionally use co-planar sets of points and lines to hypothesize planes and to enforce co-planarity and zisserman robertson and cipolla ex flexible bundle adjuster design a bundle adjuster that allows for arbitrary chains of transformations and prior knowledge about the unknowns as suggested in figures ex unordered image matching compute the camera pose and structure of a scene from an arbitrary collection of photographs and lowe snavely seitz and szeliski chapter dense motion estimation translational alignment parametric motion spline-based motion optical flow fourier-based alignment incremental refinement hierarchical motion estimation application video stabilization learned motion models application medical image registration application frame interpolation transparent layers and reflections multi-frame motion estimation application video denoising application de-interlacing layered motion additional reading exercises computer vision algorithms and applications draft flow initial layers final layers layers with pixel assignments and flow figure motion estimation b regularization-based optical flow and enkelmann ieee d layered motion estimation and adelson ieee f sample image and ground truth flow from evaluation database black lewis et al. ieee. dense motion estimation algorithms for aligning images and estimating motion in video sequences are among the most widely used in computer vision. for example frame-rate image alignment is widely used in camcorders and digital cameras to implement their image stabilization feature. an early example of a widely used image registration algorithm is the patch-based translational alignment flow technique developed by lucas and kanade variants of this algorithm are used in almost all motion-compensated video compression schemes such as mpeg and gall similar parametric motion estimation algorithms have found a wide variety of applications including video summarization and bender irani and anandan video stabilization anandan dana et al. srinivasan chellappa veeraraghavan et al. matsushita ofek ge et al. and video compression hsu and anandan lee ge chen lung bruce lin et al. more sophisticated image registration algorithms have also been developed for medical imaging and remote sensing. image registration techniques are surveyed by brown zitov aa and flusser goshtasby and szeliski to estimate the motion between two or more images a suitable error metric must first be chosen to compare the images once this has been established a suitable search technique must be devised. the simplest technique is to exhaustively try all possible alignments i.e. to do a full search. in practice this may be too slow so hierarchical coarseto-fine techniques based on image pyramids are normally used. alternatively fourier transforms can be used to speed up the computation. to get sub-pixel precision in the alignment incremental methods based on a taylor series expansion of the image function are often used. these can also be applied to parametric motion models which model global image transformations such as rotation or shearing. motion estimation can be made more reliable by learning the typical dynamics or motion statistics of the scenes or objects being tracked e.g. the natural gait of walking people for more complex motions piecewise parametric spline motion models can be used. in the presence of multiple independent perhaps non-rigid motions general-purpose optical flow optic flow techniques need to be used for even more complex motions that include a lot of occlusions layered motion models which decompose the scene into coherently moving layers can work well. in this chapter we describe each of these techniques in more detail. additional details can be found in review and comparative evaluation papers on motion estimation fleet and beauchemin mitiche and bouthemy stiller and konrad szeliski baker black lewis et al. computer vision algorithms and applications draft translational alignment the simplest way to establish an alignment between two images or image patches is to shift one image relative to the other. given a template image sampled at discrete pixel locations yi we wish to find where it is located in image a least squares solution to this problem is to find the minimum of the sum of squared differences function essdu u i where u v is the displacement and ei u is called the residual error the displaced frame difference in the video coding ignore for the moment the possibility that parts of may lie outside the boundaries of or be otherwise not visible. the assumption that corresponding pixel values remain the same in the two images is often called the brightness constancy in general the displacement u can be fractional so a suitable interpolation function must be applied to image in practice a bilinear interpolant is often used but bicubic interpolation can yield slightly better results and scharstein color images can be processed by summing differences across all three color channels although it is also possible to first transform the images into a different color space or to only use the luminance is often done in video encoders. robust error metrics. we can make the above error metric more robust to outliers by replacing the squared error terms with a robust function hampel ronchetti rousseeuw et al. black and anandan stewart to obtain esrdu u the robust norm is a function that grows less quickly than the quadratic penalty associated with least squares. one such function sometimes used in motion estimation for video coding because of its speed is the sum of absolute differences or norm i.e. esadu u the usual justification for using least squares is that it is the optimal estimate with respect to gaussian noise. see the discussion below on robust error metrics as well as appendix brightness constancy is the tendency for objects to maintain their perceived brightness under varying illumination conditions. in video compression e.g. the standard the sum of absolute transformed differences which measures the differences in a frequency transform space e.g. using a hadamard transform is often used since it more accurately predicts quality translational alignment however since this function is not differentiable at the origin it is not well suited to gradientdescent approaches such as the ones presented in section instead a smoothly varying function that is quadratic for small values but grows more slowly away from the origin is often used. black and rangarajan discuss a variety of such functions including the geman mcclure function gmx where a is a constant that can be thought of as an outlier threshold. an appropriate value for the threshold can itself be derived using robust statistics hampel ronchetti rousseeuw et al. rousseeuw and leroy e.g. by computing the median absolute deviation m ad mediei and multiplying it by to obtain a robust estimate of the standard deviation of the inlier noise process spatially varying weights. the error metrics above ignore that fact that for a given alignment some of the pixels being compared may lie outside the original image boundaries. furthermore we may want to partially or completely downweight the contributions of certain pixels. for example we may want to selectively erase some parts of an image from consideration when stitching a mosaic where unwanted foreground objects have been cut out. for applications such as background stabilization we may want to downweight the middle part of the image which often contains independently moving objects being tracked by the camera. all of these tasks can be accomplished by associating a spatially varying per-pixel weight value with each of the two images being matched. the error metric then becomes the weighted windowed ssd function ewssdu u where the weighting functions and are zero outside the image boundaries. if a large range of potential motions is allowed the above metric can have a bias towards smaller overlap solutions. to counteract this bias the windowed ssd score can be divided by the overlap area u to compute a per-pixel mean squared pixel error ewssda. the square root of this quantity is the root mean square intensity error often reported in comparative studies. rm s a computer vision algorithms and applications draft bias and gain differences. often the two images being aligned were not taken with the same exposure. a simple model of linear intensity variation between the two images is the bias and gain model u where is the bias and is the gain and kanade gennert fuh and maragos baker gross and matthews evangelidis and psarakis the least squares formulation then becomes ebgu u rather than taking a simple squared difference between corresponding patches it becomes necessary to perform a linear regression which is somewhat more costly. note that for color images it may be necessary to estimate a different bias and gain for each color channel to compensate for the automatic color correction performed by some digital cameras bias and gain compensation is also used in video codecs where it is known as weighted prediction a more general varying non-parametric model of intensity variation which is computed as part of the registration process is used in jia and tang seitz and baker this can be useful for dealing with local variations such as the vignetting caused by wide-angle lenses wide apertures or lens housings. it is also possible to pre-process the images before comparing their values e.g. using band-pass filtered images bergen anandan hanna et al. gradients papenberg bruhn brox et al. or using other local transformations such as histograms or rank transforms roy and hingorani zabih and woodfill or to maximize mutual information and wells iii kim kolmogorov and zabih hirschm uller and scharstein compare a number of these approaches and report on their relative performance in scenes with exposure differences. correlation. an alternative to taking intensity differences is to perform correlation i.e. to maximize the product cross-correlation of the two aligned images eccu u. at first glance this may appear to make bias and gain modeling unnecessary since the images will prefer to line up regardless of their relative scales and offsets. however this is actually not true. if a very bright patch exists in the maximum product may actually lie in that area. translational alignment for this reason normalized cross-correlation is more commonly used enccu u u where and u n n are the mean images of the corresponding patches and n is the number of pixels in the patch. the normalized cross-correlation score is always guaranteed to be in the range which makes it easier to handle in some higher-level applications such as deciding which patches truly match. normalized correlation works well when matching images taken with different exposures e.g. when creating high dynamic range images note however that the ncc score is undefined if either of the two patches has zero variance in fact its performance degrades for noisy low-contrast regions. a variant on ncc which is related to the bias gain regression implicit in the matching score is the normalized ssd score enssdu u u recently proposed by criminisi shotton blake et al. in their experiments they find that it produces comparable results to ncc but is more efficient when applied to a large number of overlapping patches using a moving average technique hierarchical motion estimation now that we have a well-defined alignment cost function to optimize how can we find its minimum? the simplest solution is to do a full search over some range of shifts using either integer or sub-pixel steps. this is often the approach used for block matching in motion compensated video compression where a range of possible motions pixels is to accelerate this search process hierarchical motion estimation is often used an image pyramid is constructed and a search over a smaller number of discrete pixels in stereo matching an explicit search over all possible disparities a plane sweep is almost always performed since the number of search hypotheses is much smaller due to the nature of the potential displacements. computer vision algorithms and applications draft to the same range of motion is first performed at coarser levels anandan bergen anandan hanna et al. the motion estimate from one level of the pyramid is then used to initialize a smaller local search at the next finer level. alternatively several seeds solutions from the coarse level can be used to initialize the fine-level search. while this is not guaranteed to produce the same result as a full search it usually works almost as well and is much faster. more formally let k i i k be the decimated image at level l obtained by subsampling a smoothed version of the image at level l see section for how to perform the required downsampling construction without introducing too much aliasing. at the coarsest level we search for the best displacement ul that minimizes the difference between images i this is usually done using a full search over some range of displacements ul l s where s is the desired search range at the finest resolution level optionally followed by the incremental refinement step described in section and i once a suitable motion vector has been estimated it is used to predict a likely displace ment ul for the next finer the search over displacements is then repeated at the finer level over a much narrower range of displacements say ul again optionally combined with an incremental refinement step alternatively one of the images can be warped by the current motion estimate in which case only small incremental motions need to be computed at the finer level. a nice description of the whole process extended to parametric motion estimation is provided by bergen anandan hanna et al. fourier-based alignment when the search range corresponds to a significant fraction of the larger image is the case in image stitching see chapter the hierarchical approach may not work that well since it is often not possible to coarsen the representation too much before significant features are blurred away. in this case a fourier-based approach may be preferable. this doubling of displacements is only necessary if displacements are defined in integer pixel coordinates which is the usual case in the literature anandan hanna et al. if normalized device coordinates are used instead the displacements search ranges need not change from level to level although the step sizes will need to be adjusted to keep search steps of roughly one pixel. translational alignment fourier-based alignment relies on the fact that the fourier transform of a shifted signal has the same magnitude as the original signal but a linearly varying phase i.e. f u f e ju ju where is the vector-valued angular frequency of the fourier transform and we use calligraphic notation f to denote the fourier transform of a signal another useful property of fourier transforms is that convolution in the spatial domain corresponds to multiplication in the fourier domain thus the fourier transform of the cross-correlation function ecc can be written as f where f fu gu fxigxi u is the correlation function i.e. the convolution of one signal with the reverse of the other and i is the complex conjugate of this is because convolution is defined as the summation of one signal with the reverse of the other thus to efficiently evaluate ecc over the range of all possible values of u we take the fourier transforms of both images and multiply both transforms together conjugating the second one and take the inverse transform of the result. the fast fourier transform algorithm can compute the transform of an n m image in on m log n m operations this can be significantly faster than the on operations required to do a full search when the full range of image overlaps is considered. while fourier-based convolution is often used to accelerate the computation of image correlations it can also be used to accelerate the sum of squared differences function its variants. consider the ssd formula given in its fourier transform can be written as f u i thus the ssd function can be computed by taking twice the correlation function and subtracting it from the sum of the energies in the two images. in fact the fourier shift property derives from the convolution theorem by observing that shifting is equivalent to convolution with a displaced delta function u. computer vision algorithms and applications draft windowed correlation. unfortunately the fourier convolution theorem only applies when the summation over xi is performed over all the pixels in both images using a circular shift of the image when accessing pixels outside the original boundaries. while this is acceptable for small shifts and comparably sized images it makes no sense when the images overlap by a small amount or one image is a small subset of the other. in that case the cross-correlation function should be replaced with a windowed cross-correlation function ewccu u where the weighting functions and are zero outside the valid ranges of the images and both images are padded so that circular shifts return values outside the original image boundaries. an even more interesting case is the computation of the weighted ssd function intro duced in equation ewssdu u expanding this as a sum of correlations and deriving the appropriate set of fourier transforms is left for exercise the same kind of derivation can also be applied to the bias gain corrected sum of squared difference function ebg again fourier transforms can be used to efficiently compute all the correlations needed to perform the linear regression in the bias and gain parameters in order to estimate the exposure-compensated difference for each potential shift phase correlation. a variant of regular correlation that is sometimes used for motion estimation is phase correlation and hines brown here the spectrum of the two signals being matched is whitened by dividing each per-frequency product in by the magnitudes of the fourier transforms f before taking the final inverse fourier transform. in the case of noiseless signals with perfect shift we have u and hence from equation we obtain f u ju and f e ju translational alignment the output of phase correlation ideal conditions is therefore a single spike located at the correct value of u which principle makes it easier to find the correct estimate. phase correlation has a reputation in some quarters of outperforming regular correlation but this behavior depends on the characteristics of the signals and noise. if the original images are contaminated by noise in a narrow frequency band low-frequency noise or peaked frequency hum the whitening process effectively de-emphasizes the noise in these regions. however if the original signals have very low signal-to-noise ratio at some frequencies two blurry or low-textured images with lots of high-frequency noise the whitening process can actually decrease performance exercise recently gradient cross-correlation has emerged as a promising alternative to phase correlation and vlachos although further systematic studies are probably warranted. phase correlation has also been studied by fleet and jepson as a method for estimating general optical flow and stereo disparity. rotations and scale. while fourier-based alignment is mostly used to estimate translational shifts between images it can under certain limited conditions also be used to estimate in-plane rotations and scales. consider two images that are related purely by rotation i.e. rx if we re-sample the images into polar coordinates cos r sin and cos r sin we obtain the desired rotation can then be estimated using a fast fourier transform shift-based technique. if the two images are also related by a scale s rx we can re-sample into log-polar coordinates cos es sin and cos es sin to obtain s computer vision algorithms and applications draft figure taylor series approximation of a function and the incremental computation of the optical flow correction amount. j u is the image gradient at u and ei is the current intensity difference. in this case care must be taken to choose a suitable range of s values that reasonably samples the original image. for images that are also translated by a small amount s rx t de castro and morandi propose an ingenious solution that uses several steps to estimate the unknown parameters. first both images are converted to the fourier domain and only the magnitudes of the transformed images are retained. in principle the fourier magnitude images are insensitive to translations in the image plane the usual caveats about border effects apply. next the two magnitude images are aligned in rotation and scale using the polar or log-polar representations. once rotation and scale are estimated one of the images can be de-rotated and scaled and a regular translational algorithm can be applied to estimate the translational shift. unfortunately this trick only applies when the images have large overlap translational motion. for more general motion of patches or images the parametric motion estimator described in section or the feature-based approaches described in section need to be used. incremental refinement the techniques described up till now can estimate alignment to the nearest pixel potentially fractional pixel if smaller search steps are used. in general image stabilization and stitching applications require much higher accuracies to obtain acceptable results. to obtain better sub-pixel estimates we can use one of several techniques described by tian and huhns one possibility is to evaluate several discrete or fractional values of v around the best value found so far and to interpolate the matching score to find an analytic minimum. ixei translational alignment a more commonly used approach first proposed by lucas and kanade is to perform gradient descent on the ssd energy function using a taylor series expansion of the image function elk ssdu u u u u j u u u u where j u u x y u is the image gradient or jacobian at u and ei u first introduced in is the current intensity the gradient at a particular sub-pixel location u can be computed using a variety of techniques the simplest of which is to simply take the horizontal and vertical differences between pixels x and x or x more sophisticated derivatives can sometimes lead to noticeable performance improvements. the linearized form of the incremental update to the ssd error is often called the optical flow constraint or brightness constancy constraint equation ixu iyv it where the subscripts in ix and iy denote spatial derivatives and it is called the temporal derivative which makes sense if we are computing instantaneous velocity in a video sequence. when squared and summed or integrated over a region it can be used to compute optic flow and schunck the above least squares problem can be minimized by solving the associated nor mal equations where a u b a j t uj u we follow the convention commonly used in robotics and by baker and matthews that derivatives with respect to vectors result in row vectors so that fewer transposes are needed in the formulas. computer vision algorithms and applications draft b eij t u and are called the newton approximation of the hessian and gradient-weighted residual vector these matrices are also often written as a i x ixiy ixiy i y and b iyit the gradients required for j u can be evaluated at the same time as the image warps required to estimate u and in fact are often computed as a side-product of image interpolation. if efficiency is a concern these gradients can be replaced by the gradients in the template image j u j since near the correct alignment the template and displaced target images should look similar. this has the advantage of allowing the pre-computation of the hessian and jacobian images which can result in significant computational savings and belhumeur baker and matthews a further reduction in computation can be obtained by writing the warped image u used to compute ei in as a convolution of a sub-pixel interpolation filter with the discrete samples in and rav-acha precomputing the inner product between the gradient field and shifted version of allows the iterative re-computation of ei to be performed in constant time of the number of pixels. the effectiveness of the above incremental update rule relies on the quality of the taylor series approximation. when far away from the true displacement pixels several iterations may be needed. it is possible however to estimate a value for j using a least squares fit to a series of larger displacements in order to increase the range of convergence and dhome or to learn a special-purpose recognizer for a given patch williams blake and cipolla lepetit pilet and fua hinterstoisser benhimane navab et al. ozuysal calonder lepetit et al. as discussed in section a commonly used stopping criterion for incremental updating is to monitor the magnitude of the displacement correction and to stop when it drops below a certain threshold of a pixel. for larger motions it is usual to combine the incremental update rule with a hierarchical coarse-to-fine search strategy as described in section the true hessian is the full second derivative of the error function e which may not be positive definite see section and appendix translational alignment figure aperture problems for different image regions denoted by the orange and red l-shaped structures overlaid in the same image to make it easier to diagram the flow. a window wxi centered at xi circle can uniquely be matched to its corresponding structure at xi u in the second image. a window centered on the edge exhibits the classic aperture problem since it can be matched to a family of possible locations. in a completely textureless region the matches become totally unconstrained. conditioning and aperture problems. sometimes the inversion of the linear system can be poorly conditioned because of lack of two-dimensional texture in the patch being aligned. a commonly occurring example of this is the aperture problem first identified in some of the early papers on optical flow and schunck and then studied more extensively by anandan consider an image patch that consists of a slanted edge moving to the right only the normal component of the velocity can be reliably recovered in this case. this manifests itself in as a rank-deficient matrix a i.e. one whose smaller eigenvalue is very close to when equation is solved the component of the displacement along the edge is very poorly conditioned and can result in wild guesses under small noise perturbations. one way to mitigate this problem is to add a prior constraint on the expected range of motions adelson and heeger baker gross and matthews govindu this can be accomplished by adding a small value to the diagonal of a which essentially biases the solution towards smaller u values that still minimize the squared error. however the pure gaussian model assumed when using a simple quadratic prior as in adelson and heeger does not always hold in practice e.g. because of aliasing along strong edges for this reason it may be prudent to add some small fraction of the larger eigenvalue to the smaller one before doing the matrix inversion. matrix a is by construction always guaranteed to be symmetric positive semi-definite i.e. it has real non-negative eigenvalues. xxixiuui computer vision algorithms and applications draft figure ssd surfaces corresponding to three locations crosses in an image highly textured area strong minimum low uncertainty strong edge aperture problem high uncertainty in one direction weak texture no clear minimum large uncertainty. translational alignment uncertainty modeling. the reliability of a particular patch-based motion estimate can be captured more formally with an uncertainty model. the simplest such model is a covariance matrix which captures the expected variance in the motion estimate in all possible directions. as discussed in section and appendix under small amounts of additive gaussian noise it can be shown that the covariance matrix u is proportional to the inverse of the hessian a u na n is the variance of the additive gaussian noise matthies kanade where and szeliski szeliski for larger amounts of noise the linearization performed by the lucas kanade algorithm in is only approximate so the above quantity becomes a cramer rao lower bound on the true covariance. thus the minimum and maximum eigenvalues of the hessian a can now be interpreted as the inverse variances in the least-certain and most-certain directions of motion. more detailed analysis using a more realistic model of image noise is given by steele and jaynes figure shows the local ssd surfaces for three different pixel locations in an image. as you can see the surface has a clear minimum in the highly textured region and suffers from the aperture problem near the strong edge. bias and gain weighting and robust error metrics. the lucas kanade update rule can also be applied to the bias gain equation to obtain elk bgu u u u ei and kanade gennert fuh and maragos baker gross and matthews the resulting system of equations can be solved to simultaneously estimate the translational displacement update u and the bias and gain parameters and a similar formulation can be derived for images that have a linear appearance variation u jbjx where the bjx are the basis images and the j are the unknown coefficients and belhumeur baker gross ishikawa et al. baker gross and matthews potential linear appearance variations include illumination changes and belhumeur and small non-rigid deformations and jepson a weighted version of the lucas kanade algorithm is also possible elk wssdu u uj u u computer vision algorithms and applications draft note that here in deriving the lucas kanade update from the original weighted ssd function we have neglected taking the derivative of the u weighting function with respect to u which is usually acceptable in practice especially if the weighting function is a binary mask with relatively few transitions. baker gross ishikawa et al. only use the term which is reasonable if the two images have the same extent and no cutouts in the overlap region. they also discuss the idea of making the weighting proportional to ix which helps for very noisy images where the gradient itself is noisy. similar observations formulated in terms of total least squares huffel and vandewalle van huffel and lemmerling have been made by other researchers studying optical flow and malik babhadiashar and suter m uhlich and mester lastly baker gross ishikawa et al. show how evaluating equation at just the most reliable gradient pixels does not significantly reduce performance for large enough images even if only of the pixels are used. idea was originally proposed by dellaert and collins who used a more sophisticated selection criterion. the lucas kanade incremental refinement step can also be applied to the robust error metric introduced in section elk srdu u u u ei which can be solved using the iteratively reweighted least squares technique described in section parametric motion many image alignment tasks for example image stitching with handheld cameras require the use of more sophisticated motion models as described in section since these models e.g. affine deformations typically have more parameters than pure translation a full search over the possible range of values is impractical. instead the incremental lucas kanade algorithm can be generalized to parametric motion models and used in conjunction with a hierarchical search algorithm and kanade rehg and witkin fuh and maragos bergen anandan hanna et al. shashua and toelg shashua and wexler baker and matthews for parametric motion instead of using a single constant translation vector u we use a spatially varying motion field or correspondence map p parameterized by a lowdimensional vector p where can be any of the motion models presented in section parametric motion the parametric incremental motion update rule now becomes elk pmp p p p j p p where the jacobian is now j p p i.e. the product of the image gradient with the jacobian of the correspondence field j p. the motion jacobians j for the planar transformations introduced in section and table are given in table note how we have re-parameterized the motion matrices so that they are always the identity at the origin p this becomes useful later when we talk about the compositional and inverse compositional algorithms. also makes it easier to impose priors on the motions. for parametric motion the newton hessian and gradient-weighted residual vec tor become and a i t j t b i t j t note how the expressions inside the square brackets are the same ones evaluated for the simpler translational motion case patch-based approximation. the computation of the hessian and residual vectors for parametric motion can be significantly more expensive than for the translational case. for parametric motion with n parameters and n pixels the accumulation of a and b takes operations and matthews one way to reduce this by a significant amount is to divide the image up into smaller sub-blocks pj and to only accumulate the simpler quantities inside the square brackets at the pixel level and szeliski aj pj bj pj i t ei i t computer vision algorithms and applications draft the full hessian and residual can then be approximated as a and j t pj xj i t pj b ei i t j t j t xjajj xj j t xjbj where xj is the center of each patch pj and szeliski this is equivalent to replacing the true motion jacobian with a piecewise-constant approximation. in practice this works quite well. the relationship of this approximation to feature-based registration is discussed in section compositional approach. for a complex parametric motion such as a homography the computation of the motion jacobian becomes complicated and may involve a per-pixel division. szeliski and shum observed that this can be simplified by first warping the target image according to the current motion estimate p p and then comparing this warped image against the template elk ss p xxi p p xxi p note that since the two images are assumed to be fairly similar only an incremental parametric motion is required i.e. the incremental motion can be evaluated around p which can lead to considerable simplifications. for example the jacobian of the planar projective transform now becomes j x x x y x y xy xy once the incremental motion x has been computed it can be prepended to the previously estimated motion which is easy to do for motions represented with transformation matrices such as those given in tables and baker and matthews call this the forward compositional algorithm since the target image is being re-warped and the final motion estimates are being composed. parametric motion if the appearance of the warped and template images is similar enough we can replace the gradient of with the gradient of as suggested previously this has potentially a big advantage in that it allows the pre-computation inversion of the hessian matrix a given in equation the residual vector b can also be partially precomputed i.e. the steepest descent images xx can precomputed and stored for later multiplication with the ex error images and matthews this idea was first suggested by hager and belhumeur in what baker and matthews call a inverse additive scheme. baker and matthews introduce one more variant they call the inverse compositional algorithm. rather than re-warping the warped target image they instead warp the template image and minimize elk bm p xxi xxi p this is identical to the forward warped algorithm with the gradients replaced by the gradients except for the sign of ei. the resulting update p is the negative of the one computed by the modified equation and hence the inverse of the incremental transformation must be prepended to the current transform. because the inverse compositional algorithm has the potential of pre-computing the inverse hessian and the steepest descent images this makes it the preferred approach of those surveyed by baker and matthews figure gross ishikawa et al. beautifully shows all of the steps required to implement the inverse compositional algorithm. baker and matthews also discuss the advantage of using gauss newton iteration the first-order expansion of the least squares as above compared to other approaches such as steepest descent and levenberg marquardt. subsequent parts of the series gross ishikawa et al. baker gross and matthews discuss more advanced topics such as per-pixel weighting pixel selection for efficiency a more in-depth discussion of robust metrics and algorithms linear appearance variations and priors on parameters. they make for invaluable reading for anyone interested in implementing a highly tuned implementation of incremental image registration. evangelidis and psarakis provide some detailed experimental evaluations of these and other related approaches. application video stabilization video stabilization is one of the most widely used applications of parametric motion estimation anandan dana et al. irani rousso and peleg morimoto and computer vision algorithms and applications draft figure a schematic overview of the inverse compositional algorithm with permission from gross ishikawa et al. steps arrows are performed once as a pre-computation. the main algorithm simply consists of iterating image warping image differencing image dot products multiplication with the inverse of the hessian and the update to the warp all of these steps can be performed efficiently. parametric motion chellappa srinivasan chellappa veeraraghavan et al. algorithms for stabilization run inside both hardware devices such as camcorders and still cameras and software packages for improving the visual quality of shaky videos. in their paper on full-frame video stabilization matsushita ofek ge et al. give a nice overview of the three major stages of stabilization namely motion estimation motion smoothing and image warping. motion estimation algorithms often use a similarity transform to handle camera translations rotations and zooming. the tricky part is getting these algorithms to lock onto the background motion which is a result of the camera movement without getting distracted by independent moving foreground objects. motion smoothing algorithms recover the low-frequency varying part of the motion and then estimate the high-frequency shake component that needs to be removed. finally image warping algorithms apply the high-frequency correction to render the original frames as if the camera had undergone only the smooth motion. the resulting stabilization algorithms can greatly improve the appearance of shaky videos but they often still contain visual artifacts. for example image warping can result in missing borders around the image which must be cropped filled using information from other frames or hallucinated using inpainting techniques furthermore video frames captured during fast motion are often blurry. their appearance can be improved either using deblurring techniques or stealing sharper pixels from other frames with less motion or better focus ofek ge et al. exercise has you implement and test some of these ideas. in situations where the camera is translating a lot in e.g. when the videographer is walking an even better approach is to compute a full structure from motion reconstruction of the camera motion and scene. a smooth camera path can then be computed and the original video re-rendered using view interpolation with the interpolated point cloud serving as the proxy geometry while preserving salient features gleicher jin et al. if you have access to a camera array instead of a single video camera you can do even better using a light field rendering approach zhang jin et al. learned motion models an alternative to parameterizing the motion field with a geometric deformation such as an affine transform is to learn a set of basis functions tailored to a particular application yacoob jepson et al. first a set of dense motion fields is computed from a set of training videos. next singular value decomposition is applied to the stack of motion fields utx to compute the first few singular vectors vkx. finally for a new test sequence a novel flow field is computed using a coarse-to-fine algorithm that estimates the computer vision algorithms and applications draft figure learned parameterized motion fields for a walking sequence yacoob jepson et al. ieee learned basis flow fields plots of motion coefficients over time and corresponding estimated motion fields. unknown coefficient ak in the parameterized flow field ux akvkx. figure shows a set of basis fields learned by observing videos of walking motions. figure shows the temporal evolution of the basis coefficients as well as a few of the recovered parametric motion fields. note that similar ideas can also be applied to feature tracks hertzmann and bregler which is a topic we discuss in more detail in sections and spline-based motion while parametric motion models are useful in a wide variety of applications as video stabilization and mapping onto planar surfaces most image motion is too complicated to be captured by such low-dimensional models. traditionally optical flow algorithms compute an independent motion estimate for each pixel i.e. the number of flow vectors computed is equal to the number of input pixels. the general optical flow analog to equation can thus be written as essd ofui ui spline-based motion figure spline motion field the displacement vectors ui vi are shown as pluses and are controlled by the smaller number of control vertices uj ui vj which are shown as circles notice how in the above equation the number of variables is twice the number of measurements so the problem is underconstrained. the two classic approaches to this problem which we study in section are to perform the summation over overlapping regions patch-based or window-based approach or to add smoothness terms on the field using regularization or markov random fields in this section we describe an alternative approach that lies somewhere between general optical flow flow at each pixel and parametric flow small number of global parameters. the approach is to represent the motion field as a two-dimensional spline controlled by a smaller number of control vertices uj ujwij ui ujbjxi where the bjxi are called the basis functions and are only non-zero over a small finite support interval and coughlan we call the wij bjxi weights to emphasize that the are known linear combinations of the uj. some commonly used spline basis functions are shown in figure substituting the formula for the individual per-pixel flow vectors ui into the ssd error metric yields a parametric motion formula similar to equation the biggest difference is that the jacobian j now consists of the sparse entries in the weight matrix w in situations where we know something more about the motion field e.g. when the motion is due to a camera moving in a static scene we can use more specialized motion models. for example the plane plus parallax model can be naturally combined with a spline-based motion representation where the in-plane motion is represented by a homography and the out-of-plane parallax d is represented by a scalar variable at each spline computer vision algorithms and applications draft figure sample spline basis functions and coughlan springer. the block interpolatorbasis corresponds to block-based motion estimation gall see section for more details on spline functions. spline-based motion figure quadtree spline-based motion estimation and shum ieee quadtree spline representation which can lead to cracks unless the white nodes are constrained to depend on their parents deformed quadtree spline mesh overlaid on grayscale image flow field visualized as a needle diagram. control point and kang szeliski and coughlan in many cases the small number of spline vertices results in a motion estimation problem that is well conditioned. however if large textureless regions elongated edges subject to the aperture problem persist across several spline patches it may be necessary to add a regularization term to make the problem well posed the simplest way to do this is to directly add squared difference penalties between adjacent vertices in the spline control mesh uj as in if a multi-resolution strategy is being used it is important to re-scale these smoothness terms while going from level to level. the linear system corresponding to the spline-based motion estimator is sparse and regular. because it is usually of moderate size it can often be solved using direct techniques such as cholesky decomposition alternatively if the problem becomes too large and subject to excessive fill-in iterative techniques such as hierarchically preconditioned conjugate gradient can be used instead because of its robustness spline-based motion estimation has been used for a number of applications including visual effects and medical image registration and lavall ee kybic and unser one disadvantage of the basic technique however is that the model does a poor job near motion discontinuities unless an excessive number of nodes is used. to remedy this situation szeliski and shum propose using a quadtree representation embedded in the spline control grid large cells are used to present regions of smooth motion while smaller cells are added in regions of motion discontinuities to estimate the motion a coarse-to-fine strategy is used. starting with a regular spline imposed over a lower-resolution image an initial motion estimate is obtained. spline patches where the motion is inconsistent i.e. the squared residual is above a threshold are subdivided into smaller patches. in order to avoid cracks in the resulting motion field computer vision algorithms and applications draft figure elastic brain registration and unser ieee original brain atlas and patient mri images overlaid in red green after elastic registration with eight user-specified landmarks shown a cubic b-spline deformation field shown as a deformed grid. ure the values of certain nodes in the refined mesh i.e. those adjacent to larger cells need to be restricted so that they depend on their parent values. this is most easily accomplished using a hierarchical basis representation for the quadtree spline and selectively setting some of the hierarchical basis functions to as described in and shum application medical image registration because they excel at representing smooth elastic deformation fields spline-based motion models have found widespread use in medical image registration and kovacic szeliski and lavall ee christensen joshi and miller registration techniques can be used both to track an individual patient s development or progress over time longitudinal study or to match different patient images together to find commonalities and detect variations or pathologies studies. when different imaging modalities are being registered e.g. computed tomography scans and magnetic resonance images mutual information measures of similarity are often necessary and wells iii maes collignon vandermeulen et al. kybic and unser provide a nice literature review and describe a complete working system based on representing both the images and the deformation fields as multi-resolution splines. figure shows an example of the kybic and unser system being used to register a patient s brain mri with a labeled brain atlas image. the system can be run in a fully auto in computer graphics such elastic volumetric deformation are known as free-form deformations and parry coquillart celniker and gossard optical flow figure octree spline-based image registration of two vertebral surface models and lavall ee springer after initial rigid alignment after elastic alignment a cross-section through the adapted octree spline deformation field. matic mode but more accurate results can be obtained by locating a few key landmarks. more recent papers on deformable medical image registration including performance evaluations include staring and pluim glocker komodakis tziritas et al. as with other applications regular volumetric splines can be enhanced using selective refinement. in the case of volumetric image or surface registration these are known as octree splines and lavall ee and have been used to register medical surface models such as vertebrae and faces from different patients optical flow the most general challenging version of motion estimation is to compute an independent estimate of motion at each pixel which is generally known as optical optic flow. as we mentioned in the previous section this generally involves minimizing the brightness or color difference between corresponding pixels summed over the image ui essd ofui since the number of variables is twice the number of measurements the problem is underconstrained. the two classic approaches to this problem are to perform the summation locally over overlapping regions patch-based or window-based approach or to add smoothness terms on the field using regularization or markov random fields and to search for a global minimum. the patch-based approach usually involves using a taylor series expansion of the displaced image function in order to obtain sub-pixel estimates and kanade computer vision algorithms and applications draft anandan shows how a series of local discrete search steps can be interleaved with lucas kanade incremental refinement steps in a coarse-to-fine pyramid scheme which allows the estimation of large motions as described in section he also analyzes how the uncertainty in local motion estimates is related to the eigenvalues of the local hessian matrix ai as shown in figures bergen anandan hanna et al. develop a unified framework for describing both parametric and patch-based optic flow algorithms and provide a nice introduction to this topic. after each iteration of optic flow estimation in a coarse-to-fine pyramid they re-warp one of the images so that only incremental flow estimates are computed when overlapping patches are used an efficient implementation is to first compute the outer products of the gradients and intensity errors at every pixel and then perform the overlapping window sums using a moving average instead of solving for each motion motion update independently horn and schunck develop a regularization-based framework where is simultaneously minimized over all flow vectors in order to constrain the problem smoothness constraints i.e. squared penalties on flow derivatives are added to the basic per-pixel error metric. because the technique was originally developed for small motions in a variational function framework the linearized brightness constancy constraint corresponding to i.e. is more commonly written as an analytic integral ehs iyv dx dy where iy j and it ei is the temporal derivative i.e. the brightness change between images. the horn and schunck model can also be viewed as the limiting case of spline-based motion estimation as the splines become pixel patches. it is also possible to combine ideas from local and global flow estimation into a single framework by using a locally aggregated opposed to single-pixel hessian as the brightness constancy term weickert and schn orr consider the discrete analog to the analytic global energy ehsd i ij t ut i t i ui i if we replace the per-pixel hessians ai ij t i and residuals bi j iei with areaaggregated versions we obtain a global minimization algorithm where regionbased brightness constraints are used. another extension to the basic optic flow model is to use a combination of global and local motion models. for example if we know that the motion is due to a camera smoothing or aggregation filters can also be used at this stage weickert and schn orr optical flow moving in a static scene motion we can re-formulate the problem as the estimation of a per-pixel depth along with the parameters of the global camera motion hanna bergen anandan hanna et al. szeliski and coughlan nir bruckstein and kimmel wedel cremers pock et al. such techniques are closely related to stereo matching alternatively we can estimate either per-image or per-segment affine motion models combined with per-pixel residual corrections and jepson ju black and jepson chang tekalp and sezan m emin and p erez we revisit this topic in section of course image brightness may not always be an appropriate metric for measuring appearance consistency e.g. when the lighting in an image is varying. as discussed in section matching gradients filtered images or other metrics such as image hessians derivative measures may be more appropriate. it is also possible to locally compute the phase of steerable filters in the image which is insensitive to both bias and gain transformations and jepson papenberg bruhn brox et al. review and explore such constraints and also provide a detailed analysis and justification for iteratively re-warping images during incremental flow computation. because the brightness constancy constraint is evaluated at each pixel independently rather than being summed over patches where the constant flow assumption may be violated global optimization approaches tend to perform better near motion discontinuities. this is especially true if robust metrics are used in the smoothness constraint and anandan bab-hadiashar and suter one popular choice for robust metrics in the norm also known as total variation which results in a convex energy whose global minimum can be found weickert and schn orr papenberg bruhn brox et al. anisotropic smoothness priors which apply a different smoothness in the directions parallel and perpendicular to the image gradient are another popular choice and enkelmann sun roth lewis et al. werlberger trobin pock et al. it is also possible to learn a set of better smoothness constraints filters and robust functions from a set of paired flow and intensity images roth lewis et al. additional details on some of these techniques are given by baker black lewis et al. and baker scharstein lewis et al. because of the large two-dimensional search space in estimating flow most algorithms use variations of gradient descent and coarse-to-fine continuation methods to minimize the global energy function. this contrasts starkly with stereo matching is an easier one-dimensional disparity estimation problem where combinatorial optimization techniques have been the method of choice for the last decade. fortunately combinatorial optimization methods based on markov random fields are be robust brightness metrics can also help improve the performance of window-based ap proaches and anandan computer vision algorithms and applications draft figure evaluation of the results of optical flow algorithms october http scharstein lewis et al. by moving the mouse pointer over an underlined performance score the user can interactively view the corresponding flow and error maps. clicking on a score toggles between the computed and ground truth flows. next to each score the corresponding rank in the current column is indicated by a smaller blue number. the minimum score in each column is shown in boldface. the table is sorted by the average rank over all columns three region masks for each of the eight sequences. the average rank serves as an approximate measure of performance under the selected metricstatistic. optical flow ginning to appear and tend to be among the better-performing methods on the recently released optical flow database black lewis et al. examples of such techniques include the one developed by glocker paragios komodakis et al. who use a coarse-to-fine strategy with per-pixel uncertainty estimates which are then used to guide the refinement and search at the next finer level. instead of using gradient descent to refine the flow estimates a combinatorial search over discrete displacement labels is able to find better energy minima is performed using their fast-pd algorithm tziritas and paragios lempitsky roth and rother. use fusion moves rother and blake over proposals generated from basic flow algorithms and schunck lucas and kanade to find good solutions. the basic idea behind fusion moves is to replace portions of the current best estimate with hypotheses generated by more basic techniques their shifted versions and to alternate them with local gradient descent for better energy minimization. the field of accurate motion estimation continues to evolve at a rapid pace with significant advances in performance occurring every year. the optical flow evaluation web site is a good source of pointers to high-performing recently developed algorithms multi-frame motion estimation so far we have looked at motion estimation as a two-frame problem where the goal is to compute a motion field that aligns pixels from one image with those in another. in practice motion estimation is usually applied to video where a whole sequence of frames is available to perform this task. one classic approach to multi-frame motion is to filter the spatio-temporal volume using oriented or steerable filters in a manner analogous to oriented edge detection figure shows two frames from the commonly used flower garden sequence as well as a horizontal slice through the spatio-temporal volume i.e. the volume created by stacking all of the video frames together. because the pixel motion is mostly horizontal the slopes of individual pixel tracks which correspond to their horizontal velocities can clearly be seen. spatio-temporal filtering uses a volume around each pixel to determine the best orientation in space time which corresponds directly to a pixel s velocity. unfortunately in order to obtain reasonably accurate velocity estimates everywhere in an image spatio-temporal filters have moderately large extents which severely degrades the quality of their estimates near motion discontinuities. same problem is endemic in httpvision.middlebury.eduflow. computer vision algorithms and applications draft figure slice through a spatio-temporal volume ieee b two frames from the flower garden sequence a horizontal slice through the complete spatio-temporal volume with the arrows indicating locations of potential key frames where flow is estimated. note that the colors for the flower garden sequence are incorrect the correct colors flowers are shown in figure window-based motion estimators. an alternative to full spatio-temporal filtering is to estimate more local spatio-temporal derivatives and use them inside a global optimization framework to fill in textureless regions weickert and schn orr govindu another alternative is to simultaneously estimate multiple motion estimates while also optionally reasoning about occlusion relationships figure shows schematically one potential approach to this problem. the horizontal arrows show the locations of keyframes s where motion is estimated while other slices indicate video frames t whose colors are matched with those predicted by interpolating between the keyframes. motion estimation can be cast as a global energy minimization problem that simultaneously minimizes brightness compatibility and flow compatibility terms between keyframes and other frames in addition to using robust smoothness terms. the multi-view framework is potentially even more appropriate for rigid scene motion stereo where the unknowns at each pixel are disparities and occlusion relationships can be determined directly from pixel depths kolmogorov and zabih however it may also be applicable to general motion with the addition of models for object accelerations and occlusion relationships. application video denoising video denoising is the process of removing noise and other artifacts such as scratches from film and video unlike single image denoising where the only information available is in the current picture video denoisers can average or borrow information from adjacent frames. however in order to do this without introducing blur or jitter motion they need accurate per-pixel motion estimates. exercise lists some of the steps required which include the ability to determine if the layered motion current motion estimate is accurate enough to permit averaging with other frames. gai and kang describe their recently developed restoration process which involves a series of additional steps to deal with the special characteristics of vintage film. application de-interlacing another commonly used application of per-pixel motion estimation is video de-interlacing which is the process of converting a video taken with alternating fields of even and odd lines to a non-interlaced signal that contains both fields in each frame haan and bellers two simple de-interlacing techniques are bob which copies the line above or below the missing line from the same field and weave which copies the corresponding line from the field before or after. the names come from the visual artifacts generated by these two simple techniques bob introduces an up-and-down bobbing motion along strong horizontal lines weave can lead to a zippering effect along horizontally translating edges. replacing these copy operators with averages can help but does not completely remove these artifacts. a wide variety of improved techniques have been developed for this process which is often embedded in specialized dsp chips found inside video digitization boards in computers broadcast video is often interlaced while computer monitors are not. a large class of these techniques estimates local per-pixel motions and interpolates the missing data from the information available in spatially and temporally adjacent fields. dai baker and kang review this literature and propose their own algorithm which selects among seven different interpolation functions at each pixel using an mrf framework. layered motion in many situation visual motion is caused by the movement of a small number of objects at different depths in the scene. in such situations the pixel motions can be described more succinctly estimated more reliably if pixels are grouped into appropriate objects or layers and adelson figure shows this approach schematically. the motion in this sequence is caused by the translational motion of the checkered background and the rotation of the foreground hand. the complete motion sequence can be reconstructed from the appearance of the foreground and background elements which can be represented as alpha-matted images or video objects and the parametric motion corresponding to each layer. displacing and compositing these layers in back to front order recreates the original video sequence. layered motion representations not only lead to compact representations and adelson lee ge chen lung bruce lin et al. but they also exploit the information available in multiple video frames as well as accurately modeling the appearance of computer vision algorithms and applications draft intensity map alpha map velocity map intensity map alpha map velocity map frame frame frame figure layered motion estimation framework and adelson ieee the top two rows describe the two layers each of which consists of an intensity image an alpha mask and a parametric motion field. the layers are composited with different amounts of motion to recreate the video sequence. pixels near motion discontinuities. this makes them particularly suited as a representation for image-based rendering gortler he et al. zitnick kang uyttendaele et al. as well as object-level video editing. to compute a layered representation of a video sequence wang and adelson first estimate affine motion models over a collection of non-overlapping patches and then cluster these estimates using k-means. they then alternate between assigning pixels to layers and recomputing motion estimates for each layer using the assigned pixels using a technique first proposed by darrell and pentland once the parametric motions and pixel-wise layer assignments have been computed for each frame independently layers are constructed by warping and merging the various layer pieces from all of the frames together. median filtering is used to produce sharp composite layers that are robust to small intensity variations as well as to infer occlusion relationships between the layers. figure shows the results of this process on the flower garden sequence. you can see both the initial and final layer assignments for one of the frames as well as the composite flow and the alpha-matted layers with their corresponding flow vectors overlaid. in follow-on work weiss and adelson use a formal probabilistic mixture model to infer both the optimal number of layers and the per-pixel layer assignments. weiss layered motion flow initial layers final layers color image frame layers with pixel assignments and flow figure layered motion estimation results and adelson ieee. further generalizes this approach by replacing the per-layer affine motion models with smooth regularized per-pixel motion estimates which allows the system to better handle curved and undulating layers such as those seen in most real-world sequences. the above approaches however still make a distinction between estimating the motions and layer assignments and then later estimating the layer colors. in the system described by baker szeliski and anandan the generative model illustrated in figure is generalized to account for real-world rigid motion scenes. the motion of each frame is described using a camera model and the motion of each layer is described using a plane equation plus per-pixel residual depth offsets plane plus parallax representation the initial layer estimation proceeds in a manner similar to that of wang and adelson except that rigid planar motions are used instead of affine motion models. the final model refinement however jointly re-optimizes the layer pixel color and opacity values ll and the depth plane and motion parameters zl nl and p t by minimizing the discrepancy between the re-synthesized and observed motion sequences szeliski and anandan figure shows the final results obtained with this algorithm. as you can see the motion boundaries and layer assignments are much crisper than those in figure because of the per-pixel depth offsets the individual layer color values are also sharper than those obtained with affine or planar motion models. while the original system of baker szeliski and anandan required a rough initial assignment of pixels to layers torr szeliski and anandan describe automated bayesian techniques for initializing this system and determining the optimal number of layers. layered motion estimation continues to be an active area of research. representative papers in this area include and ayer jojic and frey xiao and shah kumar torr and zisserman thayananthan iwasaki and cipolla schoenemann and cremers computer vision algorithms and applications draft figure layered stereo reconstruction szeliski and anandan ieee first and last input images initial segmentation into six layers and the six layer sprites depth map for planar sprites denotes closer front layer before and after residual depth estimation. note that the colors for the flower garden sequence are incorrect the correct colors flowers are shown in figure o of course layers are not the only way to introduce segmentation into motion estimation. a large number of algorithms have been developed that alternate between estimating optic flow vectors and segmenting them into coherent regions and jepson ju black and jepson chang tekalp and sezan m emin and p erez cremers and soatto some of the more recent techniques rely on first segmenting the input color images and then estimating per-segment motions that produce a coherent motion field while also modeling occlusions kang uyttendaele et al. zitnick jojic and kang stein hoiem and hebert thayananthan iwasaki and cipolla application frame interpolation frame interpolation is another widely used application of motion estimation often implemented in the same circuitry as de-interlacing hardware required to match an incoming video layered motion to a monitor s actual refresh rate. as with de-interlacing information from novel in-between frames needs to be interpolated from preceding and subsequent frames. the best results can be obtained if an accurate motion estimate can be computed at each unknown pixel s location. however in addition to computing the motion occlusion information is critical to prevent colors from being contaminated by moving foreground objects that might obscure a particular pixel in a preceding or subsequent frame. in a little more detail consider figure and assume that the arrows denote keyframes between which we wish to interpolate additional images. the orientations of the streaks in this figure encode the velocities of individual pixels. if the same motion estimate is obtained at location in image as is obtained at location in image the flow vectors are said to be consistent. this motion estimate can be transferred to location in the image it being generated where t is the time of interpolation. the final color value at pixel can be computed as a linear blend if however the motion vectors are different at corresponding locations some method must be used to determine which is correct and which image contains colors that are occluded. the actual reasoning is even more subtle than this. one example of such an interpolation algorithm based on earlier work in depth map interpolation gortler he et al. zitnick kang uyttendaele et al. which is the one used in the flow evaluation paper of baker black lewis et al. baker scharstein lewis et al. an even higherquality frame interpolation algorithm which uses gradient-based reconstruction is presented by mahajan huang matusik et al. transparent layers and reflections a special case of layered motion that occurs quite often is transparent motion which is usually caused by reflections seen in windows and picture frames and some of the early work in this area handles transparent motion by either just estimating the component motions and mase bergen burt hingorani et al. darrell and simoncelli irani rousso and peleg or by assigning individual pixels to competing motion layers and pentland black and anandan ju black and jepson which is appropriate for scenes partially seen through a fine occluder foliage. however to accurately separate truly transparent layers a better model for motion due to reflections is required. because of the way that light is both reflected from and transmitted through a glass surface the correct model for reflections is an additive one where each moving layer contributes some intensity to the final image avidan and anandan computer vision algorithms and applications draft figure light reflecting off the transparent glass of a picture frame first image from the input sequence dominant motion layer min-composite secondary motion residual layer max-composite e final estimated picture and reflection layers the original images are from black and anandan while the separated layers are from szeliski avidan and anandan ieee. if the motions of the individual layers are known the recovery of the individual layers is a simple constrained least squares problem with the individual layer images are constrained to be positive. however this problem can suffer from extended low-frequency ambiguities especially if either of the layers lacks dark pixels or the motion is uni-directional. in their paper szeliski avidan and anandan show that the simultaneous estimation of the motions and layer values can be obtained by alternating between robustly computing the motion layers and then making conservative or lower-bound estimates of the layer intensities. the final motion and layer estimates can then be polished using gradient descent on a joint constrained least squares formulation similar to szeliski and anandan where the over compositing operator is replaced with addition. figures and show the results of applying these techniques to two different picture frames with reflections. notice how in the second sequence the amount of reflected light is quite low compared to the transmitted light picture of the girl and yet the algorithm is still able to recover both layers. unfortunately the simple parametric motion models used in avidan and anandan are only valid for planar reflectors and scenes with shallow depth. the extension of these techniques to curved reflectors and scenes with significant depth has also been studied additional reading figure transparent motion separation avidan and anandan ieee first image from input sequence dominant motion layer min-composite secondary motion residual layer max-composite e final estimated picture and reflection layers. note that the reflected layers in and are doubled in intensity to better show their structure. kang szeliski et al. criminisi kang swaminathan et al. as has the extension to scenes with more complex depth kang and szeliski additional reading some of the earliest algorithms for motion estimation were developed for motion-compensated video coding and robbins and such techniques continue to be used in modern coding standards such as mpeg and gall richardson in computer vision this field was originally called image sequence analysis some of the early seminal papers include the variational approaches developed by horn and schunck and nagel and enkelmann and the patch-based translational alignment technique developed by lucas and kanade hierarchical versions of such algorithms were developed by quam anandan and bergen anandan hanna et al. although they have also long been used in motion estimation for video coding. translational motion models were generalized to affine motion by rehg and witkin fuh and maragos and bergen anandan hanna et al. and to quadric reference surfaces by shashua and toelg and shashua and wexler see baker and matthews for a nice review. such parametric motion estimation algorithms have found widespread application in video summarization and bender irani and anandan video stabilization anandan dana et al. srinivasan chellappa computer vision algorithms and applications draft veeraraghavan et al. matsushita ofek ge et al. and video compression hsu and anandan lee ge chen lung bruce lin et al. surveys of parametric image registration include those by brown zitov aa and flusser goshtasby and szeliski good general surveys and comparisons of optic flow algorithms include those by aggarwal and nandhakumar barron fleet and beauchemin otte and nagel mitiche and bouthemy stiller and konrad mccane novins crannitch et al. szeliski and baker black lewis et al. the topic of matching primitives i.e. pre-transforming images using filtering or other techniques before matching is treated in a number of papers bergen anandan hanna et al. scharstein zabih and woodfill cox roy and hingorani viola and wells iii negahdaripour kim kolmogorov and zabih jia and tang papenberg bruhn brox et al. seitz and baker hirschm uller and scharstein compare a number of these approaches and report on their relative performance in scenes with exposure differences. the publication of a new benchmark for evaluating optical flow algorithms black lewis et al. has led to rapid advances in the quality of estimation algorithms to the point where new datasets may soon become necessary. according to their updated technical report scharstein lewis et al. most of the best performing algorithms use robust data and smoothness norms tv and continuous variational optimization techniques although some techniques use discrete optimization or segmentations bruhn brox et al. trobin pock cremers et al. xu chen and jia lempitsky roth and rother. werlberger trobin pock et al. lei and yang wedel cremers pock et al. exercises ex correlation implement and compare the performance of the following correlation algorithms sum of squared differences sum of robust differences sum of absolute differences bias gain compensated squared differences normalized cross-correlation exercises windowed versions of the above fourier-based implementations of the above measures phase correlation gradient cross-correlation and vlachos compare a few of your algorithms on different motion sequences with different amounts of noise exposure variation occlusion and frequency variations high-frequency textures such as sand or cloth and low-frequency images such as clouds or motion-blurred video. some datasets with illumination variation and ground truth correspondences motion can be found at httpvision.middlebury.edustereodata and datasets. some additional ideas variants and questions when do you think that phase correlation will outperform regular correlation or ssd? can you show this experimentally or justify it analytically? for the fourier-based masked or windowed correlation and sum of squared differences the results should be the same as the direct implementations. note that you will have to expand into a sum of pairwise correlations just as in is part of the exercise. for the bias gain corrected variant of squared differences you will also have to expand the terms to end up with a squares system of equations. if implementing the fast fourier transform version you will need to figure out how all of these entries can be evaluated in the fourier domain. implement some of the additional techniques studied by hirschm uller and scharstein and see if your results agree with theirs. ex affine registration implement a coarse-to-fine direct method for affine and projective image alignment. does it help to use lower-order models at coarser levels of the pyramid anandan hanna et al. implement patch-based acceleration and szeliski baker and matthews see the baker and matthews survey for more comparisons and ideas. ex stabilization write a program to stabilize an input video sequence. you should implement the following steps as described in section computer vision algorithms and applications draft compute the translation optionally rotation between successive frames with ro bust outlier rejection. perform temporal high-pass filtering on the motion parameters to remove the low frequency component the motion. compensate for the high-frequency motion zooming in slightly user-specified amount to avoid missing edge pixels. do not zoom in but instead borrow pixels from previous or subsequent frames to fill in. compensate for images that are blurry because of fast motion by stealing higher frequencies from adjacent frames. ex optical flow compute optical flow or per-pixel between two images using one or more of the techniques described in this chapter. test your algorithms on the motion sequences available at httpvision.middlebury. eduflow or httppeople.csail.mit.educeliumotionannotation and compare your results to those available on these web sites. if you think your algorithm is competitive with the best consider submitting it for formal evaluation. visualize the quality of your results by generating in-between images using frame in terpolation what can you say about the relative efficiency of your approach? ex automated morphing frame interpolation write a program to automatically morph between pairs of images. implement the following steps as sketched out in section and by baker scharstein lewis et al. compute the flow both ways exercise. consider using a multi-frame technique to better deal with occluded regions. for each intermediate image compute a set of flow vectors and which im ages should be used in the final composition. blend the images and view with a sequence viewer. try this out on images of your friends and colleagues and see what kinds of morphs you get. alternatively take a video sequence and do a high-quality slow-motion effect. compare your algorithm with simple cross-fading. exercises ex motion-based user interaction write a program to compute a low-resolution motion field in order to interactively control a simple application and turk for example downsample each image using a pyramid and compute the optical flow or pixel-based from the previous frame. segment each training video sequence into different actions hand moving inwards moving up no motion and learn the velocity fields associated with each one. can simply find the mean and variance for each motion field or use something more sophisticated such as a support vector machine write a recognizer that finds successive actions of approximately the right duration and hook it up to an interactive application a sound generator or a computer game. ask your friends to test it out. ex video denoising implement the algorithm sketched in application your algorithm should contain the following steps compute accurate per-pixel flow. determine which pixels in the reference image have good matches with other frames. either average all of the matched pixels or choose the sharpest image if trying to compensate for blur. don t forget to use regular single-frame denoising techniques as part of your solution section section and exercise devise a fall-back strategy for areas where you don t think the flow estimates are accu rate enough. ex motion segmentation write a program to segment an image into separately moving regions or to reliably find motion boundaries. use the human-assisted motion segmentation database at httppeople.csail.mit.educeliu motionannotation as some of your test data. ex layered motion estimation decompose into separate layers a video sequence of a scene taken with a moving camera find the set of dominant or planar perspective motions either by computing them in blocks or finding a robust estimate and then iteratively re-fitting outliers. determine which pixels go with each motion. computer vision algorithms and applications draft construct the layers by blending pixels from different frames. add per-pixel residual flows or depths. refine your estimates using an iterative global optimization technique. write an interactive renderer to generate in-between frames or view the scene from different viewpoints gortler he et al. construct an unwrap mosaic from a more complex scene and use this to do some video editing kohli fitzgibbon et al. ex transparent motion and reflection estimation take a video sequence looking through a window picture frame and see if you can remove the reflection in order to better see what is inside. the steps are described in section and by szeliski avidan and anandan alternative approaches can be found in work by shizawa and mase bergen burt hingorani et al. darrell and simoncelli darrell and pentland irani rousso and peleg black and anandan and ju black and jepson chapter image stitching motion models global alignment planar perspective motion application whiteboard and document scanning rotational panoramas gap closing application video summarization and compression cylindrical and spherical coordinates bundle adjustment recognizing panoramas direct vs. feature-based alignment pixel selection and weighting parallax removal compositing choosing a compositing surface application photomontage blending additional reading exercises computer vision algorithms and applications draft figure image stitching portion of a cylindrical panorama and a spherical panorama constructed from photographs and shum acm a multi-image panorama automatically assembled from an unordered photo collection a multiimage stitch without and with moving object removal eden and szeliski ieee. image stitching algorithms for aligning images and stitching them into seamless photo-mosaics are among the oldest and most widely used in computer vision peleg image stitching algorithms create the high-resolution photo-mosaics used to produce today s digital maps and satellite photos. they also come bundled with most digital cameras and can be used to create beautiful ultra wide-angle panoramas. image stitching originated in the photogrammetry community where more manually intensive methods based on surveyed ground control points or manually registered tie points have long been used to register aerial photos into large-scale photo-mosaics one of the key advances in this community was the development of bundle adjustment algorithms which could simultaneously solve for the locations of all of the camera positions thus yielding globally consistent solutions mclauchlan hartley et al. another recurring problem in creating photo-mosaics is the elimination of visible seams for which a variety of techniques have been developed over the years peleg davis agarwala dontcheva agrawala et al. in film photography special cameras were developed in the to take ultra-wideangle panoramas often by exposing the film through a vertical slit as the camera rotated on its axis in the image alignment techniques started being applied to the construction of wide-angle seamless panoramas from regular hand-held cameras and picard chen szeliski more recent work in this area has addressed the need to compute globally consistent alignments and shum sawhney and kumar shum and szeliski to remove ghosts due to parallax and object movement shum and szeliski uyttendaele eden and szeliski agarwala dontcheva agrawala et al. and to deal with varying exposures and picard uyttendaele eden and szeliski levin zomet peleg et al. agarwala dontcheva agrawala et al. eden uyttendaele and szeliski kopf uyttendaele deussen et al. these techniques have spawned a large number of commercial stitching products sawhney kumar gendel et al. of which reviews and comparisons can be found on the while most of the earlier techniques worked by directly minimizing pixel-to-pixel dissimilarities more recent algorithms usually extract a sparse set of features and match them to each other as described in chapter such feature-based approaches to image stitching have the advantage of being more robust against scene movement and are potentially faster if implemented the right way. their biggest advantage however is the ability to recognize panoramas i.e. to automatically discover the adjacency relationships among an unordered set of images which makes them ideally suited for fully automated stitching of a collection of some of these papers was compiled by benosman and kang and they are surveyed by szeliski the photosynth web site httpphotosynth.net allows people to create and upload panoramas for free. computer vision algorithms and applications draft panoramas taken by casual users and lowe what then are the essential problems in image stitching? as with image alignment we must first determine the appropriate mathematical model relating pixel coordinates in one image to pixel coordinates in another section reviews the basic models we have studied and presents some new motion models related specifically to panoramic image stitching. next we must somehow estimate the correct alignments relating various pairs collections of images. chapter discussed how distinctive features can be found in each image and then efficiently matched to rapidly establish correspondences between pairs of images. chapter discussed how direct pixel-to-pixel comparisons combined with gradient descent other optimization techniques can also be used to estimate these parameters. when multiple images exist in a panorama bundle adjustment can be used to compute a globally consistent set of alignments and to efficiently discover which images overlap one another. in section we look at how each of these previously developed techniques can be modified to take advantage of the imaging setups commonly used to create panoramas. once we have aligned the images we must choose a final compositing surface for warping the aligned images we also need algorithms to seamlessly cut and blend overlapping images even in the presence of parallax lens distortion scene motion and exposure differences motion models before we can register and align images we need to establish the mathematical relationships that map pixel coordinates from one image to another. a variety of such parametric motion models are possible from simple transforms to planar perspective models camera rotations lens distortions and mapping to non-planar cylindrical surfaces. we already covered several of these models in sections and in particular we saw in section how the parametric motion describing the deformation of a planar surfaced as viewed from different positions can be described with an eight-parameter homography and picard szeliski we also saw how a camera undergoing a pure rotation induces a different kind of homography in this section we review both of these models and show how they can be applied to different stitching situations. we also introduce spherical and cylindrical compositing surfaces and show how under favorable circumstances they can be used to perform alignment using pure translations deciding which alignment model is most appropriate for a given situation or set of data is a model selection problem tibshirani and friedman torr bishop robert an important topic we do not cover in this book. motion models translation dof affine dof perspective dof rotation dof figure two-dimensional motion models and how they can be used for image stitching. planar perspective motion the simplest possible motion model to use when aligning images is to simply translate and rotate them in this is exactly the same kind of motion that you would use if you had overlapping photographic prints. it is also the kind of technique favored by david hockney to create the collages that he calls joiners and perona nomura zhang and nayar creating such collages which show visible seams and inconsistencies that add to the artistic effect is popular on web sites such as flickr where they more commonly go under the name panography translation and rotation are also usually adequate motion models to compensate for small camera motions in applications such as photo and video stabilization and merging and section in section we saw how the mapping between two cameras viewing a common plane can be described using a homography consider the matrix m that arises when mapping a pixel in one image to a point and then back onto a second image p p m when the last row of the p matrix is replaced with a plane equation and points are assumed to lie on this plane i.e. their disparity is we can ignore the last column of m and also its last row since we do not care about the final z-buffer depth. the resulting homography matrix h upper left sub-matrix of m describes the mapping between pixels in the two images h this observation formed the basis of some of the earliest automated image stitching algorithms and picard szeliski because reliable feature matching techniques had not yet been developed these algorithms used direct pixel value matching i.e. direct parametric motion estimation as described in section and equations computer vision algorithms and applications draft more recent stitching algorithms first extract features and then match them up often using robust techniques such as ransac to compute a good set of inliers. the final computation of the homography i.e. the solution of the least squares fitting problem given pairs of corresponding features and uses iterative least squares as described in section and equations application whiteboard and document scanning the simplest image-stitching application is to stitch together a number of image scans taken on a flatbed scanner. say you have a large map or a piece of child s artwork that is too large to fit on your scanner. simply take multiple scans of the document making sure to overlap the scans by a large enough amount to ensure that there are enough common features. next take successive pairs of images that you know overlap extract features match them up and estimate the rigid transform rkxk tk that best matches the features using two-point ransac if necessary to find a good set of inliers. then on a final compositing surface with the first scan for example resample your images and average them together. can you see any potential problems with this scheme? one complication is that a rigid transformation is non-linear in the rotation angle so you will have to either use non-linear least squares or constrain r to be orthonormal as described in section a bigger problem lies in the pairwise alignment process. as you align more and more pairs the solution may drift so that it is no longer globally consistent. in this case a global optimization procedure as described in section may be required. such global optimization often requires a large system of non-linear equations to be solved although in some cases such as linearized homographies or similarity transforms regular least squares may be an option. a slightly more complex scenario is when you take multiple overlapping handheld pictures of a whiteboard or other large planar object and zhang zhang and he here the natural motion model to use is a homography although a more complex model that estimates the rigid motion relative to the plane the focal length if unknown could in principle be used. motion models figure pure camera rotation. the form of the homography is particularly simple and depends only on the rotation matrix and focal lengths. rotational panoramas the most typical case for panoramic image stitching is when the camera undergoes a pure rotation. think of standing at the rim of the grand canyon. relative to the distant geometry in the scene as you snap away the camera is undergoing a pure rotation which is equivalent to assuming that all points are very far from the camera i.e. on the plane at infinity setting we get the simplified homography h k where kk diagfk fk is the simplified camera intrinsic matrix assuming that cx cy i.e. we are indexing the pixels starting from the optical center this can also be re-written as f f or which reveals the simplicity of the mapping equations and makes all of the motion parameters explicit. thus instead of the general eight-parameter homography relating a pair of images we get the three- four- or five-parameter rotation motion models corresponding to the cases where the focal length f is known fixed or variable and shum estimating the rotation matrix optionally focal length associated with each image is an initial estimate of the focal lengths can be obtained using the intrinsic calibration techniques described in section or from exif tags. p computer vision algorithms and applications draft intrinsically more stable than estimating a homography with a full eight degrees of freedom which makes this the method of choice for large-scale image stitching algorithms and shum shum and szeliski brown and lowe given this representation how do we update the rotation matrices to best align two overlapping images? given a current estimate for the homography h in the best way to update is to prepend an incremental rotation matrix r to the current estimate and shum shum and szeliski h d h note that here we have written the update rule in the compositional form where the incremental update d is prepended to the current homography h using the small-angle approximation to r given in we can write the incremental update matrix as d z z y x notice how there is now a nice one-to-one correspondence between the entries in the d matrix and the parameters used in table and equation i.e. z y z x we can therefore apply the chain rule to equations and to obtain x y y x x y z which give us the linearized update equations needed to estimate x y notice that this update rule depends on the focal length of the target view and is independent of the focal length of the template view. this is because the compositional algorithm essentially makes small perturbations to the target. once the incremental rotation vector has been computed the rotation matrix can be updated using r the formulas for updating the focal length estimates are a little more involved and are given in and szeliski we will not repeat them here since an alternative update rule based on minimizing the difference between back-projected rays is given in section figure shows the alignment of four images under the rotation motion model. this is the same as the rotational component of instantaneous rigid flow anandan hanna et al. and the update equations given by szeliski and shum and shum and szeliski motion models figure four images taken with a hand-held camera registered using a rotation motion model and shum acm. notice how the homographies rather than being arbitrary have a well-defined keystone shape whose width increases away from the origin. gap closing the techniques presented in this section can be used to estimate a series of rotation matrices and focal lengths which can be chained together to create large panoramas. unfortunately because of accumulated errors this approach will rarely produce a closed panorama. instead there will invariably be either a gap or an overlap we can solve this problem by matching the first image in the sequence with the last one. the difference between the two rotation matrix estimates associated with the repeated first indicates the amount of misregistration. this error can be distributed evenly across the whole sequence by taking the quotient of the two quaternions associated with these rotations and dividing this error quaternion by the number of images in the sequence relatively constant inter-frame rotations. we can also update the estimated focal length based on the amount of misregistration. to do this we first convert the error quaternion into a gap angle g and then update the focal length using the equation figure shows the end of registered image sequence and the first image. there is a big gap between the last image and the first which are in fact the same image. the gap is because the wrong estimate of focal length was used. figure shows the registration after closing the gap with the correct focal length notice that both mosaics show very little visual misregistration at the gap yet figure has been computed using a focal length that has error. related approaches have been developed by hartley mcmillan and bishop stein and kang and weiss to solve the focal length estimation problem using pure panning motion and cylindrical images. computer vision algorithms and applications draft figure gap closing and shum acm a gap is visible when the focal length is wrong no gap is visible for the correct focal length unfortunately this particular gap-closing heuristic only works for the kind of one-dimensional panorama where the camera is continuously turning in the same direction. in section we describe a different approach to removing gaps and overlaps that works for arbitrary camera motions. application video summarization and compression an interesting application of image stitching is the ability to summarize and compress videos taken with a panning camera. this application was first suggested by teodosio and bender who called their mosaic-based summaries salient stills. these ideas were then extended by irani hsu and anandan kumar anandan irani et al. and irani and anandan to additional applications such as video compression and video indexing. while these early approaches used affine motion models and were therefore restricted to long focal lengths the techniques were generalized by lee ge chen lung bruce lin et al. to full eight-parameter homographies and incorporated into the video compression standard where the stitched background layers were called video sprites while video stitching is in many ways a straightforward generalization of multiple-image stitching pal and szeliski baudisch tan steedly et al. the potential presence of large amounts of independent motion camera zoom and the desire to visualize dynamic events impose additional challenges. for example moving foreground objects can often be removed using median filtering. alternatively foreground objects can be extracted into a separate layer and ayer and later composited back into the stitched panoramas sometimes as multiple instances to give the impressions of a chronophotograph motion models figure video stitching the background scene to create a single sprite image that can be transmitted and used to re-create the background in each frame ge chen lung bruce lin et al. ieee. and bender and sometimes as video overlays and anandan videos can also be used to create animated panoramic video textures in which different portions of a panoramic scene are animated with independently moving video loops zheng pal et al. rav-acha pritch lischinski et al. or to shine video flashlights onto a composite mosaic of a scene arpa kumar et al. video can also provide an interesting source of content for creating panoramas taken from moving cameras. while this invalidates the usual assumption of a single point of view center interesting results can still be obtained. for example the videobrush system of sawhney kumar gendel et al. uses thin strips taken from the center of the image to create a panorama taken from a horizontally moving camera. this idea can be generalized to other camera motions and compositing surfaces using the concept of mosaics on adaptive manifold rousso rav-acha et al. and also used to generate panoramic stereograms ben-ezra and pritch related ideas have been used to create panoramic matte paintings for multi-plane cel animation finkelstein hughes et al. for creating stitched images of scenes with parallax anandan irani et al. and as representations of more complex scenes using multiple-center-of-projection images and bishop and multi-perspective panoramas an garg and levoy rom an and lensch agarwala agrawala cohen et al. another interesting variant on video-based panoramas are concentric mosaics and he here rather than trying to produce a single panoramic image the complete original video is kept and used to re-synthesize views different camera origins using ray remapping field rendering thus endowing the panorama with a sense of computer vision algorithms and applications draft figure projection from to cylindrical and spherical coordinates. depth. the same data set can also be used to explicitly reconstruct the depth using multibaseline stereo ben-ezra and pritch li shum tang et al. zheng kang cohen et al. cylindrical and spherical coordinates an alternative to using homographies or motions to align images is to first warp the images into cylindrical coordinates and then use a pure translational model to align them szeliski unfortunately this only works if the images are all taken with a level camera or with a known tilt angle. assume for now that the camera is in its canonical position i.e. its rotation matrix is the identity r i so that the optical axis is aligned with the z axis and the y axis is aligned vertically. the ray corresponding to an y pixel is therefore y f. we wish to project this image onto a cylindrical surface of unit radius points on this surface are parameterized by an angle and a height h with the cylindrical coordinates corresponding to h given by h cos y f as shown in figure from this correspondence we can compute the formula for the warped or mapped coordinates and shum s s tan x f sh s y f where s is an arbitrary scaling factor called the radius of the cylinder that can be set to s f to minimize the distortion near the center of the the inverse of the scale can also be set to a larger or smaller value for the final compositing surface depending on the desired output panorama resolution see section p hxyp cos sin cos xy motion models this mapping equation is given by s sec s x f tan f tan s y f s f images can also be projected onto a spherical surface and shum which is useful if the final panorama includes a full sphere or hemisphere of views instead of just a cylindrical strip. in this case the sphere is parameterized by two angles with spherical coordinates given by cos sin cos cos y f as shown in figure the correspondence between coordinates is now given by and shum sec s s s tan x f s s tan y f while the inverse is given by x f tan f tan s y f tan tan s f tan s note that it may be simpler to generate a scaled y z direction from equation followed by a perspective division by z and a scaling by f. cylindrical image stitching algorithms are most commonly used when the camera is known to be level and only rotating around its vertical axis under these conditions images at different rotations are related by a pure horizontal this makes it attractive as an initial class project in an introductory computer vision course since the full complexity of the perspective alignment algorithm and can be avoided. figure shows how two cylindrically warped images from a leveled rotational panorama are related by a pure translation and shum professional panoramic photographers often use pan-tilt heads that make it easy to control the tilt and to stop at specific detents in the rotation angle. motorized rotation heads are also note that these are not the usual spherical coordinates first presented in equation here the y axis points at the north pole instead of the z axis since we are used to viewing images taken horizontally i.e. with the y axis pointing in the direction of the gravity vector. vertical tilts can sometimes be compensated for with vertical translations. computer vision algorithms and applications draft figure a cylindrical panorama and shum acm two cylindrically warped images related by a horizontal translation part of a cylindrical panorama composited from a sequence of images. figure a spherical panorama constructed from photographs and shum acm. sometimes used for the acquisition of larger panoramas uyttendaele deussen et al. not only do they ensure a uniform coverage of the visual field with a desired amount of image overlap but they also make it possible to stitch the images using cylindrical or spherical coordinates and pure translations. in this case pixel coordinates y f must first be rotated using the known tilt and panning angles before being projected into cylindrical or spherical coordinates having a roughly known panning angle also makes it easier to compute the alignment since the rough relative positioning of all the input images is known ahead of time enabling a reduced search range for alignment. figure shows a full rotational panorama unwrapped onto the surface of a sphere and shum one final coordinate mapping worth mentioning is the polar mapping where the north also httpgigapan.org. global alignment pole lies along the optical axis rather than the vertical axis sin sin sin cos s y z. in this case the mapping equations become s cos s s sin s x r y r tan r z tan r z where r is the radial distance in the y plane and s plays a similar role in the plane. this mapping provides an attractive visualization surface for certain kinds of wide-angle panoramas and is also a good model for the distortion induced by fisheye lenses as discussed in section note how for small values of y the mapping equations reduce to sxz which suggests that s plays a role similar to the focal length f. global alignment so far we have discussed how to register pairs of images using a variety of motion models. in most applications we are given more than a single pair of images to register. the goal is then to find a globally consistent set of alignment parameters that minimize the mis-registration between all pairs of images and shum shum and szeliski sawhney and kumar coorg and teller in this section we extend the pairwise matching criteria and to a global energy function that involves all of the per-image pose parameters once we have computed the global alignment we often need to perform local adjustments such as parallax removal to reduce double images and blurring due to local mis-registrations finally if we are given an unordered set of images to register we need to discover which images go together to form one or more panoramas. this process of panorama recognition is described in section bundle adjustment one way to register a large number of images is to add new images to the panorama one at a time aligning the most recent image with the previous ones already in the collection and shum and discovering if necessary which images it overlaps and kumar in the case of panoramas accumulated error may lead to the presence of a gap excessive overlap between the two ends of the panorama which can be fixed computer vision algorithms and applications draft by stretching the alignment of all the images using a process called gap closing and shum however a better alternative is to simultaneously align all the images using a least-squares framework to correctly distribute any mis-registration errors. the process of simultaneously adjusting pose parameters for a large collection of overlapping images is called bundle adjustment in the photogrammetry community mclauchlan hartley et al. in computer vision it was first applied to the general structure from motion problem and kang and then later specialized for panoramic image stitching and szeliski sawhney and kumar coorg and teller in this section we formulate the problem of global alignment using a feature-based approach since this results in a simpler system. an equivalent direct approach can be obtained either by dividing images into patches and creating a virtual feature correspondence for each one discussed in section and by shum and szeliski or by replacing the per-feature error metrics with per-pixel metrics. consider the feature-based alignment problem given in equation i.e. epairwise ls p for multi-image alignment instead of having a single collection of pairwise feature correspondences we have a collection of n features with the location of the ith feature point in the jth image denoted by xij and its scalar confidence inverse variance denoted by each image also has some associated pose parameters. in this section we assume that this pose consists of a rotation matrix rj and a focal length fj although formulations in terms of homographies are also possible and shum sawhney and kumar the equation mapping a point xi into a point xij in frame j can be re-written from equations and as j xij xij kjrjxi and xi r j k where kj diagfj fj is the simplified form of the calibration matrix. the motion mapping a point xij from frame j into a point xik in frame k is similarly given by xik h kj xij kkrkr given an initial set of fj estimates obtained from chaining pairwise alignments how do we refine these estimates? one approach is to directly extend the pairwise energy epairwise ls to a multiview j k j xij. formulation eall pairs xik xij rj fj rk fk features that are not seen in image j have cij we can also use inverse covariance matrices in ij place of cij as shown in equation global alignment where the xik function is the predicted location of feature i in frame k given by xij is the observed location and the in the subscript indicates that an image-plane error is being minimized and szeliski note that since xik depends on the xij observed value we actually have an errors-in-variable problem which in principle requires more sophisticated techniques than least squares to solve huffel and lemmerling matei and meer however in practice if we have enough features we can directly minimize the above quantity using regular non-linear least squares and obtain an accurate multi-frame alignment. while this approach works well in practice it suffers from two potential disadvantages. first since a summation is taken over all pairs with corresponding features features that are observed many times are overweighted in the final solution. effect a feature observed m times instead of m times. second the derivatives of xik with respect times gets to the fj are a little cumbersome although using the incremental correction to rj introduced in section makes this more tractable. an alternative way to formulate the optimization is to use true bundle adjustment i.e. to solve not only for the pose parameters fj but also for the point positions eba xijxi rj fj where xijxi rj fj is given by the disadvantage of full bundle adjustment is that there are more variables to solve for so each iteration and also the overall convergence may be slower. how the points need to shift each time some rotation matrices are updated. however the computational complexity of each linearized gauss newton step can be reduced using sparse matrix techniques and kang triggs mclauchlan hartley et al. hartley and zisserman an alternative formulation is to minimize the error in projected ray directions and szeliski i.e. eba xi xij rj fj where xixij rj fj is given by the second half of this has no particular advantage over in fact since errors are being minimized in ray space there is a bias towards estimating longer focal lengths since the angles between rays become smaller as f increases. however if we eliminate the rays xi we can derive a pairwise energy formulated in ray space and szeliski eall pairs xi xij rj fj xi xik rk computer vision algorithms and applications draft this results in the simplest set of update equations and szeliski since the fk can be folded into the creation of the homogeneous coordinate vector as in equation thus even though this formula over-weights features that occur more frequently it is the method used by shum and szeliski and brown szeliski and winder in order to reduce the bias towards longer focal lengths we multiply each residual error which is similar to projecting the rays into a virtual camera of intermediate focal length. up vector selection. as mentioned above there exists a global ambiguity in the pose of the cameras computed by the above methods. while this may not appear to matter people prefer that the final stitched image is upright rather than twisted or tilted. more concretely people are used to seeing photographs displayed so that the vertical axis points straight up in the image. consider how you usually shoot photographs while you may pan and tilt the camera any which way you usually keep the horizontal edge of your camera x-axis parallel to the ground plane to the world gravity direction. mathematically this constraint on the rotation matrices can be expressed as follows. re call from equation that the to projection is given by xik kkrkxi. we wish to post-multiply each rotation matrix rk by a global rotation rg such that the projection of the global y-axis is perpendicular to the image x-axis this constraint can be written as t rkrg that the scaling by the calibration matrix is irrelevant here. this is equivalent to requiring that the first row of rk t rk be perpendicular to the second column of rg rg this set of constraints per input image can be written as a least squares problem thus is the smallest eigenvector of the scatter or moment matrix spanned by the individual camera rotation x-vectors which should generally be of the form s when the cameras are upright. to fully specify the rg global rotation we need to specify one additional constraint. this is related to the view selection problem discussed in section one simple heuristic is to note that here we use the convention common in computer graphics that the vertical world axis corresponds to y. this is a natural choice if we wish the rotation matrix associated with a regular image taken horizontally to be the identity rather than a rotation around the x-axis. arg min arg min r r r. global alignment t k rk to be close to the world z-axis rg k. we can therefore compute the full rotation matrix rg in three steps prefer the average z-axis of the individual rotation matrices k min eigenvector n where n normalizes a vector v. parallax removal once we have optimized the global orientations and focal lengths of our cameras we may find that the images are still not perfectly aligned i.e. the resulting stitched image looks blurry or ghosted in some places. this can be caused by a variety of factors including unmodeled radial distortion parallax to rotate the camera around its optical center small scene motions such as waving tree branches and large-scale scene motions such as people moving in and out of pictures. each of these problems can be treated with a different approach. radial distortion can be estimated ahead of time using one of the techniques discussed in section for example the plumb-line method kang el-melegy and farag adjusts radial distortion parameters until slightly curved lines become straight while mosaicbased approaches adjust them until mis-registration is reduced in image overlap areas sawhney and kumar parallax can be handled by doing a full bundle adjustment i.e. by replacing the projection equation used in equation with equation which models camera translations. the positions of the matched feature points and cameras can then be simultaneously recovered although this can be significantly more expensive than parallax-free image registration. once the structure has been recovered the scene could theory be projected to a single viewpoint that contains no parallax. however in order to do this dense stereo correspondence needs to be performed shum tang et al. zheng kang cohen et al. which may not be possible if the images contain only partial overlap. in that case it may be necessary to correct for parallax only in the overlap areas which can be accomplished using a multi-perspective plane sweep algorithm szeliski and uyttendaele uyttendaele criminisi kang et al. when the motion in the scene is very large i.e. when objects appear and disappear completely a sensible solution is to simply select pixels from only one image at a time as the source for the final composite davis agarwala dontcheva agrawala computer vision algorithms and applications draft et al. as discussed in section however when the motion is reasonably small the order of a few pixels general motion estimation flow can be used to perform an appropriate correction before blending using a process called local alignment and szeliski kang uyttendaele winder et al. this same process can also be used to compensate for radial distortion and parallax although it uses a weaker motion model than explicitly modeling the source of error and may therefore fail more often or introduce unwanted distortions. the local alignment technique introduced by shum and szeliski starts with the global bundle adjustment used to optimize the camera poses. once these have been estimated the desired location of a point xi can be estimated as the average of the backprojected locations xi cij xi xij rj cij which can be projected into each image j to obtain a target location xij. the difference between the target locations xij and the original features xij provide a set of local motion estimates uij xij xij which can be interpolated to form a dense correction field ujxj. in their system shum and szeliski use an inverse warping algorithm where the sparse uij values are placed at the new target locations xij interpolated using bilinear kernel functions and then added to the original pixel coordinates when computing the warped image. in order to get a reasonably dense set of features to interpolate shum and szeliski place a feature point at the center of each patch patch size controls the smoothness in the local alignment stage rather than relying of features extracted using an interest operator an alternative approach to motion-based de-ghosting was proposed by kang uyttendaele winder et al. who estimate dense optical flow between each input image and a central reference image. the accuracy of the flow vector is checked using a photo-consistency measure before a given warped pixel is considered valid and is used to compute a high dynamic range radiance estimate which is the goal of their overall algorithm. the requirement for a reference image makes their approach less applicable to general image mosaicing although an extension to this case could certainly be envisaged. recognizing panoramas the final piece needed to perform fully automated image stitching is a technique to recognize which images actually go together which brown and lowe call recognizing panora global alignment figure deghosting a mosaic with motion parallax and szeliski ieee composite with parallax after a single deghosting step size after multiple steps and mas. if the user takes images in sequence so that each image overlaps its predecessor and also specifies the first and last images to be stitched bundle adjustment combined with the process of topology inference can be used to automatically assemble a panorama and kumar however users often jump around when taking panoramas e.g. they may start a new row on top of a previous one jump back to take a repeat shot or create panoramas where end-to-end overlaps need to be discovered. furthermore the ability to discover multiple panoramas taken by a user over an extended period of time can be a big convenience. to recognize panoramas brown and lowe first find all pairwise image overlaps using a feature-based method and then find connected components in the overlap graph to recognize individual panoramas the feature-based matching stage first extracts scale invariant feature transform feature locations and feature descriptors from all the input images and places them in an indexing structure as described in section for each image pair under consideration the nearest matching neighbor is found for each feature in the first image using the indexing structure to rapidly find candidates and then comparing feature descriptors to find the best match. ransac is used to find a set of inlier matches pairs of matches are used to hypothesize similarity motion models that are then used to count the number of inliers. more recent ransac algorithm tailored specifically for rotational panoramas is described by brown hartley and nist er in practice the most difficult part of getting a fully automated stitching algorithm to work is deciding which pairs of images actually correspond to the same parts of the scene. repeated structures such as windows can lead to false matches when using a feature-based approach. one way to mitigate this problem is to perform a direct pixelbased comparison between the registered images to determine if they actually are different views of the same scene. unfortunately this heuristic may fail if there are moving objects in the scene while there is no magic bullet for this problem short of full scene understanding further improvements can likely be made by applying domain-specific computer vision algorithms and applications draft figure recognizing panoramas szeliski and winder figures courtesy of matthew brown input images with pairwise matches images grouped into connected components individual panoramas registered and blended into stitched composites. global alignment figure matching errors szeliski and winder accidental matching of several features can lead to matches between pairs of images that do not actually overlap. figure validation of image matches by direct pixel error comparison can fail when the scene contains moving objects eden and szeliski ieee. computer vision algorithms and applications draft heuristics such as priors on typical camera motions as well as machine learning techniques applied to the problem of match validation. direct vs. feature-based alignment given that there exist these two approaches to aligning images which is preferable? early feature-based methods would get confused in regions that were either too textured or not textured enough. the features would often be distributed unevenly over the images thereby failing to match image pairs that should have been aligned. furthermore establishing correspondences relied on simple cross-correlation between patches surrounding the feature points which did not work well when the images were rotated or had foreshortening due to homographies. today feature detection and matching schemes are remarkably robust and can even be used for known object recognition from widely separated views features not only respond to regions of high cornerness orstner harris and stephens but also to blob-like regions and uniform areas chum urban et al. tuytelaars and van gool furthermore because they operate in scale-space and use a dominant orientation orientation invariant descriptors they can match images that differ in scale orientation and even foreshortening. our own experience in working with featurebased approaches is that if the features are well distributed over the image and the descriptors reasonably designed for repeatability enough correspondences to permit image stitching can usually be found szeliski and winder the biggest disadvantage of direct pixel-based alignment techniques is that they have a limited range of convergence. even though they can be used in a hierarchical estimation framework in practice it is hard to use more than two or three levels of a pyramid before important details start to be blurred for matching sequential frames in a video direct approaches can usually be made to work. however for matching partially overlapping images in photo-based panoramas or for image collections where the contrast or content varies too much they fail too often to be useful and feature-based approaches are therefore preferred. compositing once we have registered all of the input images with respect to each other we need to decide how to produce the final stitched mosaic image. this involves selecting a final compositing surface cylindrical spherical etc. and view image. it also involves selecting fourier-based correlation szeliski and shum can extend this range but requires cylindrical images or motion prediction to be useful. compositing which pixels contribute to the final composite and how to optimally blend these pixels to minimize visible seams blur and ghosting. in this section we review techniques that address these problems namely compositing surface parameterization pixel and seam selection blending and exposure compensation. my emphasis is on fully automated approaches to the problem. since the creation of highquality panoramas and composites is as much an artistic endeavor as a computational one various interactive tools have been developed to assist this process dontcheva agrawala et al. li sun tang et al. rother kolmogorov and blake some of these are covered in more detail in section choosing a compositing surface the first choice to be made is how to represent the final image. if only a few images are stitched together a natural approach is to select one of the images as the reference and to then warp all of the other images into its reference coordinate system. the resulting composite is sometimes called a flat panorama since the projection onto the final surface is still a perspective projection and hence straight lines remain straight is often a desirable for larger fields of view however we cannot maintain a flat representation without excessively stretching pixels near the border of the image. practice flat panoramas start to look severely distorted once the field of view exceeds or so. the usual choice for compositing larger panoramas is to use a cylindrical szeliski or spherical and shum projection as described in section in fact any surface used for environment mapping in computer graphics can be used including a cube map which represents the full viewing sphere with the six square faces of a cube szeliski and shum cartographers have also developed a number of alternative methods for representing the globe and snyder the choice of parameterization is somewhat application dependent and involves a tradeoff between keeping the local appearance undistorted keeping straight lines straight and providing a reasonably uniform sampling of the environment. automatically making this selection and smoothly transitioning between representations based on the extent of the panorama is an active area of current research uyttendaele deussen et al. an interesting recent development in panoramic photography has been the use of stereographic projections looking down at the ground an outdoor scene to create little planet recently some techniques have been developed to straighten curved lines in cylindrical and spherical panora mas agrawala and agarwala kopf lischinski deussen et al. these are inspired by the little prince by antoine de saint-exupery. go to httpwww.flickr.com and search computer vision algorithms and applications draft view selection. once we have chosen the output parameterization we still need to determine which part of the scene will be centered in the final view. as mentioned above for a flat composite we can choose one of the images as a reference. often a reasonable choice is the one that is geometrically most central. for example for rotational panoramas represented as a collection of rotation matrices we can choose the image whose z-axis is closest to the average z-axis a reasonable field of view. alternatively we can use the average z-axis quaternion but this is trickier to define the reference rotation matrix. for larger e.g. cylindrical or spherical panoramas we can use the same heuristic if a subset of the viewing sphere has been imaged. in the case of full panoramas a better choice might be to choose the middle image from the sequence of inputs or sometimes the first image assuming this contains the object of greatest interest. in all of these cases having the user control the final view is often highly desirable. if the up vector computation described in section is working correctly this can be as simple as panning over the image or setting a vertical center line for the final panorama. coordinate transformations. after selecting the parameterization and reference view we still need to compute the mappings between the input and output pixels coordinates. if the final compositing surface is flat a single plane or the face of a cube map and the input images have no radial distortion the coordinate transformation is the simple homography described by this kind of warping can be performed in graphics hardware by appropriately setting texture mapping coordinates and rendering a single quadrilateral. if the final composite surface has some other analytic form cylindrical or spherical we need to convert every pixel in the final panorama into a viewing ray point and then map it back into each image according to the projection optionally radial distortion equations. this process can be made more efficient by precomputing some lookup tables e.g. the partial trigonometric functions needed to map cylindrical or spherical coordinates to coordinates or the radial distortion field at each pixel. it is also possible to accelerate this process by computing exact pixel mappings on a coarser grid and then interpolating these values. when the final compositing surface is a texture-mapped polyhedron a slightly more sophisticated algorithm must be used. not only do the and texture map coordinates have to be properly handled but a small amount of overdraw outside the triangle footprints in the texture map is necessary to ensure that the texture pixels being interpolated during rendering have valid values and shum sampling issues. while the above computations can yield the correct pixel addresses in each input image we still need to pay attention to sampling issues. for example for little planet projection compositing if the final panorama has a lower resolution than the input images pre-filtering the input images is necessary to avoid aliasing. these issues have been extensively studied in both the image processing and computer graphics communities. the basic problem is to compute the appropriate pre-filter which depends on the distance arrangement between neighboring samples in a source image. as discussed in sections and various approximate solutions such as mip mapping or elliptically weighted gaussian averaging and heckbert have been developed in the graphics community. for highest visual quality a higher order cubic interpolator combined with a spatially adaptive prefilter may be necessary kang szeliski et al. under certain conditions it may also be possible to produce images with a higher resolution than the input images using the process of super-resolution pixel selection and weighting once the source pixels have been mapped onto the final composite surface we must still decide how to blend them in order to create an attractive-looking panorama. if all of the images are in perfect registration and identically exposed this is an easy problem i.e. any pixel or combination will do. however for real images visible seams to exposure differences blurring to mis-registration or ghosting to moving objects can occur. creating clean pleasing-looking panoramas involves both deciding which pixels to use and how to weight or blend them. the distinction between these two stages is a little fluid since per-pixel weighting can be thought of as a combination of selection and blending. in this section we discuss spatially varying weighting pixel selection placement and then more sophisticated blending. feathering and center-weighting. the simplest way to create a final composite is to simply take an average value at each pixel cx wkx wkx where ikx are the warped images and wkx is at valid pixels and elsewhere. on computer graphics hardware this kind of summation can be performed in an accumulation buffer the a channel as the weight. simple averaging usually does not work very well since exposure differences misregistrations and scene movement are all very visible if rapidly moving objects are the only problem taking a median filter is a kind of pixel selection operator can often be used to remove them and anandan conversely center-weighting below and minimum likelihood selection dontcheva computer vision algorithms and applications draft figure final composites computed by a variety of algorithms average median feathered average p-norm p voronoi weighted rod vertex cover with feathering graph cut seams with poisson blending and with pyramid blending. compositing agrawala et al. can sometimes be used to retain multiple copies of a moving object a better approach to averaging is to weight pixels near the center of the image more heavily and to down-weight pixels near the edges. when an image has some cutout regions down-weighting pixels near the edges of both cutouts and the image is preferable. this can be done by computing a distance map or grassfire transform wkx arg min y ikx y is invalid where each valid pixel is tagged with its euclidean distance to the nearest invalid pixel the euclidean distance map can be efficiently computed using a two-pass raster algorithm borgefors weighted averaging with a distance map is often called feathering and shum chen and klette uyttendaele eden and szeliski and does a reasonable job of blending over exposure differences. however blurring and ghosting can still be problems note that weighted averaging is not the same as compositing the individual images with the classic over operation and duff blinn even when using the weight values to sum up to one as alpha channels. this is because the over operation attenuates the values from more distant surfaces and hence is not equivalent to a direct sum. one way to improve feathering is to raise the distance map values to some large power i.e. to use wp kx in equation the weighted averages then become dominated by the larger values i.e. they act somewhat like a p-norm. the resulting composite can often provide a reasonable tradeoff between visible exposure differences and blur in the limit as p only the pixel with the maximum weight is selected where cx ilxx l arg max k wkx is the label assignment or pixel selection function that selects which image to use at each pixel. this hard pixel selection process produces a visibility mask-sensitive variant of the familiar voronoi diagram which assigns each pixel to the nearest image center in the set finkelstein hughes et al. peleg rousso rav-acha et al. the resulting composite while useful for artistic guidance and in high-overlap panoramas mosaics tends to have very hard edges with noticeable seams when the exposures vary xiong and turkowski use this voronoi idea maximum of the grassfire transform to select seams for laplacian pyramid blending is discussed below. however computer vision algorithms and applications draft figure computation of regions of difference eden and szeliski ieee three overlapping images with a moving face corresponding rods graph of coincident rods. since the seam selection is performed sequentially as new images are added in some artifacts can occur. optimal seam selection. computing the voronoi diagram is one way to select the seams between regions where different images contribute to the final composite. however voronoi images totally ignore the local image structure underlying the seam. a better approach is to place the seams in regions where the images agree so that transitions from one source to another are not visible. in this way the algorithm avoids cutting through moving objects where a seam would look unnatural for a pair of images this process can be formulated as a simple dynamic program starting from one edge of the overlap region and ending at the other davis efros and freeman when multiple images are being composited the dynamic program idea does not readily generalize. square texture tiles being composited sequentially efros and freeman run a dynamic program along each of the four tile sides. to overcome this problem uyttendaele eden and szeliski observed that for well-registered images moving objects produce the most visible artifacts namely translucent looking ghosts. their system therefore decides which objects to keep and which ones to erase. first the algorithm compares all overlapping input image pairs to determine regions of difference where the images disagree. next a graph is constructed with the rods as vertices and edges representing rod pairs that overlap in the final composite since the presence of an edge indicates an area of disagreement vertices must be removed from the final composite until no edge spans a pair of remaining vertices. the smallest such set can be computed using a vertex cover algorithm. since several such covers may exist a weighted vertex cover is used instead where the vertex weights are computed by summing the feather weights in the rod eden and szeliski the algorithm therefore prefers removing regions that are near the edge of the image which reduces the likelihood that partially visible objects will appear in the final composite. is compositing figure photomontage dontcheva agrawala et al. acm. from a set of five source images which four are shown on the left photomontage quickly creates a composite family portrait in which everyone is smiling and looking at the camera users simply flip through the stack and coarsely draw strokes using the designated source image objective over the people they wish to add to the composite. the user-applied strokes and computed regions are color-coded by the borders of the source images on the left. also possible to infer which object in a region of difference is the foreground object by the edginess differences across the rod boundary which should be higher when an object is present once the desired excess regions of difference have been removed the final composite can be created by feathering a different approach to pixel selection and seam placement is described by agarwala dontcheva agrawala et al. their system computes the label assignment that optimizes the sum of two objective functions. the first is a per-pixel image objective that determines which pixels are likely to produce good composites cd dx lx where dx l is the data penalty associated with choosing image l at pixel x. in their system users can select which pixels to use by painting over an image with the desired object or appearance which sets dx l to a large value for all labels l other than the one selected by the user alternatively automated selection criteria can be used such as maximum likelihood which prefers pixels that occur repeatedly in the background object removal or minimum likelihood for objects that occur infrequently i.e. for moving object retention. using a more traditional center-weighted data term tends to favor objects that are centered in the input images the second term is a seam objective that penalizes differences in labelings between adja cent images cs n sx y lx ly computer vision algorithms and applications draft figure set of five photos tracking a snowboarder s jump stitched together into a seamless composite. because the algorithm prefers pixels near the center of the image multiple copies of the boarder are retained. where sx y lx ly is the image-dependent interaction penalty or seam cost of placing a seam between pixels x and y and n is the set of neighboring pixels. for example the simple color-based seam penalty used in sch odl essa et al. agarwala dontcheva agrawala et al. can be written as sx y lx ly ilxx ilxy more sophisticated seam penalties can also look at image gradients or the presence of image edges dontcheva agrawala et al. seam penalties are widely used in other computer vision applications such as stereo matching veksler and zabih to give the labeling function its coherence or smoothness. an alternative approach which places seams along strong consistent edges in overlapping images using a watershed computation is described by soille the sum of these two objective functions gives rise to a markov random field for which good optimization algorithms are described in sections and and appendix for label computations of this kind the algorithm developed by boykov veksler and zabih works particularly well zabih scharstein et al. for the result shown in figure agarwala dontcheva agrawala et al. use a large data penalty for invalid pixels and for valid pixels. notice how the seam placement algorithm avoids regions of difference including those that border the image and that might result in objects being cut off. graph cuts dontcheva agrawala et al. and compositing vertex cover eden and szeliski often produce similar looking results although the former is significantly slower since it optimizes over all pixels while the latter is more sensitive to the thresholds used to determine regions of difference. application photomontage while image stitching is normally used to composite partially overlapping photographs it can also be used to composite repeated shots of a scene taken with the aim of obtaining the best possible composition and appearance of each element. figure shows the photomontage system developed by agarwala dontcheva agrawala et al. where users draw strokes over a set of pre-aligned images to indicate which regions they wish to keep from each image. once the system solves the resulting multi-label graph cut the various pieces taken from each source photo are blended together using a variant of poisson image blending their system can also be used to automatically composite an all-focus image from a series of bracketed focus images kutulakos durand et al. or to remove wires and other unwanted elements from sets of photographs. exercise has you implement this system and try out some of its variants. blending once the seams between images have been determined and unwanted objects removed we still need to blend the images to compensate for exposure differences and other mis-alignments. the spatially varying weighting previously discussed can often be used to accomplish this. however it is difficult in practice to achieve a pleasing balance between smoothing out low-frequency exposure variations and retaining sharp enough transitions to prevent blurring using a high exponent in feathering can help. laplacian pyramid blending. an attractive solution to this problem is the laplacian pyramid blending technique developed by burt and adelson which we discussed in section instead of using a single transition width a frequency-adaptive width is used by creating a band-pass pyramid and making the transition widths within each level a function of the level i.e. the same width in pixels. in practice a small number of levels i.e. as few as two and lowe may be adequate to compensate for differences in exposure. the result of applying this pyramid blending is shown in figure gradient domain blending. an alternative approach to multi-band image blending is to perform the operations in the gradient domain. reconstructing images from their gradient fields has a long history in computer vision starting originally with work in computer vision algorithms and applications draft figure poisson image editing erez gangnet and blake acm the dog and the two children are chosen as source images to be pasted into the destination swimming pool. simple pasting fails to match the colors at the boundaries whereas poisson image blending masks these differences. brightness constancy shape from shading and brooks and photometric stereo more recently related ideas have been used for reconstructing images from their edges and goldberg removing shadows from images separating reflections from a single image zomet and weiss levin and weiss and tone mapping high dynamic range images by reducing the magnitude of image edges lischinski and werman p erez gangnet and blake show how gradient domain reconstruction can be used to do seamless object insertion in image editing applications rather than copying pixels the gradients of the new image fragment are copied instead. the actual pixel values for the copied area are then computed by solving a poisson equation that locally matches the gradients while obeying the fixed dirichlet matching conditions at the seam boundary. p erez gangnet and blake show that this is equivalent to computing an additive membrane interpolant of the mismatch between the source and destination images along the in earlier work peleg also proposed adding a smooth function to enforce consistency along the seam curve. agarwala dontcheva agrawala et al. extended this idea to a multi-source formulation where it no longer makes sense to talk of a destination image whose exact pixel values must be matched at the seam. instead each source image contributes its own gradient field and the poisson equation is solved using neumann boundary conditions i.e. dropping any equations that involve pixels outside the boundary of the image. the membrane interpolant is known to have nicer interpolation properties for arbitrary-shaped constraints than frequency-domain interpolants compositing rather than solving the poisson partial differential equations agarwala dontcheva agrawala et al. directly minimize a variational problem cx min the discretized form of this equation is a set of gradient constraint equations cx cx ilxx ilxx and cx cx ilxx ilxx where and are unit vectors in the x and y they then solve the associated sparse least squares problem. since this system of equations is only defined up to an additive constraint agarwala dontcheva agrawala et al. ask the user to select the value of one pixel. in practice a better choice might be to weakly bias the solution towards reproducing the original color values. in order to accelerate the solution of this sparse linear system fattal lischinski and werman use multigrid whereas agarwala dontcheva agrawala et al. use hierarchical basis preconditioned conjugate gradient descent in subsequent work agarwala shows how using a quadtree representation for the solution can further accelerate the computation with minimal loss in accuracy while szeliski uyttendaele and steedly show how representing the per-image offset fields using even coarser splines is even faster. this latter work also argues that blending in the log domain i.e. using multiplicative rather than additive offsets is preferable as it more closely matches texture contrasts across seam boundaries. the resulting seam blending works very well in practice although care must be taken when copying large gradient values near seams so that a double edge is not introduced. copying gradients directly from the source images after seam placement is just one approach to gradient domain blending. the paper by levin zomet peleg et al. examines several different variants of this approach which they call gradient-domain image stitching the techniques they examine include feathering the gradients from the source images as well as using an norm in performing the reconstruction of the image from the gradient field rather than using an norm as in equation their preferred technique is the optimization of a feathered cost function on the original image gradients they call since optimization using linear programming can be slow they develop a faster iterative median-based algorithm in a multigrid framework. visual comparisons between their preferred approach and what they call optimal seam on the gradients is equivalent to the approach of agarwala dontcheva agrawala et al. show similar results while significantly improving on pyramid blending and feathering algorithms. at seam locations the right hand side is replaced by the average of the gradients in the two source images. computer vision algorithms and applications draft exposure compensation. pyramid and gradient domain blending can do a good job of compensating for moderate amounts of exposure differences between images. however when the exposure differences become large alternative approaches may be necessary. uyttendaele eden and szeliski iteratively estimate a local correction between each source image and a blended composite. first a block-based quadratic transfer function is fit between each source image and an initial feathered composite. next transfer functions are averaged with their neighbors to get a smoother mapping and per-pixel transfer functions are computed by splining between neighboring block values. once each source image has been smoothly adjusted a new feathered composite is computed and the process is repeated three times. the results shown by uyttendaele eden and szeliski demonstrate that this does a better job of exposure compensation than simple feathering and can handle local variations in exposure due to effects such as lens vignetting. ultimately however the most principled way to deal with exposure differences is to stitch images in the radiance domain i.e. to convert each image into a radiance image using its exposure value and then create a stitched high dynamic range image as discussed in section uyttendaele and szeliski additional reading the literature on image stitching dates back to work in the photogrammetry community in the slama in computer vision papers started appearing in the early while the development of fully automated techniques came about a decade later and picard chen szeliski szeliski and shum sawhney and kumar shum and szeliski those techniques used direct pixel-based alignment but feature-based approaches are now the norm faugeras and deriche capel and zisserman cham and cipolla badra qumsieh and dudek mclauchlan and jaenicke brown and lowe a collection of some of these papers can be found in the book by benosman and kang szeliski provides a comprehensive survey of image stitching on which the material in this chapter is based. high-quality techniques for optimal seam finding and blending are another important component of image stitching systems. important developments in this field include work by milgram burt and adelson davis uyttendaele eden and szeliski erez gangnet and blake levin zomet peleg et al. agarwala dontcheva agrawala et al. eden uyttendaele and szeliski and kopf uyttendaele deussen et al. in addition to the merging of multiple overlapping photographs taken for aerial or ter exercises restrial panoramic image creation stitching techniques can be used for automated whiteboard scanning and zhang zhang and he scanning with a mouse kashitani and kaneyoshi and retinal image mosaics stewart roysam et al. they can also be applied to video sequences and bender irani hsu and anandan kumar anandan irani et al. sawhney and ayer massey and bender irani and anandan sawhney arpa kumar et al. agarwala zheng pal et al. rav-acha pritch lischinski et al. steedly pal and szeliski baudisch tan steedly et al. and can even be used for video compression ge chen lung bruce lin et al. exercises ex direct pixel-based alignment take a pair of images compute a coarse-to-fine affine alignment and then blend them using either averaging or a laplacian pyramid extend your motion model from affine to perspective to better deal with rotational mosaics and planar surfaces seen under arbitrary motion. ex featured-based stitching extend your feature-based alignment technique from exercise to use a full perspective model and then blend the resulting mosaic using either averaging or more sophisticated distance-based feathering ex cylindrical strip panoramas to generate cylindrical or spherical panoramas from a horizontally panning camera it is best to use a tripod. set your camera up to take a series of overlapped photos and then use the following steps to create your panorama estimate the amount of radial distortion by taking some pictures with lots of long straight lines near the edges of the image and then using the plumb-line method from exercise compute the focal length either by using a ruler and paper as in figure wenger tchou et al. or by rotating your camera on the tripod overlapping the images by exactly and counting the number of images it takes to make a panorama. convert each of your images to cylindrical coordinates using line up the images with a translational motion model using either a direct pixel-based technique such as coarse-to-fine incremental or an fft or a feature-based technique. if doing a complete panorama align the first and last images. compute the amount of accumulated vertical mis-registration and re-distribute this among the images. computer vision algorithms and applications draft blend the resulting images using feathering or some other technique. ex coarse alignment use fft or phase correlation to estimate the initial alignment between successive images. how well does this work? over what range of overlaps? if it does not work does aligning sub-sections quarters do better? ex automated mosaicing use feature-based alignment with four-point ransac for homographies equations or three-point ransac for rotational motions hartley and nist er to match up all pairs of overlapping images. merge these pairwise estimates together by finding a spanning tree of pairwise relations. visualize the resulting global alignment e.g. by displaying a blend of each image with all other images that overlap it. for greater robustness try multiple spanning trees randomly sampled based on the confidence in pairwise alignments to see if you can recover from bad pairwise matches klopschitz and pollefeys as a measure of fitness count how many pairwise estimates are consistent with the global alignment. ex global optimization use the initialization from the previous algorithm to perform a full bundle adjustment over all of the camera rotations and focal lengths as described in section and by shum and szeliski optionally estimate radial distortion parameters as well or support fisheye lenses as in the previous exercise visualize the quality of your registration by creating composites of each input image with its neighbors optionally blinking between the original image and the composite to better see mis-alignment artifacts. ex de-ghosting use the results of the previous bundle adjustment to predict the location of each feature in a consensus geometry. use the difference between the predicted and actual feature locations to correct for small mis-registrations as described in section and szeliski ex compositing surface choose a compositing surface e.g. a single reference image extended to a larger plane a sphere represented using cylindrical or spherical coordinates a stereographic little planet projection or a cube map. project all of your images onto this surface and blend them with equal weighting for now to see where the original image seams are. ex feathering and blending compute a feather map for each warped source image and use these maps to blend the warped images. alternatively use laplacian pyramid blending or gradient domain blend ing. exercises ex photomontage and object removal users can indicate desired or unwanted regions in pre-registered images using strokes or other primitives as bounding boxes. implement a photomontage system in which devise an automatic moving objects remover keeper by analyzing which inconsistent regions are more or less typical given some consensus median filtering of the aligned images. figure shows an example where the moving object was kept. try to make this work for sequences with large amounts of overlaps and consider averaging the images to make the moving object look more ghosted. computer vision algorithms and applications draft chapter computational photography tone mapping application flash photography color image demosaicing application colorization photometric calibration super-resolution and blur removal high dynamic range imaging radiometric response function noise level estimation vignetting optical blur response estimation application hole filling and inpainting application non-photorealistic rendering image matting and compositing blue screen matting natural image matting optimization-based matting smoke shadow and flash matting video matting texture analysis and synthesis additional reading exercises computer vision algorithms and applications draft figure computational photography merging multiple exposures to create high dynamic range images and malik acm merging flash and nonflash photographs agrawala hoppe et al. acm image matting and compositing curless salesin et al. ieee hole filling with inpainting p erez and toyama ieee. computational photography stitching multiple images into wide field of view panoramas which we covered in chapter allows us create photographs that could not be captured with a regular camera. this is just one instance of computational photography where image analysis and processing algorithms are applied to one or more photographs to create images that go beyond the capabilities of traditional imaging systems. some of these techniques are now being incorporated directly into digital still cameras. for example some of the newer digital still cameras have sweep panorama modes and take multiple shots in low-light conditions to reduce image noise. in this chapter we cover a number of additional computational photography algorithms. we begin with a review of photometric image calibration i.e. the measurement of camera and lens responses which is a prerequisite for many of the algorithms we describe later. we then discuss high dynamic range imaging which captures the full range of brightness in a scene through the use of multiple exposures we also discuss tone mapping operators which map rich images back into regular display devices such as screens and printers as well as algorithms that merge flash and regular images to obtain better exposures next we discuss how the resolution of images can be improved either by merging multiple photographs together or using sophisticated image priors this includes algorithms for extracting full-color images from the patterned bayer mosaics present in most cameras. in section we discuss algorithms for cutting pieces of images from one photograph and pasting them into others in section we describe how to generate novel textures from real-world samples for applications such as filling holes in images we close with a brief overview of non-photorealistic rendering which can turn regular photographs into artistic renderings that resemble traditional drawings and paintings. one topic that we do not cover extensively in this book is novel computational sensors optics and cameras. a nice survey can be found in an article by nayar a recently published book by raskar and tumblin and more recent research papers fergus durand et al. some related discussion can also be found in sections and a good general-audience introduction to computational photography can be found in the article by hayes as well as survey papers by nayar cohen and szeliski levoy and debevec raskar and tumblin give extensive coverage of topics in this area with particular emphasis on computational cameras and sensors. the sub-field of high dynamic range imaging has its own book discussing research in this area ward pattanaik et al. as well as a wonderful book aimed more at profes see also the two special issue journals edited by bimber and durand and szeliski computer vision algorithms and applications draft sional photographers a good survey of image matting is provided by wang and cohen there are also several courses on computational photography where the instructors have provided extensive on-line materials e.g. fr edo durand s computation photography course at alyosha efros class at carnegie marc levoy s class at and a series of siggraph courses on computational photometric calibration before we can successfully merge multiple photographs we need to characterize the functions that map incoming irradiance into pixel values and also the amounts of noise present in each image. in this section we examine three components of the imaging pipeline that affect this mapping. the first is the radiometric response function and nayar which maps photons arriving at the lens into digital values stored in the image file the second is vignetting which darkens pixel values near the periphery of images especially at large apertures the third is the point spread function which characterizes the blur induced by the lens anti-aliasing filters and finite sensor areas the material in this section builds on the image formation processes described in sections and so if it has been a while since you looked at those sections please go back and review them. radiometric response function as we can see in figure a number of factors affect how the intensity of light arriving at the lens ends up being mapped into stored digital values. let us ignore for now any nonuniform attenuation that may occur inside the lens which we cover in section the first factors to affect this mapping are the aperture and shutter speed which can be modeled as global multipliers on the incoming light most conveniently measured in exposure values brightness ratios. next the analog to digital converter on the sensing chip applies an electronic gain usually controlled by the iso setting on your camera. while in theory this gain is linear as with any electronics non-linearities may be gulbins and gulbins discuss related photographic techniques. mit cmu stanford cs httpweb.media.mit.edu raskarphoto. additional photometric camera and lens effects include sensor glare blooming and chromatic aberration which can also be thought of as a spectrally varying form of geometric aberration photometric calibration figure image sensing pipeline block diagram showing the various sources of noise as well as the typical digital post-processing steps equivalent signal transforms including convolution gain and noise injection. the abbreviations are rd radial distortion aa anti-aliasing filter cfa color filter array and quantization noise. sceneradiancedsprawsensor chipcamera bodyopticsaperturesensorccdcmosadcdemosaicsharpenwhite balancegammacurvecompressshuttergainisojpegadcrawdspsensor chipcamera bodyscene radianceopticsaperturesensorccdcmosdemosaicsharpenwhite balancegammacurvecompressshuttergainisojpegblur kern. rdf-stop vignetteexposure taa cfanoiseiso s-curve computer vision algorithms and applications draft figure radiometric response calibration typical camera response function showing the mapping between incoming log irradiance and output eight-bit pixel values for one color channel and malik acm color checker chart. present unintentionally or by design. ignoring for now photon noise on-chip noise amplifier noise and quantization noise which we discuss shortly you can often assume that the mapping between incoming light and the values stored in a raw camera file your camera supports this is roughly linear. if images are being stored in the more common jpeg format the camera s digital signal processor next performs bayer pattern demosaicing and which is a mostly linear often non-stationary process. some sharpening is also often applied at this stage. next the color values are multiplied by different constants sometimes a color twist matrix to perform color balancing i.e. to move the white point closer to pure white. finally a standard gamma is applied to the intensities in each color channel and the colors are converted into ycbcr format before being transformed by a dct quantized and then compressed into the jpeg format figure shows all of these steps in pictorial form. given the complexity of all of this processing it is difficult to model the camera response function i.e. the mapping between incoming irradiance and digital rgb values from first principles. a more practical approach is to calibrate the camera by measuring correspondences between incoming light and final values. the most accurate but most expensive approach is to use an integrating sphere which is a large diameter sphere carefully painted on the inside with white matte paint. an accurately calibrated light at the top controls the amount of radiance inside the sphere is constant everywhere because of the sphere s radiometry and a small opening at the side allows for a cameralens combination to be mounted. by slowly varying the current going into the light an accurate correspondence can be established between incoming radiance and photometric calibration measured pixel values. the vignetting and noise characteristics of the camera can also be simultaneously determined. a more practical alternative is to use a calibration chart such as the macbeth or munsell colorchecker the biggest problem with this approach is to ensure uniform lighting. one approach is to use a large dark room with a high-quality light source far away from perpendicular to the chart. another is to place the chart outdoors away from any shadows. results will differ under these two conditions because the color of the illuminant will be different. the easiest approach is probably to take multiple exposures of the same scene while the camera is on a tripod and to recover the response function by simultaneously estimating the incoming irradiance at each pixel and the response curve and picard debevec and malik mitsunaga and nayar this approach is discussed in more detail in section on high dynamic range imaging. if all else fails i.e. you just have one or more unrelated photos you can use an international color consortium profile for the camera even more simply you can just assume that the response is linear if they are raw files and that the images have a non-linearity clipping applied to each rgb channel if they are jpeg images. noise level estimation in addition to knowing the camera response function it is also often important to know the amount of noise being injected under a particular camera setting isogain level. the simplest characterization of noise is a single standard deviation usually measured in gray levels independent of pixel value. a more accurate model can be obtained by estimating the noise level as a function of pixel value which is known as the noise level function szeliski kang et al. as with the camera response function the simplest way to estimate these quantities is in the lab using either an integrating sphere or a calibration chart. the noise can be estimated either at each pixel independently by taking repeated exposures and computing the temporal variance in the measurements and kondepudy or over regions by assuming that pixel values should all be the same within some region inside a color checker square and computing a spatial variance. this approach can be generalized to photos where there are regions of constant or slowly varying intensity szeliski kang et al. first segment the image into such regions and fit a constant or linear function inside each region. next measure the standard deviation of the differences between the noisy input pixels and the smooth fitted function httpwww.xrite.com. see also the icc information on profiles httpwww.color.orginfo computer vision algorithms and applications draft figure noise level function estimates obtained from a single color photograph szeliski kang et al. ieee. the colored curves are the estimated nlf fit as the probabilistic lower envelope of the measured deviations between the noisy piecewise-smooth images. the ground truth nlfs obtained by averaging images are shown in gray. away from large gradients and region boundaries. plot these as a function of output level for each color channel as shown in figure finally fit a lower envelope to this distribution in order to ignore pixels or deviations that are outliers. a fully bayesian approach to this problem that models the statistical distribution of each quantity is presented by szeliski kang et al. a simpler approach which should produce useful results in most cases is to fit a low-dimensional function positive valued b-spline to the lower envelope exercise in more recent work matsushita and lin present a technique for simultaneously estimating a camera s response and noise level functions based on skew in level-dependent noise distributions. their paper also contains extensive references to previous work in these areas. vignetting a common problem with using wide-angle and wide-aperture lenses is that the image tends to darken in the corners this problem is generally known as vignetting and comes in several different forms including natural optical and mechanical vignetting as with radiometric response function calibration the most accurate way to calibrate vignetting is to use an integrating sphere or a picture of a uniformly colored and illuminated blank wall. an alternative approach is to stitch a panoramic scene and to assume that the true radiance at each pixel comes from the central portion of each input image. this is easier to do if the radiometric response function is already known by shooting in raw mode and if the exposure is kept constant. if the response function image exposures and vignetting function are unknown they can still be recovered by optimizing a large least squares fitting photometric calibration figure single image vignetting correction yu kang et al. ieee original image with strong visible vignetting vignetting compensation as described by zheng zhou georgescu et al. d vignetting compensation as described by zheng yu kang et al. figure simultaneous estimation of vignetting exposure and radiometric response ieee original average of the input images after compensating for vignetting using gradient domain blending only the remaining mottled look after both vignetting compensation and blending. problem and schechner goldman figure shows an example of simultaneously estimating the vignetting exposure and radiometric response function from a set of overlapping photographs note that unless vignetting is modeled and compensated regular gradient-domain image blending will not create an attractive image. if only a single image is available vignetting can be estimated by looking for slow consistent intensity variations in the radial direction. the original algorithm proposed by zheng lin and kang first pre-segmented the image into smoothly varying regions and then performed an analysis inside each region. instead of pre-segmenting the image zheng yu kang et al. compute the radial gradients at all the pixels and use the asymmetry in this distribution gradients away from the center are on average slightly negative to computer vision algorithms and applications draft estimate the vignetting. figure shows the results of applying each of these algorithms to an image with a large amount of vignetting. exercise has you implement some of the above techniques. optical blur response estimation one final characteristic of imaging systems that you should calibrate is the spatial response function which encodes the optical blur that gets convolved with the incoming image to produce the point-sampled image. the shape of the convolution kernel which is also known as point spread function or optical transfer function depends on several factors including lens blur and radial distortion anti-aliasing filters in front of the sensor and the shape and extent of each active pixel area a good estimate of this function is required for applications such as multi-image super-resolution and de-blurring in theory one could estimate the psf by simply observing an infinitely small point light source everywhere in the image. creating an array of samples by drilling through a dark plate and backlighting with a very bright light source is difficult in practice. a more practical approach is to observe an image composed of long straight lines or bars since these can be fitted to arbitrary precision. because the location of a horizontal or vertical edge can be aliased during acquisition slightly slanted edges are preferred. the profile and locations of such edges can be estimated to sub-pixel precision which makes it possible to estimate the psf at sub-pixel resolutions park and narayanswamy burns and williams williams and burns goesele fuchs and seidel the thesis by murphy contains a nice survey of all aspects of camera calibration including the spatial frequency response spatial uniformity tone reproduction color reproduction noise dynamic range color channel registration and depth of field. it also includes a description of a slant-edge calibration algorithm called the slant-edge technique can be used to recover a projection of the psf e.g. slightly vertical edges are used to recover the horizontal line spread function the lsf is then often converted into the fourier domain and its magnitude plotted as a one-dimensional modulation transfer function which indicates which image frequencies are lost and aliased during the acquisition process for most computational photography applications it is preferable to directly estimate the full psf since it can be hard to recover from its projections figure shows a pattern containing edges at all orientations which can be used to directly recover a two-dimensional psf. first corners in the pattern are located by extracting edges in the sensed image linking them and finding the intersections of the circular arcs. next the ideal pattern whose analytic form is known is warped a homography to photometric calibration figure calibration pattern with edges equally distributed at all orientations that can be used for psf and radial distortion estimation szeliski and kriegman ieee. a portion of an actual sensed image is shown in the middle and a close-up of the ideal pattern is on the right. fit the central portion of the input image and its intensities are adjusted to fit the ones in the sensed image. if desired the pattern can be rendered at a higher resolution than the input image which enables the estimation of the psf to sub-pixel resolution finally a large linear least squares system is solved to recover the unknown psf kernel k k arg min k di where b is the sensed image i is the predicted image and d is an optional downsampling operator that matches the resolution of the ideal and sensed images szeliski and kriegman in terms of the notation introduced in section this could also be written as b arg min b ds where o is the observed image s is the sharp image and b is the blur kernel. if the process of estimating the psf is done locally in overlapping patches of the image it can also be used to estimate the radial distortion and chromatic aberration induced by the lens because the homography mapping the ideal target to the sensed image is estimated in the central part of the image any shifts induced by the optics manifest themselves as a displacement in the psf compensating for these shifts eliminates both the achromatic radial distortion and the inter-channel shifts that result in visible chromatic aberration. the color-dependent blurring caused by chromatic aberration can also be removed using the de-blurring techniques discussed in this process confounds the distinction between geometric and photometric calibration. in principle any geometric distortion could be modeled by spatially varying displaced psfs. in practice it is easier to fold any large shifts into the geometric correction component. computer vision algorithms and applications draft figure point spread function estimation using a calibration target szeliski and kriegman ieee. sub-pixel psfs at successively higher resolutions the interaction between the square sensing area and the circular lens blur. the radial distortion and chromatic aberration can also be estimated and removed. psf for a misfocused lens showing some diffraction and vignetting effects in the corners. section figure shows how the radial distortion and chromatic aberration manifest themselves as elongated and displaced psfs along with the result of removing these effects in a region of the calibration target. the local psf estimation technique can also be used to estimate vignetting. figure shows how the mechanical vignetting manifests itself as clipping of the psf in the corners of the image. in order for the overall dimming associated with vignetting to be properly captured the modified intensities of the ideal pattern need to be extrapolated from the center which is best done with a uniformly illuminated target. when working with raw bayer-pattern images the correct way to estimate the psf is to only evaluate the least squares terms in at sensed pixel values while interpolating the ideal image to all values. for jpeg images you should linearize your intensities first e.g. remove the gamma and any other non-linearities in your estimated radiometric response function. what if you have an image that was taken with an uncalibrated camera? can you still recover the psf an use it to correct the image? in fact with a slight modification the previous algorithms still work. instead of assuming a known calibration image you can detect strong elongated edges and fit ideal step edges in such regions resulting in the sharp image shown high dynamic range imaging figure estimating the psf without using a calibration pattern szeliski and kriegman ieee input image with blue cross-section location profile of sensed and predicted step edges d locations and values of the predicted colors near the edge locations. in figure for every pixel that is surrounded by a complete set of valid estimated neighbors pixels in figure apply the least squares formula to estimate the kernel k. the resulting locally estimated psfs can be used to correct for chromatic aberration the relative displacements between per-channel psfs can be computed as shown by joshi szeliski and kriegman exercise provides some more detailed instructions for implementing and testing edge-based psf estimation algorithms. an alternative approach which does not require the explicit detection of edges but uses image statistics distributions instead is presented by fergus singh hertzmann et al. high dynamic range imaging as we mentioned earlier in this chapter registered images taken at different exposures can be used to calibrate the radiometric response function of a camera. more importantly they can help you create well-exposed photographs under challenging conditions such as brightly lit minmaxrrvalid regionminmaxrrvalid regionminmaxrrvalid regionminmaxrrvalid region computer vision algorithms and applications draft figure sample indoor image where the areas outside the window are overexposed and inside the room are too dark. figure relative brightness of different scenes ranging from inside a dark room lit by a monitor to looking at the sun. photos courtesy of paul debevec. scenes where any single exposure contains saturated and dark regions this problem is quite common because the natural world contains a range of radiance values that is far greater than can be captured with any photographic sensor or film taking a set of bracketed exposures taken by a camera in automatic exposure bracketing mode to deliberately under- and over-expose the image gives you the material from which to create a properly exposed photograph as shown in figure ward pattanaik et al. freeman gulbins and gulbins hasinoff durand and freeman while it is possible to combine pixels from different exposures directly into a final com figure a bracketed set of shots the camera s automatic exposure bracketing mode and the resulting high dynamic range composite. high dynamic range imaging posite and kolczynski mertens kautz and reeth this approach runs the risk of creating contrast reversals and halos. instead the more common approach is to proceed in three stages estimate the radiometric response function from the aligned images. estimate a radiance map by selecting or blending pixels from different exposures. tone map the resulting high dynamic range image back into a displayable gamut. the idea behind estimating the radiometric response function is relatively straightforward and picard debevec and malik mitsunaga and nayar reinhard ward pattanaik et al. suppose you take three sets of images at different exposures speeds say at exposure if we were able to determine the irradiance ei at each pixel we could plot it against the measured pixel value zij for each exposure time tj as shown in figure unfortunately we do not know the irradiance values ei so these have to be estimated at the same time as the radiometric response function f which can be written and malik as zij fei tj where tj is the exposure time for the jth image. the inverse response curve f is given by f ei tj. taking logarithms of both sides is convenient as we can now measure quantities in evs we obtain gzij log f log ei log tj where g log f maps pixel values zij into log irradiance is the curve we are estimating turned on its side. debevec and malik assume that the exposure times tj are known. that these can be obtained from a camera s exif tags but that they actually follow a power of progression instead of the marked values see exercise the unknowns are therefore the per-pixel exposures ei and the response values gk gk where g can be discretized according to the pixel values commonly observed in eight-bit images. response curves are calibrated separately for each color channel. changing the shutter speed is preferable to changing the aperture as the latter can modify the vignetting and focus. using f-stops exposure values or evs since f-stops refer to apertures is usually the right compromise between capturing a good dynamic range and having properly exposed pixels everywhere. computer vision algorithms and applications draft figure radiometric calibration using multiple exposures and malik corresponding pixel values are plotted as functions of log exposures the curves on the left are shifted to account for each pixel s unknown radiance until they all line up into a single smooth curve. in order to make the response curve smooth debevec and malik add a second order smoothness constraint gk which is similar to the one used in snakes since pixel values are more reliable in the middle of their range the g function becomes singular near saturation values they also add a weighting function wk that decays to zero at both ends of the pixel value range wz z zmin zmax z z z putting all of these terms together they obtain a least squares problem in the unknowns and e wzijgzij log ei log order to remove the overall shift ambiguity in the response curve and irradiance values the middle of the response curve is set to debevec and malik show how this can be implemented in lines of matlab code which partially accounts for the popularity of their technique. while debevec and malik assume that the exposure times tj are known exactly there is no reason why these additional variables cannot be thrown into the least squares problem constraining their final estimated values to lie close to their nominal values tj with an extra term log exposurepixel exposurepixel value high dynamic range imaging figure recovered response function and radiance image for a real digital camera and malik acm. figure shows the recovered radiometric response function for a digital camera along with select radiance values in the overall radiance map. figure shows the bracketed input images captured on color film and the corresponding radiance map. while debevec and malik use a general second-order smooth curve g to parame terize their response curve mann and picard use a three-parameter function fe e while mitsunaga and nayar use a low-order polynomial for the inverse response function g. pal szeliski uyttendaele et al. derive a bayesian model that estimates an independent smooth response function for each image which can better model the more sophisticated hence less predictable automatic contrast and tone adjustment performed in today s digital cameras. once the response function has been estimated the second step in creating high dynamic range photographs is to merge the input images into a composite radiance map. if the response function and images were known exactly i.e. if they were noise free you could use any non-saturated pixel value to estimate the corresponding radiance by mapping it through the inverse response curve e gz. unfortunately pixels are noisy especially under low-light conditions when fewer photons arrive at the sensor. to compensate for this mann and picard use the derivative of the response function as a weight in determining the final radiance estimate since flatter regions of the curve tell us less about the incoming irradiance. debevec and malik use a hat function which accentuates mid-tone pixels while avoiding saturated values. mitsunaga and nayar show that in order to maximize the signal-to-noise ratio computer vision algorithms and applications draft figure bracketed set of exposures captured with a film camera and the resulting radiance image displayed in pseudocolor and malik acm. figure merging multiple exposures to create a high dynamic range composite uyttendaele winder et al. c three different exposures merging the exposures using classic algorithms the ghosting due to the horse s head movement merging the exposures with motion compensation. high dynamic range imaging figure hdr merging with large amounts of motion uyttendaele and szeliski ieee registered bracketed input images results after the first pass of image selection reference labels image and tone-mapped image results after the second pass of image selection final labels compressed hdr image and tone-mapped image the weighting function must emphasize both higher pixel values and larger gradients in the transfer function i.e. wz where the weights w are used to form the final irradiance estimate log ei wzijgzij log tj wzij exercise has you implement one of the radiometric response function calibration techniques and then use it to create radiance maps. under real-world conditions casually acquired images may not be perfectly registered and may contain moving objects. ward uses a global transform to align the input images while kang uyttendaele winder et al. present an algorithm that combines global registration with local motion estimation flow to accurately align the images before blending their radiance estimates since the images may computer vision algorithms and applications draft figure fuji superccd high dynamic range image sensor. the paired large and small active areas provide two different effective exposures. have widely different exposures care must be taken when estimating the motions which must themselves be checked for consistency to avoid the creation of ghosts and object fragments. even this approach however may not work when the camera is simultaneously undergoing large panning motions and exposure changes which is a common occurrence in casually acquired panoramas. under such conditions different parts of the image may be seen at one or more exposures. devising a method to blend all of these different sources while avoiding sharp transitions and dealing with scene motion is a challenging problem. one approach is to first find a consensus mosaic and to then selectively compute radiances in under- and over-exposed regions uyttendaele and szeliski as shown in figure recently some cameras such as the sony and pentax have started integrating multiple exposure merging and tone mapping directly into the camera body. in the future the need to compute high dynamic range images from multiple exposures may be eliminated by advances in camera sensor technology el gamal fowler et al. nayar and mitsunaga nayar and branzoi kang uyttendaele winder et al. narasimhan and nayar tumblin agrawal and raskar however the need to blend such images and to tone map them to lower-gamut displays is likely to remain. hdr image formats. before we discuss techniques for mapping hdr images back to a displayable gamut we should discuss the commonly used formats for storing hdr images. if storage space is not an issue storing each of the r g and b values as a ieee float is the best solution. the commonly used portable pixmap format which supports both uncompressed ascii and raw binary encodings of values can be extended to a portable floatmap format by modifying the header. tiff also supports full floating point values. a more compact representation is the radiance format which uses a single common exponent and per-channel mantissas an intermediate encod high dynamic range imaging figure hdr image encoding formats portable pixmap radiance openexr ing openexr from uses floats for each channel which is a format supported natively on most modern gpus. ward describes these and other data formats such as logluv in more detail as do the books by reinhard ward pattanaik et al. and freeman an even more recent hdr image format is the jpeg xr tone mapping once a radiance map has been computed it is usually necessary to display it on a lower gamut eight-bit screen or printer. a variety of tone mapping techniques has been developed for this purpose which involve either computing spatially varying transfer functions or reducing image gradients to fit the available dynamic range ward pattanaik et al. the simplest way to compress a high dynamic range radiance image into a low dynamic range gamut is to use a global transfer curve rushmeier and piatko figure shows one such example where a gamma curve is used to map an hdr image back httpwww.openexr.net. bits bits bits pixelsignexponentmantissac computer vision algorithms and applications draft figure global tone mapping input hdr image linearly mapped gamma applied to each color channel independently gamma applied to intensity are less washed out. original hdr image courtesy of paul debevec httpict.debevec.org debevecresearchhdr. processed images courtesy of fr edo durand mit course on computational photography. into a displayable gamut. if gamma is applied separately to each channel the colors become muted saturated since higher-valued color channels contribute less to the final color. splitting the image up into its luminance and chrominance lab components applying the global mapping to the luminance channel and then reconstituting a color image works better unfortunately when the image has a really wide range of exposures this global approach still fails to preserve details in regions with widely varying exposures. what is needed instead is something akin to the dodging and burning performed by photographers in the darkroom. mathematically this is similar to dividing each pixel by the average brightness in a region around that pixel. figure shows how this process works. as before the image is split into its lumi nance and chrominance channels. the log luminance image hx y log lx y is then low-pass filtered to produce a base layer and a high-pass detail layer hlx y bx y hx y hhx y hx y hlx y. the base layer is then contrast reduced by scaling to the desired log-luminance range y s hhx y high dynamic range imaging and added to the detail layer to produce the new log-luminance image ix y y hlx y which can then be exponentiated to produce the tone-mapped luminance image. note that this process is equivalent to dividing each luminance value by monotonic mapping of the average log-luminance value in a region around that pixel. figure shows the low-pass and high-pass log luminance image and the resulting tone-mapped color image. note how the detail layer has visible halos around the highcontrast edges which are visible in the final tone-mapped image. this is because linear filtering which is not edge preserving produces halos in the detail layer the solution to this problem is to use an edge-preserving filter to create the base layer. durand and dorsey study a number of such edge-preserving filters including anisotropic and robust anisotropic diffusion and select bilateral filtering as their edgepreserving filter. more recent paper by farbman fattal lischinski et al. argues in favor of using a weighted least squares filter as an alternative to the bilateral filter and paris kornprobst tumblin et al. reviews bilateral filtering and its applications in computer vision and computational photography. figure shows how replacing the linear low-pass filter with a bilateral filter produces tone-mapped images with no visible halos. figure summarizes the complete information flow in this process starting with the decomposition into log luminance and chrominance images bilateral filtering contrast reduction and re-composition into the final output image. an alternative to compressing the base layer is to compress its derivatives i.e. the gradient of the log-luminance image lischinski and werman figure illustrates this process. the log-luminance image is differentiated to obtain a gradient image this gradient image is then attenuated by a spatially varying attenuation function y y hx y. gx y y y. the attenuation function ix y is designed to attenuate large-scale brightness changes and is designed to take into account gradients at different spatial scales lischinski and werman after attenuation the resulting gradient field is re-integrated by solving a first-order vari ational squares problem ix y gx dy computer vision algorithms and applications draft figure local tone mapping using linear filters low-pass and high-pass filtered log luminance images and color image resulting tone-mapped image attenuating the low-pass log luminance image shows visible halos around the trees. processed images courtesy of fr edo durand mit course on computational photography. figure local tone mapping using bilateral filter and dorsey lowpass and high-pass bilateral filtered log luminance images and color image resulting tone-mapped image attenuating the low-pass log luminance image shows no halos. processed images courtesy of fr edo durand mit course on computational photography. high dynamic range imaging figure gaussian vs. bilateral filtering agrawala hoppe et al. acm a gaussian low-pass filter blurs across all edges and therefore creates strong peaks and valleys in the detail image that cause halos. the bilateral filter does not smooth across strong edges and thereby reduces halos while still capturing detail. figure local tone mapping using bilateral filter and dorsey summary of algorithm workflow. images courtesy of fr edo durand mit course on computational photography. computer vision algorithms and applications draft figure gradient domain tone mapping lischinski and werman acm. the original image with a dynamic range of is first converted into the log domain hx and its gradients are computed these are attenuated based on local contrast gx and integrated to produce the new logarithmic exposure image ix which is exponentiated to produce the final intensity image whose dynamic range is to obtain the compressed log-luminance image ix y. this least squares problem is the same that was used for poisson blending and was first introduced in our study of regularization it can efficiently be solved using techniques such as multigrid and hierarchical basis preconditioning lischinski and werman szeliski farbman fattal lischinski et al. once the new luminance image has been computed it is combined with the original color image using cout cin lout where c g b and lin and lout are the original and compressed luminance images. the exponent s controls the saturation of the colors and is typically in the range s figure shows the final tone-mapped color image which shows no visible halos despite the extremely large variation in input radiance values. yet another alternative to these two approaches is to perform the local dodging and burning using a locally scale-selective operator stark shirley et al. figure shows how such a scale selection operator can determine a radius that only includes high dynamic range imaging figure gradient domain tone mapping lischinski and werman acm attenuation map with darker values corresponding to more attenuation final tone-mapped image. similar color values within the inner circle while avoiding much brighter values in the surrounding circle. in practice a difference of gaussians normalized by the inner gaussian response is evaluated over a range of scales and the largest scale whose metric is below a threshold is selected stark shirley et al. what all of these techniques have in common is that they adaptively attenuate or brighten different regions of the image so that they can be displayed in a limited gamut without loss of contrast. lischinski farbman uyttendaele et al. introduce an interactive technique that performs this operation by interpolating a set of sparse user-drawn adjustments and associated exposure value corrections to a piecewise-continuous exposure correction map the interpolation is performed by minimizing a locally weighted least square variational problem wdx y gx dy wsx fx dy where gx y and fx y are the input and output log exposure maps the data weighting term wdx y is at stroke locations and elsewhere. the smoothness weighting term wsx y is inversely proportional to the log-luminance gradient and hence encourages the fx y map to be smoother in low-gradient areas than along highgradient the same approach can also be used for fully automated tone in practice the x and y discrete derivatives are weighted separately farbman uyttendaele et al. ws computer vision algorithms and applications draft figure scale selection for tone mapping stark shirley et al. acm. ping by setting target exposure values at each pixel and allowing the weighted least squares to convert these into piecewise smooth adjustment maps. the weighted least squares algorithm which was originally developed for image colorization applications lischinski and weiss has recently been applied to general edge-preserving smoothing in applications such as contrast enhancement paris and durand and tone mapping fattal lischinski et al. where the bilateral filtering was previously used. it can also be used to perform hdr merging and tone mapping simultaneously and chaudhuri given the wide range of locally adaptive tone mapping algorithms that have been developed which ones should be used in practice? freeman provides a great discussion of commercially available algorithms their artifacts and the parameters that can be used to control them. he also has a wealth of tips for hdr photography and workflow. i highly recommend his book for anyone contemplating additional research personal photography in this area. application flash photography while high dynamic range imaging combines images of a scene taken at different exposures it is also possible to combine flash and non-flash images to achieve better exposure and color balance and to reduce noise and durand petschnigg agrawala hoppe et al. the problem with flash images is that the color is often unnatural fails to capture the ambient illumination there may be strong shadows or specularities and there is a radial falloff in brightness away from the camera and non-flash photos their default parameter settings are and high dynamic range imaging figure interactive local tone mapping farbman uyttendaele et al. acm user-drawn strokes with associated exposure values gx y corresponding piecewise-smooth exposure adjustment map fx y. figure detail transfer in flashno-flash photography agrawala hoppe et al. acm details of input ambient a and flash f images joint bilaterally filtered no-flash image anr detail layer f detail computed from the flash image f final merged image afinal. computer vision algorithms and applications draft taken under low light conditions often suffer from excessive noise of the high iso gains and low photon counts and blur to longer exposures. is there some way to combine a non-flash photo taken just before the flash goes off with the flash photo to produce an image with good color values sharpness and low petschnigg agrawala hoppe et al. approach this problem by first filtering the noflash image a with a variant of the bilateral filter called the joint bilateral in which the range kernel ri j k l j fk r is evaluated on the flash image f instead of the ambient image a since the flash image is less noisy and hence has more reliable edges because the contents of the flash image can be unreliable inside and at the boundaries of shadows and specularities these are detected and a regular bilaterally filtered image abase is used instead the second stage of their algorithm computes a flash detail image f detail f f base where f base is a bilaterally filtered version of the flash image f and this detail image encodes details that may have been filtered away from the noise-reduced no-flash image anr as well as additional details created by the flash camera which often add crispness. the detail image is used to modulate the noise-reduced ambient image anr to produce the final results afinal manrf detail m abase shown in figures and eisemann and durand present an alternative algorithm that shares some of the same basic concepts. both papers are well worth reading and contrasting flash images can also be used for a variety of additional applications such as extracting more reliable foreground mattes of objects tan feris et al. sun li kang et al. flash photography is just one instance of the more general topic of active illumination which is discussed in more detail by raskar and tumblin in fact the discontinued fujifilm finepix camera takes a pair of flash and no flash images in quick succession however it only lets you decide to keep one of them. eisemann and durand call this the cross bilateral filter. super-resolution and blur removal figure flashno-flash photography algorithm agrawala hoppe et al. acm. the ambient image a is filtered with a regular bilateral filter to produce abase which is used in shadow and specularity regions and a joint bilaterally filtered noise reduced image anr. the flash image f is bilaterally filtered to produce a base image f base and a detail image f detail which is used to modulate the denoised ambient image. the shadowspecularity mask m is computed by comparing linearized versions of the flash and no-flash images. super-resolution and blur removal while high dynamic range imaging enables us to obtain an image with a larger dynamic range than a single regular image super-resolution enables us to create images with higher spatial resolution and less noise than regular camera images park park and kang capel and zisserman capel van ouwerkerk most commonly super-resolution refers to the process of aligning and combining several input images to produce such high-resolution composites and peleg cheeseman kanefsky hanson et al. pickup capel roberts et al. however some newer techniques can super-resolve a single image jones and pasztor baker and kanade fattal and are hence closely related to techniques for removing blur and the most principled way to formulate the super-resolution problem is to write down the stochastic image formation equations and image priors and to then use bayesian inference to recover the super-resolved sharp image. we can do this by generalizing the image computer vision algorithms and applications draft formation equations used for image deblurring which we also used in for blur kernel estimation in this case we have several observed images as well as an image warping function hkx for each observed image combining all of these elements we get the observation okx dbx s hkx nkx where d is the downsampling operator which operates after the super-resolved warped image s hkx has been convolved with the blur kernel bx. the above image formation equations lead to the following least squares problem dbkx s in most super-resolution algorithms the alignment hk is estimated using one of the input frames as the reference frame either feature-based or direct parametric alignment techniques can be used. few algorithms such as those described by schultz and stevenson or capel use dense flow estimates. a better approach is to re-compute the alignment by directly minimizing once an initial estimate of sx has been computed barnard and armstrong or to marginalize out the motion parameters altogether capel roberts et al. see also the work of protter and elad for some related video super-resolution work. the point spread function kernel bk is either inferred from knowledge of the image formation process the amount of motion or defocus blur and the camera sensor optics or calibrated from a test image or the observed images using one of the techniques described in section the problem of simultaneously inferring the blur kernel and the sharp image is known as blind image deconvolution and hatzinakos levin given an estimate of hk and bkx can be re-written using matrixvector notation as a large sparse least squares problem in the unknown values of the super-resolved pixels s dbkw it is also possible to add an unknown bias gain term to each observation as was done for motion estimation in notice that there is a chicken-and-egg problem if both the blur kernel and the super-resolved image are unknown. this can be broken either using structural assumptions about the sharp image e.g. the presence of edges szeliski and kriegman or prior models for the image such as edge sparsity singh hertzmann et al. super-resolution and blur removal from that once the warping function hk is known values of s hkx depend linearly on those in sx. an efficient way to solve this least squares problem is to use preconditioned conjugate gradient descent although some earlier algorithms such as the one developed by irani and peleg used regular gradient descent known as iterative back projection in the computed tomography literature. the above formulation assumes that warping can be expressed as a simple or bicubic interpolated resampling of the super-resolved sharp image followed by a stationary invariant blurring and area integration process. however if the surface is severely foreshortened we have to take into account the spatially varying filtering that occurs during the image warping before we can then model the psf induced by the optics and camera sensor kang szeliski et al. capel how well does this least squares approach to super-resolution work? in practice this depends a lot on the amount of blur and aliasing in the camera optics as well as the accuracy in the motion and psf estimates and kanade jiang wong and bao capel less blurring and more aliasing means that there is more high frequency information available to be recovered. however because the least squares likelihood formulation uses no image prior a lot of high-frequency noise can be introduced into the solution for this reason most super-resolution algorithms assume some form of image prior. the simplest of these is to place a penalty on the image derivatives similar to equations and e.g. psi j si j psi j si j as discussed in section when p is quadratic this is a form of tikhonov regularization and the overall problem is still linear least squares. the resulting prior image model is a gaussian markov random field which can be extended to other diagonal differences as in unfortunately gmrfs tend to produce solutions with visible ripples which can also be interpreted as increased noise sensitivity in middle frequencies a better image prior is a robust prior that encourages piecewise continuous solutions and rangarajan see appendix examples of such priors include the huber potential and stevenson capel and zisserman which is a blend of a gaussian with a longer-tailed laplacian and the even sparser hyper-laplacians used by levin fergus durand et al. and krishnan and fergus it is also possible to learn the parameters for such priors using cross-validation pickup while sparse derivative priors can reduce rippling effects and increase edge sharpness they cannot hallucinate higher-frequency texture or details. to do this a train computer vision algorithms and applications draft figure super-resolution results using a variety of image priors lowres roi zoom average image mle pixel-zoom simple prior gmrf hmrf images are used as input and a super-resolved image is produced in each case except for the mle result in figure example-based super-resolution original low-resolution image example-based super-resolved image jones and pasztor ieee upsampling via imposed edge statistics acm. super-resolution and blur removal ing set of sample images can be used to find plausible mappings between low-frequency originals and the missing higher frequencies. inspired by some of the example-based texture synthesis algorithms we discuss in section the example-based super-resolution algorithm developed by freeman jones and pasztor uses training images to learn the mapping between local texture patches and missing higher-frequency details. to ensure that overlapping patches are similar in appearance a markov random field is used and optimized using either belief propagation pasztor and carmichael or a raster-scan deterministic variant jones and pasztor figure shows the results of hallucinating missing details using this approach and compares these results to a more recent algorithm by fattal this latter algorithm learns to predict oriented gradient magnitudes in the finer resolution image based on a pixel s location relative to the nearest detected edge along with the corresponding edge statistics and width. it is also possible to combine sparse derivative priors with example-based super-resolution as shown by tappen russell and freeman an alternative closely related form of hallucination is to recognize the parts of a training database of images to which a low-resolution pixel might correspond. in their work baker and kanade use local derivative-of-gaussian filter responses as features and then match parent structure vectors in a manner similar to de bonet the highfrequency gradient at each recognized training image location is then used as a constraint on the super-resolved image along with the usual reconstruction equation figure shows the result of hallucinating higher-resolution faces from lower-resolution inputs baker and kanade also show examples of super-resolving known-font text. exercise gives more details on how to implement and test one or more of these superresolution techniques. under favorable conditions super-resolution and related upsampling techniques can increase the resolution of a well-photographed image or image collection. when the input images are blurry to start with the best one can often hope for is to reduce the amount of blur. this problem is closely related super-resolution with the biggest differences being that the blur kernel b is usually much larger and the downsampling factor d is unity. a large literature on image deblurring exists some of the more recent publications with nice literature reviews include those by fergus singh hertzmann et al. yuan sun quan et al. and joshi zitnick szeliski et al. it is also possible to reduce blur by combining sharp noisy images with blurrier cleaner images sun quan et al. take lots of quick and kutulakos hasinoff kutulakos durand et al. hasinoff durand and freeman or use coded aperture techniques to simultaneously for face super-resolution where all the images are pre-aligned only corresponding pixels in different images are examined. the sony takes multiple shots to produce better low-light photos. computer vision algorithms and applications draft figure recognition-based super-resolution and kanade ieee. the hallucinated column shows the results of the recognition-based algorithm compared to the regularization-based approach of hardie barnard and armstrong estimate depth and reduce blur fergus durand et al. zhou lin and nayar color image demosaicing a special case of super-resolution which is used daily in most digital still cameras is the process of demosaicing samples from a color filter array into a full-color rgb image. figure shows the most commonly used cfa known as the bayer pattern which has twice as many green sensors as red and blue sensors. the process of going from the known cfa pixels values to the full rgb image is quite challenging. unlike regular super-resolution where small errors in guessing unknown values usually show up as blur or aliasing demosaicing artifacts often produce spurious colors or high-frequency patterned zippering which are quite visible to the eye over the years a variety of techniques have been developed for image demosaicing bennett uyttendaele zitnick et al. present a recently developed algorithm along with some good references while longere delahunt zhang et al. and tappen russell and freeman compare some previously developed techniques using perceptually motivated metrics. to reduce the zippering effect most techniques use the edge or super-resolution and blur removal figure bayer rgb pattern color filter array layout interpolated pixel values with unknown values shown as lower case. figure cfa demosaicing results uyttendaele zitnick et al. springer original full-resolution image color subsampled version is used as the input to the algorithms bilinear interpolation results showing color fringing near the tip of the blue crayon and zippering near its left edge the high-quality linear interpolation results of malvar he and cutler the strong halocheckerboard artifacts on the yellow crayon using the local two-color prior of bennett uyttendaele zitnick et al. computer vision algorithms and applications draft figure two-color model computed from a collection of local neighborhoods uyttendaele zitnick et al. springer. after two-means clustering and reprojection along the line joining the two dominant colors dots the majority of the pixels fall near the fitted line. the distribution along the line projected along the rgb axes is peaked at and the two dominant colors. gradient information from the green channel which is more reliable because it is sampled more densely to infer plausible values for the red and blue channels which are more sparsely sampled. to reduce color fringing some techniques perform a color space analysis e.g. using median filtering on color opponent channels delahunt zhang et al. the approach of bennett uyttendaele zitnick et al. locally forms a two-color model from an initial demosaicing result using a moving window to find the two dominant colors once the local color model has been estimated at each pixel a bayesian approach is then used to encourage pixel values to lie along each color line and to cluster around the dominant color values which reduces halos the bayesian approach also supports the simultaneous application of demosaicing and super-resolution i.e. multiple cfa inputs can be merged into a higher-quality full-color image which becomes more important as additional processing becomes incorporated into today s cameras. application colorization although not strictly an example of super-resolution the process of colorization i.e. manually adding colors to a black and white image is another example of a sparse interpolation problem. in most applications of colorization the user draws some scribbles indicating the desired colors in certain regions and the system interpolates the previous work on locally linear color models shafer and kanade omer and werman focuses on color and illumination variation within a single material whereas bennett uyttendaele zitnick et al. use the two-color model to describe variations across color edges. image matting and compositing figure colorization using optimization lischinski and weiss acm grayscale image some color scribbles overlaid resulting colorized image original color image from which the grayscale image and the chrominance values for the scribbles were derived. original photograph by rotem weiss. specified chrominance v values to the whole image which are then re-combined with the input luminance channel to produce a final colorized image as shown in figure in the system developed by levin lischinski and weiss the interpolation is performed using locally weighted regularization where the local smoothness weights are inversely proportional to luminance gradients. this approach to locally weighted regularization has inspired later algorithms for high dynamic range tone mapping farbman uyttendaele et al. see section as well as other applications of the weighted least squares formulation fattal lischinski et al. an alternative approach to performing the sparse chrominance interpolation based on geodesic distance functions has been developed by yatziv and sapiro image matting and compositing image matting and compositing is the process of cutting a foreground object out of one image and pasting it against a new background and blinn wang and cohen it is commonly used in television and film production to composite a live actor in front of computer-generated imagery such as weather maps or virtual characters and scenery brinkmann we have already seen a number of tools for interactively segmenting objects in an image including snakes scissors and grabcut segmentation while these techniques can generate reasonable pixel-accurate segmentations they fail to capture the subtle interplay of foreground and background colors at mixed pixels along the boundary and golland in order to successfully copy a foreground object from one image to another without computer vision algorithms and applications draft figure softening a hard segmentation boundary matting kolmogorov and blake acm the region surrounding a segmentation boundary where pixels of mixed foreground and background colors are visible pixel values along the boundary are used to compute a soft alpha matte at each point along the curve t a displacement and a width are estimated. visible discretization artifacts we need to pull a matte i.e. to estimate a soft opacity channel and the uncontaminated foreground colors f from the input composite image c. recall from section that the compositing equation can be written as c f. this operator attenuates the influence of the background image b by a factor and then adds in the color values corresponding to the foreground element f while the compositing operation is easy to implement the reverse matting operation of estimating f and b given an input image c is much more challenging to see why observe that while the composite pixel color c provides three measurements the f and b unknowns have a total of seven degrees of freedom. devising techniques to estimate these unknowns despite the underconstrained nature of the problem is the essence of image matting. in this section we review a number of image matting techniques. we begin with blue screen matting which assumes that the background is a constant known color and discuss its variants two-screen matting multiple backgrounds can be used and difference matting the known background is arbitrary. we then discuss local variants of natural image matting where both the foreground and background are unknown. in these applications it is usual to first specify a trimap i.e. a three-way labeling of the image into foreground background and unknown regions next we present some global optimization approaches to natural image matting. finally we discuss variants on the matting problem including shadow matting flash matting and environment matting. image matting and compositing figure natural image matting curless salesin et al. ieee input image with a natural background hand-drawn trimap gray indicates unknown regions extracted alpha map extracted foreground colors composite over a new background. blue screen matting blue screen matting involves filming an actor object in front of a constant colored background. while originally bright blue was the preferred color bright green is now more commonly used brinkmann smith and blinn discuss a number of techniques for blue screen matting which are mostly described in patents rather than in the open research literature. early techniques used linear combination of object color channels with user-tuned parameters to estimate the opacity chuang curless salesin et al. describe a newer technique called mishima s algorithm which involves fitting two polyhedral surfaces at the mean background color separating the foreground and background color distributions and then measuring the relative distance of a novel color to these surfaces to estimate while this technique works well in many studio settings it can still suffer from blue spill where translucent pixels around the edges of an object acquire some of the background blue coloration two-screen matting. in their paper smith and blinn also introduce an algorithm called triangulation matting that uses more than one known background color to over-constrain the equations required to estimate the opacity and foreground color f computer vision algorithms and applications draft figure blue-screen matting results curless salesin et al. ieee. mishima s method produces visible blue spill fringing in the hair while chuang s bayesian matting approach produces accurate results. for example consider in the compositing equation setting the background color to black i.e. b the resulting composite image c is therefore equal to f replacing the background color with a different known non-zero value b now results in c f which is an overconstrained set of equations for estimating in practice b should be chosen so as not to saturate c and for best accuracy several values of b should be used. it is also important that colors be linearized before processing which is the case for all image matting algorithms. papers that generate ground truth alpha mattes for evaluation purposes normally use these techniques to obtain accurate matte estimates curless salesin et al. wang and cohen levin acha and lischinski rhemann rother rav-acha et al. rhemann rother wang et al. exercise has you do this as well. see the alpha matting evaluation web site at httpalphamatting.com. image matting and compositing difference matting. a related approach when the background is irregular but known is called difference matting brinkmann it is most commonly used when the actor or object is filmed against a static background e.g. for office videoconferencing person tracking applications krumm brumitt et al. or to produce silhouettes for volumetric reconstruction techniques seitz and dyer seitz curless diebel et al. it can also be used with a panning camera where the background is composited from frames where the foreground has been removed using a garbage matte agarwala curless et al. another recent application is the detection of visual continuity errors in films i.e. differences in the background when a shot is re-taken at later time and zisserman in the case where the foreground and background motions can both be specified with parametric transforms high-quality mattes can be extracted using a generalization of triangulation matting fitzgibbon and zisserman when frames need to be processed independently however the results are often of poor quality in such cases using a pair of stereo cameras as input can dramatically improve the quality of the results cross blake et al. yin criminisi winn et al. natural image matting the most general version of image matting is when nothing is known about the background except perhaps for a rough segmentation of the scene into foreground background and unknown regions which is known as the trimap some recent techniques however relax this requirement and allow the user to just draw a few strokes or scribbles in the image see figures and and cohen wang agrawala and cohen levin lischinski and weiss rhemann rother rav-acha et al. rhemann rother and gelautz fully automated single image matting results have also been reported acha and lischinski singaraju rother and rhemann the survey paper by wang and cohen has detailed descriptions and comparisons of all of these techniques a selection of which are described briefly below. a relatively simple algorithm for performing natural image matting is knockout as described by chuang curless salesin et al. and illustrated in figure in this algorithm the nearest known foreground and background pixels image space are determined and then blended with neighboring known pixels to produce a per-pixel foreground f and background b color estimate. the background color is then adjusted so that the measured color c lies on the line between f and b. finally opacity is estimated on a per-channel basis and the three estimates are combined based on per-channel color differences. is an approximation to the least squares solution for figure shows that knockout has problems when the background consists of more than one dominant local color. computer vision algorithms and applications draft mishima knockout ruzon tomasi bayesian figure image matting algorithms curless salesin et al. ieee. mishima s algorithm models global foreground and background color distribution as polyhedral surfaces centered around the mean background color. knockout uses a local color estimate of foreground and background for each pixel and computes along each color axis. ruzon and tomasi s algorithm locally models foreground and background colors and variances. chuang et al. s bayesian matting approach computes a map estimate of foreground color and opacity given the local foreground and background distributions. more accurate matting results can be obtained if we treat the foreground and background colors as distributions sampled over some region h. ruzon and tomasi model local color distributions as mixtures of gaussians and compute these models in strips. they then find the pairing of mixture components f and b that best describes the observed color c compute the as the relative distance between these means and adjust the estimates of f and b so they are collinear with c. chuang curless salesin et al. and hillman hannah and renshaw use full color covariance matrices to model mixtures of correlated gaussians and compute estimates independently for each pixel. matte extraction proceeds in strips starting from known color values growing into the unknown regions so that recently computed f and b colors can be used in later stages. to estimate the most likely value of an unknown pixel s opacity and foreground and background colors chuang et al. use a fully bayesian formulation that maximizes p b p b image matting and compositing figure natural image matting results curless salesin et al. ieee. difference matting and knockout both perform poorly on this kind of background while the more recent natural image matting techniques perform well. chuang et al. s results are slightly smoother and closer to the ground truth. computer vision algorithms and applications draft this is equivalent to minimizing the negative log likelihood lf b lcf b lf lb l the lc term since it is constant. let us examine each of these terms in turn. the first lcf b is the likelihood that pixel color c was observed given values for the unknowns b if we assume gaussian noise in our observation with variance c this negative log likelihood term is lc f c as illustrated in figure the second term lf corresponds to the likelihood that a particular foreground color f comes from the mixture of gaussians distribution. after partitioning the sample foreground colors into clusters a weighted mean and covariance is computed where the weights are proportional to a given foreground pixel s opacity and distance from the unknown pixel. the negative log likelihood for each cluster is thus given by lf f f f a similar method is used to estimate unknown background color distributions. if the background is already known i.e. for blue screen or difference matting applications its measured color value and variance are used instead. an alternative to modeling the foreground and background color distributions as mixtures of gaussians is to keep around the original color samples and to compute the most likely pairings that explain the observed color c and cohen these techniques are described in more detail in and cohen in their bayesian matting paper chuang curless salesin et al. assume a constant distribution for l more recent papers assume this distribution to be more peaked around and or sometimes use markov random fields to define a global correlated prior on p and cohen to compute the most likely estimates for b the bayesian matting algorithm alternates between computing b and since each of these problems is quadratic and hence can be solved as a small linear system. when several color clusters are estimated the most likely pairing of foreground and background color clusters is used. bayesian image matting produces results that improve on the original natural image matting algorithm by ruzon and tomasi as can be seen in figure however compared to more recent techniques and cohen its performance is not as good for complex background or inaccurate trimaps image matting and compositing optimization-based matting an alternative to estimating each pixel s opacity and foreground color independently is to use global optimization to compute a matte that takes into account correlations between neighboring values. two examples of this are border matting in the grabcut interactive segmentation system kolmogorov and blake and poisson matting jia tang et al. border matting first dilates the region around the binary segmentation produced by grabcut and then solves for a sub-pixel boundary location and a blur width for every point along the boundary smoothness in these parameters along the boundary is enforced using regularization and the optimization is performed using dynamic programming. while this technique can obtain good results for smooth boundaries such as a person s face it has difficulty with fine details such as hair. poisson matting jia tang et al. assumes a known foreground and background color for each pixel in the trimap with bayesian matting. however instead of independently estimating each value it assumes that the gradient of the alpha matte and the gradient of the color image are related by f b c which can be derived by taking gradients of both sides of and assuming that the foreground and background vary slowly. the per-pixel gradient estimates are then integrated into a continuous field using the regularization squares technique first described in section and subsequently used in poisson blending and gradient-based dynamic range compression mapping this technique works well when good foreground and background color estimates are available and these colors vary slowly. instead of computing per-pixel foreground and background colors levin lischinski and weiss assume only that these color distribution can locally be well approximated as mixtures of two colors which is known as the color line model c. under this assumption a closed-form estimate for at each pixel i in a window wk is given by i ak ak c bk where ci is the pixel color treated as a three-vector is any pixel along the background color line and ak is the vector joining the two closest points on the foreground and background color lines as shown in figure that the geometric derivation shown in this figure is an alternative to the algebraic derivation presented by levin lischinski and weiss minimizing the deviations of the alpha values i from their respective color computer vision algorithms and applications draft figure color line matting lischinski and weiss local patch of colors potential assignment of values foreground and background color lines the vector ak joining their closest points of intersection and the family of parallel planes of constant values i ak a scatter plot of sample colors and the deviations from the mean k for two sample colors ci and cj. line models over all overlapping windows wk in the image gives rise to the cost e wk i ak ci where the term is used to regularize the value of ak in the case where the two color distributions overlap in constant regions. because this formula is quadratic in the unknowns bk they can be eliminated inside each window wk leading to a final energy where the entries in the l matrix are given by e t l lij wk j ij m i kt k where m is the number of pixels in each window k is the mean color of the pixels in window wk and k is the covariance of the pixel colors plus figure shows the intuition behind the entries in this affinity matrix which is called the matting laplacian. note how when two pixels ci and cj in wk point in opposite directions away from the mean k their weighted dot product is close to and so their affinity becomes close to pixels close to each other in color space hence with similar expected values will have affinities close to minimizing the quadratic energy constrained by the known values of at scribbles only requires the solution of a sparse set of linear equations which is why the k kcj image matting and compositing figure comparative matting results for a medium accuracy trimap. wang and cohen describe the individual techniques being compared. authors call their technique a closed-form solution to natural image matting. once has been computed the foreground and background colors are estimated using a least squares minimization of the compositing equation regularized with a spatially varying firstorder smoothness fi ebf where the i weight is applied separately for the x and y components of the f and b derivatives lischinski and weiss laplacian matting is just one of many optimization-based techniques surveyed and compared by wang and cohen some of these techniques use alternative formulations for the affinities or smoothness terms on the matte alternative estimation techniques such as belief propagation or alternative representations local histograms for modeling local foreground and background color distributions and cohen some of these techniques also provide real-time results as the user draws a contour line or sparse set of scribbles agrawala and cohen rhemann rother ravacha et al. or even pre-segment the image into a small number of mattes that the user can select with simple clicks acha and lischinski figure shows the results of running a number of the surveyed algorithms on a region of toy animal fur where a trimap has been specified while figure shows results for techniques that can produce mattes with only a few scribbles as input. figure shows a result for an even more recent algorithm rother rav-acha et al. that claims to outperform all of the techniques surveyed by wang and cohen pasting. once a matte has been pulled from an image it is usually composited directly over the new background unless the seams between the cutout and background regions are computer vision algorithms and applications draft figure comparative matting results with scribble-based inputs. wang and cohen describe the individual techniques being compared. figure stroke-based segmentation result rother rav-acha et al. ieee. to be hidden in which case poisson blending erez gangnet and blake can be used in the latter case it is helpful if the matte boundary passes through regions that either have little texture or look similar in the old and new images. papers by jia sun tang et al. and wang and cohen explain how to do this. smoke shadow and flash matting in addition to matting out solid objects with fractional boundaries it is also possible to matte out translucent media such as smoke agarwala curless et al. starting with a video sequence each pixel is modeled as a linear combination of its background color and a constant foreground color that is common to all pixels. voting in color image matting and compositing figure smoke matting agarwala curless et al. acm input video frame after removing the foreground object estimated alpha matte insertion of new objects into the background. figure shadow matting goldman curless et al. acm. instead of simply darkening the new scene with the shadow shadow matting correctly dims the lit scene with the new shadow and drapes the shadow over geometry space is used to estimate this foreground color and the distance along each color line is used to estimate the per-pixel temporally varying alpha extracting and re-inserting shadows is also possible using a related technique goldman curless et al. here instead of assuming a constant foreground color each pixel is assumed to vary between its fully lit and fully shadowed colors which can be estimated by taking minimum and maximum values over time as a shadow passes over the scene the resulting fractional shadow matte can be used to re-project the shadow into a new scene. if the destination scene has a non-planar geometry it can be scanned by waving a straight stick shadow across the scene. the new shadow matte can then be warped with the computed deformation field to have it drape correctly over the new scene the quality and reliability of matting algorithms can also be enhanced using more sophisticated acquisition systems. for example taking a flash and non-flash image pair supports the reliable extraction of foreground mattes which show up as regions of large illumination change between the two images li kang et al. taking simultaneous video streams focused at different distances matusik pfister et al. or using multicamera arrays matusik and avidan are also good approaches to producing computer vision algorithms and applications draft high-quality mattes. these techniques are described in more detail in and cohen lastly photographing a refractive object in front of a number of patterned backgrounds allows the object to be placed in novel environments. these environment matting techniques werner curless et al. chuang zongker hindorff et al. are discussed in section video matting while regular single-frame matting techniques such as blue or green screen matting and blinn wright brinkmann can be applied to video sequences the presence of moving objects can sometimes make the matting process easier as portions of the background may get revealed in preceding or subsequent frames. chuang agarwala curless et al. describe a nice approach to this video matting problem where foreground objects are first removed using a conservative garbage matte and the resulting background plates are aligned and composited to yield a high-quality background estimate. they also describe how trimaps drawn at sparse keyframes can be interpolated to in-between frames using bi-direction optic flow. alternative approaches to video matting such as rotoscoping which involves drawing and tracking curves in video sequences hertzmann seitz et al. are discussed in the matting survey paper by wang and cohen texture analysis and synthesis while texture analysis and synthesis may not at first seem like computational photography techniques they are in fact widely used to repair defects such as small holes in images or to create non-photorealistic painterly renderings from regular photographs. the problem of texture synthesis can be formulated as follows given a small sample of a texture generate a larger similar-looking image as you can imagine for certain sample textures this problem can be quite challenging. traditional approaches to texture analysis and synthesis try to match the spectrum of the source image while generating shaped noise. matching the frequency characteristics which is equivalent to matching spatial correlations is in itself not sufficient. the distributions of the responses at different frequencies must also match. heeger and bergen develop an algorithm that alternates between matching the histograms of multi-scale pyramid responses and matching the final image histogram. portilla and simoncelli improve on this technique by also matching pairwise statistics across scale and orientations. de bonet uses a coarse-to-fine strategy to find locations in the source texture with a similar texture analysis and synthesis radishes lots more radishes rocks yogurt figure texture synthesis given a small patch of texture the task is to synthesize a similar-looking larger patch other semi-structured textures that are challenging to synthesize. courtesy of alyosha efros. parent structure i.e. similar multi-scale oriented filter responses and then randomly chooses one of these matching locations as the current sample value. more recent texture synthesis algorithms sequentially generate texture pixels by looking for neighborhoods in the source texture that are similar to the currently synthesized image and leung consider the yet unknown pixel p in the partially constructed texture on the left side of figure since some of its neighboring pixels have been already been synthesized we can look for similar partial neighborhoods in the sample texture image on the right and randomly select one of these as the new value of p. this process can be repeated down the new image either in a raster fashion or by scanning around the periphery onion peeling when filling holes as discussed in in their actual implementation efros and leung find the most similar neighborhood and then include all other neighborhoods within a d distance with they also optionally weight the random pixel selections by the similarity metric d. to accelerate this process and improve its visual quality wei and levoy extend this technique using a coarse-to-fine generation process where coarser levels of the pyramid which have already been synthesized are also considered during the matching bonet to accelerate the nearest neighbor finding tree-structured vector quantization is used. efros and freeman propose an alternative acceleration and visual quality improve computer vision algorithms and applications draft figure texture synthesis using non-parametric sampling and leung the value of the newest pixel p is randomly chosen from similar local patches in the source texture image. courtesy of alyosha efros. figure texture synthesis by image quilting and freeman instead of generating a single pixel at a time larger blocks are copied from the source texture. the transitions in the overlap regions between the selected blocks are then optimized using dynamic programming. courtesy of alyosha efros. ment technique. instead of synthesizing a single pixel at a time overlapping square blocks are selected using similarity with previously synthesized regions once the appropriate blocks have been selected the seam between newly overlapping blocks is determined using dynamic programming. graph cut seam selection is not required since only one seam location per row is needed for a vertical boundary. because this process involves selecting small patches and them stitching them together efros and freeman call their system image quilting. komodakis and tziritas present an mrf-based version of this block synthesis algorithm that uses a new efficient version of loopy belief propagation they call priority-bp ppnon-parametricsamplinginput imageoutput imageppinput imagenon-parametricsamplingbboutput errorboundary cut texture analysis and synthesis figure image inpainting filling b propagation along isophote directions sapiro caselles et al. acm d exemplar-based inpainting with confidence-based filling order p erez and toyama application hole filling and inpainting filling holes left behind when objects or defects are excised from photographs which is known as inpainting is one of the most common applications of texture synthesis. such techniques are used not only to remove unwanted people or interlopers from photographs but also to fix small defects in old photos and movies removal or to remove wires holding props or actors in mid-air during filming removal. bertalmio sapiro caselles et al. solve the problem by propagating pixel values along isophote directions interleaved with some anisotropic diffusion steps b. telea develops a faster technique that uses the fast marching method from level sets however these techniques will not hallucinate texture in the missing regions. bertalmio vese sapiro et al. augment their earlier technique by adding synthetic texture to the infilled regions. the example-based texture generation techniques discussed in the previous section can also be used by filling the holes from the outside in onion-peel ordering. however this approach may fail to propagate strong oriented structures. criminisi p erez and toyama use exemplar-based texture synthesis where the order of synthesis is determined by the strength of the gradient along the region boundary and d. sun yuan jia et al. present a related approach where the user draws interactive lines to indicate where structures should be preferentially propagated. additional techniques related to these approaches include those developed by drori cohen-or and yeshurun kwatra sch odl essa et al. kwatra essa bobick et al. wilczkowiak brostow tordoff et al. komodakis and tziritas and wexler shechtman and irani most hole filling algorithms borrow small pieces of the original image to fill in the holes. when a large database of source images is available e.g. when images are taken from a computer vision algorithms and applications draft figure texture transfer and freeman acm reference image source texture image rendered using the texture. photo sharing site or the internet it is sometimes possible to copy a single contiguous image region to fill the hole. hays and efros present such a technique which uses image context and boundary compatibility to select the source image which is then blended with the original image using graph cuts and poisson blending. this technique is discussed in more detail in section and figure application non-photorealistic rendering two more applications of the exemplar-based texture synthesis ideas are texture transfer and freeman and image analogies jacobs oliver et al. which are both examples of non-photorealistic rendering and gooch in addition to using a source texture image texture transfer also takes a reference target image and tries to match certain characteristics of the target image with the newly synthesized image. for example the new image being rendered in figure not only tries to satisfy the usual similarity constraints with the source texture in figure but it also tries to match the luminance characteristics of the reference image. efros and freeman mention that blurred image intensities or local image orientation angles are alternative quantities that could be matched. hertzmann jacobs oliver et al. formulate the following problem given a pair of images a and unfiltered and filtered source images respectively along with some additional unfiltered target image b synthesize a new filtered target image such that a b texture analysis and synthesis a b figure image analogies jacobs oliver et al. acm. given an example pair of a source image a and its rendered version generate the rendered version from another unfiltered source image b. instead of having the user program a certain non-photorealistic rendering effect it is sufficient to supply the system with examples of before and after images and let the system synthesize the novel image using exemplar-based synthesis as shown in figure the algorithm used to solve image analogies proceeds in a manner analogous to the texture synthesis algorithms of and leung wei and levoy once gaussian pyramids have been computed for all of the source and reference images the algorithm looks for neighborhoods in the source filtered pyramids generated from that are similar to the partially constructed neighborhood in while at the same time having similar multi-resolution appearances at corresponding locations in a and b. as with texture transfer appearance characteristics can include not only color or luminance values but also orientations. this general framework allows image analogies to be applied to a variety of rendering tasks. in addition to exemplar-based non-photorealistic rendering image analogies can be used for traditional texture synthesis super-resolution and texture transfer the same textured image for both a and if only the filtered image is available as is the case with paintings the missing reference image a can be hallucinated using a smart preserving blur operator. finally it is possible to train a system to perform texture-bynumbers by manually painting over a natural image with pseudocolors corresponding to pixels semantic meanings e.g. water trees and grass b. the resulting system can then convert a novel sketch into a fully rendered synthetic photograph d. in more recent work cheng vishwanathan and zhang add ideas from image quilting and freeman and mrf inference tziritas and paragios to the basic image analogies algorithm while ramanarayanan and bala recast this process as energy minimization which means it can also be viewed as a conditional random field and devise an efficient algorithm to find a good minimum. more traditional filtering and feature detection techniques can also be used for non computer vision algorithms and applications draft original painted a novel textured figure texture-by-numbers jacobs oliver et al. acm. given a textured image and a hand-labeled version a synthesize a new image given just the painted version b. novel painted b figure non-photorealistic abstraction of photographs acm and farbman fattal lischinski et al. acm. decarlo and santella photorealistic for example pen-and-ink illustration and salesin and painterly rendering techniques use local color intensity and orientation estimates as an input to their procedural rendering algorithms. techniques for stylizing and simplifying photographs and video and santella winnem oller olsen and gooch farbman fattal lischinski et al. as in figure use combinations of edge-preserving blurring and edge detection and enhancement additional reading a good overview of computational photography can be found in the book by raskar and tumblin survey articles by nayar cohen and szeliski levoy for a good selection of papers see the symposia on non-photorealistic animation and rendering at httpwww.npar.org. additional reading debevec and hayes as well as two special journal issues edited by bimber and durand and szeliski notes from the courses on computational photography mentioned at the beginning of this chapter are another great source of material and the sub-field of high dynamic range imaging has its own book discussing research in this area ward pattanaik et al. as well as some books describing related photographic techniques gulbins and gulbins algorithms for calibrating the radiometric response function of a camera can be found in articles by mann and picard debevec and malik and mitsunaga and nayar the subject of tone mapping is treated extensively in ward pattanaik et al. representative papers from the large volume of literature on this topic include those by tumblin and rushmeier larson rushmeier and piatko pattanaik ferwerda fairchild et al. tumblin and turk durand and dorsey fattal lischinski and werman reinhard stark shirley et al. lischinski farbman uyttendaele et al. and farbman fattal lischinski et al. the literature on super-resolution is quite extensive park park and kang capel and zisserman capel van ouwerkerk the term superresolution usually describes techniques for aligning and merging multiple images to produce higher-resolution composites peleg and brada irani and peleg cheeseman kanefsky hanson et al. mann and picard chiang and boult bascle blake and zisserman capel and zisserman smelyanskiy cheeseman maluf et al. capel and zisserman pickup capel roberts et al. gulbins and gulbins however single-image super-resolution techniques have also been developed jones and pasztor baker and kanade fattal a good survey on image matting is given by wang and cohen representative papers which include extensive comparisons with previous work include those by chuang curless salesin et al. wang and cohen levin acha and lischinski rhemann rother rav-acha et al. and rhemann rother wang et al. the literature on texture synthesis and hole filling includes traditional approaches to texture synthesis which try to match image statistics between source and destination images and bergen de bonet portilla and simoncelli as well as newer approaches which search for matching neighborhoods or patches inside the source sample and leung wei and levoy efros and freeman in a similar vein traditional approaches to hole filling involve the solution of local variational continuation problems sapiro caselles et al. bertalmio vese sapiro et al. mit cmu httpgraphics.cs. fall stanford cs and siggraph courses httpweb.media.mit.edu raskarphoto. computer vision algorithms and applications draft telea more recent techniques use data-driven texture synthesis approaches cohen-or and yeshurun kwatra sch odl essa et al. criminisi p erez and toyama sun yuan jia et al. kwatra essa bobick et al. wilczkowiak brostow tordoff et al. komodakis and tziritas wexler shechtman and irani exercises ex radiometric calibration implement one of the multi-exposure radiometric calibration algorithms described in section and malik mitsunaga and nayar reinhard ward pattanaik et al. this calibration will be useful in a number of different applications such as stitching images or stereo matching with different exposures and shape from shading. take a series of bracketed images with your camera on a tripod. if your camera has an automatic exposure bracketing mode taking three images may be sufficient to calibrate most of your camera s dynamic range especially if your scene has a lot of bright and dark regions. outdoors or through a window on a sunny day is best. if your images are not taken on a tripod first perform a global alignment transform. estimate the radiometric response function using one of the techniques cited above. estimate the high dynamic range radiance image by selecting or blending pixels from different exposures and malik mitsunaga and nayar eden uyttendaele and szeliski repeat your calibration experiments under different conditions e.g. indoors under incandescent light to get a sense for the range of color balancing effects that your camera imposes. if your camera supports raw and jpeg mode calibrate both sets of images simultaneously and to each other radiance at each pixel will correspond. see if you can come up with a model for what your camera does e.g. whether it treats color balance as a diagonal or full matrix multiply whether it uses non-linearities in addition to gamma whether it sharpens the image while developing the jpeg image etc. develop an interactive viewer to change the exposure of an image based on the average exposure of a region around the mouse. variant is to show the adjusted image exercises inside a window around the mouse. another is to adjust the complete image based on the mouse position. implement a tone mapping operator and use this to map your radiance image to a displayable gamut. ex noise level function determine your camera s noise level function using either multiple shots or by analyzing smooth regions. set up your camera on a tripod looking at a calibration target or a static scene with a good variation in input levels and colors. your camera s histogram to ensure that all values are being sampled. take repeated images of the same scene with a remote shutter release and average them to compute the variance at each pixel. discarding pixels near high gradients are affected by camera motion plot for each color channel the standard deviation at each pixel as a function of its output value. fit a lower envelope to these measurements and use this as your noise level function. how much variation do you see in the noise as a function of input level? how much of this is significant i.e. away from flat regions in your camera response function where you do not want to be sampling anyway? using the same images develop a technique that segments the image into near-constant regions szeliski kang et al. is easier if you are photographing a calibration chart. compute the deviations for each region from a single image and use them to estimate the nlf. how does this compare to the multi-image technique and how stable are your estimates from image to image? ex vignetting estimate the amount of vignetting in some of your lenses using one of the following three techniques devise one of your choosing take an image of a large uniform intensity region wall or blue sky but be careful of brightness gradients and fit a radial polynomial curve to estimate the vignetting. construct a center-weighted panorama and compare these pixel values to the input image values to estimate the vignetting function. weight pixels in slowly varying regions more highly as small misalignments will give large errors at high gradients. optionally estimate the radiometric response function as well and schechner goldman computer vision algorithms and applications draft analyze the radial gradients in low-gradient regions and fit the robust means of these gradients to the derivative of the vignetting function as described by zheng yu kang et al. for the parametric form of your vignetting function you can either use a simple radial function e.g. fr or one of the specialized equations developed by kang and weiss and zheng lin and kang in all of these cases be sure that you are using linearized intensity measurements by using either raw images or images linearized through a radiometric response function or at least images where the gamma curve has been removed. what happens if you forget to undo the gamma before fitting a vignetting function? ex optical blur estimation compute the optical psf either using a known target or by detecting and fitting step edges szeliski and kriegman detect strong edges to sub-pixel precision. fit a local profile to each oriented edge and fill these pixels into an ideal target image either at image resolution or at a higher resolution d. use least squares at valid pixels to estimate the psf kernel k either globally or in locally overlapping sub-regions of the image. visualize the recovered psfs and use them to remove chromatic aberration or de-blur the image. ex tone mapping implement one of the tone mapping algorithms discussed in section and dorsey fattal lischinski and werman reinhard stark shirley et al. lischinski farbman uyttendaele et al. or any of the numerous additional algorithms discussed by reinhard ward pattanaik et al. and http compare your algorithm to local histogram equalization ex flash enhancement develop an algorithm to combine flash and non-flash photographs to best effect. you can use ideas from eisemann and durand and petschnigg agrawala hoppe et al. or anything else you think might work well. exercises ex super-resolution implement one or more super-resolution algorithms and compare their performance. take a set of photographs of the same scene using a hand-held camera ensure that there is some jitter between the photographs. determine the psf for the images you are trying to super-resolve using one of the techniques in exercise alternatively simulate a collection of lower-resolution images by taking a high-quality photograph those with compression artifacts and applying your own pre-filter kernel and downsampling. estimate the relative motion between the images using a parametric translation and rotation motion estimation algorithm or implement a basic least squares super-resolution algorithm by minimizing the differ ence between the observed and downsampled images add in a gradient image prior either as another least squares term or as a robust term that can be minimized using iteratively reweighted least squares implement one of the example-based super-resolution techniques where matching against a set of exemplar images is used either to infer higher-frequency information to be added to the reconstruction jones and pasztor or higher-frequency gradients to be matched in the super-resolved image and kanade use local edge statistic information to improve the quality of the super resolved image ex image matting develop an algorithm for pulling a foreground matte from natural images as described in section make sure that the images you are taking are linearized and section and that your camera exposure is fixed manual mode at least when taking multiple shots of the same scene. to acquire ground truth data place your object in front of a computer monitor and display a variety of solid background colors as well as some natural imagery. remove your object and re-display the same images to acquire known background colors. computer vision algorithms and applications draft use triangulation matting and blinn to estimate the ground truth opacities and pre-multiplied foreground colors f for your objects. implement one or more of the natural image matting algorithms described in section and compare your results to the ground truth values you computed. alternatively use the matting test images published on httpalphamatting.com. run your algorithms on other images taken with the same calibrated camera other images you find interesting. ex smoke and shadow matting extract smoke or shadow mattes from one scene and insert them into another agarwala curless et al. chuang goldman curless et al. take a still or video sequence of images with and without some intermittent smoke and shadows. to linearize your images before proceeding with any computations. for each pixel fit a line to the observed color values. if performing smoke matting robustly compute the intersection of these lines to obtain the smoke color estimate. then estimate the background color as the other extremum you already took a smoke-free background image. if performing shadow matting compute robust shadow and lit values for each pixel. extract the smoke or shadow mattes from each frame as the fraction between these two values and smoke or shadowed and lit. scan a new scene or modify the original background with an image editor. re-insert the smoke or shadow matte along with any other foreground objects you may have extracted. using a series of cast stick shadows estimate the deformation field for the destination scene in order to correctly warp the shadows across the new geometry. is related to the shadow scanning technique developed by bouguet and perona and implemented in exercise chuang goldman curless et al. only demonstrated their technique for planar source geometries. can you extend their technique to capture shadows acquired from an irregular source geometry? exercises can you change the direction of the shadow i.e. simulate the effect of changing the light source direction? ex texture synthesis rithms presented in section here is one possible procedure implement one of the texture synthesis or hole filling algo implement the basic efros and leung algorithm i.e. starting from the outside hole filling or in raster order texture synthesis search for a similar neighborhood in the source texture image and copy that pixel. add in the wei and levoy extension of generating the pixels in a coarse-to-fine fashion i.e. generate a lower-resolution synthetic texture filled image and use this as a guide for matching regions in the finer resolution version. add in the criminisi p erez and toyama idea of prioritizing pixels to be filled by some function of the local structure or orientation strength. extend any of the above algorithms by selecting sub-blocks in the source texture and using optimization to determine the seam between the new block and the existing image that it overlaps and freeman implement one of the isophote continuation inpainting algorithms sapiro caselles et al. telea add the ability to supply a target image and freeman or to provide sample filtered or unfiltered and rendered images jacobs oliver et al. see section ex colorization implement the levin lischinski and weiss colorization algorithm that is sketched out in section and figure find some historic monochrome photographs and some modern color ones. write an interactive tool that lets you pick colors from a modern photo and paint over the old one. tune the algorithm parameters to give you good results. are you pleased with the results? can you think of ways to make them look more antique e.g. with softer saturated and edgy colors? computer vision algorithms and applications draft chapter stereo correspondence epipolar geometry sparse correspondence curves and profiles similarity measures dense correspondence local methods rectification plane sweep sub-pixel estimation and uncertainty application stereo-based head tracking dynamic programming segmentation-based techniques application z-keying and background replacement volumetric and surface reconstruction shape from silhouettes global optimization multi-view stereo additional reading exercises computer vision algorithms and applications draft figure stereo reconstruction techniques can convert b a pair of images into a depth map or e a sequence of images into a model an analytical stereo plotter courtesy of kenney aerial mapping inc. can generate contour plots. stereo correspondence stereo matching is the process of taking two or more images and estimating a model of the scene by finding matching pixels in the images and converting their positions into depths. in chapters we described techniques for recovering camera positions and building sparse models of scenes or objects. in this chapter we address the question of how to build a more complete model e.g. a sparse or dense depth map that assigns relative depths to pixels in the input images. we also look at the topic of multi-view stereo algorithms that produce complete volumetric or surface-based object models. why are people interested in stereo matching? from the earliest inquiries into visual perception it was known that we perceive depth based on the differences in appearance between the left and right as a simple experiment hold your finger vertically in front of your eyes and close each eye alternately. you will notice that the finger jumps left and right relative to the background of the scene. the same phenomenon is visible in the image pair shown in figure b in which the foreground objects shift left and right relative to the background. as we will shortly see under simple imaging configurations eyes or cameras looking straight ahead the amount of horizontal motion or disparity is inversely proportional to the distance from the observer. while the basic physics and geometry relating visual disparity to scene structure are well understood automatically measuring this disparity by establishing dense and accurate inter-image correspondences is a challenging task. the earliest stereo matching algorithms were developed in the field of photogrammetry for automatically constructing topographic elevation maps from overlapping aerial images. prior to this operators would use photogrammetric stereo plotters which displayed shifted versions of such images to each eye and allowed the operator to float a dot cursor around constant elevation contours the development of fully automated stereo matching algorithms was a major advance in this field enabling much more rapid and less expensive processing of aerial imagery hsieh mckeown and perlant in computer vision the topic of stereo matching has been one of the most widely studied and fundamental problems and poggio barnard and fischler dhond and aggarwal scharstein and szeliski brown burschka and hager seitz curless diebel et al. and continues to be one of the most active research areas. while photogrammetric matching concentrated mainly on aerial imagery computer vision applications include modeling the human visual system robotic navigation and manipulation konolige thrun montemerlo dahlkamp et al. as well as view interpolation and image-based rendering d model building f and h j and mixing live action with computer-generated imagery in this chapter we describe the fundamental principles behind stereo matching following the word stereo comes from the greek for solid stereo vision is how we perceive solid shape computer vision algorithms and applications draft figure applications of stereo vision input image computed depth map and new view generation from multi-view stereo kanade and szeliski springer view morphing between two images and dyer acm f face modeling courtesy of fr ed eric devernay z-keying live and computergenerated imagery yoshida oda et al. ieee j building surface models from multiple video streams in virtualized reality rander and narayanan epipolar geometry the general taxonomy proposed by scharstein and szeliski we begin in section with a review of the geometry of stereo image matching i.e. how to compute for a given pixel in one image the range of possible locations the pixel might appear at in the other image i.e. its epipolar line. we describe how to pre-warp images so that corresponding epipolar lines are coincident we also describe a general resampling algorithm called plane sweep that can be used to perform multi-image stereo matching with arbitrary camera configurations. next we briefly survey techniques for the sparse stereo matching of interest points and edge-like features we then turn to the main topic of this chapter namely the estimation of a dense set of pixel-wise correspondences in the form of a disparity map this involves first selecting a pixel matching criterion and then using either local area-based aggregation or global optimization to help disambiguate potential matches. in section we discuss multi-view stereo methods that aim to reconstruct a complete model instead of just a single disparity image f. epipolar geometry given a pixel in one image how can we compute its correspondence in the other image? in chapter we saw that a variety of search techniques can be used to match pixels based on their local appearance as well as the motions of neighboring pixels. in the case of stereo matching however we have some additional information available namely the positions and calibration data for the cameras that took the pictures of the same static scene how can we exploit this information to reduce the number of potential correspondences and hence both speed up the matching and increase its reliability? figure shows how a pixel in one image projects to an epipolar line segment in the other image. the segment is bounded at one end by the projection of the original viewing ray at infinity p and at the other end by the projection of the original camera center into the second camera which is known as the epipole if we project the epipolar line in the second image back into the first we get another line this time bounded by the other corresponding epipole extending both line segments to infinity we get a pair of corresponding epipolar lines which are the intersection of the two image planes with the epipolar plane that passes through both camera centers and as well as the point of interest p and luong hartley and zisserman computer vision algorithms and applications draft figure epipolar geometry epipolar line segment corresponding to one ray corresponding set of epipolar lines and their epipolar plane. rectification as we saw in section the epipolar geometry for a pair of cameras is implicit in the relative pose and calibrations of the cameras and can easily be computed from seven or more point matches using the fundamental matrix five or more points for the calibrated essential matrix faugeras and luong hartley and zisserman once this geometry has been computed we can use the epipolar line corresponding to a pixel in one image to constrain the search for corresponding pixels in the other image. one way to do this is to use a general correspondence algorithm such as optical flow but to only consider locations along the epipolar line to project any flow vectors that fall off back onto the line. a more efficient algorithm can be obtained by first rectifying warping the input images so that corresponding horizontal scanlines are epipolar lines and zhang faugeras and luong hartley and zisserman afterwards it is possible to match horizontal scanlines independently or to shift images horizontally while computing matching scores a simple way to rectify the two images is to first rotate both cameras so that they are looking perpendicular to the line joining the camera centers and since there is a degree of freedom in the tilt the smallest rotations that achieve this should be used. next to determine the desired twist around the optical axes make the up vector camera y axis this makes most sense if the cameras are next to each other although by rotating the cameras rectification can be performed on any pair that is not verged too much or has too much of a scale change. in those latter cases using plane sweep or hypothesizing small planar patch locations in snavely curless et al. may be preferable. epipolar planep epipolar geometry figure the multi-stage stereo rectification algorithm of loop and zhang ieee. original image pair overlaid with several epipolar lines images transformed so that epipolar lines are parallel images rectified so that epipolar lines are horizontal and in vertial correspondence final rectification that minimizes horizontal distortions. perpendicular to the camera center line. this ensures that corresponding epipolar lines are horizontal and that the disparity for points at infinity is finally re-scale the images if necessary to account for different focal lengths magnifying the smaller image to avoid aliasing. full details of this procedure can be found in fusiello trucco and verri and exercise note that in general it is not possible to rectify an arbitrary collection of images simultaneously unless their optical centers are collinear although rotating the cameras so that they all point in the same direction reduces the inter-camera pixel movements to scalings and translations. the resulting standard rectified geometry is employed in a lot of stereo camera setups and stereo algorithms and leads to a very simple inverse relationship between depths z and disparities d d f b z where f is the focal length in pixels b is the baseline and x dx y y describes the relationship between corresponding pixel coordinates in the left and right images baker and marimont okutomi and kanade scharstein and szeliski computer vision algorithms and applications draft figure slices through a typical disparity space image and szeliski springer original color image ground truth disparities e three y slices for d an d slice for y dashed line in various dark regions are visible in e e.g. the bookshelves table and cans and head statue and three disparity levels can be seen as horizontal lines in the dark bands in the dsis indicate regions that match at this disparity. dark regions are often the result of textureless regions. additional examples of dsis are discussed by bobick and intille the task of extracting depth from a set of images then becomes one of estimating the disparity map dx y. after rectification we can easily compare the similarity of pixels at corresponding locations y and d y and store them in a disparity space image cx y d for further processing the concept of the disparity space y d dates back to early work in stereo matching and poggio while the concept of a disparity space image is generally associated with yang yuille and lu and intille and bobick plane sweep an alternative to pre-rectifying the images before matching is to sweep a set of planes through the scene and to measure the photoconsistency of different images as they are re-projected onto these planes this process is commonly known as the plane sweep algorithm szeliski and golland saito and kanade as we saw in section where we introduced projective depth known as plane plus parallax anandan and hanna sawhney szeliski and coughlan the term disparity was first introduced in the human vision literature to describe the difference in location of corresponding features seen by the left and right eyes horizontal disparity is the most commonly studied phenomenon but vertical disparity is possible if the eyes are verged. epipolar geometry figure sweeping a set of planes through a scene and golland springer the set of planes seen from a virtual camera induces a set of homographies in any other source camera image. the warped images from all the other cameras can be stacked into a generalized disparity space volume ix y d k indexed by pixel location y disparity d and camera k. the last row of a full-rank projection matrix p can be set to an arbitrary plane equation the resulting four-dimensional projective transform maps world points p y z into screen coordinates xs ys d where the projective depth parallax d is on the reference plane sweeping d through a series of disparity hypotheses as shown in figure corresponds to mapping each input image into the virtual camera p defining the disparity space through a series of homographies xk p k p xs h k x tkd h k d x as shown in figure where xk and x are the homogeneous pixel coordinates in the source and virtual images and golland the members of the family of homographies h kd h k d which are parametererized by the addition of a matrix are related to each other through a planar homology and zisserman the choice of virtual camera and parameterization is application dependent and is what gives this framework a lot of its flexibility. in many applications one of the input cameras reference camera is used thus computing a depth map that is registered with one of the input images and which can later be used for image-based rendering and in other applications such as view interpolation for gaze correction in video-conferencing virtual cameradxyinput image kuvhomography u h x xykdk computer vision algorithms and applications draft lewis and cox criminisi shotton blake et al. a camera centrally located between the two input cameras is preferable since it provides the needed per-pixel disparities to hallucinate the virtual middle image. the choice of disparity sampling i.e. the setting of the zero parallax plane and the scaling of integer disparities is also application dependent and is usually set to bracket the range of interest i.e. the working volume while scaling disparities to sample the image in pixel sub-pixel shifts. for example when using stereo vision for obstacle avoidance in robot navigation it is most convenient to set up disparity to measure per-pixel elevation above the ground shen and coughlan as each input image is warped onto the current planes parameterized by disparity d it can be stacked into a generalized disparity space image ix y d k for further processing and golland in most stereo algorithms the photoconsistency sum of squared or robust differences with respect to the reference image ir is calculated and stored in the dsi cx y d ix y d k irx y. however it is also possible to compute alternative statistics such as robust variance focus or entropy szeliski zitnick et al. or to use this representation to reason about occlusions and golland kang and szeliski the generalized dsi will come in particularly handy when we come back to the topic of multi-view stereo in section of course planes are not the only surfaces that can be used to define a sweep through the space of interest. cylindrical surfaces especially when coupled with panoramic photography are often used yamamoto and tsuji kang and szeliski shum and szeliski li shum tang et al. zheng kang cohen et al. it is also possible to define other manifold topologies e.g. ones where the camera rotates around a fixed axis once the dsi has been computed the next step in most stereo correspondence algorithms is to produce a univalued function in disparity space dx y that best describes the shape of the surfaces in the scene. this can be viewed as finding a surface embedded in the disparity space image that has some optimality property such as lowest cost and best smoothness yuille and lu figure shows examples of slices through a typical dsi. more figures of this kind can be found in the paper by bobick and intille sparse correspondence sparse correspondence early stereo matching algorithms were feature-based i.e. they first extracted a set of potentially matchable image locations using either interest operators or edge detectors and then searched for corresponding locations in other images using a patch-based metric marr and poggio mayhew and frisby baker and binford arnold grimson ohta and kanade bolles baker and marimont matthies kanade and szeliski hsieh mckeown and perlant bolles baker and hannah this limitation to sparse correspondences was partially due to computational resource limitations but was also driven by a desire to limit the answers produced by stereo algorithms to matches with high certainty. in some applications there was also a desire to match scenes with potentially very different illuminations where edges might be the only stable features such sparse reconstructions could later be interpolated using surface fitting algorithms such as those discussed in sections and more recent work in this area has focused on first extracting highly reliable features and then using these as seeds to grow additional matches and shan lhuillier and quan similar approaches have also been extended to wide baseline multi-view stereo problems and combined with surface reconstruction and quan strecha tuytelaars and van gool goesele snavely curless et al. or free-space reasoning as described in more detail in section curves and profiles another example of sparse correspondence is the matching of profile curves occluding contours which occur at the boundaries of objects and at interior self occlusions where the surface curves away from the camera viewpoint. the difficulty in matching profile curves is that in general the locations of profile curves vary as a function of camera viewpoint. therefore matching curves directly in two images and then triangulating these matches can lead to erroneous shape measurements. fortunately if three or more closely spaced frames are available it is possible to fit a local circular arc to the locations of corresponding edgels and therefore obtain semi-dense curved surface meshes directly from the matches and g. another advantage of matching such curves is that they can be used to reconstruct surface shape for untextured surfaces so long as there is a visible difference between foreground and background colors. over the years a number of different techniques have been developed for reconstructing surface shape from profile curves and weiss cipolla and blake vaillant and faugeras zheng boyer and berger szeliski and weiss cipolla and giblin describe many of these techniques as well as related topics such as in computer vision algorithms and applications draft figure surface reconstruction from occluding contours and weiss springer circular arc fitting in the epipolar plane synthetic example of an ellipsoid with a truncated side and elliptic surface markings partially reconstructed surface mesh seen from an oblique and top-down view real-world image sequence of a soda can on a turntable extracted edges partially reconstructed profile curves partially reconstructed surface mesh. reconstructions are shown so as not to clutter the images. ferring camera motion from profile curve sequences. below we summarize the approach developed by szeliski and weiss which assumes a discrete set of images rather than formulating the problem in a continuous differential framework. let us assume that the camera is moving smoothly enough that the local epipolar geometry varies slowly i.e. the epipolar planes induced by the successive camera centers and an edgel under consideration are nearly co-planar. the first step in the processing pipeline is to extract and link edges in each of the input images and e. next edgels in successive images are matched using pairwise epipolar geometry proximity and appearance. this provides a linked set of edges in the spatio-temporal volume which is sometimes called the weaving wall to reconstruct the location of an individual edgel along with its local in-plane normal and curvature we project the viewing rays corresponding to its neighbors onto the instantaneous epipolar plane defined by the camera center the viewing ray and the camera velocity as shown in figure we then fit an osculating circle to the projected lines parameteriz dense correspondence ing the circle by its centerpoint c yc and radius r cixc siyc r di where ci ti and si ti are the cosine and sine of the angle between viewing ray i and the central viewing ray and di is the perpendicular distance between viewing ray i and the local origin which is a point chosen on the central viewing ray close to the line intersections and weiss the resulting set of linear equations can be solved using least squares and the quality of the solution error can be used to check for erroneous correspondences. the resulting set of points along with their spatial and temporal neighbors form a surface mesh with local normal and curvature estimates and g. note that whenever a curve is due to a surface marking or a sharp crease edge rather than a smooth surface profile curve this shows up as a or small radius of curvature. such curves result in isolated space curves rather than elements of smooth surface meshes but can still be incorporated into the surface model during a later stage of surface interpolation dense correspondence while sparse matching algorithms are still occasionally used most stereo matching algorithms today focus on dense correspondence since this is required for applications such as image-based rendering or modeling. this problem is more challenging than sparse correspondence since inferring depth values in textureless regions requires a certain amount of guesswork. of a solid colored background seen through a picket fence. what depth should it be? in this section we review the taxonomy and categorization scheme for dense correspondence algorithms first proposed by scharstein and szeliski the taxonomy consists of a set of algorithmic building blocks from which a large set of algorithms can be constructed. it is based on the observation that stereo algorithms generally perform some subset of the following four steps matching cost computation cost aggregation disparity computation and optimization and disparity refinement. computer vision algorithms and applications draft for example local algorithms where the disparity computation at a given point depends only on intensity values within a finite window usually make implicit smoothness assumptions by aggregating support. some of these algorithms can cleanly be broken down into steps for example the traditional sum-of-squareddifferences algorithm can be described as the matching cost is the squared difference of intensity values at a given disparity. aggregation is done by summing the matching cost over square windows with constant disparity. disparities are computed by selecting the minimal aggregated value at each pixel. some local algorithms however combine steps and and use a matching cost that is based on a support region e.g. normalized cross-correlation bolles baker and hannah and the rank transform and woodfill and other ordinal measures and nayar can also be viewed as a preprocessing step see global algorithms on the other hand make explicit smoothness assumptions and then solve a a global optimization problem such algorithms typically do not perform an aggregation step but rather seek a disparity assignment that minimizes a global cost function that consists of data terms and smoothness terms. the main distinctions among these algorithms is the minimization procedure used e.g. simulated annealing mitter and poggio barnard probabilistic diffusion and szeliski expectation maximization natarajan and tomasi graph cuts veksler and zabih or loopy belief propagation zheng and shum to name just a few. in between these two broad classes are certain iterative algorithms that do not explicitly specify a global function to be minimized but whose behavior mimics closely that of iterative optimization algorithms and poggio zitnick and kanade hierarchical algorithms resemble such iterative algorithms but typically operate on an image pyramid where results from coarser levels are used to constrain a more local search at finer levels terzopoulos and kass quam bergen anandan hanna et al. similarity measures the first component of any dense stereo matching algorithm is a similarity measure that compares pixel values in order to determine how likely they are to be in correspondence. in this section we briefly review the similarity measures introduced in section and mention a dense correspondence few others that have been developed specifically for stereo matching and szeliski hirschm uller and scharstein the most common pixel-based matching costs include sums of squared intensity differences and absolute intensity differences in the video processing community these matching criteria are referred to as the mean-squared error and mean absolute difference measures the term displaced frame difference is also often used more recently robust measures including truncated quadratics and contaminated gaussians have been proposed and anandan black and rangarajan scharstein and szeliski these measures are useful because they limit the influence of mismatches during aggregation. vaish szeliski zitnick et al. compare a number of such robust measures including a new one based on the entropy of the pixel values at a particular disparity hypothesis kang uyttendaele et al. which is particularly useful in multi-view stereo. other traditional matching costs include normalized cross-correlation bolles baker and hannah evangelidis and psarakis which behaves similarly to sum-of-squared-differences and binary matching costs match or no match and poggio based on binary features such as edges and binford grimson or the sign of the laplacian because of their poor discriminability simple binary matching costs are no longer used in dense stereo matching. some costs are insensitive to differences in camera gain or bias for example gradientbased measures scharstein phase and filter-bank responses and poggio kass jenkin jepson and tsotsos jones and malik filters that remove regular or robust filtered means castano and matthies hirschm uller and scharstein dense feature descriptor lepetit and fua and non-parametric measures such as rank and census transforms and woodfill ordinal measures and nayar or entropy kang uyttendaele et al. zitnick and kang the census transform which converts each pixel inside a moving window into a bit vector representing which neighbors are above or below the central pixel was found by hirschm uller and scharstein to be quite robust against large-scale nonstationary exposure and illumination changes. it is also possible to correct for differing global camera characteristics by performing a preprocessing or iterative refinement step that estimates inter-image bias gain variations using global regression histogram equalization roy and hingorani or mutual information kolmogorov and zabih hirschm uller local smoothly varying compensation fields have also been proposed tuytelaars and van gool zhang mcmillan and yu in order to compensate for sampling issues i.e. dramatically different pixel values in computer vision algorithms and applications draft high-frequency areas birchfield and tomasi proposed a matching cost that is less sensitive to shifts in image sampling. rather than just comparing pixel values shifted by integral amounts may miss a valid match they compare each pixel in the reference image against a linearly interpolated function of the other image. more detailed studies of these and additional matching costs are explored in and scharstein hirschm uller and scharstein in particular if you expect there to be significant exposure or appearance variation between images that you are matching some of the more robust measures that performed well in the evaluation by hirschm uller and scharstein such as the census transform and woodfill ordinal measures and nayar bilateral subtraction castano and matthies or hierarchical mutual information uller should be used. local methods local and window-based methods aggregate the matching cost by summing or averaging over a support region in the dsi cx y a support region can be either two-dimensional at a fixed disparity fronto-parallel surfaces or three-dimensional in x-y-d space slanted surfaces. two-dimensional evidence aggregation has been implemented using square windows or gaussian convolution multiple windows anchored at different points i.e. shiftable windows fusiello roberto and trucco bobick and intille windows with adaptive sizes and kanade kanade and okutomi kang szeliski and chai veksler windows based on connected components of constant disparity veksler and zabih or the results of color-based segmentation and kweon tombari mattoccia di stefano et al. three-dimensional support functions that have been proposed include limited disparity difference limited disparity gradient mayhew and frisby prazdny s coherence principle and the more recent work includes visibility and occlusion reasoning by zitnick and kanade aggregation with a fixed support region can be performed using or convolution cx y d wx y d y d or in the case of rectangular windows using efficient moving average box-filters yoshida oda et al. kimura shinbo yamaguchi et al. shiftable windows can also be implemented efficiently using a separable sliding min-filter and szeliski section selecting among windows of different shapes and sizes can be performed more efficiently by first computing a summed area for two recent surveys and comparisons of such techniques please see the work of gong yang wang et al. and tombari mattoccia di stefano et al. local methods figure shiftable window and szeliski springer. the effect of trying all shifted windows around the black pixel is the same as taking the minimum matching score across all centered windows in the same neighborhood. clarity only three of the neighboring shifted windows are shown here. figure aggregation window sizes and weights adapted to image content mattoccia di stefano et al. ieee original image with selected evaluation points variable windows adaptive weights and kweon segmentation-based mattoccia and di stefano notice how the adaptive weights and segmentation-based techniques adapt their support to similarly colored pixels. table selecting the right window is important since windows must be large enough to contain sufficient texture and yet small enough so that they do not straddle depth discontinuities an alternative method for aggregation is iterative diffusion i.e. repeatedly adding to each pixel s cost the weighted values of its neighboring pixels costs and hinton shah scharstein and szeliski of the local aggregation methods compared by gong yang wang et al. and tombari mattoccia di stefano et al. the fast variable window approach of veksler and the locally weighting approach developed by yoon and kweon consistently stood out as having the best tradeoff between performance and the local weighting technique in particular is interesting because instead of using square windows with uniform weighting each pixel within an aggregation window influences the final match more recent and extensive results from tombari mattoccia di stefano et al. can be found at http computer vision algorithms and applications draft ing cost based on its color similarity and spatial distance just as in bilinear filtering fact their aggregation step is closely related to doing a joint bilateral filter on the colordisparity image except that it is done symmetrically in both reference and target images. the segmentation-based aggregation method of tombari mattoccia and di stefano did even better although a fast implementation of this algorithm does not yet exist. in local methods the emphasis is on the matching cost computation and cost aggregation steps. computing the final disparities is trivial simply choose at each pixel the disparity associated with the minimum cost value. thus these methods perform a local winnertake-all optimization at each pixel. a limitation of this approach many other correspondence algorithms is that uniqueness of matches is only enforced for one image reference image while points in the other image might match multiple points unless cross-checking and subsequent hole filling is used hirschm uller and scharstein sub-pixel estimation and uncertainty most stereo correspondence algorithms compute a set of disparity estimates in some discretized space e.g. for integer disparities include continuous optimization techniques such as optical flow anandan hanna et al. or splines and coughlan for applications such as robot navigation or people tracking these may be perfectly adequate. however for image-based rendering such quantized maps lead to very unappealing view synthesis results i.e. the scene appears to be made up of many thin shearing layers. to remedy this situation many algorithms apply a sub-pixel refinement stage after the initial discrete correspondence stage. alternative is to simply start with more discrete disparity levels and scharstein sub-pixel disparity estimates can be computed in a variety of ways including iterative gradient descent and fitting a curve to the matching costs at discrete disparity levels gray and hunt lucas and kanade tian and huhns matthies kanade and szeliski kanade and okutomi this provides an easy way to increase the resolution of a stereo algorithm with little additional computation. however to work well the intensities being matched must vary smoothly and the regions over which these estimates are computed must be on the same surface. recently some questions have been raised about the advisability of fitting correlation curves to integer-sampled matching costs and okutomi this situation may even be worse when sampling-insensitive dissimilarity measures are used and tomasi these issues are explored in more depth by szeliski and scharstein besides sub-pixel computations there are other ways of post-processing the computed disparities. occluded areas can be detected using cross-checking i.e. comparing left-to local methods figure uncertainty in stereo depth estimation input image estimated depth map is closer estimated confidencered is higher. as you can see more textured areas have higher confidence. right and right-to-left disparity maps a median filter can be applied to clean up spurious mismatches and holes due to occlusion can be filled by surface fitting or by distributing neighboring disparity estimates and tomasi scharstein hirschm uller and scharstein another kind of post-processing which can be useful in later processing stages is to associate confidences with per-pixel depth estimates which can be done by looking at the curvature of the correlation surface i.e. how strong the minimum in the dsi image is at the winning disparity. matthies kanade and szeliski show that under the assumption of small noise photometrically calibrated images and densely sampled disparities the variance of a local depth estimate can be estimated as v ard i a where a is the curvature of the dsi as a function of d which can be measured using a local parabolic fit or by squaring all the horizontal gradients in the window and i is the variance of the image noise which can be estimated from the minimum ssd score. also section and appendix application stereo-based head tracking a common application of real-time stereo algorithms is for tracking the position of a user interacting with a computer or game system. the use of stereo can dramatically improve the reliability of such a system compared to trying to use monocular color and intensity information gordon harville et al. once recovered this information can be used in a variety of applications including controlling a virtual environment or game computer vision algorithms and applications draft correcting the apparent gaze during video conferencing and background replacement. we discuss the first two applications below and defer the discussion of background replacement to section the use of head tracking to control a user s virtual viewpoint while viewing a object or environment on a computer monitor is sometimes called fish tank virtual reality since the user is observing a world as if it were contained inside a fish tank arthur and booth early versions of these systems used mechanical head tracking devices and stereo glasses. today such systems can be controlled using stereo-based head tracking and stereo glasses can be replaced with autostereoscopic displays. head tracking can also be used to construct a virtual mirror where the user s head can be modified in real-time using a variety of visual effects baker crow et al. another application of stereo head tracking and reconstruction is in gaze correction lewis and cox when a user participates in a desktop video-conference or video chat the camera is usually placed on top of the monitor. since the person is gazing at a window somewhere on the screen it appears as if they are looking down and away from the other participants instead of straight at them. replacing the single camera with two or more cameras enables a virtual view to be constructed right at the position where they are looking resulting in virtual eye contact. real-time stereo matching is used to construct an accurate head model and view interpolation is used to synthesize the novel in-between view shotton blake et al. global optimization global stereo matching methods perform some optimization or iteration steps after the disparity computation phase and often skip the aggregation step altogether because the global smoothness constraints perform a similar function. many global methods are formulated in an energy-minimization framework where as we saw in sections and the objective is to find a solution d that minimizes a global energy ed edd esd. the data term edd measures how well the disparity function d agrees with the input image pair. using our previously defined disparity space image we define this energy as edd cx y dx y where c is the or aggregated matching cost dsi. the smoothness term esd encodes the smoothness assumptions made by the algorithm. to make the optimization computationally tractable the smoothness term is often restricted global optimization to measuring only the differences between neighboring pixels disparities esd y dx y y dx y where is some monotonically increasing function of disparity difference. it is also possible to use larger neighborhoods such as which can lead to better boundaries and kolmogorov or to use second-order smoothness terms reid torr et al. but such terms require more complex optimization techniques. an alternative to smoothness functionals is to use a lower-dimensional representation such as splines and coughlan in standard regularization is a quadratic function which makes d smooth everywhere and may lead to poor results at object boundaries. energy functions that do not have this problem are called discontinuity-preserving and are based on robust functions black and rangarajan the seminal paper by geman and geman gave a bayesian interpretation of these kinds of energy functions and proposed a discontinuity-preserving energy function based on markov random fields and additional line processes which are additional binary variables that control whether smoothness penalties are enforced or not. black and rangarajan show how independent line process variables can be replaced by robust pairwise disparity terms. the terms in es can also be made to depend on the intensity differences e.g. ddx y dx y y ix where i is some monotonically decreasing function of intensity differences that lowers smoothness costs at high-intensity gradients. this idea and poggio fua bobick and intille boykov veksler and zabih encourages disparity discontinuities to coincide with intensity or color edges and appears to account for some of the good performance of global optimization approaches. while most researchers set these functions heuristically scharstein and pal show how the free parameters in such conditional random fields can be learned from ground truth disparity maps. once the global energy has been defined a variety of algorithms can be used to find a minimum. traditional approaches associated with regularization and markov random fields include continuation and zisserman simulated annealing and geman marroquin mitter and poggio barnard highest confidence first and brown and mean-field annealing and girosi more recently max-flow and graph cut methods have been proposed to solve a special class of global optimization problems and cox boykov veksler and zabih ishikawa such methods are more efficient than simulated annealing and have produced good results as have techniques based on loopy belief propagation zheng and shum computer vision algorithms and applications draft tappen and freeman appendix and a recent survey paper on mrf inference zabih scharstein et al. discuss and compare such techniques in more detail. while global optimization techniques currently produce the best stereo matching results there are some alternative approaches worth studying. cooperative algorithms. cooperative algorithms inspired by computational models of human stereo vision were among the earliest methods proposed for disparity computation marr and poggio marroquin szeliski and hinton zitnick and kanade such algorithms iteratively update disparity estimates using non-linear operations that result in an overall behavior similar to global optimization algorithms. in fact for some of these algorithms it is possible to explicitly state a global function that is being minimized and szeliski coarse-to-fine and incremental warping. most of today s best algorithms first enumerate all possible matches at all possible disparities and then select the best set of matches in some way. faster approaches can sometimes be obtained using methods inspired by classic optical flow computation. here images are successively warped and disparity estimates incrementally updated until a satisfactory registration is achieved. these techniques are most often implemented within a coarse-to-fine hierarchical refinement framework bergen anandan hanna et al. barron fleet and beauchemin szeliski and coughlan dynamic programming a different class of global optimization algorithm is based on dynamic programming. while the optimization of equation can be shown to be np-hard for common classes of smoothness functions dynamic programming can find the global minimum for independent scanlines in polynomial time. dynamic programming was first used for stereo vision in sparse edge-based methods and binford ohta and kanade more recent approaches have focused on the dense scanline matching problem geiger ladendorf and yuille cox hingorani rao et al. bobick and intille birchfield and tomasi these approaches work by computing the minimum-cost path through the matrix of all pairwise matching costs between two corresponding scanlines i.e. through a horizontal slice of the dsi. partial occlusion is handled explicitly by assigning a group of pixels in one image to a single pixel in the other image. figure schematically shows how dp works while figure shows a real dsi slice over which the dp is applied. global optimization figure stereo matching using dynamic programming as illustrated by scharstein and szeliski springer and kolmogorov criminisi blake et al. ieee. for each pair of corresponding scanlines a minimizing path through the matrix of all pairwise matching costs is selected. lowercase letters k symbolize the intensities along each scanline. uppercase letters represent the selected path through the matrix. matches are indicated by m while partially occluded points have a fixed cost are indicated by l or r corresponding to points only visible in the left or right images respectively. usually only a limited disparity range is considered in the figure indicated by the non-shaded squares. the representation in allows for diagonal moves while the representation in does not. note that these diagrams which use the cyclopean representation of depth i.e. depth relative to a camera between the two input cameras show an unskewed x-d slice through the dsi. to implement dynamic programming for a scanline y each entry in a cost matrix dm n is computed by combining its dsi value n cm n m n y with one of its predecessor cost values. using the representation shown in figure which allows for diagonal moves the aggregated match costs can be recursively computed as dm n m mindm n m dm n l dm n r n dm n l mindm n m dm n l o dm n r mindm n m dm n r o where o is a per-pixel occlusion cost. the aggregation rules corresponding to figure are given by kolmogorov criminisi blake et al. who also use a two-state foreground background model for bi-layer segmentation. cdefgkaleft scanlineiright computer vision algorithms and applications draft problems with dynamic programming stereo include the selection of the right cost for occluded pixels and the difficulty of enforcing inter-scanline consistency although several methods propose ways of addressing the latter and kanade belhumeur cox hingorani rao et al. bobick and intille birchfield and tomasi kolmogorov criminisi blake et al. another problem is that the dynamic programming approach requires enforcing the monotonicity or ordering constraint and poggio this constraint requires that the relative ordering of pixels on a scanline remain the same between the two views which may not be the case in scenes containing narrow foreground objects. an alternative to traditional dynamic programming introduced by scharstein and szeliski is to neglect the vertical smoothness constraints in and simply optimize independent scanlines in the global energy function which can easily be done using a recursive algorithm dx y d cx y d min y dd the advantage of this scanline optimization algorithm is that it computes the same representation and minimizes a reduced version of the same energy function as the full energy function unfortunately it still suffers from the same streaking artifacts as dynamic programming. a much better approach is to evaluate the cumulative cost function from multiple directions e.g from the eight cardinal directions n e w s ne se sw nw uller the resulting semi-global optimization performs quite well and is extremely efficient to implement. even though dynamic programming and scanline optimization algorithms do not generally produce the most accurate stereo reconstructions when combined with sophisticated aggregation strategies they can produce very fast and high-quality results. segmentation-based techniques while most stereo matching algorithms perform their computations on a per-pixel basis some of the more recent techniques first segment the images into regions and then try to label each region with a disparity. for example tao sawhney and kumar segment the reference image estimate per-pixel disparities using a local technique and then do local plane fits inside each segment before applying smoothness constraints between neighboring segments. zitnick kang uyttendaele et al. and zitnick and kang use over-segmentation to mitigate initial bad segmentations. after a set of initial cost values for each segment has been stored into a disparity space distribution iterative relaxation loopy belief propagation in the global optimization figure segmentation-based stereo matching kang uyttendaele et al. acm input color image color-based segmentation initial disparity estimates final piecewise-smoothed disparities mrf neighborhood defined over the segments in the disparity space distribution and kang springer. figure stereo matching with adaptive over-segmentation and matting wilburn and zitnick ieee segment boundaries are refined during the optimization leading to more accurate results the thin green leaf in the bottom row alpha mattes are extracted at segment boundaries which leads to visually better compositing results column. more recent work of zitnick and kang is used to adjust the disparity estimates for each segment as shown in figure taguchi wilburn and zitnick refine the segment shapes as part of the optimization process which leads to much improved results as shown in figure even more accurate results are obtained by klaus sormann and karner who first segment the reference image using mean shift run a small sad plus gradient sad by cross-checking to get initial disparity estimates fit local planes re-fit with global planes and then run a final mrf on plane assignments with loopy belief propagation. when the algorithm was first introduced in it was the top ranked algorithm on the evaluation site at httpvision.middlebury.edustereo in early it still had the top rank on the new evaluation datasets. the highest ranked algorithm by wang and zheng follows a similar approach of segmenting the image doing local plane fits and then performing cooperative optimization computer vision algorithms and applications draft of neighboring plane fit parameters. another highly ranked algorithm by yang wang yang et al. uses the color correlation approach of yoon and kweon and hierarchical belief propagation to obtain an initial set of disparity estimates. after left right consistency checking to detect occluded pixels the data terms for low-confidence and occluded pixels are recomputed using segmentation-based plane fits and one or more rounds of hierarchical belief propagation are used to obtain the final disparity estimates. another important ability of segmentation-based stereo algorithms which they share with algorithms that use explicit layers szeliski and anandan szeliski and golland or boundary extraction kang and szeliski is the ability to extract fractional pixel alpha mattes at depth discontinuities gelautz rother et al. this ability is crucial when attempting to create virtual view interpolation without clinging boundary or tearing artifacts kang uyttendaele et al. and also to seamlessly insert virtual objects wilburn and zitnick as shown in figure since new stereo matching algorithms continue to be introduced every year it is a good idea to periodically check the middlebury evaluation site at httpvision.middlebury.edu stereo for a listing of the most recent algorithms to be evaluated. application z-keying and background replacement another application of real-time stereo matching is z-keying which is the process of segmenting a foreground actor from the background using depth information usually for the purpose of replacing the background with some computer-generated imagery as shown in figure originally z-keying systems required expensive custom-built hardware to produce the desired depth maps in real time and were therefore restricted to broadcast studio applications yoshida oda et al. iddan and yahav off-line systems were also developed for estimating multi-viewpoint geometry from video streams rander and narayanan carranza theobalt magnor et al. zitnick kang uyttendaele et al. vedula baker and kanade recent advances in highly accurate real-time stereo matching however now make it possible to perform z-keying on regular pcs enabling desktop videoconferencing applications such as those shown in figure criminisi blake et al. multi-view stereo while matching pairs of images is a useful way of obtaining depth information matching more images can lead to even better results. in this section we review not only techniques for multi-view stereo figure background replacement using z-keying with a bi-layer segmentation algorithm criminisi blake et al. ieee. creating complete object models but also simpler techniques for improving the quality of depth maps using multiple source images. as we saw in our discussion of plane sweep it is possible to resample all neighboring k images at each disparity hypothesis d into a generalized disparity space volume ix y d k. the simplest way to take advantage of these additional images is to sum up their differences from the reference image ir as in cx y d ix y d k irx y. this is the basis of the well-known sum of summed-squared-difference and ssad approaches and kanade kang webb zitnick et al. which can be extended to reason about likely patterns of occlusion matsuura satoh et al. more recent work by gallup frahm mordohai et al. show how to adapt the baselines used to the expected depth in order to get the best tradeoff between geometric accuracy baseline and robustness to occlusion baseline. alternative multi-view cost metrics include measures such as synthetic focus sharpness and the entropy of the pixel color distribution szeliski zitnick et al. a useful way to visualize the multi-frame stereo estimation problem is to examine the epipolar plane image formed by stacking corresponding scanlines from all the images as shown in figures and baker and marimont baker and bolles baker as you can see in figure as a camera translates horizontally a standard horizontally rectified geometry objects at different depths move sideways at a rate inversely proportional to their depth foreground objects occlude background objects which can be seen as epi-strips kang swaminathan et al. occluding other the four-dimensional generalization of the epi is the light field which we study in section in principle computer vision algorithms and applications draft figure epipolar plane image grzeszczuk szeliski et al. acm and a schematic epi szeliski and chai ieee. the lumigraph field is the space of all light rays passing through a volume of space. taking a slice results in all of the light rays embedded in a plane and is equivalent to a scanline taken from a stacked epi volume. objects at different depths move sideways with velocities proportional to their inverse depth. occlusion translucency effects can easily be seen in this representation. the epi corresponding to figure showing the three images left and right as slices through the epi volume. the spatially and temporally shifted window around the black pixel is indicated by the rectangle showing the right image is not being used in matching. strips in the epi. if we are given a dense enough set of images we can find such strips and reason about their relationships in order to both reconstruct the scene and make inferences about translucent objects kang and szeliski and specular reflections kang szeliski et al. criminisi kang swaminathan et al. alternatively we can treat the series of images as a set of sequential observations and merge them using kalman filtering kanade and szeliski or maximum likelihood inference when fewer images are available it becomes necessary to fall back on aggregation techniques such as sliding windows or global optimization. with additional input images however the likelihood of occlusions increases. it is therefore prudent to adjust not only the best window locations using a shiftable window approach as shown in figure but also to optionally select a subset of neighboring frames in order to discount those images where the region of interest is occluded as shown in figure szeliski and chai there is enough information in a light field to recover both the shape and the brdf of objects yezzi and jin although relatively little progress has been made to date on this topic. aedcbleftmiddlerightfxt multi-view stereo figure spatio-temporally shiftable windows szeliski and chai ieee a simple three-image sequence middle image is the reference image which has a moving frontal gray square f and a stationary background. regions b c d and e are partially occluded. a regular ssd algorithm will make mistakes when matching pixels in these regions the window centered on the black pixel in region b and in windows straddling depth discontinuities window centered on the white pixel in region f. shiftable windows help mitigate the problems in partially occluded regions and near depth discontinuities. the shifted window centered on the white pixel in region f matches correctly in all frames. the shifted window centered on the black pixel in region b matches correctly in the left image but requires temporal selection to disable matching the right image. figure shows an epi corresponding to this sequence and describes in more detail how temporal selection works. shows how such spatio-temporal selection or shifting of windows corresponds to selecting the most likely un-occluded volumetric region in the epipolar plane image volume. the results of applying these techniques to the multi-frame flower garden image sequence are shown in figure which compares the results of using regular sssd with spatially shifted windows and full spatio-temporal window selection. task of applying stereo to a rigid scene filmed with a moving camera is sometimes called motion stereo. similar improvements from using spatio-temporal selection are reported by and szeliski and are evident even when local measurements are combined with global optimization. while computing a depth map from multiple inputs outperforms pairwise stereo matching even more dramatic improvements can be obtained by estimating multiple depth maps simultaneously kang and szeliski the existence of multiple depth maps enables more accurate reasoning about occlusions as regions which are occluded in one image may be visible matchable in others. the multi-view reconstruction problem can be formulated as the simultaneous estimation of depth maps at key frames while maximizing not only photoconsistency and piecewise disparity smoothness but also the consistency between disparity estimates at different frames. while szeliski and kang abcdefabcdefabcdefleftmiddlerightabcdefabcdefabcdefleftmiddleright computer vision algorithms and applications draft figure local window-based matching results szeliski and chai ieee window that is not spatially perturbed spatially perturbed window using the best five of neighboring frames using the better half sequence. notice how the results near the tree trunk are improved using temporal selection. and szeliski use soft constraints to encourage multiple disparity maps to be consistent kolmogorov and zabih show how such consistency measures can be encoded as hard constraints which guarantee that the multiple depth maps are not only similar but actually identical in overlapping regions. newer algorithms that simultaneously estimate multiple disparity maps include papers by maitre shinagawa and do and zhang jia wong et al. a closely related topic to multi-frame stereo estimation is scene flow in which multiple cameras are used to capture a dynamic scene. the task is then to simultaneously recover the shape of the object at every instant in time and to estimate the full motion of every surface point between frames. representative papers in this area include those by vedula baker rander et al. zhang and kambhamettu pons keriven and faugeras huguet and devernay and wedel rabe vaudrey et al. figure shows an image of the scene flow for the tango dancer shown in figure j while figure shows scene flows captured from a moving vehicle for the purpose of obstacle avoidance. in addition to supporting mensuration and safety applications scene flow can be used to support both spatial and temporal view interpolation as demonstrated by vedula baker and kanade volumetric and surface reconstruction according to seitz curless diebel et al. the goal of multi-view stereo is to reconstruct a complete object model from a collection of images taken from known camera viewpoints. the most challenging but potentially most useful variant of multi-view stereo reconstruction is to create globally consistent models. this topic has a long history in computer vision starting with surface mesh reconstruction techniques such as the one developed by multi-view stereo figure three-dimensional scene flow computed from a multi-camera dome surrounding the dancer shown in figure j baker rander et al. ieee computed from stereo cameras mounted on a moving vehicle rabe vaudrey et al. springer. fua and leclerc a variety of approaches and representations have been used to solve this problem including voxel representations and dyer szeliski and golland de bonet and viola kutulakos and seitz eisert steinbach and girod slabaugh culbertson slabaugh et al. sinha and pollefeys vogiatzis hernandez torr et al. hiep keriven pons et al. level sets and keriven pons keriven and faugeras polygonal meshes and leclerc narayanan rander and kanade hernandez and schmitt furukawa and ponce and multiple depth maps and zabih figure shows representative examples of object models reconstructed using some of these techniques. in order to organize and compare all these techniques seitz curless diebel et al. developed a six-point taxonomy that can help classify algorithms according to the scene representation photoconsistency measure visibility model shape priors reconstruction algorithm and initialization requirements they use. below we summarize some of these choices and list a few representative papers. for more details please consult the full survey paper curless diebel et al. and the evaluation web site httpvision.middlebury.edu mview which contains pointers to even more recent papers and results. scene representation. one of the more popular representations is a uniform grid of which can be reconstructed using a variety of carving and dyer kutulakos and seitz or optimization and pollefeys vogiatzis hernandez torr et al. hiep keriven pons et al. techniques. level set techniques also operate on a uniform grid but instead of representing a binary occupancy map they represent the signed distance to the surface and keriven pons keriven and faugeras which can encode a finer level of detail. polygonal meshes are another pop for outdoor scenes that go to infinity a non-uniform gridding of space may be preferable culbertson slabaugh et al. computer vision algorithms and applications draft figure multi-view stereo algorithms surface-based stereo and leclerc voxel coloring and dyer springer depth map merging rander and kanade level set evolution and keriven ieee silhouette and stereo fusion and schmitt elsevier multi-view image matching keriven and faugeras ieee volumetric graph cut torr and cipolla ieee carved visual hulls and ponce springer. ular representation and leclerc narayanan rander and kanade isidoro and sclaroff hernandez and schmitt furukawa and ponce hiep keriven pons et al. meshes are the standard representation used in computer graphics and also readily support the computation of visibility and occlusions. finally as we discussed in the previous section multiple depth maps can also be used kolmogorov and zabih kang and szeliski many algorithms also use more than a single representation e.g. they may start by computing multiple depth maps and then merge them into a object model rander and kanade furukawa and ponce goesele curless and seitz goesele snavely curless et al. furukawa curless seitz et al. multi-view stereo photoconsistency measure. as we discussed in a variety of similarity measures can be used to compare pixel values in different images including measures that try to discount illumination effects or be less sensitive to outliers. in multi-view stereo algorithms have a choice of computing these measures directly on the surface of the model i.e. in scene space or projecting pixel values from one image from a textured model back into another image i.e. in image space. latter corresponds more closely to a bayesian approach since input images are noisy measurements of the colored model. the geometry of the object i.e. its distance to each camera and its local surface normal when available can be used to adjust the matching windows used in the computation to account for foreshortening and scale change snavely curless et al. visibility model. a big advantage that multi-view stereo algorithms have over single-depthmap approaches is their ability to reason in a principled manner about visibility and occlusions. techniques that use the current state of the model to predict which surface pixels are visible in each image and seitz faugeras and keriven vogiatzis hernandez torr et al. hiep keriven pons et al. are classified as using geometric visibility models in the taxonomy of seitz curless diebel et al. techniques that select a neighboring subset of image to match are called quasi-geometric rander and kanade kang and szeliski hernandez and schmitt while techniques that use traditional robust similarity measures are called outlier-based. while full geometric reasoning is the most principled and accurate approach it can be very slow to evaluate and depends on the evolving quality of the current surface estimate to predict visibility which can be a bit of a chicken-and-egg problem unless conservative assumptions are used as they are by kutulakos and seitz shape priors. because stereo matching is often underconstrained especially in textureless regions most matching algorithms adopt explicitly or implicitly some form of prior model for the expected shape. many of the techniques that rely on optimization use a smoothness or area-based photoconsistency constraint which because of the natural tendency of smooth surfaces to shrink inwards often results in a minimal surface prior and keriven sinha and pollefeys vogiatzis hernandez torr et al. approaches that carve away the volume of space often stop once a photoconsistent solution is found and dyer kutulakos and seitz which corresponds to a maximal surface bias i.e. these techniques tend to over-estimate the true shape. finally multiple depth map approaches often adopt traditional image-based smoothness constraints. reconstruction algorithm. the details of how the actual reconstruction algorithm proceeds is where the largest variety and greatest innovation in multi-view stereo algorithms computer vision algorithms and applications draft can be found. some approaches use global optimization defined over a three-dimensional photoconsistency volume to recover a complete surface. approaches based on graph cuts use polynomial complexity binary segmentation algorithms to recover the object model defined on the voxel grid and pollefeys vogiatzis hernandez torr et al. hiep keriven pons et al. level set approaches use a continuous surface evolution to find a good minimum in the configuration space of potential surfaces and therefore require a reasonably good initialization and keriven pons keriven and faugeras in order for the photoconsistency volume to be meaningful matching costs need to be computed in some robust fashion e.g. using sets of limited views or by aggregating multiple depth maps. an alternative approach to global optimization is to sweep through the volume while computing both photoconsistency and visibility simultaneously. the voxel coloring algorithm of seitz and dyer performs a front-to-back plane sweep. on every plane any voxels that are sufficiently photoconsistent are labeled as part of the object. the corresponding pixels in the source images can then be erased since they are already accounted for and therefore do not contribute to further photoconsistency computations. similar approach albeit without the front-to-back sweep order is used by szeliski and golland the resulting volume under noise- and resampling-free conditions is guaranteed to produce both a photoconsistent model and to enclose whatever true object model generated the images. unfortunately voxel coloring is only guaranteed to work if all of the cameras lie on the same side of the sweep planes which is not possible in general ring configurations of cameras. kutulakos and seitz generalize voxel coloring to space carving where subsets of cameras that satisfy the voxel coloring constraint are iteratively selected and the voxel grid is alternately carved away along different axes. another popular approach to multi-view stereo is to first independently compute multiple depth maps and then merge these partial maps into a complete model. approaches to depth map merging which are discussed in more detail in section include signed distance functions and levoy used by goesele curless and seitz and poisson surface reconstruction bolitho and hoppe used by goesele snavely curless et al. it is also possible to reconstruct sparser representations such as points and lines and to interpolate them to full surfaces initialization requirements. one final element discussed by seitz curless diebel et al. is the varying degrees of initialization required by different algorithms. because some algorithms refine or evolve a rough model they require a reasonably accurate overcomplete initial model which can often be obtained by reconstructing a volume from object multi-view stereo figure the multi-view stereo data sets captured by seitz curless diebel et al. springer. only and are currently used for evaluation. silhouettes as discussed in section however if the algorithm performs a global optimization klodt brox et al. kolev and cremers this dependence on initialization is not an issue. empirical evaluation. in order to evaluate the large number of design alternatives in multiview stereo seitz curless diebel et al. collected a dataset of calibrated images using a spherical gantry. a representative image from each of the six datasets is shown in figure although only the first two datasets have as yet been fully processed and used for evaluation. figure shows the results of running seven different algorithms on the temple dataset. as you can see most of the techniques do an impressive job of capturing the fine details in the columns although it is also clear that the techniques employ differing amounts of smoothing to achieve these results. since the publication of the survey by seitz curless diebel et al. the field of multi-view stereo has continued to advance at a rapid pace fransens and van gool hernandez vogiatzis and cipolla habbecke and kobbelt furukawa and ponce vogiatzis hernandez torr et al. goesele snavely curless et al. sinha mordohai and pollefeys gargallo prados and sturm merrell akbarzadeh wang et al. zach pock and bischof furukawa and ponce hornung zeng and kobbelt bradley boubekeur and heidrich zach campbell vogiatzis hern andez et al. kolev klodt brox et al. hiep keriven pons et al. furukawa curless seitz et al. the multi-view stereo evaluation site httpvision.middlebury.edumview provides quantitative results for these algorithms along with pointers to where to find these papers. shape from silhouettes in many situations performing a foreground background segmentation of the object of interest is a good way to initialize or fit a model shakhnarovich and darrell computer vision algorithms and applications draft figure reconstruction results for seven algorithms and schmitt furukawa and ponce pons keriven and faugeras goesele curless and seitz vogiatzis torr and cipolla tran and davis kolmogorov and zabih evaluated by seitz curless diebel et al. on the temple ring dataset. the numbers underneath each detail image are the accuracy of each of these techniques measured in millimeters. vlasic baran matusik et al. or to impose a convex set of constraints on multiview stereo and cremers over the years a number of techniques have been developed to reconstruct a volumetric model from the intersection of the binary silhouettes projected into the resulting model is called a visual hull sometimes a line hull analogous with the convex hull of a set of points since the volume is maximal with respect to the visual silhouettes and surface elements are tangent to the viewing rays along the silhouette boundaries it is also possible to carve away a more accurate reconstruction using multi-view stereo and pollefeys or by analyzing cast shadows andreetto rushmeier et al. some techniques first approximate each silhouette with a polygonal representation and then intersect the resulting faceted conical regions in three-space to produce polyhedral models martin and aggarwal matusik buehler and mcmillan which can later be refined using triangular splines and ponce other approaches use voxel-based representations usually encoded as octrees because of the resulting space time efficiency. figures b show an example of a octree model and its associated colored tree where black nodes are interior to the model white multi-view stereo figure volumetric octree reconstruction from binary silhouettes elsevier octree representation and its corresponding tree structure input image of an object on a turntable computed volumetric octree model. nodes are exterior and gray nodes are of mixed occupancy. examples of octree-based reconstruction approaches include those by potmesil noborio fukada and arimoto srivasan liang and hackwood and szeliski the approach of szeliski first converts each binary silhouette into a one-sided variant of a distance map where each pixel in the map indicates the largest square that is completely inside outside the silhouette. this makes it fast to project an octree cell into the silhouette to confirm whether it is completely inside or outside the object so that it can be colored black white or left as gray for further refinement on a smaller grid. the octree construction algorithm proceeds in a coarse-to-fine manner first building an octree at a relatively coarse resolution and then refining it by revisiting and subdividing all the input images for the gray cells whose occupancy has not yet been determined. figure shows the resulting octree model computed from a coffee cup rotating on a turntable. more recent work on visual hull computation borrows ideas from image-based rendering and is hence called an image-based visual hull buehler raskar et al. instead of precomputing a global model an image-based visual hull is recomputed for each new computer vision algorithms and applications draft viewpoint by successively intersecting viewing ray segments with the binary silhouettes in each image. this not only leads to a fast computation algorithm but also enables fast texturing of the recovered model with color values from the input images. this approach can also be combined with high-quality deformable templates to capture and re-animate whole body motion baran matusik et al. additional reading the field of stereo correspondence and depth estimation is one of the oldest and most widely studied topics in computer vision. a number of good surveys have been written over the years and poggio barnard and fischler dhond and aggarwal scharstein and szeliski brown burschka and hager seitz curless diebel et al. and they can serve as good guides to this extensive literature. because of computational limitations and the desire to find appearance-invariant correspondences early algorithms often focused on finding sparse correspondences marr and poggio mayhew and frisby baker and binford arnold grimson ohta and kanade bolles baker and marimont matthies kanade and szeliski hsieh mckeown and perlant bolles baker and hannah the topic of computing epipolar geometry and pre-rectifying images is covered in sections and and is also treated in textbooks on multi-view geometry and luong hartley and zisserman and articles specifically on this topic and murray zhang the concepts of the disparity space and disparity space image are often associated with the seminal work by marr and the papers of yang yuille and lu and intille and bobick the plane sweep algorithm was first popularized by collins and then generalized to a full arbitrary projective setting by szeliski and golland and saito and kanade plane sweeps can also be formulated using cylindrical surfaces yamamoto and tsuji kang and szeliski shum and szeliski li shum tang et al. zheng kang cohen et al. or even more general topologies once the topology for the cost volume or dsi has been set up we need to compute the actual photoconsistency measures for each pixel and potential depth. a wide range of such measures have been proposed as discussed in section some of these are compared in recent surveys and evaluations of matching costs and szeliski hirschm uller and scharstein to compute an actual depth map from these costs some form of optimization or selection criterion must be used. the simplest of these are sliding windows of various kinds which exercises are discussed in section and surveyed by gong yang wang et al. and tombari mattoccia di stefano et al. more commonly global optimization frameworks are used to compute the best disparity field as described in section these techniques include dynamic programming and truly global optimization algorithms such as graph cuts and loopy belief propagation. because the literature on this is so extensive it is described in more detail in section a good place to find pointers to the latest results in this field is the middlebury stereo vision page at httpvision.middlebury.edustereo. algorithms for multi-view stereo typically fall into two categories. the first include algorithms that compute traditional depth maps using several images for computing photoconsistency measures and kanade kang webb zitnick et al. nakamura matsuura satoh et al. szeliski and golland kang szeliski and chai vaish szeliski zitnick et al. gallup frahm mordohai et al. optionally some of these techniques compute multiple depth maps and use additional constraints to encourage the different depth maps to be consistent kolmogorov and zabih kang and szeliski maitre shinagawa and do zhang jia wong et al. the second category consists of papers that compute true volumetric or surface-based object models. again because of the large number of papers published on this topic rather than citing them here we refer you to the material in section the survey by seitz curless diebel et al. and the on-line evaluation web site at httpvision.middlebury. edumview. exercises ex stereo pair rectification implement the following simple algorithm rotate both cameras so that they are looking perpendicular to the line joining the two camera centers and the smallest rotation can be computed from the cross product between the original and desired optical axes. twist the optical axes so that the horizontal axis of each camera looks in the direction of the other camera. the cross product between the current x-axis after the first rotation and the line joining the cameras gives the rotation. if needed scale up the smaller detailed image so that it has the same resolution hence line-to-line correspondence as the other image. now compare your results to the algorithm proposed by loop and zhang can you think of situations where their approach may be preferable? computer vision algorithms and applications draft ex rigid direct alignment modify your spline-based or optical flow motion estimator from exercise to use epipolar geometry i.e. to only estimate disparity. extend your algorithm to simultaneously estimate the epipolar geometry first using point correspondences by estimating a base homography corresponding to a reference plane for the dominant motion and then an epipole for the residual parallax ex shape from profiles reconstruct a surface model from a series of edge images extract edges and link them based on previously computed epipolar geometry match up edges in triplets longer sets of images. reconstruct the locations of the curves using osculating circles render the resulting surface model as a sparse mesh i.e. drawing the reconstructed profile curves and links between points in neighboring images with similar osculating circles. ex plane sweep implement a plane sweep algorithm if the images are already pre-rectified this consists simply of shifting images relative to each other and comparing pixels. if the images are not pre-rectified compute the homography that resamples the target image into the reference image s coordinate system for each plane. evaluate a subset of the following similarity measures and compare their performance by visualizing the disparity space image which should be dark for pixels at correct depths squared difference absolute difference truncated or robust measures gradient differences rank or census transform latter usually performs better mutual information from a pre-computed joint density function. consider using the birchfield and tomasi technique of comparing ranges between neighboring pixels shifted or warped images. also try pre-compensating images for bias or gain variations using one or more of the techniques discussed in section exercises ex aggregation and window-based stereo implement one or more of the matching cost aggregation strategies described in section convolution with a box or gaussian kernel shifting window locations by applying a min filter and szeliski picking a window that maximizes some match-reliability metric weighting pixels by their similarity to the central pixel and kweon once you have aggregated the costs in the dsi pick the winner at each pixel and then optionally perform one or more of the following post-processing steps compute matches both ways and pick only the reliable matches the others in another color tag matches that are unsure confidence is too low fill in the matches that are unsure from neighboring values refine your matches to sub-pixel disparity by either fitting a parabola to the dsi values around the winner or by using an iteration of lukas kanade. ex optimization-based stereo compute the disparity space image volume using one of the techniques you implemented in exercise and then implement one more of the global optimization techniques described in section to compute the depth map. potential choices include dynamic programming or scanline optimization easy semi-global optimization uller which is a simple extension of scanline optimization and performs well graph cuts using alpha expansions veksler and zabih for which you will need to find a max-flow or min-cut algorithm loopy belief propagation evaluate your algorithm by running it on the middlebury stereo data sets. how well does your algorithm do against local aggregation and kweon can you think of some extensions or modifications to make it even better? computer vision algorithms and applications draft ex view interpolation revisited compute a dense depth map using one of the techniques you developed above and use it better yet a depth map for each source image to generate smooth in-between views from a stereo data set. compare your results against using the ground truth depth data available. what kinds of artifacts do you see? can you think of ways to reduce them? more details on implementing such algorithms can be found in section and exercises ex multi-frame stereo extend one of your previous techniques to use multiple input frames and try to improve the results you obtained with just two views. if helpful try using temporal selection and szeliski to deal with the increased number of occlusions in multi-frame data sets. you can also try to simultaneously estimate multiple depth maps and make them consis tent and zabih kang and szeliski test your algorithms out on some standard multi-view data sets. ex volumetric stereo implement voxel coloring and dyer as a simple extension to the plane sweep algorithm you implemented in exercise instead of computing the complete dsi all at once evaluate each plane one at a time from front to back. tag every voxel whose photoconsistency is below a certain threshold as being part of the object and remember its average robust color and dyer eisert steinbach and girod kutulakos slabaugh culbertson slabaugh et al. erase the input pixels corresponding to tagged voxels in the input images e.g. by setting their alpha value to to some reduced number depending on occupancy. as you evaluate the next plane use the source image alpha values to modify your photoconsistency score e.g. only consider pixels that have full alpha or weight pixels by their alpha values. if the cameras are not all on the same side of your plane sweeps use space carving and seitz to cycle through different subsets of source images while carving away the volume from different directions. ex depth map merging use the technique you developed for multi-frame stereo in exercise or a different technique such as the one described by goesele snavely curless et al. to compute a depth map for every input image. exercises merge these depth maps into a coherent model e.g. using poisson surface reconstruc tion bolitho and hoppe ex shape from silhouettes build a silhouette-based volume reconstruction algorithm use an octree or some other representation of your choosing. computer vision algorithms and applications draft chapter reconstruction shape from x active rangefinding range data merging application digital heritage surface interpolation surface simplification geometry images point-based representations volumetric representations surface representations shape from shading and photometric stereo shape from texture shape from focus implicit surfaces and level sets architecture heads and faces application facial animation whole body modeling and tracking estimating brdfs application photography model-based reconstruction recovering texture maps and albedos additional reading exercises computer vision algorithms and applications draft figure shape acquisition and modeling techniques shaded image tsai cryer et al. ieee texture gradient springer real-time depth from focus watanabe and noguchi ieee scanning a scene with a stick shadow and perona springer merging range maps into a model and levoy acm point-based surface modeling keiser kobbelt et al. acm automated modeling of a building using lines and planes and zisserman springer face model from spacetime stereo snavely curless et al. acm person tracking bhatia roth et al. ieee. reconstruction as we saw in the previous chapter a variety of stereo matching techniques have been developed to reconstruct high quality models from two or more images. however stereo is just one of the many potential cues that can be used to infer shape from images. in this chapter we investigate a number of such techniques which include not only visual cues such as shading and focus but also techniques for merging multiple range or depth images into models as well as techniques for reconstructing specialized models such as heads bodies or architecture. among the various cues that can be used to infer shape the shading on a surface can provide a lot of information about local surface orientations and hence overall surface shape this approach becomes even more powerful when lights shining from different directions can be turned on and off separately stereo. texture gradients i.e. the foreshortening of regular patterns as the surface slants or bends away from the camera can provide similar cues on local surface orientation focus is another powerful cue to scene depth especially when two or more images with different focus settings are used shape can also be estimated using active illumination techniques such as light stripes or time of flight range finders the partial surface models obtained using such techniques passive image-based stereo can then be merged into more coherent surface models as discussed in section such techniques have been used to construct highly detailed and accurate models of cultural heritage such as historic sites the resulting surface models can then be simplified to support viewing at different resolutions and streaming across the web an alternative to working with continuous surfaces is to represent surfaces as dense collections of oriented points or as volumetric primitives modeling can be more efficient and effective if we know something about the objects we are trying to reconstruct. in section we look at three specialized but commonly occurring examples namely architecture heads and faces and whole bodies in addition to modeling people we also discuss techniques for tracking them. the last stage of shape and appearance modeling is to extract some textures to paint onto our models some techniques go beyond this and actually estimate full brdfs because there exists such a large variety of techniques to perform modeling this chapter does not go into detail on any one of these. readers are encouraged to find more information in the cited references or more specialized publications and conferences devoted to these topics e.g. the international symposium on data processing visualization and transmission the international conference on digital imaging and modeling the international conference on automatic face and gesture recognition computer vision algorithms and applications draft the ieee workshop on analysis and modeling of faces and gestures and the international workshop on tracking humans for the evaluation of their motion in image sequences shape from x in addition to binocular disparity shading texture and focus all play a role in how we perceive shape. the study of how shape can be inferred from such cues is sometimes called shape from x since the individual instances are called shape from shading shape from texture and shape from in this section we look at these three cues and how they can be used to reconstruct geometry. a good overview of all these topics can be found in the collection of papers on physics-based shape inference edited by wolff shafer and healey shape from shading and photometric stereo when you look at images of smooth shaded objects such as the ones shown in figure you can clearly see the shape of the object from just the shading variation. how is this possible? the answer is that as the surface normal changes across the object the apparent brightness changes as a function of the angle between the local surface orientation and the incident illumination as shown in figure the problem of recovering the shape of a surface from this intensity variation is known as shape from shading and is one of the classic problems in computer vision the collection of papers edited by horn and brooks is a great source of information on this topic especially the chapter on variational approaches. the survey by zhang tsai cryer et al. not only reviews more recent techniques but also provides some comparative results. most shape from shading algorithms assume that the surface under consideration is of a uniform albedo and reflectance and that the light source directions are either known or can be calibrated by the use of a reference object. under the assumptions of distant light sources and observer the variation in intensity equation become purely a function of the local surface orientation ix y rpx y qx y where q zy are the depth map derivatives and rp q is called the reflectance map. for example a diffuse surface has a reflectance map that is the we have already seen examples of shape from stereo shape from profiles and shape from silhouettes in chap ter shape from x figure synthetic shape from shading tsai cryer et al. ieee shaded images b with light from in front and d with light the front right f corresponding shape from shading reconstructions using the technique of tsai and shah negative dot product between the surface normal n q and the light source direction v vy vz rp q pvx qvy vz where is the surface reflectance factor in principle equations can be used to estimate q using non-linear least squares or some other method. unfortunately unless additional constraints are imposed there are more unknowns per pixel q than there are measurements one commonly used constraint is the smoothness constraint es x y x y dx dy dx dy which we already saw in section the other is the integrability constraint ei dx dy computer vision algorithms and applications draft which arises naturally since for a valid depth map zx y with q zy we have py zxy zyx qx. instead of first recovering the orientation fields q and integrating them to obtain a surface it is also possible to directly minimize the discrepancy in the image formation equation while finding the optimal depth map zx y unfortunately shape from shading is susceptible to local minima in the search space and like other variational problems that involve the simultaneous estimation of many variables can also suffer from slow convergence. using multi-resolution techniques can help accelerate the convergence while using more sophisticated optimization techniques and oliensis can help avoid local minima. in practice surfaces other than plaster casts are rarely of a single uniform albedo. shape from shading therefore needs to be combined with some other technique or extended in some way to make it useful. one way to do this is to combine it with stereo matching and leclerc or known texture patterns and forsyth the stereo and texture components provide information in textured regions while shape from shading helps fill in the information across uniformly colored regions and also provides finer information about surface shape. photometric stereo. another way to make shape from shading more reliable is to use multiple light sources that can be selectively turned on and off. this technique is called photometric stereo since the light sources play a role analogous to the cameras located at different locations in traditional stereo for each light source we have a different reflectance map q q etc. given the corresponding intensities etc. at a pixel we can in principle recover both an unknown albedo and a surface orientation estimate q. for diffuse surfaces if we parameterize the local orientation by n we get non-shadowed pixels a set of linear equations of the form ik n vk from which we can recover n using linear least squares. these equations are well conditioned as long as the or more vectors vk are linearly independent i.e. they are not along the same azimuth away from the viewer. once the surface normals or gradients have been recovered at each pixel they can be integrated into a depth map using a variant of regularized surface fitting rusinkiewicz davis et al. and harker and o leary have produced some recent work in this area. an alternative to turning lights on-and-off is to use three colored lights hernandez vogiatzis brostow et al. hernandez and vogiatzis shape from x figure synthetic shape from texture springer regular texture wrapped onto a curved surface and the corresponding surface normal estimates. shape from mirror reflections chen and perona springer a regular pattern reflecting off a curved mirror gives rise to curved lines from which point locations and normals can be inferred. when surfaces are specular more than three light directions may be required. in fact the irradiance equation given in not only requires that the light sources and camera be distant from the surface it also neglects inter-reflections which can be a significant source of the shading observed on object surfaces e.g. the darkening seen inside concave structures such as grooves and crevasses ikeuchi and kanade shape from texture the variation in foreshortening observed in regular textures can also provide useful information about local surface orientation. figure shows an example of such a pattern along with the estimated local surface orientations. shape from texture algorithms require a number of processing steps including the extraction of repeated patterns or the measurement of local frequencies in order to compute local affine deformations and a subsequent stage to infer local surface orientation. details on these various stages can be found in the research literature ikeuchi blostein and ahuja garding malik and rosenholtz lobay and forsyth when the original pattern is regular it is possible to fit a regular but slightly deformed grid to the image and use this grid for a variety of image replacement or analysis tasks collins and tsin liu lin and hays hays leordeanu efros et al. lin hays wu et al. park brocklehurst collins et al. this process becomes even easier if specially printed textured cloth patterns are used and forsyth white crane and forsyth the deformations induced in a regular pattern when it is viewed in the reflection of a curved mirror as shown in figure d can be used to recover the shape of the surface estimate tangent planesscene patternbccamerascenemirror surface caestimate tangent planesscene patternbccamerascenemirror surface caestimate tangent planesscene patternbccamerascenemirror surface ca computer vision algorithms and applications draft figure real time depth from defocus watanabe and noguchi ieee the real-time focus range sensor which includes a half-silvered mirror between the two telecentric lenses right a prism that splits the image into two ccd sensors left and an edged checkerboard pattern illuminated by a xenon lamp c input video frames from the two cameras along with the corresponding depth map f two frames can see the texture if you zoom in and the corresponding mesh model. chen and perona rozenfeld shimshoni and lindenbaum it is is also possible to infer local shape information from specular flow i.e. the motion of specularities when viewed from a moving camera and nayar zisserman giblin and blake swaminathan kang szeliski et al. shape from focus a strong cue for object depth is the amount of blur which increases as the object s surface moves away from the camera s focusing distance. as shown in figure moving the object surface away from the focus plane increases the circle of confusion according to a formula that is easy to establish using similar triangles a number of techniques have been developed to estimate depth from the amount of defocus from defocus nayar and nakagawa nayar watanabe and noguchi watanabe and nayar chaudhuri and rajagopalan favaro and soatto in order to make such a technique practical a number issues need to be addressed the amount of blur increase in both directions as you move away from the focus plane. therefore it is necessary to use two or more images captured with different focus active rangefinding figure range data scanning and levoy acm a laser dot on a surface is imaged by a ccd sensor a laser stripe is imaged by the sensor deformation of the stripe encodes the distance to the object the resulting set of points are turned into a triangulated mesh. distance settings nayar watanabe and noguchi or to translate the object in depth and look for the point of maximum sharpness and nakagawa the magnification of the object can vary as the focus distance is changed or the object is moved. this can be modeled either explicitly correspondence more difficult or using telecentric optics which approximate an orthographic camera and require an aperture in front of the lens watanabe and noguchi the amount of defocus must be reliably estimated. a simple approach is to average the squared gradient in a region but this suffers from several problems including the image magnification problem mentioned above. a better solution is to use carefully designed rational filters and nayar figure shows an example of a real-time depth from defocus sensor which employs two imaging chips at slightly different depths sharing a common optical path as well as an active illumination system that projects a checkerboard pattern from the same direction. as you can see in figure g the system produces high-accuracy real-time depth maps for both static and dynamic scenes. active rangefinding as we have seen in the previous section actively lighting a scene whether for the purpose of estimating normals using photometric stereo or for adding artificial texture for shape from defocus can greatly improve the performance of vision systems. this kind of active illumination has been used from the earliest days of machine vision to construct highly reliable surfaceccdlaseradirection of travelobjectccdccd image planelasercylindrical lenslaser sheet z xbcd computer vision algorithms and applications draft figure shape scanning using cast shadows and perona springer camera setup with a point light source desk lamp without its reflector a hand-held stick casting a shadow and the objects being scanned in front of two planar backgrounds. real-time depth map using a pulsed illumination system and yahav spie. sensors for estimating depth images using a variety of rangefinding range sensing techniques curless hebert one of the most popular active illumination sensors is a laser or light stripe sensor which sweeps a plane of light across the scene or object while observing it from an offset viewpoint as shown in figure and bird curless and levoy as the stripe falls across the object it deforms its shape according to the shape of the surface it is illuminating. it is then a simple matter of using optical triangulation to estimate the locations of all the points seen in a particular stripe. in more detail knowledge of the plane equation of the light stripe allows us to infer the location corresponding to each illuminated pixel as previously discussed in the accuracy of light striping techniques can be improved by finding the exact temporal peak in illumination for each pixel and levoy the final accuracy of a scanner can be determined using slant edge modulation techniques i.e. by imaging sharp creases in a calibration object fuchs and seidel an interesting variant on light stripe rangefinding is presented by bouguet and perona instead of projecting a light stripe they simply wave a stick casting a shadow over a scene or object illuminated by a point light source such as a lamp or the sun as the shadow falls across two background planes whose orientation relative to the camera is known inferred during pre-calibration the plane equation for each stripe can be inferred from the two projected lines whose equations are known the deformation of the shadow as it crosses the object being scanned then reveals its shape as with regular light stripe rangefinding this technique can also be used to estimate the geometry of a background scene and how its appearance varies as it moves into shadow in order to cast new shadows onto the scene goldman curless et al. active rangefinding the time it takes to scan an object using a light stripe technique is proportional to the number of depth planes used which is usually comparable to the number of pixels across an image. a much faster scanner can be constructed by turning different projector pixels on and off in a structured manner e.g. using a binary or gray code for example let us assume that the lcd projector we are using has columns of pixels. taking the binary code corresponding to each column s address we project the first bit then the second etc. after projections a third of a second for a synchronized camera-projector system each pixel in the camera knows which of the columns of projector light it is seeing. a similar approach can also be used to estimate the refractive properties of an object by placing a monitor behind the object werner curless et al. chuang zongker hindorff et al. very fast scanners can also be constructed with a single laser beam i.e. a real-time flying spot optical triangulation scanner bechthold taylor et al. if even faster i.e. frame-rate scanning is required we can project a single textured pattern into the scene. proesmans van gool and defoort describe a system where a checkerboard grid is projected onto an object a person s face and the deformation of the grid is used to infer shape. unfortunately such a technique only works if the surface is continuous enough to link all of the grid points. a much better system can be constructed using high-speed custom illumination and sensing hardware. iddan and yahav describe the construction of their zcam videorate depth sensing camera which projects a pulsed plane of light onto the scene and then integrates the returning light for a short interval essentially obtaining time-of-flight measurement for the distance to individual pixels in the scene. a good description of earlier time-of-flight systems including amplitude and frequency modulation schemes for lidar can be found in instead of using a single camera it is also possible to construct an active illumination range sensor using stereo imaging setups. the simplest way to do this is to just project random stripe patterns onto the scene to create synthetic texture which helps match textureless surfaces webb zitnick et al. projecting a known series of stripes just as in coded pattern single-camera rangefinding makes the correspondence between pixels unambiguous and allows for the recovery of depth estimates at pixels only seen in a single camera and szeliski this technique has been used to produce large numbers of highly accurate registered multi-image stereo pairs and depth maps for the purpose of evaluating stereo correspondence algorithms and szeliski hirschm uller and scharstein and learning depth map priors and parameters and pal while projecting multiple patterns usually requires the scene or object to remain still additional processing can enable the production of real-time depth maps for dynamic scenes. computer vision algorithms and applications draft figure real-time dense face capture using spacetime stereo snavely curless et al. acm set of five consecutive video frames from one of two stereo cameras fifth frame is free of stripe patterns in order to extract texture resulting high-quality surface model map visualized as a shaded rendering. the basic idea ramamoorthi and rusinkiewicz zhang curless and seitz is to assume that depth is nearly constant within a space time window around each pixel and to use the window for matching and reconstruction. depending on the surface shape and motion this assumption may be error-prone as shown in nahab ramamoorthi et al. to model shapes more accurately zhang curless and seitz model the linear disparity variation within the space time window and show that better results can be obtained by globally optimizing disparity and disparity gradient estimates over video volumes snavely curless et al. figure shows the results of applying this system to a person s face the frame-rate surface model can then be used for further model-based fitting and computer graphics manipulation range data merging while individual range images can be useful for applications such as real-time z-keying or facial motion capture they are often used as building blocks for more complete object modeling. in such applications the next two steps in processing are the registration of partial surface models and their integration into coherent surfaces if desired this can be followed by a model fitting stage using either parametric representations such as generalized cylinders and binford nevatia and binford marr and nishihara brooks superquadrics solina and bajcsy terzopoulos and metaxas or non-parametric models such as triangular meshes or physically-based models witkin and kass delingette hebert and ikeuichi terzopoulos and metaxas mcinerney and terzopoulos terzopoulos a number of techniques have also been developed for segmenting range images into simpler constituent surfaces jean-baptiste jiang et al. the most widely used registration technique is the iterated closest point algo active rangefinding rithm which alternates between finding the closest point matches between the two surfaces being aligned and then solving a absolute orientation problem and mckay chen and medioni zhang szeliski and lavall ee gold rangarajan lu et al. david dementhon duraiswami et al. li and hartley enqvist josephson and kahl since the two surfaces being aligned usually only have partial overlap and may also have outliers robust matching criteria and appendix are typically used. in order to speed up the determination of the closest point and also to make the distance-to-surface computation more accurate one of the two point sets the current merged model can be converted into a signed distance function optionally represented using an octree spline for compactness ee and szeliski variants on the basic icp algorithm can be used to register point sets under non-rigid deformations e.g. for medical applications and ayache szeliski and lavall ee color values associated with the points or range measurements can also be used as part of the registration process to improve robustness and kang pulli unfortunately the icp algorithm and its variants can only find a locally optimal alignment between surfaces. if this is not known a priori more global correspondence or search techniques based on local descriptors invariant to rigid transformations need to be used. an example of such a descriptor is the spin image which is a local circular projection of a surface patch around the local normal axis and hebert another example is the splash representation introduced by stein and medioni once two or more surfaces have been aligned they can be merged into a single model. one approach is to represent each surface using a triangulated mesh and combine these meshes using a process that is sometimes called zippering and laurendeau turk and levoy another now more widely used approach is to compute a signed distance function that fits all of the data points derose duchamp et al. curless and levoy hilton stoddart illingworth et al. wheeler sato and ikeuchi figure shows one such approach the volumetric range image processing technique developed by curless and levoy which first computes a weighted signed distance function from each range image and then merges them using a weighted averaging process. to make the representation more compact run-length coding is used to encode the empty seen and varying distance voxels and only the signed distance values near each surface are once the merged signed distance function has been computed a zero-crossing surface extraction algorithm such as marching cubes and cline can be used to recover a meshed surface model. figure shows an example of the some techniques such as the one developed by chen and medioni use local surface tangent planes to make this computation more accurate and to accelerate convergence. an alternative even more compact representation could be to use octrees ee and szeliski computer vision algorithms and applications draft figure range data merging and levoy acm two signed distance functions left are merged with their bottom left to produce a combined set of functions column from which an isosurface can be extracted dashed line the signed distance functions are combined with empty and unseen space labels to fill holes in the isosurface. complete range data merging and isosurface extraction pipeline. volumetric range data merging techniques based on signed distance or characteristic outside functions are also widely used to extract smooth well-behaved surfaces from oriented or unoriented sets of points derose duchamp et al. ohtake belyaev alexa et al. kazhdan bolitho and hoppe lempitsky and boykov zach pock and bischof zach as discussed in more detail in section application digital heritage active rangefinding technologies combined with surface modeling and appearance modeling techniques are widely used in the fields of archeological and historical preservation which often also goes under the name digital heritage in such applications detailed models of cultural objects are acquired and later used for applications such as analysis preservation restoration and the production of duplicate artwork and bird a more recent example of such an endeavor is the digital michelangelo project of levoy pulli curless et al. which used cyberware laser stripe scanners and high-quality digital slr cameras mounted on a large gantry to obtain detailed scans of michelangelo s david and other sculptures in florence. the project also took scans of the forma urbis romae an ancient stone map of rome that had shattered into pieces for which new matches were obtained using digital techniques. the whole process from initial planning to software surface representations figure reconstruction and hardcopy of the happy buddha statuette and levoy acm photograph of the original statue after spray painting with matte gray partial range scan merged range scans colored rendering of the reconstructed model hardcopy of the model constructed using stereolithography. development acquisition and post-processing took several years many volunteers and produced a wealth of shape and appearance modeling techniques as a result. even larger-scale projects are now being attempted for example the scanning of complete temple sites such as angkor-thom and sato ikeuchi and miyazaki banno masuda oishi et al. figure shows details from this project including a sample photograph a detailed head model scanned from ground level and an aerial overview of the final merged site model which was acquired using a balloon. surface representations in previous sections we have seen different representations being used to integrate range scans. we now look at several of these representations in more detail. explicit surface representations such as triangle meshes splines and subdivision surfaces derose and salesin zorin schr oder and sweldens warren and weimer peters and reif enable not only the creation of highly detailed models but also processing operations such as interpolation fairing or smoothing and decimation and simplification we also examine discrete point-based representations and volumetric representations computer vision algorithms and applications draft figure laser range modeling of the bayon temple at angkor-thom masuda oishi et al. springer sample photograph from the site a detailed head model scanned from the ground final merged model of the temple scanned using a laser range sensor mounted on a balloon. surface interpolation one of the most common operations on surfaces is their reconstruction from a set of sparse data constraints i.e. scattered data interpolation. when formulating such problems surfaces may be parameterized as height fields fx as parametric surfaces fx or as nonparametric models such as collections of triangles. in the section on image processing we saw how two-dimensional function interpolation and approximation problems fx could be cast as energy minimization problems using regularization such problems can also specify the locations of discontinuities in the surface as well as local orientation constraints zhang dugas-phocion samson et al. one approach to solving such problems is to discretize both the surface and the energy on a discrete grid or mesh using finite element analysis such problems can then be solved using sparse system solving techniques such as multigrid henson and mccormick or hierarchically preconditioned conjugate gradient the surface can also be represented using a hierarchical combination of multilevel b-splines wolberg and shin an alternative approach is to use radial basis kernel functions and kender nielson to interpolate a field fx through near a number of data values di located at xi the radial basis function approach uses fx wixdi wix the difference between interpolation and approximation is that the former requires the surface or function to pass through the data while the latter allows the function to pass near the data and can therefore be used for surface smoothing as well. surface representations where the weights wix are computed using a radial basis symmetrical function kr. if we want the function fx to exactly interpolate the data points the kernel functions must either be singular at the origin limr kr or a dense linear system must be solved to determine the magnitude associated with each basis function and kender it turns out that for certain regularized problems e.g. there exist radial basis functions that give the same results as a full analytical solution and kender unfortunately because the dense system solving is cubic in the number of data points basis function approaches can only be used for small problems such as feature-based image morphing and neely when a three-dimensional parametric surface is being modeled the vector-valued function f in or encodes coordinates y z on the surface and the domain x t encodes the surface parameterization. one example of such surfaces are symmetry-seeking parametric models which are elastically deformable versions of generalized witkin and kass in these models s is the parameter along the spine of the deformable tube and t is the parameter around the tube. a variety of smoothness and radial symmetry forces are used to constrain the model while it is fitted to image-based silhouette curves. it is also possible to define non-parametric surface models such as general triangulated meshes and to equip such meshes finite element analysis with both internal smoothness metrics and external data fitting metrics and zucker fua and sander delingette hebert and ikeuichi mcinerney and terzopoulos while most of these approaches assume a standard elastic deformation model which uses quadratic internal smoothness terms it is also possible to use sub-linear energy models in order to better preserve surface creases thrun and br unig triangle meshes can also be augmented with either spline elements and ponce or subdivision surfaces derose and salesin zorin schr oder and sweldens warren and weimer peters and reif to produce surfaces with better smoothness control. both parametric and non-parametric surface models assume that the topology of the surface is known and fixed ahead of time. for more flexible surface modeling we can either represent the surface as a collection of oriented points or use implicit functions which can also be combined with elastic surface models and terzopoulos a generalized cylinder is a solid of revolution i.e. the result of rotating a smooth curve around an axis. it can also be generated by sweeping a slowly varying circular cross-section along the axis. two interpretations are equivalent. computer vision algorithms and applications draft figure progressive mesh representation of an airplane model acm base mesh m faces mesh m faces mesh m faces original mesh m m n faces. surface simplification once a triangle mesh has been created from data it is often desirable to create a hierarchy of mesh models for example to control the displayed level of detail in a computer graphics application. essence this is a analog to image pyramids one approach to doing this is to approximate a given mesh with one that has subdivision connectivity over which a set of triangular wavelet coefficients can then be computed derose duchamp et al. a more continuous approach is to use sequential edge collapse operations to go from the original fine-resolution mesh to a coarse base-level mesh the resulting progressive mesh representation can be used to render the model at arbitrary levels of detail as shown in figure geometry images while multi-resolution surface representations such as derose duchamp et al. hoppe support level of detail operations they still consist of an irregular collection of triangles which makes them more difficult to compress and store in a cache-efficient to make the triangulation completely regular and gridded gu gortler and hoppe describe how to create geometry images by cutting surface meshes along wellchosen lines and flattening the resulting representation into a square. figure shows the resulting y z values of the surface mesh mapped over the unit square while figure shows the associated ny nz normal map i.e. the surface normals associated with each mesh vertex which can be used to compensate for loss in visual fidelity if the original geometry image is heavily compressed. subdivision triangulations such as those in derose duchamp et al. are semi-regular i.e. regular and nested within each subdivided base triangle. point-based representations y z ny nz figure geometry images gortler and hoppe acm the geometry image defines a mesh over the surface the normal map defines vertex normals final lit model. point-based representations as we mentioned previously triangle-based surface models assume that the topology often the rough shape of the model is known ahead of time. while it is possible to re-mesh a model as it is being deformed or fitted a simpler solution is to dispense with an explicit triangle mesh altogether and to have triangle vertices behave as oriented points or particles or surface elements and tonnesen in order to endow the resulting particle system with internal smoothness constraints pairwise interaction potentials can be defined that approximate the equivalent elastic bending energies that would be obtained using local finite-element instead of defining the finite element neighborhood for each particle ahead of time a soft influence function is used to couple nearby particles. the resulting model can change both topology and particle density as it evolves and can therefore be used to interpolate partial data with holes tonnesen and terzopoulos discontinuities in both the surface orientation and crease curves can also be modeled tonnesen and terzopoulos to render the particle system as a continuous surface local dynamic triangulation heuristics and tonnesen or direct surface element splatting zwicker van baar et al. can be used. another alternative is to first convert the point cloud into an implicit signed distance or inside outside function using either minimum signed distances to the oriented points derose duchamp et al. or by interpolating a characteristic outside function using radial basis functions and o brien dinh turk and slabaugh even greater precision over the implicit function fitting including the ability to handle irregular point densities can be obtained by computing a moving least as mentioned before an alternative is to use sub-linear interaction potentials which encourage the preservation of surface creases thrun and br unig computer vision algorithms and applications draft figure point-based surface modeling with moving least squares keiser kobbelt et al. acm a set of points dots is turned into an implicit inside outside function curve the signed distance to the nearest oriented point can serve as an approximation to the inside outside distance a set of oriented points with variable sampling density representing a surface model local estimate of sampling density which is used in the moving least squares reconstructed continuous surface. squares estimate of the signed distance function behr cohen-or et al. pauly keiser kobbelt et al. as shown in figure further improvements can be obtained using local sphere fitting and gross faster and more accurate re-sampling germann and gross and kernel regression to better tolerate outliers guennebaud and gross volumetric representations a third alternative for modeling surfaces is to construct volumetric inside outside functions. we already saw examples of this in section where we looked at voxel coloring and dyer space carving and seitz and level set and keriven pons keriven and faugeras techniques for stereo matching and section where we discussed using binary silhouette images to reconstruct volumes. in this section we look at continuous implicit outside functions to represent shape. implicit surfaces and level sets while polyhedral and voxel-based representations can represent three-dimensional shapes to an arbitrary precision they lack some of the intrinsic smoothness properties available with continuous implicit surfaces which use an indicator function function f y z to indicate which points are inside f y z or outside f y z volumetric representations the object. an early example of using implicit functions to model objects in computer vision are superquadrics which are a generalization of quadric ellipsoidal parametric volumetric models f y z x y x solina and bajcsy waithe and ferrie leonardis jakli c and solina the values of control the extent of model along each y z axis while the values of control how square it is. to model a wider variety of shapes superquadrics are usually combined with either rigid or non-rigid deformations and metaxas metaxas and terzopoulos superquadric models can either be fit to range data or used directly for stereo matching. a different kind of implicit shape model can be constructed by defining a signed distance function over a regular three-dimensional grid optionally using an octree spline to represent this function more coarsely away from its surface ee and szeliski szeliski and lavall ee frisken perry rockwood et al. ohtake belyaev alexa et al. we have already seen examples of signed distance functions being used to represent distance transforms level sets for contour fitting and tracking volumetric stereo range data merging and point-based modeling the advantage of representing such functions directly on a grid is that it is quick and easy to look up distance function values for any y z location and also easy to extract the isosurface using the marching cubes algorithm and cline the work of ohtake belyaev alexa et al. is particularly notable since it allows for several distance functions to be used simultaneously and then combined locally to produce sharp features such as creases. poisson surface reconstruction bolitho and hoppe uses a closely related volumetric function namely a smoothed inside outside function which can be thought of as a clipped signed distance function. the gradients for this function are set to lie along oriented surface normals near known surface points and elsewhere. the function itself is represented using a quadratic tensor-product b-spline over an octree which provides a compact representation with larger cells away from the surface or in regions of lower point density and also admits the efficient solution of the related poisson equations see section erez gangnet and blake it is also possible to replace the quadratic penalties used in the poisson equations with variation constraints and still obtain a convex optimization problem which can be solved using either continuous pock and bischof zach or discrete graph cut and boykov techniques. computer vision algorithms and applications draft signed distance functions also play an integral role in level-set evolution equations and where the values of distance transforms on the mesh are updated as the surface evolves to fit multi-view stereo photoconsistency measures and keriven model-based reconstruction when we know something ahead of time about the objects we are trying to model we can construct more detailed and reliable models using specialized techniques and representations. for example architecture is usually made up of large planar regions and other parametric forms as surfaces of revolution usually oriented perpendicular to gravity and to each other heads and faces can be represented using low-dimensional non-rigid shape models since the variability in shape and appearance of human faces while extremely large is still bounded human bodies or parts such as hands form highly articulated structures which can be represented using kinematic chains of piecewise rigid skeletal elements linked by joints in this section we highlight some of the main ideas representations and modeling algorithms used for these three cases. additional details and references can be found in specialized conferences and workshops devoted to these topics e.g. the international symposium on data processing visualization and transmission the international conference on digital imaging and modeling the international conference on automatic face and gesture recognition the ieee workshop on analysis and modeling of faces and gestures and the international workshop on tracking humans for the evaluation of their motion in image sequences architecture architectural modeling especially from aerial photography has been one of the longest studied problems in both photogrammetry and computer vision and herman recently the development of reliable image-based modeling techniques as well as the prevalence of digital cameras and computer games has spurred renewed interest in this area. the work by debevec taylor and malik was one of the earliest hybrid geometryand image-based modeling and rendering systems. their fac ade system combines an interactive image-guided geometric modeling tool with model-based plane plus parallax stereo matching and view-dependent texture mapping. during the interactive photogrammetric modeling phase the user selects block elements and aligns their edges with visible edges in the input images the system then automatically computes the dimensions and locations of the blocks along with the camera positions using constrained optimization model-based reconstruction figure interactive architectural modeling using the fac ade system taylor and malik acm input image with user-drawn edges shown in green shaded solid model geometric primitives overlaid onto the input image final view-dependent texture-mapped model. c. this approach is intrinsically more reliable than general feature-based structure from motion because it exploits the strong geometry available in the block primitives. related work by becker and bove horry anjyo and arai and criminisi reid and zisserman exploits similar information available from vanishing points. in the interactive image-based modeling system of sinha steedly szeliski et al. vanishing point directions are used to guide the user drawing of polygons which are then automatically fitted to sparse points recovered using structure from motion. once the rough geometry has been estimated more detailed offset maps can be computed for each planar face using a local plane sweep which debevec taylor and malik call model-based stereo. finally during rendering images from different viewpoints are warped and blended together as the camera moves around the scene using a process to light field and lumigraph rendering see section called view-dependent texture mapping for interior modeling instead of working with single pictures it is more useful to work with panoramas since you can see larger extents of walls and other structures. the modeling system developed by shum han and szeliski first constructs calibrated panoramas from multiple images and then has the user draw vertical and horizontal lines in the image to demarcate the boundaries of planar regions. the lines are initially used to establish an absolute rotation for each panorama and are later used with the inferred vertices and planes to optimize the structure which can be recovered up to scale from one or more images high dynamic range panoramas can also be used for outdoor modeling since they provide highly reliable estimates of relative camera orientations as well as vanishing point directions and teller teller antone bodnar et al. computer vision algorithms and applications draft figure interactive modeling from panoramas han and szeliski ieee wide-angle view of a panorama with user-drawn vertical and horizontal lines single-view reconstruction of the corridors. while earlier image-based modeling systems required some user authoring werner and zisserman present a fully automated line-based reconstruction system. as described in section they first detect lines and vanishing points and use them to calibrate the camera then they establish line correspondences using both appearance matching and trifocal tensors which enables them to reconstruct families of line segments as shown in figure they then generate plane hypotheses using both co-planar lines and a plane sweep based on cross-correlation scores evaluated at interest points. intersections of planes are used to determine the extent of each plane i.e. an initial coarse geometry which is then refined with the addition of rectangular or wedge-shaped indentations and extrusions note that when top-down maps of the buildings being modeled are available these can be used to further constrain the modeling process and cipolla the idea of using matched lines for estimating vanishing point directions and dominant planes continues to be used in a number of recent fully automated image-based architectural modeling systems bauer karner et al. mi cu s k and ko seck a furukawa curless seitz et al. sinha steedly and szeliski another common characteristic of architecture is the repeated use of primitives such as windows doors and colonnades. architectural modeling systems can be designed to search for such repeated elements and to use them as part of the structure inference process torr and cipolla mueller zeng wonka et al. schindler krishnamurthy lublinerman et al. sinha steedly szeliski et al. the combination of all these techniques now makes it possible to reconstruct the structure of large scenes and kanade for example the urbanscan system of pollefeys nist er frahm et al. reconstructs texture-mapped models of city streets from videos acquired with a gps-equipped vehicle. to obtain real-time performance they use both optimized on-line structure-from-motion algorithms as well as gpu implementations model-based reconstruction figure automated architectural reconstruction using lines and planes and zisserman springer reconstructed lines color coded by their vanishing directions wire-frame model superimposed onto an input image triangulated piecewise-planar model with windows final texture-mapped model. of plane-sweep stereo aligned to dominant planes and depth map fusion. cornelis leibe cornelis et al. present a related system that also uses plane-sweep stereo to vertical building fac ades combined with object recognition and segmentation for vehicles. mi cu s k and ko seck a build on these results using omni-directional images and superpixel-based stereo matching along dominant plane orientations. reconstruction directly from active range scanning data combined with color imagery that has been compensated for exposure and lighting variations is also possible and chen stamos liu chen et al. troccoli and allen heads and faces another area in which specialized shape and appearance models are extremely helpful is in the modeling of heads and faces. even though the appearance of people seems at first glance to be infinitely variable the actual shape of a person s head and face can be described reasonably well using a few dozen parameters hecker lischinski et al. guenter grimm wood et al. decarlo metaxas and stone blanz and vetter shan liu and zhang figure shows an example of an image-based modeling system where user-specified keypoints in several images are used to fit a generic head model to a person s face. as you can see in figure after specifying just over keypoints the shape of the face has become quite adapted and recognizable. extracting a texture map from the original images and then applying it to the head model results in an animatable model with striking visual fidelity a more powerful system can be built by applying principal component analysis to a collection of scanned faces which is a topic we discuss in section as you can see in figure it is then possible to fit morphable models to single images and to computer vision algorithms and applications draft figure model fitting to a collection of images hecker lischinski et al. acm set of five input images along with user-selected keypoints the complete set of keypoints and curves three meshes the original adapted after keypoints and after an additional keypoints the partition of the image into separately animatable regions. figure head and expression tracking and re-animation using deformable models. models fit directly to five input video streams szeliski and salesin springer the bottom row shows the results of re-animating a synthetic texture-mapped model with pose and expression parameters fitted to the input images in the top row. models fit to frame-rate spacetime stereo surface models snavely curless et al. acm the top row shows the input images with synthetic green markers overlaid while the bottom row shows the fitted surface model. model-based reconstruction use such models for a variety of animation and visual effects and vetter it is also possible to design stereo matching algorithms that optimize directly for the head model parameters liu and zhang kang and jones or to use the output of realtime stereo with active illumination snavely curless et al. and as the sophistication of facial capture systems evolves so does the detail and realism in the reconstructed models. newer systems can capture real-time not only surface details such as wrinkles and creases but also accurate models of skin reflection translucency and sub-surface scattering matusik pfister et al. golovinskiy matusik ster et al. bickel botsch angst et al. igarashi nishino and nayar once a head model has been constructed it can be used in a variety of applications such as head tracking lepetit pilet and fua matthews xiao and baker as shown in figures and and face transfer i.e. replacing one person s face with another in a video covell and slaney vlasic brand pfister et al. additional applications include face beautification by warping face images toward a more attractive standard cohen-or dror et al. face de-identification for privacy protection sweeney de la torre et al. and face swapping kumar dhillon et al. application facial animation perhaps the most widely used application of head modeling is facial animation. once a parameterized model of shape and appearance texture has been constructed it can be used directly to track a person s facial motions and to animate a different character with these same motions and expressions szeliski and salesin an improved version of such a system can be constructed by first applying principal component analysis to the space of possible head shapes and facial appearances. blanz and vetter describe a system where they first capture a set of colored range scans of faces which can be represented as a large collection of y z r g b samples in order for morphing to be meaningful corresponding vertices in different people s scans must first be put into correspondence hecker lischinski et al. once this is done pca can be applied to more naturally parameterize the morphable model. the flexibility of this model can be increased by performing separate analyses in different subregions such as the eyes nose and mouth just as in modular eigenspaces and pentland a cylindrical coordinate system provides a natural two-dimensional embedding for this collection but such an embedding is not necessary to perform pca. computer vision algorithms and applications draft figure morphable face model and vetter acm original face model with the addition of shape and texture variations in specific directions deviation from the mean gender expression weight and nose shape a morphable model is fit to a single image after which its weight or expression can be manipulated another example of a reconstruction along with a different set of manipulations such as lighting and pose change. model-based reconstruction after computing a subspace representation different directions in this space can be associated with different characteristics such as gender facial expressions or facial features as in the work of rowland and perrett faces can be turned into caricatures by exaggerating their displacement from the mean image. morphable models can be fitted to a single image using gradient descent on the error between the input image and the re-synthesized model image after an initial manual placement of the model in an approximately correct pose scale and location c. the efficiency of this fitting process can be increased using inverse compositional image alignment as described by romdhani and vetter the resulting texture-mapped model can then be modified to produce a variety of visual effects including changing a person s weight or expression or three-dimensional effects such as re-lighting or video-based animation such models can also be used for video compression e.g. by only transmitting a small number of facial expression and pose parameters to drive a synthetic avatar wiegand and girod gao chen wang et al. facial animation is often matched to the performance of an actor in what is known as performance-driven animation traditional performancedriven animation systems use marker-based motion capture jones chiang et al. while some newer systems use video footage to control the animation finkelstein jacobs et al. pighin szeliski and salesin zhang snavely curless et al. vlasic brand pfister et al. an example of the latter approach is the system developed for the film benjamin button in which digital domain used the contour system from to capture actor brad pitt s facial motions and expressions and zafar contour uses a combination of phosphorescent paint and multiple high-resolution video cameras to capture real-time range scans of the actor. these models were then translated into facial action coding system shape and expression parameters and friesen to drive a different synthetically animated computer-generated imagery character. whole body modeling and tracking the topics of tracking humans modeling their shape and appearance and recognizing their activities are some of the most actively studied areas of computer vision. annual and special journal issues fua and ronfard are devoted to this subject and two recent surveys arikan ikemoto et al. moeslund hilton and httpwww.mova.com. international conference on automatic face and gesture recognition ieee workshop on analysis and modeling of faces and gestures and international workshop on tracking humans for the evaluation of their motion in image sequences computer vision algorithms and applications draft kr uger each list over papers devoted to these the humaneva database of articulated human contains multi-view video sequences of human actions along with corresponding motion capture data evaluation code and a reference tracker based on particle filtering. the companion paper by sigal balan and black not only describes the database and evaluation but also has a nice survey of important work in this field. given the breadth of this area it is difficult to categorize all of this research especially since different techniques usually build on each other. moeslund hilton and kr uger divide their survey into initialization tracking includes background modeling and segmentation pose estimation and action recognition. forsyth arikan ikemoto et al. divide their survey into sections on tracking subtraction deformable templates flow and probabilistic models recovering pose from observations and data association and body parts. they also include a section on motion synthesis which is more widely studied in computer graphics and forsyth kovar gleicher and pighin lee chai reitsma et al. li wang and shum pullen and bregler see section another potential taxonomy for work in this field would be along the lines of whether or multi-view images are used as input and whether or kinematic models are used. in this section we briefly review some of the more seminal and widely cited papers in the areas of background subtraction initialization and detection tracking with flow kinematic models probabilistic models adaptive shape modeling and activity recognition. we refer the reader to the previously mentioned surveys for other topics and more details. background subtraction. one of the first steps in many certainly not all human tracking systems is to model the background in order to extract the moving foreground objects corresponding to people. toyama krumm brumitt et al. review several difference matting and background maintenance techniques and provide a good introduction to this topic. stauffer and grimson describe some techniques based on mixture models while sidenbladh and black develop a more comprehensive treatment which models not only the background image statistics but also the appearance of the foreground objects e.g. their edge and motion difference statistics. once silhouettes have been extracted from one or more cameras they can then be modeled using deformable templates or other contour models and hogg wren azarbayejani darrell et al. tracking such silhouettes over time supports the analysis of multiple people moving around a scene including building shape and appearance models older surveys include those by gavrila and moeslund and granum some surveys on gesture recognition which we do not cover in this book include those by pavlovi c sharma and huang and yang ahuja and tabb httpvision.cs.brown.eduhumaneva. model-based reconstruction and detecting if they are carrying objects harwood and davis mittal and davis dimitrijevic lepetit and fua initialization and detection. in order to track people in a fully automated manner it is necessary to first detect re-acquire their presence in individual video frames. this topic is closely related to pedestrian detection which is often considered as a kind of object recognition ren efros et al. felzenszwalb and huttenlocher felzenszwalb mcallester and ramanan and is therefore treated in more depth in section additional techniques for initializing trackers based on images include those described by howe leventon and freeman rosales and sclaroff shakhnarovich viola and darrell sminchisescu kanaujia li et al. agarwal and triggs lee and cohen sigal and black and stenger thayananthan torr et al. single-frame human detection and pose estimation algorithms can sometimes be used by themselves to perform tracking forsyth and zisserman rogez rihan ramalingam et al. bourdev and malik as described in section more often however they are combined with frame-to-frame tracking techniques to provide better reliability dimitrijevic lepetit et al. andriluka roth and schiele ferrari marin-jimenez and zisserman tracking with flow. the tracking of people and their pose from frame to frame can be enhanced by computing optic flow or matching the appearance of their limbs from one frame to another. for example the cardboard people model of ju black and yacoob models the appearance of each leg portion and lower as a moving rectangle and uses optic flow to estimate their location in each subsequent frame. cham and rehg and sidenbladh black and fleet track limbs using optical flow and templates along with techniques for dealing with multiple hypotheses and uncertainty. bregler malik and pullen use a full model of limb and body motion as described below. it is also possible to match the estimated motion field itself to some prototypes in order to identify the particular phase of a running motion or to match two low-resolution video portions in order to perform video replacement berg mori et al. kinematic models. the effectiveness of human modeling and tracking can be greatly enhanced using a more accurate model of a person s shape and motion. underlying such representations which are ubiquitous in computer animation in games and special effects is a kinematic model or kinematic chain which specifies the length of each limb in a skeleton as well as the or rotation angles between the limbs or segments b. inferring the values of the joint angles from the locations of the visible surface points is called inverse kinematics and is widely studied in computer graphics. computer vision algorithms and applications draft figure tracking human motion kinematic chain model for a human hand morris and kanade reprinted by permission of sage tracking a kinematic chain blob model in a video sequence malik and pullen springer d probabilistic loose-limbed collection of body parts bhatia roth et al. figure shows the kinematic model for a human hand used by rehg morris and kanade to track hand motion in a video. as you can see the attachment points between the fingers and the thumb have two degrees of freedom while the finger joints themselves have only one. using this kind of model can greatly enhance the ability of an edge-based tracker to cope with rapid motion ambiguities in pose and partial occlusions. kinematic chain models are even more widely used for whole body modeling and tracking rourke and badler hogg rohr one popular approach is to associate an ellipsoid or superquadric with each rigid limb in the kinematic model as shown in figure this model can then be fitted to each frame in one or more video streams either by matching silhouettes extracted from known backgrounds or by matching and tracking the locations of occluding edges and davis kakadiaris and metaxas bregler malik and pullen kehl and van gool note that some techniques use models coupled to measurements some use measurements data or multi-view video with models and some use monocular video to infer and track models directly. it is also possible to use temporal models to improve the tracking of periodic motions such as walking by analyzing the joint angles as functions of time and nelson seitz and dyer cutler and davis the generality and applicability of such techniques can be improved by learning typical motion patterns using principal component analysis black and fleet urtasun fleet and fua probabilistic models. because tracking can be such a difficult task sophisticated probabilistic inference techniques are often used to estimate the likely states of the person being tracked. one popular approach called particle filtering and blake was originally developed for tracking the outlines of people and hands as described in section model-based reconstruction figure estimating human shape and pose from a single image using a parametric model weiss b alan et al. ieee. it was subsequently applied to whole-body tracking blake and reid sidenbladh black and fleet deutscher and reid and continues to be used in modern trackers micilotta bowden et al. alternative approaches to handling the uncertainty inherent in tracking include multiple hypothesis tracking and rehg and inflated covariances and triggs figure d shows an example of a sophisticated spatio-temporal probabilistic graphical model called loose-limbed people which models not only the geometric relationship between various limbs but also their likely temporal dynamics bhatia roth et al. the conditional probabilities relating various limbs and time instances are learned from training data and particle filtering is used to perform the final pose inference. adaptive shape modeling. another essential component of whole body modeling and tracking is the fitting of parameterized shape models to visual data. as we saw in section the availability of large numbers of registered range scans can be used to create morphable models of shape and appearance curless and popovi c building on this work anguelov srinivasan koller et al. develop a sophisticated system called scape completion and animation for people which first computer vision algorithms and applications draft acquires a large number of range scans of different people and of one person in different poses and then registers these scans using semi-automated marker placement. the registered datasets are used to model the variation in shape as a function of personal characteristics and skeletal pose e.g. the bulging of muscles as certain joints are flexed top row. the resulting system can then be used for shape completion i.e. the recovery of a full mesh model from a small number of captured markers by finding the best model parameters in both shape and pose space that fit the measured data. because it is constructed completely from scans of people in close-fitting clothing and uses a parametric shape model the scape system cannot cope with people wearing loosefitting clothing. b alan and black overcome this limitation by estimating the body shape that fits within the visual hull of the same person observed in multiple poses while vlasic baran matusik et al. adapt an initial surface mesh fitted with a parametric shape model to better match the visual hull. while the preceding body fitting and pose estimation systems use multiple views to estimate body shape even more recent work by guan weiss b alan et al. can fit a human shape and pose model to a single image of a person on a natural background. manual initialization is used to estimate a rough pose and height model and this is then used to segment the person s outline using the grab cut segmentation algorithm the shape and pose estimate are then refined using a combination of silhouette edge cues and shading information the resulting model can be used to create novel animations. activity recognition. the final widely studied topic in human modeling is motion activity and action recognition hu tan wang et al. hilton fua and ronfard examples of actions that are commonly recognized include walking and running jumping dancing picking up objects sitting down and standing up and waving. recent representative papers on these topics have been written by robertson and reid sminchisescu kanaujia and metaxas weinland ronfard and boyer yilmaz and shah and gorelick blank shechtman et al. recovering texture maps and albedos after a model of an object or person has been acquired the final step in modeling is usually to recover a texture map to describe the object s surface appearance. this first requires establishing a parameterization for the v texture coordinates as a function of surface position. one simple way to do this is to associate a separate texture map with each triangle pair of triangles. more space-efficient techniques involve unwrapping the surface onto recovering texture maps and albedos one or more maps e.g. using a subdivision mesh derose duchamp et al. or a geometry image gortler and hoppe once the v coordinates for each triangle have been fixed the perspective projection equations mapping from texture v to an image j s pixel vj coordinates can be obtained by concatenating the affine v y z mapping with the perspective homography y z vj and shum the color values for the v texture map can then be re-sampled and stored or the original image can itself be used as the texture source using projective texture mapping the situation becomes more involved when more than one source image is available for appearance recovery which is the usual case. one possibility is to use a view-dependent texture map in which a different source image combination of source images is used for each polygonal face based on the angles between the virtual camera the surface normals and the source images taylor and malik pighin hecker lischinski et al. an alternative approach is to estimate a complete surface light field for each surface point azuma aldinger et al. as described in section in some situations e.g. when using models in traditional games it is preferable to merge all of the source images into a single coherent texture map during pre-processing. ideally each surface triangle should select the source image where it is seen most directly to its normal and at the resolution best matching the texture map this can be posed as a graph cut optimization problem where the smoothness term encourages adjacent triangles to use similar source images followed by blending to compensate for exposure differences and ivanov sinha steedly szeliski et al. even better results can be obtained by explicitly modeling geometric and photometric misalignments between the source images and szeliski gal wexler ofek et al. these kinds of approaches produce good results when the lighting stays fixed with respect to the object i.e. when the camera moves around the object or space. when the lighting is strongly directional however and the object is being moved relative to this lighting strong shading effects or specularities may be present which will interfere with the reliable recovery of a texture map. in this case it is preferable to explicitly undo the shading effects by modeling the light source directions and estimating the surface reflectance properties while recovering the texture map and ikeuchi sato wheeler and ikeuchi yu and malik yu debevec malik et al. figure shows the results of one such approach where the specularities are first removed while estimating the matte reflectance component and then later re-introduced by estimating the specular component ks in a torrance sparrow reflection model when surfaces are seen at oblique viewing angles it may be necessary to blend different images together to obtain the best resolution kang szeliski et al. computer vision algorithms and applications draft figure estimating the diffuse albedo and reflectance parameters for a scanned model wheeler and ikeuchi acm set of input images projected onto the model the complete diffuse reflection model rendering from the reflectance model including the specular component. estimating brdfs a more ambitious approach to the problem of view-dependent appearance modeling is to estimate a general bidirectional reflectance distribution function for each point on an object s surface. dana van ginneken nayar et al. jensen marschner levoy et al. and lensch kautz goesele et al. present different techniques for estimating such functions while dorsey rushmeier and sillion and weyrich lawrence lensch et al. present more recent surveys of the topics of brdf modeling recovery and rendering. as we saw in section the brdf can be written as fr i i r r where i i and r r are the angles the incident vi and reflected vr light ray directions make with the local surface coordinate frame dx dy n shown in figure when modeling the appearance of an object as opposed to the appearance of a patch of material we need to estimate this function at every point y on the object s surface which gives us the spatially varying brdf or svbrdf lawrence lensch et al. fvx y i i r r if sub-surface scattering effects are being modeled such as the long-range transmission of light through materials such as alabaster the eight-dimensional bidirectional scatteringsurface reflectance-distribution function is used instead fexi yi i i xe ye e e where the e subscript now represents the emitted rather than the reflected light directions. recovering texture maps and albedos figure image-based reconstruction of appearance and detailed geometry kautz goesele et al. acm. appearance models are re-estimated using divisive clustering. in order to model detailed spatially varying appearance each lumitexel is projected onto the basis formed by the clustered materials. weyrich lawrence lensch et al. provide a nice survey of these and related topics including basic photometry brdf models traditional brdf acquisition using gonio reflectometry precise measurement of visual angles and reflectances multiplexed illumination nayar and belhumeur skin modeling hawkins tchou et al. weyrich matusik pfister et al. and image-based acquisition techniques which simultaneously recover an object s shape and reflectometry from multiple photographs. a nice example of this latter approach is the system developed by lensch kautz goesele et al. who estimate locally varying brdfs and refine their shape models using local estimates of surface normals. to build up their models they first associate a lumitexels which contains a position a surface normal and a set of sparse radiance samples with each surface point. next they cluster such lumitexels into materials that share common properties using a lafortune reflectance model foo torrance et al. and a divisive clustering approach finally in order to model detailed spatially varying appearance each lumitexel point is projected onto the basis of clustered appearance models while most of the techniques discussed in this section require large numbers of views to estimate surface properties a challenging future direction will be to take these techniques out of the lab and into the real world and to combine them with regular and internet photo image-based modeling approaches. application photography the techniques described in this chapter for building complete models from multiple images and then recovering their surface appearance have opened up a whole new range of applications that often go under the name photography. pollefeys and van gool provide a nice introduction to this field including the processing steps of feature matching computer vision algorithms and applications draft structure from motion dense depth map estimation model building and texture map recovery. a complete web-based system for automatically performing all of these tasks called is described by vergauwen and van gool and moons van gool and vergauwen the latter paper provides not only an in-depth survey of this whole field but also a detailed description of their complete end-to-end system. an alternative to such fully automated systems is to put the user in the loop in what is sometimes called interactive computer vision. van den hengel dick thormhlen et al. describe their videotrace system which performs automated point tracking and structure recovery from video and then lets the user draw triangles and surfaces on top of the resulting point cloud as well as interactively adjusting the locations of model vertices. sinha steedly szeliski et al. describe a related system that uses matched vanishing points in multiple images to infer line orientations and plane normals. these are then used to guide the user drawing axis-aligned planes which are automatically fitted to the recovered point cloud. fully automated variants on these ideas are described by zebedin bauer karner et al. furukawa curless seitz et al. furukawa curless seitz et al. mi cu s k and ko seck a and sinha steedly and szeliski as the sophistication and reliability of these techniques continues to improve we can expect to see even more user-friendly applications for photorealistic modeling from images additional reading shape from shading is one of the classic problems in computer vision some representative papers in this area include those by horn ikeuchi and horn pentland horn and brooks horn szeliski mancini and wolff dupuis and oliensis and fua and leclerc the collection of papers edited by horn and brooks is a great source of information on this topic especially the chapter on variational approaches. the survey by zhang tsai cryer et al. not only reviews more recent techniques but also provides some comparative results. woodham wrote the seminal paper of photometric stereo. shape from texture techniques include those by witkin ikeuchi blostein and ahuja garding malik and rosenholtz liu collins and tsin liu lin and hays hays leordeanu efros et al. lin hays wu et al. lobay and forsyth white and forsyth white crane and forsyth and park brocklehurst collins et al. good papers and books on depth from defocus have been written by pentland nayar and nakagawa nayar watanabe and noguchi these earlier steps are also discussed in section additional reading watanabe and nayar chaudhuri and rajagopalan and favaro and soatto additional techniques for recovering shape from various kinds of illumination effects including inter-reflections ikeuchi and kanade are discussed in the book on shape recovery edited by wolff shafer and healey active rangefinding systems which use laser or natural light illumination projected into the scene have been described by besl rioux and bird kang webb zitnick et al. curless and levoy curless and levoy proesmans van gool and defoort bouguet and perona curless hebert iddan and yahav goesele fuchs and seidel scharstein and szeliski davis ramamoorthi and rusinkiewicz zhang curless and seitz zhang snavely curless et al. and moons van gool and vergauwen individual range scans can be aligned using correspondence and distance optimization techniques such as iterated closest points and its variants and mckay zhang szeliski and lavall ee johnson and kang gold rangarajan lu et al. johnson and hebert pulli david dementhon duraiswami et al. li and hartley enqvist josephson and kahl once they have been aligned range scans can be merged using techniques that model the signed distance of surfaces to volumetric sample points derose duchamp et al. curless and levoy hilton stoddart illingworth et al. wheeler sato and ikeuchi kazhdan bolitho and hoppe lempitsky and boykov zach pock and bischof zach once constructed surfaces can be modeled and manipulated using a variety of threedimensional representations which include triangle meshes derose duchamp et al. hoppe splines lee wolberg and shin subdivision surfaces derose and salesin zorin schr oder and sweldens warren and weimer peters and reif and geometry images gortler and hoppe alternatively they can be represented as collections of point samples with local orientation estimates derose duchamp et al. szeliski and tonnesen turk and o brien pfister zwicker van baar et al. alexa behr cohen-or et al. pauly keiser kobbelt et al. diebel thrun and br unig guennebaud and gross guennebaud germann and gross oztireli guennebaud and gross they can also be modeled using implicit inside outside characteristic or signed distance functions sampled on regular or irregular volumetric grids ee and szeliski szeliski and lavall ee frisken perry rockwood et al. dinh turk and slabaugh kazhdan bolitho and hoppe lempitsky and boykov zach pock and bischof zach the literature on model-based reconstruction is extensive. for modeling architecture and urban scenes both interactive and fully automated systems have been developed. a special journal issue devoted to the reconstruction of large-scale scenes and kanade computer vision algorithms and applications draft is a good source of references and robertson and cipolla give a nice description of a complete system. lots of additional references can be found in section face and whole body modeling and tracking is a very active sub-field of computer vision with its own conferences and workshops e.g. the international conference on automatic face and gesture recognition the ieee workshop on analysis and modeling of faces and gestures and the international workshop on tracking humans for the evaluation of their motion in image sequences recent survey articles on the topic of whole body modeling and tracking include those by forsyth arikan ikemoto et al. moeslund hilton and kr uger and sigal balan and black exercises ex shape from focus grab a series of focused images with a digital slr set to manual focus get one that allows for programmatic focus control and recover the depth of an object. take some calibration images e.g. of a checkerboard so you can compute a mapping between the amount of defocus and the focus setting. try both a fronto-parallel planar target and one which is slanted so that it covers the working range of the sensor. which one works better? now put a real object in the scene and perform a similar focus sweep. for each pixel compute the local sharpness and fit a parabolic curve over focus settings to find the most in-focus setting. map these focus settings to depth and compare your result to ground truth. if you are using a known simple object such as sphere or cylinder ball or a soda can it s easy to measure its true shape. see if you can recover the depth map from just two or three focus settings. use an lcd projector to project artificial texture onto the scene. use a pair of cameras to compare the accuracy of your shape from focus and shape from stereo techniques. create an all-in-focus image using the technique of agarwala dontcheva agrawala et al. ex shadow striping implement the handheld shadow striping system of bouguet and perona the basic steps include the following. exercises set up two background planes behind the object of interest and calculate their orienta tion relative to the viewer e.g. with fiducial marks. cast a moving shadow with a stick across the scene record the video or capture the data with a webcam. estimate each light plane equation from the projections of the cast shadow against the two backgrounds. triangulate to the remaining points on each curve to get a stripe and display the stripes using a graphics engine. remove the requirement for a known second plane and infer its location that of the light source using the techniques described by bouguet and perona the techniques from exercise may also be helpful here. ex range data registration register two or more datasets using either iterated closest points and mckay zhang gold rangarajan lu et al. or octree signed distance fields and lavall ee apply your technique to narrow-baseline stereo pairs e.g. obtained by moving a camera around an object using structure from motion to recover the camera poses and using a standard stereo matching algorithm. ex range data merging merge the datasets that you registered in the previous exercise using signed distance fields and levoy hilton stoddart illingworth et al. you can optionally use an octree to represent and compress this field if you already implemented it in the previous registration step. extract a meshed surface model from the signed distance field using marching cubes and display the resulting model. ex surface simplification use progressive meshes or some other technique from section to create a hierarchical simplification of your surface model. ex architectural modeler build a interior or exterior model of some architectural structure such as your house from a series of handheld wide-angle photographs. extract lines and vanishing points to estimate the dominant di rections in each image. use structure from motion to recover all of the camera poses and match up the vanish ing points. computer vision algorithms and applications draft let the user sketch the locations of the walls by drawing lines corresponding to wall bottoms tops and horizontal extents onto the images steedly szeliski et al. see also exercise do something similar for openings and windows and simple furniture and countertops. convert the resulting polygonal meshes into a model vrml and optionally texture-map these surfaces from the images. ex body tracker download the video sequences from the humaneva web either implement a human motion tracker from scratch or extend the code on that web site balan and black in some interesting way. ex photography combine all of your previously developed techniques to produce a system that takes a series of photographs or a video and constructs a photorealistic texture-mapped model. httpvision.cs.brown.eduhumaneva. chapter image-based rendering view interpolation layered depth images light fields and lumigraphs view-dependent texture maps application photo tourism impostors sprites and layers unstructured lumigraph surface light fields application concentric mosaics higher-dimensional light fields the modeling to rendering continuum video-based animation video textures application animating pictures video application video-based walkthroughs additional reading exercises environment mattes video-based rendering computer vision algorithms and applications draft figure image-based and video-based rendering a view of a photo tourism reconstruction seitz and szeliski acm a slice through a light field grzeszczuk szeliski et al. acm sprites with depth gortler he et al. acm surface light field azuma aldinger et al. acm environment matte in front of a novel background werner curless et al. acm real-time video environment matte zongker hindorff et al. acm video rewrite used to re-animate old video covell and slaney acm video texture of a candle flame odl szeliski salesin et al. acm video view interpolation kang uyttendaele et al. acm. view interpolation over the last two decades image-based rendering has emerged as one of the most exciting applications of computer vision li tong et al. shum chan and kang in image-based rendering reconstruction techniques from computer vision are combined with computer graphics rendering techniques that use multiple views of a scene to create interactive photo-realistic experiences such as the photo tourism system shown in figure commercial versions of such systems include immersive street-level navigation in on-line mapping and the creation of from large collections of casually acquired photographs. in this chapter we explore a variety of image-based rendering techniques such as those illustrated in figure we begin with view interpolation which creates a seamless transition between a pair of reference images using one or more pre-computed depth maps. closely related to this idea are view-dependent texture maps which blend multiple texture maps on a model s surface. the representations used for both the color imagery and the geometry in view interpolation include a number of clever variants such as layered depth images and sprites with depth we continue our exploration of image-based rendering with the light field and lumigraph four-dimensional representations of a scene s appearance which can be used to render the scene from any arbitrary viewpoint. variants on these representations include the unstructured lumigraph surface light fields concentric mosaics and environment mattes the last part of this chapter explores the topic of video-based rendering which uses one or more videos in order to create novel video-based experiences the topics we cover include video-based facial animation as well as video textures in which short video clips can be seamlessly looped to create dynamic realtime video-based renderings of a scene. we close with a discussion of videos created from multiple video streams as well as video-based walkthroughs of environments which have found widespread application in immersive outdoor mapping and driving direction systems. view interpolation while the term image-based rendering first appeared in the papers by chen and mcmillan and bishop the work on view interpolation by chen and williams is considered as the seminal paper in the field. in view interpolation pairs of rendered color images are combined with their pre-computed depth maps to generate interpolated views that httpmaps.bing.com and httpmaps.google.com. httpphotosynth.net. computer vision algorithms and applications draft figure view interpolation and williams acm holes from one source image in blue holes after combining two widely spaced images holes after combining two closely spaced images after interpolation filling. mimic what a virtual camera would see in between the two reference views. view interpolation combines two ideas that were previously used in computer vision and computer graphics. the first is the idea of pairing a recovered depth map with the reference image used in its computation and then using the resulting texture-mapped model to generate novel views the second is the idea of morphing where correspondences between pairs of images are used to warp each reference image to an in-between location while simultaneously cross-dissolving between the two warped images. figure illustrates this process in more detail. first both source images are warped to the novel view using both the knowledge of the reference and virtual camera pose along with each image s depth map in the paper by chen and williams a forward warping algorithm and figure is used. the depth maps are represented as quadtrees for both space and rendering time efficiency during the forward warping process multiple pixels occlude one another may land on the same destination pixel. to resolve this conflict either a z-buffer depth value can be associated with each destination pixel or the images can be warped in back-to-front order which can be computed based on the knowledge of epipolar geometry and williams laveau and faugeras mcmillan and bishop once the two reference images have been warped to the novel view b they can be merged to create a coherent composite whenever one of the images has a hole as a cyan pixel the other image is used as the final value. when both images have pixels to contribute these can be blended as in usual morphing i.e. according to the relative distances between the virtual and source cameras. note that if the two images have very different exposures which can happen when performing view interpolation on real images the hole-filled regions and the blended regions will have different exposures leading view interpolation to subtle artifacts. the final step in view interpolation is to fill any remaining holes or cracks due to the forward warping process or lack of source data visibility. this can be done by copying pixels from the further pixels adjacent to the hole. foreground objects are subject to a fattening effect the above process works well for rigid scenes although its visual quality of aliasing can be improved using a two-pass forward backward algorithm gortler he et al. or full rendering kang uyttendaele et al. in the case where the two reference images are views of a non-rigid scene e.g. a person smiling in one image and frowning in the other view morphing which combines ideas from view interpolation with regular morphing can be used and dyer while the original view interpolation paper describes how to generate novel views based on similar pre-computed perspective images the plenoptic modeling paper of mcmillan and bishop argues that cylindrical images should be used to store the pre-computed rendering or real-world images. also propose using environment maps cubic or spherical as source images for view interpolation. view-dependent texture maps view-dependent texture maps taylor and malik are closely related to view interpolation. instead of associating a separate depth map with each input image a single model is created for the scene but different images are used as texture map sources depending on the virtual camera s current position in more detail given a new virtual camera position the similarity of this camera s view of each polygon pixel is compared to that of potential source images. the images are then blended using a weighting that is inversely proportional to the angles i between the virtual view and the source views even though the geometric model can be fairly coarse blending between different views gives a strong sense of more detailed geometry because of the parallax motion between corresponding pixels. while the original paper performs the weighted blend computation separately at each pixel or coarsened polygon face follow-on work by debevec yu and borshukov presents a more efficient implementation based on precomputing contributions for various portions of viewing space and then using projective texture mapping the idea of view-dependent texture mapping has been used in a large number of subsequent image-based rendering systems including facial modeling and animation the term image-based modeling which is now commonly used to describe the creation of texture-mapped models from multiple images appears to have first been used by debevec taylor and malik who also used the term photogrammetric modeling to describe the same process. computer vision algorithms and applications draft figure view-dependent texture mapping taylor and malik acm. the weighting given to each input view depends on the relative angles between the novel view and the original views simplified model geometry with viewdependent texture mapping the geometry appears to have more detail windows. hecker lischinski et al. and scanning and visualization abi-rached duchamp et al. closely related to view-dependent texture mapping is the idea of blending between light rays in space which forms the basis of the lumigraph and unstructured lumigraph systems grzeszczuk szeliski et al. buehler bosse mcmillan et al. in order to provide even more realism in their fac ade system debevec taylor and malik also include a model-based stereo component which optionally computes an offset map for each coarse planar facet of their model. they call the resulting analysis and rendering system a hybrid geometry- and image-based approach since it uses traditional geometric modeling to create the global model but then uses local depth offsets along with view interpolation to add visual realism. application photo tourism while view interpolation was originally developed to accelerate the rendering of scenes on low-powered processors and systems without graphics acceleration it turns out that it can be applied directly to large collections of casually acquired photographs. the photo tourism system developed by snavely seitz and szeliski uses structure from motion to compute the locations and poses of all the cameras taking the images along with a sparse point-cloud model of the scene figure to perform an image-based exploration of the resulting sea of images funkhouser yanovsky et al. photo tourism first associates a proxy with each image. while a triangulated mesh obtained from the point cloud can sometimes form a suitable proxy e.g. for outdoor terrain models a simple dominant plane fit to the points visible in each image view interpolation figure photo tourism seitz and szeliski acm a overview of the scene with translucent washes and lines painted onto the planar impostors once the user has selected a region of interest a set of related thumbnails is displayed along the bottom planar proxy selection for optimal stabilization garg seitz et al. acm. often performs better because it does not contain any erroneous segments or connections that pop out as artifacts. as automated modeling techniques continue to improve however the pendulum may swing back to more detailed geometry snavely curless et al. sinha steedly and szeliski the resulting image-based navigation system lets users move from photo to photo either by selecting cameras from a top-down view of the scene or by selecting regions of interest in an image navigating to nearby views or selecting related thumbnails to create a background for the scene e.g. when being viewed from above non-photorealistic techniques such as translucent color washes or highlighted line segments can be used the system can also be used to annotate regions of images and to automatically propagate such annotations to other photographs. the planar proxies used in photo tourism and the related photosynth system from microsoft result in non-photorealistic transitions reminiscent of visual effects such as page flips selecting a stable axis for all the planes can reduce the amount of swimming and enhance the perception of garg seitz et al. it is also possible to automatically detect objects in the scene that are seen from multiple views and create orbits of viewpoints around such objects. furthermore nearby images in both position and viewing direction can be linked to create virtual paths which can then be used to navigate between arbitrary pairs of images such as those you might take yourself while walking around a popular tourist site garg seitz et al. the spatial matching of image features and regions performed by photo tourism can also be used to infer more information from large image collections. for example simon snavely and seitz show how the match graph between images of popular tourist sites computer vision algorithms and applications draft can be used to find the most iconic photographed objects in the collection along with their related tags. in follow-on work simon and seitz show how such tags can be propagated to sub-regions of each image using an analysis of which points appear in the central portions of photographs. extensions of these techniques to all of the world s images including the use of gps tags where available have been investigated as well wu zach et al. quack leibe and van gool crandall backstrom huttenlocher et al. li crandall and huttenlocher zheng zhao song et al. layered depth images traditional view interpolation techniques associate a single depth map with each source or reference image. unfortunately when such a depth map is warped to a novel view holes and cracks inevitably appear behind the foreground objects. one way to alleviate this problem is to keep several depth and color values pixels at every pixel in a reference image at least for pixels near foreground background transitions the resulting data structure which is called a layered depth image can be used to render new views using a back-to-front forward warping algorithm gortler he et al. impostors sprites and layers an alternative to keeping lists of color-depth values at each pixel as is done in the ldi is to organize objects into different layers or sprites. the term sprite originates in the computer game industry where it is used to designate flat animated characters in games such as pacman or mario bros. when put into a setting such objects are often called impostors because they use a piece of flat alpha-matted geometry to represent simplified versions of objects that are far away from the camera lischinski salesin et al. lengyel and snyder torborg and kajiya in computer vision such representations are usually called layers and adelson baker szeliski and anandan torr szeliski and anandan birchfield natarajan and tomasi section discusses the topics of transparent layers and reflections which occur on specular and transparent surfaces such as glass. while flat layers can often serve as an adequate representation of geometry and appearance for far-away objects better geometric fidelity can be achieved by also modeling the per-pixel offsets relative to a base plane as shown in figures and b. such representations are called plane plus parallax in the computer vision literature anandan and hanna sawhney szeliski and coughlan baker szeliski and anandan as discussed in section in addition to fully automated stereo techniques it is also possible to paint in depth layers oh chen dorsey et al. layered depth images figure a variety of image-based rendering primitives which can be used depending on the distance between the camera and the object of interest gortler he et al. acm. closer objects may require more detailed polygonal representations while mid-level objects can use a layered depth image and far-away objects can use sprites with depth and environment maps. figure sprites with depth gortler he et al. acm alphamatted color sprite corresponding relative depth or parallax rendering without relative depth rendering with depth the curved object boundaries. computer vision algorithms and applications draft shum sun yamazaki et al. or to infer their structure from monocular image cues efros and hebert saxena sun and ng how can we render a sprite with depth from a novel viewpoint? one possibility as with a regular depth map is to just forward warp each pixel to its new location which can cause aliasing and cracks. a better way which we already mentioned in section is to first warp the depth v displacement map to the novel view fill in the cracks and then use higher-quality inverse warping to resample the color image gortler he et al. figure shows the results of applying such a two-pass rendering algorithm. from this still image you can appreciate that the foreground sprites look more rounded however to fully appreciate the improvement in realism you would have to look at the actual animated sequence. sprites with depth can also be rendered using conventional graphics hardware as described in kang uyttendaele et al. rogmans lu bekaert et al. describe gpu implementations of both real-time stereo matching and real-time forward and inverse rendering algorithms. light fields and lumigraphs while image-based rendering approaches can synthesize scene renderings from novel viewpoints they raise the following more general question is is possible to capture and render the appearance of a scene from all possible viewpoints and if so what is the complexity of the resulting structure? let us assume that we are looking at a static scene i.e. one where the objects and illuminants are fixed and only the observer is moving around. under these conditions we can describe each image by the location and orientation of the virtual camera dof as well as its intrinsics its focal length. however if we capture a two-dimensional spherical image around each possible camera location we can re-render any view from this thus taking the cross-product of the three-dimensional space of camera positions with the space of spherical images we obtain the plenoptic function of adelson and bergen which forms the basis of the image-based rendering system of mcmillan and bishop notice however that when there is no light dispersion in the scene i.e. no smoke or fog all the coincident rays along a portion of free space solid or refractive objects have the same color value. under these conditions we can reduce the plenoptic function to since we are counting dimensions we ignore for now any sampling or resolution issues. light fields and lumigraphs figure the lumigraph grzeszczuk szeliski et al. acm a ray is represented by its two-plane parameters t and v a slice through the light field subset v s. the light field of all possible rays grzeszczuk szeliski et al. levoy and hanrahan levoy to make the parameterization of this function simpler let us put two planes in the scene roughly bounding the area of interest as shown in figure any light ray terminating at a camera that lives in front of the st plane that this space is empty passes through the two planes at t and v and can be described by its coordinate t u v. this diagram parameterization can be interpreted as describing a family of cameras living on the st plane with their image planes being the uv plane. the uv plane can be placed at infinity which corresponds to all the virtual cameras looking in the same direction. in practice if the planes are of finite extent the finite light slab ls t u v can be used to generate any synthetic view that a camera would see through a viewport in the st plane with a view frustum that wholly intersects the far uv plane. to enable the camera to move all the way around an object the space surrounding the object can be split into multiple domains each with its own light slab parameterization. conversely if the camera is moving inside a bounded volume of free space looking outward multiple cube faces surrounding the camera can be used as t planes. levoy and hanrahan borrowed the term light field from a paper by gershun another name for this representation is the photic field and spencer stuvstuvcamera centerimage plane pixel computer vision algorithms and applications draft figure depth compensation in the lumigraph grzeszczuk szeliski et al. acm. to resample the u dashed light ray the u parameter corresponding to each discrete si camera location is modified according to the out-of-plane depth z to yield new coordinates u and in s ray space the original sample is resampled from the and samples which are themselves linear blends of their adjacent samples. thinking about spaces is difficult so let us drop our visualization by one dimension. if we fix the row value t and constrain our camera to move along the s axis while looking at the uv plane we can stack all of the stabilized images the camera sees to get the v s epipolar volume which we discussed in section a horizontal cross-section through this volume is the well-known epipolar plane image baker and marimont which is the us slice shown in figure as you can see in this slice each color pixel moves along a linear track whose slope is related to its depth from the uv plane. exactly on the uv plane appear vertical i.e. they do not move as the camera moves along s. furthermore pixel tracks occlude one another as their corresponding surface elements occlude. translucent pixels however composite over background pixels rather than occluding them. thus we can think of adjacent pixels sharing a similar planar geometry as epi strips or epi tubes kang swaminathan et al. the equations mapping from pixels y in a virtual camera and the corresponding t u v coordinates are relatively straightforward to derive and are sketched out in exercise it is also possible to show that the set of pixels corresponding to a regular orthographic or perspective camera i.e. one that has a linear projective relationship between points and y pixels lie along a two-dimensional hyperplane in the t u v light field light fields and lumigraphs while a light field can be used to render a complex scene from novel viewpoints a much better rendering less ghosting can be obtained if something is known about its geometry. the lumigraph system of gortler grzeszczuk szeliski et al. extends the basic light field rendering approach by taking into account the location of surface points corresponding to each ray. consider the ray u corresponding to the dashed line in figure which intersects the object s surface at a distance z from the uv plane. when we look up the pixel s color in camera si that the light field is discretely sampled on a regular t u v grid the actual pixel coordinate is instead of the original u value specified by the u ray. similarly for camera si s pixel address is used. thus instead of using quadri-linear interpolation of the nearest sampled t u v values around a given ray to determine its color the v values are modified for each discrete ti camera. figure also shows the same reasoning in ray space. here the original continuousvalued u ray is represented by a triangle and the nearby sampled discrete values are shown as circles. instead of just blending the four nearest samples as would be indicated by the vertical and horizontal dashed lines the modified and values are sampled instead and their values are then blended. the resulting rendering system produces images of much better quality than a proxy-free light field and is the method of choice whenever geometry can be inferred. in subsequent work isaksen mcmillan and gortler show how a planar proxy for the scene which is a simpler model can be used to simplify the resampling equations. they also describe how to create synthetic aperture photos which mimic what might be seen by a wide-aperture lens by blending more nearby samples and hanrahan a similar approach can be used to re-focus images taken with a plenoptic array camera levoy br eedif et al. ng or a light field microscope ng adams et al. it can also be used to see through obstacles using extremely large synthetic apertures focused on a background that can blur out foreground objects and make them appear translucent joshi vaish et al. vaish szeliski zitnick et al. now that we understand how to render new images from a light field how do we go about capturing such data sets? one answer is to move a calibrated camera with a motion control rig or another approach is to take handheld photographs and to determine the pose and intrinsic calibration of each image using either a calibrated stage or structure from motion. in this case the images need to be rebinned into a regular t u v space before they can be used for rendering grzeszczuk szeliski et al. alternatively the original images can be used directly using a process called the unstructured lumigraph which we see httplightfield.stanford.eduacq.html for a description of some of the gantries and camera arrays built at the stanford computer graphics laboratory. this web site also provides a number of light field data sets that are a great source of research and project material. computer vision algorithms and applications draft describe below. because of the large number of images involved light fields and lumigraphs can be quite voluminous to store and transmit. fortunately as you can tell from figure there is a tremendous amount of redundancy in a light field which can be made even more explicit by first computing a model as in the lumigraph. a number of techniques have been developed to compress and progressively transmit such representations grzeszczuk szeliski et al. levoy and hanrahan rademacher and bishop magnor and girod wood azuma aldinger et al. shum kang and chan magnor ramanathan and girod shum chan and kang unstructured lumigraph when the images in a lumigraph are acquired in an unstructured manner it can be counterproductive to resample the resulting light rays into a regularly binned t u v data structure. this is both because resampling always introduces a certain amount of aliasing and because the resulting gridded light field can be populated very sparsely or irregularly. the alternative is to render directly from the acquired images by finding for each light ray in a virtual camera the closest pixels in the original images. the unstructured lumigraph rendering system of buehler bosse mcmillan et al. describes how to select such pixels by combining a number of fidelity criteria including epipole consistency of rays to a source camera s center angular deviation incidence direction on the surface resolution sampling density along the surface continuity nearby pixels and consistency the ray. these criteria can all be combined to determine a weighting function between each virtual camera s pixel and a number of candidate input cameras from which it can draw colors. to make the algorithm more efficient the computations are performed by discretizing the virtual camera s image plane using a regular grid overlaid with the polyhedral object mesh model and the input camera centers of projection and interpolating the weighting functions between vertices. the unstructured lumigraph generalizes previous work in both image-based rendering and light field rendering. when the input cameras are gridded the ulr behaves the same way as regular lumigraph rendering. when fewer cameras are available but the geometry is accurate the algorithm behaves similarly to view-dependent texture mapping surface light fields of course using a two-plane parameterization for a light field is not the only possible choice. is the one usually presented first since the projection equations and visualizations are the easiest to draw and understand. as we mentioned on the topic of light field compression light fields and lumigraphs figure surface light fields azuma aldinger et al. acm example of a highly specular object with strong inter-reflections the surface light field stores the light emanating from each surface point in all visible directions as a lumisphere if we know the shape of the object or scene whose light field is being modeled we can effectively compress the field because nearby rays emanating from nearby surface elements have similar color values. in fact if the object is totally diffuse ignoring occlusions which can be handled using graphics algorithms or z-buffering all rays passing through a given surface point will have the same color value. hence the light field collapses to the usual texture-map defined over an object s surface. conversely if the surface is totally specular mirrored each surface point reflects a miniature copy of the environment surrounding that point. in the absence of inter-reflections a convex object in a large open space each surface point simply reflects the far-field environment map which again is two-dimensional. therefore is seems that re-parameterizing the light field to lie on the object s surface can be extremely beneficial. these observations underlie the surface light field representation introduced by wood azuma aldinger et al. in their system an accurate model is built of the object being represented. then the lumisphere of all rays emanating from each surface point is estimated or captured nearby lumispheres will be highly correlated and hence amenable to both compression and manipulation. to estimate the diffuse component of each lumisphere a median filtering over all visible exiting directions is first performed for each channel. once this has been subtracted from the lumisphere the remaining values which should consist mostly of the specular components are reflected around the local surface normal which turns each lumisphere into a copy of the local environment around that point. nearby lumispheres can then be compressed using predictive coding vector quantization or principal component analysis. computer vision algorithms and applications draft the decomposition into a diffuse and specular component can also be used to perform editing or manipulation operations such as re-painting the surface changing the specular component of the reflection by blurring or sharpening the specular lumispheres or even geometrically deforming the object while preserving detailed surface appearance. application concentric mosaics a useful and simple version of light field rendering is a panoramic image with parallax i.e. a video or series of photographs taken from a camera swinging in front of some rotation point. such panoramas can be captured by placing a camera on a boom on a tripod or even more simply by holding a camera at arm s length while rotating your body around a fixed axis. the resulting set of images can be thought of as a concentric mosaic and he shum wang chai et al. or a layered depth panorama kang cohen et al. the term concentric mosaic comes from a particular structure that can be used to re-bin all of the sampled rays essentially associating each column of pixels with the radius of the concentric circle to which it is tangent and he peleg ben-ezra and pritch rendering from such data structures is fast and straightforward. if we assume that the scene is far enough away for any virtual camera location we can associate each column of pixels in the virtual camera with the nearest column of pixels in the input image set. a regularly captured set of images this computation can be performed analytically. if we have some rough knowledge of the depth of such pixels columns can be stretched vertically to compensate for the change in depth between the two cameras. if we have an even more detailed depth map ben-ezra and pritch li shum tang et al. zheng kang cohen et al. we can perform pixel-by-pixel depth corrections. while the virtual camera s motion is constrained to lie in the plane of the original cameras and within the radius of the original capture ring the resulting experience can exhibit complex rendering phenomena such as reflections and translucencies which cannot be captured using a texture-mapped model of the world. exercise has you construct a concentric mosaic rendering system from a series of hand-held photos or video. environment mattes so far in this chapter we have dealt with view interpolation and light fields which are techniques for modeling and rendering complex static scenes seen from different viewpoints. what if instead of moving around a virtual camera we take a complex refractive object such as the water goblet shown in figure and place it in front of a new background? environment mattes figure environment mattes b a refractive object can be placed in front of a series of backgrounds and their light patterns will be correctly refracted werner curless et al. multiple refractions can be handled using a mixture of gaussians model and real-time mattes can be pulled using a single graded colored background zongker hindorff et al. acm. instead of modeling the space of rays emanating from a scene we now need to model how each pixel in our view of this object refracts incident light coming from its environment. what is the intrinsic dimensionality of such a representation and how do we go about capturing it? let us assume that if we trace a light ray from the camera at pixel y toward the object it is reflected or refracted back out toward its environment at an angle if we assume that other objects and illuminants are sufficiently distant same assumption we made for surface light fields in section this mapping y captures all the information between a refractive object and its environment. zongker werner curless et al. call such a representation an environment matte since it generalizes the process of object matting to not only cut and paste an object from one image into another but also take into account the subtle refractive or reflective interplay between the object and its environment. recall from equations and that a foreground object can be represented by its premultiplied colors and opacities f such a matte can then be composited onto a new background b using ci ifi ibi where i is the pixel under consideration. in environment matting we augment this equation with a reflective or refractive term to model indirect light paths between the environment and the camera. in the original work of zongker werner curless et al. this indirect component ii is modeled as ii aixbxdx where ai is the rectangular area of support for that pixel ri is the colored reflectance or computer vision algorithms and applications draft transmittance colored glossy surfaces or glass and bx is the background image which is integrated over the area aix. in follow-on work chuang zongker hindorff et al. use a superposition of oriented gaussians where each gaussian ii gijxbxdx gijx cij ij ij is modeled by its center cij unrotated widths ij x ij y ij and orientation ij. given a representation for an environment matte how can we go about estimating it for a particular object? the trick is to place the object in front of a monitor surrounded by a set of monitors where we can change the illumination patterns bx and observe the value of each composite pixel as with traditional two-screen matting we can use a variety of solid colored backgrounds to estimate each pixel s foreground color ifi and partial coverage i. to estimate the area of support ai in zongker werner curless et al. use a series of periodic horizontal and vertical solid stripes at different frequencies and phases which is reminiscent of the structured light patterns used in active rangefinding for the more sophisticated mixture of gaussian model chuang zongker hindorff et al. sweep a series of narrow gaussian stripes at four different orientations vertical and two diagonals which enables them to estimate multiple oriented gaussian responses at each pixel. once an environment matte has been pulled it is then a simple matter to replace the background with a new image bx to obtain a novel composite of the object placed in a different environment c. the use of multiple backgrounds during the matting process however precludes the use of this technique with dynamic scenes e.g. water pouring into a glass in this case a single graded color background can be used to estimate a single monochromatic displacement for each pixel zongker hindorff et al. higher-dimensional light fields as you can tell from the preceding discussion an environment matte in principle maps every pixel y into a distribution over light rays and is hence a six-dimensional representation. practice each pixel s response is parameterized using a dozen or so parameters if we relax the assumption that the environment is distant the monitor can be placed at several depths to estimate a depth-dependent mapping function werner curless et al. environment mattes figure the geometry-image continuum in image-based rendering szeliski and anandan ieee. representations at the left of the spectrum use more detailed geometry and simpler image representations while representations and algorithms on the right use more images and less geometry. e.g. b r a instead of a full mapping. what if we want to model an object s refractive properties from every potential point of view? in this case we need a mapping from every incoming light ray to every potential exiting light ray which is an representation. if we use the same trick as with surface light fields we can parameterize each surface point by its brdf to reduce this mapping back down to but this loses the ability to handle multiple refractive paths. if we want to handle dynamic light fields we need to add another temporal dimension. gardner tchou et al. gives a nice example of a dynamic appearance and illumination acquisition system. similarly if we want a continuous distribution over wavelengths this becomes another dimension. these examples illustrate how modeling the full complexity of a visual scene through sampling can be extremely expensive. fortunately constructing specialized models which exploit knowledge about the physics of light transport along with the natural coherence of real-world objects can make these problems more tractable. the modeling to rendering continuum the image-based rendering representations and algorithms we have studied in this chapter span a continuum ranging from classic texture-mapped models all the way to pure sampled ray-based representations such as light fields representations such as viewdependent texture maps and lumigraphs still use a single global geometric model but select the colors to map onto these surfaces from nearby images. view-dependent geometry e.g. computer vision algorithms and applications draft multiple depth maps sidestep the need for coherent geometry and can sometimes better model local non-rigid effects such as specular motion kang szeliski et al. criminisi kang swaminathan et al. sprites with depth and layered depth images use image-based representations of both color and geometry and can be efficiently rendered using warping operations rather than geometric rasterization. the best choice of representation and rendering algorithm depends on both the quantity and quality of the input imagery as well as the intended application. when nearby views are being rendered image-based representations capture more of the visual fidelity of the real world because they directly sample its appearance. on the other hand if only a few input images are available or the image-based models need to be manipulated e.g. to change their shape or appearance more abstract representations such as geometric and local reflection models are a better fit. as we continue to capture and manipulate increasingly larger quantities of visual data research into these aspects of image-based modeling and rendering will continue to evolve. video-based rendering since multiple images can be used to render new images or interactive experiences can something similar be done with video? in fact a fair amount of work has been done in the area of video-based rendering and video-based animation two terms first introduced by sch odl szeliski salesin et al. to denote the process of generating new video sequences from captured video footage. an early example of such work is video rewrite covell and slaney in which archival video footage is re-animated by having actors say new utterances more recently the term video-based rendering has been used by some researchers to denote the creation of virtual camera moves from a set of synchronized video cameras placed in a studio terms free-viewpoint video and video are also sometimes used see section in this section we present a number of video-based rendering systems and applications. we start with video-based animation in which video footage is re-arranged or modified e.g. in the capture and re-rendering of facial expressions. a special case of this are video textures in which source video is automatically cut into segments and re-looped to create infinitely long video animations. it is also possible to create such animations from still pictures or paintings by segmenting the image into separately moving regions and animating them using stochastic motion fields next we turn our attention to video in which multiple synchronized video cameras are used to film a scene from different directions. the source video frames can then be re-combined using image-based rendering techniques such as view interpolation to video-based rendering figure video rewrite covell and slaney acm the video frames are composed from bits and pieces of old video footage matched to a new audio track. create virtual camera paths between the source cameras as part of a real-time viewing experience. finally we discuss capturing environments by driving or walking through them with panoramic video cameras in order to create interactive video-based walkthrough experiences video-based animation as we mentioned above an early example of video-based animation is video rewrite in which frames from original video footage are rearranged in order to match them to novel spoken utterances e.g. for movie dubbing this is similar in spirit to the way that concatenative speech synthesis systems work in their system bregler covell and slaney first use speech recognition to extract phonemes from both the source video material and the novel audio stream. phonemes are grouped into triphones of phonemes since these better model the coarticulation effect present when people speak. matching triphones are then found in the source footage and audio track. the mouth images corresponding to the selected video frames are then cut and pasted into the desired video footage being re-animated or dubbed with appropriate geometric transformations to account for head motion. during the analysis phase features corresponding to the lips chin and head are tracked using computer vision techniques. during synthesis image morphing techniques are used to blend and stitch adjacent mouth shapes into a more coherent whole. in more recent work ezzat geiger and poggio describe how to use a multidimensional morphable model combined with regularized trajectory synthesis to improve these results. a more sophisticated version of this system called face transfer uses a novel source video instead of just an audio track to drive the animation of a previously captured video i.e. to re-render a video of a talking head with the appropriate visual speech expression and head pose elements brand pfister et al. this work is one of many performancedriven animation systems which are often used to animate facial models computer vision algorithms and applications draft while traditional performance-driven animation systems use markerbased motion capture litwinowicz and williams ma jones chiang et al. video footage can now often be used directly to control the animation finkelstein jacobs et al. pighin szeliski and salesin zhang snavely curless et al. vlasic brand pfister et al. roble and zafar in addition to its most common application to facial animation video-based animation can also be applied to whole body motion e.g. by matching the flow fields between two different source videos and using one to drive the other berg mori et al. another approach to video-based rendering is to use flow or modeling to unwrap surface textures into stabilized images which can then be manipulated and re-rendered onto the original video szeliski and salesin rav-acha kohli fitzgibbon et al. video textures video-based animation is a powerful means of creating photo-realistic videos by re-purposing existing video footage to match some other desired activity or script. what if instead of constructing a special animation or narrative we simply want the video to continue playing in a plausible manner? for example many web sites use images or videos to highlight their destinations e.g. to portray attractive beaches with surf and palm trees waving in the wind. instead of using a static image or a video clip that has a discontinuity when it loops can we transform the video clip into an infinite-length animation that plays forever? this idea is the basis of video textures in which a short video clip can be arbitrarily extended by re-arranging video frames while preserving visual continuity odl szeliski salesin et al. the basic problem in creating video textures is how to perform this re-arrangement without introducing visual artifacts. can you think of how you might do this? the simplest approach is to match frames by visual similarity distance and to jump between frames that appear similar. unfortunately if the motions in the two frames are different a dramatic visual artifact will occur video will appear to stutter for example if we fail to match the motions of the clock pendulum in figure it can suddenly change direction in mid-swing. how can we extend our basic frame matching to also match motion? in principle we could compute optic flow at each frame and match this. however flow estimates are often unreliable in textureless regions and it is not clear how to weight the visual and motion similarities relative to each other. as an alternative sch odl szeliski salesin et al. suggest matching triplets or larger neighborhoods of adjacent video frames much in the same way as video rewrite matches triphones. once we have constructed an n n similarity matrix between all video frames n is the number of frames a simple video-based rendering figure video textures odl szeliski salesin et al. acm a clock pendulum with correctly matched direction of motion a candle flame showing temporal transition arcs the flag is generated using morphing at jumps a bonfire uses longer cross-dissolves a waterfall cross-dissolves several sequences at once a smiling animated face two swinging children are animated separately the balloons are automatically segmented into separate moving regions a synthetic fish tank consisting of bubbles plants and fish. videos corresponding to these images can be found at http computer vision algorithms and applications draft finite impulse response filtering of each match sequence can be used to emphasize subsequences that match well. the results of this match computation gives us a jump table or equivalently a transition probability between any two frames in the original video. this is shown schematically as red arcs in figure where the red bar indicates which video frame is currently being displayed and arcs light up as a forward or backward transition is taken. we can view these transition probabilities as encoding the hidden markov model that underlies a stochastic video generation process. sometimes it is not possible to find exactly matching subsequences in the original video. in this case morphing i.e. warping and blending frames during transitions can be used to hide the visual differences if the motion is chaotic enough as in a bonfire or a waterfall e simple blending cross-dissolves may be sufficient. improved transitions can also be obtained by performing graph cuts on the spatio-temporal volume around a transition sch odl essa et al. video textures need not be restricted to chaotic random phenomena such as fire wind and water. pleasing video textures can be created of people e.g. a smiling face in figure or someone running on a treadmill odl szeliski salesin et al. when multiple people or objects are moving independently as in figures h we must first segment the video into independently moving regions and animate each region separately. it is also possible to create large panoramic video textures from a slowly panning camera zheng pal et al. instead of just playing back the original frames in a stochastic manner video textures can also be used to create scripted or interactive animations. if we extract individual elements such as fish in a fishtank into separate video sprites we can animate them along pre-specified paths matching the path direction with the original sprite motion to make our video elements move in a desired fashion odl and essa in fact work on video textures inspired research on systems that re-synthesize new motion sequences from motion capture data which some people refer to as mocap soup and forsyth kovar gleicher and pighin lee chai reitsma et al. li wang and shum pullen and bregler while video textures primarily analyze the video as a sequence of frames regions that can be re-arranged in time temporal textures and picard bar-joseph elyaniv lischinski et al. and dynamic textures chiuso wu et al. yuan wen liu et al. doretto and soatto treat the video as a spatio-temporal volume with textural properties which can be described using auto-regressive temporal models. video-based rendering figure animating still pictures goldman zheng et al. acm. the input still image is manually segmented into several layers. each layer is then animated with a different stochastic motion texture the animated layers are then composited to produce the final animation application animating pictures while video textures can turn a short video clip into an infinitely long video can the same thing be done with a single still image? the answer is yes if you are willing to first segment the image into different layers and then animate each layer separately. chuang goldman zheng et al. describe how an image can be decomposed into separate layers using interactive matting techniques. each layer is then animated using a class-specific synthetic motion. as shown in figure boats rock back and forth trees sway in the wind clouds move horizontally and water ripples using a shaped noise displacement map. all of these effects can be tied to some global control parameters such as the velocity and direction of a virtual wind. after being individually animated the layers can be composited to create a final dynamic rendering. video in recent years the popularity of movies has grown dramatically with recent releases ranging from hannah montana through s concert movie to james cameron s avatar. currently such releases are filmed using stereoscopic camera rigs and displayed in theaters at home to viewers wearing polarized in the future however home audiences may wish to view such movies with multi-zone auto-stereoscopic displays where each person gets his or her own customized stereo stream and can move around a scene to see it from displacement map...abcde...... displacement map displacement map displacement map displacement mapd ld sinterestovermorethanashortperiodoftimeonaccountofitspe-riodicityandpredictability.theapproachweultimatelysettledupon whichhastheadvan-tagesofbeingquitesimpleforuserstospecifyandofcreatinginterestingcomplexandplausiblyrealisticmotion computer vision algorithms and applications draft figure video view interpolation kang uyttendaele et al. acm the capture hardware consists of eight synchronized cameras the background and foreground images from each camera are rendered and composited before blending the two-layer representation before and after boundary matting background color estimates background depth estimates foreground color estimates. different the stereo matching techniques developed in the computer vision community along with image-based rendering interpolation techniques from graphics are both essential components in such scenarios which are sometimes called free-viewpoint video theobalt magnor et al. or virtual viewpoint video kang uyttendaele et al. in addition to solving a series of per-frame reconstruction and view interpolation problems the depth maps or proxies produced by the analysis phase must be temporally consistent in order to avoid flickering artifacts. shum chan and kang and magnor present nice overviews of various video view interpolation techniques and systems. these include the virtualized reality system of kanade rander and narayanan and vedula baker and kanade immersive video katkere kuramura et al. image-based visual hulls buehler raskar et al. matusik buehler and mcmillan and free-viewpoint video theobalt magnor et al. which all use global geometric models or volumetric as their proxies for rendering. the work of vedula baker and kanade also computes scene flow i.e. the motion between corresponding surface elements which can then be used to perform spatio-temporal interpolation of the multi-view video stream. the virtual viewpoint video system of zitnick kang uyttendaele et al. on the renderbackgroundbirenderforegroundfiovercompositecamera video-based rendering other hand associates a two-layer depth map with each input image which allows them to accurately model occlusion effects such as the mixed pixels that occur at object boundaries. their system which consists of eight synchronized video cameras connected to a disk array first uses segmentation-based stereo to extract a depth map for each input image near object boundaries discontinuities the background layer is extended along a strip behind the foreground object and its color is estimated from the neighboring images where it is not occluded automated matting techniques are then used to estimate the fractional opacity and color of boundary pixels in the foreground layer at render time given a new virtual camera that lies between two of the original cameras the layers in the neighboring cameras are rendered as texture-mapped triangles and the foreground layer may have fractional opacities is then composited over the background layer the resulting two images are merged and blended by comparing their respective z-buffer values. the two z-values are sufficiently close a linear blend of the two colors is computed. the interactive rendering system runs in real time using regular graphics hardware. it can therefore be used to change the observer s viewpoint while playing the video or to freeze the scene and explore it in more recently rogmans lu bekaert et al. have developed gpu implementations of both real-time stereo matching and real-time rendering algorithms which enable them to explore algorithmic alternatives in a real-time setting. at present the depth maps computed from the eight stereo cameras using off-line stereo matching have produced the highest quality depth maps associated with live they are therefore often used in studies of video compression which is an active area of research and kauff gotchev and rosenhahn active video-rate depth sensing cameras such as the zcam and yahav which we discussed in section are another potential source of such data. when large numbers of closely spaced cameras are available as in the stanford light field camera joshi vaish et al. it may not always be necessary to compute explicit depth maps to create video-based rendering effects although the results are usually of higher quality if you do szeliski zitnick et al. application video-based walkthroughs video camera arrays enable the simultaneous capture of dynamic scenes from multiple viewpoints which can then enable the viewer to explore the scene from viewpoints near the original capture locations. what if instead we wish to capture an extended area such as a home a movie set or even an entire city? httpresearch.microsoft.comen-usumredmondgroupsivmvvv. computer vision algorithms and applications draft in this case it makes more sense to move the camera through the environment and play back the video as an interactive video-based walkthrough. in order to allow the viewer to look around in all directions it is preferable to use a panoramic video camera criminisi kang et al. one way to structure the acquisition process is to capture these images in a horizontal plane e.g. over a grid superimposed inside a room. the resulting sea of images funkhouser yanovsky et al. can be used to enable continuous motion between the captured however extending this idea to larger settings e.g. beyond a single room can become tedious and data-intensive. instead a natural way to explore a space is often to just walk through it along some prespecified paths just as museums or home tours guide users along a particular path say down the middle of each similarly city-level exploration can be achieved by driving down the middle of each street and allowing the user to branch at each intersection. this idea dates back to the aspen moviemap project which recorded analog video taken from moving cars onto videodiscs for later interactive playback. recent improvements in video technology now enable the capture of panoramic video using a small co-located array of cameras such as the point grey ladybug developed by uyttendaele criminisi kang et al. for their interactive video-based walkthrough project. in their system the synchronized video streams from the six cameras are stitched together into panoramas using a variety of techniques developed specifically for this project. because the cameras do not share the same center of projection parallax between the cameras can lead to ghosting in the overlapping fields of view to remove this a multi-perspective plane sweep stereo algorithm is used to estimate per-pixel depths at each column in the overlap area. to calibrate the cameras relative to each other the camera is spun in place and a constrained structure from motion algorithm is used to estimate the relative camera poses and intrinsics. feature tracking is then run on the walkthrough video in order to stabilize the video sequence liu gleicher jin et al. have carried out more recent work along these lines. indoor environments with windows as well as sunny outdoor environments with strong shadows often have a dynamic range that exceeds the capabilities of video sensors. for this reason the ladybug camera has a programmable exposure capability that enables the bracketing of exposures at subsequent video frames. in order to merge the resulting video see httpwww.cis.upenn.edu kostasomni.html for descriptions of panoramic vision sys tems and associated workshops. photo tourism system of snavely seitz and szeliski applies this idea to less structured collections. in computer games restricting a player to forward and backward motion along predetermined paths is called rail-based gaming. httpwww.ptgrey.com. video-based rendering figure video-based walkthroughs criminisi kang et al. ieee system diagram of video pre-processing the point grey ladybug camera ghost removal using multi-perspective plane sweep point tracking used both for calibration and stabilization interactive garden walkthrough with map below overhead map authoring and sound placement interactive home walkthrough with navigation bar and icons of interest computer vision algorithms and applications draft frames into high dynamic range video pixels from adjacent frames need to be motioncompensated before being merged uyttendaele winder et al. the interactive walk-through experience becomes much richer and more navigable if an overview map is available as part of the experience. in figure the map has annotations which can show up during the tour and localized sound sources which play different volumes when the viewer is nearby. the process of aligning the video sequence with the map can be automated using a process called map correlation and szeliski all of these elements combine to provide the user with a rich interactive and immersive experience. figure shows a walk through the bellevue botanical gardens with an overview map in perspective below the live video window. arrows on the ground are used to indicate potential directions of travel. the viewer simply orients his view towards one of the arrows experience can be driven using a game controller and walks forward along the desired path. figure shows an indoor home tour experience. in addition to a schematic map in the lower left corner and adjacent room names along the top navigation bar icons appear along the bottom whenever items of interest such as a homeowner s art pieces are visible in the main window. these icons can then be clicked to provide more information and views. the development of interactive video tours spurred a renewed interest in video-based virtual travel and mapping experiences as evidenced by commercial sites such as google s street view and bing maps. the same videos can also be used to generate turn-by-turn driving directions taking advantage of both expanded fields of view and image-based rendering to enhance the experience neubert ofek et al. as we continue to capture more and more of our real world with large amounts of highquality imagery and video the interactive modeling exploration and rendering techniques described in this chapter will play an even bigger role in bringing virtual experiences based on remote areas of the world closer to everyone. additional reading two good recent surveys of image-based rendering are by kang li tong et al. and shum chan and kang with earlier surveys available from kang mcmillan and gortler and debevec the term image-based rendering was introduced by mcmillan and bishop although the seminal paper in the field is the view interpolation paper by chen and williams debevec taylor and malik describe their fac ade system which not only created a variety of image-based modeling tools but also introduced the widely used technique of view-dependent texture mapping. additional reading early work on planar impostors and layers was carried out by shade lischinski salesin et al. lengyel and snyder and torborg and kajiya while newer work based on sprites with depth is described by shade gortler he et al. the two foundational papers in image-based rendering are light field rendering by levoy and hanrahan and the lumigraph by gortler grzeszczuk szeliski et al. buehler bosse mcmillan et al. generalize the lumigraph approach to irregularly spaced collections of images while levoy provides a survey and more gentle introduction to the topic of light field and image-based rendering. surface light fields azuma aldinger et al. provide an alternative parameterization for light fields with accurately known surface geometry and support both better compression and the possibility of editing surface properties. concentric mosaics and he shum wang chai et al. and panoramas with depth ben-ezra and pritch li shum tang et al. zheng kang cohen et al. provide useful parameterizations for light fields captured with panning cameras. multi-perspective images and bishop and manifold projections and herman although not true light fields are also closely related to these ideas. among the possible extensions of light fields to higher-dimensional structures environment mattes werner curless et al. chuang zongker hindorff et al. are the most useful especially for placing captured objects into new scenes. video-based rendering i.e. the re-use of video to create new animations or virtual experiences started with the seminal work of szummer and picard bregler covell and slaney and sch odl szeliski salesin et al. important follow-on work to these basic re-targeting approaches was carried out by sch odl and essa kwatra sch odl essa et al. doretto chiuso wu et al. wang and zhu zhong and sclaroff yuan wen liu et al. doretto and soatto zhao and pietik ainen and chan and vasconcelos systems that allow users to change their viewpoint based on multiple synchronized video streams include those by moezzi katkere kuramura et al. kanade rander and narayanan matusik buehler raskar et al. matusik buehler and mcmillan carranza theobalt magnor et al. zitnick kang uyttendaele et al. magnor and vedula baker and kanade video coding and compression is also an active area of research and kauff gotchev and rosenhahn with blu-ray discs encoded using the multiview video coding extension to avc expected by the end of computer vision algorithms and applications draft exercises ex depth image rendering develop a view extrapolation algorithm to re-render a previously computed stereo depth map coupled with its corresponding reference color image. use a graphics mesh rendering system such as opengl or with two triangles per pixel quad and perspective texture mapping yu and borshukov alternatively use the one- or two-pass forward warper you constructed in exercise extended using to convert from disparities or depths into displacements. kinks in straight lines introduced during view interpolation or extrapolation are visually noticeable which is one reason why image morphing systems let you specify line correspondences and neely modify your depth estimation algorithm to match and estimate the geometry of straight lines and incorporate it into your image-based rendering algorithm. ex view interpolation extend the system you created in the previous exercise to render two reference views and then blend the images using a combination of z-buffering hole filing and blending to create the final image if the two source images have very different exposures the hole-filled regions and the blended regions will have different exposures. can you extend your algorithm to mitigate this? extend your algorithm to perform three-way interpolation between neighboring views. you can triangulate the reference camera poses and use barycentric coordinates for the virtual camera in order to determine the blending weights. ex view morphing modify your view interpolation algorithm to perform morphs between views of a non-rigid object such as a person changing expressions. instead of using a pure stereo algorithm use a general flow algorithm to compute displacements but separate them into a rigid displacement due to camera motion and a non-rigid deformation. at render time use the rigid geometry to determine the new pixel location but then add a fraction of the non-rigid displacement as well. alternatively compute a stereo depth map but let the user specify additional correspon dences or use a feature-based matching algorithm to provide them automatically. exercises take a single image such as the mona lisa or a friend s picture and create an animated view morph and dyer find the vertical axis of symmetry in the image and reflect your reference image to provide a virtual pair the person s hairstyle is somewhat symmetric. use structure from motion to determine the relative camera pose of the pair. use dense stereo matching to estimate the shape. use view morphing to create a animation. ex view dependent texture mapping use a model you created along with the original images to implement a view-dependent texture mapping system. use one of the reconstruction techniques you developed in exercises or to build a triangulated image-based model from multiple photographs. extract textures for each model face from your photographs either by performing the appropriate resampling or by figuring out how to use the texture mapping software to directly access the source images. at run time for each new camera view select the best source image for each visible model face. extend this to blend between the top two or three textures. this is trickier since it involves the use of texture blending or pixel shading taylor and malik debevec yu and borshukov pighin hecker lischinski et al. ex layered depth images extend your view interpolation algorithm to store more than one depth or color value per pixel gortler he et al. i.e. a layered depth image modify your rendering algorithm accordingly. for your data you can use synthetic ray tracing a layered reconstructed model or a volumetric reconstruction. ex rendering from sprites or layers extend your view interpolation algorithm to handle multiple planes or sprites gortler he et al. extract your layers using the technique you developed in exercise alternatively use an interactive painting and placement system to extract your lay ers oh chen dorsey et al. shum sun yamazaki et al. determine a back-to-front order based on expected visibility or add a z-buffer to your rendering algorithm to handle occlusions. computer vision algorithms and applications draft render and composite all of the resulting layers with optional alpha matting to handle the edges of layers and sprites. ex light field transformations derive the equations relating regular images to light field coordinates. determine the mapping between the far plane v coordinates and a virtual camera s y coordinates. start by parameterizing a point on the uv plane in terms of its v coordi nates. project the resulting point to the camera pixels y using the usual camera matrix p derive the homography relating v and y coordinates. write down a similar transformation for t to y coordinates. prove that if the virtual camera is actually on the t plane the t value depends only on the camera s optical center and is independent of y. prove that an image taken by a regular orthographic or perspective camera i.e. one that has a linear projective relationship between points and y pixels samples the t u v light field along a two-dimensional hyperplane. ex light field and lumigraph rendering implement a light field or lumigraph rendering system download one of the light field data sets from httplightfield.stanford.edu. write an algorithm to synthesize a new view from this light field using quadri-linear interpolation of t u v ray samples. try varying the focal plane corresponding to your desired view mcmillan and gortler and see if the resulting image looks sharper. determine a proxy for the objects in your scene. you can do this by running multi view stereo over one of your light fields to obtain a depth map per image. implement the lumigraph rendering algorithm which modifies the sampling of rays according to the location of each surface element. collect a set of images yourself and determine their pose using structure from motion. exercises implement the unstructured lumigraph rendering algorithm from buehler bosse mcmil lan et al. ex surface light fields construct a surface light field azuma aldinger et al. and see how well you can compress it. acquire an interesting light field of a specular scene or object or download one from httplightfield.stanford.edu. build a model of the object using a multi-view stereo algorithm that is robust to outliers due to specularities. estimate the lumisphere for each surface point on the object. estimate its diffuse components. is the median the best way to do this? why not use the minimum color value? what happens if there is lambertian shading on the diffuse component? model and compress the remaining portion of the lumisphere using one of the techniques suggested by wood azuma aldinger et al. or invent one of your own. study how well your compression algorithm works and what artifacts it produces. develop a system to edit and manipulate your surface light field. ex handheld concentric mosaics develop a system to navigate a handheld concentric mosaic. stand in the middle of a room with a camcorder held at arm s length in front of you and spin in a circle. use a structure from motion system to determine the camera pose and sparse struc ture for each input frame. re-bin your image pixels into a more regular concentric mosaic structure. at view time determine from the new camera s view should be near the plane of your original capture which source pixels to display. you can simplify your computations to determine a source column scaling for each output column. use your sparse structure interpolated to a dense depth map to improve your rendering kang cohen et al. computer vision algorithms and applications draft ex video textures capture some videos of natural phenomena such as a water fountain fire or smiling face and loop the video seamlessly into an infinite length video odl szeliski salesin et al. compare all the frames in the original clip using an of square difference metric. assumes the videos were shot on a tripod or have already been stabilized. filter the comparison table temporally to accentuate temporal sub-sequences that match well together. convert your similarity table into a jump probability table through some exponential distribution. be sure to modify transitions near the end so you do not get stuck in the last frame. starting with the first frame use your transition table to decide whether to jump for ward backward or continue to the next frame. add any of the other extensions to the original video textures idea such as multiple moving regions interactive control or graph cut spatio-temporal texture seaming. chapter recognition object detection instance recognition face detection pedestrian detection face recognition eigenfaces active appearance and shape models application personal photo collections category recognition bag of words part-based models recognition with segmentation application intelligent photo editing geometric alignment large databases application location recognition context and scene understanding learning and large image collections application image search recognition databases and test sets additional reading exercises computer vision algorithms and applications draft figure recognition face recognition with pictorial structures and elschlager ieee and eigenfaces and pentland realtime face detection and jones springer instance object recognition ieee feature-based recognition perona and zisserman region-based recognition ren efros et al. ieee simultaneous recognition and segmentation winn rother et al. springer location recognition chum isard et al. ieee using context torralba liu et al. recognition of all the visual tasks we might ask a computer to perform analyzing a scene and recognizing all of the constituent objects remains the most challenging. while computers excel at accurately reconstructing the shape of a scene from images taken from different views they cannot name all the objects and animals present in a picture even at the level of a twoyear-old child. there is not even any consensus among researchers on when this level of performance might be achieved. why is recognition so hard? the real world is made of a jumble of objects which all occlude one another and appear in different poses. furthermore the variability intrinsic within a class dogs due to complex non-rigid articulation and extreme variations in shape and appearance between different breeds makes it unlikely that we can simply perform exhaustive matching against a database of the recognition problem can be broken down along several axes. for example if we know what we are looking for the problem is one of object detection which involves quickly scanning an image to determine where a match may occur if we have a specific rigid object we are trying to recognize recognition section we can search for characteristic feature points and verify that they align in a geometrically plausible way the most challenging version of recognition is general category class recognition which may involve recognizing instances of extremely varied classes such as animals or furniture. some techniques rely purely on the presence of features as a bag of words model see section their relative positions models figure while others involve segmenting the image into semantically meaningful regions in many instances recognition depends heavily on the context of surrounding objects and scene elements woven into all of these techniques is the topic of learning since hand-crafting specific object recognizers seems like a futile approach given the complexity of the problem. given the extremely rich and complex nature of this topic this chapter is structured to build from simpler concepts to more complex ones. we begin with a discussion of face and object detection where we introduce a number of machine-learning techniques such as boosting neural networks and support vector machines. next we study face recognition which is one of the more widely known applications of recognition. this topic serves as an introduction to subspace models and bayesian approaches to recognition and classification. we then present techniques for instance recognition building upon earlier topics in this book such as feature detection matching and geometric alignment we introduce topics from the information and document retrieval communities such as frequency vectors feature quantization and inverted indices however some recent research suggests that direct image matching may be feasible for large enough databases torralba liu et al. malisiewicz and efros torralba freeman and fergus computer vision algorithms and applications draft we also present applications of location recognition in the second half of the chapter we address the most challenging variant of recognition namely the problem of category recognition this includes approaches that use bags of features parts and segmentation we show how such techniques can be used to automate photo editing tasks such as modeling scene completion and creating collages next we discuss the role that context can play in both individual object recognition and more holistic scene understanding we close this chapter with a discussion of databases and test sets for constructing and evaluating recognition systems while there is no comprehensive reference on object recognition an excellent set of notes can be found in the iccv short course fergus and torralba antonio torralba s more comprehensive mit course and two recent collections of papers hebert schmid et al. dickinson leonardis schiele et al. and a survey on object categorization an evaluation of some of the best performing recognition algorithms can be found on the pascal visual object classes challenge web site at httppascallin.ecs.soton.ac.ukchallengesvoc. object detection if we are given an image to analyze such as the group portrait in figure we could try to apply a recognition algorithm to every possible sub-window in this image. such algorithms are likely to be both slow and error-prone. instead it is more effective to construct specialpurpose detectors whose job it is to rapidly find likely regions where particular objects might occur. we begin this section with face detectors which are some of the more successful examples of recognition. for example such algorithms are built into most of today s digital cameras to enhance auto-focus and into video conferencing systems to control pan-tilt heads. we then look at pedestrian detectors as an example of more general methods for object detection. such detectors can be used in automotive safety applications e.g. detecting pedestrians and other cars from moving vehicles cornelis cornelis et al. face detection before face recognition can be applied to a general image the locations and sizes of any faces must first be found and in principle we could apply a face recognition algorithm at every pixel and scale and pentland but such a process would be too slow in practice. object detection figure face detection results produced by rowley baluja and kanade ieee. can you find the one false positive box around a non-face among the true positive results? over the years a wide variety of fast face detection algorithms have been developed. yang kriegman and ahuja provide a comprehensive survey of earlier work in this field yang s icpr and the torralba short course provide more recent according to the taxonomy of yang kriegman and ahuja face detection techniques can be classified as feature-based template-based or appearance-based. featurebased techniques attempt to find the locations of distinctive image features such as the eyes nose and mouth and then verify whether these features are in a plausible geometrical arrangement. these techniques include some of the early approaches to face recognition and elschlager kanade yuille as well as more recent approaches based on modular eigenspaces and pentland local filter jets burl and perona penev and atick wiskott fellous kr uger et al. support vector machines ho wu et al. heisele serre and poggio and boosting and kanade template-based approaches such as active appearance models can deal with a wide range of pose and expression variability. typically they require good initialization near a real face and are therefore not suitable as fast face detectors. httpvision.ai.uiuc.edumhyangface-detection-survey.html. an alternative approach to detecting faces is to look for regions of skin color in the image and fleck jones and rehg see exercise for some additional discussion and references. computer vision algorithms and applications draft figure pre-processing stages for face detector training baluja and kanade ieee artificially mirroring rotating scaling and translating training images for greater variability using images without faces up at a tree to generate non-face examples pre-processing the patches by subtracting a best fit linear function gradient and histogram equalizing. appearance-based approaches scan over small overlapping rectangular patches of the image searching for likely face candidates which can then be refined using a cascade of more expensive but selective detection algorithms and poggio rowley baluja and kanade romdhani torr sch olkopf et al. fleuret and geman viola and jones in order to deal with scale variation the image is usually converted into a sub-octave pyramid and a separate scan is performed on each level. most appearance-based approaches today rely heavily on training classifiers using sets of labeled face and non-face patches. sung and poggio and rowley baluja and kanade present two of the earliest appearance-based face detectors and introduce a number of innovations that are widely used in later work by others. to start with both systems collect a set of labeled face patches as well as a set of patches taken from images that are known not to contain faces such as aerial images or vegetation the collected face images are augmented by artificially mirroring rotating scaling and translating the images by small amounts to make the face detectors less sensitive to such effects after an initial set of training images has been collected some optional pre-processing can be performed such as subtracting an average gradient function from the image to compensate for global shading effects and using histogram equalization to compensate for varying camera contrast object detection figure learning a mixture of gaussians model for face detection and poggio ieee. the face and non-face images vectors are first clustered into six separate clusters using k-means and then analyzed using pca. the cluster centers are shown in the right-hand columns. clustering and pca. once the face and non-face patterns have been pre-processed sung and poggio cluster each of these datasets into six separate clusters using k-means and then fit pca subspaces to each of the resulting clusters at detection time the difs and dffs metrics first developed by moghaddam and pentland figure and are used to produce mahalanobis distance measurements per cluster. the resulting measurements are input to a multi-layer perceptron which is a neural network with alternating layers of weighted summations and sigmoidal nonlinearities trained using the backpropagation algorithm hinton and williams neural networks. instead of first clustering the data and computing mahalanobis distances to the cluster centers rowley baluja and kanade apply a neural network directly to the pixel patches of gray-level intensities using a variety of differently sized hand-crafted receptive fields to capture both large-scale and smaller scale structure the resulting neural network directly outputs the likelihood of a face at the center computer vision algorithms and applications draft figure a neural network for face detection baluja and kanade ieee. overlapping patches are extracted from different levels of a pyramid and then pre-processed as shown in figure a three-layer neural network is then used to detect likely face locations. of every overlapping patch in a multi-resolution pyramid. since several overlapping patches both space and resolution may fire near a face an additional merging network is used to merge overlapping detections. the authors also experiment with training several networks and merging their outputs. figure shows a sample result from their face detector. to make the detector run faster a separate network operating on patches is trained to detect both faces and faces shifted by pixels. this network is evaluated at every pixel in the image and vertically and the results of this coarse or sloppy detector are used to select regions on which to run the slower single-pixel overlap technique. to deal with in-plane rotations of faces rowley baluja and kanade train a router network to estimate likely rotation angles from input patches and then apply the estimated rotation to each patch before running the result through their upright face detector. support vector machines. instead of using a neural network to classify patches osuna freund and girosi use a support vector machine tibshirani and friedman sch olkopf and smola bishop lampert to classify the same preprocessed patches as sung and poggio an svm searches for a series of maximum margin separating planes in feature space between different classes this case face and non-face patches. in those cases where linear classification boundaries are insufficient the feature space can be lifted into higher-dimensional features using kernels tibshirani and friedman sch olkopf and smola bishop svms have been used by other researchers for both face detection and face recognition ho wu et al. object detection figure simple features used in boosting-based face detector and jones springer difference of rectangle feature composed of different rectangles inside the white rectangles are subtracted from the gray ones the first and second features selected by adaboost. the first feature measures the differences in intensity between the eyes and the cheeks the second one between the eyes and the bridge of the nose. heisele serre and poggio and are a widely used tool in object recognition in general. boosting. of all the face detectors currently in use the one introduced by viola and jones is probably the best known and most widely used. their technique was the first to introduce the concept of boosting to the computer vision community which involves training a series of increasingly discriminating simple classifiers and then blending their outputs tibshirani and friedman bishop in more detail boosting involves constructing a classifier hx as a sum of simple weak learners where each of the weak learners hjx is an extremely simple function of the input and hence is not expected to contribute much isolation to the classification performance. in most variants of boosting the weak learners are threshold functions hjx ajfj j bjfj j aj bj if fj j otherwise which are also known as decision stumps the simplest possible version of decision trees. in most cases it is also traditional simpler to set aj and bj to i.e. aj sj bj so that only the feature fj the threshold value j and the polarity of the threshold sj need to be variants such as that of viola and jones use bj and adjust the learning algorithm hx sign m jhjx computer vision algorithms and applications draft figure schematic illustration of boosting courtesy of svetlana lazebnik after original illustrations from paul viola and david lowe. after each weak classifier stump or hyperplane is selected data points that are erroneously classified have their weights increased. the final classifier is a linear combination of the simple weak classifiers. in many applications of boosting the features are simply coordinate axes xk i.e. the boosting algorithm selects one of the input vector components as the best one to threshold. in viola and jones face detector the features are differences of rectangular regions in the input patch as shown in figure the advantage of using these features is that while they are more discriminating than single pixels they are extremely fast to compute once a summed area table has been pre-computed as described in section essentially for the cost of an on pre-computation phase n is the number of pixels in the image subsequent differences of rectangles can be computed in additions or subtractions where r is the number of rectangles in the feature. the key to the success of boosting is the method for incrementally selecting the weak learners and for re-weighting the training examples after each stage the adaboost boosting algorithm tibshirani and friedman bishop does this by re-weighting each sample as a function of whether it is correctly classified at each stage and using the stage-wise average classification error to determine the final weightings j among the weak classifiers as described in algorithm while the resulting classifier is extremely fast in practice the training time can be quite slow the order of weeks because of the large number of feature of rectangle hypotheses that need to be examined at each stage. to further increase the speed of the detector it is possible to create a cascade of classifiers where each classifier uses a small number of tests a two-term adaboost classifier to reject a large fraction of non-faces while trying to pass through all potential face candidates and geman viola and jones an even faster algorithm for performing cascade learning has recently been developed by brubaker wu sun et al. accordingly. weak classifier increasedweak classifier increasedweak classifier classifier object detection input the positive and negative training examples along with their labels yi where yi for positive examples and yi for negative examples. initialize all the weights to n where n is the number of training examples. and jones use a separate and for positive and negative examples. for each training stage j m renormalize the weights so that they sum up to them by their sum. select the best classifier hjx fj j sj by finding the one that minimizes the weighted classification error ej wijeij n eij hjxi fj j sj. for any given fj function the optimal values of j sj can be found in linear time using a variant of weighted median computation compute the modified error rate j and classifier weight j j ej ej and j log j. update the weights according to the classification errors eij wij eij j i.e. downweight the training samples that were correctly classified in proportion to the overall classification error. set the final classifier to hx sign m jhjx algorithm the adaboost training algorithm adapted from hastie tibshirani and friedman viola and jones and bishop computer vision algorithms and applications draft figure pedestrian detection using histograms of oriented gradients and triggs ieee the average gradient image over the training examples each pixel shows the maximum positive svm weight in the block centered on the pixel likewise for the negative svm weights a test image the computed r-hog histogram of gradients descriptor the r-hog descriptor weighted by the positive svm weights the r-hog descriptor weighted by the negative svm weights. pedestrian detection while a lot of the research on object detection has focused on faces the detection of other objects such as pedestrians and cars has also received widespread attention and philomin gavrila papageorgiou and poggio mohan papageorgiou and poggio schneiderman and kanade some of these techniques maintain the same focus as face detection on speed and efficiency. others however focus instead on accuracy viewing detection as a more challenging variant of generic class recognition in which the locations and extents of objects are to be determined as accurately as possible. for example the pascal voc detection challenge httppascallin.ecs.soton.ac.uk challengesvoc. an example of a well-known pedestrian detector is the algorithm developed by dalal and triggs who use a set of overlapping histogram of oriented gradients descriptors fed into a support vector machine each hog has cells to accumulate magnitude-weighted votes for gradients at particular orientations just as in the scale invariant feature transform developed by lowe which we discussed in section and figure unlike sift however which is only evaluated at interest point locations hogs are evaluated on a regular overlapping grid and their descriptor magnitudes are normalized using an even coarser grid they are only computed at a single scale and a fixed orientation. in order to capture the subtle variations in orientation around a person s outline a large number of orientation bins is used and no smoothing is performed in the central difference gradient computation see the work of dalal and triggs for more implementation details. object detection figure shows a sample input image while figure shows the associated hog descriptors. once the descriptors have been computed a support vector machine is trained on the resulting high-dimensional continuous descriptor vectors. figures c show a diagram of the positive and negative svm weights in each block while figures g show the corresponding weighted hog responses for the central input image. as you can see there are a fair number of positive responses around the head torso and feet of the person and relatively few negative responses around the middle and the neck of the sweater. the fields of pedestrian and general object detection have continued to evolve rapidly over the last decade malik and puzicha mikolajczyk schmid and zisserman leibe seemann and schiele opelt pinz and zisserman torralba andriluka roth and schiele dollar belongie and perona munder and gavrila compare a number of pedestrian detectors and conclude that those based on local receptive fields and svms perform the best with a boosting-based approach coming close. maji berg and malik improve on the best of these results using non-overlapping multi-resolution hog descriptors and a histogram intersection kernel svm based on a spatial pyramid match kernel from lazebnik schmid and ponce when detectors for several different classes are being constructed simultaneously torralba murphy and freeman show that sharing features and weak learners between detectors yields better performance both in terms of faster computation times and fewer training examples. to find the features and decision stumps that work best in a shared manner they introduce a novel joint boosting algorithm that optimizes at each stage a summed expected exponential loss function using the gentleboost algorithm of friedman hastie and tibshirani in more recent work felzenszwalb mcallester and ramanan extend the his togram of oriented gradients person detector to incorporate flexible parts models each part is trained and detected on hogs evaluated at two pyramid levels below the overall object model and the locations of the parts relative to the parent node overall bounding box are also learned and used during recognition to compensate for inaccuracies or inconsistencies in the training example bounding boxes white lines in figure the true location of the parent bounding box is considered a latent variable and is inferred during both training and recognition. since the locations of the parts are also latent the system can be trained in a semi-supervised fashion without needing part labels in the training data. an extension to this system girshick mcallester et al. which includes among its improvements a simple contextual model was among the two best object detection systems in the visual object classes detection challenge. other recent improvements to part-based person detection and pose estimation in computer vision algorithms and applications draft figure part-based object detection mcallester and ramanan ieee an input photograph and its associated person and part detection results. the detection model is defined by a coarse template several higher resolution part templates and a spatial model for the location of each part. true positive detection of a skier and false positive detection of a cow as a person. clude the work by andriluka roth and schiele and kumar zisserman and h.s.torr an even more accurate estimate of a person s pose and location is presented by rogez rihan ramalingam et al. who compute both the phase of a person in a walk cycle and the locations of individual joints using random forests built on top of hogs since their system produces full pose information it is closer in its application domain to person trackers black and fleet andriluka roth and schiele which we discussed in section one final note on person and object detection. when video sequences are available the additional information present in the optic flow and motion discontinuities can greatly aid in the detection task as discussed by efros berg mori et al. viola jones and snow and dalal triggs and schmid face recognition among the various recognition tasks that computers might be asked to perform face recognition is the one where they have arguably had the most while computers cannot pick out suspects from thousands of people streaming in front of video cameras people cannot readily distinguish between similar people with whom they are not familiar toole jiang roark et al. o toole phillips jiang et al. their ability to distinguish recognition i.e. the re-recognition of known objects such as locations or planar objects is the other most successful application of general image recognition. in the general domain of biometrics i.e. identity recognition specialized images such as irises and fingerprints perform even better bolle and pankanti pankanti bolle and jain daugman face recognition figure part-based object detection results for people bicycles and horses mcallester and ramanan ieee. the first three columns show correct detections while the rightmost column shows false positives. figure pose detection using random forests rihan ramalingam et al. ieee. the estimated pose of the kinematic model is drawn over each input frame. computer vision algorithms and applications draft figure humans can recognize low-resolution faces of familiar people balas ostrovsky et al. ieee. among a small number of family members and friends has found its way into consumer-level photo applications such as picasa and iphoto. face recognition can also be used in a variety of additional applications including human computer interaction identity verification jojic and jancke desktop login parental controls and patient monitoring chellappa phillips et al. today s face recognizers work best when they are given full frontal images of faces under relatively uniform illumination conditions although databases that include large amounts of pose and lighting variation have been collected moon rizvi et al. sim baker and bsat gross shi and cohn huang ramesh berg et al. phillips scruggs o toole et al. table in section for more details. some of the earliest approaches to face recognition involved finding the locations of distinctive image features such as the eyes nose and mouth and measuring the distances between these feature locations and elschlager kanade yuille more recent approaches rely on comparing gray-level images projected onto lower dimensional subspaces called eigenfaces and jointly modeling shape and appearance variations discounting pose variations using active appearance models descriptions of additional face recognition techniques can be found in a number of surveys and books on this topic wilson and sirohey zhao chellappa phillips et al. li and jain as well as the face recognition web the survey on face recognition by humans by sinha balas ostrovsky et al. is also well worth reading it includes a number of surprising results such as humans ability to recognize low-resolution images of familiar faces and the importance of eyebrows in recognition. httpwww.face-rec.org. face recognition figure face modeling and compression using eigenfaces and pentland ieee input image the first eight eigenfaces image reconstructed by projecting onto this basis and compressing the image to bytes image reconstructed using jpeg bytes. eigenfaces eigenfaces rely on the observation first made by kirby and sirovich that an arbitrary face image x can be compressed and reconstructed by starting with a mean image m and adding a small number of scaled signed images x m aiui where the signed basis images can be derived from an ensemble of training images using principal component analysis known as eigenvalue analysis or the karhunen loeve transform. turk and pentland recognized that the coefficients ai in the eigenface expansion could themselves be used to construct a fast image matching algorithm. in more detail let us start with a collection of training images from which we can compute the mean image m and a scatter or covariance matrix c n n mxj mt we can apply the eigenvalue decomposition to represent this matrix as m n c u u t iuiut i where the i are the eigenvalues of c and the ui are the eigenvectors. for general images kirby and sirovich call these vectors eigenpictures for faces turk and pentland in previous chapters we used i to indicate images in this chapter we use the more abstract quantities x and u to indicate collections of pixels in an image turned into a vector. computer vision algorithms and applications draft figure projection onto the linear subspace spanned by the eigenface images and pentland ieee. the distance from face space is the orthogonal distance to the plane while the distance in face space is the distance along the plane from the mean image. both distances can be turned into mahalanobis distances and given probabilistic interpretations. call them eigenfaces two important properties of the eigenvalue decomposition are that the optimal ap proximation coefficients ai for any new image x can be computed as ai m ui and that assuming the eigenvalues i are sorted in decreasing order truncating the approximation given in at any point m gives the best possible approximation error between x and x. figure shows the resulting approximation corresponding to figure and shows how much better it is at compressing a face image than jpeg. truncating the eigenface decomposition of a face image after m components is equivalent to projecting the image onto a linear subspace f which we can call the face space because the eigenvectors are orthogonal and of unit norm the distance of a projected face x to the mean face m can be written as difs x m i where difs stands for distance in face space and pentland the remaining distance between the original image x and its projection onto face space x i.e. the in actual practice the full p p scatter matrix is never computed. instead a smaller n n matrix consisting of the inner products between all the signed deviations m is accumulated instead. see appendix for details. xdffsdifsxxxxxxxxxxxxxxxxxxxxxxxxxxxxff_mx face recognition distance from face space can be computed directly in pixel space and represents the faceness of a particular it is also possible to measure the distance between two different faces in face space as difsx y x m where the bi m ui are the eigenface coefficients corresponding to y. computing such distances in euclidean vector space however does not exploit the additional information that the eigenvalue decomposition of our covariance matrix provides. if we interpret the covariance matrix c as the covariance of a multi-variate gaussian we can turn the difs into a log likelihood by computing the mahalanobis distance x m i i instead of measuring the squared distance along each principal component in face space f the mahalanobis distance measures the ratio between the squared distance and the corresponding variance an alternative way to implement this is to pre-scale each eigenvector by the inverse square root of its corresponding eigenvalue i i and then sums these squared ratios log-likelihoods. u u this whitening transformation then means that euclidean distances in feature space now correspond directly to log likelihoods jebara and pentland same whitening approach can also be used in feature-based matching algorithms as discussed in section if the distribution in eigenface space is very elongated the mahalanobis distance properly scales the components to come up with a sensible distance from the mean. a similar analysis can be performed for computing a sensible difference from face space and pentland and the two terms can be combined to produce an estimate of the likelihood of being a true face which can be useful in doing face detection more detailed explanations of probabilistic and bayesian pca can be found in textbooks on statistical learning tibshirani and friedman bishop which also discuss techniques for selecting the optimum number of components m to use in modeling a distribution. this can be used to form a simple face detector as mentioned in section the ellipse shown in figure denotes an equi-probability contour of this multi-variate gaussian. computer vision algorithms and applications draft figure images from the harvard database used by belhumeur hespanha and kriegman ieee. note the wide range of illumination variation which can be more dramatic than inter-personal variations. one of the biggest advantages of using eigenfaces is that they reduce the comparison of a new face image x to a prototype face image xk of the colored xs in figure from a p difference in pixel space to an m-dimensional difference in face space where a u t m involves computing a dot product between the signed difference-from-mean image m and each of the eigenfaces ui. once again however this euclidean distance ignores the fact that we have more information about face likelihoods available in the distribution of training images. consider the set of images of one person taken under a wide range of illuminations shown in figure as you can see the intrapersonal variability within these images is much greater than the typical extrapersonal variability between any two people taken under the same illumination. regular pca analysis fails to distinguish between these two sources of variability and may in fact devote most of its principal components to modeling the intrapersonal variability. if we are going to approximate faces by a linear subspace it is more useful to have a space that discriminates between different classes and is less sensitive to within-class variations hespanha and kriegman consider the three classes shown as different colors in figure as you can see the distributions within a class by the tilted colored axes are elongated and tilted with respect to the main face space pca face recognition figure simple example of fisher linear discriminant analysis. the samples come from three different classes shown in different colors along with their principal axes which are scaled to i. intersections of the tilted axes are the class means mk. the dashed line is the fisher linear discriminant direction and the dotted lines are the linear discriminants between the classes. note how the discriminant direction is a blend between the principal directions of the between-class and within-class scatter matrices. which is aligned with the black x and y axes. we can compute the total within-class scatter matrix as sw sk mkxi mkt k k ck where mk is the mean of class k and sk is its within-class scatter similarly we can compute the between-class scatter as sb k nkmk mmk mt where nk are the number of exemplars in each class and m is the overall mean. for the three distributions shown in figure we have sw and sb to be consistent with belhumeur hespanha and kriegman we use sw and sb to denote the scatter matrices even though we use c elsewhere computer vision algorithms and applications draft where n nk is the number of samples in each class. to compute the most discriminating direction fisher s linear discriminant hespanha and kriegman hastie tibshirani and friedman bishop which is also known as linear discriminant analysis selects the direction u that results in the largest ratio between the projected between-class and within-class variations u arg max u ut sbu ut swu which is equivalent to finding the eigenvector corresponding to the largest eigenvalue of the generalized eigenvalue problem sbu swu or u s w sbu. for the problem shown in figure s w sb and u as you can see using this direction results in a better separation between the classes than using the dominant pca direction which is the horizontal axis. in their paper belhumeur hespanha and kriegman show that fisherfaces significantly outperform the original eigenfaces algorithm especially when faces have large amounts of illumination variation as in figure an alternative for modeling within-class and between-class variations is to model each distribution separately and then use bayesian techniques to find the closest exemplar jebara and pentland instead of computing the mean for each class and then the within-class and between-class distributions consider evaluating the difference images ij xi xj between all pairs of training images xj. the differences between pairs that are in the same class same person are used to estimate the intrapersonal covariance matrix i while differences between different people are used to estimate the extrapersonal covariance the principal components corresponding to these two classes are shown in figure at recognition time we can compute the distance i between a new face x and a stored training image xi and evaluate its intrapersonal likelihood as pi i pn i i exp i note that the difference distributions are zero mean because for every ij there corresponds a negative ji. face recognition figure dual eigenfaces jebara and pentland elsevier intrapersonal and extrapersonal. where pn is a normal distribution with covariance i and j is its volume. the mahalanobis distance i t i i i ai i can be computed more efficiently by first projecting the new image x into the whitened intrapersonal face space ai u i x and then computing a euclidean distance to the training image vector ai i which can be precomputed offline. the extrapersonal likelihood pe i can be computed in a similar fashion. once the intrapersonal and extrapersonal likelihoods have been computed we can com pute the bayesian likelihood of a new image x matching a training image xi as p i pi ili pi ili pe ile where li and le are the prior probabilities of two images being in the same or in different classes jebara and pentland a simpler approach which does not require the evaluation of extrapersonal probabilities is to simply choose the training image with computer vision algorithms and applications draft figure modular eigenspace for face recognition and pentland ieee. by detecting separate features in the faces nose mouth separate eigenspaces can be estimated for each one. the relative positions of each feature can be detected at recognition time thus allowing for more flexibility in viewpoint and expression. the highest likelihood pi i. in this case nearest neighbor search techniques in the space i vectors could be used to speed up finding the best spanned by the precomputed another way to improve the performance of eigenface-based approaches is to break up the image into separate regions such as the eyes nose and mouth and to match each of these modular eigenspaces independently and pentland heisele ho wu et al. heisele serre and poggio the advantage of such a modular approach is that it can tolerate a wider range of viewpoints because each part can move relative to the others. it also supports a larger variety of combinations e.g. we can model one person as having a narrow nose and bushy eyebrows without requiring the eigenfaces to span all possible combinations of nose mouth and eyebrows. you remember the cardboard children s books where you can select different top and bottom faces or mr. potato head you get the idea. another approach to dealing with large variability in appearance is to create view-based eigenspaces as shown in figure and pentland we can think of these view-based eigenspaces as local descriptors that select different axes depending on which part of the face space you are in. note that such approaches however potentially require large amounts of training data i.e. pictures of every person in every possible pose or expression. this is in contrast to the shape and appearance models we study in note that while the covariance matrices i and e are computed by looking at differences between all pairs of images the run-time evaluation selects the nearest image to determine the facial identity. whether this is statistically correct is explored in exercise face recognition figure view-based eigenspace and pentland ieee. comparison between a regular eigenspace reconstruction column and a view-based eigenspace reconstruction column corresponding to the input image column. the top row is from a training image the bottom row is from the test set. a schematic representation of the two approaches showing how each view computes its own local basis representation. section which can learn deformations across all individuals. it is also possible to generalize the bilinear factorization implicit in pca and svd approaches to multilinear formulations that can model several interacting factors simultaneously and terzopoulos these ideas are related to currently active topics in machine learning such as subspace learning he hu et al. local distance functions singer sha et al. and metric learning and baker learning approaches play an increasingly important role in face recognition e.g. in the work of sivic everingham and zisserman and guillaumin verbeek and schmid active appearance and shape models the need to use modular or view-based eigenspaces for face recognition is symptomatic of a more general observation i.e. that facial appearance and identifiability depend as much on shape as they do on color or texture is what eigenfaces capture. furthermore when dealing with head rotations the pose of a person s head should be discounted when performing recognition. in fact the earliest face recognition systems such as those by fischler and elschlager kanade and yuille found distinctive feature points on facial images and performed recognition on the basis of their relative positions or distances. newer techniques such as local feature analysis and atick and elastic bunch graph matching fellous kr uger et al. combine local filter responses at distinctive computer vision algorithms and applications draft figure manipulating facial appearance through shape and color and perrett ieee. by adding or subtracting gender-specific shape and color characteristics to an input image different amounts of gender variation can be induced. the amounts added the mean are enhancement androgyny switched and gender attributes enhanced. feature locations together with shape models to perform recognition. a visually compelling example of why both shape and texture are important is the work of rowland and perrett who manually traced the contours of facial features and then used these contours to normalize each image to a canonical shape. after analyzing both the shape and color images for deviations from the mean they were able to associate certain shape and color deformations with personal characteristics such as age and gender their work demonstrates that both shape and color have an important influence on the perception of such characteristics. around the same time researchers in computer vision were beginning to use simultaneous shape deformations and texture interpolation to model the variability in facial appearance caused by identity or expression vetter and poggio developing techniques such as active shape models taylor and cootes morphable models and vetter and elastic bunch graph matching fellous kr uger et al. of all these techniques the active appearance models of cootes edwards and taylor are among the most widely used for face recognition and tracking. like other shape and texture models an aam models both the variation in the shape of an image s which is normally encoded by the location of key feature points on the image we have already seen the application of pca to head and face modeling and animation in section face recognition figure active appearance models edwards and taylor ieee input image with registered feature points the feature points vector s the shape-free appearance image vector t. as well as the variation in texture t which is normalized to a canonical shape before being analyzed both shape and texture are represented as deviations from a mean shape s and texture t s s u sa t t u ta where the eigenvectors in u s and u t have been pre-scaled so that unit vectors in a represent one standard deviation of variation observed in the training data. in addition to these principal deformations the shape parameters are transformed by a global similarity to match the location size and orientation of a given face. similarly the texture image contains a scale and offset to best match novel illumination conditions. as you can see the same appearance parameters a in simultaneously control both the shape and texture deformations from the mean which makes sense if we believe them to be correlated. figure shows how moving three standard deviations along each of the first four principal directions ends up changing several correlated factors in a person s appearance including expression gender age and identity. in order to fit an active appearance model to a novel image cootes edwards and taylor pre-compute a set of difference decomposition images using an approach related to other fast techniques for incremental tracking such as those we discussed in sections and hager and belhumeur which often learn a discriminative mapping between matching errors and incremental displacements jurie and dhome liu chen and kumar sclaroff and isidoro romdhani and vetter williams blake and cipolla when only the shape variation is being captured such models are called active shape models cooper taylor et al. davies twining and taylor these were already discussed in section computer vision algorithms and applications draft figure principal modes of variation in active appearance models edwards and taylor ieee. the four images show the effects of simultaneously changing the first four modes of variation in both shape and texture by from the mean. you can clearly see how the shape of the face and the shading are simultaneously affected. in more detail cootes edwards and taylor compute the derivatives of a set of training images with respect to each of the parameters in a using finite differences and then compute a set of displacement weight images w xt a x xt a which can be multiplied by the current error residual to produce an update step in the parameters a w r. matthews and baker use their inverse compositional method which they first developed for parametric optical flow to further speed up active appearance model fitting and tracking. examples of aams being fitted to two input images are shown in figure although active appearance models are primarily designed to accurately capture the variability in appearance and deformation that are characteristic of faces they can be adapted to face recognition by computing an identity subspace that separates variation in identity from other sources of variability such as lighting pose and expression cootes edwards et al. the basic idea which is modeled after similar work in eigenfaces hespanha and kriegman moghaddam jebara and pentland is to compute separate statistics for intrapersonal and extrapersonal variation and then find discriminating directions in these subspaces. while aams have sometimes been used directly for recognition and vetter their main use in the context of recognition is to align faces into a canonical pose xiao wen et al. so that more traditional methods of face face recognition figure multiresolution model fitting in active appearance models edwards and taylor ieee. the columns show the initial model the results after and iterations and the final convergence. the rightmost column shows the input image. recognition and atick wiskott fellous kr uger et al. ahonen hadid and pietik ainen zhao and pietik ainen cao yin tang et al. can be used. aams actually their simpler version active shape models can also be used to align face images to perform automated morphing and fuentes active appearance models continue to be an active research area with enhancements to deal with illumination and viewpoint variation baker matthews et al. as well as occlusions matthews and baker one of the most significant extensions is to construct models of shape xiao and baker which are much better at capturing and explaining the full variability of facial appearance across wide changes in pose. figure head tracking with aams xiao and baker springer. each image shows a video frame along with the estimate yaw pitch and roll parameters and the fitted deformable mesh. computer vision algorithms and applications draft figure person detection and re-recognition using a combined face hair and torso model zitnick and szeliski springer. using face detection alone the combined face and clothing model successfully several of the heads are missed. re-finds all the people. such models can be constructed either from monocular video sequences xiao and baker as shown in figure or from multi-view video sequences koterba xiao et al. which provide even greater reliability and accuracy in reconstruction and tracking. a recent review of progress in head pose estimation please see the survey paper by murphy-chutorian and trivedi application personal photo collections in addition to digital cameras automatically finding faces to aid in auto-focusing and video cameras finding faces in video conferencing to center on the speaker mechanically or digitally face detection has found its way into most consumer-level photo organization packages such as iphoto picasa and windows live photo gallery. finding faces and allowing users to tag them makes it easier to find photos of selected people at a later date or to automatically share them with friends. in fact the ability to tag friends in photos is one of the more popular features on facebook. sometimes however faces can be hard to find and recognize especially if they are small instance recognition figure recognizing objects in a cluttered scene springer. two of the training images in the database are shown on the left. they are matched to the cluttered scene in the middle using sift features shown as small squares in the right image. the affine warp of each recognized database image onto the scene is shown as a larger parallelogram in the right image. turned away from the camera or otherwise occluded. in such cases combining face recognition with person detection and clothes recognition can be very effective as illustrated in figure zitnick and szeliski combining person recognition with other kinds of context such as location recognition or activity or event recognition can also help boost performance kapoor hua et al. instance recognition general object recognition falls into two broad categories namely instance recognition and class recognition. the former involves re-recognizing a known or rigid object potentially being viewed from a novel viewpoint against a cluttered background and with partial occlusions. the latter which is also known as category-level or generic object recognition hebert schmid et al. is the much more challenging problem of recognizing any instance of a particular general class such as cat car or bicycle over the years many different algorithms have been developed for instance recognition. mundy surveys earlier approaches which focused on extracting lines contours or surfaces from images and matching them to known object models. another popular approach was to acquire images from a large set of viewpoints and illuminations and to represent them using an eigenspace decomposition and nayar more recent approaches rothganger lazebnik schmid et al. ferrari tuytelaars and van gool gordon and lowe obdr z alek and matas sivic and zisserman tend to use viewpoint-invariant features such as those we saw in section af computer vision algorithms and applications draft ter extracting informative sparse features from both the new image and the images in the database image features are matched against the object database using one of the sparse feature matching strategies described in section whenever a sufficient number of matches have been found they are verified by finding a geometric transformation that aligns the two sets of features below we describe some of the techniques that have been proposed for representing the geometric relationships between such features we also discuss how to make the feature matching process more efficient using ideas from text and information retrieval geometric alignment to recognize one or more instances of some known objects such as those shown in the left column of figure the recognition system first extracts a set of interest points in each database image and stores the associated descriptors original positions in an indexing structure such as a search tree at recognition time features are extracted from the new image and compared against the stored object features. whenever a sufficient number of matching features three or more are found for a given object the system then invokes a match verification stage whose job is to determine whether the spatial arrangement of matching features is consistent with those in the database image. because images can be highly cluttered and similar features may belong to several objects the original set of feature matches can have a large number of outliers. for this reason lowe suggests using a hough transform to accumulate votes for likely geometric transformations. in his system he uses an affine transformation between the database object and the collection of scene features which works well for objects that are mostly planar or where at least several corresponding features share a quasi-planar since sift features carry with them their own location scale and orientation lowe uses a four-dimensional similarity transformation as the original hough binning structure i.e. each bin denotes a particular location for the object center scale and in-plane rotation. each matching feature votes for the nearest bins and peaks in the transform are then selected for a more careful affine motion fit. figure image shows three instances of the two objects on the left that were recognized by the system. obdr z alek and matas generalize lowe s approach to use feature descriptors with full local affine frames and evaluate their approach on a number of object recognition databases. another system that uses local affine frames is the one developed by rothganger lazeb when a larger number of features is available a full fundamental matrix can be used and lowe gordon and lowe when image stitching is being performed and lowe the motion models discussed in section can be used instead. instance recognition figure object recognition with affine regions lazebnik schmid et al. springer sample input image five of the recognized objects along with their bounding boxes a few of the local affine regions local affine region reprojected into a canonical frame along with its geometric affine transformations. nik schmid et al. in their system the affine region detector of mikolajczyk and schmid is used to rectify local image patches from which both a sift descriptor and a uv color histogram are computed and used for matching and recognition. corresponding patches in different views of the same object along with their local affine deformations are used to compute a affine model for the object using an extension of the factorization algorithm of section which can then be upgraded to a euclidean reconstruction and kanade at recognition time local euclidean neighborhood constraints are used to filter potential matches in a manner analogous to the affine geometric constraints used by lowe and obdr z alek and matas figure shows the results of recognizing five objects in a cluttered scene using this approach. while feature-based approaches are normally used to detect and localize known objects in scenes it is also possible to get pixel-level segmentations of the scene based on such matches. ferrari tuytelaars and van gool describe such a system for simultaneously recognizing objects and segmenting scenes while kannala rahtu brandt et al. extend this approach to non-rigid deformations. section re-visits this topic of joint recognition and segmentation in the context of generic class recognition. large databases as the number of objects in the database starts to grow large millions of objects or video frames being searched the time it takes to match a new image against each database image can become prohibitive. instead of comparing the images one at a time techniques are needed to quickly narrow down the search to a few likely images which can then be compared using a more detailed and conservative verification stage. computer vision algorithms and applications draft figure visual words obtained from elliptical normalized affine regions and zisserman ieee. affine covariant regions are extracted from each frame and clustered into visual words using k-means clustering on sift descriptors with a learned mahalanobis distance. the central patch in each grid shows the query and the surrounding patches show the nearest neighbors. the problem of quickly finding partial matches between documents is one of the central problems in information retrieval and ribeiro-neto manning raghavan and sch utze the basic approach in fast document retrieval algorithms is to pre-compute an inverted index between individual words and the documents web pages or news stories where they occur. more precisely the frequency of occurrence of particular words in a document is used to quickly find documents that match a particular query. sivic and zisserman were the first to adapt ir techniques to visual search. in their video google system affine invariant features are first detected in all the video frames they are indexing using both shape adapted regions around harris feature points and zisserman mikolajczyk and schmid and maximally stable extremal regions chum urban et al. as shown in figure next sift descriptors are computed from each normalized region the patches shown in figure then an average covariance matrix for these descriptors is estimated by accumulating statistics for features tracked from frame to frame. the feature descriptor covariance is then used to define a mahalanobis distance between feature descriptors in practice feature descriptors are whitened by pre-multiplying them by so that euclidean distances can be in order to apply fast information retrieval techniques to images the high-dimensional feature descriptors that occur in each image must first be mapped into discrete visual words. note that the computation of feature covariances from matched feature points is much more sensible than simply performing a pca on the descriptor space and brown this corresponds roughly to the within-class scatter matrix we studied in section instance recognition figure matching based on visual words and zisserman ieee. features in the query region on the left are matched to corresponding features in a highly ranked video frame. results after removing the stop words and filtering the results using spatial consistency. sivic and zisserman perform this mapping using k-means clustering while some of newer methods discussed below er and stew enius philbin chum isard et al. use alternative techniques such as vocabulary trees or randomized forests. to keep the clustering time manageable only a few hundred video frames are used to learn the cluster centers which still involves estimating several thousand clusters from about descriptors. at visual query time each feature in a new query region figure which is a cropped region from a larger video frame is mapped to its corresponding visual word. to keep very common patterns from contaminating the results a stop list of the most common visual words is created and such words are dropped from further consideration. once a query image or region has been mapped into its constituent visual words likely matching images or video frames must then be retrieved from the database. information retrieval systems do this by matching word distributions frequencies nidnd between the query and target documents where nid is how many times word i occurs in document d and nd is the total number of words in document d. in order to downweight words that occur frequently and to focus the search on rarer hence more informative terms an inverse document frequency weighting log nni is applied where ni is the number of documents containing word i and n is the total number of documents in the database. the combination of these two factors results in the term frequency-inverse document frequency measure at match time each document query region is represented by its tf-idf vector ti nid nd log n ni t ti tm. the similarity between two documents is measured by the dot product between their corresponding normalized vectors t which means that their dissimilarity is proportional to their euclidean distance. in their journal paper sivic and zisserman compare this computer vision algorithms and applications draft vocabulary construction extract affine covariant regions from each database image. compute descriptors and optionally whiten them to make euclidean dis tances meaningful and zisserman cluster the descriptors into visual words either using k-means and zisserman hierarchical clustering er and stew enius or randomized k-d trees chum isard et al. decide which words are too common and put them in the stop list. database construction compute term frequencies for the visual word in each image document fre quencies for each word and normalized tf-idf vectors for each document. compute inverted indices from visual words to images word counts. image retrieval extract regions descriptors and visual words and compute a tf-idf vector for the query image or region. retrieve the top image candidates either by exhaustively comparing sparse tf-idf vectors and zisserman or by using inverted indices to examine only a subset of the images er and stew enius optionally re-rank or verify all the candidate matches using either spatial consistency and zisserman or an affine simpler transformation model chum isard et al. optionally expand the answer set by re-submitting highly ranked matches as new queries philbin sivic et al. algorithm image retrieval using visual words and zisserman nist er and stew enius philbin chum isard et al. chum philbin sivic et al. philbin chum sivic et al. instance recognition simple metric to a dozen other metrics and conclude that it performs just about as well as more complicated metrics. because the number of non-zero ti terms in a typical query or document is small compared to the number of visual words the distance between pairs of tf-idf vectors can be computed quite quickly. after retrieving the top ns documents based on word frequencies sivic and zisserman re-rank these results using spatial consistency. this step involves taking every matching feature and counting the number of k nearest adjacent features that also match between the two documents. latter process is accelerated using inverted files which we discuss in more detail below. as shown in figure this step helps remove spurious false positive matches and produces a better estimate of which frames and regions in the video are actually true matches. algorithm summarizes the processing steps involved in image retrieval using visual words. while this approach works well for tens of thousand of visual words and thousands of keyframes as the size of the database continues to increase both the time to quantize each feature and to find potential matching frames or images can become prohibitive. nist er and stew enius address this problem by constructing a hierarchical vocabulary tree where feature vectors are hierarchically clustered into a k-way tree of prototypes. technique is also known as tree-structured vector quantization and gray at both database construction time and query time each descriptor vector is compared to several prototypes at a given level in the vocabulary tree and the branch with the closest prototype is selected for further refinement in this way vocabularies with millions of words can be supported which enables individual words to be far more discriminative while only requiring comparisons for quantizing each descriptor. at query time each node in the vocabulary tree keeps its own inverted file index so that features that match a particular node in the tree can be rapidly mapped to potential matching images. leaf nodes just use the inverted indices of their corresponding leaf-node descendants. to score a particular query tf-idf vector tq against all document vectors using an lp the non-zero tiq entries in tq are used to fetch corresponding non-zero tij entries and the lp norm is efficiently computed as p tij tijp in order to mitigate quantization errors due to noise in the descriptor vectors nist er and stew enius not only score leaf nodes in the vocabulary tree to visual words but also score interior nodes in the tree which correspond to clusters of similar visual words. in their actual implementation nist er and stew enius use an metric. computer vision algorithms and applications draft figure scalable recognition using a vocabulary tree er and stew enius ieee. each mser elliptical region is converted into a sift descriptor which is then quantized by comparing it hierarchically to some prototype descriptors in a vocabulary tree. each leaf node stores its own inverted index list of non-zero tf-idf counts into images that contain that feature. a recognition result showing a query image row being indexed into a database of test images and correctly finding the corresponding four images. because of the high efficiency in both quantizing and scoring features their vocabularytree-based recognition system is able to process incoming images in real time against a database of cd covers and at when matching a database of one million frames taken from six feature-length movies. figure shows some typical images from the database of objects taken under varying viewpoints and illumination that was used to train and test the vocabulary tree recognition system. the state of the art in instance recognition continues to improve rapidly. philbin chum isard et al. have shown that randomized forest of k-d trees perform better than vocabulary trees on a large location recognition task they also compare the effects of using different motion models in the verification stage. in follow-on work chum philbin sivic et al. apply another idea from information retrieval namely instance recognition figure location or building recognition using randomized trees chum isard et al. ieee. the left image is the query the other images are the highest-ranked results. query expansion which involves re-submitting top-ranked images from the initial query as additional queries to generate additional candidate results to further improve recognition rates for difficult or oblique examples. philbin chum sivic et al. show how to mitigate quantization problems in visual words selection using soft assignment where each feature descriptor is mapped to a number of visual words based on its distance from the cluster prototypes. the soft weights derived from these distances are used in turn to weight the counts used in the tf-idf vectors and to retrieve additional images for later verification. taken together these recent advances hold the promise of extending current instance recognition algorithms to performing web-scale retrieval and matching tasks snavely simon et al. agarwal furukawa snavely et al. snavely simon goesele et al. application location recognition one of the most exciting applications of instance recognition today is in the area of location recognition which can be used both in desktop applications did i take this holiday snap? and in mobile applications. the latter case includes not only finding out your current location based on a cell-phone image but also providing you with navigation directions or annotating your images with useful information such as building names and restaurant reviews a portable form of augmented reality. some approaches to location recognition assume that the photos consist of architectural scenes for which vanishing directions can be used to pre-rectify the images for easier matching and cipolla other approaches use general affine covariant interest points to perform wide baseline matching and zisserman the photo tourism system of snavely seitz and szeliski was the first to apply these kinds of ideas to large-scale image matching and location recognition from computer vision algorithms and applications draft figure feature-based location recognition brown and szeliski ieee three typical series of overlapping street photos handheld camera shots and their corresponding database photos. internet photo collections taken under a wide variety of viewing conditions. the main difficulty in location recognition is in dealing with the extremely large community photo collections on web sites such as flickr chum isard et al. chum philbin sivic et al. philbin chum sivic et al. turcot and lowe or commercially captured databases brown and szeliski the prevalence of commonly appearing elements such as foliage signs and common architectural elements further complicates the task. figure shows some results on location recognition from community photo collections while figure shows sample results from denser commercially acquired datasets. in the latter case the overlap between adjacent database images can be used to verify and prune potential matches using temporal filtering i.e. requiring the query image to match nearby overlapping database images before accepting the match. another variant on location recognition is the automatic discovery of landmarks i.e. frequently photographed objects and locations. simon snavely and seitz show how these kinds of objects can be discovered simply by analyzing the matching graph constructed as part of the modeling process in photo tourism. more recent work has extended this approach to larger data sets using efficient clustering techniques and zisserman li wu zach et al. chum philbin and zisserman chum and matas as well as combining meta-data such as gps and textual tags with visual search leibe and van gool crandall backstrom huttenlocher et al. as shown in figure it is now even possible to automatically associate object tags with images based on their cooccurrence in multiple loosely tagged images and seitz gammeter bossard category recognition figure automatic mining annotation and localization of community photo collections leibe and van gool acm. this figure does not show the textual annotations or corresponding wikipedia entries which are also discovered. figure locating star fields using astrometry httpastrometry.net. input star field and some selected star quads. the coordinates of stars c and d are encoded relative to the unit square defined by a and b. quack et al. the concept of organizing the world s photo collections by location has even been recently extended to organizing all of the universe s photos in an application called astrometry httpastrometry.net. the technique used to match any two star fields is to take quadruplets of nearby stars pair of stars and another pair inside their diameter to form a geometric hash by encoding the relative positions of the second pair of points using the inscribed square as the reference frame as shown in figure traditional information retrieval techniques trees built for different parts of a sky atlas are then used to find matching quads as potential star field location hypotheses which can then be verified using a similarity transform. abcd computer vision algorithms and applications draft figure sample images from the xerox class dataset dance perronnin et al. springer. imagine trying to write a program to distinguish such images from other photographs. category recognition while instance recognition techniques are relatively mature and are used in commercial applications such as photosynth generic category recognition is still a largely unsolved problem. consider for example the set of photographs in figure which shows objects taken from different visual categories. ll leave it up to you to name each of the categories. how would you go about writing a program to categorize each of these images into the appropriate class especially if you were also given the choice none of the above as you can tell from this example visual category recognition is an extremely challenging problem no one has yet constructed a system that approaches the performance level of a twoyear-old child. however the progress in the field has been quite dramatic if judged by how much better today s algorithms are compared to those of a decade ago. figure shows a sample image from each of the categories used in the pascal visual object classes challenge. the yellow boxes represent the extent of each of the objects found in a given image. on such closed world collections where the task is to decide among categories today s classification algorithms can do remarkably well. category recognition figure a typical processing pipeline for a bag-of-words category recognition system dance perronnin et al. springer. features are first extracted at keypoints and then quantized to get a distribution over the learned visual words cluster centers. the feature distribution histogram is used to learn a decision surface using a classification algorithm such as a support vector machine. in this section we look at a number of approaches to solving category recognition. while historically part-based representations and recognition algorithms were the preferred approach and elschlager felzenszwalb and huttenlocher fergus perona and zisserman we begin by describing simpler bag-of-features approaches that represent objects and images as unordered collections of feature descriptors. we then look at the problem of simultaneously segmenting images while recognizing objects and also present some applications of such techniques to photo manipulation in section we look at how context and scene understanding as well as machine learning can improve overall recognition results. additional details on the techniques presented in this section can be found in ponce hebert schmid et al. dickinson leonardis schiele et al. fei-fei fergus and torralba bag of words one of the simplest algorithms for category recognition is the bag of words known as bag of features or bag of keypoints approach dance fan et al. lazebnik schmid and ponce csurka dance perronnin et al. zhang marszalek lazebnik et al. as shown in figure this algorithm simply computes the distribution of visual words found in the query image and compares this distribution to those found in the training images. we have already seen elements of this approach in section equations and algorithm the biggest difference from instance recognition is the absence of a geometric verification stage since individual instances of generic visual categories such as those shown in figure have relatively little spatial coherence to their features see the work by lazebnik schmid and computer vision algorithms and applications draft ponce csurka dance fan et al. were the first to use the term bag of keypoints to describe such approaches and among the first to demonstrate the utility of frequency-based techniques for category recognition. their original system used affine covariant regions and sift descriptors k-means visual vocabulary construction and both a na ve bayesian classifier and support vector machines for classification. latter was found to perform better. their newer system dance perronnin et al. uses regular sift patches boosting instead of svms and incorporates a small amount of geometric consistency information. zhang marszalek lazebnik et al. perform a more detailed study of such bag of features systems. they compare a number of feature detectors laplace and schmid and laplacian descriptors rift and spin schmid and ponce and svm kernel functions. to estimate distances for the kernel function they form an image signature s mm analogous to the tf-idf vector t in where the cluster centers mi are made explicit. they then investigate two different kernels for comparing such image signatures. the first is the earth mover s distance tomasi and guibas em ds fijdmi fij where fij is a flow value that can be computed using a linear program and dmi is the ground distance distance between mi and note that the emd can be used to compare two signatures of different lengths where the entries do not need to correspond. the second is a distance ti which measures the likelihood that the two signatures were generated from consistent random processes. these distance metrics are then converted into svm kernels using a generalized gaussian kernel ks a ds where a is a scaling parameter set to the mean distance between training images. in their experiments they find that the emd works best for visual category recognition and the measure is best for texture recognition. category recognition figure comparing collections of feature vectors using pyramid matching. the feature-space pyramid match kernel and darrell constructs a pyramid in high-dimensional feature space and uses it to compute distances implicit correspondences between sets of feature vectors. spatial pyramid matching schmid and ponce ieee divides the image into a pyramid of pooling regions and computes separate visual word histograms inside each spatial bin. instead of quantizing feature vectors to visual words grauman and darrell develop a technique for directly computing an approximate distance between two variably sized collections of feature vectors. their approach is to bin the feature vectors into a multiresolution pyramid defined in feature space and count the number of features that land in corresponding bins bil and c. the distance between the two sets of feature vectors can be thought of as points in a high-dimensional space is computed using histogram intersection between corresponding bins cl minbil these per-level counts are then summed up in a weighted fashion d wlnl with nl cl cl and wl which discounts matches already found at finer levels while weighting finer matches more heavily. is the dimension of the embedding space i.e. the length of the feature vectors. in follow-on work grauman and darrell show how an explicit construction of the pyramid can be avoided using hashing techniques. inspired by this work lazebnik schmid and ponce show how a similar idea can be employed to augment bags of keypoints with loose notions of spatial location getthefollowingdefinitionofapyramidmatchkernel lxyill orthogonal visualvocabulary paradigm long weakfeatures gist strongfeatures computer vision algorithms and applications draft figure a one-dimensional illustration of comparing collections of feature vectors using the pyramid match kernel and darrell distribution of feature vectors sets into the pyramidal bins c histogram of point counts in bins bil and for the two images histogram intersections values per-level similarity scores which are weighted and summed to form the final distancesimilarity metric. analogous to the pooling performed by sift and gist murphy freeman et al. in their work they extract affine region descriptors schmid and ponce and quantize them into visual words. on previous results by fei-fei and perona the feature descriptors are extracted densely a regular grid over the image which can be helpful in describing textureless regions such as the sky. they then form a spatial pyramid of bins containing word counts as shown in figure and use a similar pyramid match kernel to combine histogram intersection counts in a hierarchical fashion. the debate about whether to use quantized feature descriptors or continuous descriptors and also whether to use sparse or dense features continues to this day. boiman shechtman and irani show that if query images are compared to all the features representing a given class rather than just each class image individually nearest-neighbor matching followed by a na ve bayes classifier outperforms quantized visual words instead of using generic feature detectors and descriptors some authors have been investigating learning class-specific features learned-miller and malik often using randomized forests chum isard et al. moosmann nowak and jurie shotton johnson and cipolla or combining the feature generation and image classi category recognition figure image-to-image vs. image-to-class distance comparison shechtman and irani ieee. the query image on the upper left may not match the feature distribution of any of the database images in the bottom row. however if each feature in the query is matched to its closest analog in all the class images a good match can be found. fication stages jin sukthankar et al. others such as serre wolf and poggio and mutch and lowe use hierarchies of dense feature transforms inspired by biological cortical processing combined with svms for final classification. part-based models recognizing an object by finding its constituent parts and measuring their geometric relationships is one of the oldest approaches to object recognition and elschlager kanade yuille we have already seen examples of part-based approaches being used for face recognition and pentland heisele ho wu et al. heisele serre and poggio and pedestrian detection mcallester and ramanan in this section we look more closely at some of the central issues in part-based recognition namely the representation of geometric relationships the representation of individual parts and algorithms for learning such descriptions and recognizing them at run time. more details on part-based models for recognition can be found in the course notes of fergus the earliest approaches to representing geometric relationships were dubbed pictorial structures by fischler and elschlager and consisted of spring-like connections between different feature locations to fit a pictorial structure to an image an energy computer vision algorithms and applications draft figure using pictorial structures to locate and track a person and huttenlocher springer. the structure consists of articulated rectangular body parts head and limbs connected in a tree topology that encodes relative part positions and orientations. to fit a pictorial structure model a binary silhouette image is first computed using background subtraction. function of the form e vili e vijli lj is minimized over all potential part locations or poses and pairs of parts j for which an edge relationship exists in e. note how this energy is closely related to that used with markov random fields which can be used to embed pictorial structures in a probabilistic framework that makes parameter learning easier and huttenlocher part-based models can have different topologies for the geometric connections between the parts for example felzenszwalb and huttenlocher restrict the connections to a tree which makes learning and inference more tractable. a tree topology enables the use of a recursive viterbi programming algorithm bishop in which leaf nodes are first optimized as a function of their parents and the resulting values are then plugged in and eliminated from the energy function see appendix the viterbi algorithm computes an optimal match in on n p time where n is the number of potential locations or poses for each part is the number of edges constraints and p is the number of parts in the graphical model which is equal to in a tree. to further increase the efficiency of the inference algorithm felzenszwalb and huttenlocher restrict the pairwise energy functions vijli lj to be mahalanobis distances on functions of location variables and then use fast distance transform algorithms to minimize each pairwise interaction in time that is closer to linear in n. figure shows the results of using their pictorial structures algorithm to fit an articu category recognition figure graphical models for geometric spatial priors and lowe springer constellation perona and zisserman star felzenszwalb and huttenlocher fergus perona and zisserman k-fan felzenszwalb and huttenlocher tree and huttenlocher bag of features dance fan et al. hierarchy and triggs sparse flexible model and lowe lated body model to a binary image obtained by background segmentation. in this application of pictorial structures parts are parameterized by the locations sizes and orientations of their approximating rectangles. unary matching potentials vili are determined by counting the percentage of foreground and background pixels inside and just outside the tilted rectangle representing each part. over the last decade a large number of different graphical models have been proposed for part-based recognition as shown in figure carneiro and lowe discuss a number of these models and propose one of their own which they call a sparse flexible model it involves ordering the parts and having each part s location depend on at most k of its ancestor locations. the simplest models which we saw in section are bags of words where there are no geometric relationships between different parts or features. while such models can be very efficient they have a very limited capacity to express the spatial arrangement of parts. trees and stars special case of trees where all leaf nodes are directly connected to a common root are the most efficient in terms of inference and hence also learning and huttenlocher fergus perona and zisserman felzenszwalb mcallester and ramanan directed acyclic graphs g come next in terms of complexity and can still support efficient inference although at the cost of imposing a causal structure on the computer vision algorithms and applications draft part model and triggs carneiro and lowe k-fans in which a clique of size k forms the root of a star-shaped model have inference complexity on although with distance transforms and gaussian priors this can be lowered to on k felzenszwalb and huttenlocher crandall and huttenlocher finally fully connected constellation models are the most general but the assignment of features to parts becomes intractable for moderate numbers of parts p since the complexity of such an assignment is on p perona and zisserman the original constellation model was developed by burl weber and perona and consists of a number of parts whose relative positions are encoded by their mean locations and a full covariance matrix which is used to denote not only positional uncertainty but also potential correlations between different parts weber welling and perona extended this technique to a weakly supervised setting where both the appearance of each part and its locations are automatically learned given only whole image labels. fergus perona and zisserman further extend this approach to simultaneous learning of appearance and shape models from scale-invariant keypoint detections. figure shows the shape model learned for the motorcycle class. the top figure shows the mean relative locations for each part along with their position covariances covariances are not shown and likelihood of occurrence. the bottom curve shows the gaussian pdfs for the relative log-scale of each part with respect to the landmark feature. figure shows the appearance model learned for each part visualized as the patches around detected features in the training database that best match the appearance model. figure shows the features detected in the test database dots along with the corresponding parts that they were assigned to circles. as you can see the system has successfully learned and then used a fairly complex model of motorcycle appearance. the part-based approach to recognition has also been extended to learning new categories from small numbers of examples building on recognition components developed for other classes fergus and perona more complex hierarchical part-based models can be developed using the concept of grammars and triggs zhu and mumford a simpler way to use parts is to have keypoints that are recognized as being part of a class vote for the estimated part locations as shown in the top row of figure leonardis and schiele this corresponds to having a star-shaped geometric model. recognition with segmentation the most challenging version of generic object recognition is to simultaneously perform recognition with accurate boundary segmentation for instance recognition this can sometimes be achieved by backprojecting the object model into category recognition figure part-based recognition perona and zisserman springer locations and covariance ellipses for each part along with their occurrence probabilities and relative log-scale densities part examples drawn from the training images that best match the average appearance recognition results for the motorcycle class showing detected features dots and parts circles. correctcorrectincorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectcorrectincorrectincorrectcorrectincorrectcorrectcorrectcorrect computer vision algorithms and applications draft figure interleaved recognition and segmentation leonardis and schiele springer. the process starts by re-recognizing visual words entries in a new image and having each part vote for likely locations and size in a y s voting space row. once a maximum has been found the parts corresponding to this instance are determined by backprojecting the contributing votes. the foreground background segmentation for each object can be found by backprojecting probabilistic masks associated with each codebook entry. the whole recognition and segmentation process can then be repeated. the scene as shown in figure or matching portions of the new scene to pre-learned object models tuytelaars and van gool kannala rahtu brandt et al. for more complex object models such as those for humans figure a different approach is to pre-segment the image into larger or smaller pieces and then match such pieces to portions of the model ren efros et al. mori he zemel and ray gu lim arbelaez et al. an alternative approach by leibe leonardis and schiele which we introduced in the previous section votes for potential object locations and scales based on the detection of features corresponding to pre-clustered visual codebook entries to support segmentation each codebook entry has an associated foreground background mask which is learned as part of the codebook clustering process from pre-labeled object segmentation masks. during recognition once a maximum in the voting space is found the masks associated with the entries that voted for this instance are combined to obtain an object segmentation as shown on the left side of figure a more holistic approach to recognition and segmentation is to formulate the problem as one of labeling every pixel in an image with its class membership and to solve this prob category recognition figure simultaneous recognition and segmentation using textonboost winn rother et al. springer successful recognition results less successful results. computer vision algorithms and applications draft figure layout consistent random field and shotton ieee. the numbers indicate the kind of neighborhood relations that can exist between pixels assigned to the same or different classes. each pairwise relationship carries its own likelihood penalty. lem using energy minimization or bayesian inference techniques i.e. conditional random fields and hebert he zemel and carreira-perpi n an the textonboost system of shotton winn rother et al. uses unary potentials based on image-specific color distributions and jolly rother kolmogorov and blake location information foreground objects are more likely to be in the middle of the image sky is likely to be higher and road is likely to be lower and novel texture-layout classifiers trained using shared boosting. it also uses traditional pairwise potentials that look at image color gradients boykov and jolly rother kolmogorov and blake the texton-layout features first filter the image with a series of oriented filter banks and then cluster the responses to classify each pixel into different texton classes belongie leung et al. the responses are then filtered using offset rectangular regions trained with joint boosting and jones to produce the texton-layout features used as unary potentials. figure shows some examples of images successfully labeled and segmented using textonboost while figure shows examples where it does not do as well. as you can see this kind of semantic labeling can be extremely challenging. the textonboost conditional random field framework has been extended to layoutcrfs by winn and shotton who incorporate additional constraints to recognize multiple object instances and deal with occlusions and even more recently by hoiem rother and winn to incorporate full models. conditional random fields continue to be widely used and extended for simultaneous recognition and segmentation applications and hebert he zemel and ray levin and weiss verbeek and triggs yang meer and foran rabinovich vedaldi galleguillos et al. batra sukthankar and chen larlus and jurie category recognition figure scene completion using millions of photographs and efros acm original image after unwanted foreground removal plausible scene matches with the one the user selected highlighted in red output image after replacement and blending. he and zemel kumar torr and zisserman producing some of the best results on the difficult pascal voc segmentation challenge johnson and cipolla kohli ladick y and torr approaches that first segment the image into unique or multiple segmentations and ullman he zemel and ray russell efros sivic et al. combined with crf models also do quite well csurka and perronnin have one of the top algorithms in the voc segmentation challenge. hierarchical and grammar models are also sometimes used chen yuille et al. zhu chen lin et al. application intelligent photo editing recent advances in object recognition and scene understanding have greatly increased the power of intelligent photo editing applications. one example is the photo clip art system of lalonde hoiem efros et al. which recognizes and segments objects of interest such as pedestrians in internet photo collections and then allows users to paste them into their own photos. another is the scene completion system of hays and efros which tackles the same inpainting problem we studied in section given an image in which we wish to erase and fill in a large section b where do you get the pixels to fill in the gaps in the edited image? traditional approaches either use smooth continuation sapiro caselles et al. or borrowing pixels from other parts of the image and leung criminisi p erez and toyama efros and freeman with the advent of huge repositories of images on the web topic we return to in section it often makes more sense to find a different image to serve as the source of the missing pixels. in their system hays and efros compute the gist of each image and tor computer vision algorithms and applications draft figure automatic photo pop-up efros and hebert acm input image superpixels are grouped into multiple regions labelings indicating ground vertical and sky novel view of resulting piecewise-planar model. ralba torralba murphy freeman et al. to find images with similar colors and composition. they then run a graph cut algorithm that minimizes image gradient differences and composite the new replacement piece into the original image using poisson image blending erez gangnet and blake figure shows the resulting image with the erased foreground rooftops region replaced with sailboats. a different application of image recognition and segmentation is to infer structure from a single photo by recognizing certain scene structures. for example criminisi reid and zisserman detect vanishing points and have the user draw basic structures such as walls in order infer the geometry hoiem efros and hebert on the other hand work with more organic scenes such as the one shown in figure their system uses a variety of classifiers and statistics learned from labeled images to classify each pixel as either ground vertical or sky to do this they begin by computing superpixels and then group them into plausible regions that are likely to share similar geometric labels after all the pixels have been labeled the boundaries between the vertical and ground pixels can be used to infer lines along which the image can be folded into a pop-up removing the sky pixels as shown in figure in related work saxena sun and ng develop a system that directly infers the depth and orientation of each pixel instead of using just three geometric class labels. face detection and localization can also be used in a variety of photo editing applications addition to being used in-camera to provide better focus exposure and flash settings. zanella and fuentes use active shape models to register facial features for creating automated morphs. rother bordeaux hamadi et al. use face and sky detection to determine regions of interest in order to decide which pieces from a collection of images to stitch into a collage. bitouk kumar dhillon et al. describe a system that matches a given face image to a large collection of internet face images which can then be used careful relighting algorithms to replace the face in the original image. applications they describe include de-identification and getting the best possible smile from category recognition figure the importance of context courtesy of antonio torralba. can you name all of the objects in images b especially those that are circled in d. look carefully at the circled objects. did you notice that they all have the same shape being rotated as shown in column everyone in a burst mode group shot. leyvand cohen-or dror et al. show how accurately locating facial features using an active shape model edwards and taylor zhou gu and zhang can be used to warp such features hence the image towards configurations resembling those found in images whose facial attractiveness was highly rated thereby beautifying the image without completely losing a person s identity. most of these techniques rely either on a set of labeled training images which is an essential component of all learning techniques or the even more recent explosion in images available on the internet. the assumption in some of this work in recognition systems based on such very large databases is that as the collection of accessible potentially partially labeled images gets larger finding a close match gets easier. as hays and efros state in their abstract our chief insight is that while the space of images is effectively infinite the space of semantically differentiable scenes is actually not that large. in an interesting commentary on their paper levoy disputes this assertion claiming that features in natural scenes form a heavy-tailed distribution meaning that while some features in photographs are more common than others the relative occurrence of less common features drops slowly. in other words there are many unusual photographs in the world. he does however agree that in computational photography as in many other applications such as speech recognition synthesis and translation simple machine learning algorithms often outperform more sophisticated ones if trained on large enough databases. he also goes on to point out both the potential advantages of such systems such as better automatic color balancing and potential issues and pitfalls with the kind of image fakery that these new approaches enable. for additional examples of photo editing and computational photography applications computer vision algorithms and applications draft figure more examples of context read the letters in the first group the numbers in the second and the letters and numbers in the third. courtesy of antonio torralba. enabled by internet computer vision please see recent workshops on this as well as the special journal issue baker and shan and the course on internet vision by tamara berg context and scene understanding thus far we have mostly considered the task of recognizing and localizing objects in isolation from that of understanding the scene in which the object occur. this is a severe limitation as context plays a very important role in human object recognition and torralba as we will see in this section it can greatly improve the performance of object recognition algorithms hoiem hays et al. as well as providing useful semantic clues for general scene understanding consider the two photographs in figure b. can you name all of the objects especially those circled in images d? now have a closer look at the circled objects. do see any similarity in their shapes? in fact if you rotate them by they are all the same as the blob shown in figure so much for our ability to recognize object by their shape! another more artificial example of recognition in context is shown in figure try to name all of the letters and numbers and then see if you guessed right. even though we have not addressed context explicitly earlier in this chapter we have already seen several instances of this general idea being used. a simple way to incorporate spatial information into a recognition algorithm is to compute feature statistics over different regions as in the spatial pyramid system of lazebnik schmid and ponce part-based models figures use a kind of local context where various parts need to be arranged in a proper geometric relationship to constitute an object. the biggest difference between part-based and context models is that the latter combine objects into scenes and the number of constituent objects from each class is not known in advance. in fact it is possible to combine part-based and context models into the same httpwww.internetvisioner.org. context and scene understanding figure contextual scene models for object recognition torralba freeman et al. springer some street scenes and their corresponding labels buildings red cars green trees blue road some office scenes computer screen green keyboard blue mouse learned contextual models built from these labeled scenes. the top row shows a sample label image and the distribution of the objects relative to the center red or screen object. the bottom rows show the distributions of parts that make up each object. recognition architecture torralba and freeman sudderth torralba freeman et al. crandall and huttenlocher consider the street and office scenes shown in figure b. if we have enough training images with labeled regions such as buildings cars and roads or monitors keyboards and mice we can develop a geometric model for describing their relative positions. sudderth torralba freeman et al. develop such a model which can be thought of as a two-level constellation model. at the top level the distributions of objects relative to each other buildings with respect to cars is modeled as a gaussian upper right corners. at the bottom level the distribution of parts covariant features with respect to the object center is modeled using a mixture of gaussians lower two rows. however since the number of objects in the scene and parts in each object is unknown a latent dirichlet process is used to model object and part creation in a generative framework. the distributions for all of the objects and parts are learned from a large labeled database and then later used during inference to label the elements of a scene. another example of context is in simultaneous segmentation and recognition objects sfeatureswithintensityproportionaltoprobability.partsaretranslatedbythatobject smeanpositionwhilethedashedellipsesindicateeachobject smarginaltransformationcovariance computer vision algorithms and applications draft where the arrangements of various objects in a scene are used as part of the labeling process. torralba murphy and freeman describe a conditional random field where the estimated locations of building and roads influence the detection of cars and where boosting is used to learn the structure of the crf. rabinovich vedaldi galleguillos et al. use context to improve the results of crf segmentation by noting that certain adjacencies are more likely than others e.g. a person is more likely to be on a horse than on a dog. context also plays an important role in inference from single images using computer vision techniques for labeling pixels as belonging to the ground vertical surfaces or sky efros and hebert this line of work has been extended to a more holistic approach that simultaneously reasons about object identity location surface orientations occlusions and camera viewing parameters efros and hebert a number of approaches use the gist of a scene torralba murphy freeman et al. to determine where instances of particular objects are likely to occur. for example murphy torralba and freeman train a regressor to predict the vertical locations of objects such as pedestrians cars and buildings screens and keyboard for indoor office scenes based on the gist of an image. these location distributions are then used with classic object detectors to improve the performance of the detectors. gists can also be used to directly match complete images as we saw in the scene completion work of hays and efros finally some of the most recent work in scene understanding exploits the existence of large numbers of labeled even unlabeled images to perform matching directly against whole images where the images themselves implicitly encode the expected relationships between objects torralba liu et al. malisiewicz and efros we discuss such techniques in the next section where we look at the influence that large image databases have had on object recognition and scene understanding. learning and large image collections given how learning techniques are widely used in recognition algorithms you may wonder whether the topic of learning deserves its own section even chapter or whether it is just part of the basic fabric of all recognition tasks. in fact trying to build a recognition system without lots of training data for anything other than a basic pattern such as a upc code has proven to be a dismal failure. in this chapter we have already seen lots of techniques borrowed from the machine learning statistics and pattern recognition communities. these include principal component subspace and discriminant analysis and more sophisticated discriminative classification algorithms such as neural networks support vector machines and boosting context and scene understanding figure recognition by scene alignment torralba liu et al. input image matched images with similar scene configurations final labeling of the input image. tion some of the best-performing techniques on challenging recognition benchmarks and ray felzenszwalb mcallester and ramanan fritz and schiele vedaldi gulshan varma et al. rely heavily on the latest machine learning techniques whose development is often being driven by challenging vision problems perona and sch olkopf a distinction sometimes made in the recognition community is between problems where most of the variables of interest parts are already labeled and systems that learn more of the problem structure with less supervision perona and zisserman fei-fei fergus and perona in fact recent work by sivic russell zisserman et al. has demonstrated the ability to learn visual hierarchies of object parts with related visual appearance and scene segmentations in a totally unsupervised framework. perhaps the most dramatic change in the recognition community has been the appearance of very large databases of training early learning-based algorithms such as those for face and pedestrian detection used relatively few the hundreds labeled examples to train recognition algorithm parameters the thresholds used in boosting. today some recognition algorithms use databases such as labelme torralba murphy et al. which contain tens of thousands of labeled examples. the existence of such large databases opens up the possibility of matching directly against the training images rather than using them to learn the parameters of recognition algorithms. russell torralba liu et al. describe a system where a new image is matched against each of the training images from which a consensus labeling for the unknown objects in the scene can be inferred as shown in figure malisiewicz and efros start by over-segmenting each image and then use the labelme database to search for similar images and configurations in order to obtain per-pixel category labelings. it is also possible to combine feature-based correspondence algorithms with large labeled databases to perform we have already seen some computational photography applications of such databases in section computer vision algorithms and applications draft figure recognition using tiny images freeman and fergus ieee columns and show sample input images and columns and show the corresponding nearest neighbors in the database of million tiny images. simultaneous recognition and segmentation yuen and torralba when the database of images becomes large enough it is even possible to directly match complete images with the expectation of finding a good match. torralba freeman and fergus start with a database of million tiny images and compensate for the poor accuracy in their image labels which are collected automatically from the internet by using a semantic taxonomy to infer the most likely labels for a new image. somewhere in the million images there are enough examples to associate some set of images with each of the non-abstract nouns in wordnet that they use in their system. some sample recognition results are shown in figure another example of a large labeled database of images is imagenet dong socher et al. which is collecting images for the nouns sets in wordnet as of april about carefully vetted examples for context and scene understanding figure imagenet dong socher et al. ieee. this database contains over carefully vetted images for each of of april nouns from the wordnet hierarchy. synsets have been collected the paper by deng dong socher et al. also has a nice review of related databases. as we mentioned in section the existence of large databases of partially labeled internet imagery has given rise to a new sub-field of internet computer vision with its own and a special journal issue baker and shan application image search even though visual recognition algorithms are by some measures still in their infancy they are already starting to have some impact on image search i.e. the retrieval of images from the web using combinations of keywords and visual similarity. today most image search engines rely mostly on textual keywords found in captions nearby text and filenames augmented by user click-through data and szummer as recognition algorithms continue to improve however visual features and visual similarity will start being used to recognize images with missing or erroneous keywords. the topic of searching by visual similarity has a long history and goes by a variety of names including content-based image retrieval worring santini et al. lew sebe djeraba et al. vasconcelos datta joshi li et al. and query by image content sawhney niblack et al. original publications in these fields were based primarily on simple whole-image similarity metrics such as color and texture and ballard jacobs finkelstein and salesin manjunathi and ma httpwww.internetvisioner.org. computer vision algorithms and applications draft in more recent work fergus perona and zisserman use a feature-based learning and recognition algorithm to re-rank the outputs from a traditional keyword-based image search engine. in follow-on work fergus fei-fei perona et al. cluster the results returned by image search using an extension of probabilistic latest semantic analysis and then select the clusters associated with the highest ranked results as the representative images for that category. even more recent work relies on carefully annotated image databases such as labelme torralba murphy et al. for example malisiewicz and efros describe a system that given a query image can find similar labelme images whereas liu yuen and torralba combine feature-based correspondence algorithms with the labeled database to perform simultaneous recognition and segmentation. recognition databases and test sets in addition to rapid advances in machine learning and statistical modeling techniques one of the key ingredients in the continued improvement of recognition algorithms has been the increased availability and quality of image recognition databases. tables and which are based on similar tables in fei-fei fergus and torralba updated with more recent entries and urls show some of the mostly widely used recognition databases. some of these databases such as the ones for face recognition and localization date back over a decade. the most recent ones such as the pascal database are refreshed annually with ever more challenging problems. table shows examples of databases used primarily for image recognition while table shows databases where more accurate localization or segmentation information is available and expected. ponce berg everingham et al. discuss some of the problems with earlier datasets and describe how the latest pascal visual object classes challenge aims to overcome these. some examples of the visual classes in the challenge are shown in figure the slides from the voc are a great source for pointers to the best recognition techniques currently available. two of the most recent trends in recognition databases are the emergence of web-based annotation and data collection tools and the use of search and recognition algorithms to build up databases berg everingham et al. some of the most interesting work in human annotation of images comes from a series of interactive multi-person games such as esp ahn and dabbish and peekaboom ahn liu and blum in these games people help each other guess the identity of a hidden image by giving textual clues as to its contents which implicitly labels either the whole image or just regions. a more httppascallin.ecs.soton.ac.ukchallengesvoc. recognition databases and test sets name url extents contents reference face and person recognition yale face database centered face images frontal faces belhumeur resources for face detection various databases httpvision.ai.uiuc.edumhyangface-detection-survey.html centered face images feret belhumeur hespanha and kriegman faces in various poses yang kriegman and ahuja frontal faces phillips moon rizvi et al. httpwww.frvt.orgferet frvt httpwww.frvt.org cmu pie database centered face images faces in various poses centered face image faces in various poses phillips scruggs o toole et al. httpwww.ri.cmu.eduprojectsproject sim baker and bsat cmu multi-pie database centered face image faces in various poses httpmultipie.org faces in the wild internet images faces in various poses gross matthews cohn et al. httpvis-www.cs.umass.edulfw huang ramesh berg et al. consumer image person db complete images people httpchenlab.ece.cornell.edupeopleandygallagherdataset.html gallagher and chen object recognition caltech segmentation masks httpwww.vision.caltech.eduimage categories fei-fei fergus and perona caltech centered objects categories and clutter httpwww.vision.caltech.eduimage griffin holub and perona centered objects instances nene nayar and murase centered objects httpwww.mis.tu-darmstadt.dedatasets instances views leibe and schiele instance recognition benchmark objects in various poses objects httpvis.uky.edu steweukbench oxford buildings dataset pictures of buildings images nist er and stew enius norb tiny images bounding box complete images httpwww.robots.ox.ac.uk vggdataoxbuildings httpwww.cs.nyu.edu httppeople.csail.mit.edutorralbatinyimages philbin chum isard et al. toys lecun huang and bottou things torralba freeman and fergus imagenet httpwww.image-net.org complete images things deng dong socher et al. table image databases for recognition adapted and expanded from fei-fei fergus and torralba computer vision algorithms and applications draft name url extents contents reference object detection localization cmu frontal faces patches httpvasc.ri.cmu.eduidbhtmlfacefrontal images mit frontal faces patches cmu face detection databases multiple faces frontal faces rowley baluja and kanade frontal faces sung and poggio faces in various poses httpwww.ri.cmu.eduresearch project detail.html?project schneiderman and kanade uiuc image db bounding boxes cars cogcompdatacar caltech pedestrian dataset bounding boxes agarwal and roth pedestrians httpwww.vision.caltech.eduimage datasetscaltechpedestrians dollar wojek schiele et al. database segmentation masks bikes cars people httpwww.emt.tugraz.at pinzdatagraz ethz toys cluttered images httpwww.vision.ee.ethz.ch calvindatasets.html tu darmstadt db segmentation masks httpwww.vision.ee.ethz.ch bleibedatadatasets.html msr cambridge segmentation masks opelt pinz fussenegger et al. toys boxes magazines ferrari tuytelaars and van gool motorbikes cars cows leibe leonardis and schiele classes httpresearch.microsoft.comen-usprojectsobjectclassrecognition shotton winn rother et al. labelme dataset polygonal boundary categories httplabelme.csail.mit.edu russell torralba murphy et al. lotus hill segmentation masks scenes and hierarchies httpwww.imageparsing.com on-line annotation tools yao yang lin et al. esp game image descriptions web images httpwww.gwap.comgwap von ahn and dabbish peekaboom labeled regions web images httpwww.gwap.comgwap von ahn liu and blum labelme polygonal boundary high-resolution images httplabelme.csail.mit.edu collections of challenges russell torralba murphy et al. pascal segmentation boxes various httppascallin.ecs.soton.ac.ukchallengesvoc everingham van gool williams et al. table image databases for detection and localization adapted and expanded from feifei fergus and torralba recognition databases and test sets airplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor figure sample images from the pascal visual object classes challenge database van gool williams et al. the original images were obtained from flickr and the database rights are explained on serious volunteer effort is the labelme database in which vision researchers contribute manual polygonal region annotations in return for gaining access to the database torralba murphy et al. the use of computer vision algorithms for collecting recognition databases dates back to the work of fergus fei-fei perona et al. who cluster the results returned by google image search using an extension of plsa and then select the clusters associated with the highest ranked results. more recent examples of related techniques include the work of berg and forsyth and li and fei-fei whatever methods are used to collect and validate recognition databases they will continue to grow in size utility and difficulty from year to year. they will also continue to be an essential component of research into the recognition and scene understanding problems which remain as always the grand challenges of computer vision. computer vision algorithms and applications draft additional reading although there are currently no specialized textbooks on image recognition and scene understanding some surveys and collections of papers hebert schmid et al. dickinson leonardis schiele et al. can be found that describe the latest approaches. other good sources of recent research are courses on this topic such as the iccv short course fergus and torralba and antonio torralba s more comprehensive mit course the pascal voc challenge web site contains workshop slides that summarize today s best performing algorithms. the literature on face pedestrian car and other object detection is quite extensive. seminal papers in face detection include those by osuna freund and girosi sung and poggio rowley baluja and kanade and viola and jones with yang kriegman and ahuja providing a comprehensive survey of early work in this field. more recent examples include ho wu et al. heisele serre and poggio early work in pedestrian and car detection was carried out by gavrila and philomin gavrila papageorgiou and poggio mohan papageorgiou and poggio and schneiderman and kanade more recent examples include the work of belongie malik and puzicha mikolajczyk schmid and zisserman dalal and triggs leibe seemann and schiele dalal triggs and schmid opelt pinz and zisserman torralba andriluka roth and schiele felzenszwalb mcallester and ramanan rogez rihan ramalingam et al. andriluka roth and schiele kumar zisserman and h.s.torr dollar belongie and perona and felzenszwalb girshick mcallester et al. while some of the earliest approaches to face recognition involved finding the distinctive image features and measuring the distances between them and elschlager kanade yuille more recent approaches rely on comparing gray-level images often projected onto lower dimensional subspaces and pentland belhumeur hespanha and kriegman moghaddam and pentland moghaddam jebara and pentland heisele ho wu et al. heisele serre and poggio additional details on principal component analysis and its bayesian counterparts can be found in appendix and books and articles on this topic tibshirani and friedman bishop roweis tipping and bishop leonardis and bischof vidal ma and sastry the topics of subspace learning local distance functions and metric learning are covered by cai he hu et al. frome singer sha et al. guillaumin verbeek and schmid ramanan and baker and sivic everingham and zisserman an alternative to directly matching gray-level images or patches is to use non-linear local transforms such as local binary patterns hadid and pietik ainen zhao and pietik ainen cao yin tang et al. additional reading in order to boost the performance of what are essentially appearance-based models a variety of shape and pose deformation models have been developed vetter and poggio including active shape models taylor and cootes cootes cooper taylor et al. davies twining and taylor elastic bunch graph matching fellous kr uger et al. morphable models and vetter and active appearance models cootes edwards et al. cootes edwards and taylor gross baker matthews et al. gross matthews and baker matthews xiao and baker liang xiao wen et al. ramnath koterba xiao et al. the topic of head pose estimation in particular is covered in a recent survey by murphy-chutorian and trivedi additional information about face recognition can be found in a number of surveys and books on this topic wilson and sirohey zhao chellappa phillips et al. li and jain as well as on the face recognition web databases for face recognition are discussed by phillips moon rizvi et al. sim baker and bsat gross shi and cohn huang ramesh berg et al. and phillips scruggs o toole et al. algorithms for instance recognition i.e. the detection of static man-made objects that only vary slightly in appearance but may vary in pose are mostly based on detecting points of interest and describing them using viewpoint-invariant descriptors rothganger lazebnik schmid et al. ferrari tuytelaars and van gool gordon and lowe obdr z alek and matas kannala rahtu brandt et al. sivic and zisserman as the size of the database being matched increases it becomes more efficient to quantize the visual descriptors into words and zisserman schindler brown and szeliski sivic and zisserman turcot and lowe and to then use informationretrieval techniques such as inverted indices er and stew enius philbin chum isard et al. philbin chum sivic et al. query expansion philbin sivic et al. agarwal snavely simon et al. and min hashing and zisserman li wu zach et al. chum philbin and zisserman chum and matas to perform efficient retrieval and clustering. a number of surveys collections of papers and course notes have been written on the topic of category recognition ponce hebert schmid et al. dickinson leonardis schiele et al. fei-fei fergus and torralba some of the seminal papers on the bag of words of keypoints approach to whole-image category recognition have been written by csurka dance fan et al. lazebnik schmid and ponce csurka dance perronnin et al. grauman and darrell and zhang marszalek httpwww.face-rec.org. computer vision algorithms and applications draft lazebnik et al. additional and more recent papers in this area include sivic russell efros et al. serre wolf and poggio opelt pinz fussenegger et al. grauman and darrell torralba murphy and freeman boiman shechtman and irani ferencz learned-miller and malik and mutch and lowe it is also possible to recognize objects based on their contours e.g. using shape contexts malik and puzicha or other techniques and schmid shotton blake and cipolla opelt pinz and zisserman ferrari tuytelaars and van gool many object recognition algorithms use part-based decompositions to provide greater invariance to articulation and pose. early algorithms focused on the relative positions of the parts and elschlager kanade yuille while newer algorithms use more sophisticated models of appearance and huttenlocher fergus perona and zisserman felzenszwalb mcallester and ramanan good overviews on part-based models for recognition can be found in the course notes of fergus carneiro and lowe discuss a number of graphical models used for part-based recognition which include trees and stars and huttenlocher fergus perona and zisserman felzenszwalb mcallester and ramanan k-fans felzenszwalb and huttenlocher crandall and huttenlocher and constellations weber and perona weber welling and perona fergus perona and zisserman other techniques that use part-based recognition include those developed by dork o and schmid and bar-hillel hertz and weinshall combining object recognition with scene segmentation can yield strong benefits. one approach is to pre-segment the image into pieces and then match the pieces to portions of the model ren efros et al. mori he zemel and ray russell efros sivic et al. borenstein and ullman csurka and perronnin gu lim arbelaez et al. another is to vote for potential object locations and scales based on object detection leonardis and schiele one of the currently most popular approaches is to use conditional random fields and hebert he zemel and carreira-perpi n an he zemel and ray levin and weiss winn and shotton hoiem rother and winn rabinovich vedaldi galleguillos et al. verbeek and triggs yang meer and foran batra sukthankar and chen larlus and jurie he and zemel shotton winn rother et al. kumar torr and zisserman which produce some of the best results on the difficult pascal voc segmentation challenge johnson and cipolla kohli ladick y and torr more and more recognition algorithms are starting to use scene context as part of their recognition strategy. representative papers in this area include those by torralba torralba murphy freeman et al. murphy torralba and freeman torralba murphy and freeman crandall and huttenlocher rabinovich vedaldi gal exercises leguillos et al. russell torralba liu et al. hoiem efros and hebert hoiem efros and hebert sudderth torralba freeman et al. and divvala hoiem hays et al. sophisticated machine learning techniques are also becoming a key component of successful object detection and recognition algorithms and ray felzenszwalb mcallester and ramanan fritz and schiele sivic russell zisserman et al. vedaldi gulshan varma et al. as is exploiting large human-labeled databases torralba liu et al. malisiewicz and efros torralba freeman and fergus liu yuen and torralba rough three-dimensional models are also making a comeback for recognition as evidenced in some recent papers and fei-fei sun su savarese et al. su sun fei-fei et al. as always the latest conferences on computer vision are your best reference for the newest algorithms in this rapidly evolving field. exercises ex face detection build and test one of the face detectors presented in section download one or more of the labeled face detection databases in table generate your own negative examples by finding photographs that do not contain any people. implement one of the following face detectors devise one of your own boosting based on simple area features with an optional cascade of detectors and jones pca face subspace and pentland distances to clustered face and non-face prototypes followed by a neural network and poggio or svm freund and girosi classifier a multi-resolution neural network trained directly on normalized gray-level patches baluja and kanade test the performance of your detector on the database by evaluating the detector at every location in a sub-octave pyramid. optionally retrain your detector on false positive examples you get on non-face images. ex determining the threshold for adaboost given a set of function evaluations on the training examples xi fi fxi training labels yi and weights wi computer vision algorithms and applications draft as explained in algorithm devise an efficient algorithm to find values of and s that maximize wiyihsfi where hx signx ex face recognition using eigenfaces collect a set of facial photographs and then build a recognition system to re-recognize the same people. take several photos of each of your classmates and store them. align the images by automatically or manually detecting the corners of the eyes and using a similarity transform to stretch and rotate each image to a canonical position. compute the average image and a pca subspace for the face images take a new set of photographs a week later and use them as your test set. compare each new image to each database image and select the nearest one as the recognized identity. verify that the distance in pca space is close to the distance computed with a full ssd of squared difference measure. compute different principal components for identity and expression and use them to improve your recognition results. ex bayesian face recognition moghaddam jebara and pentland compute separate covariance matrices i and e by looking at differences between all pairs of images. at run time they select the nearest image to determine the facial identity. does it make sense to estimate statistics for all pairs of images and use them for testing the distance to the nearest exemplar? discuss whether this is statistically correct. how is the all-pair intrapersonal covariance matrix i related to the within-class scatter matrix sw? does a similar relationship hold between e and sb? ex modular eigenfaces extend your face recognition system to separately match the eye nose and mouth regions as shown in figure after normalizing face images to a canonical scale and location manually segment out some of the eye nose and face regions. build separate detectors for these three four kinds of region either using a subspace approach or one of the techniques presented in section for each new image to be recognized first detect the locations of the facial features. exercises then match the individual features against your database and note the locations of these features. train and test a classifier that uses the individual feature matching ids as well as tionally the feature locations to perform face recognition. ex recognition-based color balancing build a system that recognizes the most important color areas in common photographs grass skin and color balances the image accordingly. some references and ideas for skin detection are given in exercise and by forsyth and fleck jones and rehg vezhnevets sazonov and andreeva and kakumanu makrogiannis and bourbakis these may give you ideas for how to detect other regions or you can try more sophisticated mrf-based approaches winn rother et al. ex pedestrian detection build and test one of the pedestrian detectors presented in section ex simple instance recognition use the feature detection matching and alignment algorithms you developed in exercises and to find matching images given a query image or region evaluate several feature detectors descriptors and robust geometric verification strate gies either on your own or by comparing your results with those of classmates. ex large databases and location recognition extend the previous exercise to larger databases using quantized visual words and information retrieval techniques as described in algorithm test your algorithm on a large database such as the one used by nist er and stew enius or philbin chum sivic et al. which are listed in table alternatively use keyword search on the web or in a photo sharing site for a city to create your own database. ex bag of words adapt the feature extraction and matching pipeline developed in exercise to category recognition using some of the techniques described in section download the training and test images from one or more of the databases listed in tables and e.g. caltech caltech or pascal voc. extract features from each of the training images quantize them and compute the tf-idf vectors of words histograms. computer vision algorithms and applications draft as an option consider not quantizing the features and using pyramid matching and darrell or using a spatial pyramid for greater selectivity schmid and ponce choose a classification algorithm nearest neighbor classification or support vector machine and train your recognizer i.e. build up the appropriate data structures k-d trees or set the appropriate classifier parameters. test your algorithm on the test data set using the same pipeline you developed in steps and compare your results to the best reported results. explain why your results differ from the previously reported ones and give some ideas for how you could improve your system. you can find a good synopsis of the best-performing classification algorithms and their approaches in the report of the pascal visual object classes challenge found on their web site ex object detection and localization extend the classification algorithm developed in the previous exercise to localize the objects in an image by reporting a bounding box around each detected object. the easiest way to do this is to use a sliding window approach. some pointers to recent techniques in this area can be found in the workshop associated with the pascal voc challenge. ex part-based recognition choose one or more of the techniques described in section and implement a part-based recognition system. since these techniques are fairly involved you will need to read several of the research papers in this area select which general approach you want to follow and then implement your algorithm. a good starting point could be the paper by felzenszwalb mcallester and ramanan since it performed well in the pascal voc detection challenge. ex recognition and segmentation choose one or more of the techniques described in section and implement a simultaneous recognition and segmentation system. since these techniques are fairly involved you will need to read several of the research papers in this area select which general approach you want to follow and then implement your algorithm. test your algorithm on one or more of the segmentation databases in table ex context implement one or more of the context and scene understanding systems described in section and report on your experience. does context or whole scene understanding perform better at naming objects than stand-alone systems? exercises ex tiny images download the tiny images database from httppeople.csail.mit. edutorralbatinyimages and build a classifier based on comparing your test images directly against all of the labeled training images. does this seem like a promising approach? computer vision algorithms and applications draft chapter conclusion in this book we have covered a broad range of computer vision topics. starting with image formation we have seen how images can be pre-processed to remove noise or blur segmented into regions or converted into feature descriptors. multiple images can be matched and registered with the results used to estimate motion track people reconstruct models or merge images into more attractive and interesting composites and renderings. images can also be analyzed to produce semantic descriptions of their content. however the gap between computer and human performance in this area is still large and is likely to remain so for many years. our study has also exposed us to a wide range of mathematical techniques. these include continuous mathematics such as signal processing variational approaches three-dimensional and projective geometry linear algebra and least squares. we have also studied topics in discrete mathematics and computer science such as graph algorithms combinatorial optimization and even database techniques for information retrieval. since many problems in computer vision are inverse problems that involve estimating unknown quantities from noisy input data we have also looked at bayesian statistical inference techniques as well as machine learning techniques to learn probabilistic models from large amounts of training data. as the availability of partially labeled visual imagery on the internet continues to increase exponentially this latter approach will continue to have a major impact on our field. you may ask why is our field so broad and aren t there any unifying principles that can be used to simplify our study? part of the answer lies in the expansive definition of computer vision which is the analysis of images and video as well as the incredible complexity inherent in the formation of visual imagery. in some ways our field is as complex as the study of automotive engineering which requires an understanding of internal combustion mechanics aerodynamics ergonomics electrical circuitry and control systems among other computer vision algorithms and applications draft topics. computer vision similarly draws on a wide variety of sub-disciplines which makes it challenging to cover in a one-semester course let alone to achieve mastery during a course of graduate studies. conversely the incredible breadth and technical complexity of computer vision problems is what draws many people to this research field. because of this richness and the difficulty in making and measuring progress i have attempted to instill in my students and in readers of this book a discipline founded on principles from engineering science and statistics. the engineering approach to problem solving is to first carefully define the overall problem being tackled and to question the basic assumptions and goals inherent in this process. once this has been done a number of alternative solutions or approaches are implemented and carefully tested paying attention to issues such as reliability and computational cost. finally one or more solutions are deployed and evaluated in real-world settings. for this reason this book contains many different alternatives for solving vision problems many of which are sketched out in the exercises for students to implement and test on their own. the scientific approach builds upon a basic understanding of physical principles. in the case of computer vision this includes the physics of man-made and natural structures image formation including lighting and atmospheric effects optics and noisy sensors. the task is to then invert this formation using stable and efficient algorithms to obtain reliable descriptions of the scene and other quantities of interest. the scientific approach also encourages us to formulate and test hypotheses which is similar to the extensive testing and evaluation inherent in engineering disciplines. lastly because so much about the image formation process is inherently uncertain and ambiguous a statistical approach that models both uncertainty in the world the number and types of animals in a picture and noise in the image formation process is often essential. bayesian inference techniques can then be used to combine prior and measurement models to estimate the unknowns and to model their uncertainty. machine learning techniques can be used to create the probabilistic models in the first place. efficient learning and inference algorithms such as dynamic programming graph cuts and belief propagation often play a crucial role in this process. given the breadth of material we have covered in this book what new developments are we likely to see in the future? as i have mentioned before one of the recent trends in computer vision is using the massive amounts of partially labeled visual data on the internet as sources for learning visual models of scenes and objects. we have already seen data-driven approaches succeed in related fields such as speech recognition machine translation speech and music synthesis and even computer graphics in image-based rendering and animation from motion capture. a similar process has been occurring in computer vision with some of the most exciting new work occurring at the intersection of the object recognition and machine learning fields. conclusion more traditional quantitative techniques in computer vision such as motion estimation stereo correspondence and image enhancement all benefit from better prior models for images motions and disparities as well as efficient statistical inference techniques such as those for inhomogeneous and higher-order markov random fields. some techniques such as feature matching and structure from motion have matured to where they can be applied to almost arbitrary collections of images of static scenes. this has resulted in an explosion of work in modeling from internet datasets which again is related to visual recognition from massive amounts of data. while these are all encouraging developments the gap between human and machine performance in semantic scene understanding remains large. it may be many years before computers can name and outline all of the objects in a photograph with the same skill as a twoyear-old child. however we have to remember that human performance is often the result of many years of training and familiarity and often works best in special ecologically important situations. for example while humans appear to be experts at face recognition our actual performance when shown people we do not know well is not that good. combining vision algorithms with general inference techniques that reason about the real world will likely lead to more breakthroughs although some of the problems may turn out to be ai-complete in the sense that a full emulation of human experience and intelligence may be necessary. whatever the outcome of these research endeavors computer vision is already having a tremendous impact in many areas including digital photography visual effects medical imaging safety and surveillance and web-based search. the breadth of the problems and techniques inherent in this field combined with the richness of the mathematics and the utility of the resulting algorithms will ensure that this remains an exciting area of study for years to come. computer vision algorithms and applications draft appendix a linear algebra and numerical techniques linear least squares matrix decompositions singular value decomposition eigenvalue decomposition qr factorization cholesky factorization total least squares non-linear least squares direct sparse matrix techniques variable reordering iterative techniques conjugate gradient preconditioning multigrid computer vision algorithms and applications draft in this appendix we introduce some elements of linear algebra and numerical techniques that are used elsewhere in the book. we start with some basic decompositions in matrix algebra including the singular value decomposition eigenvalue decompositions and other matrix decompositions next we look at the problem of linear least squares which can be solved using either the qr decomposition or normal equations. this is followed by non-linear least squares which arise when the measurement equations are not linear in the unknowns or when robust error functions are used. such problems require iteration to find a solution. next we look at direct solution techniques for sparse problems where the ordering of the variables can have a large influence on the computation and memory requirements. finally we discuss iterative techniques for solving large linear linearized least squares problems. good general references for much of this material include the work by bj orck golub and van loan trefethen and bau meyer nocedal and wright and bj orck and dahlquist a note on vector and matrix indexing. to be consistent with the rest of the book and with the general usage in the computer science and computer vision communities i adopt a indexing scheme for vector and matrix element indexing. please note that most mathematical textbooks and papers use indexing so you need to be aware of the differences when you read this book. software implementations. highly optimized and tested libraries corresponding to the algorithms described in this appendix are readily available and are listed in appendix matrix decompositions in order to better understand the structure of matrices and more stably perform operations such as inversion and system solving a number of decompositions factorizations can be used. in this section we review singular value decomposition eigenvalue decomposition qr factorization and cholesky factorization. singular value decomposition one of the most useful decompositions in matrix algebra is the singular value decomposition which states that any real-valued m n matrix a can be written as am n u m p p p v t p n matrix decompositions up p vt vt p where p minm n. the matrices u and v are orthonormal i.e. u t u i and v t v i and so are their column vectors the singular values are all non-negative and can be ordered in decreasing order ui uj vi vj ij. p a geometric intuition for the svd of a matrix a can be obtained by re-writing a u v t in as this formula says that the matrix a takes any basis vector vj and maps it to a direction uj with length j as shown in figure av u or avj juj. if only the first r singular values are positive the matrix a is of rank r and the index p in the svd decomposition can be replaced by r. other words we can drop the last p r columns of u and v an important property of the singular value decomposition of a matrix true for the eigenvalue decomposition of a real symmetric non-negative definite matrix is that if we truncate the expansion a jujvt j we obtain the best possible least squares approximation to the original matrix a. this is used both in eigenface-based face recognition systems and in the separable approximation of convolution kernels eigenvalue decomposition if the matrix c is symmetric it can be written as an eigenvalue decomposition c u u t iuiut i n un n ut ut n in this appendix we denote symmetric matrices using c and general rectangular matrices using a. computer vision algorithms and applications draft figure the action of a matrix a can be visualized by thinking of the domain as being spanned by a set of orthonormal vectors vj each of which is transformed to a new orthogonal vector uj with a length j. when a is interpreted as a covariance matrix and its eigenvalue decomposition is performed each of the uj axes denote a principal direction and each j denotes one standard deviation along that direction. eigenvector matrix u is sometimes written as and the eigenvectors u as in this case the eigenvalues can be both positive and n a special case of the symmetric matrix c occurs when it is constructed as the sum of a number of outer products aiat i aat c which often occurs when solving least squares problems where the matrix a consists of all the ai column vectors stacked side-by-side. in this case we are guaranteed that all of the eigenvalues i are non-negative. the associated matrix c is positive semi-definite xt cx x. if the matrix c is of full rank the eigenvalues are all positive and the matrix is called symmetric positive definite symmetric positive semi-definite matrices also arise in the statistical analysis of data since they represent the covariance of a set of points around their mean x c xxi xt in this case performing the eigenvalue decomposition is known as principal component analysis since it models the principal directions magnitudes of variation of the point eigenvalue decompositions can be computed for non-symmetric matrices but the eigenvalues and eigenvectors can have complex entries in that case. matrix decompositions distribution around their mean as shown in section section and appendix figure shows how the principal components of the covariance matrix c denote the principal axes uj of the uncertainty ellipsoid corresponding to this point distribution and how the j j denote the standard deviations along each axis. the eigenvalues and eigenvectors of c and the singular values and singular vectors of a are closely related. given a u v t we get c aat u v t v u t u u t from this we see that i c. i and that the left singular vectors of a are the eigenvectors of this relationship gives us an efficient method for computing the eigenvalue decomposition of large matrices that are rank deficient such as the scatter matrices observed in computing eigenfaces observe that the covariance matrix c in is exactly the same as c in note also that the individual difference-from-mean images ai xi x are long vectors of length p number of pixels in the image while the total number of exemplars n number of faces in the training database is much smaller. instead of forming c aat which is p p we form the matrix c at a which is n n. involves taking the dot product between every pair of difference images ai and aj. the eigenvalues of c are the squared singular values of a namely and are hence also the eigenvalues of c. the eigenvectors of c are the right singular vectors v of a from which the desired eigenfaces u which are the left singular vectors of a can be computed as u av this final step is essentially computing the eigenfaces as linear combinations of the difference images and pentland if you have access to a high-quality linear algebra package such as lapack routines for efficiently computing a small number of the left singular vectors and singular values of rectangular matrices such as a are usually provided however if storing all of the images in memory is prohibitive the construction of c in can be used instead. how can eigenvalue and singular value decompositions actually be computed? notice that an eigenvector is defined by the equation iui cui or ii cui computer vision algorithms and applications draft can be derived from by post-multiplying both sides by ui. since the latter equation is homogeneous i.e. it has a zero right-hand-side it can only have a non-zero solution for ui if the system is rank deficient i.e. i c evaluating this determinant yields a characteristic polynomial equation in which can be solved for small problems e.g. or matrices in closed form. for larger matrices iterative algorithms that first reduce the matrix c to a real symmetric tridiagonal form using orthogonal transforms and then perform qr iterations are normally used and van loan trefethen and bau bj orck and dahlquist since these techniques are rather involved it is best to use a linear algebra package such as lapack bai bischof et al. see appendix factorization with missing data requires different kinds of iterative algorithms which often involve either hallucinating the missing terms or minimizing some weighted reconstruction metric which is intrinsically much more challenging than regular factorization. this area has been widely studied in computer vision ikeuchi and reddy de la torre and black huynh hartley and heyden buchanan and fitzgibbon gross matthews and baker torresani hertzmann and bregler and is sometimes called generalized pca. however this term is also sometimes used to denote algebraic subspace clustering techniques which is the subject of a forthcoming monograph by vidal ma and sastry qr factorization a widely used technique for stably solving poorly conditioned least squares problems orck and as the basis of more complex algorithms such as computing the svd and eigenvalue decompositions is the qr factorization a qr where q is an orthonormal unitary matrix qqt i and r is upper in computer vision qr can be used to convert a camera matrix into a rotation matrix and an upper-triangular calibration matrix and also in various self-calibration algorithms the most common algorithms for computing qr decompositions modified gram schmidt householder transformations and givens rotations are described by golub and van loan trefethen and bau and bj orck and dahlquist and are the term r comes from the german name for the lower upper decomposition which is lr for links and rechts and right of the diagonal. matrix decompositions procedure choleskyc r r c for i n for j i n rjjn rjjn rijr ii rijn riin r ii riin algorithm cholesky decomposition of the matrix c into its upper triangular form r. also found in lapack. unlike the svd and eigenvalue decompositions qr factorization does not require iteration and can be computed exactly in om n n operations where m is the number of rows and n is the number of columns a tall matrix. cholesky factorization cholesky factorization can be applied to any symmetric positive definite matrix c to convert it into a product of symmetric lower and upper triangular matrices c llt rt r where l is a lower-triangular matrix and r is an upper-triangular matrix. unlike gaussian elimination which may require pivoting and column reordering or may become unstable to roundoff errors or reordering cholesky factorization remains stable for positive definite matrices such as those that arise from normal equations in least squares problems because of the form of the matrices l and r are sometimes called matrix square the algorithm to compute an upper triangular cholesky decomposition of c is a straightforward symmetric generalization of gaussian elimination and is based on the decomposition orck golub and van loan ct c c c i c i in fact there exists a whole family of matrix square roots. any matrix of the form lq or qr where q is a unitary matrix is a square root of c. computer vision algorithms and applications draft rt which through recursion can be turned into c rt rt n rt r. algorithm provides a more procedural definition which can store the upper-triangular matrix r in the same space as c if desired. the total operation count for cholesky factorization is on for a dense matrix but can be significantly lower for sparse matrices with low fill-in note that cholesky decomposition can also be applied to block-structured matrices where the term in is now a square block sub-matrix and c is a rectangular matrix and van loan the computation of square roots can be avoided by leaving the on the diagonal of the middle factor in which results in the c ldlt factorization where d is a diagonal matrix. however since square roots are relatively fast on modern computers this is not worth the bother and cholesky factorization is usually preferred. linear least squares least squares fitting problems are pervasive in computer vision. for example the alignment of images based on matching feature points involves the minimization of a squared distance objective function where els p ri fxi p is the residual between the measured location and its corresponding current predicted location fxi p. more complex versions of least squares problems such as large-scale structure from motion may involve the minimization of functions of thousands of variables. even problems such as image filtering and regularization may involve the minimization of sums of squared errors. figure shows an example of a simple least squares line fitting problem where the quantities being estimated are the line equation parameters b. when the sampled vertical values yi are assumed to be noisy versions of points on the line y mx b the optimal estimates for b can be found by minimizing the squared vertical residuals evls linear least squares note that the function being fitted need not itself be linear to use linear least squares. all that is required is that the function be linear in the unknown parameters. for example polynomial fitting can be written as epls ajxj i while sinusoid fitting with unknown amplitude a and phase known frequency f can be written as esls a f xi which is linear in c. sin f xi c cos f in general it is more common to denote the unknown parameters using x and to write the general form of linear least squares ells expanding the above equation gives us ells xt ax b whose minimum value for x can be found by solving the associated normal equations orck golub and van loan ax at b. the preferred way to solve the normal equations is to use cholesky factorization. let where r is the upper-triangular cholesky factor of the hessian c and c at a rt r d at b. after factorization the solution for x can be obtained as rt z d rx z which involves the solution of two triangular systems i.e. forward and backward substitution orck be extra careful in interpreting the variable names here. in the line-fitting example x is used to denote the horizontal axis but in the general least squares problem x b denotes the unknown parameter vector. computer vision algorithms and applications draft figure least squares regression. the line y mx b is fit to the four noisy data points yi denoted by by minimizing the squared vertical residuals between the data points and the when the measurements yi are assumed to have noise in all directions the sum of orthogonal squared distances to the line byi is minimized using total least squares. in cases where the least squares problem is numerically poorly conditioned should generally be avoided by adding sufficient regularization or prior knowledge about the parameters it is possible to use qr factorization or svd directly on the matrix a orck golub and van loan trefethen and bau nocedal and wright bj orck and dahlquist e.g. ax qrx b rx qt b. note that the upper triangular matrices r produced by the cholesky factorization of c at a and the qr factorization of a are the same but that solving is generally more stable sensitive to roundoff error but slower a constant factor. total least squares in some problems e.g. when performing geometric line fitting in images or plane fitting to point cloud data instead of having measurement error along one particular axis the measured points have uncertainty in all directions which is known as the errors-in-variables model huffel and lemmerling matei and meer in this case it makes more sense to minimize a set of homogeneous squared errors of the form etls which is known as total least squares huffel and vandewalle bj orck golub and van loan van huffel and lemmerling xybmymxb linear least squares the above error metric has a trivial minimum solution at x and is in fact homogeneous in x. for this reason we augment this minimization problem with the requirement that which results in the eigenvalue problem x arg min x xt ax such that the value of x that minimizes this constrained problem is the eigenvector associated with the smallest eigenvalue of at a. this is the same as the last right singular vector of a since a u v at a v at avk k which is minimized by selecting the smallest k value. figure shows a line fitting problem where in this case the measurement errors are assumed to be isotropic in y. the solution for the best line equation ax by c is found by minimizing i.e. finding the eigenvector associated with the smallest eigenvalue etls c at a byi xi yi xi yi notice however that in is only statistically optimal pendix if all of the measured terms in the ai e.g. the yi measurements have equal noise. this is definitely not the case in the line-fitting example of figure since the values are noise-free. to mitigate this we first subtract the mean x and y values from all the measured points xi xi x yi yi y and then fit the line equation ax x by y by minimizing etls xi b again be careful with the variable names here. the measurement equation is ai yi and the unknown parameters are x b c. computer vision algorithms and applications draft the more general case where each individual measurement component can have different noise level as is the case in estimating essential and fundamental matrices is called the heteroscedastic errors-in-variable model and is discussed by matei and meer non-linear least squares in many vision problems such as structure from motion the least squares problem formulated in involves functions fxi p that are not linear in the unknown parameters p. this problem is known as non-linear least squares or non-linear regression orck madsen nielsen and tingleff nocedal and wright it is usually solved by iteratively relinearizing around the current estimate of p using the gradient derivative j f p and computing an incremental improvement p. as shown in equations this results in enls p p p p p where the jacobians jxi p and residual vectors ri play the same role in forming the normal equations as ai and bi in because the above approximation only holds near a local minimum or for small values of p the update p p p may not always decrease the summed square residual error one way to mitigate this problem is to take a smaller step p p p a simple way to determine a reasonable value of is to start with and successively halve the value which is a simple form of line search and fletcher. bj orck nocedal and wright another approach to ensuring a downhill step in error is to add a diagonal damping term to the approximate hessian i.e. to solve where c j t diagc p d d j t direct sparse matrix techniques which is called a damped gauss newton method. the damping parameter is increased if the squared residual is not decreasing as fast as expected i.e. as predicted by and is decreased if the expected decrease is obtained nielsen and tingleff the combination of the newton taylor series approximation and the adaptive damping parameter is commonly known as the levenberg marquardt algorithm marquardt and is an example of more general trust region methods which are discussed in more detail in orck conn gould and toint madsen nielsen and tingleff nocedal and wright when the initial solution is far away from its quadratic region of convergence around a local minimum large residual methods e.g. newton-type methods which add a second-order term to the taylor series expansion in may converge faster. quasi-newton methods such as bfgs which require only gradient evaluations can also be useful if memory size is an issue. such techniques are discussed in textbooks and papers on numerical optimization bj orck conn gould and toint nocedal and wright direct sparse matrix techniques many optimization problems in computer vision such as bundle adjustment and kang triggs mclauchlan hartley et al. hartley and zisserman snavely seitz and szeliski agarwal snavely simon et al. have jacobian and hessian matrices that are extremely sparse for example figure shows the bipartite model typical of structure from motion problems in which most points are only observed by a subset of the cameras which results in the sparsity patterns for the jacobian and hessian shown in figure c. whenever the hessian matrix is sparse enough it is more efficient to use sparse cholesky factorization instead of regular cholesky factorization. in such sparse direct techniques the hessian matrix c and its associated cholesky factor r are stored in compressed form in which the amount of storage is proportional to the number of non-zero entries orck davis algorithms for computing the non-zero elements in c and r from the sparsity pattern of the jacobian matrix j are given by bj orck section and algorithms for computing the numerical cholesky and qr decompositions the sparsity pattern has been computed and storage allocated are discussed by bj orck section for example you can store a list of j cij triples. one example of such a scheme is compressed sparse row storage. an alternative storage method called skyline which stores adjacent vertical spans of non-zero elements is sometimes used in finite element analysis. banded systems such as snakes can store just the non-zero band elements orck section and can be solved in where n is the number of variables and b is the bandwidth. computer vision algorithms and applications draft variable reordering the key to efficiently solving sparse problems using direct techniques is to determine an efficient ordering for the variables which reduces the amount of fill-in i.e. the number of non-zero entries in r that were zero in the original c matrix. we already saw in section how storing the more numerous point parameters before the camera parameters and using the schur complement results in a more efficient algorithm. similarly sorting parameters by time in video-based reconstruction problems usually results in lower fill-in. furthermore any problem whose adjacency graph graph corresponding to the sparsity pattern is a tree can be solved in linear time with an appropriate reordering of the variables all the children before their parents. all of these are examples of good reordering techniques. in the general case of unstructured data there are many heuristics available to find good reorderings orck davis for general adjacency graphs minimum degree orderings generally produce good results. for planar graphs which often arise on image or spline grids nested dissection which recursively splits the graph into two equal halves along a frontier boundary of small size generally works well. such domain decomposition multi-frontal techniques also enable the use of parallel processing since independent sub-graphs can be processed in parallel on separate processors the overall set of steps used to perform the direct solution of sparse least squares problems are summarized in algorithm which is a modified version of algorithm by bj orck section if a series of related least squares problems is being solved as is the case in iterative non-linear least squares steps can be performed ahead of time and reused for each new invocation with different c and d values. when the problem is block-structured as is the case in structure from motion where point variables have dense sub-entries in c and cameras have larger entries the cost of performing the reordering computation is small compared to the actual numerical factorization which can benefit from block-structured matrix operations and van loan it is also possible to apply sparse reordering and multifrontal techniques to qr factorization which may be preferable when the least squares problems are poorly conditioned. iterative techniques when problems become large the amount of memory required to store the hessian matrix c and its factor r and the amount of time it takes to compute the factorization can become prohibitively large especially when there are large amounts of fill-in. this is often the optimal reordering with minimal fill-in is provably np-hard. iterative techniques procedure sparsecholeskysolvec d determine symbolically the structure of c i.e. the adjacency graph. compute a reordering for the variables taking into ac count any block structure inherent in the problem. determine the fill-in pattern for r and allocate the compressed stor age for r as well as storage for the permuted right hand side d. copy the elements of c and d into r and d permuting the values according to the computed ordering. perform the numerical factorization of r using algorithm solve the factored system i.e. rt z d rx z. return the solution x after undoing the permutation. algorithm sparse least squares using a sparse cholesky decomposition of the matrix c. the case with image processing problems defined on pixel grids since even with the optimal reordering dissection the amount of fill can still be large. a preferable approach to solving such linear systems is to use iterative techniques which compute a series of estimates that converge to the final solution e.g. by taking a series of downhill steps in an energy function such as a large number of iterative techniques have been developed over the years including such well-known algorithms as successive overrelaxation and multi-grid. these are described in specialized textbooks on iterative solution techniques saad as well as in more general books on numerical linear algebra and least squares techniques orck golub and van loan trefethen and bau nocedal and wright bj orck and dahlquist conjugate gradient the iterative solution technique that often performs best is conjugate gradient descent which takes a series of downhill steps that are conjugate to each other with respect to the c matrix computer vision algorithms and applications draft i.e. if the u and v descent directions satisfy ut cv in practice conjugate gradient descent outperforms other kinds of gradient descent algorithm because its convergence rate is proportional to the square root of the condition number of c instead of the condition number shewchuk provides a nice introduction to this topic with clear intuitive explanations of the reasoning behind the conjugate gradient algorithm and its performance. algorithm describes the conjugate gradient algorithm and its related least squares counterpart which can be used when the original set of least squares linear equations are available in the form of ax b while it is easy to convince yourself that the two forms are mathematically equivalent the least squares form is preferable if rounding errors start to affect the results because of poor conditioning. it may also be preferable if due to the sparsity structure of a multiplies with the original a matrix are faster or more space efficient than multiplies with c. the conjugate gradient algorithm starts by computing the current residual d which is the direction of steepest descent of the energy function it sets the original descent direction next it multiplies the descent direction by the quadratic form matrix c and combines this with the residual to estimate the optimal step size k. the solution vector xk and the residual vector rk are then updated using this step size. how the least squares variant of the conjugate gradient algorithm splits the multiplication by the c at a matrix across steps and finally a new search direction is calculated by first computing a factor as the ratio of current to previous residual magnitudes. the new search direction is then set to the residual plus times the old search direction pk which keeps the directions conjugate with respect to c. it turns out that conjugate gradient descent can also be directly applied to non-quadratic energy functions e.g. those arising from non-linear least squares instead of explicitly forming a local quadratic approximation c and then computing residuals rk non-linear conjugate gradient descent computes the gradient of the energy function e directly inside each iteration and uses it to set the search direction and wright since the quadratic approximation to the energy function may not exist or may be inaccurate line search is often used to determine the step size k. furthermore to compensate for errors in finding the true function minimum alternative formulas for such as polak ribiere exk are often used and wright the condition number is the ratio of the largest and smallest eigenvalues of c. the actual convergence rate depends on the clustering of the eigenvalues as discussed in the references cited in this section. iterative techniques conjugategradientc d conjugategradientlsa b d for k wk cpk k wk xk kpk rk kwk kpk b at for k vk apk k xk kpk qk kvk at kpk algorithm conjugate gradient and conjugate gradient least squares algorithms. the algorithm is described in more detail in the text but in brief they choose descent directions pk that are conjugate to each other with respect to c by computing a factor by which to discount the previous search direction pk they then find the optimal step size and take a downhill step by an amount kpk. preconditioning as we mentioned previously the rate of convergence of the conjugate gradient algorithm is governed in large part by the condition number its effectiveness can therefore be increased dramatically by reducing this number e.g. by rescaling elements in x which corresponds to rescaling rows and columns in c. in general preconditioning is usually thought of as a change of basis from the vector x to a new vector x sx. the corresponding linear system being solved then becomes as x s or a x b computer vision algorithms and applications draft with a corresponding least squares energy of the form epls xt t cs x xt t d the actual preconditioned matrix c s t cs is usually not explicitly computed. instead algorithm is extended to insert s t and st operations at the appropriate places orck golub and van loan trefethen and bau saad nocedal and wright a good preconditioner s is easy and cheap to compute but is also a decent approximation to a square root of c so that t cs is closer to the simplest such choice is the square root of the diagonal matrix s with d diagc. this has the advantage that any scalar change in variables using radians instead of degrees for angular measurements has no effect on the range of convergence of the iterative technique. for problems that are naturally block-structured e.g. for structure from motion where point positions or camera poses are being estimated a block diagonal preconditioner is often a good choice. a wide variety of more sophisticated preconditioners have been developed over the years orck golub and van loan trefethen and bau saad nocedal and wright many of which can be directly applied to problems in computer vision od and astr om jeong nist er steedly et al. agarwal snavely seitz et al. some of these are based on an incomplete cholesky factorization of c i.e. one in which the amount of fill-in in r is strictly limited e.g. to just the original non-zero elements in other preconditioners are based on a sparsified e.g. tree-based or clustered approximation to c koutis and miller grady koutis miller and tolliver since these are known to have efficient inversion properties. for grid-based image-processing applications parallel or hierarchical preconditioners often perform extremely well szeliski pentland saad szeliski these approaches use a change of basis transformation s that resembles the pyramidal or wavelet representations discussed in section and are hence amenable to parallel and gpu-based implementations. coarser elements in the new representation quickly converge to the low-frequency components in the solution while finer-level elements encode the higher-frequency components. some of the relationships between hierarchical preconditioners incomplete cholesky factorization and multigrid techniques are explored by saad and szeliski if a complete cholesky factorization c rt r is used we get c r t cr i and all iterative algorithms converge in a single step thereby obviating the need to use them but the complete factorization is often too expensive. note that incomplete factorization can also benefit from reordering. iterative techniques multigrid one other class of iterative techniques widely used in computer vision is multigrid techniques henson and mccormick trottenberg oosterlee and schuller which have been applied to problems such as surface interpolation optical flow bruhn weickert kohlberger et al. high dynamic range tone mapping lischinski and werman colorization lischinski and weiss natural image matting lischinski and weiss and segmentation the main idea behind multigrid is to form coarser versions of the problems and use them to compute the low-frequency components of the solution. however unlike simple coarse-to-fine techniques which use the coarse solutions to initialize the fine solution multigrid techniques only correct the low-frequency component of the current solution and use multiple rounds of coarsening and refinement what are often called v and w patterns of motion across the pyramid to obtain rapid convergence. on certain simple homogeneous problems as solving poisson equations multigrid techniques can achieve optimal performance i.e. computation times linear in the number of variables. however for more inhomogeneous problems or problems on irregular grids variants on these techniques such as algebraic multigrid approaches which look at the structure of c to derive coarse level problems may be preferable. saad has a nice discussion of the relationship between multigrid and parallel preconditioners and on the relative merits of using multigrid or conjugate gradient approaches. computer vision algorithms and applications draft appendix b bayesian modeling and inference estimation theory likelihood for multivariate gaussian noise maximum likelihood estimation and least squares robust statistics prior models and bayesian inference markov random fields gradient descent and simulated annealing dynamic programming belief propagation graph cuts linear programming uncertainty estimation analysis computer vision algorithms and applications draft the following problem commonly recurs in this book given a number of measurements feature positions etc. estimate the values of some unknown structure or parameter positions object shape etc.. these kinds of problems are in general called inverse problems because they involve estimating unknown model parameters instead of simulating the forward formation computer graphics is a classic forward modeling problem some objects cameras and lighting simulate the images that would result while computer vision problems are usually of the inverse kind one or more images recover the scene that gave rise to these images. given an instance of an inverse problem there are in general several ways to proceed. for instance through clever sometimes straightforward algebraic manipulation a closed form solution for the unknowns can sometimes be derived. consider for example the camera matrix calibration problem given an image of a calibration pattern consisting of known point positions compute the camera matrix p that maps these points onto the image plane. in more detail we can write this problem as xi yi where yi is the feature position of the ith point measured in the image plane yi zi is the corresponding point position and the pij are the unknown entries of the camera matrix p moving the denominator over to the left hand side we end up with a set of simultaneous linear equations which we can solve using linear least squares to obtain an estimate of p the question then arises is this set of equations the right ones to be solving? if the measurements are totally noise-free or we do not care about getting the best possible answer then the answer is yes. however in general we cannot be sure that we have a reasonable algorithm unless we make a model of the likely sources of error and devise an algorithm that performs as well as possible given these potential errors. in machine learning these problems are called regression problems because we are trying to estimate a contin uous quantity from noisy inputs as opposed to a discrete classification task estimation theory estimation theory the study of such inference problems from noisy data is often called estimation theory and its extension to problems where we explicitly choose a loss function is called statistical decision theory hastie tibshirani and friedman bishop robert we first start by writing down the forward process that leads from our unknowns knowns to a set of noise-corrupted measurements. we then devise an algorithm that will give us an estimate set of estimates that are both insensitive to the noise best they can be and also quantify the reliability of these estimates. the specific equations above are just a particular instance of a more general set of measurement equations here the yi are the noise-corrupted measurements e.g. yi in equation and x is the unknown state yi f ix ni. each measurement comes with its associated measurement model f ix which maps the unknown into that particular measurement. an alternative formulation would be to have one general function fx pi and to use a per-measurement parameter vector pi to distinguish between different measurements e.g. yi zi in equation note that the use of the f ix form makes it straightforward to have measurements of different dimensions which becomes useful when we start adding in prior information each measurement is also contaminated with some noise ni. in equation we have indicated that ni is a zero-mean normal random variable with a covariance matrix i. in general the noise need not be gaussian and in fact it is usually prudent to assume that some measurements may be outliers. however we defer this discussion to appendix after we have explored the simpler gaussian noise case more fully. we also assume that the noise vectors ni are independent. in the case where they are not when some constant gain or offset contaminates all of the pixels in a given image we can add this effect as a nuisance parameter to our state vector x and later estimate its value discard it if so desired. likelihood for multivariate gaussian noise given all of the noisy measurements y we would like to infer a probability distribution on the unknown x vector. we can write the likelihood of having observed the given a particular value of x as l pyx pyix in the kalman filtering literature it is more common to use z instead of y to denote measurements. pyif ix pni. computer vision algorithms and applications draft when each noise vector ni is a multivariate gaussian with covariance i ni n i we can write this likelihood as l i i f f f ixt i i a is a shorthand notation for xt ax. where the matrix norm is often called the mahalanobis distance and and is the norm used to measure the distance between a measurement and the mean of a multivariate gaussian distribution. contours of equal mahalanobis distance are equi-probability contours. note that when the measurement covariance is isotropic same in all directions i.e. when i i i the likelihood can be written as i l i i f where ni is the length of the ith measurement vector yi. we can more easily visualize the structure of the covariance matrix and the corresponding mahalanobis distance if we first perform an eigenvalue or principal component analysis of the covariance matrix diag n t equal-probability contours of the corresponding multi-variate gaussian which are also equidistance contours in the mahalanobis distance are multi-dimensional ellipsoids whose axis directions are given by the columns of eigenvectors and whose lengths are given by the j j of as a cost or energy it is usually more convenient to work with the negative log likelihood which we can think e log l where k log i is a constant that depends on the measurement variances but is independent of x. f ixt i f ix k f i k maximum likelihood estimation and least squares i notice that the inverse covariance ci plays the role of a weight on each of the measurement error residuals i.e. the difference between the contaminated measurement yi and its uncontaminated value f ix. in fact the inverse covariance is often called the information matrix since it tells us how much information is contained in a given measurement i.e. how well it constrains the final estimate. we can also think of this matrix as denoting the amount of confidence to associate with each measurement the letter c. in this formulation it is quite acceptable for some information matrices to be singular degenerate rank or even zero the measurement is missing altogether. rank-deficient measurements often occur for example when using a line feature or edge to measure a edge-like feature since its exact position along the edge is unknown infinite or extremely large variance in order to make the distinction between the noise contaminated measurement and its expected value for a particular setting of x more explicit we adopt the notation y for the former of the tilde as the approximate or noisy value and y f ix for the latter of the hat as the predicted or expected value. we can then write the negative log likelihood as e log l yi i k. maximum likelihood estimation and least squares now that we have presented the likelihood and log likelihood functions how can we find the optimal value for our state estimate x? one plausible choice might be to select the value of x that maximizes l pyx. in fact in the absence of any prior model for x we have l pyx py x pxy. therefore choosing the value of x that maximizes the likelihood is equivalent to choosing the maximum of our probability density estimate for x. when might this be a good idea? if the data constrain the possible values of x so that they all cluster tightly around one value if the distribution pxy is a unimodal gaussian the maximum likelihood estimate is the optimal one in that it is both unbiased and has the least possible variance. in many other cases e.g. if a single estimate is all that is required it is still often the best however if the probability is multimodal i.e. it has several local minima in the log likelihood much more care according to the gauss-markov theorem least squares produces the best linear unbiased estimator for a linear measurement model regardless of the actual noise distribution assuming that the noise is zero mean and uncorrelated. computer vision algorithms and applications draft may be required. in particular it might be necessary to defer certain decisions as the ultimate position of an object being tracked until more measurements have been taken. the condensation algorithm presented in section is one possible method for modeling and updating such multi-modal distributions but is just one example of more general particle filtering and markov chain monte carlo techniques de freitas doucet et al. bishop koller and friedman another possible way to choose the best estimate is to maximize the expected utility conversely to minimize the expected risk or loss associated with obtaining the correct estimate i.e. by minimizing elossx y lx zpzydz. for example if a robot wants to avoid hitting a wall at all costs the loss function will be high whenever the estimate underestimates the true distance to the wall. when lx y y we obtain the maximum likelihood estimate whereas when lx y we obtain the mean square error or expected value estimate. the explicit modeling of a utility or loss function is what characterizes statistical decision theory hastie tibshirani and friedman bishop robert how do we find the maximum likelihood estimate? if the measurement noise is gaussian we can minimize the quadratic objective function this becomes even simpler if the measurement equations are linear i.e. f ix h ix where h is the measurement matrix relating unknown state variables x to measurements y. in this case becomes e yi h i yi h ixt ci yi h ix which is a simple quadratic form in x which can be solved using linear least squares when the measurements are non-linear the system must be solved iteratively using non-linear least squares robust statistics in appendix we assumed that the noise being added to each measurement was multivariate gaussian this is an appropriate model if the noise is the result of lots of tiny errors being added together e.g. from thermal noise in a silicon imager. in most cases however measurements can be contaminated with larger outliers i.e. gross failures in the robust statistics measurement process. examples of such outliers include bad feature matches occlusions in stereo matching and discontinuities in an otherwise smooth image depth map or label image and in such cases it makes more sense to model the measurement noise with a long-tailed contaminated noise model such as a laplacian. the negative log likelihood in this case rather than being quadratic in the measurement residuals has a slower growth in the penalty function to account for the increased likelihood of large errors. this formulation of the inference problem is called an m-estimator in the robust statistics literature hampel ronchetti rousseeuw et al. black and rangarajan stewart and involves applying a robust penalty function to the residuals erls p instead of squaring them. as we mentioned in section we can take the derivative of this function with respect to p and set it to p rt i ri p where is the derivative of and is called the influence function. if we introduce a weight function wr we observe that finding the stationary point of using is equivalent to minimizing the iteratively re-weighted least squares problem eirls in black and where the play the same local weighting role as ci anandan describe a variety of robust penalty functions and their corresponding influence and weighting function. i the irls algorithm alternates between computing the influence functions and solving the resulting weighted least squares problem fixed w values. alternative incremental robust least squares algorithms can be found in the work of sawhney and ayer black and anandan black and rangarajan baker gross ishikawa et al. and textbooks and tutorials on robust statistics hampel ronchetti rousseeuw et al. rousseeuw and leroy stewart it is also possible to apply general optimization techniques directly to the non-linear cost function given in equation which may sometimes have better convergence properties. most robust penalty functions involve a scale parameter which should typically be set to the variance standard deviation depending on the formulation of the non-contaminated computer vision algorithms and applications draft noise. estimating such noise levels directly from the measurements or their residuals however can be problematic as such estimates themselves become contaminated by outliers. the robust statistics literature contains a variety of techniques to estimate such parameters. one of the simplest and most effective is the median absolute deviation m ad which when multiplied by provides a robust estimate of the standard deviation of the inlier noise process. as mentioned in section it is often better to start iterative non-linear minimization techniques such as irls in the vicinity of a good solution by first randomly selecting small subsets of measurements until a good set of inliers is found. the best known of these techniques is random sample consensus and bolles although even better variants such as preemptive ransac er and progressive sample consensus and matas have since been developed. prior models and bayesian inference while maximum likelihood estimation can often lead to good solutions in some cases the range of possible solutions consistent with the measurements is too large to be useful. for example consider the problem of image denoising and if we estimate each pixel separately based on just its noisy version we cannot make any progress as there are a large number of values that could lead to each noisy instead we need to rely on typical properties of images e.g. that they tend to be piecewise smooth the propensity of images to be piecewise smooth can be encoded in a prior distribution px which measures the likelihood of an image being a natural image. for example to encode piecewise smoothness we can use a markov random field model and whose negative log likelihood is proportional to a robustified measure of image smoothness magnitudes. prior models need not be restricted to image processing applications. for example we may have some external knowledge about the rough dimensions of an object being scanned the focal length of a lens being calibrated or the likelihood that a particular object might appear in an image. all of these are examples of prior distributions or probabilities and they can be used to produce more reliable estimates. as we have already seen in and bayes rule states that a posterior distribution pxy over the unknowns x given the measurements y can be obtained by multiplying in fact the maximum likelihood estimate is just the noisy image itself. markov random fields the measurement likelihood pyx by the prior distribution px pxy pyxpx py where py pyxpx is a normalizing constant used to make the pxy distribution proper to taking the negative logarithm of both sides of equation we get log pxy log pyx log px log py which is the negative posterior log likelihood. it is common to drop the constant log py because its value does not matter during energy minimization. however if the prior distribution px depends on some unknown parameters we may wish to keep log py in order to compute the most likely value of these parameters using occam s razor i.e. by maximizing the likelihood of the observations or to select the correct number of free parameters using model selection tibshirani and friedman torr bishop robert to find the most likely a posteriori or map solution x given some measurements y we simply minimize this negative log likelihood which can also be thought of as an energy ex y edx y epx. the first term edx y is the data energy or data penalty and measures the negative log likelihood that the measurements y were observed given the unknown state x. the second term epx is the prior energy and it plays a role analogous to the smoothness energy in regularization. note that the map estimate may not always be desirable since it selects the peak in the posterior distribution rather than some more stable statistic such as mse see the discussion in appendix about loss functions and decision theory. markov random fields markov random fields kohli and rother are the most popular types of prior model for gridded image-like which include not only regular natural images but also two-dimensional fields such as optic flow or depth maps as well as binary fields such as segmentations as we discussed in section the prior probability px for a markov random field is a gibbs or boltzmann distribution whose negative log likelihood to the hammer alternative formulations include power spectra and non-local means coll and morel computer vision algorithms and applications draft figure graphical model for an neighborhood markov random field. the white circles are the unknowns fi j while the dark circles are the input data di j. the sxi j and syi j black boxes denote arbitrary interaction potentials between adjacent nodes in the random field and the wi j denote the data penalty functions. they are all examples of the general potentials vijklfi j fk l used in equation sley clifford theorem can be written as a sum of pairwise interaction potentials epx n vijklfi j fk l where n j denotes the neighbors of pixel j. in the more general case mrfs can also contain unary potentials as well as higher-order potentials defined over larger cardinality cliques and snell geman and geman bishop potetz and lee kohli kumar and torr kohli ladick y and torr rother kohli feng et al. alahari kohli and torr they can also contain line processes i.e. additional binary variables that mediate discontinuities between adjacent elements and geman black and rangarajan show how independent line process variables can be eliminated and incorporated into regular mrfs using robust pairwise penalty functions. the most commonly used neighborhood in markov random field modeling is the neighborhood where each pixel in the field fi j interacts only with its immediate neighbors figure shows such an mrf. the sxi j and syi j black boxes denote arbitrary interaction potentials between adjacent nodes in the random field and the wi j denote the elemental data penalty terms in ed these square nodes can also be interpreted as factors in a factor graph version of the undirected graphical model wainwright and jordan koller and friedman which is another name for interaction potentials. speaking the factors are improper probability functions whose product is the un-normalized posterior distribution. more complex and higher-dimensional interaction models and neighborhoods are also markov random fields possible. for example grids can be enhanced with the addition of diagonal connections neighborhood or even larger numbers of pairwise terms and kolmogorov rother kolmogorov lempitsky et al. grids can be used to compute globally optimal segmentations in volumetric medical images and funka-lea higher-order cliques can also be used to develop more sophisticated models and lee kohli ladick y and torr kohli kumar and torr one of the biggest challenges in using mrf models is to develop efficient inference algorithms that will find low-energy solutions boykov veksler and zabih kohli kumar over the years a large variety of such algorithms have been developed including simulated annealing graph cuts and loopy belief propagation. the choice of inference technique can greatly affect the overall performance of a vision system. for example most of the top-performing algorithms on the middlebury stereo evaluation page either use belief propagation or graph cuts. in the next few subsections we review some of the more widely used mrf inference techniques. more in-depth descriptions of most of these algorithms can be found in a recently published book on advances in mrf techniques kohli and rother experimental comparisons along with test datasets and reference software are provided by szeliski zabih scharstein et al. gradient descent and simulated annealing the simplest optimization technique is gradient descent which minimizes the energy by changing independent subsets of nodes to take on lower-energy configurations. such techniques go under a variety of names including contextual classification and f oglein and iterated conditional modes variables can either be updated sequentially e.g. in raster scan or in parallel e.g. using red black coloring on a checkerboard. chou and brown suggests using highest confidence first i.e. choosing variables based on how large a difference they make in reducing the energy. the problem with gradient descent is that it is prone to getting stuck in local minima which is almost always the case with mrf problems. one way around this is to use stochastic gradient descent or markov chain monte carlo rosenbluth rosenbluth et al. i.e. to randomly take occasional uphill steps in order to get out of such minima. one popular update rule is the gibbs sampler and geman rather than choosing the lowest energy state for a variable being updated it chooses the state with httpvision.middlebury.edumrf. the name comes from iteratively setting variables to the mode likely i.e. lowest energy state conditioned on its currently fixed neighbors. computer vision algorithms and applications draft probability px e ext where t is called the temperature and controls how likely the system is to choose a more random update. stochastic gradient descent is usually combined with simulated annealing gelatt and vecchi which starts at a relatively high temperature thereby randomly exploring a large part of the state space and gradually cools the temperature to find a good local minimum. during the late simulated annealing was the method of choice for solving mrf inference problems marroquin mitter and poggio barnard another variant on simulated annealing is the swendsen wang algorithm and wang barbu and zhu here instead of flipping single variables a connected subset of variables chosen using a random walk based on mrf connectively strengths is selected as the basic update unit. this can sometimes help make larger state changes and hence find better-quality solutions in less time. while simulated annealing has largely been superseded by the newer graph cuts and loopy belief propagation techniques it still occasionally finds use especially in highly connected and highly non-submodular graphs kolmogorov lempitsky et al. dynamic programming dynamic programming is an efficient inference procedure that works for any treestructured graphical model i.e. one that does not have any cycles. given such a tree pick any node as the root r and figuratively pick up the tree by its root. the depth or distance of all the other nodes from this root induces a partial ordering over the vertices from which a total ordering can be obtained by arbitrarily breaking ties. let us now lay out this graph as a tree with the root on the right and indices increasing from left to right as shown in figure before describing the dp algorithm let us re-write the potential function of equation in a more general but succinct form ex n vijxi xj vixi where instead of using pixel indices j and l we just use scalar index variables i and j. we also replace the function value fi j with the more succinct notation xi with the variables making up the state vector x. we can simplify this function even further by adding dummy nodes i for every node that has a non-zero vixi and setting vii xi vixi which lets us drop the vi terms from dynamic programming proceeds by computing partial sums in a left-to-right fashion i.e. in order of increasing variable index. let ck be the children of k i.e. i k k n markov random fields figure dynamic programming over a tree drawn as a factor graph. to compute the lowest energy solution ekxk at node xk conditioned on the best solutions to the left of this node we enumerate all possible values of eixi vikxi xk and pick the smallest one similarly for j. for higher-order cliques we need to try all combinations of xj in order to select the best possible configuration. the arrows show the basic flow of the computation. the lightly shaded factor vij in shows an additional connection that turns the tree into a cyclic graph for which exact inference cannot be efficiently computed. then define ekx j k vijxi xj xk as a partial sum of over all variables up to and including k i.e. over all parts of the graph shown in figure to the left of xk. this sum depends on the state of all the unknown variables in x with i k. it turns out that we can use a simple recursive formula now suppose we wish to find the setting for all variables i k that minimizes this sum. ekxk min ik ekx ck min xi xk to find this minimum. visually this is easy to understand. looking at figure associate an energy ekxk with each node k and each possible setting of its value xk that is based on the best possible setting of variables to the left of that node. it is easy to convince yourself that in this figure you only need to know eixi and ejxj in order to compute this value. once the flow of information in the tree has been processed from left to right the minimum value of erxr at the root gives the map solution for ex. the root node is set to the choice of xr that minimizes this function and other nodes are set in a backward chaining pass by selecting the values of child nodes i ck that were minimal in the original recursion xkvikxjxivjk.........xrvijxkvijkxjxi......... computer vision algorithms and applications draft dynamic programming is not restricted to trees with pairwise potentials. figure shows an example of a three-way potential vijkxi xj xk inside a tree. to compute the optimum value of ekxk the recursion formula in now has to evaluate the minimum over all combinations of possible state values leading into a factor node box. for this reason dynamic programming is normally exponential in complexity in the order of the clique size i.e. a clique of size n with l labels at each node requires the evaluation of ln possible states and lee kohli kumar and torr however for certain kinds of potential functions vikxi xk including the potts model function absolute values variation and quadratic mrf felzenszwalb and huttenlocher show how to reduce the complexity of the min-finding step from to ol. in appendix we also discuss how potetz and lee reduce the complexity for special kinds of higher-order clique i.e. linear summations followed by non-linearities. figure also shows what happens if we add an extra factor between nodes i and j. in this case the graph is no longer a tree i.e. it contains a cycle. it is no longer possible to use the recursion formula since eixi now appears in two different terms inside the summation i.e. as a child of both nodes j and k and the same setting for xi may not minimize both. in other words when loops exist there is no ordering of the variables that allows the recursion in to be well-founded. it is however possible to convert small loops into higher-order factors and to solve these as shown in figure however graphs with long loops or meshes result in extremely large clique sizes and hence an amount of computation potentially exponential in the size of the graph. belief propagation belief propagation is an inference technique originally developed for trees but more recently extended to loopy graphs such as mrfs and mackay freeman pasztor and carmichael yedidia freeman and weiss weiss and freeman yuille sun zheng and shum felzenszwalb and huttenlocher it is closely related to dynamic programming in that both techniques pass messages forward and backward over a tree or graph. in fact one of the two variants of belief propagation the max-product rule performs the exact same computation as dynamic programming albeit using probabilities instead of energies. recall that the energy we are minimizing in map estimation is the negative log likelihood and of a factored gibbs posterior distribution px n ijxi xj markov random fields where are the pairwise interaction potentials. we can rewrite as ijxi xj e vij where pkx j k ijxi xj ck pikx pikx ikxi xk pix. we can therefore rewrite as with pkxk max ik pkx ck pikx pikx max xi ikxi xk pix. equation is the max update rule evaluated at all square box factors in figure while is the product rule evaluated at the nodes. the probability distribution pikx is often interpreted as a message passing information about child i to parent k and is hence written as mikxk freeman and weiss or i kxk the max-product rule can be used to compute the map estimate in a tree using the same kind of forward and backward sweep as in dynamic programming is sometimes called the max-sum algorithm an alternative rule known as the sum product sums over all possible values in rather than taking the maximum in essence computing the expected distribution rather than the maximum likelihood distribution. this produces a set of probability estimates that can be used to compute the marginal distributions bixi px yedidia freeman and weiss bishop belief propagation may not produce optimal estimates for cyclic graphs for the same reason that dynamic programming fails to work i.e. because a node with multiple parents may take on different optimal values for each of the parents i.e. there is no unique elimination ordering. early algorithms for extending belief propagation to graphs with cycles dubbed loopy belief propagation performed the updates in parallel over the graph i.e. using synchronous updates and mackay freeman pasztor and carmichael yedidia freeman and weiss weiss and freeman yuille sun zheng and shum felzenszwalb and huttenlocher for example felzenszwalb and huttenlocher split an graph into its red and black components and alternate between sending messages from the red nodes to the black and vice versa. they also use multi-grid level updates to speed up the convergence. as discussed previously to reduce the complexity of the basic max-product computer vision algorithms and applications draft update rule from to ol they develop specialized update algorithms for several cost functions vikxi xk including the potts model function absolute values variation and quadratic mrf. a related algorithm mean field diffusion and szeliski also uses synchronous updates between nodes to compute marginal distributions. yuille discusses the relationships between mean field theory and loopy belief propagation. more recent loopy belief propagation algorithms and their variants use sequential scans through the graph zabih scharstein et al. for example tappen and freeman pass messages from left to right along each row and then reverse the direction once they reach the end. this is similar to treating each row as an independent tree except that messages from nodes above and below the row are also incorporated. they then perform similar computations along columns. these sequential updates allow the information to propagate much more quickly across the image than synchronous updates. the other belief propagation variant tested by szeliski zabih scharstein et al. which they call bp-s or trw-s is based on kolmogorov s sequential extension of the tree-reweighted message passing of wainwright jaakkola and willsky trw first selects a set of trees from the neighborhood graph and computes a set of probability distributions over each tree. these are then used to reweight the messages being passed during loopy belief propagation. the sequential version of trw called trw-s processed nodes in scan-line order with a forward and backward pass. in the forward pass each node sends messages to its right and bottom neighbors. in the backward pass messages are sent to the left and upper neighbors. trw-s also computes a lower bound on the energy which is used by szeliski zabih scharstein et al. to estimate how close to the best possible solution all of the mrf inference algorithms being evaluated get. as with dynamic programming belief propagation techniques also become less efficient as the order of each factor clique increases. potetz and lee shows how this complexity can be reduced back to linear in the clique order for continuous-valued problems where the factors involve linear summations followed by a non-linearity which is typical of more sophisticated mrf models such as fields of experts and black and steerable random fields and black kohli kumar and torr and alahari kohli and torr develop alternative ways for dealing with higher-order cliques in the context of graph cut algorithms. graph cuts the computer vision community has adopted graph cuts as an informal name to describe a large family of mrf inference algorithms based on solving one or more min-cut or maxflow problems veksler and zabih boykov and kolmogorov boykov markov random fields figure graph cuts for minimizing binary sub-modular mrf energies and jolly ieee energy function encoded as a max flow problem the minimum cut determines the region boundary. veksler and zabih ishikawa and veksler the simplest example of an mrf graph cut is the polynomial-time algorithm for performing exact minimization of a binary mrf originally developed by greig porteous and seheult and brought to the attention of the computer vision community by boykov veksler and zabih and boykov and jolly the basic construction of the min-cut graph from an mrf energy function is shown in figure and described in sections and in brief the nodes in an mrf are connected to special source and sink nodes and the minimum cut between these two nodes whose cost is exactly that of the mrf energy under a binary assignment of labels is computed using a polynomial-time max flow algorithm and tarjan boykov and kolmogorov as discussed in section important extensions of this basic algorithm have been made for the case of directed edges and boykov larger neighborhoods and kolmogorov kolmogorov and boykov connectivity priors kolmogorov and rother and shape priors and boykov lempitsky blake and rother kolmogorov and zabih formally characterize the class of binary energy potentials conditions for which these algorithms find the global minimum. komodakis tziritas and paragios and rother kolmogorov lempitsky et al. provide good algorithms for the cases when they do not. binary mrf problems can also be approximately solved by turning them into continuous problems solving them either as linear systems sinop and grady grady and alvino grady grady and ali singaraju grady and vidal couprie grady najman et al. random walker model or by computing geodesic objectterminal terminalbackground pqrwvstbackground object terminalterminalpqrwvstcut computer vision algorithms and applications draft distances and sapiro criminisi sharp and blake and then thresholding the results. more details on these techniques are provided in section and a nice review can be found in the work of singaraju grady sinop et al. a different connection to continuous segmentation techniques this time to the literature on level sets is made by boykov kolmogorov cremers et al. who develop an approach to solving surface propagation pdes based on combinatorial graph cut algorithms boykov and funkalea discuss this and related techniques. multi-valued mrf inference problems usually require solving a series of related binary mrf problems veksler and zabih although for special cases such as some convex functions a single graph cut may suffice schlesinger and flach the seminal work in this area is that of boykov veksler and zabih who introduced two algorithms called the swap move and the expansion move which are sketched in figure the move selects two labels by cycling through all possible pairings and then formulates a binary mrf problem that allows any pixels currently labeled as either or to optionally switch their values to the other label. the move allows any pixel in the mrf to take on the label or to keep its current identity. it is easy to see by inspection that both of these moves result in binary mrfs with well-defined energy functions. because these algorithms use a binary mrf optimization inside their inner loop they are subject to the constraints on the energy functions that occur in the binary labeling case and zabih however more recent algorithms such as those developed by komodakis tziritas and paragios and rother kolmogorov lempitsky et al. can be used to provide approximate solutions for more general energy functions. efficient algorithms for re-using previous solutions and cut-recycling have been developed for on-line applications such as dynamic mrfs and torr juan and boykov alahari kohli and torr and coarse-to-fine banded graph cuts zheng pal et al. lombaert sun grady et al. juan and boykov it is also now possible to minimize the number of labels used as part of the alpha-expansion process osokin isack et al. in experimental comparisons usually converge faster to a good solution than zabih scharstein et al. especially for problems that involve large regions of identical labels such as the labeling of source imagery in image stitching for truncated convex energy functions defined over ordinal values more accurate algorithms that consider complete ranges of labels inside each min-cut and often produce lower energies have been developed kumar and torr kumar veksler and torr the whole field of efficient mrf inference algorithms is rapidly developing as witnessed by a recent special journal issue and torr komodakis tziritas and paragios olsson eriksson and kahl potetz and lee articles markov random fields initial labeling standard move figure multi-level graph optimization from veksler and zabih ieee initial problem configuration the standard move changes only one pixel the optimally exchanges all and pixels the move optimally selects among current pixel values and the label. kohli and torr and a forthcoming book kohli and rother linear programming many successful algorithms for mrf optimization are based on the linear programming relaxation of the energy function yanover and meltzer for some practical mrf problems lp-based techniques can produce globally minimal solutions yanover and weiss even though mrf inference is in general np-hard. in order to describe this relaxation let us first rewrite the energy function as ex n subject to xi xj xi xij vixi vijxi xj vij xij j n xij j n vi and here and range over label values and xi and xij are indicator variables of assignments xi and xj respectively. the lp relaxation is obtained by replacing the discreteness constraints with linear constraints xij it is easy to show that the optimal value of is a lower bound on this section was contributed by vladimir kolmogorov. thanks! computer vision algorithms and applications draft this relaxation has been extensively studied in the literature starting with the work of schlesinger an important question is how to solve this lp efficiently. unfortunately general-purpose lp solvers cannot handle large problems in vision meltzer and weiss a large number of customized iterative techniques have been proposed. most of these solve the dual problem i.e. they formulate a lower bound on and then try to maximize this bound. the bound is often formulated using a convex combination of trees as proposed in jaakkola and willsky the lp lower bound can be maximized via a number of techniques such as max-sum diffusion tree-reweighted message passing jaakkola and willsky kolmogorov subgradient methods and giginyak komodakis paragios and tziritas and bregman projections agarwal and wainwright note that the max-sum diffusion and trw algorithms are not guaranteed to converge to a global maximum of lp they may get stuck at a suboptimal point werner however in practice this does not appear to be a problem for some vision applications algorithms based on relaxation produce excellent results. however this is not guaranteed in all cases after all the problem is np-hard. recently researchers have investigated alternative linear programming relaxations and jaakkola sontag meltzer globerson et al. komodakis and paragios schraudolph these algorithms are capable of producing tighter bounds compared to at the expense of additional computational cost. lp relaxation and alpha expansion. solving a linear program produces primal and dual solutions that satisfy complementary slackness conditions. in general the primal solution of does not have to be integer-valued so in practice we may have to round it to obtain a valid labeling x. an alternative proposed by komodakis and tziritas komodakis tziritas and paragios is to search for primal and dual solutions such that they satisfy approximate complementary slackness conditions and the primal solution is already integer-valued. several max-flow-based algorithms are proposed by and tziritas komodakis tziritas and paragios for this purpose and the fast-pd method tziritas and paragios is shown to perform best. in the case of metric interactions the default version of fast-pd produces the same primal solution as the alpha-expansion algorithm veksler and zabih this provides an interesting interpretation of the alpha expansion algorithm as trying to approximately solve relaxation unlike the standard alpha expansion algorithm fast-pd also maintains a dual solution and thus runs faster in practice. fast-pd can be extended to the case of semi-metric interactions tziritas and paragios the primal version of such extension was uncertainty estimation analysis also given by rother kumar kolmogorov et al. uncertainty estimation analysis in addition to computing the most likely estimate many applications require an estimate for the uncertainty in this the most general way to do this is to compute a complete probability distribution over all of the unknowns but this is generally intractable. the one special case where it is easy to obtain a simple description for this distribution is linear estimation problems with gaussian noise where the joint energy function log likelihood of the posterior estimate is a quadratic. in this case the posterior distribution is a multi-variate gaussian and the covariance can be computed directly from the inverse of the problem hessian. name for the inverse covariance matrix which is equal to the hessian in such simple cases is the information matrix. even here however the full covariance matrix may be too large to compute and store. for example in large structure from motion problems a large sparse hessian normally results in a full dense covariance matrix. in such cases it is often considered acceptable to report only the variance in the estimated quantities or simple covariance estimates on individual parameters such as point positions or camera pose estimates more insight into the problem e.g. the dominant modes of uncertainty can be obtained using eigenvalue analysis and kang for problems where the posterior energy is non-quadratic e.g. in non-linear or robustified least squares it is still often possible to obtain an estimate of the hessian in the vicinity of the optimal solution. in this case the cramer rao lower bound on the uncertainty can be computed as the inverse of the hessian. another way of saying this is that while the local hessian can underestimate how wide the energy function can be the covariance can never be smaller than the estimate based on this local quadratic approximation. it is also possible to estimate a different kind of uncertainty energies in general mrfs where the map inference is performed using graph cuts and torr while many computer vision applications ignore uncertainty modeling it is often useful to compute these estimates just to get an intuitive feeling for the reliability of the estimates. certain applications such as kalman filtering require the computation of this uncertainty explicitly as posterior covariances or implicitly as inverse covariances in order to optimally integrate new measurements with previously computed estimates. this is particularly true of classic photogrammetry applications where the reporting of precision is almost always considered mandatory orstner computer vision algorithms and applications draft appendix c supplementary material data sets software slides and lectures bibliography computer vision algorithms and applications draft in this final appendix i summarize some of the supplementary materials that may be useful to students instructors and researchers. the book s web site at httpszeliski.orgbook contains updated lists of datasets and software so please check there as well. data sets one of the keys to developing reliable vision algorithms is to test your procedures on challenging and representative data sets. when ground truth or other people s results are available such test can be even more informative quantitative. over the years a large number of datasets have been developed for testing and evaluating computer vision algorithms. a number of these datasets software are indexed on the computer vision some newer web sites such as cvonline inf.ed.ac.ukrbfcvonline visionbib.com and computer vision online have more recent pointers. below i list some of the more popular data sets grouped by the book chapters to which they most closely correspond chapter image formation curet columbia-utrecht reflectance and texture database educavesoftwarecuret van ginneken nayar et al. middlebury color datasets registered color images taken by different cameras to study how they transform gamuts and colors httpvision.middlebury.educolordata scharstein and zickler chapter image processing middlebury test datasets for evaluating mrf minimizationinference algorithms http zabih scharstein et al. chapter feature detection and matching affine covariant features database for evaluating feature detector and descriptor matching quality and repeatability httpwww.robots.ox.ac.uk vggresearchaffine and schmid mikolajczyk tuytelaars schmid et al. database of matched image patches for learning and feature descriptor evaluation httpcvlab.epfl.ch brownpatchdatapatchdata.html and brown hua brown and winder httpwww.cs.cmu.edu cilvision.html although it has not been maintained since data sets chapter segmentation berkeley segmentation dataset and benchmark of images labeled by humans along with an evaluation httpwww.eecs.berkeley.eduresearchprojectscsvision groupingsegbench fowlkes tal et al. weizmann segmentation evaluation database of grayscale images with ground truth segmentations httpwww.wisdom.weizmann.ac.il visionseg evaluation db index.html galun basri et al. chapter dense motion estimation the middlebury optic flow evaluation web site httpvision.middlebury.eduflowdata scharstein lewis et al. the human-assisted motion annotation database httppeople.csail.mit.educeliumotionannotation freeman adelson et al. chapter computational photography high dynamic range radiance maps httpwww.debevec.orgresearchhdr and malik alpha matting evaluation web site httpalphamatting.com rother wang et al. chapter stereo correspondence middlebury stereo datasets and evaluation httpvision.middlebury.edustereo and szeliski stereo classification and performance evaluation of different aggregation costs for stereo matching httpwww.vision.deis.unibo.itspespehome.aspx mattoccia di stefano et al. middlebury multi-view stereo datasets httpvision.middlebury.edumviewdata curless diebel et al. multi-view and oxford colleges building reconstructions httpwww.robots.ox.ac.uk vggdatadata-mview.html. multi-view stereo datasets httpcvlab.epfl.chdatastrechamvs fransens and van gool computer vision algorithms and applications draft multi-view evaluation httpcvlab.epfl.ch strechamultiview von hansen van gool et al. chapter reconstruction humaneva synchronized video and motion capture dataset for evaluation of articulated human motion httpvision.cs.brown.eduhumaneva balan and black chapter image-based rendering the stanford light field archive httplightfield.stanford.edu joshi vaish et al. virtual viewpoint video multi-viewpoint video with per-frame depth maps http kang uyttendaele et al. chapter recognition for a list of visual recognition datasets see tables in addition to those there are also buffy pose classes httpwww.robots.ox.ac.uk vggdatabuffy pose classes and buffy stickmen httpwww.robots.ox.ac.uk vggdatastickmenindex.html marinjimenez and zisserman eichner and ferrari database of posejoint annotated photographs of humans httpwww.eecs.berkeley. edu and malik action recognition datasets httpwww.cs.berkeley.eduprojectsvisionaction has pointers to several datasets for action and activity recognition as well as some papers. the human action database at httpwww.nada.kth.secvapactions contains more action sequences. software one of the best sources for computer vision algorithms is the open source computer vision library which was developed by gary bradski and his colleagues at intel and is now being maintained and extended at willow garage and kaehler a partial list of the available functions taken from httpopencv.willowgarage.comdocumentationcpp includes software image processing and transforms morphology pyramids geometric image transformations resizing miscellaneous image transformations transforms distance transforms histograms segmentation mean shift feature detection harris hough mser surf motion analysis and object tracking kanade mean shift camera calibration and reconstruction machine learning nearest neighbors support vector machines decision trees boost ing random trees expectation-maximization and neural networks. the intel performance primitives library httpsoftware.intel.comen-usintel-ipp contains highly optimized code for a variety of image processing tasks. many of the routines in opencv take advantage of this library if it is installed to run even faster. in terms of functionality it has many of the same operators as those found in opencv plus additional libraries for image and video compression signal and speech processing and matrix algebra. the matlab image processing toolbox httpwww.mathworks.comproductsimage contains routines for spatial transformations resizing normalized cross-correlation image analysis and statistics hough transform image enhancement histogram equalization median filtering and restoration linear filtering image transforms and dct and morphological operations components and distance transforms. two older libraries which no longer appear to be under active development but contain many useful routines are vxl libraries for computer vision research and implementation httpvxl.sourceforge.net and lti-lib homepage. photo editing and viewing packages such as windows live photo gallery iphoto picasa gimp and irfanview can be useful for performing common processing tasks converting formats and viewing your results. they can also serve as interesting reference implementations for image processing algorithms as tone correction or denoising that you are trying to develop from scratch. there are also software packages and infrastructure that can be helpful for building realtime video processing demos. vision on tap provides a web computer vision algorithms and applications draft service that will process your webcam video in real time and raskar videoman httpvideomanlib.sourceforge.net can be useful for getting real-time video-based demos and applications running. you can also use imread in matlab to read directly from any url such as a webcam. below i list some additional software that can be found on the web grouped by the book chapters to which they most correspond chapter image processing matlabpyrtools matlab source code for laplacian pyramids qmfwavelets and steerable pyramids httpwww.cns.nyu.edu lcvsoftware.php and adelson simoncelli freeman adelson et al. bls-gsm image denoising httpdecsai.ugr.es javierdenoise strela wainwright et al. fast bilateral filtering code httppeople.csail.mit.edujiawencode paris and durand c implementation of the fast distance transform algorithm httppeople.cs.uchicago. edu pffdt and huttenlocher greyc s magic image converter including image restoration software using regularization and anisotropic diffusion httpgmic.sourceforge.netgimp.shtml e and deriche chapter feature detection and matching vlfeat an open and portable library of computer vision algorithms httpvlfeat.org and fulkerson siftgpu a gpu implementation of scale invariant feature transform http ccwusiftgpu surf speeded up robust features httpwww.vision.ee.ethz.ch surf tuytelaars and van gool fast corner detection httpmi.eng.cam.ac.uk and drummond linux binaries for affine region detectors and descriptors as well as matlab files to compute repeatability and matching scores httpwww.robots.ox.ac.uk vggresearch affine. software kanade lucas tomasi feature trackers klt httpwww.ces.clemson.edu stbklt and tomasi gpu-klt httpcs.unc.edu cmzachopensource.html gallup and frahm and lucas kanade years on httpwww.ri.cmu.edu projectsproject and matthews chapter segmentation efficient graph-based image segmentation httppeople.cs.uchicago.edu pffsegment and huttenlocher edison edge detection and image segmentation httpcoewww.rutgers.eduriulresearch codeedison and georgescu comaniciu and meer normalized cuts segmentation including intervening contours httpwww.cis.upenn. edu jshisoftware and malik malik belongie leung et al. segmentation by weighted aggregation httpwww.cs.weizmann.ac.il vision swa galun basri et al. chapter feature-based alignment and calibration non-iterative pnp algorithm httpcvlab.epfl.chsoftwareepnp lepetit and fua tsai camera calibration software rgwtsaicode.html easy camera calibration toolkit httpresearch.microsoft.comen-usumpeoplezhang calib camera calibration toolbox for matlab httpwww.vision.caltech.edubouguetj calib doc a c version is included in opencv. matlab functions for multiple view geometry httpwww.robots.ox.ac.uk vgghzbook code and zisserman chapter structure from motion sba a generic sparse bundle adjustment cc package based on the levenberg marquardt algorithm httpwww.ics.forth.gr lourakissba and argyros simple sparse bundle adjustment httpcs.unc.edu cmzachopensource.html. computer vision algorithms and applications draft bundler structure from motion for unordered image collections httpphototour.cs. washington.edubundler seitz and szeliski chapter dense motion estimation optical flow software httpwww.cs.brown.edu blackcode.html and anandan optical flow using total variation and conjugate gradient descent httppeople.csail. mit.educeliuopticalflow optical flow on the gpu httpcs.unc.edu cmzachopensource.html pock and bischof elastix a toolbox for rigid and nonrigid registration of images httpelastix.isi.uu.nl staring and pluim deformable image registration using discrete optimization httpwww.mrf-registration. netdeformableindex.html komodakis tziritas et al. chapter image stitching microsoft research image compositing editor for stitching images httpresearch. microsoft.comen-usumredmondgroupsivmice. chapter computational photography hdrshop software for combining bracketed exposures into high-dynamic range radiance images httpprojects.ict.usc.edugraphicshdrshop. super-resolution code httpwww.robots.ox.ac.uk vggsoftwaresr pickup capel roberts et al. chapter stereo correspondence stereomatcher standalone c stereo matching code httpvision.middlebury.edu stereocode and szeliski patch-based multi-view stereo software version httpgrail.cs.washington. edusoftwarepmvs and ponce chapter reconstruction scanalyze a system for aligning and merging range data httpgraphics.stanford.edu softwarescanalyze and levoy software meshlab software for processing editing and visualizing unstructured triangular meshes httpmeshlab.sourceforge.net. vrml viewers are also a good way to visualize texture-mapped models. section whole body modeling and tracking bayesian person tracking httpwww.cs.brown.edu blackcode.html black and fleet sidenbladh and black humaneva baseline code for the tracking of articulated human motion httpvision. cs.brown.eduhumaneva balan and black section face detection sample face detection code and evaluation tools httpvision.ai.uiuc.edumhyangface-detection-survey.html. section pedestrian detection a simple object detector with boosting httppeople.csail.mit.edutorralbashortcourserloc boostingboosting.html tibshirani and friedman torralba murphy and freeman discriminatively trained deformable part models httppeople.cs.uchicago.edu pff latent girshick mcallester et al. upper-body detector httpwww.robots.ox.ac.uk vggsoftwareupperbody marin-jimenez and zisserman articulated human pose estimation software httpwww.vision.ee.ethz.ch calvin articulated human pose estimation code and ferrari section active appearance and shape models aamtools an active appearance modeling toolbox httpcvsp.cs.ntua.grsoftware aamtools and maragos section instance recognition fastann and fastcluster for approximate k-means httpwww.robots. ox.ac.uk vggsoftware chum isard et al. feature matching using fast approximate nearest neighbors httppeople.cs.ubc.ca mariusmindex.phpflannflann and lowe computer vision algorithms and applications draft section bag of words two bag of words classifiers and perona sivic russell efros et al. bag of features and hierarchical k-means httpwww.vlfeat.org er and stew enius nowak jurie and triggs section part-based models a simple parts and structure object detector partsstructure.html and elschlager felzenszwalb and huttenlocher section machine learning software support vector machines software svm soft.html has pointers to lots of svm libraries including svmlight http svmlight.joachims.org libsvm httpwww.csie.ntu.edu.tw cjlinlibsvm chen and lin and liblinear httpwww.csie.ntu.edu.tw cjlinliblinear chang hsieh et al. kernel machines learning algorithms httpwww.kernel-machines.orgsoftware. links to svm gaussian processes boosting and other machine multiple kernels for image classification httpwww.robots.ox.ac.uk vggsoftware mkl and ray vedaldi gulshan varma et al. appendix matrix decompositions and linear least blas linear algebra subprograms httpwww.netlib.orgblas demmel dongarra et al. lapack algebra package httpwww.netlib.orglapack bai bischof et al. gotoblas httpwww.tacc.utexas.edutacc-projects. atlas tuned linear algebra software httpmath-atlas.sourceforge. net dongarra eijkhout et al. intel math kernel library httpsoftware.intel.comen-usintel-mkl. amd core math library httpdeveloper.amd.comcpulibrariesacmlpages default.aspx. thanks to sameer agarwal for suggesting and describing most of these sites. software robust pca code httpwww.salle.url.edu la torre and black appendix non-linear least squares minpack httpwww.netlib.orgminpack. levmar levenberg marquardt nonlinear least squares algorithms httpwww.ics.forth. gr lourakislevmar nielsen and tingleff appendix direct and iterative sparse matrix solvers suitesparse reordering algorithms cholmod and suitesparse qr http pardiso and sparse direct solution httpwww.pardiso-project.org. taucs direct iterative out of core preconditioners httpwww.tau.ac.il stoledotaucs. hsl mathematical software library httpwww.hsl.rl.ac.ukindex.html. templates for the solution of linear systems httpwww.netlib.orglinalghtml templates templates.html berry chan et al. download the pdf for instructions on how to get the software. itsol miqr and other sparse solvers httpwww-users.cs.umn.edu saadsoftware ilupack httpwww-public.tu-bs.de bolleilupack. appendix b bayesian modeling and inference middlebury source code for mrf minimization httpvision.middlebury.edumrf code zabih scharstein et al. c code for efficient belief propagation for early vision httppeople.cs.uchicago. edu pffbp and huttenlocher fastpd mrf optimization code httpwww.csd.uoc.gr komodfastpd and tziritas komodakis tziritas and paragios computer vision algorithms and applications draft double urand return rand rand max void granddouble double m pi m pi m pi double urand double urand double guard against double log double angl m pi cosangl sinangl algorithm c algorithm for gaussian random noise generation using the box muller transform. gaussian noise generation. a lot of basic software packages come with a uniform random noise generator the rand routine in unix but not all have a gaussian random noise generator. to compute a normally distributed random variable you can use the box muller transform and muller whose c code is given in algorithm note that this routine returns pairs of random variables. alternative methods for generating gaussian random numbers are given by thomas luk leong et al. pseudocolor generation. in many applications it is convenient to be able to visualize the set of labels assigned to an image to image features such as lines. one of the easiest ways to do this is to assign a unique color to each integer label. in my work i have found it convenient to distribute these labels in a quasi-uniform fashion around the rgb color cube using the following idea. for each label value consider the bits as being split among the three color channels e.g. for a nine-bit value the bits could be labeled rgbrgbrgb. after collecting each of the three color values reverse the bits so that the low-order bits vary the most quickly. slides and lectures in practice for eight-bit color channels this bit reverse can be stored in a table or a complete table mapping from labels to pseudocolors with entries can be pre-computed. figure shows an example of such a pseudo-color mapping. gpu implementation the advent of programmable gpus with capabilities such as pixel shaders and compute shaders has led to the development of fast computer vision algorithms for real-time applications such as segmentation tracking stereo and motion estimation unger cremers et al. vineet and narayanan zach gallup and frahm a good source for learning about such algorithms is the cvpr workshop on visual computer vision on gpus httpwww.cs.unc.edu jmfworkshop on computer vision on gpu. html whose papers can be found on the cvpr proceedings dvd. additional sources for gpu algorithms include the gpgpu web site and workshops httpgpgpu.org and the openvidia web site httpopenvidia.sourceforge.netindex.phpopenvidia. slides and lectures as i mentioned in the preface i hope to post slides corresponding to the material in the book. until these are ready your best bet is to look at the slides from the courses i have co-taught at the university of washington as well as related courses that have used a similar syllabus. here is a partial list of such courses uw undergraduate computer vision httpwww.cs.washington.edueducation uw graduate computer vision httpwww.cs.washington.edueducationcourses stanford introduction to computer vision httpvision.stanford.eduteaching mit advances in computer vision httppeople.csail.mit.edutorralbacourses berkeley cs computer vision httpwww.eecs.berkeley.edu unc comp computer vision httpwww.cs.unc.edu middlebury cs computer vision httpwww.cs.middlebury.edu scharcourses computer vision algorithms and applications draft related courses have also been taught on the topic of computational photography e.g. cmu computational photography mit advanced computational photography httpstellar.mit.eduscourse stanford cs computational photography on cell phones httpgraphics.stanford. siggraph courses on computational photography httpweb.media.mit.edu raskar photo. there is also an excellent set of on-line lectures available on a range of computer vision topics such as belief propagation and graph cuts at the uw-msr course of vision algorithms bibliography while a bibliography file for all of the references cited in this book is available on the book s web site a much more comprehensive partially annotated bibliography of nearly all computer vision publications is maintained by keith price at httpiris.usc.edu vision-notesbibliographycontents.html. there is also a searchable computer graphics bibliography at httpwww.siggraph.orgpublicationsbibliography. additional good sources for technical papers are google scholar and citeseerx. references abdel-hakim a. e. and farag a. a. csift a sift descriptor with color invariant characterstics. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. adelson e. h. and bergen j. the plenoptic function and the elements of early vision. in computational models of visual processing pp. adelson e. h. simoncelli e. and hingorani r. orthogonal pyramid transforms for image coding. in spie vol. visual communications and image processing ii pp. cambridge massachusetts. adiv g. noisy flow field. inherent ambiguities in recovering motion and structure from a ieee transactions on pattern analysis and machine intelligence agarwal a. and triggs b. recovering human pose from monocular images. ieee transactions on pattern analysis and machine intelligence agarwal s. and roth d. learning a sparse representation for object detection. in seventh european conference on computer vision pp. copenhagen. agarwal s. snavely n. seitz s. m. and szeliski r. bundle adjustment in the large. in eleventh european conference on computer vision heraklion crete. agarwal s. snavely n. simon i. seitz s. m. and szeliski r. building rome in a day. in twelfth ieee international conference on computer vision kyoto japan. agarwal s. furukawa y. snavely n. curless b. seitz s. m. and szeliski r. reconstructing rome. computer agarwala a. efficient gradient-domain compositing using quadtrees. acm trans actions on graphics computer vision algorithms and applications draft agarwala a. hertzmann a. seitz s. and salesin d. keyframe-based tracking for rotoscoping and animation. acm transactions on graphics siggraph agarwala a. agrawala m. cohen m. salesin d. and szeliski r. photographing long scenes with multi-viewpoint panoramas. acm transactions on graphics siggraph agarwala a. dontcheva m. agrawala m. drucker s. colburn a. curless b. salesin d. h. and cohen m. f. interactive digital photomontage. acm transactions on graphics siggraph agarwala a. zheng k. c. pal c. agrawala m. cohen m. curless b. salesin d. and szeliski r. panoramic video textures. acm transactions on graphics siggraph aggarwal j. k. and nandhakumar n. on the computation of motion from se quences of images a review. proceedings of the ieee agin g. j. and binford t. o. computer description of curved objects. ieee transactions on computers ahonen t. hadid a. and pietik ainen m. face description with local binary patterns application to face recognition. ieee transactions on pattern analysis and machine intelligence akenine-m oller t. and haines e. real-time rendering. a k peters wellesley massachusetts second edition. al-baali m. and fletcher. r. an efficient line search for nonlinear least squares. journal journal of optimization theory and applications alahari k. kohli p. and torr p. dynamic hybrid algorithms for discrete map mrf inference. ieee transactions on pattern analysis and machine intelligence. alexa m. behr j. cohen-or d. fleishman s. levin d. and silva c. t. computing and rendering point set surfaces. ieee transactions on visualization and computer graphics aliaga d. g. funkhouser t. yanovsky d. and carlbom i. sea of images. ieee computer graphics and applications allen b. curless b. and popovi c z. the space of human body shapes reconstruction and parameterization from range scans. acm transactions on graphics siggraph allgower e. l. and georg k. introduction to numerical continuation methods. society for industrial and applied mathematics. references aloimonos j. perspective approximations. image and vision computing alpert s. galun m. basri r. and brandt a. image segmentation by probabilistic bottom-up aggregation and cue integration. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. amini a. a. weymouth t. e. and jain r. c. using dynamic programming for solving variational problems in vision. ieee transactions on pattern analysis and machine intelligence anandan p. computing dense displacement fields with confidence measures in in image understanding workshop pp. new scenes containing occlusion. orleans. anandan p. a computational framework and an algorithm for the measurement of visual motion. international journal of computer vision anandan p. and irani m. factorization with uncertainty. international journal of computer vision anderson e. bai z. bischof c. blackford s. demmel j. w. et al. lapack users guide. society for industrial and applied mathematics edition. andrieu c. de freitas n. doucet a. and jordan m. i. an introduction to mcmc for machine learning. machine learning andriluka m. roth s. and schiele b. people-tracking-by-detection and peopledetection-by-tracking. in ieee computer society conference on computer vision and pattern recognition anchorage ak. andriluka m. roth s. and schiele b. detection and articulated pose estimation. computer vision and pattern recognition miami beach fl. pictorial structures revisited people in ieee computer society conference on andriluka m. roth s. and schiele b. ocular pose estimation and tracking by detection. in ieee computer society conference on computer vision and pattern recognition san francisco ca. anguelov d. srinivasan p. koller d. thrun s. rodgers j. and davis j. scape shape completion and animation of people. acm transactions on graphics siggraph ansar a. castano a. and matthies l. enhanced real-time stereo using bilateral filtering. in international symposium on data processing visualization and transmission computer vision algorithms and applications draft antone m. and teller s. scalable extrinsic calibration of omni-directional image networks. international journal of computer vision arbel aez p. maire m. fowlkes c. and malik j. contour detection and hierarchical image segmentation. technical report eecs department university of california berkeley. submitted to pami. argyriou v. and vlachos t. estimation of sub-pixel motion using gradient cross correlation. electronic letters arikan o. and forsyth d. a. interactive motion generation from examples. acm transactions on graphics arnold r. d. automated stereo perception. technical report artificial intelligence laboratory stanford university. arya s. mount d. m. netanyahu n. s. silverman r. and wu a. y. an optimal algorithm for approximate nearest neighbor searching in fixed dimensions. journal of the acm ashdown i. near-field photometry a new approach. journal of the illuminating engineering society atkinson k. b. close range photogrammetry and machine vision. whittles publishing scotland uk. aurich v. and weule j. non-linear gaussian filters performing edge preserving diffusion. in dagm-symposium pp. bielefeld. avidan s. support vector tracking. in ieee computer society conference on computer vision and pattern recognition pp. kauai hawaii. avidan s. baker s. and shan y. special issue on internet vision. proceedings of the ieee axelsson o. iterative solution methods. cambridge university press cambridge. ayache n. vision st er eoscopique et perception multisensorielle. intereditions paris. azarbayejani a. and pentland a. p. recursive estimation of motion structure ieee transactions on pattern analysis and machine intelligence and focal length. azuma r. t. baillot y. behringer r. feiner s. k. julier s. and macintyre b. ieee computer graphics and applications recent advances in augmented reality. references bab-hadiashar a. and suter d. robust optic flow computation. international journal of computer vision bab-hadiashar a. and suter d. robust total least squares based optic flow computation. in asian conference on computer vision pp. hong kong. badra f. qumsieh a. and dudek g. rotation and zooming in image mosaicin ieee workshop on applications of computer vision pp. ing. princeton. bae s. paris s. and durand f. two-scale tone management for photographic look. acm transactions on graphics baeza-yates r. and ribeiro-neto b. modern information retrieval. addison wesley. bai x. and sapiro g. geodesic matting a framework for fast interactive iminternational journal of computer vision age and video segmentation and matting. bajcsy r. and kovacic s. multiresolution elastic matching. computer vision graphics and image processing baker h. h. three-dimensional modeling. in fifth international joint conference on artificial intelligence pp. baker h. h. depth from edge and intensity based stereo. technical report aim artificial intelligence laboratory stanford university. baker h. h. building surfaces of evolution the weaving wall. international journal of computer vision baker h. h. and binford t. o. depth from edge and intensity based stereo. in pp. baker h. h. and bolles r. c. generalizing epipolar-plane image analysis on the spatiotemporal surface. international journal of computer vision baker s. and kanade t. limits on super-resolution and how to break them. ieee transactions on pattern analysis and machine intelligence baker s. and matthews i. lucas-kanade years on a unifying framework part the quantity approximated the warp update rule and the gradient descent approximation. international journal of computer vision baker s. and nayar s. a theory of single-viewpoint catadioptric image formation. international journal of computer vision computer vision algorithms and applications draft baker s. and nayar s. k. single viewpoint catadioptric cameras. in benosman r. and kang s. b. panoramic vision sensors theory and applications pp. springer new york. baker s. gross r. and matthews i. lucas-kanade years on a unifying framework part technical report the robotics institute carnegie mellon university. baker s. gross r. and matthews i. lucas-kanade years on a unifying framework part technical report the robotics institute carnegie mellon university. baker s. szeliski r. and anandan p. a layered approach to stereo reconstruction. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. baker s. gross r. ishikawa t. and matthews i. lucas-kanade years on a unifying framework part technical report the robotics institute carnegie mellon university. baker s. black m. lewis j. p. roth s. scharstein d. and szeliski r. a database and evaluation methodology for optical flow. in eleventh international conference on computer vision rio de janeiro brazil. baker s. scharstein d. lewis j. roth s. black m. j. and szeliski r. a database and evaluation methodology for optical flow. technical report microsoft research. ballard d. h. generalizing the hough transform to detect arbitrary patterns. pattern recognition ballard d. h. and brown c. m. computer vision. prentice-hall englewood cliffs new jersey. banno a. masuda t. oishi t. and ikeuchi k. flying laser range sensor for large-scale site-modeling and its applications in bayon digital archival project. international journal of computer vision bar-hillel a. hertz t. and weinshall d. object class recognition by boosting a part based model. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. bar-joseph z. el-yaniv r. lischinski d. and werman m. texture mixing and texture movie synthesis using statistical learning. ieee transactions on visualization and computer graphics references bar-shalom y. and fortmann t. e. tracking and data association. academic press boston. barash d. a fundamental relationship between bilateral filtering adaptive smoothing and the nonlinear diffusion equation. ieee transactions on pattern analysis and machine intelligence barash d. and comaniciu d. a common framework for nonlinear diffusion image and vision computing adaptive smoothing bilateral filtering and mean shift. barbu a. and zhu s.-c. graph partition by swendsen wang cuts. in ninth international conference on computer vision pp. nice france. barbu a. and zhu s.-c. generalizing swendsen wang to sampling arbitrary posterior probabilities. ieee transactions on pattern analysis and machine intelligence barkans a. c. high quality rendering using the talisman architecture. in pro ceedings of the eurographics workshop on graphics hardware. barnard s. t. stochastic stereo matching over scale. international journal of computer vision barnard s. t. and fischler m. a. computational stereo. computing surveys barnes c. jacobs d. e. sanders j. goldman d. b. rusinkiewicz s. finkelstein a. and agrawala m. video puppetry a performative interface for cutout animation. acm transactions on graphics barreto j. p. and daniilidis k. fundamental matrix for cameras with radial distortion. in tenth international conference on computer vision pp. beijing china. barrett r. berry m. chan t. f. demmel j. donato j. et al. templates for the solution of linear systems building blocks for iterative methods edition. siam philadelphia pa. barron j. l. fleet d. j. and beauchemin s. s. performance of optical flow techniques. international journal of computer vision barrow h. g. and tenenbaum j. m. computational vision. proceedings of the ieee bartels r. h. beatty j. c. and barsky b. a. an introduction to splines for use in computer graphics and geeometric modeling. morgan kaufmann publishers los altos. computer vision algorithms and applications draft bartoli a. towards gauge invariant bundle adjustment a solution based on gauge in ninth international conference on computer vision dependent damping. pp. nice france. bartoli a. and sturm p. multiple-view structure and motion from line correspondences. in ninth international conference on computer vision pp. nice france. bartoli a. coquerelle m. and sturm p. a framework for pencil-of-points in eighth european conference on computer vision structure-from-motion. pp. prague. bascle b. blake a. and zisserman a. motion deblurring and superresolution from an image sequence. in fourth european conference on computer vision pp. cambridge england. bathe k.-j. finite element procedures. prentice-hall inc. englewood cliffs new jersey. batra d. sukthankar r. and chen t. learning class-specific affinities for image labelling. in ieee computer society conference on computer vision and pattern recognition anchorage ak. baudisch p. tan d. steedly d. rudolph e. uyttendaele m. pal c. and szeliski r. an exploration of user interface designs for real-time panoramic photography. australian journal of information systems baumberg a. reliable feature matching across widely separated views. in ieee computer society conference on computer vision and pattern recognition pp. hilton head island. baumberg a. m. and hogg d. c. generating spatiotemporal models from exam ples. image and vision computing baumgart b. g. geometric modeling for computer vision. technical re port artificial intelligence laboratory stanford university. bay h. ferrari v. and van gool l. wide-baseline stereo matching with line segments. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. bay h. tuytelaars t. and van gool l. surf speeded up robust features. in ninth european conference on computer vision pp. bayer b. e. color imaging array. us patent no. references beardsley p. torr p. and zisserman a. model acquisition from extended in fourth european conference on computer vision image sequences. pp. cambridge england. beare r. a locally constrained watershed transform. ieee transactions on pat tern analysis and machine intelligence becker s. and bove v. m. semiautomatic model extraction from uncalibrated camera views. in spie vol. visual data exploration and analysis ii pp. san jose. beier t. and neely s. feature-based image metamorphosis. computer graphics beis j. s. and lowe d. g. indexing without invariants in object recognition. ieee transactions on pattern analysis and machine intelligence belhumeur p. n. a bayesian approach to binocular stereopsis. international journal of computer vision belhumeur p. n. hespanha j. p. and kriegman d. j. eigenfaces vs. fisherfaces recognition using class specific linear projection. ieee transactions on pattern analysis and machine intelligence belongie s. and malik j. finding boundaries in natural images a new method using point descriptors and area completion. in fifth european conference on computer vision pp. freiburg germany. belongie s. malik j. and puzicha j. shape matching and object recognition using shape contexts. ieee transactions on pattern analysis and machine intelligence belongie s. fowlkes c. chung f. and malik j. indefinite kernels using the nystr om extension. computer vision pp. copenhagen. spectral partitioning with in seventh european conference on bennett e. uyttendaele m. zitnick l. szeliski r. and kang s. b. and image bayesian demosaicing with a two color image prior. conference on computer vision pp. graz. video in ninth european benosman r. and kang s. b. applications springer new york. panoramic vision sensors theory and berg t. internet vision. suny stony brook course cse httpwww. tamaraberg.comteachingfall computer vision algorithms and applications draft berg t. and forsyth d. animals on the web. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. bergen j. r. anandan p. hanna k. j. and hingorani r. hierarchical model-based motion estimation. in second european conference on computer vision pp. santa margherita liguere italy. bergen j. r. burt p. j. hingorani r. and peleg s. a three-frame algorithm for estimating two-component image motion. ieee transactions on pattern analysis and machine intelligence berger j. o. statistical decision theory and bayesian analysis. springer new york second edition. bertalmio m. sapiro g. caselles v. and ballester c. acm siggraph conference proceedings pp. image inpainting. in bertalmio m. vese l. sapiro g. and osher s. simultaneous structure and texture image inpainting. ieee transactions on image processing bertero m. poggio t. a. and torre v. ill-posed problems in early vision. pro ceedings of the ieee besag j. on the statistical analysis of dirty pictures. journal of the royal statisti cal society b besl p. active optical range imaging sensors. in sanz j. l. advances in machine vision chapter pp. springer-verlag. besl p. j. and jain r. c. surveys three-dimensional object recognition. computing besl p. j. and mckay n. d. a method for registration of shapes. ieee transactions on pattern analysis and machine intelligence betrisey c. blinn j. f. dresevic b. hill b. hitchcock g. et al. displaced filtering for patterned displays. in society for information display symposium pp. beymer d. feature correspondence by interleaving shape and texture computations. in ieee computer society conference on computer vision and pattern recognition pp. san francisco. bhat d. n. and nayar s. k. ordinal measures for image correspondence. ieee transactions on pattern analysis and machine intelligence bickel b. botsch m. angst r. matusik w. otaduy m. pfister h. and gross m. multi-scale capture of facial geometry and motion. acm transactions on graphics references billinghurst m. kato h. and poupyrev i. the magicbook a transitional ar interface. computers graphics bimber o. computational photography the next big step. computer birchfield s. and tomasi c. a pixel dissimilarity measure that is insensitive to image sampling. ieee transactions on pattern analysis and machine intelligence birchfield s. and tomasi c. depth discontinuities by pixel-to-pixel stereo. inter national journal of computer vision birchfield s. t. natarajan b. and tomasi c. correspondence as energy-based segmentation. image and vision computing bishop c. m. pattern recognition and machine learning. springer new york ny. bitouk d. kumar n. dhillon s. belhumeur p. and nayar s. k. face swapping automatically replacing faces in photographs. acm transactions on graphics bj orck a. numerical methods for least squares problems. society for industrial and applied mathematics. bj orck a. and dahlquist g. numerical methods in scientific computing. vol ume ii society for industrial and applied mathematics. black m. yacoob y. jepson a. d. and fleet d. j. learning parameterized models of image motion. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. black m. j. and anandan p. the robust estimation of multiple motions parametric and piecewise-smooth flow fields. computer vision and image understanding black m. j. and jepson a. d. estimating optical flow in segmented images using variable-order parametric models with local deformations. ieee transactions on pattern analysis and machine intelligence black m. j. and jepson a. d. eigentracking robust matching and tracking of articulated objects using a view-based representation. international journal of computer vision black m. j. and rangarajan a. on the unification of line processes outlier rejection and robust statistics with applications in early vision. international journal of computer vision computer vision algorithms and applications draft black m. j. sapiro g. marimont d. h. and heeger d. robust anisotropic diffusion. ieee transactions on image processing blackford l. s. demmel j. dongarra j. duff i. hammarling s. et al. an updated set of basic linear algebra subprograms acm transactions on mathematical software blake a. and isard m. active contours the application of techniques from graphics vision control theory and statistics to visual tracking of shapes in motion. springer verlag london. blake a. and zisserman a. visual reconstruction. mit press cambridge massachusetts. blake a. curwen r. and zisserman a. a framework for spatio-temporal control in the tracking of visual contour. international journal of computer vision blake a. kohli p. and rother c. advances in markov random fields mit press. blake a. zimmerman a. and knowles g. surface descriptions from stereo and shading. image and vision computing blake a. rother c. brown m. perez p. and torr p. interactive image segmentation using an adaptive gmmrf model. in eighth european conference on computer vision pp. prague. blanz v. and vetter t. a morphable model for the synthesis of faces. in acm siggraph conference proceedings pp. blanz v. and vetter t. face recognition based on fitting a morphable model. ieee transactions on pattern analysis and machine intelligence bleyer m. gelautz m. rother c. and rhemann c. a stereo approach that handles the matting problem via image warping. in ieee computer society conference on computer vision and pattern recognition miami beach fl. blinn j. dirty pixels. morgan kaufmann publishers san francisco. blinn j. f. jim blinn s corner compositing part theory. ieee computer graphics and applications blinn j. f. jim blinn s corner compositing part practice. ieee computer graphics and applications blinn j. f. and newell m. e. texture and reflection in computer generated images. communications of the acm references blostein d. and ahuja n. shape from texture integrating texture-element extraction and surface estimation. ieee transactions on pattern analysis and machine intelligence bobick a. f. movement activity and action the role of knowledge in the percep tion of motion. proceedings of the royal society of london b bobick a. f. and intille s. s. large occlusion stereo. international journal of computer vision boden m. a. mind as machine a history of cognitive science. oxford univer sity press oxford england. bogart r. g. view correlation. in arvo j. graphics gems ii pp. academic press boston. boiman o. shechtman e. and irani m. in defense of nearest-neighbor based image classification. in ieee computer society conference on computer vision and pattern recognition anchorage ak. boissonat j.-d. representing and shapes with the delaunay triangulation. in seventh international conference on pattern recognition pp. montreal canada. bolles r. c. baker h. h. and hannah m. j. the jisct stereo evaluation. in image understanding workshop pp. bolles r. c. baker h. h. and marimont d. h. epipolar-plane image analysis an approach to determining structure from motion. international journal of computer vision bookstein f. l. of deformations. principal warps thin-plate splines and the decomposition ieee transactions on pattern analysis and machine intelligence borenstein e. and ullman s. combined top-downbottom-up segmentation. ieee transactions on pattern analysis and machine intelligence borgefors g. distance transformations in digital images. computer vision graphics and image processing bouchard g. and triggs b. hierarchical part-based visual object categorization. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. bougnoux s. from projective to euclidean space under any practical situation a criticism of self-calibration. in sixth international conference on computer vision pp. bombay. computer vision algorithms and applications draft bouguet j.-y. and perona p. photography using shadows in dual-space geom etry. international journal of computer vision boult t. e. and kender j. r. visual surface reconstruction using sparse depth data. in ieee computer society conference on computer vision and pattern recognition pp. miami beach. bourdev l. and malik j. poselets body part detectors trained using human pose annotations. in twelfth international conference on computer vision kyoto japan. bovik a. handbook of image and video processing academic press san diego. bowyer k. w. kranenburg c. and dougherty s. edge detector evaluation using empirical roc curves. computer vision and image understanding box g. e. p. and muller m. e. a note on the generation of random normal deviates. annals of mathematical statistics boyer e. and berger m. o. surface reconstruction using occluding contours. international journal of computer vision boykov y. and funka-lea g. graph cuts and efficient n-d image segmentation. international journal of computer vision boykov y. and jolly m.-p. interactive graph cuts for optimal boundary and rein eighth international conference on gion segmentation of objects in n-d images. computer vision pp. vancouver canada. boykov y. and kolmogorov v. computing geodesics and minimal surfaces via graph cuts. in ninth international conference on computer vision pp. nice france. boykov y. and kolmogorov v. an experimental comparison of min-cutmax-flow algorithms for energy minimization in vision. ieee transactions on pattern analysis and machine intelligence boykov y. and kolmogorov v. basic graph cut algorithms. in blake a. kohli p. and rother c. advances in markov random fields mit press. boykov y. veksler o. and zabih r. a variable window approach to early vision. ieee transactions on pattern analysis and machine intelligence boykov y. veksler o. and zabih r. fast approximate energy minimization ieee transactions on pattern analysis and machine intelligence via graph cuts. references boykov y. veksler o. and zabih r. optimizing multi-label mrfs by move making algorithms. in blake a. kohli p. and rother c. advances in markov random fields mit press. boykov y. kolmogorov v. cremers d. and delong a. an integral solution to surface evolution pdes via geo-cuts. in ninth european conference on computer vision pp. bracewell r. n. the fourier transform and its applications. mcgraw-hill new york edition. bradley d. boubekeur t. and heidrich w. accurate multi-view reconstruction using robust binocular stereo and surface meshing. in ieee computer society conference on computer vision and pattern recognition anchorage ak. bradsky g. and kaehler a. learning opencv computer vision with the opencv library. o reilly sebastopol ca. brandt a. algebraic multigrid theory the symmetric case. applied mathematics and computation bregler c. and malik j. tracking people with twists and exponential maps. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. bregler c. covell m. and slaney m. video rewrite driving visual speech with audio. in acm siggraph conference proceedings pp. bregler c. malik j. and pullen k. twist based acquisition and tracking of animal and human kinematics. international journal of computer vision breu h. gil j. kirkpatrick d. and werman m. linear time euclidean distance transform algorithms. ieee transactions on pattern analysis and machine intelligence brice c. r. and fennema c. l. scene analysis using regions. artificial intelli gence briggs w. l. henson v. e. and mccormick s. f. a multigrid tutorial. society for industrial and applied mathematics philadelphia second edition. brillaut-o mahoney b. new method for vanishing point detection. computer vision graphics and image processing brinkmann r. the art and science of digital compositing. morgan kaufmann publishers san francisco edition. computer vision algorithms and applications draft brooks r. a. symbolic reasoning among models and images. artificial intelligence brown d. c. close-range camera calibration. photogrammetric engineering brown l. g. a survey of image registration techniques. computing surveys brown m. and lowe d. invariant features from interest point groups. in british machine vision conference pp. cardiff wales. brown m. and lowe d. unsupervised object recognition and reconstruction in unordered datasets. in international conference on imaging and modelling pp. nice france. brown m. and lowe d. automatic panoramic image stitching using invariant features. international journal of computer vision brown m. hartley r. and nist er d. minimal solutions for panoramic stitching. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. brown m. szeliski r. and winder s. multi-image matching using multi-scale oriented patches. technical report microsoft research. brown m. szeliski r. and winder s. multi-image matching using multi-scale oriented patches. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. brown m. z. burschka d. and hager g. d. advances in computational stereo. ieee transactions on pattern analysis and machine intelligence brox t. bregler c. and malik j. large displacement optical flow. in ieee computer society conference on computer vision and pattern recognition miami beach fl. brox t. bruhn a. papenberg n. and weickert j. high accuracy optical flow estimation based on a theory for warping. in eighth european conference on computer vision pp. prague. brubaker s. c. wu j. sun j. mullin m. d. and rehg j. m. on the design of cascades of boosted ensembles for face detection. international journal of computer vision bruhn a. weickert j. and schn orr c. combining local and global optic flow methods. vision lucaskanade meets hornschunck international journal of computer references bruhn a. weickert j. kohlberger t. and schn orr c. a multigrid platform for real-time motion computation with discontinuity-preserving variational methods. international journal of computer vision buades a. coll b. and morel j.-m. nonlocal image and movie denoising. international journal of computer vision b alan a. o. and black m. j. the naked truth estimating body shape under clothing. in tenth european conference on computer vision pp. marseilles. buchanan a. and fitzgibbon a. damped newton algorithms for matrix factorization with missing data. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. buck i. finkelstein a. jacobs c. klein a. salesin d. h. seims j. szeliski r. and toyama k. performance-driven hand-drawn animation. in symposium on non photorealistic animation and rendering pp. annecy. buehler c. bosse m. mcmillan l. gortler s. j. and cohen m. f. unstructured in acm siggraph conference proceedings pp. lumigraph rendering. bugayevskiy l. m. and snyder j. p. map projections a reference manual. crc press. burger w. and burge m. j. digital image processing an algorithmic introduc tion using java. springer new york ny. burl m. c. weber m. and perona p. a probabilistic approach to object recognition using local photometry and global geometry. in fifth european conference on computer vision pp. freiburg germany. burns j. b. hanson a. r. and riseman e. m. extracting straight lines. ieee transactions on pattern analysis and machine intelligence burns p. d. and williams d. using slanted edge analysis for color registration measurement. in ist pics conference pp. burt p. j. and adelson e. h. the laplacian pyramid as a compact image code. ieee transactions on communications burt p. j. and adelson e. h. a multiresolution spline with applications to image mosaics. acm transactions on graphics burt p. j. and kolczynski r. j. in fourth international conference on computer vision pp. berlin germany. enhanced image capture through fusion. computer vision algorithms and applications draft byr od m. and astr om k. bundle adjustment using conjugate gradients with multiscale preconditioning. in british machine vision conference cai d. he x. hu y. han j. and huang t. learning a spatially smooth in ieee computer society conference on computer subspace for face recognition. vision and pattern recognition minneapolis mn. campbell n. d. f. vogiatzis g. hern andez c. and cipolla r. using multiple hypotheses to improve depth-maps for multi-view stereo. in tenth european conference on computer vision pp. marseilles. can a. stewart c. roysam b. and tanenbaum h. a feature-based robust hierarchical algorithm for registering pairs of images of the curved human retina. ieee transactions on pattern analysis and machine intelligence canny j. a computational approach to edge detection. ieee transactions on pattern analysis and machine intelligence cao z. yin q. tang x. and sun j. face recognition with learning-based in ieee computer society conference on computer vision and pattern descriptor. recognition san francisco ca. capel d. image mosaicing and super-resolution. distinguished dissertation series british computer society springer-verlag. capel d. and zisserman a. automated mosaicing with super-resolution zoom. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. capel d. and zisserman a. super-resolution enhancement of text image sequences. in fifteenth international conference on pattern recognition pp. barcelona spain. capel d. and zisserman a. computer vision applied to super resolution. ieee signal processing magazine capel d. p. super-resolution and image mosaicing. ph.d. thesis university of oxford. caprile b. and torre v. using vanishing points for camera calibration. interna tional journal of computer vision carneiro g. and jepson a. the distinctiveness detectability and robustness of local image features. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. carneiro g. and lowe d. sparse flexible models of local features. in ninth european conference on computer vision pp. references carnevali p. coletti l. and patarnello s. image processing by simulated anneal ing. ibm journal of research and development carranza j. theobalt c. magnor m. a. and seidel h.-p. free-viewpoint video of human actors. acm transactions on graphics siggraph carroll r. agrawala m. and agarwala a. optimizing content-preserving pro jections for wide-angle images. acm transactions on graphics caselles v. kimmel r. and sapiro g. geodesic active contours. international journal of computer vision catmull e. and smith a. r. transformations of images in scanline order. computer graphics celniker g. and gossard d. deformable curve and surface finite-elements for free-form shape design. computer graphics chakrabarti a. scharstein d. and zickler t. an empirical camera model for in british machine vision conference london internet color vision. uk. cham t. j. and cipolla r. a statistical framework for long-range feature matching in uncalibrated image mosaicing. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. cham t.-j. and rehg j. m. a multiple hypothesis approach to figure tracking. in ieee computer society conference on computer vision and pattern recognition pp. fort collins. champleboux g. lavall ee s. sautot p. and cinquin p. accurate calibration of cameras and range imaging sensors the npbs method. in ieee international conference on robotics and automation pp. nice france. champleboux g. lavall ee s. szeliski r. and brunie l. from accurate range imaging sensor calibration to accurate model-based object localization. in ieee computer society conference on computer vision and pattern recognition pp. champaign illinois. chan a. b. and vasconcelos n. layered dynamic textures. ieee transactions on pattern analysis and machine intelligence chan t. f. and vese l. a. active contours without edges. ieee transactions on image processing chan t. f. osher s. and shen j. the digital tv filter and nonlinear denoising. ieee transactions on image processing computer vision algorithms and applications draft chang m. m. tekalp a. m. and sezan m. i. simultaneous motion estimation and segmentation. ieee transactions on image processing chaudhuri s. super-resolution imaging. springer. chaudhuri s. and rajagopalan a. n. depth from defocus a real aperture imaging approach. springer. cheeseman p. kanefsky b. hanson r. and stutz j. super-resolved surface reconstruction from multiple images. technical report nasa ames research center artificial intelligence branch. chellappa r. wilson c. and sirohey s. human and machine recognition of faces a survey. proceedings of the ieee chen b. neubert b. ofek e. deussen o. and cohen m. f. integrated videos and maps for driving directions. in uist proceedings of the annual acm symposium on user interface software and technology pp. victoria bc canada new york ny usa. chen c.-y. and klette r. image stitching comparisons and new techniques. in computer analysis of images and patterns pp. ljubljana. chen j. and chen b. architectural modeling from sparsely scanned range data. international journal of computer vision chen j. paris s. and durand f. real-time edge-aware image processing with the bilateral grid. acm transactions on graphics chen s. and williams l. view interpolation for image synthesis. in acm sig graph conference proceedings pp. chen s. e. quicktime vr an image-based approach to virtual environment nav igation. in acm siggraph conference proceedings pp. los angeles. chen y. and medioni g. object modeling by registration of multiple range im ages. image and vision computing cheng l. vishwanathan s. v. n. and zhang x. consistent image analogies using semi-supervised learning. in ieee computer society conference on computer vision and pattern recognition anchorage ak. cheng y. mean shift mode seeking and clustering. ieee transactions on pattern analysis and machine intelligence chiang m.-c. and boult t. e. efficient image warping and super-resolution. in ieee workshop on applications of computer vision pp. sarasota. references chiu k. and raskar r. computer vision on tap. in second ieee workshop on internet vision miami beach florida. chou p. b. and brown c. m. the theory and practice of bayesian image labeling. international journal of computer vision christensen g. joshi s. and miller m. volumetric transformation of brain anatomy. ieee transactions on medical imaging christy s. and horaud r. euclidean shape and motion from multiple perspecieee transactions on pattern analysis and machine tive views by affine iterations. intelligence chuang y.-y. curless b. salesin d. h. and szeliski r. a bayesian approach to digital matting. in ieee computer society conference on computer vision and pattern recognition pp. kauai hawaii. chuang y.-y. agarwala a. curless b. salesin d. h. and szeliski r. video matting of complex scenes. acm transactions on graphics siggraph chuang y.-y. goldman d. b. curless b. salesin d. h. and szeliski r. shadow matting. acm transactions on graphics siggraph chuang y.-y. goldman d. b. zheng k. c. curless b. salesin d. h. and szeliski r. animating pictures with stochastic motion textures. acm transactions on graphics siggraph chuang y.-y. zongker d. hindorff j. curless b. salesin d. h. and szeliski r. environment matting extensions towards higher accuracy and real-time capture. in acm siggraph conference proceedings pp. new orleans. chui c. k. wavelet analysis and its applications. academic press new york. chum o. and matas j. matching with prosac progressive sample consensus. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. chum o. and matas j. large-scale discovery of spatially related images. ieee transactions on pattern analysis and machine intelligence chum o. philbin j. and zisserman a. near duplicate image detection minhash and tf-idf weighting. in british machine vision conference leeds england. computer vision algorithms and applications draft chum o. philbin j. sivic j. isard m. and zisserman a. total recall automatic query expansion with a generative feature model for object retrieval. in eleventh international conference on computer vision rio de janeiro brazil. cipolla r. and blake a. the dynamic analysis of apparent contours. in third international conference on computer vision pp. osaka japan. cipolla r. and blake a. surface shape from the deformation of apparent contours. international journal of computer vision cipolla r. and giblin p. visual motion of curves and surfaces. cambridge university press cambridge. cipolla r. drummond t. and robertson d. p. camera calibration from vanishing points in images of architectural scenes. in british machine vision conference claus d. and fitzgibbon a. a rational function lens distortion model for general cameras. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. clowes m. b. on seeing things. artificial intelligence cohen l. d. and cohen i. finite-element methods for active contour models and balloons for and images. ieee transactions on pattern analysis and machine intelligence cohen m. and wallace j. radiosity and realistic image synthesis. morgan kaufmann. cohen m. f. and szeliski r. the moment camera. computer collins r. t. a space-sweep approach to true multi-image matching. in ieee computer society conference on computer vision and pattern recognition pp. san francisco. collins r. t. and liu y. on-line selection of discriminative tracking features. in ninth international conference on computer vision pp. nice france. collins r. t. and weiss r. s. vanishing point calculation as a statistical inference on the unit sphere. in third international conference on computer vision pp. osaka japan. comaniciu d. and meer p. mean shift a robust approach toward feature space analysis. ieee transactions on pattern analysis and machine intelligence references comaniciu d. and meer p. an algorithm for data-driven bandwidth selection. ieee transactions on pattern analysis and machine intelligence conn a. r. gould n. i. m. and toint p. l. trust-region methods. society for industrial and applied mathematics philadephia. cook r. l. and torrance k. e. a reflectance model for computer graphics. acm transactions on graphics coorg s. and teller s. spherical mosaics with quaternions and dense correlation. international journal of computer vision cootes t. edwards g. j. and taylor c. j. active appearance models. ieee transactions on pattern analysis and machine intelligence cootes t. cooper d. taylor c. and graham j. active shape models their training and application. computer vision and image understanding cootes t. taylor c. lanitis a. cooper d. and graham j. building and using flexible models incorporating grey-level information. in fourth international conference on computer vision pp. berlin germany. cootes t. f. and taylor c. j. statistical models of appearance for medical image analysis and computer vision. in medical imaging. coquillart s. extended free-form deformations a sculpturing tool for geo metric modeling. computer graphics cormen t. h. introduction to algorithms. mit press cambridge massachusetts. cornelis n. leibe b. cornelis k. and van gool l. urban scene modeling integrating recognition and reconstruction. international journal of computer vision corso j. and hager g. coherent regions for concise and stable image description. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. costeira j. and kanade t. a multi-body factorization method for motion analysis. in fifth international conference on computer vision pp. cambridge massachusetts. costen n. cootes t. f. edwards g. j. and taylor c. j. simultaneous extraction of functional face subspaces. in ieee computer society conference on computer vision and pattern recognition pp. fort collins. couprie c. grady l. najman l. and talbot h. power watersheds a new image segmentation framework extending graph cuts random walker and optimal spanning computer vision algorithms and applications draft forest. in twelfth international conference on computer vision kyoto japan. cour t. b en ezit f. and shi j. spectral segmentation with multiscale graph decomposition. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. cox d. little j. and o shea d. ideals varieties and algorithms an introduc tion to computational algebraic geometry and commutative algebra. springer. cox i. j. a maximum likelihood n-camera stereo algorithm. in ieee computer society conference on computer vision and pattern recognition pp. seattle. cox i. j. roy s. and hingorani s. l. dynamic histogram warping of image pairs for constant image brightness. in ieee international conference on image processing pp. cox i. j. hingorani s. l. rao s. b. and maggs b. m. a maximum likelihood stereo algorithm. computer vision and image understanding crandall d. and huttenlocher d. composite models of objects and scenes for category recognition. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. crandall d. felzenszwalb p. and huttenlocher d. spatial priors for part-based recognition using statistical models. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. crandall d. backstrom l. huttenlocher d. and kleinberg j. mapping the world s photos. in int. world wide web conference pp. madrid. crandall d. j. and huttenlocher d. p. weakly supervised learning of part-based spatial models for visual object recognition. in ninth european conference on computer vision pp. crane r. a simplified approach to image processing. prentice hall upper saddle river nj. craswell n. and szummer m. random walks on the click graph. in acm sigir conference on research and development in informaion retrieval pp. new york ny. cremers d. and soatto s. motion competition a variational framework for piecewise parametric motion segmentation. international journal of computer vision references cremers d. rousson m. and deriche r. a review of statistical approaches to level set segmentation integrating color texture motion and shape. international journal of computer vision crevier d. ai the tumultuous search for artificial intelligence. basicbooks new york ny. criminisi a. p erez p. and toyama k. region filling and object removal by exemplar-based inpainting. ieee transactions on image processing criminisi a. reid i. and zisserman a. single view metrology. international journal of computer vision criminisi a. sharp t. and blake a. geos geodesic image segmentation. in tenth european conference on computer vision pp. marseilles. criminisi a. cross g. blake a. and kolmogorov v. bilayer segmentation of live video. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. criminisi a. shotton j. blake a. and torr p. gaze manipulation for one-to-one teleconferencing. in ninth international conference on computer vision pp. nice france. criminisi a. kang s. b. swaminathan r. szeliski r. and anandan p. extracting layers and analyzing their specular properties using epipolar-plane-image analysis. computer vision and image understanding criminisi a. shotton j. blake a. rother c. and torr p. h. s. efficient dense international journal of stereo with occlusion by four-state dynamic programming. computer vision crow f. c. summed-area table for texture mapping. computer graphics graph crowley j. l. and stern r. m. fast computation of the difference of low-pass transform. ieee transactions on pattern analysis and machine intelligence csurka g. and perronnin f. a simple high performance approach to semantic segmentation. in british machine vision conference leeds. csurka g. dance c. r. perronnin f. and willamowski j. generic visual categorization using weak geometry. in ponce j. hebert m. schmid c. and zisserman a. toward category-level object recognition pp. springer new york. computer vision algorithms and applications draft csurka g. dance c. r. fan l. willamowski j. and bray c. visual categorization with bags of keypoints. in eccv international workshop on statistical learning in computer vision prague. cui j. yang q. wen f. wu q. zhang c. van gool l. and tang x. transductive object cutout. in ieee computer society conference on computer vision and pattern recognition anchorage ak. curless b. from range scans to models. computer graphics curless b. and levoy m. better optical triangulation through spacetime analysis. in fifth international conference on computer vision pp. cambridge massachusetts. curless b. and levoy m. a volumetric method for building complex models from range images. in acm siggraph conference proceedings pp. new orleans. cutler r. and davis l. s. robust real-time periodic motion detection analysis ieee transactions on pattern analysis and machine intelligence and applications. cutler r. and turk m. view-based interpretation of real-time optical flow for gesture recognition. in ieee international conference on automatic face and gesture recognition pp. nara japan. dai s. baker s. and kang s. b. an mrf-based deinterlacing algorithm with exemplar-based refinement. ieee transactions on image processing dalal n. and triggs b. histograms of oriented gradients for human detection. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. dalal n. triggs b. and schmid c. human detection using oriented histograms in ninth european conference on computer vision of flow and appearance. pp. dana k. j. van ginneken b. nayar s. k. and koenderink j. j. reflectance and texture of real world surfaces. acm transactions on graphics danielsson p. e. euclidean distance mapping. computer graphics and image processing darrell t. and pentland a. robust estimation of a multi-layered motion represen tation. in ieee workshop on visual motion pp. princeton new jersey. darrell t. and pentland a. cooperative robust estimation using layers of support. ieee transactions on pattern analysis and machine intelligence references darrell t. and simoncelli e. nulling filters and the separation of transparent motion. in ieee computer society conference on computer vision and pattern recognition pp. new york. darrell t. gordon g. harville m. and woodfill j. integrated person tracking using stereo color and pattern detection. international journal of computer vision darrell t. baker h. crow f. gordon g. and woodfill j. mirror face-sensitive distortion and exaggeration. proceedings los angeles. magic morphin in acm siggraph visual datta r. joshi d. li j. and wang j. z. image retrieval ideas influences and trends of the new age. acm computing surveys daugman j. how iris recognition works. ieee transactions on circuits and systems for video technology david p. dementhon d. duraiswami r. and samet h. softposit simultaneous pose and correspondence determination. international journal of computer vision davies r. twining c. and taylor c. statistical models of shape. springer verlag london. davis j. mosaics of scenes with moving objects. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. davis j. ramamoorthi r. and rusinkiewicz s. spacetime stereo a unifying in ieee computer society conference on framework for depth from triangulation. computer vision and pattern recognition pp. madison wi. davis j. nahab d. ramamoorthi r. and rusinkiewicz s. spacetime stereo a unifying framework for depth from triangulation. ieee transactions on pattern analysis and machine intelligence davis l. a survey of edge detection techniques. computer graphics and image processing davis t. a. direct methods for sparse linear systems. siam. davis t. a. multifrontal multithreaded rank-revealing sparse qr factorization. acm trans. on mathematical software davison a. reid i. molton n. d. and stasse o. monoslam real-time single camera slam. ieee transactions on pattern analysis and machine intelligence computer vision algorithms and applications draft de agapito l. hayman e. and reid i. self-calibration of rotating and zooming cameras. international journal of computer vision de berg m. cheong o. van kreveld m. and overmars m. computational geometry algorithms and applications. springer new york ny third edition. de bonet j. multiresolution sampling procedure for analysis and synthesis of texture images. in acm siggraph conference proceedings pp. los angeles. de bonet j. s. and viola p. poxels probabilistic voxelized volume reconstruction. in seventh international conference on computer vision pp. kerkyra greece. de castro e. and morandi c. registration of translated and rotated images using finite fourier transforms. ieee transactions on pattern analysis and machine intelligence de haan g. and bellers e. b. deinterlacing an overview. proceedings of the ieee de la torre f. and black m. j. a framework for robust subspace learning. inter national journal of computer vision debevec p. rendering synthetic objects into real scenes bridging traditional and image-based graphics with global illumination and high dynamic range photography. in acm siggraph conference proceedings pp. debevec p. virtual cinematography relighting through computation. computer debevec p. hawkins t. tchou c. duiker h.-p. sarokin w. and sagar m. acquiring the reflectance field of a human face. in acm siggraph conference proceedings pp. debevec p. wenger a. tchou c. gardner a. waese j. and hawkins t. a lighting reproduction approach to live-action compositing. acm transactions on graphics siggraph debevec p. e. image-based modeling and lighting. computer graphics debevec p. e. and malik j. recovering high dynamic range radiance maps from photographs. in acm siggraph conference proceedings pp. debevec p. e. taylor c. j. and malik j. modeling and rendering architecture from photographs a hybrid geometry- and image-based approach. in acm siggraph conference proceedings pp. new orleans. references debevec p. e. yu y. and borshukov g. d. efficient view-dependent image-based rendering with projective texture-mapping. in eurographics rendering workshop pp. decarlo d. and santella a. stylization and abstraction of photographs. acm transactions on graphics siggraph decarlo d. metaxas d. and stone m. an anthropometric face model using variational techniques. in acm siggraph conference proceedings pp. delingette h. hebert m. and ikeuichi k. shape representation and image seg mentation using deformable surfaces. image and vision computing dellaert f. and collins r. fast image-based tracking by selective pixel integration. in iccv workshop on frame-rate vision pp. delong a. osokin a. isack h. n. and boykov y. fast approximate energy in ieee computer society conference on computer minimization with label costs. vision and pattern recognition san francisco ca. dementhon d. i. and davis l. s. model-based object pose in lines of code. international journal of computer vision demmel j. dongarra j. eijkhout v. fuentes e. petitet a. et al. self-adapting linear algebra algorithms and software. proceedings of the ieee dempster a. laird n. m. and rubin d. b. maximum likelihood from incomplete data via the em algorithm. journal of the royal statistical society b deng j. dong w. socher r. li l.-j. li k. and fei-fei l. imagenet a largescale hierarchical image database. in ieee computer society conference on computer vision and pattern recognition miami beach fl. deriche r. using canny s criteria to derive a recursively implemented optimal edge detector. international journal of computer vision deriche r. fast algorithms for low-level vision. ieee transactions on pattern analysis and machine intelligence deutscher j. and reid i. articulated body motion capture by stochastic search. international journal of computer vision deutscher j. blake a. and reid i. articulated body motion capture by annealed particle filtering. in ieee computer society conference on computer vision and pattern recognition pp. hilton head island. dev p. segmentation processes in visual perception a cooperative neural model. coins technical report university of massachusetts at amherst. computer vision algorithms and applications draft dhond u. r. and aggarwal j. k. structure from stereo a review. ieee trans actions on systems man and cybernetics dick a. torr p. h. s. and cipolla r. modelling and interpretation of architecture from several images. international journal of computer vision dickinson s. leonardis a. schiele b. and tarr m. j. object categorization computer and human vision perspectives cambridge university press new york. dickmanns e. d. and graefe v. dynamic monocular machine vision. machine vision and applications diebel j. representing attitude euler angles quaternions and rotation vectors. technical report stanford university. httpai.stanford.edu diebelattitude.html. diebel j. r. thrun s. and br unig m. a bayesian method for probable surface reconstruction and decimation. acm transactions on graphics dimitrijevic m. lepetit v. and fua p. human body pose detection using bayesian spatio-temporal templates. computer vision and image understanding dinh h. q. turk g. and slabaugh g. reconstructing surfaces by volumetric regularization using radial basis functions. ieee transactions on pattern analysis and machine intelligence divvala s. hoiem d. hays j. efros a. a. and hebert m. an empirical study in ieee computer society conference on computer of context in object detection. vision and pattern recognition miami fl. dodgson n. a. image resampling. technical report wolfson college and computer laboratory university of cambridge. dollar p. belongie s. and perona p. the fastest pedestrian detector in the west. in british machine vision conference aberystwyth wales uk. dollar p. wojek c. schiele b. and perona p. pedestrian detection a benchmark. in ieee computer society conference on computer vision and pattern recognition miami beach fl. doretto g. and soatto s. dynamic shape and appearance models. ieee transac tions on pattern analysis and machine intelligence doretto g. chiuso a. wu y. n. and soatto s. dynamic textures. international journal of computer vision references dork o g. and schmid c. selection of scale-invariant parts for object class recognition. in ninth international conference on computer vision pp. nice france. dorsey j. rushmeier h. and sillion f. digital modeling of material appear ance. morgan kaufmann san francisco. douglas d. h. and peucker t. k. algorithms for the reduction of the number of points required to represent a digitized line or its caricature. the canadian cartographer drori i. cohen-or d. and yeshurun h. fragment-based image completion. acm transactions on graphics siggraph duda r. o. and hart p. e. use of the hough transform to detect lines and curves in pictures. communications of the acm duda r. o. hart p. e. and stork d. g. pattern classification. john wiley sons new york edition. dupuis p. and oliensis j. an optimal control formulation and related numerical methods for a problem in shape reconstruction. annals of applied probability durand f. and dorsey j. fast bilateral filtering for the display of high-dynamicrange images. acm transactions on graphics siggraph durand f. and szeliski r. computational photography. ieee computer graphics and applications guest editors introduction to special issue. durbin r. and willshaw d. an analogue approach to the traveling salesman problem using an elastic net method. nature durbin r. szeliski r. and yuille a. an analysis of the elastic net approach to the travelling salesman problem. neural computation eck m. derose t. duchamp t. hoppe h. lounsbery m. and stuetzle w. in acm siggraph conference multiresolution analysis of arbitrary meshes. proceedings pp. los angeles. eden a. uyttendaele m. and szeliski r. seamless image stitching of scenes with large motions and exposure differences. in ieee computer society conference on computer vision and pattern recognition pp. new york ny. efros a. a. and freeman w. t. image quilting for texture synthesis and transfer. in acm siggraph conference proceedings pp. computer vision algorithms and applications draft efros a. a. and leung t. k. texture synthesis by non-parametric sampling. in seventh international conference on computer vision pp. kerkyra greece. efros a. a. berg a. c. mori g. and malik j. recognizing action at a distance. in ninth international conference on computer vision pp. nice france. eichner m. and ferrari v. better appearance models for pictorial structures. in british machine vision conference eisemann e. and durand f. flash photography enhancement via intrinsic relight ing. acm transactions on graphics eisert p. steinbach e. and girod b. automatic reconstruction of stationary objects from multiple uncalibrated camera views. ieee transactions on circuits and systems for video technology eisert p. wiegand t. and girod b. model-aided coding a new approach to incorporate facial animation into motion-compensated video coding. ieee transactions on circuits and systems for video technology ekman p. and friesen w. v. facial action coding system a technique for the measurement of facial movement. consulting psychologists press palo alto ca. el-melegy m. and farag a. nonmetric lens distortion calibration closed-form solutions robust estimation and model selection. in ninth international conference on computer vision pp. nice france. elder j. h. are edges incomplete? international journal of computer vision elder j. h. and goldberg r. m. image editing in the contour domain. ieee transactions on pattern analysis and machine intelligence elder j. h. and zucker s. w. local scale control for edge detection and blur estimation. ieee transactions on pattern analysis and machine intelligence engels c. stew enius h. and nist er d. bundle adjustment rules. in photogram metric computer vision bonn germany. engl h. w. hanke m. and neubauer a. regularization of inverse problems. kluwer academic publishers dordrecht. enqvist o. josephson k. and kahl f. optimal correspondences from pairwise in twelfth international conference on computer vision constraints. kyoto japan. references estrada f. j. and jepson a. d. benchmarking image segmentation algorithms. international journal of computer vision estrada f. j. jepson a. d. and chennubhotla c. spectral embedding and min-cut for image segmentation. in british machine vision conference pp. london. evangelidis g. d. and psarakis e. z. parametric image alignment using enhanced correlation coefficient maximization. ieee transactions on pattern analysis and machine intelligence everingham m. van gool l. williams c. k. i. winn j. and zisserman a. the pascal visual object classes challenge results. httpwww. everingham m. van gool l. williams c. k. i. winn j. and zisserman a. the international journal of computer pascal visual object classes challenge. vision ezzat t. geiger g. and poggio t. trainable videorealistic speech animation. acm transactions on graphics siggraph fabbri r. costa l. d. f. torelli j. c. and bruno o. m. euclidean distance transform algorithms a comparative survey. acm computing surveys fairchild m. d. color appearance models. wiley edition. fan r.-e. chen p.-h. and lin c.-j. working set selection using second order information for training support vector machines. journal of machine learning research fan r.-e. chang k.-w. hsieh c.-j. wang x.-r. and lin c.-j. liblinear a library for large linear classification. journal of machine learning research farbman z. fattal r. lischinski d. and szeliski r. edge-preserving decompositions for multi-scale tone and detail manipulation. acm transactions on graphics siggraph farenzena m. fusiello a. and gherardi r. structure-and-motion pipeline on a hierarchical cluster tree. in ieee international workshop on digital imaging and modeling kyoto japan. farin g. from conics to nurbs a tutorial and survey. ieee computer graphics and applications farin g. e. curves and surfaces for computer aided geometric design a prac tical guide. academic press boston massachusetts edition. computer vision algorithms and applications draft fattal r. image upsampling via imposed edge statistics. acm transactions on graphics fattal r. edge-avoiding wavelets and their applications. acm transactions on graphics fattal r. lischinski d. and werman m. gradient domain high dynamic range compression. acm transactions on graphics siggraph faugeras o. three-dimensional computer vision a geometric viewpoint. mit press cambridge massachusetts. faugeras o. and keriven r. variational principles surface evolution pdes level set methods and the stereo problem. ieee transactions on image processing faugeras o. and luong q.-t. the geometry of multiple images. mit press cambridge ma. faugeras o. d. what can be seen in three dimensions with an uncalibrated stereo rig? in second european conference on computer vision pp. santa margherita liguere italy. faugeras o. d. and hebert m. the representation recognition and positioning of shapes from range data. in kanade t. three-dimensional machine vision pp. kluwer academic publishers boston. faugeras o. d. luong q.-t. and maybank s. j. camera self-calibration theory in second european conference on computer vision and experiments. pp. santa margherita liguere italy. favaro p. and soatto s. shape estimation and image restoration exploiting defocus and motion-blur. springer. fawcett t. an introduction to roc analysis. pattern recognition letters fei-fei l. and perona p. a bayesian hierarchical model for learning natural scene in ieee computer society conference on computer vision and pattern categories. recognition pp. san diego ca. fei-fei l. fergus r. and perona p. one-shot learning of object categories. ieee transactions on pattern analysis and machine intelligence fei-fei l. fergus r. and torralba a. iccv short course on recognizing and learning object categories. in twelfth international conference on computer vision kyoto japan. httppeople.csail.mit.edutorralbashortcourserloc. references feilner m. van de ville d. and unser m. an orthogonal family of quincunx wavelets with continuously adjustable order. ieee transactions on image processing feldmar j. and ayache n. rigid affine and locally affine registration of free-form surfaces. international journal of computer vision fellbaum c. wordnet an electronic lexical database bradford books. felzenszwalb p. mcallester d. and ramanan d. a discriminatively trained multiscale deformable part model. in ieee computer society conference on computer vision and pattern recognition anchorage ak. felzenszwalb p. f. and huttenlocher d. p. distance transforms of sampled functions. technical report cornell university computing and information science. felzenszwalb p. f. and huttenlocher d. p. efficient graph-based image segmen tation. international journal of computer vision felzenszwalb p. f. and huttenlocher d. p. pictorial structures for object recogni tion. international journal of computer vision felzenszwalb p. f. and huttenlocher d. p. efficient belief propagation for early vision. international journal of computer vision felzenszwalb p. f. girshick r. b. mcallester d. and ramanan d. object detection with discriminatively trained part-based models. ieee transactions on pattern analysis and machine intelligence ferencz a. learned-miller e. g. and malik j. learning to locate informative features for visual identification. international journal of computer vision fergus r. combined segmentation and recognition. in cvpr short course on recognizing and learning object categories. httppeople.csail.mit.edutorralba shortcourserloc. fergus r. part-based models. in cvpr short course on recognizing and learning object categories. httppeople.csail.mit.edutorralbashortcourserloc. fergus r. classical methods for object recognition. in iccv short course on recognizing and learning object categories kyoto japan. httppeople.csail.mit. edutorralbashortcourserloc. fergus r. perona p. and zisserman a. a visual category filter for google images. in eighth european conference on computer vision pp. prague. computer vision algorithms and applications draft fergus r. perona p. and zisserman a. a sparse object category model for efficient learning and exhaustive recognition. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. weakly supervised scale-invariant learning of models for visual recognition. international journal of computer vision fergus r. perona p. and zisserman a. fergus r. fei-fei l. perona p. and zisserman a. learning object categories from google s image search. in tenth international conference on computer vision pp. beijing china. fergus r. singh b. hertzmann a. roweis s. t. and freeman w. t. removing camera shake from a single photograph. acm transactions on graphics ferrari v. marin-jimenez m. and zisserman a. pose search retrieving people using their pose. in ieee computer society conference on computer vision and pattern recognition miami beach fl. ferrari v. marin-jimenez m. j. and zisserman a. progressive search space reduction for human pose estimation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. ferrari v. tuytelaars t. and van gool l. object detection by contour segment networks. in ninth european conference on computer vision pp. ferrari v. tuytelaars t. and van gool l. simultaneous object recognition and segmentation from single or multiple model views. international journal of computer vision field d. j. relations between the statistics of natural images and the response properties of cortical cells. journal of the optical society of america a finkelstein a. and salesin d. h. multiresolution curves. in acm siggraph conference proceedings pp. fischler m. a. and bolles r. c. random sample consensus a paradigm for model fitting with applications to image analysis and automated cartography. communications of the acm fischler m. a. and elschlager r. a. the representation and matching of pictorial structures. ieee transactions on computers fischler m. a. and firschein o. readings in computer vision. morgan kaufmann publishers inc. los altos. references fischler m. a. firschein o. barnard s. t. fua p. v. and leclerc y. the vision problem exploiting parallel computation. technical note sri international menlo park. fitzgibbon a. w. and zisserman a. automatic camera recovery for closed and open image sequences. in fifth european conference on computer vision pp. freiburg germany. fitzgibbon a. w. cross g. and zisserman a. automatic model construction for turn-table sequences. in european workshop on structure from multiple images of large-scale environments pp. freiburg. fleet d. and jepson a. computation of component image velocity from local phase information. international journal of computer vision fleuret f. and geman d. coarse-to-fine face detection. international journal of computer vision flickner m. sawhney h. niblack w. ashley j. huang q. et al. query by image and video content the qbic system. computer foley j. d. van dam a. feiner s. k. and hughes j. f. computer graphics principles and practice. addison-wesley reading ma edition. f orstner w. a feature-based correspondence algorithm for image matching. intl. arch. photogrammetry remote sensing f orstner w. uncertainty and projective geometry. in bayro-corrochano e. handbook of geometric computing pp. springer new york. forsyth d. and ponce j. computer vision a modern approach. prentice hall upper saddle river nj. forsyth d. a. and fleck m. m. automatic detection of human nudes. interna tional journal of computer vision forsyth d. a. arikan o. ikemoto l. o brien j. and ramanan d. computational studies of human motion part tracking and motion synthesis. foundations and trends in computer graphics and computer vision fossati a. dimitrijevic m. lepetit v. and fua p. bridging the gap between detection and tracking for monocular video-based motion capture. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. frahm j.-m. and koch r. camera calibration with known rotation. in ninth inter national conference on computer vision pp. nice france. computer vision algorithms and applications draft freeman m. mastering hdr photography. amphoto books new york. freeman w. perona p. and sch olkopf b. guest editorial special issue on machine learning for vision. international journal of computer vision freeman w. t. steerable filters and local analysis of image structure. ph.d. thesis massachusetts institute of technology. freeman w. t. and adelson e. h. the design and use of steerable filters. ieee transactions on pattern analysis and machine intelligence freeman w. t. jones t. r. and pasztor e. c. example-based super-resolution. ieee computer graphics and applications freeman w. t. pasztor e. c. and carmichael o. t. learning low-level vision. international journal of computer vision frey b. j. and mackay d. j. c. a revolution belief propagation in graphs with cycles. in advances in neural information processing systems. friedman j. hastie t. and tibshirani r. additive logistic regression a statistical view of boosting. annals of statistics frisken s. f. perry r. n. rockwood a. p. and jones t. r. adaptively sampled distance fields a general representation of shape for computer graphics. in acm siggraph conference proceedings pp. fritz m. and schiele b. decomposition discovery and detection of visual categories using topic models. in ieee computer society conference on computer vision and pattern recognition anchorage ak. frome a. singer y. sha f. and malik j. learning globally-consistent local distance functions for shape-based image retrieval and classification. in eleventh international conference on computer vision rio de janeiro brazil. fua p. a parallel stereo algorithm that produces dense depth maps and preserves image features. machine vision and applications fua p. and leclerc y. g. object-centered surface reconstruction combining multi-image stereo and shading. international journal of computer vision fua p. and sander p. segmenting unstructured points into surfaces. in second european conference on computer vision pp. santa margherita liguere italy. fuh c.-s. and maragos p. motion displacement estimation using an affine model for image matching. optical engineering references fukunaga k. and hostetler l. d. the estimation of the gradient of a density function with applications in pattern recognition. ieee transactions on information theory furukawa y. and ponce j. accurate dense and robust multi-view stereopsis. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. furukawa y. and ponce j. accurate calibration from multi-view stereo and bundle in ieee computer society conference on computer vision and pattern adjustment. recognition anchorage ak. furukawa y. and ponce j. carved visual hulls for image-based modeling. inter national journal of computer vision furukawa y. and ponce j. accurate dense and robust multi-view stereopsis. ieee transactions on pattern analysis and machine intelligence. furukawa y. curless b. seitz s. m. and szeliski r. manhattan-world stereo. in ieee computer society conference on computer vision and pattern recognition miami fl. furukawa y. curless b. seitz s. m. and szeliski r. reconstructing building interiors from images. in twelfth ieee international conference on computer vision kyoto japan. furukawa y. curless b. seitz s. m. and szeliski r. towards internet-scale in ieee computer society conference on computer vision and multi-view stereo. pattern recognition san francisco ca. fusiello a. roberto v. and trucco e. efficient stereo with multiple windowing. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. fusiello a. trucco e. and verri a. a compact algorithm for rectification of stereo pairs. machine vision and applications gai j. and kang s. b. matte-based restoration of vintage video. ieee transac tions on image processing gal r. wexler y. ofek e. hoppe h. and cohen-or d. seamless montage for texturing models. in proceedings of eurographics gallagher a. c. and chen t. multi-image graph cut clothing segmentation for in ieee computer society conference on computer vision and recognizing people. pattern recognition anchorage ak. computer vision algorithms and applications draft gallup d. frahm j.-m. mordohai p. and pollefeys m. variable baselineresolution stereo. in ieee computer society conference on computer vision and pattern recognition anchorage ak. gamble e. and poggio t. visual integration and detection of discontinuities the key role of intensity edges. a. i. memo artificial intelligence laboratory massachusetts institute of technology. gammeter s. bossard l. quack t. and van gool l. i know what you did last summer object-level auto-annotation of holiday snaps. in twelfth international conference on computer vision kyoto japan. gao w. chen y. wang r. shan s. and jiang d. learning and synthesizing compatible face animation from video sequence. ieee transactions on circuits and systems for video technology garding j. shape from texture for smooth curved surfaces in perspective projec tion. journal of mathematical imaging and vision gargallo p. prados e. and sturm p. minimizing the reprojection error in surface reconstruction from images. in eleventh international conference on computer vision rio de janeiro brazil. gavrila d. m. the visual analysis of human movement a survey. computer vision and image understanding gavrila d. m. and davis l. s. model-based tracking of humans in action a multi-view approach. in ieee computer society conference on computer vision and pattern recognition pp. san francisco. gavrila d. m. and philomin v. real-time object detection for smart vehicles. in seventh international conference on computer vision pp. kerkyra greece. geiger d. and girosi f. parallel and deterministic algorithms for mrfs surface reconstruction. ieee transactions on pattern analysis and machine intelligence geiger d. ladendorf b. and yuille a. in second european conference on computer vision pp. santa margherita liguere italy. occlusions and binocular stereo. gelb a. sachusetts. applied optimal estimation. mit press cambridge mas geller t. overcoming the uncanny valley. ieee computer graphics and applica tions references geman s. and geman d. stochastic relaxation gibbs distribution and the bayesian restoration of images. ieee transactions on pattern analysis and machine intelligence gennert m. a. brightness-based stereo matching. in second international con ference on computer vision pp. tampa. gersho a. and gray r. m. vector quantization and signal compression. springer. gershun a. the light field. journal of mathematics and physics gevers t. van de weijer j. and stokman h. color feature detection. in lukac r. and plataniotis k. n. color image processing methods and applications crc press. giblin p. and weiss r. reconstruction of surfaces from profiles. in first interna tional conference on computer vision pp. london england. gionis a. indyk p. and motwani r. similarity search in high dimensions via hashing. in international conference on very large data bases pp. girod b. greiner g. and niemann h. principles of image analysis and synthesis kluwer boston. glassner a. s. principles of digital image synthesis. morgan kaufmann publish ers san francisco. gleicher m. image snapping. in acm siggraph conference proceedings pp. gleicher m. projective registration with difference decomposition. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. gleicher m. and witkin a. through-the-lens camera control. computer graphics glocker b. komodakis n. tziritas g. navab n. and paragios n. dense image registration through mrfs and efficient linear programming. medical image analysis glocker b. paragios n. komodakis n. tziritas g. and navab n. optical flow estimation with uncertainties through dynamic mrfs. in ieee computer society conference on computer vision and pattern recognition anchorage ak. computer vision algorithms and applications draft gluckman j. higher order image pyramids. in ninth european conference on computer vision pp. gluckman j. scale variant image pyramids. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. goesele m. curless b. and seitz s. multi-view stereo revisited. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. goesele m. fuchs c. and seidel h.-p. accuracy of range scanners by measurement of the slanted edge modulation transfer function. in fourth international conference on digital imaging and modeling banff. goesele m. snavely n. curless b. hoppe h. and seitz s. m. multi-view stereo for community photo collections. in eleventh international conference on computer vision rio de janeiro brazil. gold s. rangarajan a. lu c. pappu s. and mjolsness e. new algorithms for and point matching pose estimation and correspondence. pattern recognition goldberg a. v. and tarjan r. e. a new approach to the maximum-flow problem. journal of the acm goldluecke b. and cremers d. superresolution texture maps for multiview reconstruction. in twelfth international conference on computer vision kyoto japan. goldman d. b. vignette and exposure calibration and compensation. ieee trans actions on pattern analysis and machine intelligence. golovinskiy a. matusik w. ster h. p. rusinkiewicz s. and funkhouser t. a statistical model for synthesis of detailed facial geometry. acm transactions on graphics golub g. and van loan c. f. matrix computation third edition. the john hopkins university press baltimore and london. gomes j. and velho l. image processing for computer graphics. springer verlag new york. gomes j. darsa l. costa b. and velho l. warping and morphing of graphical objects. morgan kaufmann publishers san francisco. references gong m. yang r. wang l. and gong m. a performance study on different cost aggregation approaches used in realtime stereo matching. international journal of computer vision gonzales r. c. and woods r. e. digital image processing. prentice-hall upper saddle river nj edition. gooch b. and gooch a. non-photorealistic rendering. a k peters ltd natick massachusetts. gordon i. and lowe d. g. what and where object recognition with accurate pose. in ponce j. hebert m. schmid c. and zisserman a. toward categorylevel object recognition pp. springer new york. gorelick l. blank m. shechtman e. irani m. and basri r. actions as space-time shapes. ieee transactions on pattern analysis and machine intelligence gortler s. j. and cohen m. f. hierarchical and variational geometric modeling with wavelets. in symposium on interactive graphics pp. monterey ca. gortler s. j. grzeszczuk r. szeliski r. and cohen m. f. the lumigraph. in acm siggraph conference proceedings pp. new orleans. goshtasby a. correction of image deformation from lens distortion using b ezier patches. computer vision graphics and image processing goshtasby a. and image registration. wiley new york. gotchev a. and rosenhahn b. proceedings of the conference the true vision capture transmission and display of video ieee computer society press. govindu v. m. revisiting the brightness constraint probabilistic formulation and algorithms. in ninth european conference on computer vision pp. grady l. random walks for image segmentation. ieee transactions on pattern analysis and machine intelligence grady l. a lattice-preserving multigrid method for solving the inhomogeneous poisson equations used in image analysis. in tenth european conference on computer vision pp. marseilles. grady l. and ali s. fast approximate random walker segmentation using eigenvector precomputation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. computer vision algorithms and applications draft grady l. and alvino c. reformulating and optimizing the mumford shah functional on a graph a faster lower energy solution. in tenth european conference on computer vision pp. marseilles. grauman k. and darrell t. efficient image matching with distributions of local invariant features. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. grauman k. and darrell t. pyramid match hashing sub-linear time indexing over partial correspondences. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. grauman k. and darrell t. the pyramid match kernel efficient learning with sets of features. journal of machine learning research grauman k. shakhnarovich g. and darrell t. inferring structure with a statistical image-based shape model. in ninth international conference on computer vision pp. nice france. greene n. environment mapping and other applications of world projections. ieee computer graphics and applications greene n. and heckbert p. creating raster omnimax images from multiple perspective views using the elliptical weighted average filter. ieee computer graphics and applications greig d. porteous b. and seheult a. exact maximum a posteriori estimation for binary images. journal of the royal statistical society series b gremban k. d. thorpe c. e. and kanade t. geometric camera calibration using systems of linear equations. in ieee international conference on robotics and automation pp. philadelphia. griffin g. holub a. and perona p. object category dataset. tech nical report california institute of technology. grimson w. e. l. an implementation of a computational theory of visual surface interpolation. computer vision graphics and image processing grimson w. e. l. computational experiments with a feature based stereo alieee transactions on pattern analysis and machine intelligence pami gorithm. gross r. matthews i. and baker s. active appearance models with occlusion. image and vision computing gross r. shi j. and cohn j. f. quo vadis face recognition? in ieee workshop on empirical evaluation methods in computer vision san diego. references gross r. baker s. matthews i. and kanade t. face recognition across pose and illumination. in li s. z. and jain a. k. handbook of face recognition springer. gross r. sweeney l. de la torre f. and baker s. semi-supervised learning of multi-factor models for face de-identification. in ieee computer society conference on computer vision and pattern recognition anchorage ak. gross r. matthews i. cohn j. kanade t. and baker s. multi-pie. image and vision computing grossberg m. d. and nayar s. k. a general imaging model and a method for finding its parameters. in eighth international conference on computer vision pp. vancouver canada. grossberg m. d. and nayar s. k. modeling the space of camera response functions. ieee transactions on pattern analysis and machine intelligence gu c. lim j. arbelaez p. and malik j. recognition using regions. in ieee computer society conference on computer vision and pattern recognition miami beach fl. gu x. gortler s. j. and hoppe h. geometry images. acm transactions on graphics guan p. weiss a. b alan a. o. and black m. j. estimating human shape and in twelfth international conference on computer vision pose from a single image. kyoto japan. guennebaud g. and gross m. algebraic point set surfaces. acm transactions on graphics guennebaud g. germann m. and gross m. dynamic sampling and rendering of algebraic point set surfaces. computer graphics forum guenter b. grimm c. wood d. malvar h. and pighin f. making faces. in acm siggraph conference proceedings pp. guillaumin m. verbeek j. and schmid c. is that you? metric learning approaches for face identification. in twelfth international conference on computer vision kyoto japan. gulbins j. and gulbins r. photographic multishot techniques high dynamic range super-resolution extended depth of field stitching. rocky nook. computer vision algorithms and applications draft habbecke m. and kobbelt l. a surface-growing approach to multi-view stereo reconstruction. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. hager g. d. and belhumeur p. n. efficient region tracking with parametric models ieee transactions on pattern analysis and machine of geometry and illumination. intelligence hall r. illumination and color in computer generated imagery. springer-verlag new york. haller m. billinghurst m. and thomas b. emerging technologies of augmented reality interfaces and design. igi publishing. hampel f. r. ronchetti e. m. rousseeuw p. j. and stahel w. a. robust statistics the approach based on influence functions. wiley new york. han f. and zhu s.-c. bottom-uptop-down image parsing by attribute graph grammar. in tenth international conference on computer vision pp. beijing china. hanna k. j. direct multi-resolution estimation of ego-motion and structure from motion. in ieee workshop on visual motion pp. princeton new jersey. hannah m. j. computer matching of areas in stereo images. ph.d. thesis stan ford university. hannah m. j. test results from sri s stereo system. in image understanding workshop pp. cambridge massachusetts. hansen m. anandan p. dana k. van der wal g. and burt p. real-time scene stabilization and mosaic construction. in ieee workshop on applications of computer vision pp. sarasota. hanson a. r. and riseman e. m. computer vision systems academic press new york. haralick r. m. and shapiro l. g. image segmentation techniques. computer vision graphics and image processing haralick r. m. and shapiro l. g. computer and robot vision. addison-wesley reading ma. haralick r. m. lee c.-n. ottenberg k. and n olle m. review and analysis of solutions of the three point perspective pose estimation problem. international journal of computer vision references hardie r. c. barnard k. j. and armstrong e. e. joint map registration and ieee high-resolution image estimation using a sequence of undersampled images. transactions on image processing haritaoglu i. harwood d. and davis l. s. real-time surveillance of people and their activities. ieee transactions on pattern analysis and machine intelligence harker m. and o leary p. least squares surface reconstruction from measured gradient fields. in ieee computer society conference on computer vision and pattern recognition anchorage ak. harris c. and stephens m. j. a combined corner and edge detector. in alvey vision conference pp. hartley r. and kang s. b. parameter-free radial distortion correction with center of distortion estimation. ieee transactions on pattern analysis and machine intelligence hartley r. gupta r. and chang t. estimation of relative camera positions for uncalibrated cameras. in second european conference on computer vision pp. santa margherita liguere italy. hartley r. i. projective reconstruction and invariants from multiple images. ieee transactions on pattern analysis and machine intelligence hartley r. i. self-calibration from multiple views of a rotating camera. in third european conference on computer vision pp. stockholm sweden. hartley r. i. in defense of the algorithm. ieee transactions on pattern analysis and machine intelligence hartley r. i. self-calibration of stationary cameras. international journal of computer vision hartley r. i. chirality. international journal of computer vision hartley r. i. and kang s. b. parameter-free radial distortion correction with centre of distortion estimation. in tenth international conference on computer vision pp. beijing china. hartley r. i. and sturm p. triangulation. computer vision and image under standing hartley r. i. and zisserman a. multiple view geometry. cambridge university press cambridge uk. computer vision algorithms and applications draft hartley r. i. hayman e. de agapito l. and reid i. camera calibration and the search for infinity. in ieee computer society conference on computer vision and pattern recognition pp. hilton head island. hasinoff s. w. and kutulakos k. n. light-efficient photography. in tenth euro pean conference on computer vision pp. marseilles. hasinoff s. w. durand f. and freeman w. t. noise-optimal capture for high dynamic range photography. in ieee computer society conference on computer vision and pattern recognition san francisco ca. hasinoff s. w. kang s. b. and szeliski r. boundary matting for view synthesis. computer vision and image understanding hasinoff s. w. kutulakos k. n. durand f. and freeman w. t. timein twelfth international conference on computer vision constrained photography. kyoto japan. hastie t. tibshirani r. and friedman j. the elements of statistical learning data mining inference and prediction. springer-verlag new york. hayes b. computational photography. american scientist hays j. and efros a. a. scene completion using millions of photographs. acm transactions on graphics hays j. leordeanu m. efros a. a. and liu y. discovering texture regularity as a higher-order correspondence problem. in ninth european conference on computer vision pp. he l.-w. and zhang z. video camera for teleconferencing. speech and signal processing pp. philadelphia. real-time whiteboard capture and processing using a in ieee international conference on acoustics he x. and zemel r. s. learning hybrid models for image annotation with partially labeled data. in advances in neural information processing systems. he x. zemel r. s. and carreira-perpi n an m. a. multiscale conditional random fields for image labeling. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. he x. zemel r. s. and ray d. learning and incorporating top-down cues in image segmentation. in ninth european conference on computer vision pp. healey g. e. and kondepudy r. radiometric ccd camera calibration and ieee transactions on pattern analysis and machine intelligence noise estimation. references healey g. e. and shafer s. a. color. physics-based vision principles and practice jones bartlett cambridge ma. heath m. d. sarkar s. sanocki t. and bowyer k. w. comparison of edge detectors. computer vision and image understanding hebert m. active and passive range sensing for robotics. in ieee international conference on robotics and automation pp. san francisco. hecht e. optics. pearson addison wesley reading ma edition. heckbert p. survey of texture mapping. ieee computer graphics and applica tions heckbert p. fundamentals of texture mapping and image warping. master s thesis the university of california at berkeley. heeger d. j. optical flow using spatiotemporal filters. international journal of computer vision heeger d. j. and bergen j. r. pyramid-based texture analysissynthesis. in acm siggraph conference proceedings pp. heisele b. serre t. and poggio t. a component-based framework for face detection and identification. international journal of computer vision heisele b. ho p. wu j. and poggio t. face recognition component-based versus global approaches. computer vision and image understanding herley c. automatic occlusion removal from minimum number of images. in international conference on image processing pp. genova. hernandez c. and schmitt f. silhouette and stereo fusion for object modeling. computer vision and image understanding hernandez c. and vogiatzis g. self-calibrating a real-time monocular facial capture system. in fifth international symposium on data processing visualization and transmission paris. hernandez c. vogiatzis g. and cipolla r. probabilistic visibility for multiview stereo. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. hernandez c. vogiatzis g. brostow g. j. stenger b. and cipolla r. nonrigid photometric stereo with colored lights. in eleventh international conference on computer vision rio de janeiro brazil. computer vision algorithms and applications draft hershberger j. and snoeyink j. speeding up the douglas-peucker linesimplification algorithm. technical report computer science department the university of british columbia. hertzmann a. jacobs c. e. oliver n. curless b. and salesin d. h. image analogies. in acm siggraph conference proceedings pp. hiep v. h. keriven r. pons j.-p. and labatut p. towards high-resolution largein ieee computer society conference on computer vision scale multi-view stereo. and pattern recognition miami beach fl. hillman p. hannah j. and renshaw d. alpha channel estimation in high resolution images and image sequences. in ieee computer society conference on computer vision and pattern recognition pp. kauai hawaii. hilton a. fua p. and ronfard r. modeling people vision-based understanding of a person s shape appearance movement and behaviour. computer vision and image understanding hilton a. stoddart a. j. illingworth j. and windeatt t. reliable surface reconstruction from multiple range images. in fourth european conference on computer vision pp. cambridge england. hinckley k. sinclair m. hanson e. szeliski r. and conway m. the videoin annual acm mouse a camera-based multi-degree-of-freedom input device. symposium on user interface software and technology pp. hinterstoisser s. benhimane s. navab n. fua p. and lepetit v. online learning of patch perspective rectification for efficient object detection. in ieee computer society conference on computer vision and pattern recognition anchorage ak. hinton g. e. relaxation and its role in vision. ph.d. thesis university of edin burgh. hirschm uller h. stereo processing by semiglobal matching and mutual information. ieee transactions on pattern analysis and machine intelligence hirschm uller h. and scharstein d. evaluation of stereo matching costs on images with radiometric differences. ieee transactions on pattern analysis and machine intelligence hjaltason g. r. and samet h. index-driven similarity search in metric spaces. acm transactions on database systems hofmann t. probabilistic latent semantic indexing. in acm sigir conference on research and development in informaion retrieval pp. berkeley ca. references hogg d. model-based vision a program to see a walking person. image and vision computing hoiem d. efros a. a. and hebert m. automatic photo pop-up. acm transac tions on graphics siggraph hoiem d. efros a. a. and hebert m. geometric context from a single image. in tenth international conference on computer vision pp. beijing china. hoiem d. efros a. a. and hebert m. closing the loop in scene interpretation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. hoiem d. efros a. a. and hebert m. putting objects in perspective. interna tional journal of computer vision hoiem d. rother c. and winn j. layoutcrf for multi-view object class in ieee computer society conference on computer recognition and segmentation. vision and pattern recognition minneapolis mn. hoover a. jean-baptiste g. jiang x. flynn p. j. bunke h. et al. an experieee transactions on imental comparison of range image segmentation algorithms. pattern analysis and machine intelligence hoppe h. progressive meshes. in acm siggraph conference proceed ings pp. new orleans. hoppe h. derose t. duchamp t. mcdonald j. and stuetzle w. surface reconstruction from unorganized points. computer graphics horn b. k. p. determining lightness from an image. computer graphics and image processing horn b. k. p. obtaining shape from shading information. in winston p. h. the psychology of computer vision pp. mcgraw-hill new york. horn b. k. p. understanding image intensities. artificial intelligence horn b. k. p. robot vision. mit press cambridge massachusetts. horn b. k. p. closed-form solution of absolute orientation using unit quaternions. journal of the optical society of america a horn b. k. p. height and gradient from shading. international journal of com puter vision computer vision algorithms and applications draft horn b. k. p. and brooks m. j. the variational approach to shape from shading. computer vision graphics and image processing horn b. k. p. and brooks m. j. shape from shading mit press cam bridge massachusetts. horn b. k. p. and schunck b. g. determining optical flow. artificial intelligence horn b. k. p. and weldon jr. e. j. direct methods for recovering motion. inter national journal of computer vision hornung a. zeng b. and kobbelt l. image selection for improved multiview stereo. in ieee computer society conference on computer vision and pattern recognition anchorage ak. horowitz s. l. and pavlidis t. picture segmentation by a tree traversal algorithm. journal of the acm horry y. anjyo k.-i. and arai k. tour into the picture using a spidery mesh interface to make animation from a single image. in acm siggraph conference proceedings pp. hough p. v. c. method and means for recognizing complex patterns. u. s. patent houhou n. thiran j.-p. and bresson x. fast texture segmentation using the shape operator and active contour. in ieee computer society conference on computer vision and pattern recognition anchorage ak. howe n. r. leventon m. e. and freeman w. t. bayesian reconstruction of human motion from single-camera video. in advances in neural information processing systems. hsieh y. c. mckeown d. and perlant f. p. performance evaluation of scene registration and stereo matching for cartographic feature extraction. ieee transactions on pattern analysis and machine intelligence hu w. tan t. wang l. and maybank s. a survey on visual surveillance of object motion and behaviors. ieee transactions on systems man and cybernetics part c applications and reviews hua g. brown m. and winder s. discriminant embedding for local image descriptors. in eleventh international conference on computer vision rio de janeiro brazil. references huang g. b. ramesh m. berg t. and learned-miller e. labeled faces in the wild a database for studying face recognition in unconstrained environments. technical report university of massachusetts amherst. huang t. s. image sequence analysis. springer-verlag berlin heidelberg. huber p. j. robust statistics. john wiley sons new york. huffman d. a. impossible objects and nonsense sentences. machine intelligence huguet f. and devernay f. a variational method for scene flow estimation from in eleventh international conference on computer vision stereo sequences. rio de janeiro brazil. huttenlocher d. p. klanderman g. and rucklidge w. comparing images using the hausdorff distance. ieee transactions on pattern analysis and machine intelligence huynh d. q. hartley r. and heyden a. outlier correcton in image sequences for the affine camera. in ninth international conference on computer vision pp. nice france. iddan g. j. and yahav g. imaging in the studio elsewhere.... in three dimensional image capture and applications iv pp. igarashi t. nishino k. and nayar s. the appearance of human skin a survey. foundations and trends in computer graphics and computer vision ikeuchi k. shape from regular patterns. artificial intelligence ikeuchi k. and horn b. k. p. numerical shape from shading and occluding boundaries. artificial intelligence ikeuchi k. and miyazaki d. digitally archiving cultural objects springer boston ma. ikeuchi k. and sato y. modeling from reality kluwer academic publish ers boston. illingworth j. and kittler j. a survey of the hough transform. computer vision graphics and image processing intille s. s. and bobick a. f. disparity-space images and large occlusion stereo. in third european conference on computer vision stockholm sweden. irani m. and anandan p. video indexing based on mosaic representations. pro ceedings of the ieee computer vision algorithms and applications draft irani m. and peleg s. improving resolution by image registration. graphical models and image processing irani m. hsu s. and anandan p. video compression using mosaic representa tions. signal processing image communication irani m. rousso b. and peleg s. computing occluding and transparent motions. international journal of computer vision irani m. rousso b. and peleg s. recovery of ego-motion using image stabilization. ieee transactions on pattern analysis and machine intelligence isaksen a. mcmillan l. and gortler s. j. dynamically reparameterized light fields. in acm siggraph conference proceedings pp. isard m. and blake a. condensation conditional density propagation for visual tracking. international journal of computer vision ishiguro h. yamamoto m. and tsuji s. omni-directional stereo. ieee trans actions on pattern analysis and machine intelligence ishikawa h. exact optimization for markov random fields with convex priors. ieee transactions on pattern analysis and machine intelligence ishikawa h. and veksler o. convex and truncated convex priors for multi-label in blake a. kohli p. and rother c. advances in markov random mrfs. fields mit press. isidoro j. and sclaroff s. stochastic refinement of the visual hull to satisfy photometric and silhouette consistency constraints. in ninth international conference on computer vision pp. nice france. ivanchenko v. shen h. and coughlan j. elevation-based stereo implemented in real-time on a gpu. in ieee workshop on applications of computer vision snowbird utah. jacobs c. e. finkelstein a. and salesin d. h. fast multiresolution image querying. in acm siggraph conference proceedings pp. j ahne b. digital image processing. springer-verlag berlin. jain a. k. and dubes r. c. algorithms for clustering data. prentice hall englewood cliffs new jersey. jain a. k. bolle r. m. and pankanti s. biometrics personal identifica tion in networked society kluwer. jain a. k. duin r. p. w. and mao j. statistical pattern recognition a review. ieee transactions on pattern analysis and machine intelligence references jain a. k. topchy a. law m. h. c. and buhmann j. m. landscape of clusin international conference on pattern recognition tering algorithms. pp. jenkin m. r. m. jepson a. d. and tsotsos j. k. techniques for disparity measurement. cvgip image understanding jensen h. w. marschner s. r. levoy m. and hanrahan p. a practical model for subsurface light transport. in acm siggraph conference proceedings pp. jeong y. nist er d. steedly d. szeliski r. and kweon i.-s. pushing the envelope of modern methods for bundle adjustment. in ieee computer society conference on computer vision and pattern recognition san francisco ca. jia j. and tang c.-k. image registration with global and local luminance alignment. in ninth international conference on computer vision pp. nice france. jia j. sun j. tang c.-k. and shum h.-y. drag-and-drop pasting. acm trans actions on graphics jiang z. wong t.-t. and bao h. practical super-resolution from dynamic video in ieee computer society conference on computer vision and pattern sequences. recognition pp. madison wi. johnson a. e. and hebert m. using spin images for efficient object recognition in cluttered scenes. ieee transactions on pattern analysis and machine intelligence johnson a. e. and kang s. b. registration and integration of textured data. in international conference on recent advances in digital imaging and modeling pp. ottawa. jojic n. and frey b. j. learning flexible sprites in video layers. in ieee computer society conference on computer vision and pattern recognition pp. kauai hawaii. jones d. g. and malik j. a computational framework for determining stereo correspondence from a set of linear spatial filters. in second european conference on computer vision pp. santa margherita liguere italy. jones m. j. and rehg j. m. statistical color models with application to skin detection. international journal of computer vision joshi n. matusik w. and avidan s. natural video matting using camera arrays. acm transactions on graphics computer vision algorithms and applications draft joshi n. szeliski r. and kriegman d. j. psf estimation using sharp edge in ieee computer society conference on computer vision and pattern prediction. recognition anchorage ak. joshi n. zitnick c. l. szeliski r. and kriegman d. j. image deblurring and denoising using color priors. in ieee computer society conference on computer vision and pattern recognition miami fl. ju s. x. black m. j. and jepson a. d. skin and bones multi-layer locally in ieee computer society affine optical flow and regularization with transparency. conference on computer vision and pattern recognition pp. san francisco. ju s. x. black m. j. and yacoob y. cardboard people a parameterized model of articulated image motion. in international conference on automatic face and gesture recognition pp. killington vt. juan o. and boykov y. active graph cuts. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. jurie f. and dhome m. hyperplane approximation for template matching. ieee transactions on pattern analysis and machine intelligence jurie f. and schmid c. scale-invariant shape features for recognition of object in ieee computer society conference on computer vision and pattern categories. recognition pp. washington dc. kadir t. zisserman a. and brady m. an affine invariant salient region detector. in eighth european conference on computer vision pp. prague. kaftory r. schechner y. and zeevi y. variational distance-dependent image in ieee computer society conference on computer vision and pattern restoration. recognition minneapolis mn. kahl f. and hartley r. multiple-view geometry under the l ieee trans actions on pattern analysis and machine intelligence kakadiaris i. and metaxas d. model-based estimation of human motion. ieee transactions on pattern analysis and machine intelligence kakumanu p. makrogiannis s. and bourbakis n. a survey of skin-color model ing and detection methods. pattern recognition kamvar s. d. klein d. and manning c. d. interpreting and extending classical in international agglomerative clustering algorithms using a model-based approach. references conference on machine learning pp. kanade t. computer recognition of human faces. birkhauser basel. kanade t. a theory of the origami world. artificial intelligence kanade t. three-dimensional machine vision kluwer academic publish ers boston. kanade t. development of a video-rate stereo machine. in image understanding workshop pp. monterey. kanade t. and okutomi m. a stereo matching algorithm with an adaptive winieee transactions on pattern analysis and machine dow theory and experiment. intelligence kanade t. rander p. w. and narayanan p. j. virtualized reality constructing virtual worlds from real scenes. ieee multimedia magazine kanade t. yoshida a. oda k. kano h. and tanaka m. a stereo machine for video-rate dense depth mapping and its new applications. in ieee computer society conference on computer vision and pattern recognition pp. san francisco. kanatani k. and morris d. d. gauges and gauge transformations for uncertainty description of geometric structure with indeterminacy. ieee transactions on information theory kang s. b. depth painting for image-based rendering applications. technical report compaq computer corporation cambridge research lab. kang s. b. a survey of image-based rendering techniques. in videometrics vi pp. san jose. kang s. b. radial distortion snakes. ieice trans. inf. syst. kang s. b. and jones m. appearance-based structure from motion using linear classes of models. international journal of computer vision kang s. b. and szeliski r. scene data recovery using omnidirectional multi baseline stereo. international journal of computer vision kang s. b. and szeliski r. extracting view-dependent depth maps from a collec tion of images. international journal of computer vision kang s. b. and weiss r. characterization of errors in compositing panoramic images. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. computer vision algorithms and applications draft kang s. b. and weiss r. characterization of errors in compositing panoramic images. computer vision and image understanding kang s. b. and weiss r. can we calibrate a camera using an image of a flat in sixth european conference on computer vision textureless lambertian surface? pp. dublin ireland. kang s. b. szeliski r. and anandan p. the geometry-image representation tradeoff for rendering. in international conference on image processing pp. vancouver. kang s. b. szeliski r. and chai j. handling occlusions in dense multi-view stereo. in ieee computer society conference on computer vision and pattern recognition pp. kauai hawaii. kang s. b. szeliski r. and shum h.-y. a parallel feature tracker for extended image sequences. computer vision and image understanding kang s. b. szeliski r. and uyttendaele m. seamless stitching using multi perspective plane sweep. technical report microsoft research. kang s. b. li y. tong x. and shum h.-y. image-based rendering. founda tions and trends in computer graphics and computer vision kang s. b. uyttendaele m. winder s. and szeliski r. high dynamic range video. acm transactions on graphics siggraph kang s. b. webb j. zitnick l. and kanade t. a multibaseline stereo system with active illumination and real-time image acquisition. in fifth international conference on computer vision pp. cambridge massachusetts. kannala j. rahtu e. brandt s. s. and heikkila j. object recognition and segmentation by non-rigid quasi-dense matching. in ieee computer society conference on computer vision and pattern recognition anchorage ak. kass m. linear image features in stereopsis. international journal of computer vision kass m. witkin a. and terzopoulos d. snakes active contour models. inter national journal of computer vision kato h. billinghurst m. poupyrev i. imamoto k. and tachibana k. virtual in international symposium on object manipulation on a table-top ar environment. augmented reality kaufman l. and rousseeuw p. j. finding groups in data an introduction to cluster analysis. john wiley sons hoboken. references kazhdan m. bolitho m. and hoppe h. poisson surface reconstruction. in eurographics symposium on geometry processing pp. ke y. and sukthankar r. pca-sift a more distinctive representation for local in ieee computer society conference on computer vision and image descriptors. pattern recognition pp. washington dc. kehl r. and van gool l. markerless tracking of complex human motions from multiple views. computer vision and image understanding kenney c. zuliani m. and manjunath b. an axiomatic approach to corner detection. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. keren d. peleg s. and brada r. image sequence enhancement using sub-pixel displacements. in ieee computer society conference on computer vision and pattern recognition pp. ann arbor michigan. kim j. kolmogorov v. and zabih r. visual correspondence using energy minin ninth international conference on computer imization and mutual information. vision pp. nice france. kimmel r. demosaicing image reconstruction from color ccd samples. ieee transactions on image processing kimura s. shinbo t. yamaguchi h. kawamura e. and nakano k. a convolver-based real-time stereo machine in ieee computer society conference on computer vision and pattern recognition pp. fort collins. kindermann r. and snell j. l. markov random fields and their applications. american mathematical society. king d. the commissar vanishes. henry holt and company. kirby m. and sirovich l. application of the karhunen loeve procedure for the characterization of human faces. ieee transactions on pattern analysis and machine intelligence kirkpatrick s. gelatt c. d. j. and vecchi m. p. optimization by simulated annealing. science kirovski d. jojic n. and jancke g. tamper-resistant biometric ids. in isse securing electronic business processes highlights of the information security solutions europe conference pp. kittler j. and f oglein j. contextual classification of multispectral pixel data. image and vision computing computer vision algorithms and applications draft klaus a. sormann m. and karner k. segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure. in international conference on pattern recognition pp. klein g. and murray d. parallel tracking and mapping for small ar workspaces. in international symposium on mixed and augmented reality nara. klein g. and murray d. improving the agility of keyframe-based slam. in tenth european conference on computer vision pp. marseilles. klein s. staring m. and pluim j. p. w. evaluation of optimization methods for nonrigid medical image registration using mutual information and b-splines. ieee transactions on image processing klinker g. j. a physical approach to color image understanding. a k peters wellesley massachusetts. klinker g. j. shafer s. a. and kanade t. a physical approach to color image understanding. international journal of computer vision koch r. pollefeys m. and van gool l. j. realistic surface reconstruction of scenes from uncalibrated image sequences. journal visualization and computer animation koenderink j. j. solid shape. mit press cambridge massachusetts. koethe u. integrated edge and junction detection with the boundary tensor. in ninth international conference on computer vision pp. nice france. kohli p. minimizing dynamic and higher order energy functions using graph cuts. ph.d. thesis oxford brookes university. kohli p. and torr p. h. s. effciently solving dynamic markov random fields using graph cuts. in tenth international conference on computer vision pp. beijing china. kohli p. and torr p. h. s. dynamic graph cuts for efficient inference in markov ieee transactions on pattern analysis and machine intelligence random fields. kohli p. and torr p. h. s. measuring uncertainty in graph cut solutions. computer vision and image understanding kohli p. kumar m. p. and torr p. h. s. p beyond move making algorithms for solving higher order functions. ieee transactions on pattern analysis and machine intelligence references kohli p. ladick y l. and torr p. h. s. robust higher order potentials for enforc ing label consistency. international journal of computer vision kokaram a. on missing data treatment for degraded video and film archives a survey and a new bayesian approach. ieee transactions on image processing kolev k. and cremers d. integration of multiview stereo and silhouettes via convex functionals on convex domains. in tenth european conference on computer vision pp. marseilles. kolev k. and cremers d. continuous ratio optimization via convex relaxation with applications to multiview reconstruction. in ieee computer society conference on computer vision and pattern recognition miami beach fl. kolev k. klodt m. brox t. and cremers d. continuous global optimization in multiview reconstruction. international journal of computer vision probabilistic graphical models principles and koller d. and friedman n. techniques. mit press cambridge massachusetts. kolmogorov v. convergent tree-reweighted message passing for energy minimization. ieee transactions on pattern analysis and machine intelligence kolmogorov v. and boykov y. what metrics can be approximated by geo-cuts or global optimization of lengtharea and flux. in tenth international conference on computer vision pp. beijing china. kolmogorov v. and zabih r. multi-camera scene reconstruction via graph cuts. in seventh european conference on computer vision pp. copenhagen. kolmogorov v. and zabih r. what energy functions can be minimized via graph cuts? ieee transactions on pattern analysis and machine intelligence kolmogorov v. criminisi a. blake a. cross g. and rother c. probabilistic fusion of stereo with color and contrast for bi-layer segmentation. ieee transactions on pattern analysis and machine intelligence komodakis n. and paragios n. beyond loose lp-relaxations optimizing mrfs by repairing cycles. in tenth european conference on computer vision pp. marseilles. komodakis n. and tziritas g. approximate labeling via graph cuts based on linear programming. ieee transactions on pattern analysis and machine intelligence computer vision algorithms and applications draft komodakis n. and tziritas g. agation via priority scheduling and dynamic pruning. processing image completion using efficient belief propieee transactions on image komodakis n. paragios n. and tziritas g. mrf optimization via dual decomin eleventh international conference on com position message-passing revisited. puter vision rio de janeiro brazil. komodakis n. tziritas g. and paragios n. fast approximately optimal solutions for single and dynamic mrfs. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. komodakis n. tziritas g. and paragios n. performance vs computational efficiency for optimizing single and dynamic mrfs setting the state of the art with primal dual strategies. computer vision and image understanding konolige k. small vision systems hardware and implementation. in eighth international symposium on robotics research pp. hayama japan. kopf j. cohen m. f. lischinski d. and uyttendaele m. joint bilateral upsam pling. acm transactions on graphics kopf j. uyttendaele m. deussen o. and cohen m. f. capturing and viewing gigapixel images. acm transactions on graphics kopf j. lischinski d. deussen o. cohen-or d. and cohen m. locally adapted projections to reduce panorama distortions. computer graphics forum of egsr koutis i. combinatorial and algebraic tools for optimal multilevel algorithms. ph.d. thesis carnegie mellon university. technical report koutis i. and miller g. l. clusters theory computation and applications to preconditioning. parallel algorithms and architectures pp. munich. graph partitioning into isolated high conductance in symposium on koutis i. miller g. l. and tolliver d. combinatorial preconditioners and multilevel solvers for problems in computer vision and image processing. in international symposium on visual computing las vegas. kovar l. gleicher m. and pighin f. motion graphs. acm transactions on graphics ko seck a j. and zhang w. extraction matching and pose recovery based on dominant rectangular structures. computer vision and image understanding references kraus k. photogrammetry. d ummler bonn. krishnan d. and fergus r. fast image deconvolution using hyper-laplacian priors. in advances in neural information processing systems. kuglin c. d. and hines d. c. the phase correlation image alignment method. in ieee conference on cybernetics and society pp. new york. kulis b. and grauman k. kernelized locality-sensitive hashing for scalable image search. in twelfth international conference on computer vision kyoto japan. kumar m. p. combinatorial and convex optimization for probabilistic models in computer vision. ph.d. thesis oxford brookes university. kumar m. p. and torr p. h. s. fast memory-efficient generalized belief propaga tion. in ninth european conference on computer vision pp. kumar m. p. and torr p. h. s. improved moves for truncated convex models. in advances in neural information processing systems. kumar m. p. torr p. h. s. and zisserman a. learning layered motion segmen tations of video. international journal of computer vision kumar m. p. torr p. h. s. and zisserman a. objcut efficient segmentation using top-down and bottom-up cues. ieee transactions on pattern analysis and machine intelligence kumar m. p. veksler o. and torr p. h. s. improved moves for truncated convex models. journal of machine learning research kumar m. p. zisserman a. and h.s.torr p. efficient discriminative learning of parts-based models. in twelfth international conference on computer vision kyoto japan. kumar r. anandan p. and hanna k. direct recovery of shape from multiple in twelfth international conference on pattern views a parallax based approach. recognition pp. jerusalem israel. kumar r. anandan p. irani m. bergen j. and hanna k. representation of in ieee workshop on representations of visual scenes from collections of images. scenes pp. cambridge massachusetts. kumar s. and hebert m. discriminative random fields a discriminative framework for contextual interaction in classification. in ninth international conference on computer vision pp. nice france. computer vision algorithms and applications draft kumar s. and hebert m. discriminative random fields. international journal of computer vision kundur d. and hatzinakos d. blind image deconvolution. ieee signal process ing magazine kutulakos k. n. approximate n-view stereo. in sixth european conference on computer vision pp. dublin ireland. kutulakos k. n. and seitz s. m. a theory of shape by space carving. interna tional journal of computer vision kwatra v. essa i. bobick a. and kwatra n. graphcut textures image and video synthesis using graph cuts. acm transactions on graphics siggraph kwatra v. sch odl a. essa i. turk g. and bobick a. graphcut textures image and video synthesis using graph cuts. acm transactions on graphics siggraph kybic j. and unser m. fast parametric elastic image registration. ieee transac tions on image processing lafferty j. mccallum a. and pereira f. conditional random fields probabilistic in international conference on models for segmenting and labeling sequence data. machine learning. lafortune e. p. f. foo s.-c. torrance k. e. and greenberg d. p. non-linear approximation of reflectance functions. in acm siggraph conference proceedings pp. los angeles. lai s.-h. and vemuri b. c. physically based adaptive preconditioning for early vision. ieee transactions on pattern analysis and machine intelligence lalonde j.-f. hoiem d. efros a. a. rother c. winn j. and criminisi a. photo clip art. acm transactions on graphics lampert c. h. kernel methods in computer vision. foundations and trends in computer graphics and computer vision langer m. s. and zucker s. w. shape from shading on a cloudy day. journal optical society america a lanitis a. taylor c. j. and cootes t. f. automatic interpretation and coding of face images using flexible models. ieee transactions on pattern analysis and machine intelligence references larlus d. and jurie f. combining appearance models and markov random fields for category level object segmentation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. larson g. w. logluv encoding for full-gamut high-dynamic range images. jour nal of graphics tools larson g. w. rushmeier h. and piatko c. a visibility matching tone reproduction operator for high dynamic range scenes. ieee transactions on visualization and computer graphics laurentini a. the visual hull concept for silhouette-based image understanding. ieee transactions on pattern analysis and machine intelligence lavall ee s. and szeliski r. recovering the position and orientation of free-form objects from image contours using distance maps. ieee transactions on pattern analysis and machine intelligence laveau s. and faugeras o. d. scene representation as a collection of images. in twelfth international conference on pattern recognition pp. jerusalem israel. lazebnik s. schmid c. and ponce j. a sparse texture representation using local affine regions. ieee transactions on pattern analysis and machine intelligence lazebnik s. schmid c. and ponce j. beyond bags of features spatial pyramid matching for recognizing natural scene categories. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. le gall d. mpeg a video compression standard for multimedia applications. communications of the acm leclerc y. g. constructing simple stable descriptions for image partitioning. international journal of computer vision lecun y. huang f. j. and bottou l. learning methods for generic object recognition with invariance to pose and lighting. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. lee j. chai j. reitsma p. s. a. hodgins j. k. and pollard n. s. interactive control of avatars animated with human motion data. acm transactions on graphics computer vision algorithms and applications draft lee m.-c. ge chen w. lung bruce lin c. gu c. markoc t. zabinsky s. i. and szeliski r. a layered video object coding system using sprite and affine motion model. ieee transactions on circuits and systems for video technology lee m. e. and redner r. a. a note on the use of nonlinear filtering in computer graphics. ieee computer graphics and applications lee m. w. and cohen i. a model-based approach for estimating human poses ieee transactions on pattern analysis and machine intelligence in static images. lee s. wolberg g. and shin s. y. data interpolation using multilevel b-splines. ieee transactions on visualization and computer graphics lee s. wolberg g. chwa k.-y. and shin s. y. image metamorphosis with scattered feature constraints. ieee transactions on visualization and computer graphics lee y. d. terzopoulos d. and waters k. realistic facial modeling for animation. in acm siggraph conference proceedings pp. lei c. and yang y.-h. optical flow estimation on coarse-to-fine region-trees using discrete optimization. in twelfth international conference on computer vision kyoto japan. leibe b. and schiele b. analyzing appearance and contour based methods for object categorization. in ieee computer society conference on computer vision and pattern recognition pp. madison wi. leibe b. leonardis a. and schiele b. leaved categorization and segmentation. robust object detection with interinternational journal of computer vision leibe b. seemann e. and schiele b. pedestrian detection in crowded scenes. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. leibe b. cornelis n. cornelis k. and van gool l. dynamic scene analysis from a moving vehicle. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. leibowitz d. camera calibration and reconstruction of geometry from images. ph.d. thesis university of oxford. lempitsky v. and boykov y. global optimization for shape fitting. in ieee computer society conference on computer vision and pattern recognition references minneapolis mn. lempitsky v. and ivanov d. seamless mosaicing of image-based texture maps. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. lempitsky v. blake a. and rother c. image segmentation by branch-andmincut. in tenth european conference on computer vision pp. marseilles. lempitsky v. roth s. and rother. c. flowfusion discrete-continuous optimization for optical flow estimation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. lempitsky v. rother c. and blake a. logcut efficient graph cut optimization for markov random fields. in eleventh international conference on computer vision rio de janeiro brazil. lengyel j. and snyder j. rendering with coherent layers. in acm siggraph conference proceedings pp. los angeles. lensch h. p. a. kautz j. goesele m. heidrich w. and seidel h.-p. imagebased reconstruction of spatial appearance and geometric detail. acm transactions on graphics leonardis a. and bischof h. robust recognition using eigenimages. computer vision and image understanding leonardis a. jakli c a. and solina f. superquadrics for segmenting and modieee transactions on pattern analysis and machine intelligence eling range data. lepetit v. and fua p. monocular model-based tracking of rigid objects. foun dations and trends in computer graphics and computer vision lepetit v. pilet j. and fua p. point matching as a classification problem for fast and robust object pose estimation. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. lepetit v. pilet j. and fua p. keypoint recognition using randomized trees. ieee transactions on pattern analysis and machine intelligence leung t. k. burl m. c. and perona p. finding faces in cluttered scenes using random labeled graph matching. in fifth international conference on computer vision pp. cambridge massachusetts. levenberg k. a method for the solution of certain problems in least squares. quarterly of applied mathematics computer vision algorithms and applications draft levin a. blind motion deblurring using image statistics. in advances in neural information processing systems. levin a. and szeliski r. visual odometry and map correlation. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. levin a. and szeliski r. motion uncertainty and field of view. technical report microsoft research. levin a. and weiss y. learning to combine bottom-up and top-down segmenta tion. in ninth european conference on computer vision pp. levin a. and weiss y. user assisted separation of reflections from a single image using a sparsity prior. ieee transactions on pattern analysis and machine intelligence levin a. acha a. r. and lischinski d. spectral matting. ieee transactions on pattern analysis and machine intelligence levin a. lischinski d. and weiss y. colorization using optimization. acm transactions on graphics levin a. lischinski d. and weiss y. a closed form solution to natural image matting. ieee transactions on pattern analysis and machine intelligence levin a. zomet a. and weiss y. separating reflections from a single image using local features. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. levin a. fergus r. durand f. and freeman w. t. image and depth from a conventional camera with a coded aperture. acm transactions on graphics levin a. weiss y. durand f. and freeman b. understanding and evaluating blind deconvolution algorithms. in ieee computer society conference on computer vision and pattern recognition miami beach fl. levin a. zomet a. peleg s. and weiss y. seamless image stitching in the gradient domain. in eighth european conference on computer vision pp. prague. levoy m. display of surfaces from volume data. ieee computer graphics and applications levoy m. light fields and computational imaging. computer references levoy m. technical perspective computational photography on large collections of images. communications of the acm levoy m. and hanrahan p. light field rendering. in acm siggraph conference proceedings pp. new orleans. levoy m. and whitted t. the use of points as a display primitive. technical report university of north carolina at chapel hill. levoy m. ng r. adams a. footer m. and horowitz m. croscopy. acm transactions on graphics light field mi levoy m. pulli k. curless b. rusinkiewicz s. koller d. et al. the digital michelangelo project scanning of large statues. in acm siggraph conference proceedings pp. lew m. s. sebe n. djeraba c. and jain r. content-based multimedia information retrieval state of the art and challenges. acm transactions on multimedia computing communications and applications leyvand t. cohen-or d. dror g. and lischinski d. data-driven enhancement of facial attractiveness. acm transactions on graphics lhuillier m. and quan l. match propagation for image-based modeling and rendering. ieee transactions on pattern analysis and machine intelligence lhuillier m. and quan l. a quasi-dense approach to surface reconstruction from uncalibrated images. ieee transactions on pattern analysis and machine intelligence li h. and hartley r. the registration problem revisited. in eleventh international conference on computer vision rio de janeiro brazil. li l.-j. and fei-fei l. optimol automatic object picture collection via incre mental model learning. international journal of computer vision li s. markov random field modeling in computer vision. springer-verlag. li s. z. and jain a. k. handbook of face recognition springer. li x. wu c. zach c. lazebnik s. and frahm j.-m. modeling and recogin tenth european nition of landmark image collections using iconic scene graphs. conference on computer vision pp. marseilles. li y. and huttenlocher d. p. learning for optical flow using stochastic optimization. in tenth european conference on computer vision pp. marseilles. computer vision algorithms and applications draft li y. crandall d. j. and huttenlocher d. p. landmark classification in largescale image collections. in twelfth international conference on computer vision kyoto japan. li y. wang t. and shum h.-y. motion texture a two-level statistical model for character motion synthesis. acm transactions on graphics li y. shum h.-y. tang c.-k. and szeliski r. stereo reconstruction from multiperspective panoramas. ieee transactions on pattern analysis and machine intelligence li y. sun j. tang c.-k. and shum h.-y. lazy snapping. acm transactions on graphics siggraph liang l. xiao r. wen f. and sun j. face alignment via component-based discriminative search. in tenth european conference on computer vision pp. marseilles. liang l. liu c. xu y.-q. guo b. and shum h.-y. real-time texture synthesis by patch-based sampling. acm transactions on graphics liebowitz d. and zisserman a. metric rectification for perspective images of planes. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. lim j. two-dimensional signal and image processing. prentice-hall engle wood nj. lim j. j. arbel aez p. gu c. and malik j. context by region ancestry. in twelfth international conference on computer vision kyoto japan. lin d. kapoor a. hua g. and baker s. joint people event and location recognition in personal photo collections using cross-domain context. in eleventh european conference on computer vision heraklion crete. lin w.-c. hays j. wu c. kwatra v. and liu y. quantitative evaluation of in ieee computer society conference on near regular texture synthesis algorithms. computer vision and pattern recognition pp. new york city ny. lindeberg t. scale-space for discrete signals. ieee transactions on pattern analysis and machine intelligence lindeberg t. detecting salient blob-like image structures and their scales with a scale-space primal sketch a method for focus-of-attention. international journal of computer vision references lindeberg t. scale-space theory a basic tool for analysing structures at different scales. journal of applied statistics lindeberg t. edge detection and ridge detection with automatic scale selection. international journal of computer vision lindeberg t. feature detection with automatic scale selection. international journal of computer vision lindeberg t. and garding j. shape-adapted smoothing in estimation of shape cues from affine deformations of local brightness structure. image and vision computing lippman a. movie maps an application of the optical videodisc to computer graphics. computer graphics lischinski d. farbman z. uyttendaele m. and szeliski r. interactive local adjustment of tonal values. acm transactions on graphics siggraph lischinski d. farbman z. uyttendaele m. and szeliski r. interactive local adjustment of tonal values. acm transactions on graphics litvinov a. and schechner y. y. radiometric framework for image mosaicking. journal of the optical society of america a litwinowicz p. processing images and video for an impressionist effect. in acm siggraph conference proceedings pp. litwinowicz p. and williams l. animating images with drawings. in acm siggraph conference proceedings pp. liu c. beyond pixels exploring new representations and applications for mo tion analysis. ph.d. thesis massachusetts institute of technology. liu c. yuen j. and torralba a. nonparametric scene parsing label transfer via dense scene alignment. in ieee computer society conference on computer vision and pattern recognition miami beach fl. liu c. freeman w. t. adelson e. and weiss y. human-assisted motion in ieee computer society conference on computer vision and pattern annotation. recognition anchorage ak. liu c. szeliski r. kang s. b. zitnick c. l. and freeman w. t. automatic estimation and removal of noise from a single image. ieee transactions on pattern analysis and machine intelligence computer vision algorithms and applications draft liu f. gleicher m. jin h. and agarwala a. content-preserving warps for video stabilization. acm transactions on graphics liu x. chen t. and kumar b. v. face authentication for multiple subjects using eigenflow. pattern recognition liu y. collins r. t. and tsin y. a computational model for periodic pattern perception based on frieze and wallpaper groups. ieee transactions on pattern analysis and machine intelligence liu y. lin w.-c. and hays j. near-regular texture analysis and manipulation. acm transactions on graphics livingstone m. vision and art the biology of seeing. abrams new york. lobay a. and forsyth d. a. shape from texture without boundaries. international journal of computer vision lombaert h. sun y. grady l. and xu c. a multilevel banded graph cuts method for fast image segmentation. in tenth international conference on computer vision pp. beijing china. longere p. delahunt p. b. zhang x. and brainard d. h. perceptual assessment of demosaicing algorithm performance. proceedings of the ieee longuet-higgins h. c. a computer algorithm for reconstructing a scene from two projections. nature loop c. and zhang z. computing rectifying homographies for stereo vision. in ieee computer society conference on computer vision and pattern recognition pp. fort collins. lorensen w. e. and cline h. e. marching cubes a high resolution surface construction algorithm. computer graphics lorusso a. eggert d. and fisher r. b. a comparison of four algorithms for estimating rigid transformations. in british machine vision conference pp. birmingham england. lourakis m. i. a. and argyros a. a. sba a software package for generic sparse bundle adjustment. acm transactions on mathematical software lowe d. g. organization of smooth image curves at multiple scales. in second international conference on computer vision pp. tampa. lowe d. g. organization of smooth image curves at multiple scales. international journal of computer vision references lowe d. g. object recognition from local scale-invariant features. in seventh international conference on computer vision pp. kerkyra greece. lowe d. g. distinctive image features from scale-invariant keypoints. interna tional journal of computer vision lucas b. d. and kanade t. an iterative image registration technique with an in seventh international joint conference on artificial application in stereo vision. intelligence pp. vancouver. luong q.-t. and faugeras o. d. the fundamental matrix theory algorithms and stability analysis. international journal of computer vision luong q.-t. and vi eville t. canonical representations for the geometries of multiple projective views. computer vision and image understanding lyu s. and simoncelli e. nonlinear image representation using divisive norin ieee computer society conference on computer vision and pattern malization. recognition anchorage ak. lyu s. and simoncelli e. modeling multiscale subbands of photographic images ieee transactions on pattern analysis and with fields of gaussian scale mixtures. machine intelligence ma w.-c. jones a. chiang j.-y. hawkins t. frederiksen s. peers p. vukovic m. ouhyoung m. and debevec p. facial performance synthesis using deformation-driven polynomial displacement maps. acm transactions on graphics ma y. derksen h. hong w. and wright j. segmentation of multivariate mixed ieee transactions on pattern analysis data via lossy data coding and compression. and machine intelligence macdonald l. digital heritage applying digital imaging to cultural heritage butterworth-heinemann. madsen k. nielsen h. b. and tingleff o. methods for non-linear least squares problems. informatics and mathematical modelling technical university of denmark maes f. collignon a. vandermeulen d. marchal g. and suetens p. multimodality image registration by maximization of mutual information. ieee transactions on medical imaging magnor m. video-based rendering. a. k. peters wellesley ma. computer vision algorithms and applications draft magnor m. and girod b. data compression for light-field rendering. ieee transactions on circuits and systems for video technology magnor m. ramanathan p. and girod b. multi-view coding for image-based rendering using scene geometry. ieee transactions on circuits and systems for video technology mahajan d. huang f.-c. matusik w. ramamoorthi r. and belhumeur p. moving gradients a path-based method for plausible image interpolation. acm transactions on graphics maimone m. cheng y. and matthies l. two years of visual odometry on the mars exploration rovers. journal of field robotics maire m. arbelaez p. fowlkes c. and malik j. and localize junctions in natural images. computer vision and pattern recognition anchorage ak. using contours to detect in ieee computer society conference on maitin-shepard j. cusumano-towner m. lei j. and abbeel p. cloth grasp point detection based on multiple-view geometric cues with application to robotic towel folding. in ieee international conference on robotics and automation anchorage ak. maitre m. shinagawa y. and do m. n. symmetric multi-view stereo reconstruction from planar camera arrays. in ieee computer society conference on computer vision and pattern recognition anchorage ak. maji s. berg a. and malik j. classification using intersection kernel support vector machines is efficient. in ieee computer society conference on computer vision and pattern recognition anchorage ak. malik j. and rosenholtz r. computing local surface orientation and shape from texture for curved surfaces. international journal of computer vision malik j. belongie s. leung t. and shi j. contour and texture analysis for image segmentation. international journal of computer vision malisiewicz t. and efros a. a. recognition by association via learning perin ieee computer society conference on computer vision and exemplar distances. pattern recognition anchorage ak. malladi r. sethian j. a. and vemuri b. c. shape modeling with front propagation. ieee transactions on pattern analysis and machine intelligence mallat s. g. a theory for multiresolution signal decomposition the wavelet representation. ieee transactions on pattern analysis and machine intelligence references malvar h. s. lapped transforms for efficient transformsubband coding. ieee transactions on acoustics speech and signal processing malvar h. s. biorthogonal and nonuniform lapped transforms for transform coding with reduced blocking and ringing artifacts. ieee transactions on signal processing malvar h. s. fast progressive image coding without wavelets. in ieee data compressions conference pp. snowbird ut. malvar h. s. he l.-w. and cutler r. demosaicing of bayer-patterned color images. acoustics speech and signal processing pp. montreal. high-quality linear interpolation for in ieee international conference on mancini t. a. and wolff l. b. shape and light source location from depth and in ieee computer society conference on computer vision and pattern reflectance. recognition pp. champaign illinois. manjunathi b. s. and ma w. y. texture features for browsing and retrieval of image data. ieee transactions on pattern analysis and machine intelligence mann s. and picard r. w. virtual bellows constructing high-quality images from video. in first ieee international conference on image processing pp. austin. mann s. and picard r. w. on being undigital with digital cameras extending dynamic range by combining differently exposed pictures. in ist s annual conference pp. washington d. c. manning c. d. raghavan p. and sch utze h. introduction to information re trieval. cambridge university press. marquardt d. w. an algorithm for least-squares estimation of nonlinear parame ters. journal of the society for industrial and applied mathematics marr d. vision a computational investigation into the human representation and processing of visual information. w. h. freeman san francisco. marr d. and hildreth e. theory of edge detection. proceedings of the royal society of london b marr d. and nishihara h. k. representation and recognition of the spatial orga nization of three-dimensional shapes. proc. roy. soc. london b marr d. and poggio t. cooperative computation of stereo disparity. science computer vision algorithms and applications draft marr d. c. and poggio t. a computational theory of human stereo vision. pro ceedings of the royal society of london b marroquin j. mitter s. and poggio t. probabilistic solution of ill-posed problems in computational vision. in image understanding workshop pp. miami beach. marroquin j. mitter s. and poggio t. probabilistic solution of ill-posed problems in computational vision. journal of the american statistical association marroquin j. l. design of cooperative networks. working paper artificial intelligence laboratory massachusetts institute of technology. martin d. fowlkes c. and malik j. learning to detect natural image boundaries using local brightness color and texture cues. ieee transactions on pattern analysis and machine intelligence martin d. fowlkes c. tal d. and malik j. a database of human segmented natural images and its application to evaluating segmentation algorithms and measuring in eighth international conference on computer vision ecological statistics. pp. vancouver canada. martin w. n. and aggarwal j. k. volumetric description of objects from multiple views. ieee transactions on pattern analysis and machine intelligence martinec d. and pajdla t. robust rotation and translation estimation in multiview reconstruction. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. massey m. and bender w. salient stills process and practice. ibm systems journal matas j. chum o. urban m. and pajdla t. robust wide baseline stereo from maximally stable extremal regions. image and vision computing matei b. c. and meer p. estimation of nonlinear errors-in-variables models for ieee transactions on pattern analysis and machine computer vision applications. intelligence matsushita y. and lin s. radiometric calibration from noise distributions. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. matsushita y. ofek e. ge w. tang x. and shum h.-y. full-frame video stabilization with motion inpainting. ieee transactions on pattern analysis and machine references intelligence matthews i. and baker s. active appearance models revisited. international journal of computer vision matthews i. xiao j. and baker s. vs. deformable face models representational power construction and real-time fitting. international journal of computer vision matthies l. kanade t. and szeliski r. estimating depth from image sequences. kalman filter-based algorithms for international journal of computer vision matusik w. buehler c. and mcmillan l. polyhedral visual hulls for real-time in eurographics workshop on rendering techniques pp. rendering. london. matusik w. buehler c. raskar r. gortler s. j. and mcmillan l. image-based visual hulls. in acm siggraph conference proceedings pp. mayhew j. e. w. and frisby j. p. the computation of binocular edges. perception mayhew j. e. w. and frisby j. p. psychophysical and computational studies towards a theory of human stereopsis. artificial intelligence mccamy c. s. marcus h. and davidson j. g. a color-rendition chart. journal of applied photogrammetric engineering mccane b. novins k. crannitch d. and galvin b. on benchmarking optical flow. computer vision and image understanding mcguire m. matusik w. pfister h. hughes j. f. and durand f. defocus video matting. acm transactions on graphics siggraph mcinerney t. and terzopoulos d. a finite element model for shape reconstruction and nonrigid motion tracking. in fourth international conference on computer vision pp. berlin germany. mcinerney t. and terzopoulos d. deformable models in medical image analysis a survey. medical image analysis mcinerney t. and terzopoulos d. for medical image volume segmentation. topology adaptive deformable surfaces ieee transactions on medical imaging mcinerney t. and terzopoulos d. t-snakes topology adaptive snakes. medical image analysis computer vision algorithms and applications draft mclauchlan p. f. a batchrecursive algorithm for scene reconstruction. in ieee computer society conference on computer vision and pattern recognition pp. hilton head island. mclauchlan p. f. and jaenicke a. image mosaicing using sequential bundle adjustment. image and vision computing mclean g. f. and kotturi d. vanishing point detection by line clustering. ieee transactions on pattern analysis and machine intelligence mcmillan l. and bishop g. plenoptic modeling an image-based rendering system. in acm siggraph conference proceedings pp. mcmillan l. and gortler s. image-based rendering a new interface between computer vision and computer graphics. computer graphics meehan j. panoramic photography. watson-guptill. meer p. and georgescu b. edge detection with embedded confidence. transactions on pattern analysis and machine intelligence ieee meil a m. and shi j. learning segmentation by random walks. in advances in neural information processing systems. meil a m. and shi j. a random walks view of spectral segmentation. in workshop on artificial intelligence and statistics pp. key west fl. meltzer j. and soatto s. edge descriptors for robust wide-baseline correspondence. in ieee computer society conference on computer vision and pattern recognition anchorage ak. meltzer t. yanover c. and weiss y. globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation. in tenth international conference on computer vision pp. beijing china. m emin e. and p erez p. hierarchical estimation and segmentation of dense motion fields. international journal of computer vision menet s. saint-marc p. and medioni g. active contour models overview implementation and applications. in ieee international conference on systems man and cybernetics pp. los angeles. menet s. saint-marc p. and medioni g. b-snakes implementation and appli cations to stereo. in image understanding workshop pp. pittsburgh. merrell p. akbarzadeh a. wang l. mordohai p. frahm j.-m. yang r. nister d. and pollefeys m. real-time visibility-based fusion of depth maps. in eleventh international conference on computer vision rio de janeiro brazil. references mertens t. kautz j. and reeth f. v. exposure fusion. in proceedings of pacific graphics pp. metaxas d. and terzopoulos d. dynamic deformation of solid primitives with constraints. acm transactions on graphics siggraph metropolis n. rosenbluth a. w. rosenbluth m. n. teller a. h. and teller e. journal of chemical equations of state calculations by fast computing machines. physics meyer c. d. matrix analysis and applied linear algebra. society for industrial and applied mathematics philadephia. meyer y. wavelets algorithms and applications. society for industrial and applied mathematics philadephia. mikolajczyk k. and schmid c. scale affine invariant interest point detectors. international journal of computer vision mikolajczyk k. and schmid c. a performance evaluation of local descriptors. ieee transactions on pattern analysis and machine intelligence mikolajczyk k. schmid c. and zisserman a. human detection based on a probabilistic assembly of robust part detectors. in eighth european conference on computer vision pp. prague. mikolajczyk k. tuytelaars t. schmid c. zisserman a. matas j. schaffalitzky f. kadir t. and van gool l. j. a comparison of affine region detectors. international journal of computer vision milgram d. l. computer methods for creating photomosaics. ieee transactions on computers milgram d. l. adaptive techniques for photomosaicking. ieee transactions on computers miller i. campbell m. huttenlocher d. kline f.-r. nathan a. et al. team cornell s skynet robust perception and planning in an urban environment. journal of field robotics mitiche a. and bouthemy p. computation and analysis of image motion a synopsis of current problems and methods. international journal of computer vision mitsunaga t. and nayar s. k. radiometric self calibration. in ieee computer society conference on computer vision and pattern recognition pp. fort collins. computer vision algorithms and applications draft mittal a. and davis l. s. and tracking people in a cluttered scene. tracker a multi-view approach to segmenting international journal of computer vision mi cu s k b. and ko seck a j. piecewise planar city modeling from street view panoramic sequences. in ieee computer society conference on computer vision and pattern recognition miami beach fl. mi cu s k b. wildenauer h. and ko seck a j. detection and matching of rectilinear in ieee computer society conference on computer vision and pattern structures. recognition anchorage ak. moeslund t. b. and granum e. a survey of computer vision-based human motion capture. computer vision and image understanding moeslund t. b. hilton a. and kr uger v. a survey of advances in visionbased human motion capture and analysis. computer vision and image understanding moezzi s. katkere a. kuramura d. and jain r. reality modeling and visualization from multiple video sequences. ieee computer graphics and applications moghaddam b. and pentland a. probabilistic visual learning for object representation. ieee transactions on pattern analysis and machine intelligence moghaddam b. jebara t. and pentland a. bayesian face recognition. pattern recognition mohan a. papageorgiou c. and poggio t. example-based object detection in images by components. ieee transactions on pattern analysis and machine intelligence m oller k. d. optics. university science books mill valley ca. montemerlo m. becker j. bhat s. dahlkamp h. dolgov d. et al. junior the stanford entry in the urban challenge. journal of field robotics moon p. and spencer d. e. the photic field. mit press cambridge mas sachusetts. moons t. van gool l. and vergauwen m. reconstruction from multiple images. foundations and trends in computer graphics and computer vision moosmann f. nowak e. and jurie f. randomized clustering forests for imieee transactions on pattern analysis and machine intelligence age classification. references moravec h. towards automatic visual obstacle avoidance. in fifth international joint conference on artificial intelligence p. cambridge massachusetts. moravec h. the stanford cart and the cmu rover. proceedings of the ieee moreno-noguer f. lepetit v. and fua p. accurate non-iterative on solution to the pnp problem. in eleventh international conference on computer vision rio de janeiro brazil. mori g. guiding model search using segmentation. in tenth international con ference on computer vision pp. beijing china. mori g. ren x. efros a. and malik j. recovering human body configurations combining segmentation and recognition. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. mori m. the uncanny valley. energy httpwww.androidscience. morimoto c. and chellappa r. fast stabilization and mosaic construction. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. morita t. and kanade t. a sequential factorization method for recovering shape and motion from image streams. ieee transactions on pattern analysis and machine intelligence morris d. d. and kanade t. a unified factorization algorithm for points line in sixth international conference on segments and planes with uncertainty models. computer vision pp. bombay. morrone m. and burr d. feature detection in human vision a phase dependent energy model. proceedings of the royal society of london b mortensen e. n. vision-assisted image editing. computer graphics mortensen e. n. and barrett w. a. intelligent scissors for image composition. in acm siggraph conference proceedings pp. mortensen e. n. and barrett w. a. interactive segmentation with intelligent scissors. graphical models and image processing mortensen e. n. and barrett w. a. toboggan-based intelligent scissors with a four parameter edge model. in ieee computer society conference on computer vision and pattern recognition pp. fort collins. computer vision algorithms and applications draft mueller p. zeng g. wonka p. and van gool l. image-based procedural mod eling of facades. acm transactions on graphics m uhlich m. and mester r. the role of total least squares in motion analysis. in fifth european conference on computer vision pp. freiburg germany. muja m. and lowe d. g. fast approximate nearest neighbors with automatic algorithm configuration. in international conference on computer vision theory and applications lisbon portugal. mumford d. and shah j. optimal approximations by piecewise smooth functions and variational problems. comm. pure appl. math. munder s. and gavrila d. m. an experimental study on pedestrian classification. ieee transactions on pattern analysis and machine intelligence mundy j. l. object recognition in the geometric era a retrospective. in ponce j. hebert m. schmid c. and zisserman a. toward category-level object recognition pp. springer new york. mundy j. l. and zisserman a. geometric invariance in computer vision. mit press cambridge massachusetts. murase h. and nayar s. k. visual learning and recognition of objects from appearance. international journal of computer vision murphy e. p. a testing procedure to characterize color and spatial quality of digital cameras used to image cultural heritage. master s thesis rochester institute of technology. murphy k. torralba a. and freeman w. t. using the forest to see the trees a graphical model relating features objects and scenes. in advances in neural information processing systems. murphy-chutorian e. and trivedi m. m. head pose estimation in computer ieee transactions on pattern analysis and machine intelligence vision a survey. murray r. m. li z. x. and sastry s. s. a mathematical introduction to robotic manipulation. crc press. mutch j. and lowe d. g. object class recognition and localization using sparse features with limited receptive fields. international journal of computer vision references nagel h. h. image sequences ten years from phenomenology towards in eighth international conference on pattern recognition a theoretical foundation. pp. paris. nagel h.-h. and enkelmann w. an investigation of smoothness constraints for the estimation of displacement vector fields from image sequences. ieee transactions on pattern analysis and machine intelligence nakamura y. matsuura t. satoh k. and ohta y. occlusion detectable stereo occlusion patterns in camera matrix. in ieee computer society conference on computer vision and pattern recognition pp. san francisco. nakao t. kashitani a. and kaneyoshi a. scanning a document with a small camera attached to a mouse. in ieee workshop on applications of computer vision pp. princeton. nalwa v. s. edge-detector resolution improvement by image interpolation. ieee transactions on pattern analysis and machine intelligence nalwa v. s. a guided tour of computer vision. addison-wesley reading ma. nalwa v. s. and binford t. o. on detecting edges. ieee transactions on pattern analysis and machine intelligence narasimhan s. g. and nayar s. k. enhancing resolution along multiple imaging dimensions using assorted pixels. ieee transactions on pattern analysis and machine intelligence narayanan p. rander p. and kanade t. constructing virtual worlds using dense in sixth international conference on computer vision pp. stereo. bombay. nayar s. watanabe m. and noguchi m. real-time focus range sensor. in fifth international conference on computer vision pp. cambridge massachusetts. nayar s. k. computational cameras redefining the image. computer nayar s. k. and branzoi v. adaptive dynamic range imaging optical control of pixel exposures over space and time. in ninth international conference on computer vision pp. nice france. nayar s. k. and mitsunaga t. high dynamic range imaging spatially varying pixel exposures. in ieee computer society conference on computer vision and pattern recognition pp. hilton head island. computer vision algorithms and applications draft nayar s. k. and nakagawa y. shape from focus. ieee transactions on pattern analysis and machine intelligence nayar s. k. ikeuchi k. and kanade t. shape from interreflections. interna tional journal of computer vision nayar s. k. watanabe m. and noguchi m. real-time focus range sensor. ieee transactions on pattern analysis and machine intelligence negahdaripour s. revised definition of optical flow integration of radiometric and geometric cues for dynamic scene analysis. ieee transactions on pattern analysis and machine intelligence nehab d. rusinkiewicz s. davis j. and ramamoorthi r. efficiently combining positions and normals for precise geometry. acm transactions on graphics siggraph nene s. and nayar s. k. a simple algorithm for nearest neighbor search in ieee transactions on pattern analysis and machine intelligence high dimensions. nene s. a. nayar s. k. and murase h. columbia object image library technical report department of computer science columbia university. netravali a. and robbins j. motion-compensated television coding part bell system tech. nevatia r. a color edge detector and its use in scene segmentation. ieee trans actions on systems man and cybernetics nevatia r. and binford t. description and recognition of curved objects. artifi cial intelligence ng a. y. jordan m. i. and weiss y. on spectral clustering analysis and an algorithm. in advances in neural information processing systems pp. ng r. fourier slice photography. acm transactions on graphics sig graph ng r. levoy m. br eedif m. duval g. horowitz m. and hanrahan p. light field photography with a hand-held plenoptic camera. technical report cstr stanford university. nielsen m. florack l. m. j. and deriche r. regularization scale-space and edge-detection filters. journal of mathematical imaging and vision references nielson g. m. scattered data modeling. ieee computer graphics and applica tions nir t. bruckstein a. m. and kimmel r. over-parameterized variational optical flow. international journal of computer vision nishihara h. k. practical real-time imaging stereo matcher. opteng nist er d. preemptive ransac for live structure and motion estimation. in ninth international conference on computer vision pp. nice france. nist er d. an efficient solution to the five-point relative pose problem. ieee transactions on pattern analysis and machine intelligence nist er d. and stew enius h. scalable recognition with a vocabulary tree. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. nist er d. and stew enius h. linear time maximally stable extremal regions. in tenth european conference on computer vision pp. marseilles. nist er d. naroditsky o. and bergen j. visual odometry for ground vehicle applications. journal of field robotics noborio h. fukada s. and arimoto s. construction of the octree approximating three-dimensional objects by using multiple views. ieee transactions on pattern analysis and machine intelligence nocedal j. and wright s. j. numerical optimization. springer new york second edition. nomura y. zhang l. and nayar s. k. scene collages and flexible camera arrays. in eurographics symposium on rendering. nordstr om n. biased anisotropic diffusion a unified regularization and diffusion approach to edge detection. image and vision computing nowak e. jurie f. and triggs b. sampling strategies for bag-of-features image classification. in ninth european conference on computer vision pp. obdr z alek s. and matas j. object recognition using local affine frames on maximally stable extremal regions. in ponce j. hebert m. schmid c. and zisserman a. toward category-level object recognition pp. springer new york. computer vision algorithms and applications draft oh b. m. chen m. dorsey j. and durand f. image-based modeling and photo editing. in acm siggraph conference proceedings pp. ohlander r. price k. and reddy d. r. picture segmentation using a recursive region splitting method. computer graphics and image processing ohta y. and kanade t. stereo by intra- and inter-scanline search using dynamic programming. ieee transactions on pattern analysis and machine intelligence ohtake y. belyaev a. alexa m. turk g. and seidel h.-p. multi-level partition of unity implicits. acm transactions on graphics siggraph okutomi m. and kanade t. a locally adaptive window for signal matching. international journal of computer vision okutomi m. and kanade t. a multiple baseline stereo. ieee transactions on pattern analysis and machine intelligence okutomi m. and kanade t. a stereo matching algorithm with an adaptive winieee transactions on pattern analysis and machine dow theory and experiment. intelligence oliensis j. the least-squares error for structure from infinitesimal motion. inter national journal of computer vision oliensis j. and hartley r. convergence and nonconvergence. chine intelligence iterative extensions of the sturmtriggs algorithm ieee transactions on pattern analysis and ma oliva a. and torralba a. modeling the shape of the scene a holistic representation of the spatial envelope. international journal of computer vision oliva a. and torralba a. the role of context in object recognition. trends in cognitive sciences olsson c. eriksson a. p. and kahl f. improved spectral relaxation methods for binary quadratic optimization problems. computer vision and image understanding omer i. and werman m. image specific color representation. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. color lines ong e.-j. micilotta a. s. bowden r. and hilton a. viewpoint invariant exemplar-based human tracking. computer vision and image understanding references opelt a. pinz a. and zisserman a. a boundary-fragment-model for object detection. in ninth european conference on computer vision pp. opelt a. pinz a. fussenegger m. and auer p. generic object recognition with boosting. ieee transactions on pattern analysis and machine intelligence opengl-arb. opengl reference manual the official reference document to opengl version addison-wesley reading ma edition. oppenheim a. v. and schafer a. s. signals and systems. prentice hall engle wood cliffs new jersey edition. oppenheim a. v. schafer r. w. and buck j. r. discrete-time signal process ing. prentice hall englewood cliffs new jersey edition. oren m. and nayar s. a theory of specular surface geometry. international journal of computer vision o rourke j. and badler n. i. model-based image analysis of human motion using ieee transactions on pattern analysis and machine intelli constraint propagation. gence osher s. and paragios n. geometric level set methods in imaging vision and graphics springer. osuna e. freund r. and girosi f. training support vector machines an application to face detection. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. o toole a. j. jiang f. roark d. and abdi h. predicting human face recognition. in zhao w.-y. and chellappa r. face processing advanced methods and models elsevier. o toole a. j. phillips p. j. jiang f. ayyad j. p enard n. and abdi h. face recognition algorithms surpass humans matching faces over changes in illumination. ieee transactions on pattern analysis and machine intelligence ott m. lewis j. p. and cox i. j. teleconferencing eye contact using a virtual in interact and chi conference companion on human factors in camera. computing systems pp. amsterdam. otte m. and nagel h.-h. optical flow estimation advances and comparisons. in third european conference on computer vision pp. stockholm sweden. computer vision algorithms and applications draft oztireli c. guennebaud g. and gross m. feature preserving point set surfaces. computer graphics forum ozuysal m. calonder m. lepetit v. and fua p. fast keypoint recognition using random ferns. ieee transactions on pattern analysis and machine intelligence paglieroni d. w. distance transforms properties and machine vision applications. graphical models and image processing pal c. szeliski r. uyttendaele m. and jojic n. probability models for high dynamic range imaging. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. palmer s. e. vision science photons to phenomenology. the mit press cam bridge massachusetts. pankanti s. bolle r. m. and jain a. k. biometrics the future of identification. computer papageorgiou c. and poggio t. a trainable system for object detection. interna tional journal of computer vision papandreou g. and maragos p. adaptive and constrained algorithms for inverse compositional active appearance model fitting. in ieee computer society conference on computer vision and pattern recognition anchorage ak. papenberg n. bruhn a. brox t. didas s. and weickert j. highly accurate international journal of optic flow computation with theoretically justified warping. computer vision papert s. artificial the summer vision project. intelligence group massachusetts technical report aiminstitute of technology. paragios n. and deriche r. geodesic active contours and level sets for the deieee transactions on pattern analysis and tection and tracking of moving objects. machine intelligence paragios n. and sgallari f. special issue on scale space and variational methods in computer vision. international journal of computer vision paragios n. faugeras o. d. chan t. and schn orr c. third international workshop on variational geometric and level set methods in computer vision springer. paris s. and durand f. a fast approximation of the bilateral filter using a signal processing approach. in ninth european conference on computer vision pp. references paris s. and durand f. a topological approach to hierarchical segmentation using in ieee computer society conference on computer vision and pattern mean shift. recognition minneapolis mn. paris s. kornprobst p. tumblin j. and durand f. bilateral filtering theory and applications. foundations and trends in computer graphics and computer vision park m. brocklehurst k. collins r. t. and liu y. deformed lattice detection in real-world images using mean-shift belief propagation. ieee transactions on pattern analysis and machine intelligence park s. c. park m. k. and kang m. g. super-resolution image reconstruction a technical overview. ieee signal processing magazine parke f. i. and waters k. computer facial animation. a k peters wellesley massachusetts. parker j. a. kenyon r. v. and troxel d. e. comparison of interpolating meth ods for image resampling. ieee transactions on medical imaging pattanaik s. n. ferwerda j. a. fairchild m. d. and greenberg d. p. a multiscale model of adaptation and spatial vision for realistic image display. in acm siggraph conference proceedings pp. orlando. pauly m. keiser r. kobbelt l. p. and gross m. shape modeling with point-sampled geometry. acm transactions on graphics siggraph pavlidis t. structural pattern recognition. springer-verlag berlin new york. pavlidis t. and liow y.-t. integrating region growing and edge detection. ieee transactions on pattern analysis and machine intelligence pavlovi c v. sharma r. and huang t. s. visual interpretation of hand gestures for human-computer interaction a review. ieee transactions on pattern analysis and machine intelligence pearl j. probabilistic reasoning in intelligent systems networks of plausible inference. morgan kaufmann publishers los altos. peleg r. ben-ezra m. and pritch y. omnistereo panoramic stereo imaging. ieee transactions on pattern analysis and machine intelligence peleg s. elimination of seams from photomosaics. computer vision graphics and image processing computer vision algorithms and applications draft peleg s. and herman j. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. panoramic mosaics by manifold projection. peleg s. and rav-acha a. lucas-kanade without iterative warping. in interna tional conference on image processing pp. atlanta. peleg s. rousso b. rav-acha a. and zomet a. mosaicing on adaptive manifolds. ieee transactions on pattern analysis and machine intelligence penev p. and atick j. local feature analysis a general statistical theory for object representation. network computation and neural systems pentland a. p. local shading analysis. ieee transactions on pattern analysis and machine intelligence pentland a. p. perceptual organization and the representation of natural form. artificial intelligence pentland a. p. a new sense for depth of field. ieee transactions on pattern analysis and machine intelligence pentland a. p. interpolation using wavelet bases. ieee transactions on pattern analysis and machine intelligence p erez p. blake a. and gangnet m. jetstream probabilistic contour extraction with particles. in eighth international conference on computer vision pp. vancouver canada. p erez p. gangnet m. and blake a. poisson image editing. acm transactions on graphics siggraph deformable kernels for early vision. perona p. analysis and machine intelligence ieee transactions on pattern perona p. and malik j. detecting and localizing edges composed of steps peaks and roofs. in third international conference on computer vision pp. osaka japan. perona p. and malik j. scale space and edge detection using anisotropic diffusion. ieee transactions on pattern analysis and machine intelligence peters j. and reif u. subdivision surfaces. springer. petschnigg g. agrawala m. hoppe h. szeliski r. cohen m. and toyama k. digital photography with flash and no-flash image pairs. acm transactions on graphics siggraph references pfister h. zwicker m. van baar j. and gross m. surfels surface elements as rendering primitives. in acm siggraph conference proceedings pp. pflugfelder r. self-calibrating cameras in video surveillance. ph.d. thesis graz university of technology. philbin j. and zisserman a. object mining using a matching graph on very large in indian conference on computer vision graphics and image image collections. processing bhubaneswar india. philbin j. chum o. isard m. sivic j. and zisserman a. object retrieval with large vocabularies and fast spatial matching. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. philbin j. chum o. sivic j. isard m. and zisserman a. lost in quantization in ieee comimproving particular object retrieval in large scale image databases. puter society conference on computer vision and pattern recognition anchorage ak. phillips p. j. moon h. rizvi s. a. and rauss p. j. the feret evaluation methodology for face recognition algorithms. ieee transactions on pattern analysis and machine intelligence phillips p. j. scruggs w. t. o toole a. j. flynn p. j. bowyer k. w. et al. frvt and ice large-scale experimental results. ieee transactions on pattern analysis and machine intelligence phong b. t. illumination for computer generated pictures. communications of the acm pickup l. c. machine learning in multi-frame image super-resolution. ph.d. thesis university of oxford. pickup l. c. and zisserman a. automatic retrieval of visual continuity errors in movies. in acm international conference on image and video retrieval santorini greece. pickup l. c. capel d. p. roberts s. j. and zisserman a. overcoming registration uncertainty in image super-resolution maximize or marginalize? eurasip journal on advances in signal processing id pickup l. c. capel d. p. roberts s. j. and zisserman a. bayesian methods for image super-resolution. the computer journal pighin f. szeliski r. and salesin d. h. modeling and animating realistic faces from images. international journal of computer vision computer vision algorithms and applications draft pighin f. hecker j. lischinski d. salesin d. h. and szeliski r. synthesizing realistic facial expressions from photographs. in acm siggraph conference proceedings pp. orlando. pilet j. lepetit v. and fua p. fast non-rigid surface detection registration and realistic augmentation. international journal of computer vision pinz a. object categorization. foundations and trends in computer graphics and computer vision pizer s. m. amburn e. p. austin j. d. cromartie r. geselowitz a. et al. adaptive histogram equalization and its variations. computer vision graphics and image processing platel b. balmachnova e. florack l. and ter haar romeny b. top-points as interest points for image matching. in ninth european conference on computer vision pp. platt j. c. optimal filtering for patterned displays. ieee signal processing let ters pock t. unger m. cremers d. and bischof h. fast and exact solution of total variation models on the gpu. in cvpr workshop on visual computer vision on gpus anchorage ak. poelman c. j. and kanade t. a paraperspective factorization method for shape and motion recovery. ieee transactions on pattern analysis and machine intelligence poggio t. and koch c. ill-posed problems in early vision from computational theory to analogue networks. proceedings of the royal society of london b poggio t. gamble e. and little j. parallel integration of vision modules. sci ence poggio t. torre v. and koch c. computational vision and regularization theory. nature poggio t. little j. gamble e. gillet w. geiger d. et al. the mit vision machine. in image understanding workshop pp. boston. polana r. and nelson r. c. detection and recognition of periodic nonrigid motion. international journal of computer vision pollard s. b. mayhew j. e. w. and frisby j. p. pmf a stereo correspondence algorithm using a disparity gradient limit. perception references pollefeys m. and van gool l. from images to models. communications of the acm pollefeys m. nist er d. frahm j.-m. akbarzadeh a. mordohai p. et al. detailed real-time urban reconstruction from video. international journal of computer vision ponce j. hebert m. schmid c. and zisserman a. toward category-level object recognition springer new york. ponce j. berg t. everingham m. forsyth d. hebert m. et al. dataset issues in object recognition. in ponce j. hebert m. schmid c. and zisserman a. toward category-level object recognition pp. springer new york. pons j.-p. keriven r. and faugeras o. modelling dynamic scenes by registering multi-view image sequences. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. pons j.-p. keriven r. and faugeras o. multi-view stereo reconstruction and scene flow estimation with a global image-based matching score. international journal of computer vision porter t. and duff t. compositing digital images. computer graphics graph portilla j. and simoncelli e. p. a parametric texture model based on joint statistics of complex wavelet coefficients. international journal of computer vision portilla j. strela v. wainwright m. and simoncelli e. p. image denoising using scale mixtures of gaussians in the wavelet domain. ieee transactions on image processing potetz b. and lee t. s. efficient belief propagation for higher-order cliques using linear constraint nodes. computer vision and image understanding potmesil m. generating octree models of objects from their silhouettes in a sequence of images. computer vision graphics and image processing pratt w. k. edition. digital image processing. wiley-interscience hoboken nj prazdny k. detection of binocular disparities. biological cybernetics pritchett p. and zisserman a. wide baseline stereo matching. in sixth interna tional conference on computer vision pp. bombay. computer vision algorithms and applications draft proesmans m. van gool l. and defoort f. reading between the lines a method for extracting dynamic with texture. in sixth international conference on computer vision pp. bombay. protter m. and elad m. super resolution with probabilistic motion estimation. ieee transactions on image processing pullen k. and bregler c. motion capture assisted animation texturing and syn thesis. acm transactions on graphics pulli k. multiview registration for large data sets. in second international conference on digital imaging and modeling pp. ottawa canada. pulli k. abi-rached h. duchamp t. shapiro l. and stuetzle w. acquisition and visualization of colored objects. in international conference on pattern recognition pp. quack t. leibe b. and van gool l. world-scale mining of objects and events from community photo collections. in conference on image and video retrieval pp. niagara falls. quam l. h. hierarchical warp stereo. in image understanding workshop pp. new orleans. quan l. and lan z. linear n-point camera pose determination. ieee transac tions on pattern analysis and machine intelligence quan l. and mohr r. determining perspective structures using hierarchical hough transform. pattern recognition letters rabinovich a. vedaldi a. galleguillos c. wiewiora e. and belongie s. objects in context. in eleventh international conference on computer vision rio de janeiro brazil. rademacher p. and bishop g. multiple-center-of-projection images. in acm siggraph conference proceedings pp. orlando. raginsky m. and lazebnik s. locality-sensitive binary codes from shift-invariant kernels. in advances in neural information processing systems. raman s. and chaudhuri s. a matte-less variational approach to automatic scene compositing. in eleventh international conference on computer vision rio de janeiro brazil. raman s. and chaudhuri s. bilateral filter based compositing for variable expo sure photography. in proceedings of eurographics references ramanan d. and baker s. local distance functions a taxonomy new algorithms in twelfth international conference on computer vision and an evaluation. kyoto japan. ramanan d. forsyth d. and zisserman a. strike a pose tracking people by finding stylized poses. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. ramanarayanan g. and bala k. constrained texture synthesis via energy minimization. ieee transactions on visualization and computer graphics ramer u. an iterative procedure for the polygonal approximation of plane curves. computer graphics and image processing ramnath k. koterba s. xiao j. hu c. matthews i. baker s. cohn j. and kanade t. multi-view aam fitting and construction. international journal of computer vision raskar r. and tumblin j. computational photography mastering new tech niques for lenses lighting and sensors. a k peters wellesley massachusetts. raskar r. tan k.-h. feris r. yu j. and turk m. non-photorealistic camera depth edge detection and stylized rendering using multi-flash imaging. acm transactions on graphics rav-acha a. kohli p. fitzgibbon a. and rother c. unwrap mosaics a new representation for video editing. acm transactions on graphics rav-acha a. pritch y. lischinski d. and peleg s. dynamosaics video mosaics with non-chronological time. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. ravikumar p. agarwal a. and wainwright m. j. message-passing for graphstructured linear programs proximal projections convergence and rounding schemes. in international conference on machine learning pp. ray s. f. applied photographic optics. focal press oxford edition. rehg j. and kanade t. visual tracking of high dof articulated structures an application to human hand tracking. in third european conference on computer vision pp. stockholm sweden. rehg j. and witkin a. visual tracking with deformation models. in ieee inter national conference on robotics and automation pp. sacramento. rehg j. morris d. d. and kanade t. ticulated objects using two- and three-dimensional models. robotics research ambiguities in visual tracking of arinternational journal of computer vision algorithms and applications draft reichenbach s. e. park s. k. and narayanswamy r. characterizing digital image acquisition devices. optical engineering reinhard e. stark m. shirley p. and ferwerda j. photographic tone reproduction for digital images. acm transactions on graphics siggraph reinhard e. ward g. pattanaik s. and debevec p. high dynamic range imaging acquisition display and image-based lighting. morgan kaufmann. rhemann c. rother c. and gelautz m. improving color modeling for alpha matting. in british machine vision conference leeds. rhemann c. rother c. rav-acha a. and sharp t. high resolution matting via interactive trimap segmentation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. rhemann c. rother c. wang j. gelautz m. kohli p. and rott p. a perceptually motivated online benchmark for image matting. in ieee computer society conference on computer vision and pattern recognition miami beach fl. richardson i. e. g. and video compression video coding for next generation multimedia. wiley. rioul o. and vetterli m. wavelets and signal processing. ieee signal processing magazine rioux m. and bird t. white laser synced scan. ieee computer graphics and applications rioux m. bechthold g. taylor d. and duggan m. design of a large depth of view three-dimensional camera for robot vision. optical engineering riseman e. m. and arbib m. a. computational techniques in the visual segmen tation of static scenes. computer graphics and image processing ritter g. x. and wilson j. n. handbook of computer vision algorithms in image algebra. crc press boca raton edition. robert c. p. the bayesian choice from decision-theoretic foundations to computational implementation. springer-verlag new york. roberts l. g. machine perception of three-dimensional solids. in tippett j. t. borkowitz d. a. clapp l. c. koester c. j. and vanderburgh jr. a. optical and electro-optical information processing pp. mit press cambridge massachusetts. references robertson d. and cipolla r. an image-based system for urban navigation. in british machine vision conference pp. kingston. robertson d. p. and cipolla r. building architectural models from many views using map constraints. in seventh european conference on computer vision pp. copenhagen. robertson d. p. and cipolla r. architectural modelling. in varga m. practical image processing and computer vision john wiley. robertson n. and reid i. a general method for human activity recognition in video. computer vision and image understanding roble d. vision in film and special effects. computer graphics roble d. and zafar n. b. computer don t trust your eyes cutting-edge visual effects. rogez g. rihan j. ramalingam s. orrite c. and torr p. h. s. randomized trees for human pose detection. in ieee computer society conference on computer vision and pattern recognition anchorage ak. rogmans s. lu j. bekaert p. and lafruit g. real-time stereo-based views synthesis algorithms a unified framework and evaluation on commodity gpus. signal processing image communication rohr k. towards model-based recognition of human movements in image se quences. computer vision graphics and image processing rom an a. and lensch h. p. a. automatic multiperspective images. in euro graphics symposium on rendering pp. rom an a. garg g. and levoy m. interactive design of multi-perspective images for visualizing urban landscapes. in ieee visualization pp. minneapolis. romdhani s. and vetter t. efficient robust and accurate fitting of a morphable model. in ninth international conference on computer vision pp. nice france. romdhani s. torr p. h. s. sch olkopf b. and blake a. computationally efficient face detection. in eighth international conference on computer vision pp. vancouver canada. rosales r. and sclaroff s. inferring body pose without tracking body parts. in ieee computer society conference on computer vision and pattern recognition pp. hilton head island. computer vision algorithms and applications draft rosenfeld a. quadtrees and pyramids for pattern recognition and image processing. in fifth international conference on pattern recognition pp. miami beach. rosenfeld a. multiresolution image processing and analysis springer verlag new york. rosenfeld a. and davis l. s. image segmentation and image models. proceed ings of the ieee rosenfeld a. and kak a. c. digital picture processing. academic press new york. rosenfeld a. and pfaltz j. l. sequential operations in digital picture processing. journal of the acm rosenfeld a. hummel r. a. and zucker s. w. scene labeling by relaxation operations. ieee transactions on systems man and cybernetics rosten e. and drummond t. fusing points and lines for high performance tracking. in tenth international conference on computer vision pp. beijing china. rosten e. and drummond t. machine learning for high-speed corner detection. in ninth european conference on computer vision pp. roth s. and black m. j. on the spatial statistics of optical flow. international journal of computer vision roth s. and black m. j. steerable random fields. in eleventh international conference on computer vision rio de janeiro brazil. roth s. and black m. j. fields of experts. international journal of computer vision rother c. a new approach for vanishing point detection in architectural environ ments. image and vision computing rother c. linear multi-view reconstruction of points lines planes and cameras using a reference plane. in ninth international conference on computer vision pp. nice france. rother c. and carlsson s. linear multi view reconstruction and camera recovery using a reference plane. international journal of computer vision rother c. kolmogorov v. and blake a. grabcut interactive foreground extraction using iterated graph cuts. acm transactions on graphics siggraph references rother c. bordeaux l. hamadi y. and blake a. autocollage. acm transac tions on graphics rother c. kohli p. feng w. and jia j. minimizing sparse higher order energy in ieee computer society conference on computer functions of discrete variables. vision and pattern recognition miami beach fl. rother c. kolmogorov v. lempitsky v. and szummer m. optimizing binary mrfs via extended roof duality. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. rother c. kumar s. kolmogorov v. and blake a. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. digital tapestry. rothganger f. lazebnik s. schmid c. and ponce j. object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints. international journal of computer vision rousseeuw p. j. least median of squares regresssion. journal of the american statistical association rousseeuw p. j. and leroy a. m. robust regression and outlier detection. wiley new york. rousson m. and paragios n. prior knowledge level set representations and visual grouping. international journal of computer vision roweis s. em algorithms for pca and spca. in advances in neural information processing systems pp. rowland d. a. and perrett d. i. manipulating facial appearance through shape and color. ieee computer graphics and applications rowley h. a. baluja s. and kanade t. neural network-based face detection. ieee transactions on pattern analysis and machine intelligence rowley h. a. baluja s. and kanade t. rotation invariant neural networkbased face detection. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. roy s. and cox i. j. a maximum-flow formulation of the n-camera stereo correspondence problem. in sixth international conference on computer vision pp. bombay. rozenfeld s. shimshoni i. and lindenbaum m. dense mirroring surface recovery from homographies and sparse correspondences. in ieee computer society computer vision algorithms and applications draft conference on computer vision and pattern recognition minneapolis mn. rubner y. tomasi c. and guibas l. j. the earth mover s distance as a metric for image retrieval. international journal of computer vision rumelhart d. e. hinton g. e. and williams r. j. learning internal representations by error propagation. in rumelhart d. e. mcclelland j. l. and the pdp research group parallel distributed processing explorations in the microstructure of cognition pp. bradford books cambridge massachusetts. rusinkiewicz s. and levoy m. qsplat a multiresolution point rendering system for large meshes. in acm siggraph conference proceedings pp. russ j. c. the image processing handbook. crc press boca raton edition. russell b. efros a. sivic j. freeman w. and zisserman a. using multiple segmentations to discover objects and their extent in image collections. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. russell b. c. torralba a. murphy k. p. and freeman w. t. labelme a database and web-based tool for image annotation. international journal of computer vision russell b. c. torralba a. liu c. fergus r. and freeman w. t. object recog nition by scene alignment. in advances in neural information processing systems. ruzon m. a. and tomasi c. alpha estimation in natural images. in ieee computer society conference on computer vision and pattern recognition pp. hilton head island. ruzon m. a. and tomasi c. edge junction and corner detection using color distributions. ieee transactions on pattern analysis and machine intelligence ryan t. w. gray r. t. and hunt b. r. prediction of correlation errors in stereo pair images. optical engineering saad y. iterative methods for sparse linear systems. society for industrial and applied mathematics second edition. saint-marc p. chen j. s. and medioni g. adaptive smoothing a general tool for early vision. ieee transactions on pattern analysis and machine intelligence references saito h. and kanade t. shape reconstruction in projective grid space from large in ieee computer society conference on computer vision and number of images. pattern recognition pp. fort collins. samet h. the design and analysis of spatial data structures. addison-wesley reading massachusetts. sander p. t. and zucker s. w. inferring surface trace and differential structure from images. ieee transactions on pattern analysis and machine intelligence sapiro g. geometric partial differential equations and image analysis. cam bridge university press. sato y. and ikeuchi k. reflectance analysis for computer graphics model generation. graphical models and image processing sato y. wheeler m. and ikeuchi k. object shape and reflectance modeling in acm siggraph conference proceedings pp. from observation. los angeles. savarese s. and fei-fei l. generic object categorization localization and pose in eleventh international conference on computer vision estimation. rio de janeiro brazil. savarese s. and fei-fei l. view synthesis for recognizing unseen poses of object classes. in tenth european conference on computer vision pp. marseilles. savarese s. chen m. and perona p. local shape from mirror reflections. inter national journal of computer vision savarese s. andreetto m. rushmeier h. e. bernardini f. and perona p. reconstruction by shadow carving theory and practical evaluation. international journal of computer vision sawhney h. s. simplifying motion and structure analysis using planar parallax and image warping. in twelfth international conference on pattern recognition pp. jerusalem israel. sawhney h. s. and ayer s. compact representation of videos through dominant multiple motion estimation. ieee transactions on pattern analysis and machine intelligence sawhney h. s. and hanson a. r. identification and description of shallow environmental structure over a sequence of images. in ieee computer society con computer vision algorithms and applications draft ference on computer vision and pattern recognition pp. maui hawaii. sawhney h. s. and kumar r. true multi-image alignment and its application to mosaicing and lens distortion correction. ieee transactions on pattern analysis and machine intelligence sawhney h. s. kumar r. gendel g. bergen j. dixon d. and paragano v. videobrush experiences with consumer video mosaicing. in ieee workshop on applications of computer vision pp. princeton. sawhney h. s. arpa a. kumar r. samarasekera s. aggarwal m. hsu s. nister d. and hanna k. video flashlights real time rendering of multiple videos for immersive model visualization. in proceedings of the eurographics workshop on rendering pp. pisa italy. saxena a. sun m. and ng a. y. learning scene structure from a single still image. ieee transactions on pattern analysis and machine intelligence schaffalitzky f. and zisserman a. planar grouping for automatic detection of vanishing lines and points. image and vision computing schaffalitzky f. and zisserman a. multi-view matching for unordered image sets or how do i organize my holiday snaps? in seventh european conference on computer vision pp. copenhagen. scharr h. black m. j. and haussecker h. w. image statistics and anisotropic diffusion. in ninth international conference on computer vision pp. nice france. scharstein d. matching images by comparing their gradient fields. in twelfth international conference on pattern recognition pp. jerusalem israel. scharstein d. view synthesis using stereo vision. volume springer-verlag. scharstein d. and pal c. learning conditional random fields for stereo. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. scharstein d. and szeliski r. stereo matching with nonlinear diffusion. interna tional journal of computer vision scharstein d. and szeliski r. a taxonomy and evaluation of dense two-frame stereo correspondence algorithms. international journal of computer vision references scharstein d. and szeliski r. high-accuracy stereo depth maps using structured light. in ieee computer society conference on computer vision and pattern recognition pp. madison wi. schechner y. y. nayar s. k. and belhumeur p. n. multiplexing for optimal lighting. ieee transactions on pattern analysis and machine intelligence schindler g. brown m. and szeliski r. city-scale location recognition. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. schindler g. krishnamurthy p. lublinerman r. liu y. and dellaert f. detecting and matching repeated patterns for automatic geo-tagging in urban environments. in ieee computer society conference on computer vision and pattern recognition anchorage ak. schlesinger d. and flach b. transforming an arbitrary minsum problem into a binary one. technical report dresden university of technology. schlesinger m. i. syntactic analysis of two-dimensional visual signals in noisy conditions. kibernetika schlesinger m. i. and giginyak v. v. solution to structural recognition by their equivalent transformations part control systems and computers schlesinger m. i. and giginyak v. v. solution to structural recognition by their equivalent transformations part control systems and computers schmid c. and mohr r. local grayvalue invariants for image retrieval. ieee transactions on pattern analysis and machine intelligence schmid c. and zisserman a. automatic line matching across views. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. schmid c. mohr r. and bauckhage c. evaluation of interest point detectors. international journal of computer vision schneiderman h. and kanade t. object detection using the statistics of parts. international journal of computer vision sch odl a. and essa i. controlled animation of video sprites. in acm symposium on computater animation san antonio. computer vision algorithms and applications draft sch odl a. szeliski r. salesin d. h. and essa i. video textures. in acm siggraph conference proceedings pp. new orleans. schoenemann t. and cremers d. high resolution motion layer decomposition using dual-space graph cuts. in ieee computer society conference on computer vision and pattern recognition anchorage ak. sch olkopf b. and smola a. learning with kernels support vector machines regularization optimization and beyond. mit press cambridge massachusetts. schraudolph n. n. polynomial-time exact inference in np-hard binary mrfs via reweighted perfect matching. in international conference on artificial intelligence and statistics pp. schr oder p. and sweldens w. spherical wavelets efficiently representing functions on the sphere. in acm siggraph conference proceedings pp. schultz r. r. and stevenson r. l. extraction of high-resolution frames from video sequences. ieee transactions on image processing sclaroff s. and isidoro j. active blobs region-based deformable appearance models. computer vision and image understanding scott g. l. and longuet-higgins h. c. feature grouping by relocalization of eigenvectors of the proximity matrix. in british machine vision conference pp. sebastian t. b. and kimia b. b. curves vs. skeletons in object recognition. signal processing sederberg t. w. and parry s. r. free-form deformations of solid geometric models. computer graphics sederberg t. w. gao p. wang g. and mu h. shape blending an intrinsic solution to the vertex path problem. in acm siggraph conference proceedings pp. seitz p. using local orientation information as image primitive for robust object recognition. in spie vol. visual communications and image processing iv pp. seitz s. the space of all stereo images. in eighth international conference on computer vision pp. vancouver canada. seitz s. and szeliski r. applications of computer vision to computer graphics. computer graphics guest editors introduction to the special issue. references seitz s. curless b. diebel j. scharstein d. and szeliski r. a comparison and evaluation of multi-view stereo reconstruction algorithms. in ieee computer society conference on computer vision and pattern recognition pp. new york ny. seitz s. m. and baker s. filter flow. computer vision kyoto japan. in twelfth international conference on seitz s. m. and dyer c. m. view morphing. in acm siggraph confer ence proceedings pp. new orleans. seitz s. m. and dyer c. m. photorealistic scene reconstruction by voxel coloring. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. seitz s. m. and dyer c. m. photorealistic scene reconstruction by voxel coloring. international journal of computer vision seitz s. m. and dyer c. r. view invariant analysis of cyclic motion. international journal of computer vision serra j. image analysis and mathematical morphology. academic press new york. serra j. and vincent l. an overview of morphological filtering. circuits systems and signal processing serre t. wolf l. and poggio t. object recognition with features inspired by visual cortex. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. sethian j. level set methods and fast marching methods. cambridge university press cambridge edition. shade j. gortler s. he l. and szeliski r. layered depth images. in acm siggraph conference proceedings pp. orlando. shade j. lischinski d. salesin d. derose t. and snyder j. hierarchical images caching for accelerated walkthroughs of complex environments. in acm siggraph conference proceedings pp. new orleans. shafer s. a. using color to separate reflection components. color research and applications shafer s. a. healey g. and wolff l. physics-based vision principles and practice. jones bartlett cambridge ma. computer vision algorithms and applications draft shafique k. and shah m. a noniterative greedy algorithm for multiframe point ieee transactions on pattern analysis and machine intelligence correspondence. shah j. a nonlinear diffusion model for discontinuous disparity and half-occlusion in ieee computer society conference on computer vision and pattern in stereo. recognition pp. new york. shakhnarovich g. darrell t. and indyk p. nearest-neighbor methods in learning and vision theory and practice mit press. shakhnarovich g. viola p. and darrell t. fast pose estimation with parametersensitive hashing. in ninth international conference on computer vision pp. nice france. shan y. liu z. and zhang z. model-based bundle adjustment with application to face modeling. in eighth international conference on computer vision pp. vancouver canada. sharon e. galun m. sharon d. basri r. and brandt a. hierarchy and adap tivity in segmenting visual scenes. nature shashua a. and toelg s. the quadric reference surface theory and applications. international journal of computer vision shashua a. and wexler y. q-warping direct computation of quadratic reference surfaces. ieee transactions on pattern analysis and machine intelligence shaw d. and barnes n. perspective rectangle detection. in workshop on applica tions of computer vision at eccv shewchuk j. r. an introduction to the conjugate gradient method without the agonizing pain. unpublished manuscript available on author s homepage jrs. an earlier version appeared as a carnegie mellon university technical report shi j. and malik j. normalized cuts and image segmentation. ieee transactions on pattern analysis and machine intelligence shi j. and tomasi c. good features to track. in ieee computer society confer ence on computer vision and pattern recognition pp. seattle. shimizu m. and okutomi m. precise sub-pixel estimation on area-based matching. in eighth international conference on computer vision pp. vancouver canada. references shirley p. fundamentals of computer graphics. a k peters wellesley mas sachusetts second edition. shizawa m. and mase k. a unified computational theory of motion transparency and motion boundaries based on eigenenergy analysis. in ieee computer society conference on computer vision and pattern recognition pp. maui hawaii. shoemake k. animating rotation with quaternion curves. computer graphics shotton j. blake a. and cipolla r. contour-based learning for object detection. in tenth international conference on computer vision pp. beijing china. shotton j. johnson m. and cipolla r. semantic texton forests for image categorization and segmentation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. shotton j. winn j. rother c. and criminisi a. textonboost for image understanding multi-class object recognition and segmentation by jointly modeling appearance shape and context. international journal of computer vision shufelt j. performance evaluation and analysis of vanishing point detection techniques. ieee transactions on pattern analysis and machine intelligence shum h.-y. and he l.-w. rendering with concentric mosaics. in acm sig graph conference proceedings pp. los angeles. shum h.-y. and szeliski r. stereo reconstruction from multiperspective panoramas. in seventh international conference on computer vision pp. kerkyra greece. shum h.-y. and szeliski r. construction of panoramic mosaics with global and local alignment. international journal of computer vision erratum published july shum h.-y. chan s.-c. and kang s. b. image-based rendering. springer new york ny. shum h.-y. han m. and szeliski r. interactive construction of models from panoramic mosaics. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. shum h.-y. ikeuchi k. and reddy r. principal component analysis with missieee transactions on pattern ing data and its application to polyhedral modeling. computer vision algorithms and applications draft analysis and machine intelligence shum h.-y. kang s. b. and chan s.-c. survey of image-based representations ieee transactions on circuits and systems for video and compression techniques. technology shum h.-y. wang l. chai j.-x. and tong x. rendering by manifold hopping. international journal of computer vision shum h.-y. sun j. yamazaki s. li y. and tang c.-k. pop-up light field an interactive image-based modeling and rendering system. acm transactions on graphics sidenbladh h. and black m. j. learning the statistics of people in images and video. international journal of computer vision sidenbladh h. black m. j. and fleet d. j. stochastic tracking of human in sixth european conference on computer vision figures using image motion. pp. dublin ireland. sigal l. and black m. j. predicting people from pictures. in amdo iv conference on articulated motion and deformable objects pp. mallorca spain. sigal l. balan a. and black m. j. humaneva synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. international journal of computer vision sigal l. bhatia s. roth s. black m. j. and isard m. tracking loose-limbed people. in ieee computer society conference on computer vision and pattern recognition pp. washington dc. sillion f. and puech c. radiosity and global illumination. morgan kaufmann. sim t. baker s. and bsat m. the cmu pose illumination and expresieee transactions on pattern analysis and machine intelligence sion database. simard p. y. bottou l. haffner p. and cun y. l. boxlets a fast convolution algorithm for signal processing and neural networks. in advances in neural information processing systems pp. simon i. and seitz s. m. scene segmentation using the wisdom of crowds. in tenth european conference on computer vision pp. marseilles. references simon i. snavely n. and seitz s. m. scene summarization for online image in eleventh international conference on computer vision collections. rio de janeiro brazil. simoncelli e. p. bayesian denoising of visual images in the wavelet domain. in m uller p. and vidakovic b. bayesian inference in wavelet based models pp. springer-verlag new york. simoncelli e. p. and adelson e. h. non-separable extensions of quadrature mirror filters to multiple dimensions. proceedings of the ieee simoncelli e. p. and adelson e. h. subband transforms. in woods j. subband coding pp. kluwer academic press norwell ma. simoncelli e. p. adelson e. h. and heeger d. j. probability distributions of in ieee computer society conference on computer vision and pattern optic flow. recognition pp. maui hawaii. simoncelli e. p. freeman w. t. adelson e. h. and heeger d. j. shiftable multiscale transforms. ieee transactions on information theory singaraju d. grady l. and vidal r. interactive image segmentation via minimization of quadratic energies on directed graphs. in ieee computer society conference on computer vision and pattern recognition anchorage ak. singaraju d. rother c. and rhemann c. new appearance models for natural image matting. in ieee computer society conference on computer vision and pattern recognition miami beach fl. singaraju d. grady l. sinop a. k. and vidal r. a continuous valued mrf for image segmentation. in blake a. kohli p. and rother c. advances in markov random fields mit press. sinha p. balas b. ostrovsky y. and russell r. face recognition by humans nineteen results all computer vision researchers should know about. proceedings of the ieee sinha s. mordohai p. and pollefeys m. multi-view stereo via graph cuts on the dual of an adaptive tetrahedral mesh. in eleventh international conference on computer vision rio de janeiro brazil. sinha s. n. and pollefeys m. multi-view reconstruction using photo-consistency and exact silhouette constraints a maximum-flow formulation. in tenth international conference on computer vision pp. beijing china. sinha s. n. steedly d. and szeliski r. piecewise planar stereo for image-based rendering. in twelfth ieee international conference on computer vision computer vision algorithms and applications draft kyoto japan. sinha s. n. steedly d. szeliski r. agrawala m. and pollefeys m. interactive architectural modeling from unordered photo collections. acm transactions on graphics siggraph asia sinop a. k. and grady l. a seeded image segmentation framework unifying graph cuts and random walker which yields a new algorithm. in eleventh international conference on computer vision rio de janeiro brazil. sivic j. and zisserman a. matching in videos. pp. nice france. video google a text retrieval approach to object in ninth international conference on computer vision sivic j. and zisserman a. efficient visual search of videos cast as text retrieval. ieee transactions on pattern analysis and machine intelligence sivic j. everingham m. and zisserman a. who are you? learning person in ieee computer society conference on computer specific classifiers from video. vision and pattern recognition miami beach fl. sivic j. zitnick c. l. and szeliski r. finding people in repeated shots of in british machine vision conference pp. the same scene. edinburgh. sivic j. russell b. zisserman a. freeman w. t. and efros a. a. unsupervised discovery of visual object class hierarchies. in ieee computer society conference on computer vision and pattern recognition anchorage ak. sivic j. russell b. c. efros a. a. zisserman a. and freeman w. t. discovering objects and their localization in images. in tenth international conference on computer vision pp. beijing china. slabaugh g. g. culbertson w. b. slabaugh t. g. culbertson b. malzbender t. and stevens m. methods for volumetric reconstruction of visual scenes. international journal of computer vision slama c. c. manual of photogrammetry. american society of photogram metry falls church virginia fourth edition. smelyanskiy v. n. cheeseman p. maluf d. a. and morris r. d. bayesian super-resolved surface reconstruction from images. in ieee computer society conference on computer vision and pattern recognition pp. hilton head island. references smeulders a. w. m. worring m. santini s. gupta a. and jain r. c. contentbased image retrieval at the end of the early years. ieee transactions on pattern analysis and machine intelligence sminchisescu c. and triggs b. covariance scaled sampling for monocular body tracking. in ieee computer society conference on computer vision and pattern recognition pp. kauai hawaii. sminchisescu c. kanaujia a. and metaxas d. conditional models for contextual human motion recognition. computer vision and image understanding sminchisescu c. kanaujia a. li z. and metaxas d. discriminative density propagation for human motion estimation. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. smith a. r. and blinn j. f. blue screen matting. in acm siggraph conference proceedings pp. new orleans. smith b. m. zhang l. jin h. and agarwala a. light field video stabilization. in twelfth international conference on computer vision kyoto japan. smith s. m. and brady j. m. susan a new approach to low level image processing. international journal of computer vision smolic a. and kauff p. interactive video representation and coding technolo gies. proceedings of the ieee snavely n. seitz s. m. and szeliski r. photo tourism exploring photo collections in acm transactions on graphics siggraph snavely n. seitz s. m. and szeliski r. modeling the world from internet photo collections. international journal of computer vision snavely n. seitz s. m. and szeliski r. skeletal graphs for efficient structure from motion. in ieee computer society conference on computer vision and pattern recognition anchorage ak. snavely n. garg r. seitz s. m. and szeliski r. finding paths through the world s photos. acm transactions on graphics siggraph snavely n. simon i. goesele m. szeliski r. and seitz s. m. scene reconstruction and visualization from community photo collections. proceedings of the ieee computer vision algorithms and applications draft soatto s. yezzi a. j. and jin h. tales of shape and radiance in multiview stereo. in ninth international conference on computer vision pp. nice france. soille p. morphological image compositing. ieee transactions on pattern anal ysis and machine intelligence solina f. and bajcsy r. recovery of parametric models from range images ieee transactions on pattern the case for superquadrics with global deformations. analysis and machine intelligence sontag d. and jaakkola t. new outer bounds on the marginal polytope. in advances in neural information processing systems. sontag d. meltzer t. globerson a. jaakkola t. and weiss y. tightening lp relaxations for map using message passing. in uncertainty in artificial intelligence soucy m. and laurendeau d. multi-resolution surface modeling from multiple range views. in ieee computer society conference on computer vision and pattern recognition pp. champaign illinois. srinivasan s. chellappa r. veeraraghavan a. and aggarwal g. electronic image stabilization and mosaicking algorithms. in bovik a. handbook of image and video processing academic press. srivasan p. liang p. and hackwood s. computational geometric methods in volumetric intersections for reconstruction. pattern recognition stamos i. liu l. chen c. wolberg g. yu g. and zokai s. integrating automated range registration with multiview geometry for the photorealistic modeling of large-scale scenes. international journal of computer vision stark j. a. adaptive image contrast enhancement using generalizations of his togram equalization. ieee transactions on image processing stauffer c. and grimson w. adaptive background mixture models for realtime tracking. in ieee computer society conference on computer vision and pattern recognition pp. fort collins. steedly d. and essa i. propagation of innovative information in non-linear leastsquares structure from motion. in eighth international conference on computer vision pp. vancouver canada. steedly d. essa i. and dellaert f. spectral partitioning for structure from moin ninth international conference on computer vision pp. tion. nice france. references steedly d. pal c. and szeliski r. efficiently registering video into panoramic in tenth international conference on computer vision mosaics. pp. beijing china. steele r. and jaynes c. feature uncertainty arising from covariant image noise. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. steele r. m. and jaynes c. overconstrained linear estimation of radial distortion and multi-view geometry. in ninth european conference on computer vision pp. stein a. hoiem d. and hebert m. learning to extract object boundaries using motion cues. in eleventh international conference on computer vision rio de janeiro brazil. stein f. and medioni g. structural indexing efficient object recognition. ieee transactions on pattern analysis and machine intelligence stein g. accurate internal camera calibration using rotation with analysis of in fifth international conference on computer vision sources of error. pp. cambridge massachusetts. stein g. lens distortion calibration using point correspondences. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. stenger b. thayananthan a. torr p. h. s. and cipolla r. model-based hand tracking using a hierarchical bayesian filter. ieee transactions on pattern analysis and machine intelligence stewart c. v. robust parameter estimation in computer vision. siam reviews stiller c. and konrad j. estimating motion in image sequences a tutorial on modeling and computation of motion. ieee signal processing magazine stollnitz e. j. derose t. d. and salesin d. h. wavelets for computer graphics theory and applications. morgan kaufmann san francisco. strang g. linear algebra and its applications. harcourt brace jovanovich publishers san diego edition. strang g. wavelets and dilation equations a brief introduction. siam reviews computer vision algorithms and applications draft strecha c. fransens r. and van gool l. combined depth and outlier estimation in multi-view stereo. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. strecha c. tuytelaars t. and van gool l. dense matching of multiple widebaseline views. in ninth international conference on computer vision pp. nice france. strecha c. von hansen w. van gool l. fua p. and thoennessen u. on in ieee computer society benchmarking camera calibration and multi-view stereo. conference on computer vision and pattern recognition anchorage ak. sturm p. multi-view geometry for general camera models. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. sturm p. and ramalingam s. a generic concept for camera calibration. in eighth european conference on computer vision pp. prague. sturm p. and triggs w. a factorization based algorithm for multi-image projective structure and motion. in fourth european conference on computer vision pp. cambridge england. su h. sun m. fei-fei l. and savarese s. learning a dense multi-view representation for detection viewpoint classification and synthesis of object categories. in twelfth international conference on computer vision kyoto japan. sudderth e. b. torralba a. freeman w. t. and willsky a. s. describing visual scenes using transformed objects and parts. international journal of computer vision sullivan s. and ponce j. automatic model construction and pose estimation from photographs using triangular splines. ieee transactions on pattern analysis and machine intelligence sun d. roth s. lewis j. p. and black m. j. learning optical flow. in tenth european conference on computer vision pp. marseilles. sun j. zheng n. and shum h. stereo matching using belief propagation. ieee transactions on pattern analysis and machine intelligence sun j. jia j. tang c.-k. and shum h.-y. poisson matting. acm transactions on graphics siggraph sun j. li y. kang s. b. and shum h.-y. flash matting. acm transactions on graphics references sun j. yuan l. jia j. and shum h.-y. image completion with structure propa gation. acm transactions on graphics siggraph sun m. su h. savarese s. and fei-fei l. a multi-view probabilistic model for object classes. in ieee computer society conference on computer vision and pattern recognition miami beach fl. sung k.-k. and poggio t. example-based learning for view-based human face detection. ieee transactions on pattern analysis and machine intelligence sutherland i. e. three-dimensional data input by tablet. proceedings of the ieee swain m. j. and ballard d. h. color indexing. international journal of computer vision swaminathan r. kang s. b. szeliski r. criminisi a. and nayar s. k. on the motion and appearance of specularities in image sequences. in seventh european conference on computer vision pp. copenhagen. sweldens w. wavelets and the lifting scheme a minute tour. z. angew. math. mech. sweldens w. the lifting scheme a construction of second generation wavelets. siam j. math. anal. swendsen r. h. and wang j.-s. nonuniversal critical dynamics in monte carlo simulations. physical review letters szeliski r. cooperative algorithms for solving random-dot stereograms. technical report computer science department carnegie mellon university. szeliski r. bayesian modeling of uncertainty in low-level vision. kluwer academic publishers boston. szeliski r. bayesian modeling of uncertainty in low-level vision. international journal of computer vision szeliski r. fast surface interpolation using hierarchical basis functions. ieee transactions on pattern analysis and machine intelligence szeliski r. fast shape from shading. cvgip image understanding szeliski r. shape from rotation. in ieee computer society conference on computer vision and pattern recognition pp. maui hawaii. computer vision algorithms and applications draft szeliski r. rapid octree construction from image sequences. cvgip image understanding szeliski r. image mosaicing for tele-reality applications. in ieee workshop on applications of computer vision pp. sarasota. szeliski r. video mosaics for virtual environments. ieee computer graphics and applications szeliski r. a multi-view approach to motion and stereo. in ieee computer society conference on computer vision and pattern recognition pp. fort collins. szeliski r. image alignment and stitching a tutorial. foundations and trends in computer graphics and computer vision szeliski r. locally adapted hierarchical basis preconditioning. acm transac tions on graphics siggraph szeliski r. and coughlan j. spline-based image registration. international jour nal of computer vision szeliski r. and golland p. stereo matching with transparency and matting. international journal of computer vision special issue for marr prize papers. szeliski r. and hinton g. solving random-dot stereograms using the heat equation. in ieee computer society conference on computer vision and pattern recognition pp. san francisco. szeliski r. and ito m. r. new hermite cubic interpolator for two-dimensional curve generation. iee proceedings e szeliski r. and kang s. b. recovering shape and motion from image streams using nonlinear least squares. journal of visual communication and image representation szeliski r. and kang s. b. direct methods for visual scene reconstruction. in ieee workshop on representations of visual scenes pp. cambridge massachusetts. szeliski r. and kang s. b. shape ambiguities in structure from motion. ieee transactions on pattern analysis and machine intelligence szeliski r. and lavall ee s. matching anatomical surfaces with non-rigid deformations using octree-splines. international journal of computer vision references szeliski r. and scharstein d. sampling the disparity space image. ieee trans actions on pattern analysis and machine intelligence szeliski r. and shum h.-y. motion estimation with quadtree splines. transactions on pattern analysis and machine intelligence ieee szeliski r. and shum h.-y. creating full view panoramic image mosaics and texture-mapped models. in acm siggraph conference proceedings pp. los angeles. szeliski r. and tonnesen d. surface modeling with oriented particle systems. computer graphics szeliski r. and torr p. geometrically constrained structure from motion points on planes. in european workshop on structure from multiple images of large-scale environments pp. freiburg germany. szeliski r. and weiss r. robust shape recovery from occluding contours using a linear smoother. international journal of computer vision szeliski r. avidan s. and anandan p. layer extraction from multiple imin ieee computer society conference ages containing reflections and transparency. on computer vision and pattern recognition pp. hilton head island. szeliski r. tonnesen d. and terzopoulos d. curvature and continuity control in particle-based surface models. in spie vol. geometric methods in computer vision ii pp. san diego. szeliski r. tonnesen d. and terzopoulos d. modeling surfaces of arbitrary topology with dynamic particles. in ieee computer society conference on computer vision and pattern recognition pp. new york. szeliski r. uyttendaele m. and steedly d. fast poisson blending using multi splines. technical report microsoft research. szeliski r. winder s. and uyttendaele m. high-quality multi-pass image re sampling. technical report microsoft research. szeliski r. zabih r. scharstein d. veksler o. kolmogorov v. agarwala a. tappen m. and rother c. a comparative study of energy minimization methods for ieee transactions on pattern markov random fields with smoothness-based priors. analysis and machine intelligence szummer m. and picard r. w. temporal texture modeling. in ieee international conference on image processing pp. lausanne. computer vision algorithms and applications draft tabb m. and ahuja n. multiscale image segmentation by integrated edge and region detection. ieee transactions on image processing taguchi y. wilburn b. and zitnick c. l. stereo reconstruction with mixed pixels using adaptive over-segmentation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. tanaka m. and okutomi m. locally adaptive learning for translation-variant mrf image priors. in ieee computer society conference on computer vision and pattern recognition anchorage ak. tao h. sawhney h. s. and kumar r. a global matching framework for stereo in eighth international conference on computer vision computation. pp. vancouver canada. tappen m. f. utilizing variational optimization to learn markov random fields. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. tappen m. f. and freeman w. t. comparison of graph cuts with belief propagation for stereo using identical mrf parameters. in ninth international conference on computer vision pp. nice france. tappen m. f. freeman w. t. and adelson e. h. recovering intrinsic images from a single image. ieee transactions on pattern analysis and machine intelligence tappen m. f. russell b. c. and freeman w. t. exploiting the sparse derivative prior for super-resolution and image demosaicing. in third international workshop on statistical and computational theories of vision nice france. tappen m. f. liu c. freeman w. and adelson e. learning gaussian conditional random fields for low-level vision. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. tardif j.-p. non-iterative approach for fast and accurate vanishing point detection. in twelfth international conference on computer vision kyoto japan. tardif j.-p. sturm p. and roy s. plane-based self-calibration of radial distortion. in eleventh international conference on computer vision rio de janeiro brazil. tardif j.-p. sturm p. trudeau m. and roy s. calibration of cameras with ieee transactions on pattern analysis and machine radially symmetric distortion. intelligence references taubin g. curve and surface smoothing without shrinkage. in fifth international conference on computer vision pp. cambridge massachusetts. taubman d. s. and marcellin m. w. standard for interactive imaging. proceedings of the ieee taylor c. j. surface reconstruction from feature based stereo. in ninth interna tional conference on computer vision pp. nice france. taylor c. j. debevec p. e. and malik j. reconstructing polyhedral models of architectural scenes from photographs. in fourth european conference on computer vision pp. cambridge england. taylor c. j. kriegman d. j. and anandan p. dimensions from multiple images a least squares approach. visual motion pp. princeton new jersey. structure and motion in two in ieee workshop on taylor p. text-to-speech synthesis. cambridge university press cambridge. tek k. and kimia b. b. symmetry maps of free-form curve segments via wave propagation. international journal of computer vision tekalp m. digital video processing. prentice hall upper saddle river nj. telea a. an image inpainting technique based on fast marching method. journal of graphics tools teller s. antone m. bodnar z. bosse m. coorg s. jethwa m. and master n. calibrated registered images of an extended urban area. international journal of computer vision teodosio l. and bender w. salient video stills content and context preserved. in acm multimedia pp. anaheim california. terzopoulos d. multilevel computational processes for visual surface reconstruc tion. computer vision graphics and image processing terzopoulos d. image analysis using multigrid relaxation methods. ieee trans actions on pattern analysis and machine intelligence terzopoulos d. tinuities. regularization of inverse visual problems involving disconieee transactions on pattern analysis and machine intelligence pami terzopoulos d. the computation of visible-surface representations. ieee trans actions on pattern analysis and machine intelligence terzopoulos d. visual modeling for computer animation graphics with a vision. computer graphics computer vision algorithms and applications draft terzopoulos d. and fleischer k. deformable models. the visual computer terzopoulos d. and metaxas d. dynamic models with local and global deieee transactions on pattern analysis and formations deformable superquadrics. machine intelligence terzopoulos d. and szeliski r. tracking with kalman snakes. in blake a. and yuille a. l. active vision pp. mit press cambridge massachusetts. terzopoulos d. and waters k. analysis of facial images using physical and anatomical models. in third international conference on computer vision pp. osaka japan. terzopoulos d. and witkin a. physically-based models with rigid and deformable components. ieee computer graphics and applications terzopoulos d. witkin a. and kass m. symmetry-seeking models and object reconstruction. international journal of computer vision terzopoulos d. witkin a. and kass m. constraints on deformable models recovering shape and nonrigid motion. artificial intelligence thayananthan a. iwasaki m. and cipolla r. principled fusion of high-level model and low-level cues for motion segmentation. in ieee computer society conference on computer vision and pattern recognition anchorage ak. thirthala s. and pollefeys m. the radial trifocal tensor a tool for calibrating the radial distortion of wide-angle cameras. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. gaussian random thomas d. b. luk w. leong p. h. and villasenor j. d. number generators. acm computing surveys thrun s. burgard w. and fox d. probabilistic robotics. the mit press cambridge massachusetts. thrun s. montemerlo m. dahlkamp h. stavens d. aron a. et al. stanley the robot that won the darpa grand challenge. journal of field robotics tian q. and huhns m. n. algorithms for subpixel registration. computer vision graphics and image processing tikhonov a. n. and arsenin v. y. solutions of ill-posed problems. v. h. winston washington d. c. tipping m. e. and bishop c. m. probabilistic principal components analysis. journal of the royal statistical society series b references toint p. l. on large scale nonlinear least squares calculations. siam j. sci. stat. comput. tola e. lepetit v. and fua p. daisy an efficient dense descriptor applied to wide-baseline stereo. ieee transactions on pattern analysis and machine intelligence tolliver d. and miller g. graph partitioning by spectral rounding applications in image segmentation and clustering. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. tomasi c. and kanade t. shape and motion from image streams under orthography a factorization method. international journal of computer vision tomasi c. and manduchi r. bilateral filtering for gray and color images. in sixth international conference on computer vision pp. bombay. tombari f. mattoccia s. and di stefano l. segmentation-based adaptive support in pacific-rim symposium on image and video for accurate stereo correspondence. technology. tombari f. mattoccia s. di stefano l. and addimanda e. classification and evaluation of cost aggregation methods for stereo correspondence. in ieee computer society conference on computer vision and pattern recognition anchorage ak. tommasini t. fusiello a. trucco e. and roberto v. making good features track better. in ieee computer society conference on computer vision and pattern recognition pp. santa barbara. torborg j. and kajiya j. t. talisman commodity realtime graphics for the pc. in acm siggraph conference proceedings pp. new orleans. torr p. h. s. bayesian model estimation and selection for epipolar geometry and generic manifold fitting. international journal of computer vision torr p. h. s. and fitzgibbon a. w. invariant fitting of two view geometry. ieee transactions on pattern analysis and machine intelligence torr p. h. s. and murray d. the development and comparison of robust methods for estimating the fundamental matrix. international journal of computer vision torr p. h. s. szeliski r. and anandan p. an integrated bayesian approach to layer extraction from image sequences. in seventh international conference on computer vision pp. kerkyra greece. computer vision algorithms and applications draft torr p. h. s. szeliski r. and anandan p. an integrated bayesian approach to layer extraction from image sequences. ieee transactions on pattern analysis and machine intelligence torralba a. contextual priming for object detection. international journal of computer vision torralba a. classifier-based methods. recognizing and learning object categories. shortcourserloc. in cvpr short course on httppeople.csail.mit.edutorralba torralba a. object recognition and scene understanding. mit course torralba a. freeman w. t. and fergus r. million tiny images a large dataset for non-parametric object and scene recognition. ieee transactions on pattern analysis and machine intelligence torralba a. murphy k. p. and freeman w. t. contextual models for object detection using boosted random fields. in advances in neural information processing systems. torralba a. murphy k. p. and freeman w. t. sharing visual features for mulieee transactions on pattern analysis and ticlass and multiview object detection. machine intelligence torralba a. weiss y. and fergus r. small codes and large databases of images for object recognition. in ieee computer society conference on computer vision and pattern recognition anchorage ak. torralba a. murphy k. p. freeman w. t. and rubin m. a. context-based vision system for place and object recognition. in ninth international conference on computer vision pp. nice france. torrance k. e. and sparrow e. m. theory for off-specular reflection from rough ened surfaces. journal of the optical society of america a torresani l. hertzmann a. and bregler c. non-rigid structure-from-motion estimating shape and motion with hierarchical priors. ieee transactions on pattern analysis and machine intelligence toyama k. prolegomena for robust face tracking. technical report msr-tr microsoft research. toyama k. krumm j. brumitt b. and meyers b. wallflower principles and practice of background maintenance. in seventh international conference on computer vision pp. kerkyra greece. references tran s. and davis l. surface reconstruction using graph cuts with surface constraints. in seventh european conference on computer vision pp. copenhagen. trefethen l. n. and bau d. numerical linear algebra. siam. treisman a. preattentive processing in vision. computer vision graphics and image processing triggs b. factorization methods for projective structure and motion. in ieee computer society conference on computer vision and pattern recognition pp. san francisco. triggs b. detecting keypoints with stable position orientation and scale under illumination changes. in eighth european conference on computer vision pp. prague. triggs b. mclauchlan p. f. hartley r. i. and fitzgibbon a. w. bundle adjustment a modern synthesis. in international workshop on vision algorithms pp. kerkyra greece. trobin w. pock t. cremers d. and bischof h. continuous energy minimizain tenth european conference on computer vision tion via repeated binary fusion. pp. marseilles. troccoli a. and allen p. building illumination coherent models of large-scale outdoor scenes. international journal of computer vision trottenberg u. oosterlee c. w. and schuller a. multigrid. academic press. trucco e. and verri a. introductory techniques for computer vision. pren tice hall upper saddle river nj. tsai p. s. and shah m. shape from shading using linear approximation. image and vision computing tsai r. y. a versatile camera calibration technique for high-accuracy machine vision metrology using off-the-shelf tv cameras and lenses. ieee journal of robotics and automation tschumperl e d. curvature-preserving regularization of multi-valued images using pdes. in ninth european conference on computer vision pp. tschumperl e d. and deriche r. vector-valued image regularization with pdes a common framework for different applications. ieee transactions on pattern analysis and machine intelligence computer vision algorithms and applications draft tsin y. kang s. b. and szeliski r. stereo matching with linear superposition of layers. ieee transactions on pattern analysis and machine intelligence tsin y. ramesh v. and kanade t. statistical calibration of ccd imaging process. in eighth international conference on computer vision pp. vancouver canada. tu z. chen x. yuille a. l. and zhu s.-c. image parsing unifying segmentation detection and recognition. international journal of computer vision tumblin j. and rushmeier h. e. tone reproduction for realistic images. ieee computer graphics and applications tumblin j. and turk g. lcis a boundary hierarchy for detail-preserving contrast reduction. in acm siggraph conference proceedings pp. los angeles. tumblin j. agrawal a. and raskar r. why i want a gradient camera. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. turcot p. and lowe d. g. better matching with fewer features the selection of useful features in large database recognition problems. in iccv workshop on emergent issues in large amounts of visual data kyoto japan. turk g. and levoy m. zippered polygonal meshes from range images. in acm siggraph conference proceedings pp. turk g. and o brien j. modelling with implicit surfaces that interpolate. acm transactions on graphics turk m. and pentland a. eigenfaces for recognition. journal of cognitive neuroscience turk m. and pentland a. face recognition using eigenfaces. in ieee computer society conference on computer vision and pattern recognition pp. maui hawaii. tuytelaars t. and mikolajczyk k. local invariant feature detectors. foundations and trends in computer graphics and computer vision tuytelaars t. and van gool l. matching widely separated views based on affine invariant regions. international journal of computer vision tuytelaars t. van gool l. and proesmans m. the cascaded hough transform. in international conference on image processing pp. references ullman s. the interpretation of structure from motion. proceedings of the royal society of london unnikrishnan r. pantofaru c. and hebert m. toward objective evaluation of image segmentation algorithms. ieee transactions on pattern analysis and machine intelligence unser m. splines a perfect fit for signal and image processing. ieee signal processing magazine urmson c. anhalt j. bagnell d. baker c. bittner r. et al. autonomous driving in urban environments boss and the urban challenge. journal of field robotics urtasun r. fleet d. j. and fua p. temporal motion models for monocular and multiview human body tracking. computer vision and image understanding uyttendaele m. eden a. and szeliski r. eliminating ghosting and exposure artifacts in image mosaics. in ieee computer society conference on computer vision and pattern recognition pp. kauai hawaii. uyttendaele m. criminisi a. kang s. b. winder s. hartley r. and szeliski r. image-based interactive exploration of real-world environments. ieee computer graphics and applications vaillant r. and faugeras o. d. using extremal boundaries for object modeling. ieee transactions on pattern analysis and machine intelligence vaish v. szeliski r. zitnick c. l. kang s. b. and levoy m. reconstructing occluded surfaces using synthetic apertures shape from focus vs. shape from stereo. in ieee computer society conference on computer vision and pattern recognition pp. new york ny. van de weijer j. and schmid c. coloring local feature extraction. in ninth european conference on computer vision pp. van den hengel a. dick a. thormhlen t. ward b. and torr p. h. s. videotrace rapid interactive scene modeling from video. acm transactions on graphics van huffel s. and lemmerling p. total least squares and errors-in variables modeling springer. van huffel s. and vandewalle j. the total least squares problem computational aspects and analysis. society for industrial and applied mathematics philadephia. computer vision algorithms and applications draft van ouwerkerk j. d. image super-resolution survey. image and vision computing varma m. and ray d. learning the discriminative power-invariance trade-off. in eleventh international conference on computer vision rio de janeiro brazil. vasconcelos n. from pixels to semantic spaces advances in content-based image retrieval. computer vasilescu m. a. o. and terzopoulos d. multilinear image synthesis analysis and recognition. ieee signal processing magazine vedaldi a. and fulkerson b. vlfeat an open and portable library of computer vision algorithms. httpwww.vlfeat.org. vedaldi a. gulshan v. varma m. and zisserman a. multiple kernels for object detection. in twelfth international conference on computer vision kyoto japan. vedula s. baker s. and kanade t. image-based spatio-temporal modeling and view interpolation of dynamic events. acm transactions on graphics vedula s. baker s. rander p. collins r. and kanade t. dimensional scene flow. ligence threeieee transactions on pattern analysis and machine intel veeraraghavan a. raskar r. agrawal a. mohan a. and tumblin j. dappled photography mask enhanced cameras for heterodyned light fields and coded aperture refocusing. acm transactions on graphics veksler o. efficient graph-based energy minimization methods in computer vision. ph.d. thesis cornell university. veksler o. stereo matching by compact windows via minimum ratio cycle. in eighth international conference on computer vision pp. vancouver canada. veksler o. fast variable window for stereo correspondence using integral images. in ieee computer society conference on computer vision and pattern recognition pp. madison wi. veksler o. graph cut based optimization for mrfs with truncated convex priors. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. references verbeek j. and triggs b. region classification with markov field aspect models. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. vergauwen m. and van gool l. web-based reconstruction service. machine vision and applications vetter t. and poggio t. linear object classes and image synthesis from a single example image. ieee transactions on pattern analysis and machine intelligence vezhnevets v. sazonov v. and andreeva a. a survey on pixel-based skin color detection techniques. in pp. vicente s. kolmogorov v. and rother c. graph cut based image segmentation with connectivity priors. in ieee computer society conference on computer vision and pattern recognition anchorage ak. vidal r. ma y. and sastry s. s. generalized principal component analysis. springer. vi eville t. and faugeras o. d. feedforward recovery of motion and structure from a sequence of matches. in third international conference on computer vision pp. osaka japan. vincent l. and soille p. watersheds in digital spaces an efficient algorithm based on immersion simulations. ieee transactions on pattern analysis and machine intelligence vineet v. and narayanan p. j. cuda cuts fast graph cuts on the gpu. in cvpr workshop on visual computer vision on gpus anchorage ak. viola p. and wells iii w. alignment by maximization of mutual information. international journal of computer vision viola p. jones m. j. and snow d. detecting pedestrians using patterns of motion and appearance. in ninth international conference on computer vision pp. nice france. viola p. a. and jones m. j. robust real-time face detection. international journal of computer vision vlasic d. baran i. matusik w. and popovi c j. articulated mesh animation from multi-view silhouettes. acm transactions on graphics vlasic d. brand m. pfister h. and popovi c j. face transfer with multilinear models. acm transactions on graphics siggraph computer vision algorithms and applications draft vogiatzis g. torr p. and cipolla r. multi-view stereo via volumetric graph-cuts. in ieee computer society conference on computer vision and pattern recognition pp. san diego ca. vogiatzis g. hernandez c. torr p. and cipolla r. multi-view stereo via volumetric graph-cuts and occlusion robust photo-consistency. ieee transactions on pattern analysis and machine intelligence von ahn l. and dabbish l. labeling images with a computer game. in chi sigchi conference on human factors in computing systems pp. vienna austria. von ahn l. liu r. and blum m. peekaboom a game for locating objects in images. in chi sigchi conference on human factors in computing systems pp. montr eal qu ebec canada. wainwright m. j. and jordan m. i. graphical models exponential families and variational inference. foundations and trends in machine learning wainwright m. j. jaakkola t. s. and willsky a. s. map estimation via agreement on trees message-passing and linear programming. ieee transactions on information theory waithe p. and ferrie f. from uncertainty to visual exploration. ieee transactions on pattern analysis and machine intelligence walker e. l. and herman m. geometric reasoning for constructing scene descriptions from images. artificial intelligence wallace g. k. the jpeg still picture compression standard. communications of the acm wallace j. r. cohen m. f. and greenberg d. p. a two-pass solution to the rendering equation a synthesis of ray tracing and radiosity methods. computer graphics waltz d. l. understanding line drawings of scenes with shadows. in winston p. h. the psychology of computer vision mcgraw-hill new york. wang h. and oliensis j. shape matching by segmentation averaging. ieee transactions on pattern analysis and machine intelligence wang j. and cohen m. f. an iterative optimization approach for unified image segmentation and matting. in tenth international conference on computer vision beijing china. wang j. and cohen m. f. image and video matting a survey. foundations and trends in computer graphics and computer vision references wang j. and cohen m. f. optimized color sampling for robust matting. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. wang j. and cohen m. f. simultaneous matting and compositing. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. wang j. agrawala m. and cohen m. f. soft scissors an interactive tool for realtime high quality matting. acm transactions on graphics wang j. thiesson b. xu y. and cohen m. image and video segmentation by anisotropic kernel mean shift. in eighth european conference on computer vision pp. prague. wang j. bhat p. colburn r. a. agrawala m. and cohen m. f. video cutout. acm transactions on graphics siggraph wang j. y. a. and adelson e. h. representing moving images with layers. ieee transactions on image processing wang l. kang s. b. szeliski r. and shum h.-y. optimal texture map reconstruction from multiple views. in ieee computer society conference on computer vision and pattern recognition pp. kauai hawaii. wang y. and zhu s.-c. modeling textured motion particle wave and sketch. in ninth international conference on computer vision pp. nice france. wang z. bovik a. c. and simoncelli e. p. structural approaches to image quality assessment. in bovik a. c. handbook of image and video processing pp. elsevier academic press. wang z. bovik a. c. sheikh h. r. and simoncelli e. p. image quality asieee transactions on image sessment from error visibility to structural similarity. processing wang z.-f. and zheng z.-g. a region based stereo matching algorithm using cooperative optimization. in ieee computer society conference on computer vision and pattern recognition anchorage ak. ward g. measuring and modeling anisotropic reflection. computer graphics ward g. the radiance lighting simulation and rendering system. in acm sig graph conference proceedings pp. computer vision algorithms and applications draft ward g. fast robust image registration for compositing high dynamic range photographs from hand-held exposures. journal of graphics tools ward g. high dynamic range image encodings. httpwww.anyhere.comgward hdrenchdr encodings.html. ware c. arthur k. and booth k. s. fish tank virtual reality. in interchi pp. amsterdam. warren j. and weimer h. subdivision methods for geometric design a con structive approach. morgan kaufmann. watanabe m. and nayar s. k. rational filters for passive depth from defocus. international journal of computer vision watt a. computer graphics. addison-wesley harlow england third edition. weber j. and malik j. robust computation of optical flow in a multi-scale differ ential framework. international journal of computer vision weber m. welling m. and perona p. unsupervised learning of models for recognition. in sixth european conference on computer vision pp. dublin ireland. wedel a. cremers d. pock t. and bischof h. structure- and motion-adaptive in twelfth international conference on regularization for high accuracy optic flow. computer vision kyoto japan. wedel a. rabe c. vaudrey t. brox t. franke u. and cremers d. efficient dense scene flow from sparse or dense stereo data. in tenth european conference on computer vision pp. marseilles. wei c. y. and quan l. region-based progressive stereo matching. in ieee computer society conference on computer vision and pattern recognition pp. washington d. c. wei l.-y. and levoy m. fast texture synthesis using tree-structured vector quan tization. in acm siggraph conference proceedings pp. weickert j. anisotropic diffusion in image processing. tuebner stuttgart. weickert j. ter haar romeny b. m. and viergever m. a. efficient and reliable schemes for nonlinear diffusion filtering. ieee transactions on image processing weinland d. ronfard r. and boyer e. free viewpoint action recognition using motion history volumes. computer vision and image understanding references weiss y. smoothness in layers motion segmentation using nonparametric mixture in ieee computer society conference on computer vision and pattern estimation. recognition pp. san juan puerto rico. weiss y. segmentation using eigenvectors a unifying view. in seventh interna tional conference on computer vision pp. kerkyra greece. weiss y. deriving intrinsic images from image sequences. in eighth international conference on computer vision pp. vancouver canada. weiss y. and adelson e. h. a unified mixture framework for motion segmentation incorporating spatial coherence and estimating the number of models. in ieee computer society conference on computer vision and pattern recognition pp. san francisco. weiss y. and freeman b. what makes a good model of natural images? in ieee computer society conference on computer vision and pattern recognition minneapolis mn. weiss y. and freeman w. t. correctness of belief propagation in gaussian graphical models of arbitrary topology. neural computation weiss y. and freeman w. t. on the optimality of solutions of the max-product ieee transactions on information belief propagation algorithm in arbitrary graphs. theory weiss y. torralba a. and fergus r. spectral hashing. in advances in neural information processing systems. weiss y. yanover c. and meltzer t. linear programming and variants of belief propagation. in blake a. kohli p. and rother c. advances in markov random fields mit press. wells iii w. m. efficient synthesis of gaussian filters by cascaded uniform filters. ieee transactions on pattern analysis and machine intelligence weng j. ahuja n. and huang t. s. optimal motion and structure estimation. ieee transactions on pattern analysis and machine intelligence wenger a. gardner a. tchou c. unger j. hawkins t. and debevec p. performance relighting and reflectance transformation with time-multiplexed illumination. acm transactions on graphics siggraph werlberger m. trobin w. pock t. bischof h. wedel a. and cremers d. in british machine vision conference anisotropic optical flow. london. computer vision algorithms and applications draft werner t. a linear programming approach to max-sum problem a review. ieee transactions on pattern analysis and machine intelligence werner t. and zisserman a. new techniques for automated architectural reconstruction from photographs. in seventh european conference on computer vision pp. copenhagen. westin s. h. arvo j. r. and torrance k. e. predicting reflectance functions from complex surfaces. computer graphics westover l. interactive volume rendering. in workshop on volume visualization pp. chapel hill. wexler y. fitzgibbon a. and zisserman a. bayesian estimation of layers from multiple images. in seventh european conference on computer vision pp. copenhagen. wexler y. shechtman e. and irani m. space-time completion of video. ieee transactions on pattern analysis and machine intelligence weyrich t. lawrence j. lensch h. p. a. rusinkiewicz s. and zickler t. principles of appearance acquisition and representation. foundations and trends in computer graphics and computer vision weyrich t. matusik w. pfister h. bickel b. donner c. et al. analysis of human faces using a measurement-based skin reflectance model. acm transactions on graphics wheeler m. d. sato y. and ikeuchi k. consensus surfaces for modeling in sixth international conference on computer objects from multiple range images. vision pp. bombay. white r. and forsyth d. combining cues shape from shading and texture. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. white r. crane k. and forsyth d. a. capturing and animating occluded cloth. acm transactions on graphics wiejak j. s. buxton h. and buxton b. f. convolution with separable masks for early image processing. computer vision graphics and image processing wilburn b. joshi n. vaish v. talvala e.-v. antunez e. et al. high performance imaging using large camera arrays. acm transactions on graphics siggraph references wilczkowiak m. brostow g. j. tordoff b. and cipolla r. hole filling through photomontage. in british machine vision conference pp. oxford brookes. williams d. and burns p. d. diagnostics for digital capture using mtf. in ist pics conference pp. williams d. j. and shah m. a fast algorithm for active contours and curvature estimation. computer vision graphics and image processing williams l. pyramidal parametrics. computer graphics williams l. performace driven facial animation. computer graphics graph williams o. blake a. and cipolla r. a sparse probabilistic learning algorithm for real-time tracking. in ninth international conference on computer vision pp. nice france. williams t. l. the optical transfer function of imaging systems. institute of physics publishing london. winder s. and brown m. learning local image descriptors. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. winkenbach g. and salesin d. h. computer-generated pen-and-ink illustration. in acm siggraph conference proceedings pp. orlando florida. winn j. and shotton j. the layout consistent random field for recognizing and segmenting partially occluded objects. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. winnem oller h. olsen s. c. and gooch b. real-time video abstraction. acm transactions on graphics winston p. h. the psychology of computer vision mcgraw-hill new york. wiskott l. fellous j.-m. kr uger n. and von der malsburg c. face recognition by elastic bunch graph matching. ieee transactions on pattern analysis and machine intelligence witkin a. recovering surface shape and orientation from texture. artificial intel ligence computer vision algorithms and applications draft witkin a. scale-space filtering. in eighth international joint conference on artificial intelligence pp. witkin a. terzopoulos d. and kass m. signal matching through scale space. in fifth national conference on artificial intelligence pp. philadelphia. witkin a. terzopoulos d. and kass m. signal matching through scale space. international journal of computer vision wolberg g. digital image warping. ieee computer society press los alamitos. wolberg g. and pavlidis t. restoration of binary images using stochastic relax ation with annealing. pattern recognition letters wolff l. b. shafer s. a. and healey g. e. radiometry. physics-based vision principles and practice jones bartlett cambridge ma. wolff l. b. shafer s. a. and healey g. e. shape recovery. physics based vision principles and practice jones bartlett cambridge ma. wood d. n. finkelstein a. hughes j. f. thayer c. e. and salesin d. h. multiperspective panoramas for cel animation. in acm siggraph conference proceedings pp. los angeles. wood d. n. azuma d. i. aldinger k. curless b. duchamp t. salesin d. h. and in acm siggraph surface light fields for photography. stuetzle w. conference proceedings pp. woodford o. reid i. torr p. h. and fitzgibbon a. global stereo reconstruction under second order smoothness priors. in ieee computer society conference on computer vision and pattern recognition anchorage ak. woodham r. j. analysing images of curved surfaces. artificial intelligence woodham r. j. gradient and curvature from photometric stereo including local confidence estimation. journal of the optical society of america a wren c. r. azarbayejani a. darrell t. and pentland a. p. pfinder realtime tracking of the human body. ieee transactions on pattern analysis and machine intelligence wright s. digital compositing for film and video. focal press edition. wu c. siftgpu a gpu implementation of scale invariant feature transform httpwww.cs.unc.edu ccwusiftgpu. references wyszecki g. and stiles w. s. color science concepts and methods quantitative data and formulae. john wiley sons new york edition. xiao j. and shah m. two-frame wide baseline matching. in ninth international conference on computer vision pp. nice france. xiao j. and shah m. using graph cuts. motion layer extraction in the presence of occlusion ieee transactions on pattern analysis and machine intelligence xiong y. and turkowski k. creating image-based vr using a self-calibrating fisheye lens. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. xiong y. and turkowski k. registration calibration and blending in creating high quality panoramas. in ieee workshop on applications of computer vision pp. princeton. xu l. chen j. and jia j. a segmentation based variational model for accurate in tenth european conference on computer vision optical flow estimation. pp. marseilles. yang d. el gamal a. fowler b. and tian h. a cmos image sensor with ultra-wide dynamic range floating-point pixel level adc. ieee journal of solid state circuits yang l. and albregtsen f. fast and exact computation of cartesian geometric moments using discrete green s theorem. pattern recognition yang l. meer p. and foran d. multiple class segmentation using a unified framework over mean-shift patches. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. yang l. jin r. sukthankar r. and jurie f. unifying discriminative visual codebook generation with classifier training for object category recognition. in ieee computer society conference on computer vision and pattern recognition anchorage ak. yang m.-h. ahuja n. and tabb m. extraction of motion trajectories and its application to hand gesture recognition. ieee transactions on pattern analysis and machine intelligence yang m.-h. kriegman d. j. and ahuja n. detecting faces in images a survey. ieee transactions on pattern analysis and machine intelligence yang q. wang l. yang r. stew enius h. and nist er d. stereo matching with color-weighted correlation hierarchical belief propagation and occlusion handling. computer vision algorithms and applications draft ieee transactions on pattern analysis and machine intelligence yang y. yuille a. and lu j. local global and multilevel stereo matching. in ieee computer society conference on computer vision and pattern recognition pp. new york. yanover c. meltzer t. and weiss y. linear programming relaxations and belief propagation an empirical study. journal of machine learning research yao b. z. yang x. lin l. lee m. w. and zhu s.-c. image parsing to text description. proceedings of the ieee yaou m.-h. and chang w.-t. fast surface interpolation using multiresolution wavelets. ieee transactions on pattern analysis and machine intelligence yatziv l. and sapiro g. fast image and video colorization using chrominance blending. ieee transactions on image processing yedidia j. s. freeman w. t. and weiss y. understanding belief propagation and its generalization. in international joint conference on artificial intelligence yezzi jr. a. j. kichenassamy s. kumar a. olver p. and tannenbaum a. a ieee transactions on geometric snake model for segmentation of medical imagery. medical imaging yilmaz a. and shah m. matching actions in presence of camera motion. com puter vision and image understanding yilmaz a. javed o. and shah m. object tracking a survey. acm computing surveys yin p. criminisi a. winn j. and essa i. tree-based classifiers for bilayer video segmentation. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. yoon k.-j. and kweon i.-s. adaptive support-weight approach for correspondence search. ieee transactions on pattern analysis and machine intelligence yserentant h. on the multi-level splitting of finite element spaces. numerische mathematik yu s. x. and shi j. multiclass spectral clustering. in ninth international confer ence on computer vision pp. nice france. references yu y. and malik j. recovering photometric properties of architectural scenes from photographs. in acm siggraph conference proceedings pp. orlando. yu y. debevec p. malik j. and hawkins t. inverse global illumination recovering reflectance models of real scenes from photographs. in acm siggraph conference proceedings pp. yuan l. sun j. quan l. and shum h.-y. image deblurring with blurrednoisy image pairs. acm transactions on graphics yuan l. sun j. quan l. and shum h.-y. progressive inter-scale and intra-scale non-blind image deconvolution. acm transactions on graphics yuan l. wen f. liu c. and shum h.-y. synthesizing dynamic texture with closed-loop linear dynamic system. in eighth european conference on computer vision pp. prague. yuille a. deformable templates for face recognition. journal of cognitive neuro science yuille a. cccp algorithms to minimize the bethe and kikuchi free energies convergent alternatives to belief propagation. neural computation yuille a. loopy belief propagation mean-field and bethe approximations. in blake a. kohli p. and rother c. advances in markov random fields mit press. yuille a. and poggio t. a generalized ordering constraint for stereo correspondence. a. i. memo artificial intelligence laboratory massachusetts institute of technology. yuille a. vincent l. and geiger d. statistical morphology and bayesian recon struction. journal of mathematical imaging and vision zabih r. and woodfill j. non-parametric local transforms for computing visual correspondence. in third european conference on computer vision pp. stockholm sweden. zach c. fast and high quality fusion of depth maps. in fourth international symposium on data processing visualization and transmission atlanta. zach c. gallup d. and frahm j.-m. fast gain-adaptive klt tracking on the gpu. in cvpr workshop on visual computer vision on gpus anchorage ak. computer vision algorithms and applications draft zach c. klopschitz m. and pollefeys m. disambiguating visual relations using loop constraints. in ieee computer society conference on computer vision and pattern recognition san francisco ca. zach c. pock t. and bischof h. a duality based approach for realtime optical flow. in pattern recognition zach c. pock t. and bischof h. a globally optimal algorithm for robust range image integration. in eleventh international conference on computer vision rio de janeiro brazil. zanella v. and fuentes o. an approach to automatic morphing of face images in frontal view. in mexican international conference on artificial intelligence pp. mexico city. zebedin l. bauer j. karner k. and bischof h. fusion of feature- and areabased information for urban buildings modeling from aerial imagery. in tenth european conference on computer vision pp. marseilles. zelnik-manor l. and perona p. automating joiners. in symposium on non pho torealistic animation and rendering annecy. zhang g. jia j. wong t.-t. and bao h. recovering consistent video depth in ieee computer society conference on computer maps via bundle optimization. vision and pattern recognition anchorage ak. zhang j. mcmillan l. and yu j. robust tracking and stereo matching under variable illumination. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. zhang j. marszalek m. lazebnik s. and schmid c. local features and kernels for classification of texture and object categories a comprehensive study. international journal of computer vision zhang l. curless b. and seitz s. spacetime stereo shape recovery for dynamic scenes. in ieee computer society conference on computer vision and pattern recognition pp. madison wi. zhang l. dugas-phocion g. samson j.-s. and seitz s. m. single view modeling of free-form scenes. journal of visualization and computer animation zhang l. snavely n. curless b. and seitz s. m. spacetime faces high resolution capture for modeling and animation. acm transactions on graphics references zhang r. tsai p.-s. cryer j. e. and shah m. shape from shading a survey. ieee transactions on pattern analysis and machine intelligence zhang y. and kambhamettu c. on scene flow and structure recovery from ieee transactions on systems man and cybernetics multiview image sequences. zhang z. iterative point matching for registration of free-form curves and surfaces. international journal of computer vision zhang z. determining the epipolar geometry and its uncertainty a review. international journal of computer vision zhang z. on the optimization criteria used in two-view motion analysis. ieee transactions on pattern analysis and machine intelligence zhang z. a flexible new technique for camera calibration. ieee transactions on pattern analysis and machine intelligence zhang z. and he l.-w. whiteboard scanning and image enhancement. digital signal processing zhang z. and shan y. in second european workshop on structure from multiple images of large-scale environments pp. dublin ireland. a progressive scheme for stereo matching. zhang z. deriche r. faugeras o. and luong q. a robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry. artificial intelligence zhao g. and pietik ainen m. dynamic texture recognition using local binary patterns with an application to facial expressions. ieee transactions on pattern analysis and machine intelligence zhao w. chellappa r. phillips p. j. and rosenfeld a. face recognition a literature survey. acm computing surveys zheng j. y. acquiring models from sequences of contours. ieee transac tions on pattern analysis and machine intelligence zheng k. c. kang s. b. cohen m. and szeliski r. layered depth panoramas. in ieee computer society conference on computer vision and pattern recognition minneapolis mn. zheng y. lin s. and kang s. b. in ieee computer society conference on computer vision and pattern recognition pp. new york city ny. single-image vignetting correction. computer vision algorithms and applications draft zheng y. yu j. kang s.-b. lin s. and kambhamettu c. single-image vignetting correction using radial gradient symmetry. in ieee computer society conference on computer vision and pattern recognition anchorage ak. zheng y. zhou x. s. georgescu b. zhou s. k. and comaniciu d. example based non-rigid shape detection. in ninth european conference on computer vision pp. zheng y.-t. zhao m. song y. adam h. buddemeier u. bissacco a. brucher f. chua t.-s. and neven h. tour the world building a web-scale landmark in ieee computer society conference on computer vision and recognition engine. pattern recognition miami beach fl. zhong j. and sclaroff s. segmenting foreground objects from a dynamic textured background via a robust kalman filter. in ninth international conference on computer vision pp. nice france. zhou c. lin s. and nayar s. coded aperture pairs for depth from defocus. in twelfth international conference on computer vision kyoto japan. zhou y. gu l. and zhang h.-j. bayesian tangent shape model estimating shape and pose parameters via bayesian inference. in ieee computer society conference on computer vision and pattern recognition pp. madison wi. zhu l. chen y. lin y. lin c. and yuille a. recursive segmentation and recognition templates for parsing. in advances in neural information processing systems. zhu s.-c. and mumford d. a stochastic grammar of images. foundations and trends in computer graphics and computer vision zhu s. c. and yuille a. l. region competition unifying snakes region growing ieee transactions on pattern and bayesmdl for multiband image segmentation. analysis and machine intelligence zhu z. and kanade t. modeling and representations of large-scale scenes. international journal of computer vision zisserman a. giblin p. j. and blake a. the information available to a moving observer from specularities. image and vision computing zitnick c. l. and kanade t. a cooperative algorithm for stereo matching and occlusion detection. ieee transactions on pattern analysis and machine intelligence references zitnick c. l. and kang s. b. stereo for image-based rendering using image over-segmentation. international journal of computer vision zitnick c. l. jojic n. and kang s. b. consistent segmentation for optical flow estimation. in tenth international conference on computer vision pp. beijing china. zitnick c. l. kang s. b. uyttendaele m. winder s. and szeliski r. highquality video view interpolation using a layered representation. acm transactions on graphics siggraph zitov aa b. and flusser j. vision computing image registration methods a survey. image and zoghlami i. faugeras o. and deriche r. using geometric corners to build a mosaic from a set of images. in ieee computer society conference on computer vision and pattern recognition pp. san juan puerto rico. zongker d. e. werner d. m. curless b. and salesin d. h. environment matting and compositing. in acm siggraph conference proceedings pp. zorin d. schr oder p. and sweldens w. interpolating subdivision for meshes with arbitrary topology. in acm siggraph conference proceedings pp. new orleans. computer vision algorithms and applications draft index rotations see rotations alignment absolute orientation orthogonal procrustes photography video absolute orientation active appearance model active contours active illumination active rangefinding active shape model activity recognition adaptive smoothing affine transforms affinities normalizing algebraic multigrid algorithms testing viii aliasing alignment see image alignment alpha opacity pre-multiplied alpha matte ambient illumination analog to digital conversion anisotropic diffusion anisotropic filtering anti-aliasing filter aperture aperture problem applications model reconstruction photography augmented reality automotive safety background replacement biometrics colorization de-interlacing digital heritage document scanning edge editing facial animation flash photography frame interpolation gaze correction head tracking hole filling image restoration image search computer vision algorithms and applications draft industrial intelligent photo editing internet photos location recognition machine inspection match move medical imaging morphing mosaic-based video compression non-photorealistic rendering optical character recognition panography performance-driven animation photo pop-up photo tourism photomontage planar pattern tracking rotoscoping scene completion scratch removal single view reconstruction tonal adjustment video denoising video stabilization video summarization video-based walkthroughs videomouse view morphing visual effects whiteboard scanning z-keying arc length parameterization of a curve architectural reconstruction area statistics mean perimeter second moment aspect ratio augmented reality auto-calibration automatic gain control axisangle representation of rotations b-snake b-spline cubic multilevel octree background plate background subtraction bag of words distance metrics band-pass filter bartlett filter see bilinear kernel bayer pattern sensor mosaic demosaicing bayes rule map a posteriori estimate posterior distribution bayesian modeling map estimate matting posterior distribution prior distribution uncertainty belief propagation update rule bias bidirectional reflectance distribution func tion see brdf bilateral filter joint range kernel tone mapping bilinear blending index bilinear kernel biometrics bipartite problem blind image deconvolution block-based motion estimation matching blocks world blue screen matting blur kernel estimation blur removal body color boltzmann distribution boosting adaboost algorithm decision stump weak learner border effects boundary detection box filter boxlet brdf intrinsic optical blur patterns photometric plumb-line method point spread function radial distortion radiometric rotational motion slant edge vanishing points vignetting camera matrix catadioptric optics category-level recognition bag of words data sets part-based segmentation surveys ccd blooming anisotropic isotropic recovery spatially varying brightness brightness constancy brightness constancy constraint bundle adjustment calibration see camera calibration calibration matrix camera calibration accuracy aliasing extrinsic central difference chained transformations chamfer matching characteristic function characteristic polynomial chirality cholesky factorization algorithm incomplete sparse chromatic aberration chromaticity coordinates cie lab see color cie luv see color cie xyz see color circle of confusion computer vision algorithms and applications draft clahe see histogram equalization clustering agglomerative cluster analysis divisive cmos co-vector coefficient matrix collineation color balance camera demosaicing fringing hue saturation value lab luv primaries profile ratios rgb transform twist xyz yiq yuv color filter array color line model colorchecker chart colorization compositing image stitching opacity over operator surface transparency compression computational photography active illumination flash and non-flash high dynamic range references tone mapping concentric mosaic condensation condition number conditional random field confusion matrix conic section conjugate gradient descent algorithm non-linear preconditioned connected components constellation model content based image retrieval continuation method contour arc length parameterization chain code matching smoothing contrast controlled-continuity spline convolution kernel mask superposition coring correlation windowed correspondence map cramer rao lower bound cube map hough transform index image stitching selectivity curve arc length parameterization evolution matching smoothing discrete cosine transform discrete fourier transform discriminative random field disparity disparity map cylindrical coordinates multiple data energy data sets and test databases recognition de-interlacing decimation decimation kernels bicubic binomial qmf windowed sinc demosaicing depth from defocus depth map see disparity map depth of field di-chromatic reflection model difference matting difference of gaussians difference of low-pass diffuse reflection diffusion anisotropic digital camera color color filter array compression direct current direct linear transform direct sparse matrix techniques directional derivative disparity space image generalized displaced frame difference displacement field distance from face space distance in face space distance map see distance transform distance transform euclidean image stitching manhattan block signed domain a function domain scaling law downsampling see decimation dynamic programming monotonicity ordering constraint scanline optimization dynamic snake dynamic texture earth mover s distance edge detection boundary detection canny chain code color difference of gaussian edgel element hysteresis computer vision algorithms and applications draft laplacian of gaussian linking marching cubes scale selection steerable filter zero crossing eigenface eigenvalue decomposition eigenvector elastic deformations image registration elastic nets elliptical weighted average environment map environment matte epanechnikov kernel epipolar constraint epipolar geometry pure rotation pure translation epipolar line epipolar plane image epipolar volume epipole error rates accuracy false negative false positive positive predictive value precision recall roc curve true negative true positive errors-in-variable model heteroscedastic essential matrix algorithm eight-point algorithm re-normalization seven-point algorithm twisted pair estimation theory euclidean transformation euler angles expectation maximization exponential twist exposure bracketing exposure value f-number face detection boosting cascade of classifiers clustering and pca data sets neural networks support vector machines face modeling face recognition active appearance model data sets eigenface elastic bunch graph matching local binary patterns local feature analysis face transfer facial motion capture factor graph factorization missing data projective fast fourier transform fast marching method feature descriptor index bias and gain normalization gloh patch pca-sift performance quantization sift steerable filter feature detection adaptive non-maximal suppression affine invariance auto-correlation f orstner harris laplacian of gaussian mser region repeatability rotation invariance scale invariance feature matching densification efficiency error rates hashing indexing structure k-d trees locality sensitive hashing nearest neighbor strategy verification feature tracking affine learning feature tracks feature-based alignment iterative jacobian least squares match verification ransac robust field of experts fill factor fill-in filter adaptive band-pass bilateral directional derivative edge-preserving laplacian of gaussian median moving average non-linear separable steerable filter coefficients filter kernel see kernel finding faces see face detection finite element analysis stiffness matrix finite impulse response filter fisher information matrix fisher s linear discriminant fisheye lens flash and non-flash merging flash matting flip-book animation flying spot scanner focal length focus shape-from computer vision algorithms and applications draft focus of expansion form factor forward mapping see forward warping forward warping fourier transform discrete examples magnitude pairs parseval s theorem phase power spectrum properties two-dimensional fourier-based motion estimation rotations and scale frame interpolation free-viewpoint video fundamental matrix estimation see essential matrix fundamental radiometric relation gain gamma gamma correction gap closing stitching garbage matte gaussian kernel gaussian markov random field gaussian mixtures see mixture of gaussians gaussian pyramid gaussian scale mixtures gaze correction geman mcclure function generalized cylinders geodesic active contour geodesic distance geometric image formation geometric lens aberrations geometric primitives homogeneous coordinates lines normal vector normal vectors planes points geometric transformations perspective rotations affine bilinear calibration matrix collineation euclidean forward warping hierarchy homography inverse warping perspective projections projective rigid body scaled rotation similarity translation geometry image gesture recognition gibbs distribution gibbs sampler gimbal lock gist a scene global illumination global optimization index gpu algorithms gradient location-orientation histogram graduated non-convexity graph cuts mrf inference normalized cuts graph-based segmentation grassfire transform ground control points hammersley clifford theorem hann window harris corner detector see feature detection head tracking active appearance model helmholtz reciprocity hessian eigenvalues image inverse local patch-based rank-deficient reduced motion sparse heteroscedastic hidden markov model hierarchical motion estimation high dynamic range imaging formats tone mapping highest confidence first highest confidence first hilbert transform pair histogram equalization locally adaptive histogram intersection histogram of oriented gradients history of computer vision hole filling homogeneous coordinates homography hough transform cascaded cube map generalized human body shape modeling human motion tracking activity recognition adaptive shape modeling background subtraction flow-based initialization kinematic models particle filtering probabilistic models hyper-laplacian ideal points ill-posed problems illusions image alignment feature-based intensity-based intensity-based vs. feature-based image analogies image blending feathering gist gradient domain image stitching poisson pyramid image compositing see compositing computer vision algorithms and applications draft image compression image decimation image deconvolution see blur removal image filtering image formation geometric photometric image gradient constraint image interpolation image matting image processing textbooks image pyramid image resampling test images image restoration blur removal deblocking inpainting noise removal using mrfs image search image segmentation see segmentation image sensing see sensing image statistics image stitching automatic bundle adjustment compositing coordinate transformations cube map cylindrical de-ghosting direct vs. feature-based exposure compensation feathering gap closing global alignment homography motion models panography parallax removal photogrammetry pixel selection planar perspective motion recognizing panoramas rotational motion seam selection spherical up vector selection image warping image-based modeling image-based rendering concentric mosaic environment matte impostors layered depth image layers light field lumigraph modeling vs. rendering continuum sprites surface light field unstructured lumigraph view interpolation view-dependent texture maps image-based visual hull imagenet implicit surface impostors see sprites impulse response incremental refinement motion estimation incremental rotation indexing structure index indicator function industrial applications infinite impulse response filter influence function information matrix inpainting instance recognition algorithm data sets geometric alignment inverted index large scale match verification query expansion stop list visual words vocabulary tree integrability constraint integral image integrating sphere intelligent scissors interaction potential interactive computer vision international color consortium internet photos interpolation interpolation kernels bicubic bilinear binomial sinc spline intrinsic camera calibration intrinsic images inverse kinematics inverse mapping see inverse warping inverse problems inverse warping iso setting iterated closest point iterated conditional modes iterative back projection iterative feature-based alignment iterative sparse matrix techniques conjugate gradient iteratively reweighted least squares jacobian image motion sparse joint bilateral filter joint domain space k-d trees k-means kalman snakes kanade lucas tomasi tracker karhunen loeve transform kernel bilinear gaussian low-pass sobel operator unsharp mask kernel basis function kernel density estimation keypoint detection see feature detection kinematic model kruppa equations lab see color luv see color norm l norm lambertian reflection computer vision algorithms and applications draft laplacian matting laplacian of gaussian filter laplacian pyramid blending perfect reconstruction latent dirichlet process layered depth image layered depth panorama layered motion estimation transparent layers image-based rendering layout consistent random field learning in computer vision least median of squares least squares iterative solvers linear non-linear robust see robust least squares sparse total weighted lens compound nodal point thin lens distortions calibration decentering radial spline-based tangential lens law level of detail level sets fast marching method geodesic active contour levenberg marquardt lifting see wavelets light field higher dimensional light slab ray space rendering surface lightness line at infinity line detection hough transform ransac simplification successive approximation line equation line fitting uncertainty line hull see visual hull line labeling line process line spread function line-based structure from motion linear algebra least squares matrix decompositions references linear blend linear discriminant analysis linear filtering linear operator superposition linear shift invariant filter live-wire local distance functions local operator index locality sensitive hashing locally adaptive histogram equalization location recognition loopy belief propagation low-pass filter sinc lumigraph unstructured luminance lumisphere m-estimator mahalanobis distance manifold mosaic markov chain monte carlo markov random field cliques directed edges dynamic flux inference see mrf inference layout consistent learning parameters line process neighborhood order random walker stereo matching marr s framework computational theory hardware implementation representations and algorithms match move matrix decompositions cholesky eigenvalue qr singular value square root matte reflection matting alpha matte bayesian blue screen difference flash grabcut laplacian natural optimization-based poisson shadow smoke triangulation trimap two screen video maximally stable extremal region maximum a posteriori estimate mean absolute difference mean average precision mean shift bandwidth selection mean square error measurement equation measurement matrix measurement model see bayesian model medial axis transform median absolute deviation median filter weighted medical image registration medical image segmentation computer vision algorithms and applications draft membrane mesh-based warping metamer metric learning metric tree mip-mapping trilinear mixture of gaussians color model expectation maximization mixing coefficient soft assignment model selection model-based reconstruction architecture heads and faces human body model-based stereo models bayesian forward physically based physics-based probabilistic modular eigenspace modulation transfer function morphable model body face multidimensional morphing body face automated facial feature feature-based flow-based video textures view morphing morphological operator closing dilation erosion opening morphology mosaic see image stitching mosaics motion models video compression whiteboard and document scanning motion compensated video compression motion compensation motion estimation affine aperture problem compositional fourier-based frame interpolation hierarchical incremental refinement layered learning linear appearance variation optical flow parametric patch-based phase correlation quadtree spline-based reflections spline-based translational transparent uncertainty modeling motion field motion models index learned motion segmentation motion stereo motion-based user interaction moving least squares mrf inference alpha expansion belief propagation dynamic programming expansion move gradient descent graph cuts highest confidence first highest confidence first iterated conditional modes linear programming loopy belief propagation markov chain monte carlo simulated annealing stochastic gradient descent swap move swendsen wang volumetric voxel coloring multigrid algebraic multiple hypothesis tracking multiple-center-of-projection images multiresolution representation mutual information natural image matting nearest neighbor distance ratio matching see feature matching negative posterior log likelihood neighborhood operator neural networks nintendo wii nodal point noise sensor multi-frame motion estimation multi-pass transforms multi-perspective panoramas multi-perspective plane sweep multi-view stereo epipolar plane image evaluation initialization requirements reconstruction algorithm scene representation shape priors silhouettes space carving spatio-temporally shiftable window taxonomy visibility noise level function noise removal non-linear filter non-linear least squares seeleast squares non-maximal suppression see feature detec tion non-parametric density modeling non-photorealistic rendering non-rigid motion normal equations normal map image normal vector normalized cross-correlation normalized cuts computer vision algorithms and applications draft intervening contour total variation normalized device coordinates normalized sum of squared differences norms l nyquist rate frequency object detection car face part-based pedestrian object-centered projection occluding contours octree reconstruction octree spline omnidirectional vision systems opacity operator linearity optic flow see optical flow optical center optical flow anisotropic smoothness evaluation fusion move global and local markov random field multi-frame normal flow patch-based region-based regularization robust regularization smoothness optical flow constraint equation optical illusions optical transfer function optical triangulation optics chromatic aberration seidel aberrations vignetting optimal motion estimation oriented particles orthogonal procrustes orthographic projection osculating circle over operator overview padding panography panorama see image stitching panorama with depth para-perspective projection parallel tracking and mapping parameter sensitive hashing parametric motion estimation parametric surface parametric transformation parseval s theorem see fourier transform part-based recognition constellation model particle filtering parzen window pascal visual object classes challenge patch-based motion estimation peak signal-to-noise ratio pedestrian detection penumbra index performance-driven animation perspective n-point problem perspective projection perspective transform phase correlation phong shading photo pop-up photo tourism photo-mosaic photoconsistency photometric image formation calibration global illumination lighting optics radiosity reflectance shading photometric stereo photometry photomontage physically based models physics-based vision pictorial structures pixel transform pl ucker coordinates planar pattern tracking plane at infinity plane equation plane plus parallax plane sweep plane-based structure from motion plenoptic function plenoptic modeling plumb-line calibration method point distribution model point operator point process point spread function estimation point-based representations points at infinity poisson blending equations matting noise surface reconstruction polar coordinates polar projection polyphase filter pop-out effect pose estimation iterative power spectrum precision see error rates mean average preconditioning principal component analysis face modeling generalized missing data prior energy prior model see bayesian model profile curves progressive mesh projections object-centered orthographic para-perspective perspective projective reconstruction projective depth projective disparity projective space computer vision algorithms and applications draft prosac sample consensus psnr see peak signal-to-noise ratio pyramid blending gaussian half-octave laplacian motion estimation octave radial frequency implementation steerable pyramid match kernel qr factorization quadratic form quadrature mirror filter quadric equation quadtree spline motion estimation restricted quaternions antipodal multiplication query by image content query expansion quincunx sampling radial basis function radial distortion barrel calibration parameters pincushion radiance map radiometric image formation radiometric response function radiometry radiosity random walker range a function range data see range scan range image see range scan range scan alignment large scenes merging registration segmentation volumetric range sensing coded pattern light stripe shadow stripe spacetime stereo stereo texture pattern time of flight ransac sample consensus inliers preemptive progressive raw image format ray space field ray tracing rayleigh quotient recall see error rates receiver operating characteristic area under the curve mean average precision roc curve recognition models category color similarity context index contour-based data sets face instance large scale learning part-based scene understanding segmentation shape context rectangle detection rectification standard rectified geometry recursive filter reference plane reflectance reflectance map reflectance modeling reflection di-chromatic diffuse specular region merging splitting region segmentation see segmentation registration see image alignment feature-based intensity-based medical image regularization robust tion robust least squares iteratively reweighted robust penalty function robust regularization robust statistics inliers m-estimator rodriguez s formula root mean square error rotations euler angles axisangle exponential twist incremental interpolation quaternions rodriguez s formula sampling scale invariant feature transform scale-space scatter matrix between-class within-class scattered data interpolation scene completion scene flow scene understanding regularization parameter residual error rgb green blue see color rigid body transformation robust error metric see robust penalty func gist scene alignment schur complement scratch removal seam selection image stitching computer vision algorithms and applications draft second-order cone programming seed and grow stereo structure from motion segmentation active contours affinities binary mrf condensation connected components energy-based for recognition geodesic active contour geodesic distance grabcut graph cuts graph-based hierarchical intelligent scissors joint feature space k-means level sets mean shift medical image merging minimum description length mixture of gaussians mumford shah non-parametric normalized cuts probabilistic aggregation random walker snakes splitting stereo matching thresholding tobogganing watershed weighted aggregation seidel aberrations self-calibration bundle adjustment kruppa equations sensing aliasing color color balance gamma pipeline sampling sampling pitch sensor noise amplifier dark current fixed pattern shot noise separable filtering shading equation shape-from shadow matting shape context shape from focus photometric stereo profiles shading silhouettes specularities stereo texture shape parameters shape-from-x focus index photometric stereo shading texture shift invariance shiftable multi-scale transform shutter speed signed distance function silhouette-based reconstruction octree visual hull similarity transform simulated annealing simultaneous localization and mapping sinc filter interpolation low-pass windowed single view metrology singular value decomposition skeletal set skeleton skew skin color detection slant edge calibration slippery spring smoke matting smoothness constraint smoothness penalty snakes ballooning dynamic internal energy kalman shape priors slippery spring soft assignment software space carving multi-view stereo spacetime stereo sparse flexible model sparse matrices compressed sparse row skyline storage sparse methods direct iterative spatial pyramid matching spectral response function spectral sensitivity specular flow specular reflection spherical coordinates spherical linear interpolation spin image splatting see forward warping volumetric spline controlled continuity octree quadtree thin plate spline-based motion estimation splining images blending see laplacian pyramid sprites image-based rendering motion estimation video video compression with depth statistical decision theory steerable filter steerable pyramid steerable random field computer vision algorithms and applications draft stereo aggregation methods coarse-to-fine cooperative algorithms correspondence curve-based dense correspondence depth map dynamic programming edge-based epipolar geometry feature-based global optimization graph cut layers local methods model-based multi-view non-parametric similarity measures photoconsistency plane sweep rectification region-based scanline optimization seed and grow segmentation-based semi-global optimization shiftable window similarity measure spacetime sparse correspondence sub-pixel refinement support region taxonomy uncertainty window-based winner-take-all stereo-based head tracking stiffness matrix stitching see image stitching stochastic gradient descent structural similarity index structure from motion affine bas-relief ambiguity bundle adjustment constrained factorization feature tracks iterative factorization line-based multi-frame non-rigid orthographic plane-based projective factorization seed and grow self-calibration skeletal set two-frame uncertainty subdivision surface subdivision connectivity subspace learning sum of absolute differences sum of squared differences bias and gain fourier-based computation normalized surface weighted windowed sum of sum of squared differences index summed area table super-resolution example-based faces hallucination prior superposition principle superquadric support vector machine surface element surface interpolation surface light field surface representations non-parametric parametric point-based simplification splines subdivision surface symmetry-seeking triangle mesh surface simplification swendsen wang algorithm telecentric lens temporal derivative temporal texture term frequency-inverse document frequency testing algorithms viii textonboost texture shape-from texture addressing mode texture map recovery view-dependent texture mapping anisotropic filtering mip-mapping multi-pass trilinear interpolation texture synthesis by numbers hole filling image quilting non-parametric transfer thin lens thin-plate spline thresholding through-the-lens camera control tobogganing tonal adjustment tone mapping adaptive bilateral filter global gradient domain halos interactive local scale selection total least squares total variation tracking feature head human motion multiple hypothesis planar pattern ptam translational motion estimation bias and gain transparency travelling salesman problem computer vision algorithms and applications draft tri-chromatic sensing tri-stimulus values triangulation trilinear interpolation see mip-mapping trimap trust region method two-dimensional fourier transform uncanny valley uncertainty correspondence modeling weighting unsharp mask upsampling see interpolation vanishing point detection hough least squares modeling uncertainty variable reordering minimum degree multi-frontal nested dissection video animating pictures sprites video texture virtual viewpoint video walkthroughs videomouse view correlation view interpolation view morphing view-based eigenspace view-dependent texture maps vignetting mechanical natural virtual viewpoint video visual hull image-based visual illusions visual odometry visual words vocabulary tree volumetric reconstruction volumetric range image processing variable state dimension filter variational method video compression volumetric representations voronoi diagram voxel coloring motion compensated video compression video denoising video matting video objects video sprites video stabilization video texture video-based animation video-based rendering multi-view stereo watershed basins oriented wavelets compression lifting overcomplete second generation index self-inverting tight frame weighted weaving wall weighted least squares weighted prediction and gain white balance whitening transform wiener filter wire removal wrapping mode xyz see color zippering