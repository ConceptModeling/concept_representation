c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn gaussian processes for machine learning c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn adaptive computation and machine learning thomas dietterich editor christopher bishop david heckerman michael jordan and michael kearns associate editors bioinformatics the machine learning approach pierre baldi and s ren brunak reinforcement learning an introduction richard s. sutton and andrew g. barto graphical models for machine learning and digital communication brendan j. frey learning in graphical models michael i. jordan causation prediction and search second edition peter spirtes clark glymour and richard scheines principles of data mining david hand heikki mannila and padhraic smyth bioinformatics the machine learning approach second edition pierre baldi and s ren brunak learning kernel classifiers theory and algorithms ralf herbrich learning with kernels support vector machines regularization optimization and beyond bernhard sch olkopf and alexander j. smola introduction to machine learning ethem alpaydin gaussian processes for machine learning carl edward rasmussen and christopher k. i. williams c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn gaussian processes for machine learning carl edward rasmussen christopher k. i. williams the mit press cambridge massachusetts london england c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn massachusetts institute of technology all rights reserved. no part of this book may be reproduced in any form by any electronic or mechanical means photocopying recording or information storage and retrieval without permission in writing from the publisher. mit press books may be purchased at special quantity discounts for business or sales promotional use. for information please email special salesmitpress.mit.edu or write to special sales department the mit press hayward street cambridge ma typeset by the authors using latex this book was printed and bound in the united states of america. library of congress cataloging-in-publication data rasmussen carl edward. gaussian processes for machine learning carl edward rasmussen christopher k. i. williams. p. cm. computation and machine learning includes bibliographical references and indexes. isbn gaussian processes data processing. machine learning mathematical models. i. williams christopher k. i. ii. title. iii. series. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn the actual science of logic is conversant at present only with things either certain impossible or entirely doubtful none of which we have to reason on. therefore the true logic for this world is the calculus of probabilities which takes account of the magnitude of the probability which is or ought to be in a reasonable man s mind. james clerk maxwell c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn contents series foreword xi preface xiii symbols and notation xvii introduction a pictorial introduction to bayesian modelling roadmap regression weight-space view the standard linear model projections of inputs into feature space function-space view varying the hyperparameters decision theory for regression an example application smoothing weight functions and equivalent kernels incorporating explicit basis functions marginal likelihood history and related work exercises classification classification problems decision theory for classification linear models for classification gaussian process classification the laplace approximation for the binary gp classifier posterior predictions implementation marginal likelihood multi-class laplace approximation implementation expectation propagation predictions marginal likelihood implementation experiments a toy problem one-dimensional example binary handwritten digit classification example handwritten digit classification example discussion sections marked by an asterisk contain advanced material that may be omitted on a first reading. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn viii contents appendix moment derivations exercises covariance functions preliminaries examples of covariance functions mean square continuity and differentiability stationary covariance functions dot product covariance functions other non-stationary covariance functions making new kernels from old eigenfunction analysis of kernels an analytic example numerical approximation of eigenfunctions string kernels fisher kernels kernels for non-vectorial inputs exercises model selection and adaptation of hyperparameters the model selection problem bayesian model selection cross-validation model selection for gp regression marginal likelihood cross-validation examples and discussion model selection for gp classification derivatives of the marginal likelihood for laplace s approximation derivatives of the marginal likelihood for ep cross-validation example exercises relationships between gps and other models reproducing kernel hilbert spaces regularization regularization defined by differential operators obtaining the regularized solution the relationship of the regularization view to gaussian process support vector machines prediction spline models a gaussian process spline construction support vector classification support vector regression least-squares classification probabilistic least-squares classification c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn contents ix relevance vector machines exercises some specific examples of equivalent kernels theoretical perspectives the equivalent kernel asymptotic analysis consistency equivalence and orthogonality average-case learning curves pac-bayesian analysis the pac framework pac-bayesian analysis pac-bayesian analysis of gp classification comparison with other supervised learning methods appendix learning curve for the ornstein-uhlenbeck process exercises approximation methods for large datasets reduced-rank approximations of the gram matrix greedy approximation approximations for gpr with fixed hyperparameters subset of regressors the nystr om method projected process approximation bayesian committee machine comparison of approximate gpr methods approximations for gpc with fixed hyperparameters approximating the marginal likelihood and its derivatives appendix equivalence of sr and gpr using the nystr om approximate iterative solution of linear systems subset of datapoints kernel exercises further issues and conclusions multiple outputs noise models with dependencies non-gaussian likelihoods derivative observations prediction with uncertain inputs mixtures of gaussian processes global optimization evaluation of integrals student s t process invariances latent variable models conclusions and future directions c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn x contents appendix a mathematical background joint marginal and conditional probability gaussian identities matrix identities matrix derivatives matrix norms cholesky decomposition entropy and kullback-leibler divergence limits measure and integration lp spaces fourier transforms convexity appendix b gaussian markov processes fourier analysis continuous-time gaussian markov processes sampling and periodization continuous-time gmps on r the solution of the corresponding sde on the circle discrete-time gmps on z the solution of the corresponding difference equation on pn discrete-time gaussian markov processes the relationship between discrete-time and sampled continuous-time gmps markov processes in higher dimensions appendix c datasets and code bibliography author index subject index c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn series foreword the goal of building systems that can adapt to their environments and learn from their experience has attracted researchers from many fields including computer science engineering mathematics physics neuroscience and cognitive science. out of this research has come a wide variety of learning techniques that have the potential to transform many scientific and industrial fields. recently several research communities have converged on a common set of issues surrounding supervised unsupervised and reinforcement learning problems. the mit press series on adaptive computation and machine learning seeks to unify the many diverse strands of machine learning research and to foster high quality research and innovative applications. one of the most active directions in machine learning has been the development of practical bayesian methods for challenging learning problems. gaussian processes for machine learning presents one of the most important bayesian machine learning approaches based on a particularly effective method for placing a prior distribution over the space of functions. carl edward rasmussen and chris williams are two of the pioneers in this area and their book describes the mathematical foundations and practical application of gaussian processes in regression and classification tasks. they also show how gaussian processes can be interpreted as a bayesian version of the well-known support vector machine methods. students and researchers who study this book will be able to apply gaussian process methods in creative ways to solve a wide range of problems in science and engineering. thomas dietterich c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn preface over the last decade there has been an explosion of work in the kernel machines area of machine learning. probably the best known example of this is work on support vector machines but during this period there has also been much activity concerning the application of gaussian process models to machine learning tasks. the goal of this book is to provide a systematic and unified treatment of this area. gaussian processes provide a principled practical probabilistic approach to learning in kernel machines. this gives advantages with respect to the interpretation of model predictions and provides a wellfounded framework for learning and model selection. theoretical and practical developments of over the last decade have made gaussian processes a serious competitor for real supervised learning applications. roughly speaking a stochastic process is a generalization of a probability distribution describes a finite-dimensional random variable to functions. by focussing on processes which are gaussian it turns out that the computations required for inference and learning become relatively easy. thus the supervised learning problems in machine learning which can be thought of as learning a function from examples can be cast directly into the gaussian process framework. our interest in gaussian process models in the context of machine learning was aroused in while we were both graduate students in geoff hinton s neural networks lab at the university of toronto. this was a time when the field of neural networks was becoming mature and the many connections to statistical physics probabilistic models and statistics became well known and the first kernel-based learning algorithms were becoming popular. in retrospect it is clear that the time was ripe for the application of gaussian processes to machine learning problems. many researchers were realizing that neural networks were not so easy to apply in practice due to the many decisions which needed to be made what architecture what activation functions what learning rate etc. and the lack of a principled framework to answer these questions. the probabilistic framework was pursued using approximations by mackay and using markov chain monte carlo methods by neal neal was also a graduate student in the same lab and in his thesis he sought to demonstrate that using the bayesian formalism one does not necessarily have problems with overfitting when the models get large and one should pursue the limit of large models. while his own work was focused on sophisticated markov chain methods for inference in large finite networks he did point out that some of his networks became gaussian processes in the limit of infinite size and there may be simpler ways to do inference in this case. it is perhaps interesting to mention a slightly wider historical perspective. the main reason why neural networks became popular was that they allowed the use of adaptive basis functions as opposed to the well known linear models. the adaptive basis functions or hidden units could learn hidden features kernel machines gaussian process gaussian processes in machine learning neural networks large neural networks gaussian processes adaptive basis functions c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn xiv many fixed basis functions useful representations supervised learning in statistics statistics and machine learning data and models algorithms and predictions bridging the gap preface useful for the modelling problem at hand. however this adaptivity came at the cost of a lot of practical problems. later with the advancement of the kernel era it was realized that the limitation of fixed basis functions is not a big restriction if only one has enough of them i.e. typically infinitely many and one is careful to control problems of overfitting by using priors or regularization. the resulting models are much easier to handle than the adaptive basis function models but have similar expressive power. thus one could claim that far a machine learning is concerned the adaptive basis functions were merely a decade-long digression and we are now back to where we came from. this view is perhaps reasonable if we think of models for solving practical learning problems although mackay ch. for example raises concerns by asking did we throw out the baby with the bath water? as the kernel view does not give us any hidden representations telling us what the useful features are for solving a particular problem. as we will argue in the book one answer may be to learn more sophisticated covariance functions and the hidden properties of the problem are to be found here. an important area of future developments for gp models is the use of more expressive covariance functions. supervised learning problems have been studied for more than a century in statistics and a large body of well-established theory has been developed. more recently with the advance of affordable fast computation the machine learning community has addressed increasingly large and complex problems. much of the basic theory and many algorithms are shared between the statistics and machine learning community. the primary differences are perhaps the types of the problems attacked and the goal of learning. at the risk of oversimplification one could say that in statistics a prime focus is often in understanding the data and relationships in terms of models giving approximate summaries such as linear relations or independencies. in contrast the goals in machine learning are primarily to make predictions as accurately as possible and to understand the behaviour of learning algorithms. these differing objectives have led to different developments in the two fields for example neural network algorithms have been used extensively as black-box function approximators in machine learning but to many statisticians they are less than satisfactory because of the difficulties in interpreting such models. gaussian process models in some sense bring together work in the two communities. as we will see gaussian processes are mathematically equivalent to many well known models including bayesian linear models spline models large neural networks suitable conditions and are closely related to others such as support vector machines. under the gaussian process viewpoint the models may be easier to handle and interpret than their conventional counterparts such as e.g. neural networks. in the statistics community gaussian processes have also been discussed many times although it would probably be excessive to claim that their use is widespread except for certain specific applications such as spatial models in meteorology and geology and the analysis of computer experiments. a rich theory also exists for gaussian process models c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn preface in the time series analysis literature some pointers to this literature are given in appendix b. the book is primarily intended for graduate students and researchers in machine learning at departments of computer science statistics and applied mathematics. as prerequisites we require a good basic grounding in calculus linear algebra and probability theory as would be obtained by graduates in numerate disciplines such as electrical engineering physics and computer science. for preparation in calculus and linear algebra any good university-level textbook on mathematics for physics or engineering such as arfken would be fine. for probability theory some familiarity with multivariate distributions the gaussian and conditional probability is required. some background mathematical material is also provided in appendix a. the main focus of the book is to present clearly and concisely an overview of the main ideas of gaussian processes in a machine learning context. we have also covered a wide range of connections to existing models in the literature and cover approximate inference for faster practical algorithms. we have presented detailed algorithms for many methods to aid the practitioner. software implementations are available from the website for the book see appendix c. we have also included a small set of exercises in each chapter we hope these will help in gaining a deeper understanding of the material. in order limit the size of the volume we have had to omit some topics such as for example markov chain monte carlo methods for inference. one of the most difficult things to decide when writing a book is what sections not to write. within sections we have often chosen to describe one algorithm in particular in depth and mention related work only in passing. although this causes the omission of some material we feel it is the best approach for a monograph and hope that the reader will gain a general understanding so as to be able to push further into the growing literature of gp models. the book has a natural split into two parts with the chapters up to and including chapter covering core material and the remaining sections covering the connections to other methods fast approximations and more specialized properties. some sections are marked by an asterisk. these sections may be omitted on a first reading and are not pre-requisites for later material. we wish to express our considerable gratitude to the many people with whom we have interacted during the writing of this book. in particular moray allan david barber peter bartlett miguel carreira-perpi n an marcus gallagher manfred opper anton schwaighofer matthias seeger hanna wallach joe whittaker and andrew zisserman all read parts of the book and provided valuable feedback. dilan g or ur malte kuss iain murray joaquin qui nonerocandela leif rasmussen and sam roweis were especially heroic and provided comments on the whole manuscript. we thank chris bishop miguel carreiraperpi n an nando de freitas zoubin ghahramani peter gr unwald mike jordan john kent radford neal joaquin qui nonero-candela ryan rifkin stefan schaal anton schwaighofer matthias seeger peter sollich ingo steinwart xv intended audience focus scope book organization acknowledgements c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn xvi preface errata looking ahead amos storkey volker tresp sethu vijayakumar grace wahba joe whittaker and tong zhang for valuable discussions on specific issues. we also thank bob prior and the staff at mit press for their support during the writing of the book. we thank the gatsby computational neuroscience unit and neil lawrence at the department of computer science university of sheffield for hosting our visits and kindly providing space for us to work and the department of computer science at the university of toronto for computer support. thanks to john and fiona for their hospitality on numerous occasions. some of the diagrams in this book have been inspired by similar diagrams appearing in published work as follows figure sch olkopf and smola figure mackay cer gratefully acknowledges financial support from the german research foundation ckiw thanks the school of informatics university of edinburgh for granting him sabbatical leave for the period october finally we reserve our deepest appreciation for our wives agnes and barbara and children ezra kate miro and ruth for their patience and understanding while the book was being written. despite our best efforts it is inevitable that some errors will make it through to the printed version of the book. errata will be made available via the book s website at httpwww.gaussianprocess.orggpml we have found the joint writing of this book an excellent experience. although hard at times we are confident that the end result is much better than either one of us could have written alone. now ten years after their first introduction into the machine learning community gaussian processes are receiving growing attention. although gps have been known for a long time in the statistics and geostatistics fields and their use can perhaps be traced back as far as the end of the century their application to real problems is still in its early phases. this contrasts somewhat the application of the non-probabilistic analogue of the gp the support vector machine which was taken up more quickly by practitioners. perhaps this has to do with the probabilistic mind-set needed to understand gps which is not so generally appreciated. perhaps it is due to the need for computational short-cuts to implement inference for large datasets. or it could be due to the lack of a self-contained introduction to this exciting field with this volume we hope to contribute to the momentum gained by gaussian processes in machine learning. carl edward rasmussen and chris williams t ubingen and edinburgh summer second printing we thank baback moghaddam mikhail parakhin leif rasmussen benjamin sobotta kevin s. van horn and aki vehtari for reporting errors in the first printing which have now been corrected. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn symbols and notation matrices are capitalized and vectors are in bold type. we do not generally distinguish between probabilities and probability densities. a subscript asterisk such as in x indicates reference to a test set quantity. a superscript asterisk denotes complex conjugate. symbol c hf gih kfkh y or f or or c choleskya covf d d diagw diagw pq e or eqxzx fx or f f f gp hx or hx h or hx i or in j kx k or kx x k kx or k kf or k i i meaning left matrix divide ab is the vector x which solves ax b an equality which acts as a definition equality up to an additive constant determinant of k matrix euclidean length of vector y rkhs inner product rkhs norm the transpose of vector y proportional to e.g. pxy fx y means that pxy is equal to fx y times a factor which is independent of x distributed according to example x n partial derivatives f the matrix of second derivatives vector of all s length n vector of all s length n number of classes in a classification problem cholesky decomposition l is a lower triangular matrix such that ll a gaussian process posterior covariance dimension of input space x data set d yii n argument a diagonal matrix containing the elements of vector w argument a vector containing the diagonal elements of matrix w kronecker delta pq iff p q and otherwise expectation expectation of zx when x qx gaussian process vector of latent function values f fxn gaussian process prediction variable gaussian process posterior mean gaussian process with mean function mx and covariance function kx either fixed basis function set of basis functions or weight function set of basis functions evaluated at all training points the identity matrix size n bessel function of the first kind covariance kernel function evaluated at x and n n covariance gram matrix n n matrix kx x the covariance between training and test cases vector short for kx x when there is only a single test case covariance matrix for the free f values gaussian process f kx the function f is distributed as a c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn xviii symbol ky k la b logz or d mx n or n n n and n n nh n o o yx and pyx pn or r rlf or rlc rllx rc ss f n tra tl v or vqxzx x x x xi xdi z symbols and notation meaning covariance matrix for the y values for independent homoscedastic noise ky kf ni modified bessel function loss function the loss of predicting b when a is true note argument order natural logarithm e logarithm to the base characteristic length-scale input dimension d logistic function exp the mean function of a gaussian process a measure section variable x has a gaussian distribution with mean vector and covariance matrix short for unit gaussian x n i number of training test cases dimension of feature space number of hidden units in a neural network the natural numbers the positive integers big oh for functions f and g on n we write fn ogn if the ratio fngn remains bounded as n either matrix of all zeros or differential operator conditional random variable y given x and its probability the regular n-polygon feature map of input xi input set x cumulative unit gaussian z exp the sigmoid of the latent value if fx is stochastic map prediction evaluated at fx mean prediction expected value of note in general that the real numbers the risk or expected loss for f or classifier c w.r.t. inputs and outputs expected loss for predicting l averaged w.r.t. the model s pred. distr. at x decision region for class c power spectrum any sigmoid function e.g. logistic cumulative gaussian etc. variance of the free signal noise variance vector of hyperparameters of the covariance function trace of matrix a the circle with circumference l variance variance of zx when x qx input space and also the index set for the stochastic process d n matrix of the training inputs matrix of test inputs the ith training input the dth coordinate of the ith training input xi the integers the design matrix c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter introduction in this book we will be concerned with supervised learning which is the problem of learning input-output mappings from empirical data training dataset. depending on the characteristics of the output this problem is known as either regression for continuous outputs or classification when outputs are discrete. a well known example is the classification of images of handwritten digits. the training set consists of small digitized images together with a classification from normally provided by a human. the goal is to learn a mapping from image to classification label which can then be used on new unseen images. supervised learning is an attractive way to attempt to tackle this problem since it is not easy to specify accurately the characteristics of say the handwritten digit an example of a regression problem can be found in robotics where we wish to learn the inverse dynamics of a robot arm. here the task is to map from the state of the arm by the positions velocities and accelerations of the joints to the corresponding torques on the joints. such a model can then be used to compute the torques needed to move the arm along a given trajectory. another example would be in a chemical plant where we might wish to predict the yield as a function of process parameters such as temperature pressure amount of catalyst etc. in general we denote the input as x and the output target as y. the input is usually represented as a vector x as there are in general many input variables in the handwritten digit recognition example one may have a input obtained from a raster scan of a image and in the robot arm example there are three input measurements for each joint in the arm. the target y may either be continuous in the regression case or discrete in the classification case. we have a dataset d of n observations d yii n. given this training data we wish to make predictions for new inputs x that we have not seen in the training set. thus it is clear that the problem at hand is inductive we need to move from the finite training data d to a digit classification robotic control the dataset training is inductive c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn introduction two approaches gaussian process consistency tractability function f that makes predictions for all possible input values. to do this we must make assumptions about the characteristics of the underlying function as otherwise any function which is consistent with the training data would be equally valid. a wide variety of methods have been proposed to deal with the supervised learning problem here we describe two common approaches. the first is to restrict the class of functions that we consider for example by only considering linear functions of the input. the second approach is rather loosely to give a prior probability to every possible function where higher probabilities are given to functions that we consider to be more likely for example because they are smoother than other the first approach has an obvious problem in that we have to decide upon the richness of the class of functions considered if we are using a model based on a certain class of functions linear functions and the target function is not well modelled by this class then the predictions will be poor. one may be tempted to increase the flexibility of the class of functions but this runs into the danger of overfitting where we can obtain a good fit to the training data but perform badly when making test predictions. the second approach appears to have a serious problem in that surely there are an uncountably infinite set of possible functions and how are we going to compute with this set in finite time? this is where the gaussian process comes to our rescue. a gaussian process is a generalization of the gaussian probability distribution. whereas a probability distribution describes random variables which are scalars or vectors multivariate distributions a stochastic process governs the properties of functions. leaving mathematical sophistication aside one can loosely think of a function as a very long vector each entry in the vector specifying the function value fx at a particular input x. it turns out that although this idea is a little na ve it is surprisingly close what we need. indeed the question of how we deal computationally with these infinite dimensional objects has the most pleasant resolution imaginable if you ask only for the properties of the function at a finite number of points then inference in the gaussian process will give you the same answer if you ignore the infinitely many other points as if you would have taken them all into account! and these answers are consistent with answers to any other finite queries you may have. one of the main attractions of the gaussian process framework is precisely that it unites a sophisticated and consistent view with computational tractability. it should come as no surprise that these ideas have been around for some time although they are perhaps not as well known as they might be. indeed many models that are commonly employed in both machine learning and statistics are in fact special cases of or restricted kinds of gaussian processes. in this volume we aim to give a systematic and unified treatment of the area showing connections to related models. two approaches may be regarded as imposing a restriction bias and a preference bias respectively see e.g. mitchell c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn a pictorial introduction to bayesian modelling prior posterior figure panel shows four samples drawn from the prior distribution. panel shows the situation after two datapoints have been observed. the mean prediction is shown as the solid line and four samples from the posterior are shown as dashed lines. in both plots the shaded region denotes twice the standard deviation at each input value x. a pictorial introduction to bayesian mod elling in this section we give graphical illustrations of how the second method works on some simple regression and classification examples. we first consider a simple regression problem mapping from an input x to an output fx. in figure we show a number of sample functions drawn at random from the prior distribution over functions specified by a particular gaussian process which favours smooth functions. this prior is taken to represent our prior beliefs over the kinds of functions we expect to observe before seeing any data. in the absence of knowledge to the contrary we have assumed that the average value over the sample functions at each x is zero. although the specific random functions drawn in figure do not have a mean of zero the mean of fx values for any fixed x would become zero independent of x as we kept on drawing more functions. at any value of x we can also characterize the variability of the sample functions by computing the variance at that point. the shaded region denotes twice the pointwise standard deviation in this case we used a gaussian process which specifies that the prior variance does not depend on x. suppose that we are then given a dataset d consisting of two observations and we wish now to only consider functions that pass though these two data points exactly. is also possible to give higher preference to functions that merely pass close to the datapoints. this situation is illustrated in figure the dashed lines show sample functions which are consistent with d and the solid line depicts the mean value of such functions. notice how the uncertainty is reduced close to the observations. the combination of the prior and the data leads to the posterior distribution over functions. regression random functions mean function pointwise variance functions that agree with observations posterior over functions xfx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn non-parametric inference prior specification covariance function modelling and interpreting classification squashing function introduction if more datapoints were added one would see the mean function adjust itself to pass through these points and that the posterior uncertainty would reduce close to the observations. notice that since the gaussian process is not a parametric model we do not have to worry about whether it is possible for the model to fit the data would be the case if e.g. you tried a linear model on strongly non-linear data. even when a lot of observations have been added there may still be some flexibility left in the functions. one way to imagine the reduction of flexibility in the distribution of functions as the data arrives is to draw many random functions from the prior and reject the ones which do not agree with the observations. while this is a perfectly valid way to do inference it is impractical for most purposes the exact analytical computations required to quantify these properties will be detailed in the next chapter. the specification of the prior is important because it fixes the properties of the functions considered for inference. above we briefly touched on the mean and pointwise variance of the functions. however other characteristics can also be specified and manipulated. note that the functions in figure are smooth and stationary stationarity means that the functions look similar at all x locations. these are properties which are induced by the covariance function of the gaussian process many other covariance functions are possible. suppose that for a particular application we think that the functions in figure vary too rapidly that their characteristic length-scale is too short. slower variation is achieved by simply adjusting parameters of the covariance function. the problem of learning in gaussian processes is exactly the problem of finding suitable properties for the covariance function. note that this gives us a model of the data and characteristics a smoothness characteristic length-scale etc. which we can interpret. we now turn to the classification case and consider the binary twoclass classification problem. an example of this is classifying objects detected in astronomical sky surveys into stars or galaxies. our data has the label for stars and for galaxies and our task will be to predict the probability that an example with input vector x is a star using as inputs some features that describe each object. obviously should lie in the interval a gaussian process prior over functions does not restrict the output to lie in this interval as can be seen from figure the approach that we shall adopt is to squash the prior function f pointwise through a response function which restricts the output to lie in a common choice for this function is the logistic function exp z illustrated in figure thus the prior over f induces a prior over probabilistic classifications this set up is illustrated in figure for a input space. in panel we see a sample drawn from the prior over functions f which is squashed through the logistic function a dataset is shown in panel where the white and black circles denote classes and respectively. as in the regression case the effect of the data is to downweight in the posterior those functions that are incompatible with the data. a contour plot of the posterior mean for is shown in panel in this example we have chosen a short characteristic length-scale for the process so that it can vary fairly rapidly in c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn roadmap figure panel shows a sample from prior distribution on f in a input space. panel is a plot of the logistic function panel shows the location of the data points where the open circles denote the class label and closed circles denote the class label panel shows a contour plot of the mean predictive probability as a function of x the decision boundaries between the two classes are shown by the thicker lines. this case notice that all of the training points are correctly classified including the two outliers in the ne and sw corners. by choosing a different lengthscale we can change this behaviour as illustrated in section roadmap the book has a natural split into two parts with the chapters up to and including chapter covering core material and the remaining chapters covering the connections to other methods fast approximations and more specialized properties. some sections are marked by an asterisk. these sections may be omitted on a first reading and are not pre-requisites for later material. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression classification covariance functions learning connections theory fast approximations introduction chapter contains the definition of gaussian processes in particular for the use in regression. it also discusses the computations needed to make predictions for regression. under the assumption of gaussian observation noise the computations needed to make predictions are tractable and are dominated by the inversion of a n n matrix. in a short experimental section the gaussian process model is applied to a robotics task. chapter considers the classification problem for both binary and multiclass cases. the use of a non-linear response function means that exact computation of the predictions is no longer possible analytically. we discuss a number of approximation schemes include detailed algorithms for their implementation and discuss some experimental comparisons. as discussed above the key factor that controls the properties of a gaussian process is the covariance function. much of the work on machine learning so far has used a very limited set of covariance functions possibly limiting the power of the resulting models. in chapter we discuss a number of valid covariance functions and their properties and provide some guidelines on how to combine covariance functions into new ones tailored to specific needs. many covariance functions have adjustable parameters such as the characteristic length-scale and variance illustrated in figure chapter describes how such parameters can be inferred or learned from the data based on either bayesian methods the marginal likelihood or methods of crossvalidation. explicit algorithms are provided for some schemes and some simple practical examples are demonstrated. gaussian process predictors are an example of a class of methods known as kernel machines they are distinguished by the probabilistic viewpoint taken. in chapter we discuss other kernel machines such as support vector machines splines least-squares classifiers and relevance vector machines and their relationships to gaussian process prediction. in chapter we discuss a number of more theoretical issues relating to gaussian process methods including asymptotic analysis average-case learning curves and the pac-bayesian framework. one issue with gaussian process prediction methods is that their basic complexity is due to the inversion of a n n matrix. for large datasets this is prohibitive both time and space and so a number of approximation methods have been developed as described in chapter the main focus of the book is on the core supervised learning problems of regression and classification. in chapter we discuss some rather less standard settings that gps have been used in and complete the main part of the book with some conclusions. appendix a gives some mathematical background while appendix b deals specifically with gaussian markov processes. appendix c gives details of how to access the data and programs that were used to make the some of the figures and run the experiments described in the book. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter regression supervised learning can be divided into regression and classification problems. whereas the outputs for classification are discrete class labels regression is concerned with the prediction of continuous quantities. for example in a financial application one may attempt to predict the price of a commodity as a function of interest rates currency exchange rates availability and demand. in this chapter we describe gaussian process methods for regression problems classification problems are discussed in chapter there are several ways to interpret gaussian process regression models. one can think of a gaussian process as defining a distribution over functions and inference taking place directly in the space of functions the function-space view. although this view is appealing it may initially be difficult to grasp so we start our exposition in section with the equivalent weight-space view which may be more familiar and accessible to many and continue in section with the function-space view. gaussian processes often have characteristics that can be changed by setting certain parameters and in section we discuss how the properties change as these parameters are varied. the predictions from a gp model take the form of a full predictive distribution in section we discuss how to combine a loss function with the predictive distributions using decision theory to make point predictions in an optimal way. a practical comparative example involving the learning of the inverse dynamics of a robot arm is presented in section we give some theoretical analysis of gaussian process regression in section and discuss how to incorporate explicit basis functions into the models in section as much of the material in this chapter can be considered fairly standard we postpone most references to the historical overview in section weight-space view the simple linear regression model where the output is a linear combination of the inputs has been studied and used extensively. its main virtues are simplic two equivalent views c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression ity of implementation and interpretability. its main drawback is that it only allows a limited flexibility if the relationship between input and output cannot reasonably be approximated by a linear function the model will give poor predictions. in this section we first discuss the bayesian treatment of the linear model. we then make a simple enhancement to this class of models by projecting the inputs into a high-dimensional feature space and applying the linear model there. we show that in some feature spaces one can apply the kernel trick to carry out computations implicitly in the high dimensional space this last step leads to computational savings when the dimensionality of the feature space is large compared to the number of data points. we have a training set d of n observations d yi i n where x denotes an input vector of dimension d and y denotes a scalar output or target variable the column vector inputs for all n cases are aggregated in the d n design matrix x and the targets are collected in the vector y so we can write d y. in the regression setting the targets are real values. we are interested in making inferences about the relationship between inputs and targets i.e. the conditional distribution of the targets given the inputs we are not interested in modelling the input distribution itself. the standard linear model we will review the bayesian analysis of the standard linear regression model with gaussian noise fx xw y fx where x is the input vector w is a vector of weights of the linear model f is the function value and y is the observed target value. often a bias weight or offset is included but as this can be implemented by augmenting the input vector x with an additional element whose value is always one we do not explicitly include it in our notation. we have assumed that the observed values y differ from the function values fx by additive noise and we will further assume that this noise follows an independent identically distributed gaussian distribution with zero mean and variance n n n. training set design matrix bias offset likelihood this noise assumption together with the model directly gives rise to the likelihood the probability density of the observations given the parameters which is statistics texts the design matrix is usually taken to be the transpose of our definition but our choice is deliberate and has the advantage that a data point is a standard vector. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn weight-space view factored over cases in the training set of the independence assumption to give ny pyx w pyixi w ny n x n i n n ni where denotes the euclidean length of vector z. in the bayesian formalism we need to specify a prior over the parameters expressing our beliefs about the parameters before we look at the observations. we put a zero mean gaussian prior with covariance matrix p on the weights w n p. the r ole and properties of this prior will be discussed in section for now we will continue the derivation with the prior as specified. inference in the bayesian linear model is based on the posterior distribution over the weights computed by bayes rule eq. posterior likelihood prior marginal likelihood pwy x pyx wpw pyx where the normalizing constant also known as the marginal likelihood page is independent of the weights and given by pyx pyx wpw dw. z prior posterior marginal likelihood the posterior in eq. combines the likelihood and the prior and captures everything we know about the parameters. writing only the terms from the likelihood and prior which depend on the weights and completing the square we obtain pwx y xwy xx p w n p n where w posterior distribution as gaussian with mean w and covariance matrix a p and we recognize the form of the n xx n pwx y n w a a n n xx where a p notice that for this model indeed for any gaussian posterior the mean of the posterior distribution pwy x is also its mode which is also called the maximum a posteriori estimate of bayes rule is stated as pab pbapapb here we use it in a form where we additionally condition everywhere on the inputs x neglect this extra conditioning for the prior which is independent of the inputs. map estimate c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression figure example of bayesian linear model f with intercept and slope parameter panel shows the contours of the prior distribution pw n i eq. panel shows three training points marked by crosses. panel shows contours of the likelihood pyx w eq. assuming a noise level of n note that the slope is much more well determined than the intercept. panel shows the posterior pwx y eq. comparing the maximum of the posterior to the likelihood we see that the intercept has been shrunk towards zero whereas the more well determined slope is almost unchanged. all contour plots give the and standard deviation equi-probability contours. superimposed on the data in panel are the predictive mean plusminus two standard deviations of the predictive distribution pf x y eq. w. in a non-bayesian setting the negative log prior is sometimes thought of as a penalty term and the map point is known as the penalized maximum likelihood estimate of the weights and this may cause some confusion between the two approaches. note however that in the bayesian setting the map estimate plays no special r the penalized maximum likelihood procedure this case due to symmetries in the model and posterior it happens that the mean of the predictive distribution is the same as the prediction at the mean of the posterior. however this is not the case in general. intercept xoutput yintercept c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn ridge regression predictive distribution feature space polynomial regression linear in the parameters weight-space view is known in this case as ridge regression and kennard because of the effect of the quadratic penalty term p w from the log prior. w to make predictions for a test case we average over all possible parameter values weighted by their posterior probability. this is in contrast to nonbayesian schemes where a single parameter is typically chosen by some criterion. thus the predictive distribution for f fx at x is given by averaging the output of all possible linear models w.r.t. the gaussian posterior pf x y pf wpwx y dw z n a a x x the predictive distribution is again gaussian with a mean given by the posterior mean of the weights from eq. multiplied by the test input as one would expect from symmetry considerations. the predictive variance is a quadratic form of the test input with the posterior covariance matrix showing that the predictive uncertainties grow with the magnitude of the test input as one would expect for a linear model. an example of bayesian linear regression is given in figure here we have chosen a input space so that the weight-space is two-dimensional and can be easily visualized. contours of the gaussian prior are shown in panel the data are depicted as crosses in panel this gives rise to the likelihood shown in panel and the posterior distribution in panel the predictive distribution and its error bars are also marked in panel projections of inputs into feature space in the previous section we reviewed the bayesian linear model which suffers from limited expressiveness. a very simple idea to overcome this problem is to first project the inputs into some high dimensional space using a set of basis functions and then apply the linear model in this space instead of directly on the inputs themselves. for example a scalar input x could be projected into the space of powers of x x to implement polynomial regression. as long as the projections are fixed functions independent of the parameters w the model is still linear in the parameters and therefore analytically this idea is also used in classification where a dataset which is not linearly separable in the original data space may become linearly separable in a high dimensional feature space see section application of this idea begs the question of how to choose the basis functions? as we shall demonstrate chapter the gaussian process formalism allows us to answer this question. for now we assume that the basis functions are given. specifically we introduce the function which maps a d-dimensional input vector x into an n dimensional feature space. further let the matrix with adaptive basis functions such as e.g. multilayer perceptrons may at first seem like a useful extension but they are much harder to treat except in the limit of an infinite number of hidden units see section c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression explicit feature space formulation alternative formulation computational load kernel kernel trick be the aggregation of columns for all cases in the training set. now the model is where the vector of parameters now has length n. the analysis for this model is analogous to the standard linear model except that everywhere is substituted for x. thus the predictive distribution becomes fx f x y y n with and a p to make predictions using this equation we need to invert the a matrix of size n n which may not be convenient if n the dimension of the feature space is large. however we can rewrite the equation in the following way n f x y ni p p p ni p ni n p where we have used the shorthand and defined k p to show this for the mean first note that using the definitions of a and k we have n ni a p now multiplying through by a from left and n a ni showing the equivalence of the mean expressions in eq. p and eq. for the variance we use the matrix inversion lemma eq. setting z p w in eq. we need to invert matrices of size n n which is more convenient when n n. geometrically note that n datapoints can span at most n dimensions in the feature space. ni from the right gives ni and v u therein. p or notice that in eq. the feature space always enters in the form of p p thus the entries of these matrices are invariably of the form p where x and are in either the training or the test sets. let us define kx p for reasons that will become clear later we call k a covariance function or kernel. notice that p is an inner product respect to p. as p is positive definite we can define so that p for example if the svd value decomposition of p u du where d is diagonal then one form for is u then defining p we obtain a simple dot product representation kx p p p if an algorithm is defined solely in terms of inner products in input space then it can be lifted into feature space by replacing occurrences of those inner products by kx this is sometimes called the kernel trick. this technique is particularly valuable in situations where it is more convenient to compute the kernel than the feature vectors themselves. as we will see in the coming sections this often leads to considering the kernel as the object of primary interest and its corresponding feature space as having secondary practical importance. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn function-space view function-space view an alternative and equivalent way of reaching identical results to the previous section is possible by considering inference directly in function space. we use a gaussian process to describe a distribution over functions. formally definition a gaussian process is a collection of random variables any finite number of which have a joint gaussian distribution. gaussian process a gaussian process is completely specified by its mean function and covariance function. we define mean function mx and the covariance function kx of a real process fx as covariance and mean function mx efx kx efx and will write the gaussian process as fx kx usually for notational simplicity we will take the mean function to be zero although this need not be done see section in our case the random variables represent the value of the function fx at location x. often gaussian processes are defined over time i.e. where the index set of the random variables is time. this is not the case in our use of gps here the index set x is the set of possible inputs which could be more general e.g. rd. for notational convenience we use the enumeration of the cases in the training set to identify the random variables such that fi fxi is the random variable corresponding to the case yi as would be expected. a gaussian process is defined as a collection of random variables. thus the definition automatically implies a consistency requirement which is also sometimes known as the marginalization property. this property simply means that if the gp e.g. specifies n then it must also specify n where is the relevant submatrix of see eq. in other words examination of a larger set of variables does not change the distribution of the smaller set. notice that the consistency requirement is automatically fulfilled if the covariance function specifies entries of the covariance the definition does not exclude gaussian processes with finite index sets would be simply gaussian distributions but these are not particularly interesting for our purposes. however that if you instead specified e.g. a function for the entries of the inverse covariance matrix then the marginalization property would no longer be fulfilled and one could not think of this as a consistent collection of random variables this would not qualify as a gaussian process. index set input domain marginalization property finite index set c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bayesian linear model is a gaussian process basis functions smoothness characteristic length-scale regression a simple example of a gaussian process can be obtained from our bayesian linear regression model fx with prior w n p. we have for the mean and covariance efx p thus fx and are jointly gaussian with zero mean and covariance given by p indeed the function values fxn corresponding to any number of input points n are jointly gaussian although if n n then this gaussian is singular the joint covariance matrix will be of rank n. in this chapter our running example of a covariance function will be the squared exponential covariance function other covariance functions are discussed in chapter the covariance function specifies the covariance between pairs of random variables kxp xq note that the covariance between the outputs is written as a function of the inputs. for this particular covariance function we see that the covariance is almost unity between variables whose corresponding inputs are very close and decreases as their distance in the input space increases. it can be shown section that the squared exponential covariance function corresponds to a bayesian linear regression model with an infinite number of basis functions. indeed for every positive definite covariance function k there exists a infinite expansion in terms of basis functions mercer s theorem in section we can also obtain the se covariance function from the linear combination of an infinite number of gaussian-shaped basis functions see eq. and eq. the specification of the covariance function implies a distribution over functions. to see this we can draw samples from the distribution of functions evaluated at any number of points in detail we choose a number of input x and write out the corresponding covariance matrix using eq. elementwise. then we generate a random gaussian vector with this covariance matrix f kx x and plot the generated values as a function of the inputs. figure shows three such samples. the generation of multivariate gaussian samples is described in section in the example in figure the input values were equidistant but this need not be the case. notice that informally the functions look smooth. in fact the squared exponential covariance function is infinitely differentiable leading to the process being infinitely mean-square differentiable section we also see that the functions seem to have a characteristic length-scale this covariance function is called the radial basis function or gaussian here we prefer squared exponential. these input points play the r ole of test inputs and therefore carry a subscript asterisk this will become clearer later when both training and test points are involved. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn function-space view prior posterior figure panel shows three functions drawn at random from a gp prior the dots indicate values of y actually generated the two other functions have correctly been drawn as lines by joining a large number of evaluated points. panel shows three random functions drawn from the posterior i.e. the prior conditioned on the five noise free observations indicated. in both plots the shaded area represents the pointwise mean plus and minus two times the standard deviation for each input value to the confidence region for the prior and posterior respectively. which informally can be thought of as roughly the distance you have to move in input space before the function value can change significantly see section for eq. the characteristic length-scale is around one unit. by replacing xq by xq in eq. for some positive constant we could change the characteristic length-scale of the process. also the overall variance of the random function can be controlled by a positive pre-factor before the exp in eq. we will discuss more about how such factors affect the predictions in section and say more about how to set such scale parameters in chapter prediction with noise-free observations we are usually not primarily interested in drawing random functions from the prior but want to incorporate the knowledge that the training data provides about the function. initially we will consider the simple special case where the observations are noise free that is we know fii n. the joint distribution of the training outputs f and the test outputs f according to the prior is kx x kx x f kx x kx x f n magnitude joint prior if there are n training points and n test points then kx x denotes the n n matrix of the covariances evaluated at all pairs of training and test points and similarly for the other entries kx x kx x and kx x. to get the posterior distribution over functions we need to restrict this joint prior distribution to contain only those functions which agree with the observed data points. graphically in figure you may think of generating functions from the prior and rejecting the ones that disagree with the observations al graphical rejection xoutput fx xoutput fx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression noise-free predictive distribution predictive distribution though this strategy would not be computationally very efficient. fortunately in probabilistic terms this operation is extremely simple corresponding to conditioning the joint gaussian prior distribution on the observations section for further details to give f x f xkx x kx x kx xkx x x function values f to test inputs x can be sampled from the joint posterior distribution by evaluating the mean and covariance matrix from eq. and generating samples according to the method described in section figure shows the results of these computations given the five datapoints marked with symbols. notice that it is trivial to extend these computations to multidimensional inputs one simply needs to change the evaluation of the covariance function in accordance with eq. although the resulting functions may be harder to display graphically. prediction using noisy observations it is typical for more realistic modelling situations that we do not have access to function values themselves but only noisy versions thereof y fx assuming additive independent identically distributed gaussian noise with variance n the prior on the noisy observations becomes covyp yq kxp xq n pq or covy kx x ni where pq is a kronecker delta which is one iff p q and zero otherwise. it follows from the assumption about the noise that a diagonal is added in comparison to the noise free case eq. introducing the noise term in eq. we can write the joint distribution of the observed target values and the function values at the test locations under the prior as y f kx x kx x n ni kx x kx x deriving the conditional distribution corresponding to eq. we arrive at the key predictive equations for gaussian process regression f y x f covf where f ef y x kx xkx x covf kx x kx xkx x ni kx x are some situations where it is reasonable to assume that the observations are noise-free for example for computer simulations see e.g. sacks et al. complicated noise models with non-trivial covariance structure can also be handled see section that the kronecker delta is on the index of the cases not the value of the input for the signal part of the covariance function the input value is the index set to the random variables describing the function for the noise part it is the identity of the point. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn function-space view f y observations gaussian field yc fc correspondence with weight-space view compact notation predictive distribution linear predictor inputs x xc figure graphical model graph for a gp for regression. squares represent observed variables and circles represent unknowns. the thick horizontal bar represents a set of fully connected nodes. note that an observation yi is conditionally independent of all other nodes given the corresponding latent variable fi. because of the marginalization property of gps addition of further inputs x latent variables f and unobserved targets y does not change the distribution of any other variables. notice that we now have exact correspondence with the weight space view in eq. when identifying kc d p where c d stand for either x or x for any set of basis functions we can compute the corresponding covariance function as kxp xq p conversely for every definite covariance function k there exists a infinite expansion in terms of basis functions see section the expressions involving kx x kx x and kx x etc. can look rather unwieldy so we now introduce a compact form of the notation setting k kx x and k kx x in the case that there is only one test point x we write kx k to denote the vector of covariances between the test point and the n training points. using this compact notation and for a single test point x equations and reduce to f k ni vf kx x k ni let us examine the predictive distribution as given by equations and note first that the mean prediction eq. is a linear combination of observations y this is sometimes referred to as a linear predictor. another way to look at this equation is to see it as a linear combination of n kernel functions each one centered on a training point by writing nx fx ikxi x ni the fact that the mean prediction for fx can be where written as eq. despite the fact that the gp can be represented in terms of a infinite number of basis functions is one manifestation of the representer theorem see section for more on this point. we can understand this result intuitively because although the gp defines a joint gaussian distribution over all of the y variables one for each point in the index set x for representer theorem c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression posterior posterior covariance figure panel is identical to figure showing three random functions drawn from the posterior. panel shows the posterior co-variance between f and f for the same data for three different values of note that the covariance at close points is high falling to zero at the training points there is no variance since it is a noise-free process then becomes negative etc. this happens because if the smooth function happens to be less than the mean on one side of the data point it tends to exceed the mean on the other side causing a reversal of the sign of the covariance at the data points. note for contrast that the prior covariance is simply of gaussian shape and never negative. making predictions at x we only care about the distribution defined by the n training points and the test point. as a gaussian distribution is marginalized by just taking the relevant block of the joint covariance matrix section it is clear that conditioning this distribution on the observations gives us the desired result. a graphical model representation of a gp is given in figure note also that the variance in eq. does not depend on the observed targets but only on the inputs this is a property of the gaussian distribution. the variance is the difference between two terms the first term kx x is simply the prior covariance from that is subtracted a term representing the information the observations gives us about the function. we can very simply compute the predictive distribution of test targets y by adding ni to the variance in the expression for covf the predictive distribution for the gp model gives more than just pointwise errorbars of the simplified eq. although not stated explicitly eq. holds unchanged when x denotes multiple test inputs in this case the covariance of the test targets are computed diagonal elements are the pointwise variances. in fact eq. is the mean function and eq. the covariance function of the posterior process recall the definition of gaussian process from page the posterior covariance in illustrated in figure it will be useful for chapter to introduce the marginal likelihood evidence pyx at this point. the marginal likelihood is the integral noisy predictions joint predictions posterior process marginal likelihood xoutput fx xpost. covariance covfxfx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn varying the hyperparameters o input x y k function n level x input ni o predictive mean eq. y p l choleskyk lly f k v lk predictive variance eq. vf kx x vv log pyx eq. return f vf log pyx marginal likelihood algorithm predictions and log marginal likelihood for gaussian process regression. the implementation addresses the matrix inversion required by eq. and using cholesky factorization see section for multiple test cases lines are repeated. the log determinant required in eq. is computed from the cholesky factor large n it may not be possible to represent the determinant itself. the computational complexity is for the cholesky decomposition in line and for solving triangular systems in line and each test case in line i log lii n log of the likelihood times the prior pyx z pyf xpfx df the term marginal likelihood refers to the marginalization over the function values f. under the gaussian process model the prior is gaussian fx n k or log pfx log n log and the likelihood is a factorized gaussian yf n ni so we can make use of equations and to perform the integration yielding the log marginal likelihood fk log pyx yk ni log ni n log this result can also be obtained directly by observing that y n k ni. a practical implementation of gaussian process regression is shown in algorithm the algorithm uses cholesky decomposition instead of directly inverting the matrix since it is faster and numerically more stable see section the algorithm returns the predictive mean and variance for noise free test data to compute the predictive distribution for noisy test data y simply add the noise variance n to the predictive variance of f varying the hyperparameters typically the covariance functions that we use will have some free parameters. for example the squared-exponential covariance function in one dimension has the following form f n pq. kyxp xq c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression hyperparameters figure data is generated from a gp with hyperparameters f n as shown by the symbols. using gaussian process prediction with these hyperparameters we obtain a confidence region for the underlying function f in grey. panels and again show the confidence region but this time for hyperparameter values and respectively. the covariance is denoted ky as it is for the noisy targets y rather than for the underlying function f. observe that the length-scale the signal variance f and the noise variance n can be varied. in general we call the free parameters in chapter we will consider various methods for determining the hyperparameters from training data. however in this section our aim is more simply to explore the effects of varying the hyperparameters on gp prediction. consider the data shown by signs in figure this was generated from a gp with the se kernel with f n the figure also shows the standard-deviation error bars for the predictions obtained using these values of the hyperparameters as per eq. notice how the error bars get larger for input values that are distant from any training points. indeed if the x-axis refer to the parameters of the covariance function as hyperparameters to emphasize that they are parameters of a non-parametric model in accordance with the weight-space view section the parameters of the underlying parametric model have been integrated out. xoutput y xoutput y xoutput y c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn decision theory for regression were extended one would see the error bars reflect the prior standard deviation of the process f away from the data. if we set the length-scale shorter so that and kept the other parameters the same then generating from this process we would expect to see plots like those in figure except that the x-axis should be rescaled by a factor of equivalently if the same x-axis was kept as in figure then a sample function would look much more wiggly. if we make predictions with a process with on the data generated from the process then we obtain the result in figure the remaining two parameters were set by optimizing the marginal likelihood as explained in chapter in this case the noise parameter is reduced to n as the greater flexibility of the signal means that the noise level can be reduced. this can be observed at the two datapoints near x in the plots. in figure these are essentially explained as a similar function value with differing noise. however in figure the noise level is very low so these two points have to be explained by a sharp variation in the value of the underlying function f. notice also that the short length-scale means that the error bars in figure grow rapidly away from the datapoints. in contrast we can set the length-scale longer for example to as shown in figure again the remaining two parameters were set by optimizing the marginal likelihood. in this case the noise level has been increased to n and we see that the data is now explained by a slowly varying function with a lot of noise. of course we can take the position of a quickly-varying signal with low noise or a slowly-varying signal with high noise to extremes the former would give rise to a white-noise process model for the signal while the latter would give rise to a constant signal with added white noise. under both these models the datapoints produced should look like white noise. however studying figure we see that white noise is not a convincing model of the data as the sequence of y s does not alternate sufficiently quickly but has correlations due to the variability of the underlying function. of course this is relatively easy to see in one dimension but methods such as the marginal likelihood discussed in chapter generalize to higher dimensions and allow us to score the various models. in this case the marginal likelihood gives a clear preference for f n over the other two alternatives. decision theory for regression in the previous sections we have shown how to compute predictive distributions for the outputs y corresponding to the novel test input x the predictive distribution is gaussian with mean and variance given by eq. and eq. in practical applications however we are often forced to make a decision about how to act i.e. we need a point-like prediction which is optimal in some sense. to this end we need a loss function lytrue yguess which specifies the loss too short length-scale too long length-scale model comparison optimal predictions loss function c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression non-bayesian paradigm bayesian paradigm penalty incurred by guessing the value yguess when the true value is ytrue. for example the loss function could equal the absolute deviation between the guess and the truth. notice that we computed the predictive distribution without reference to the loss function. in non-bayesian paradigms the model is typically trained by minimizing the empirical risk loss. in contrast in the bayesian setting there is a clear separation between the likelihood function for training in addition to the prior and the loss function. the likelihood function describes how the noisy measurements are assumed to deviate from the underlying noisefree function. the loss function on the other hand captures the consequences of making a specific choice given an actual true state. the likelihood and loss function need not have anything in expected loss risk absolute error loss squared error loss robot arm our goal is to make the point prediction yguess which incurs the smallest loss but how can we achieve that when we don t know ytrue? instead we minimize the expected loss or risk by averaging w.r.t. our model s opinion as to what the truth might be z rlyguessx ly yguesspy dy thus our best guess in the sense that it minimizes the expected loss is yoptimalx argmin yguess rlyguessx in general the value of yguess that minimizes the risk for the loss function y is the median of py while for the squared loss y it is the mean of this distribution. when the predictive distribution is gaussian the mean and the median coincide and indeed for any symmetric loss function and symmetric predictive distribution we always get yguess as the mean of the predictive distribution. however in many practical problems the loss functions can be asymmetric e.g. in safety critical applications and point predictions may be computed directly from eq. and eq. a comprehensive treatment of decision theory can be found in berger an example application in this section we use gaussian process regression to learn the inverse dynamics of a seven degrees-of-freedom sarcos anthropomorphic robot arm. the task is to map from a input space joint positions joint velocities joint accelerations to the corresponding joint torques. this task has previously been used to study regression algorithms by vijayakumar and schaal vijayakumar et al. and vijayakumar et al. following of fallacious arguments like a gaussian likelihood implies a squared error loss function. thank sethu vijayakumar for providing us with the data. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn an example application this previous work we present results below on just one of the seven mappings from the input variables to the first of the seven torques. one might ask why it is necessary to learn this mapping indeed there exist physics-based rigid-body-dynamics models which allow us to obtain the torques from the position velocity and acceleration variables. however the real robot arm is actuated hydraulically and is rather lightweight and compliant so the assumptions of the rigid-body-dynamics model are violated we see below. it is worth noting that the rigid-body-dynamics model is nonlinear involving trigonometric functions and squares of the input variables. an inverse dynamics model can be used in the following manner a planning module decides on a trajectory that takes the robot from its start to goal states and this specifies the desired positions velocities and accelerations at each time. the inverse dynamics model is used to compute the torques needed to achieve this trajectory and errors are corrected using a feedback controller. the dataset consists of input-output pairs of which were used as a training set and the remaining were used as a test set. the inputs were linearly rescaled to have zero mean and unit variance on the training set. the outputs were centered so as to have zero mean on the training set. given a prediction method we can evaluate the quality of predictions in several ways. perhaps the simplest is the squared error loss where we compute the squared residual fx between the mean prediction and the target at each test point. this can be summarized by the mean squared error by averaging over the test set. however this quantity is sensitive to the overall scale of the target values so it makes sense to normalize by the variance of the targets of the test cases to obtain the standardized mean squared error this causes the trivial method of guessing the mean of the training targets to have a smse of approximately additionally if we produce a predictive distribution at each test input we can evaluate the negative log probability of the target under the as gpr produces a gaussian predictive density one obtains log py x fx where the predictive variance for gpr is computed as vf n where vf is given by eq. we must include the noise variance n as we are predicting the noisy target y this loss can be standardized by subtracting the loss that would be obtained under the trivial model which predicts using a gaussian with the mean and variance of the training data. we denote this the standardized log loss the mean sll is denoted msll. thus the msll will be approximately zero for simple methods and negative for better methods. a number of models were tested on the data. a linear regression model provides a simple baseline for the smse. by estimating the noise level from the it makes sense to use the negative log probability so as to obtain a loss not a utility. why learning? mse smse msll c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression method lr rbd lwpr gpr smse msll table test results on the inverse dynamics problem for a number of different methods. the denotes a missing entry caused by two methods not producing full predictive distributions so msll could not be evaluated. residuals on the training set one can also obtain a predictive variance and thus get a msll value for lr. the rigid-body-dynamics model has a number of free parameters these were estimated by vijayakumar et al. using a least-squares fitting procedure. we also give results for the locally weighted projection regression method of vijayakumar et al. which is an on-line method that cycles through the dataset multiple times. for the gp models it is computationally expensive to make use of all training cases due to the scaling of the basic algorithm. in chapter we present several different approximate gp methods for large datasets. the result given in table was obtained with the subset of regressors approximation with a subset size of this result is taken from table which gives full results of the various approximation methods applied to the inverse dynamics problem. the squared exponential covariance function was used with a separate length-scale parameter for each of the input dimensions plus the signal and noise variance n. these parameters were set by optimizing the marginal parameters likelihood eq. on a subset of the data also chapter f and the results for the various methods are presented in table notice that the problem is quite non-linear so the linear regression model does poorly in comparison to non-linear the non-linear method lwpr improves over linear regression but is outperformed by gpr. smoothing weight functions and equiva lent kernels gaussian process regression aims to reconstruct the underlying signal f by removing the contaminating noise to do this it computes a weighted average of the noisy observations y as fx kx ni as fx is a linear combination of the y values gaussian process regression is a linear smoother hastie and tibshirani sec. for further details. in this section we study smoothing first in terms of a matrix analysis of the predictions at the training points and then in terms of the equivalent kernel. is perhaps surprising that rbd does worse than linear regression. however stefan schaal comm. states that the rbd parameters were optimized on a very large dataset of which the training data used here is subset and if the rbd model were optimized w.r.t. this training set one might well expect it to outperform linear regression. linear smoother c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn smoothing weight functions and equivalent kernels the predicted mean values f at the training points are given by ni f kk let k have the eigendecomposition k pn eigenvectors are mutually orthogonal. let y i where i is the ith eigenvalue and ui is the corresponding eigenvector. as k is real and symmetric positive semidefinite its eigenvalues are real and non-negative and its iui for some coefficients i u iuiu i y. then f i i i n ui. nx n then the component in y along ui is effectively notice that if i i eliminated. for most covariance functions that are used in practice the eigenvalues are larger for more slowly varying eigenvectors fewer zero-crossings so that this means that high-frequency components in y are smoothed out. the effective number of parameters or degrees of freedom of the smoother is n see hastie and tibshirani defined as trkk sec. notice that this counts the number of eigenvectors which are not eliminated. ni i i we can define a vector of functions hx ni thus we have fx hx making it clear that the mean prediction at a point x is a linear combination of the target values y. for a fixed test point x hx gives the vector of weights applied to targets y. hx is called the weight function as gaussian process regression is a linear smoother the weight function does not depend on y. note the difference between a linear model where the prediction is a linear combination of the inputs and a linear smoother where the prediction is a linear combination of the training set targets. understanding the form of the weight function is made complicated by the matrix inversion of k ni and the fact that k depends on the specific locations of the n datapoints. idealizing the situation one can consider the observations to be smeared out in x-space at some density of observations. in this case analytic tools can be brought to bear on the problem as shown in section by analogy to kernel smoothing silverman called the idealized weight function the equivalent kernel see also girosi et al. sec. a kernel smoother centres a kernel on x and then computes i x for each data point yi where is a length-scale. the gaussian is a commonly used kernel function. the prediction for fx is j. this is also known where wi ipn computed as fx as the nadaraya-watson estimator see e.g. scott sec. the weight function and equivalent kernel for a gaussian process are illustrated in figure for a one-dimensional input variable x. we have used the squared exponential covariance function and have set the length-scale that there are n training points spaced randomly along that this kernel function does not need to be a valid covariance function. eigendecomposition degrees of freedom weight function equivalent kernel kernel smoother c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regression figure panels show the weight function hx corresponding to the n training points the equivalent kernel and the original squared exponential kernel panel shows the equivalent kernels for two different data densities. see text for further details. the small cross at the test point is to scale in all four plots. the x-axis. figures and show the weight function and equivalent kernel for x and x respectively for n figure is also for x but uses n in each case the dots correspond to the weight function hx and the solid line is the equivalent kernel whose construction is explained below. the dashed line shows a squared exponential kernel centered on the test point scaled to have the same height as the maximum value in the equivalent kernel. figure shows the variation in the equivalent kernel as a function of n the number of datapoints in the unit interval. many interesting observations can be made from these plots. observe that the equivalent kernel has general a shape quite different to the original se kernel. in figure the equivalent kernel is clearly oscillatory negative sidelobes and has a higher spatial frequency than the original kernel. figure shows similar behaviour although due to edge effects the equivalent kernel is truncated relative to that in figure in figure we see that at higher noise levels the negative sidelobes are reduced and the width of the equivalent kernel is similar to the original kernel. also note that the overall height of the equivalent kernel in is reduced compared to that in and c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn incorporating explicit basis functions it averages over a wider area. the more oscillatory equivalent kernel for lower noise levels can be understood in terms of the eigenanalysis above at higher noise levels only the large varying components of y remain while for smaller noise levels the more oscillatory components are also retained. in figure we have plotted the equivalent kernel for n and n datapoints in notice how the width of the equivalent kernel decreases as n increases. we discuss this behaviour further in section the plots of equivalent kernels in figure were made by using a dense grid of ngrid points on and then computing the smoother matrix kk gridi each row of this matrix is the equivalent kernel at the appropriate location. however in order to get the scaling right one has to set grid nngridn for ngrid n this means that the effective variance at each of the ngrid points is larger but as there are correspondingly more points this effect cancels out. this can be understood by imagining the situation if there were ngridn independent gaussian observations with variance grid at a single xposition this would be equivalent to one gaussian observation with variance n. in effect the n observations have been smoothed out uniformly along the interval. the form of the equivalent kernel can be obtained analytically if we go to the continuum limit and look to smooth a noisy function. the relevant theory and some example equivalent kernels are given in section incorporating explicit basis functions it is common but by no means necessary to consider gps with a zero mean function. note that this is not necessarily a drastic limitation since the mean of the posterior process is not confined to be zero. yet there are several reasons why one might wish to explicitly model a mean function including interpretability of the model convenience of expressing prior information and a number of analytical limits which we will need in subsequent chapters. the use of explicit basis functions is a way to specify a non-zero mean over functions but as we will see in this section one can also use them to achieve other interesting effects. using a fixed mean function mx is trivial simply apply the usual zero mean gp to the difference between the observations and the fixed mean function. with fx kx the predictive mean becomes f mx kx xk y mx where ky k eq. ni and the predictive variance remains unchanged from however in practice it can often be difficult to specify a fixed mean function. in many cases it may be more convenient to specify a few fixed basis functions fixed mean function c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn stochastic mean function polynomial regression whose coefficients are to be inferred from the data. consider gx fx hx where fx kx regression here fx is a zero mean gp hx are a set of fixed basis functions and are additional parameters. this formulation expresses that the data is close to a global linear model with the residuals being modelled by a gp. this idea was explored explicitly as early as by blight and ott who used the gp to model the residuals from a polynomial regression i.e. hx x when fitting the model one could optimize over the parameters jointly with the hyperparameters of the covariance function. alternatively if we take the prior on to be gaussian n b we can also integrate out these parameters. following o hagan we obtain another gp gx kx now with an added contribution in the covariance function caused by the uncertainty in the parameters of the mean. predictions are made by plugging the mean and covariance functions of gx into eq. and eq. after rearranging we obtain gx h covg covf rb hk y h fx r k k y h y h where the h matrix collects the hx vectors for all training h all test cases hk y k notice the nice interpretation of the mean expression eq. top line is the mean of the global linear model parameters being a compromise between the data term and prior and the predictive mean is simply the mean linear output plus what the gp model predicts from the residuals. the covariance is the sum of the usual covariance term and a new non-negative contribution. y y b and r h hk exploring the limit of the above expressions as the prior on the parameter becomes vague b o o is the matrix of zeros we obtain a predictive distribution which is independent of b gx fx r covg covf rhk y h y h where the limiting y y. notice that predictions under the limit b o should not be implemented na vely by plugging the modified covariance function from eq. into the standard prediction equations since the entries of the covariance function tend to infinity thus making it unsuitable for numerical implementation. instead eq. must be used. even if the non-limiting case is of interest eq. is numerically preferable to a direct implementation based on eq. since the global linear part will often add some very large eigenvalues to the covariance matrix affecting its condition number. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn history and related work marginal likelihood in this short section we briefly discuss the marginal likelihood for the model with a gaussian prior n b on the explicit parameters from eq. as this will be useful later particularly in section we can express the marginal likelihood from eq. as log pyx b b yky hbh y log hbh n log where we have included the explicit mean. we are interested in exploring the limit where b o i.e. when the prior is vague. in this limit the mean of the prior is irrelevant was the case in eq. so without loss of generality the limiting case we assume for now that the mean is zero b giving log pyx b y h and c k yk y y log where a b hk the matrix inversion lemma eq. and eq. ycy log y ha y and we have used log n log we now explore the behaviour of the log marginal likelihood in the limit of vague priors on in this limit the variances of the gaussian in the directions spanned by columns of h will become infinite and it is clear that this will require special treatment. the log marginal likelihood consists of three terms a quadratic form in y a log determinant term and a term involving log performing an eigendecomposition of the covariance matrix we see that the contributions to quadratic form term from the infinite-variance directions will be zero. however the log determinant term will tend to minus infinity. the standard solution ansley and kohn in this case is to project y onto the directions orthogonal to the span of h and compute the marginal likelihood in this subspace. let the rank of h be m. then as shown in ansley and kohn this means that we must discard the terms log m log pyx where a hk log from eq. to give ycy y ha y y h and c k log n m log yk y y log history and related work prediction with gaussian processes is certainly not a very recent topic especially for time series analysis the basic theory goes back at least as far as the work of wiener and kolmogorov in the s. indeed lauritzen discusses relevant work by the danish astronomer t. n. thiele dating from time series c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn geostatistics kriging computer experiments machine learning regression gaussian process prediction is also well known in the geostatistics field e.g. matheron journel and huijbregts where it is known as and in meteorology daley although this literature naturally has focussed mostly on two- and three-dimensional input spaces. whittle sec. also suggests the use of such methods for spatial prediction. ripley and cressie provide useful overviews of gaussian process prediction in spatial statistics. gradually it was realized that gaussian process prediction could be used in a general regression context. for example o hagan presents the general theory as given in our equations and and applies it to a number of one-dimensional regression problems. sacks et al. describe gpr in the context of computer experiments the observations y are noise free and discuss a number of interesting directions such as the optimization of parameters in the covariance function our chapter and experimental design the choice of x-points that provide most information on f. the authors describe a number of computer simulations that were modelled including an example where the response variable was the clock asynchronization in a circuit and the inputs were six transistor widths. santner et al. is a recent book on the use of gps for the design and analysis of computer experiments. williams and rasmussen described gaussian process regression in a machine learning context and described optimization of the parameters in the covariance function see also rasmussen they were inspired to use gaussian process by the connection to infinite neural networks as described in section and in neal the kernelization of linear ridge regression described above is also known as kernel ridge regression see e.g. saunders et al. relationships between gaussian process prediction and regularization theory splines support vector machines and relevance vector machines are discussed in chapter exercises replicate the generation of random functions from figure use a regular random grid of scalar inputs and the covariance function from eq. hints on how to generate random samples from multi-variate gaussian distributions are given in section invent some training data points and make random draws from the resulting gp posterior using eq. in eq. we saw that the predictive variance at x under the feature space regression model was varfx show that covfx check that this is compatible with the expression given in eq. named the method after the south african mining engineer d. g. krige. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn exercises the wiener process is defined for x and has section for further details. it has mean zero and a non-stationary covariance function kx minx if we condition on the wiener process passing through we obtain a process known as the brownian bridge tied-down wiener process. show that this process has covariance kx minx for x and mean write a computer program to draw samples from this process at a finite grid of x points in let varnfx be the predictive variance of a gaussian process regression model at x given a dataset of size n. the corresponding predictive variance using a dataset of only the first n training points is denoted varn show that varnfx varn i.e. that the predictive variance at x cannot increase as more training data is obtained. one way to approach this problem is to use the partitioned matrix equations given in section to decompose varnfx kx x k ni an alternative information theoretic argument is given in williams and vivarelli note that while this conclusion is true for gaussian process priors and gaussian noise models it does not hold generally see barber and saad c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter classification in chapter we have considered regression problems where the targets are real valued. another important class of problems is classification problems where we wish to assign an input pattern x to one of c classes practical examples of classification problems are handwritten digit recognition we wish to classify a digitized image of a handwritten digit into one of ten classes and the classification of objects detected in astronomical sky surveys into stars or galaxies. on the distribution of galaxies in the universe is important for theories of the early universe. these examples nicely illustrate that classification problems can either be binary two-class c or multi-class we will focus attention on probabilistic classification where test predictions take the form of class probabilities this contrasts with methods which provide only a guess at the class label and this distinction is analogous to the difference between predictive distributions and point predictions in the regression setting. since generalization to test cases inherently involves some level of uncertainty it seems natural to attempt to make predictions in a way that reflects these uncertainties. in a practical application one may well seek a class guess which can be obtained as the solution to a decision problem involving the predictive probabilities as well as a specification of the consequences of making specific predictions loss function. both classification and regression can be viewed as function approximation problems. unfortunately the solution of classification problems using gaussian processes is rather more demanding than for the regression problems considered in chapter this is because we assumed in the previous chapter that the likelihood function was gaussian a gaussian process prior combined with a gaussian likelihood gives rise to a posterior gaussian process over functions and everything remains analytically tractable. for classification models where the targets are discrete class labels the gaussian likelihood is the statistics literature classification is often called discrimination. may choose to ignore the discreteness of the target values and use a regression treatment where all targets happen to be say for binary classification. this is known as binary multi-class probabilistic classification non-gaussian likelihood c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification in this chapter we treat methods of approximate inference for classification where exact inference is not section provides a general discussion of classification problems and describes the generative and discriminative approaches to these problems. in section we saw how gaussian process regression can be obtained in section we describe an analogue of by generalizing linear regression. linear regression in the classification case logistic regression. in section logistic regression is generalized to yield gaussian process classification using again the ideas behind the generalization of linear regression to gpr. for gpr the combination of a gp prior with a gaussian likelihood gives rise to a posterior which is again a gaussian process. in the classification case the likelihood is non-gaussian but the posterior process can be approximated by a gp. the laplace approximation for gpc is described in section binary classification and in section multi-class classification and the expectation propagation algorithm binary classification is described in section both of these methods make use of a gaussian approximation to the posterior. experimental results for gpc are given in section and a discussion of these results is provided in section classification problems generative approach discriminative approach generative model example the natural starting point for discussing approaches to classification is the joint probability py x where y denotes the class label. using bayes theorem this joint probability can be decomposed either as pypxy or as pxpyx. this gives rise to two different approaches to classification problems. the first which we call the generative approach models the class-conditional distributions pxy for y and also the prior probabilities of each class and then computes the posterior probability for each class using pyx pc pypxy pccpxcc the alternative approach which we call the discriminative approach focusses on modelling pyx directly. dawid calls the generative and discriminative approaches the sampling and diagnostic paradigms respectively. to turn both the generative and discriminative approaches into practical methods we will need to create models for either pxy or pyx these could either be of parametric form or non-parametric models such as those based on nearest neighbours. for the generative case a simple com least-squares classification see section that the important distinction is between gaussian and non-gaussian likelihoods regression with a non-gaussian likelihood requires a similar treatment but since classification defines an important conceptual and application area we have chosen to treat it in a separate chapter for non-gaussian likelihoods in general see section the generative approach inference for py is generally straightforward being estimation of a binomial probability in the binary case or a multinomial probability in the multi-class case. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification problems discriminative model example response function probit regression generative or discriminative? missing values mon choice would be to model the class-conditional densities with gaussians pxcc n c c. a bayesian treatment can be obtained by placing appropriate priors on the mean and covariance of each of the gaussians. however note that this gaussian model makes a strong assumption on the form of classconditional density and if this is inappropriate the model may perform poorly. for the binary discriminative case one simple idea is to turn the output of a regression model into a class probability using a response function inverse of a link function which squashes its argument which can lie in the domain into the range guaranteeing a valid probabilistic interpretation. one example is the linear logistic regression model where exp z standard normal distribution z which combines the linear model with the logistic response function. another common choice of response function is the cumulative density function of a n this approach is known as probit regression. just as we gave a bayesian approach to linear regression in chapter we can take a parallel approach to logistic regression as discussed in section as in the regression case this model is an important step towards the gaussian process classifier. given that there are the generative and discriminative approaches which one should we prefer? this is perhaps the biggest question in classification and we do not believe that there is a right answer as both ways of writing the joint py x are correct. however it is possible to identify some strengths and weaknesses of the two approaches. the discriminative approach is appealing in that it is directly modelling what we want pyx. also density estimation for the class-conditional distributions is a hard problem particularly when x is high dimensional so if we are just interested in classification then the generative approach may mean that we are trying to solve a harder problem than we need to. however to deal with missing input values outliers and unlabelled data points in a principled fashion it is very helpful to have access to px and this can be obtained from marginalizing out the class label y from the joint y pypxy in the generative approach. a further factor in the choice of a generative or discriminative approach could also be which one is most conducive to the incorporation of any prior information which is available. see ripley sec. for further discussion of these issues. the gaussian process classifiers developed in this chapter are discriminative. as px p decision theory for classification the classifiers described above provide predictive probabilities py for a test input x however sometimes one actually needs to make a decision and to do this we need to consider decision theory. decision theory for the regression problem was considered in section here we discuss decision theory for classification problems. a comprehensive treatment of decision theory can be found in berger c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn loss risk zero-one loss asymmetric loss bayes classifier decision regions reject option risk minimization classification given x is let lc be the loss incurred by making decision if the true class is cc. usually lc c for all c. the expected risk of taking decision c lc and the optimal decision c is the one that minimizes one common choice of loss function is the zero-one loss where a penalty of one unit is paid for an incorrect classification and for a correct one. in this case the optimal decision rule is to choose the class cc that pccx as this minimizes the expected error at x. however the zero-one loss is not always appropriate. a classic example of this is the difference in loss of failing to spot a disease when carrying out a medical test compared to the cost of a false positive on the test so that lc c. the optimal classifier zero-one loss is known as the bayes classifier. by this construction the feature space is divided into decision regions such that a pattern falling in decision region rc is assigned to class cc. can be more than one decision region corresponding to a single class. the boundaries between the decision regions are known as decision surfaces or decision boundaries. one would expect misclassification errors to occur in regions where the maximum class probability maxj pcjx is relatively low. this could be due to either a region of strong overlap between classes or lack of training examples within this region. thus one sensible strategy is to add a reject option so that if maxj pcjx for a threshold in then we go ahead and classify the pattern otherwise we reject it and leave the classification task to a more sophisticated system. for multi-class classification we could alternatively require the gap between the most probable and the second most probable class to exceed and otherwise reject. as is varied from to one obtains an errorreject curve plotting the percentage of patterns classified incorrectly against the percentage rejected. typically the error rate will fall as the rejection rate increases. hansen et al. provide an analysis of the error-reject trade-off. we have focused above on the probabilistic approach to classification which involves a two-stage approach of first computing a posterior distribution over functions and then combining this with the loss function to produce a decision. however it is worth noting that some authors argue that if our goal is to eventually make a decision then we should aim to approximate the classification function that minimizes the risk loss which is defined as z x dydx rlc where py x is the joint distribution of inputs and targets and cx is a classification function that assigns an input pattern x to one of c classes pn e.g. vapnik ch. as py x is unknown in this approach one often then seeks to minimize an objective function which includes the empirical risk lyi cxi as well as a regularization term. while this is a reasonable economics one usually talks of maximizing expected utility rather than minimizing expected loss loss is negative utility. this suggests that statisticians are pessimists while economists are optimists. more than one class has equal posterior probability then ties can be broken arbitrarily. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn linear models for classification method we note that the probabilistic approach allows the same inference stage to be re-used with different loss functions it can help us to incorporate prior knowledge on the function andor noise model and has the advantage of giving probabilistic predictions which can be helpful e.g. for the reject option. linear models for classification in this section we briefly review linear models for binary classification which form the foundation of gaussian process classification models in the next section. we follow the svm literature and use the labels y and y to distinguish the two classes although for the multi-class case in section we use labels. the likelihood is py w given the weight vector w and can be any function. when using the logistic from eq. the model is usually called simply logistic regression but to emphasize the parallels to linear regression we prefer the term linear logistic regression. when using the cumulative gaussian we call the model linear probit regression. as the probability of the two classes must sum to we have py w py w. thus for a data point yi the likelihood is given by i w if yi for symmetric likelihood i w if yi and functions such as the logistic or probit where z this can be written more concisely as pyixi w we see that the logistic regression model can be where fi fxi x i w. defining the logit transformation as logitx written as logitx xw. the logitx function is also called the log odds ratio. generalized linear modelling and nelder deals with the issue of extending linear models to non-gaussian data scenarios the logit transformation is the canonical link function for binary data and this choice simplifies the algebra and algorithms. given a dataset d yii n we assume that the labels are generated independently conditional on fx. using the same gaussian prior w n p as for regression in eq. we then obtain the un-normalized log posterior nx log pwx y c w p w log in the linear regression case with gaussian noise the posterior was gaussian with mean and covariance as given in eq. for classification the posterior sigmoid function is a monotonically increasing function mapping from r to it derives its name from being shaped like a letter s. linear logistic regression linear probit regression concise notation logit log odds ratio c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn concavity unique maximum irls algorithm properties of maximum likelihood classification does not have a simple analytic form. however it is easy to show that for some sigmoid functions such as the logistic and cumulative gaussian the log likelihood is a concave function of w for fixed d. as the quadratic penalty on w is also concave then the log posterior is a concave function which means that it is relatively easy to find its unique maximum. the concavity can also be derived from the fact that the hessian of log pwx y is negative definite section for further details. the standard algorithm for finding the maximum is newton s method which in this context is usually called the iteratively reweighted least squares algorithm as described e.g. in mccullagh and nelder however note that minka provides evidence that other optimization methods conjugate gradient ascent may be faster than irls. notice that a maximum likelihood treatment to an unpenalized version of eq. may result in some undesirable outcomes. if the dataset is linearly separable if there exists a hyperplane which separates the positive and negative examples then maximizing the likelihood will cause to tend to infinity however this will still give predictions in for py w although these predictions will be hard zero or one. if the problem is ill-conditioned e.g. due to duplicate linearly dependent input dimensions there will be no unique solution. predictions softmax multiple logistic as an example consider linear logistic regression in the case where x-space is two dimensional and there is no bias weight so that w is also two-dimensional. the prior in weight space is gaussian and for simplicity we have set p i. contours of the prior pw are illustrated in figure if we have a data set d as shown in figure then this induces a posterior distribution in weight space as shown in figure notice that the posterior is non-gaussian and unimodal as expected. the dataset is not linearly separable but a weight vector in the direction is clearly a reasonable choice as the posterior distribution shows. to make predictions based the training set d for a test point x we have py py x dw z integrating the prediction py x w over the posterior distribution of weights. this leads to contours of the predictive distribution as shown in figure notice how the contours are bent reflecting the integration of many different but plausible w s. in the multi-class case we use the multiple logistic softmax function where wc is the weight vector for class c and all weight vectors are collected into the matrix w the corresponding log likelihood is of the form i as in the binary case the log i wc logp pc cyix expx pn likelihood is a concave function of w it is interesting to note that in a generative approach where the classconditional distributions pxy are gaussian with the same covariance matrix py ccx w p expxwc c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn gaussian process classification figure linear logistic regression panel shows contours of the prior distribution pw n i. panel shows the dataset with circles indicating class and crosses denoting class panel shows contours of the posterior distribution pwd. panel shows contours of the predictive distribution py pyx has the form given by eq. and eq. for the two- and multi-class cases respectively the constant function is included in x. gaussian process classification for binary classification the basic idea behind gaussian process prediction is very simple we place a gp prior over the latent function fx and then squash this through the logistic function to obtain a prior on py note that is a deterministic function of f and since f is stochastic so is this construction is illustrated in figure for a oneit is a natural generalization of the linear logistic dimensional input space. latent function c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification figure panel shows a sample latent function f drawn from a gaussian process as a function of x. panel shows the result of squashing this sample function through the logistic logit function exp z to obtain the class probability regression model and parallels the development from linear regression to gp regression that we explored in section specifically we replace the linear fx function from the linear logistic model in eq. by a gaussian process and correspondingly the gaussian prior on the weights by a gp prior. the latent function f plays the r ole of a nuisance function we do not observe values of f itself observe only the inputs x and the class labels y and we are not particularly interested in the values of f but rather in in particular for test cases the purpose of f is solely to allow a convenient formulation of the model and the computational goal pursued in the coming sections will be to remove out f. we have tacitly assumed that the latent gaussian process is noise-free and combined it with smooth likelihood functions such as the logistic or probit. however one can equivalently think of adding independent noise to the latent process in combination with a step-function likelihood. in particular assuming gaussian noise and a step-function likelihood is exactly equivalent to a latent process and probit likelihood see exercise nuisance function noise-free latent process inference is naturally divided into two steps first computing the distribution of the latent variable corresponding to a test case pf y x pf x fpfx y df z z where pfx y pyfpfxpyx is the posterior over the latent variables and subsequently using this distribution over the latent f to produce a probabilistic prediction py y x y x df equivalence explains why no numerical problems arise from considering a noise-free process if care is taken with the implementation see also comment at the end of section xlatent function xclass probability c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn the laplace approximation for the binary gp classifier in the regression case gaussian likelihood computation of predictions was straightforward as the relevant integrals were gaussian and could be computed analytically. in classification the non-gaussian likelihood in eq. makes the integral analytically intractable. similarly eq. can be intractable analytically for certain sigmoid functions although in the binary case it is only a one-dimensional integral so simple numerical techniques are generally adequate. thus we need to use either analytic approximations of integrals or solutions based on monte carlo sampling. in the coming sections we describe two analytic approximations which both approximate the non-gaussian joint posterior with a gaussian one the first is the straightforward laplace approximation method and barber and the second is the more sophisticated expectation propagation method due to minka cavity tap approximation of opper and winther is closely related to the ep method. a number of other approximations have also been suggested see e.g. gibbs and mackay jaakkola and haussler and seeger neal describes the use of markov chain monte carlo approximations. all of these methods will typically scale as for large datasets there has been much work on further approximations to reduce computation time as discussed in chapter the laplace approximation for the binary case is described in section and for the multi-class case in section the ep method for binary classification is described in section relationships between gaussian process classifiers and other techniques such as spline classifiers support vector machines and least-squares classification are discussed in sections and respectively. the laplace approximation for the binary gp classifier laplace s method utilizes a gaussian approximation qfx y to the posterior pfx y in the integral doing a second order taylor expansion of log pfx y around the maximum of the posterior we obtain a gaussian approximation qfx y n f a faf where f argmaxf pfx y and a log pfx yf f is the hessian of the negative log posterior at that point. the structure of the rest of this section is as follows in section we describe how to find f and a. section explains how to make predictions having obtained qfy and section gives more implementation details for the laplace gp classifier. the laplace approximation for the marginal likelihood is described in section c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification logistic probit figure the log likelihood and its derivatives for a single case as a function of zi yifi for the logistic and the cumulative gaussian likelihood. the two likelihood functions are fairly similar the main qualitative difference being that for large negative arguments the log logistic behaves linearly whereas the log cumulative gaussian has a quadratic penalty. both likelihoods are log concave. posterior by bayes rule the posterior over the latent variables is given by pfx y pyfpfxpyx but as pyx is independent of f we need only consider the un-normalized posterior when maximizing w.r.t. f. taking the logarithm and introducing expression eq. for the gp prior gives log pyf log pfx log pyf fk log n log differentiating eq. w.r.t. f we obtain log pyf k log pyf k w k where w log pyf is diagonal since the likelihood factorizes over cases distribution for yi depends only on fi not on note that if the likelihood pyf is log concave the diagonal elements of w are non-negative and the hessian in eq. is negative definite so that is concave and has a unique maximum section for further details. there are many possible functional forms of the likelihood which gives the target class probability as a function of the latent variable f. two commonly used likelihood functions are the logistic and the cumulative gaussian see figure the expressions for the log likelihood for these likelihood functions and their first and second derivatives w.r.t. the latent variable are given in the un-normalized posterior log likelihoods and their derivatives times target ziyifi log likelihood log pyifilog derivative times target ziyifi log likelihood log pyifilog derivative c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn the laplace approximation for the binary gp classifier following table log pyifi exp fi log pyifi ti i yin log pyifi f i i log where we have defined i pyi and t at the maximum of we have n yifin f log py as a self-consistent equation for f since log py f is a non-linear function of f eq. cannot be solved directly. to find the maximum of we use newton s method with the iteration f new f f w log pyf k w f log to gain more intuition about this update let us consider what happens to datapoints that are well-explained under f so that log pyifi fi and wii are close to zero for these points. as an approximation break f into two subvectors that corresponds to points that are not well-explained and to those that are. then it is easy to show exercise that log f new f new f new where denotes the block of k containing the covariance between is computed by ignoring the two groups of points etc. this means that f new using the entirely the well-explained points and f new usual gp prediction methods treating these points like test points. of course if the predictions of f new fail to match the targets correctly they would cease to be well-explained and so be updated on the next iteration. is predicted from f new having found the maximum posterior f we can now specify the laplace approximation to the posterior as a gaussian with mean f and covariance matrix given by the negative inverse hessian of from eq. qfx y f w one problem with the laplace approximation is that it is essentially uncontrolled in that the hessian at f may give a poor approximation to the true shape of the posterior. the peak could be much broader or narrower than the hessian indicates or it could be a skew peak while the laplace approximation assumes it has elliptical contours. newton s method intuition on influence of well-explained points c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification predictions latent mean the posterior mean for f under the laplace approximation can be expressed by combining the gp predictive mean eq. with eq. into eqf y x kx f kx log py f. compare this with the exact mean given by opper and winther as epf y x ef x x ydf kx pfx ydf kx y z z where we have used the fact that for a gp ef x x kx and have let efx y denote the posterior mean of f given x and y. notice the similarity between the middle expression of eq. and eq. where the exact average efx y has been replaced with the modal value f eqfx y. a simple observation from eq. is that positive training examples will give rise to a positive coefficient for their kernel function i log pyifi in this case while negative examples will give rise to a negative coefficient this is analogous to the solution to the support vector machine see eq. also note that training points which have i log pyifi that are well-explained under f do not contribute strongly to predictions at novel test points this is similar to the behaviour of non-support vectors in the support vector machine section we can also compute vqf y the variance of f y under the gaussian approximation. this comprises of two terms i.e. vqf y x epf ef x eqfxyef x f ef y x the first term is due to the variance of f if we condition on a particular value of f and is given by kx x kx cf. eq. the second term in eq. is due to the fact that ef x f kx depends on f and thus there is an additional term of kx covfx yk under the gaussian approximation covfx y w and thus k w vqf y x kx x k kx x k k k w where the last line is obtained using the matrix inversion lemma eq. given the mean and variance of f we make predictions by computing eq y x y x df z sign of kernel coefficients latent variance averaged predictive probability c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn the laplace approximation for the binary gp classifier map prediction identical binary decisions where qf y x is gaussian with mean and variance given by equations and respectively. notice that because of the non-linear form of the sigmoid the predictive probability from eq. is different from the sigmoid of the expectation of f we will call the latter the map prediction to distinguish it from the averaged predictions from eq. in fact as shown in bishop sec. the predicted test labels given by choosing the class of highest probability obtained by averaged and map predictions are identical for binary classification. to see this note that the decision boundary using the the map value eqf y x corresponds to y x or eqf y x the decision boundary of the averaged prediction eq y x also corresponds to eqf y x this follows from the fact that is antisymmetric while qf y x is symmetric. thus if we are concerned only about the most probable classification it is not necessary to compute predictions using eq. however as soon as we also need a confidence in the prediction if we are concerned about a reject option we need eq y x if is the cumulative gaussian function then eq. can be computed analytically as shown in section on the other hand if is the logistic function then we need to resort to sampling methods or analytical approximations to compute this one-dimensional integral. one attractive method is to note that the logistic function is the c.d.f. density function corresponding to the p.d.f. density function pz this is known as the logistic or sech-squared distribution see johnson et al. ch. then by approximating pz as a mixture of gaussians one can approximate by a linear combination of error functions. this approximation was used by williams and barber app. a and wood and kohn another approximation suggested in mackay is f where vqf y x the effect of the latent predictive variance is as the approximation suggests to soften the prediction that would be obtained using the map prediction f i.e. to move it towards implementation we give implementations for finding the laplace approximation in algorithm and for making predictions in algorithm care is taken to avoid numerically unstable computations while minimizing the computational effort both can be achieved simultaneously. it turns out that several of the desired terms can be expressed in terms of the symmetric positive definite matrix b i w computation of which costs only since w is diagonal. the b matrix has eigenvalues bounded below by and bounded above by n so for many covariance functions b is guaranteed to be well-conditioned and it is kw multi-class predictions discussed in section the situation is more complicated. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification kw b i w kw input k matrix y targets pyf function initialization newton iteration eval. w e.g. using eq. or f repeat w log pyf l choleskyi w b w f log pyf a b w llw f ka kb i log lii objective af log pyf p eq. using eq. af log pyf until convergence log qyx eq. return f f mode log qyx log marg. likelihood algorithm mode-finding for binary laplace gpc. commonly used convergence criteria depend on the difference in successive values of the objective function from eq. the magnitude of the gradient vector from eq. andor the magnitude of the difference in successive values of f in a practical implementation one needs to secure against divergence by checking that each iteration leads to an increase in the objective trying a smaller step size if not. the computational complexity is dominated by the cholesky decomposition in line which takes operations the number of newton iterations all other operations are at most quadratic in n. thus numerically safe to compute its cholesky decomposition ll b which is useful in computing terms involving b and the mode-finding procedure uses the newton iteration given in eq. involving the matrix using the matrix inversion lemma eq. we get w k kw where b is given in eq. the advantage is that whereas k may have eigenvalues arbitrarily close to zero thus be numerically unstable to invert we can safely work with b. in addition algorithm keeps the vector a k in addition to f as this allows evaluation of the part of the objective in eq. which depends on f without explicit reference to k to avoid possible numerical problems. b k similarly for the computation of the predictive variance vqf from eq. we need to evaluate a quadratic form involving the matrix w rewriting this as w w w w w w b achieves numerical stability opposed to inverting w itself which may have arbitrarily small eigenvalues. thus the predictive variance from eq. can be computed as vqf kx x kx kx kx x vv where v lw kx which was also used by seeger p. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn the laplace approximation for the binary gp classifier input f x y targets k function pyf function x test input w log py f l choleskyi w kw f kx log py f vf kx x vv v f vf kx o b i w kw eq. eq. using eq. eq. return class probability class algorithm predictions for binary laplace gpc. the posterior mode f can be computed using algorithm is input. for multiple test inputs lines are applied to each test input. computational complexity is operations once plus operations per test case the one-dimensional integral in line can be done analytically for cumulative gaussian likelihood otherwise it is computed using an approximation or numerical quadrature. in practice we compute the cholesky decomposition ll b during the newton steps in algorithm which can be re-used to compute the predictive variance by doing backsubstitution with l as discussed above. in addition l may again be re-used to compute w for the computation of the marginal likelihood eq. as log log lii. to save computation one could use an incomplete cholesky factorization in the newton steps as suggested by fine and scheinberg kw sometimes it is suggested that it can be useful to replace k by k where is a small constant to improve the numerical of k. however by taking care with the implementation details as above this should not be necessary. marginal likelihood incomplete cholesky factorization z it will also be useful for chapter to compute the laplace approximation of the marginal likelihood pyx. the regression case with gaussian noise the marginal likelihood can again be calculated analytically see eq. we have pyx pyfpfx df using a taylor expansion of locally around f we obtain f faf f and thus an approximation qyx to the marginal likelihood as pyx qyx faf df z df refers to this as adding jitter in the context of markov chain monte carlo based inference in his work the latent variables f are explicitly represented in the markov chain which makes addition of jitter difficult to avoid. within the analytical approximations of the distribution of f considered here jitter is unnecessary. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification this gaussian integral can be evaluated analytically to obtain an approximation to the log marginal likelihood log qyx fk f log py f where w w and is a vector of hyperparameters of the covariance function have previously been suppressed from the notation for brevity. kw log multi-class laplace approximation f our presentation follows williams and barber we first introduce the vector of latent function values at all n training points and for all c classes n f f n f c f c n f thus f has length cn. in the following we will generally refer to quantities pertaining to a particular class with superscript c and a particular case by subscript i usual thus e.g. the vector of c latents for a particular case is fi. however as an exception vectors or matrices formed from the covariance function for class c will have a subscript c. the prior over f has the form f n k. as we have assumed that the c latent processes are uncorrelated the covariance matrix k is block diagonal in the matrices kc. each individual matrix kc expresses the correlations of the latent function values within the class c. note that the covariance functions pertaining to the different classes can be different. let y be a vector of the same length as f which for each i n has an entry of for the class which is the label for example i and for the other c entries. softmax un-normalized posterior let c i denote output of the softmax at training point i i.e. pyc ifi c i p i expf c i expf then is a vector of the same length as f with entries c analogue of eq. is the log of the un-normalized posterior i the multi-class fk nx cx exp f c i log cn log as in the binary case we seek the map value f of pfx y. by differentiating eq. w.r.t. f we obtain thus at the maximum we have f ky differentiating again and using k y expf j i c i c i i logx j f c i f i c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn multi-class laplace approximation we k w where w diag where is a cn n matrix obtained by stacking vertically the diagonal matrices diag c and c is the subvector of pertaining to class c. as in the binary case notice that is positive definite thus is concave and the maximum is unique also exercise predictive distribution for f z as in the binary case we use newton s method to search for the mode of giving f new w f y this update if coded na vely would take oc as matrices of size cn have to be inverted. however as described in section we can utilize the structure of w to bring down the computational load to the laplace approximation gives us a gaussian approximation qfx y to the posterior pfx y. to make predictions at a test point x we need to compute the posterior distribution qf y x where fx f f c in general we have qf y x pf x fqfx y df as pf x f and qfx y are both gaussian qf y x will also be gaussian and we need only compute its mean and covariance. the predictive mean for class c is given by eqf cx y x kcx c f c kcx c where kcx is the vector of covariances between the test point and each of the training points for the cth covariance function and f c is the subvector of f pertaining to class c. the last equality comes from using eq. at the maximum f. note the close correspondence to eq. this can be put into a vector form eqf q by defining the cn c matrix q kcx using a similar argument to eq. we obtain covqf y x q k w diagkx x q w where is a diagonal c c matrix with cc kcx x k c and kx x is a vector of covariances whose c th element is kcx x c kcx is a sign error in equation of williams and barber but not in their implementation. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification input k matrix y targets f repeat initialization newton iteration compute and from f with eq. and defn. of under eq. for c c do c e is block diag. d d kd log determinant compute c kcd zc l choleskyin d c lld ec d c i log lii m choleskyp c ec end for b y c ekb a b c ermmrc f ka b w f y from eq. af yf i p c expf i i eq. using eq. and af yf objective c expf i until convergence log qyx eq. return f f mode log qyx log marg. likelihood algorithm mode-finding for multi-class laplace gpc where d diag r is a matrix of stacked identity matrices and a subscript c on a block diagonal matrix indicates the n n submatrix pertaining to class c. the computational complexity is dominated by the cholesky decomposition in lines and and the forward and backward substitutions in line with total complexity oc the number of newton iterations all other operations are at most when exploiting diagonal and block diagonal structures. the memory requirement is for comments on convergence criteria for line and avoiding divergence refer to the caption of algorithm on page c zc we now need to consider the predictive distribution q which is obtained by softmaxing the gaussian qf in the binary case we saw that the predicted classification could be obtained by thresholding the mean value of the gaussian. in the multi-class case one does need to take the variability around the mean into account as it can affect the overall classification exercise one simple way will be used in algorithm to estimate the mean prediction eq is to draw samples from the gaussian qf softmax them and then average. marginal likelihood the laplace approximation to the marginal likelihood can be obtained in the same way as for the binary case yielding log pyx log qyx fk f y f nx cx log exp f c i log w kw as for the inversion of k w the determinant term can be computed efficiently by exploiting the structure of w see section in this section we have described the laplace approximation for multi-class classification. however there has also been some work on ep-type methods for the multi-class case see seeger and jordan c kcd c for c c do l choleskyin d c lld ec d c m choleskyp end for for c c do c ec c ckc b eckc c ecrmmrb for c do end for cc cc kcx x bkc e is block diag. d d kd latent test mean from eq. latent test covariance from eq. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn multi-class laplace approximation input k matrix f mode x input compute and from f using eq. and defn. of under eq. end for for i s do f n expf c initialize monte carlo loop to estimate predictive class probabilities using s samples sample latent values from joint gaussian posterior expf accumulate probability eq. end for normalize mc estimate of prediction vector return eqf x y class probability vector algorithm predictions for multi-class laplace gpc where d diag r is a matrix of stacked identity matrices and a subscript c on a block diagonal matrix indicates the n n submatrix pertaining to class c. the computational complexity is dominated by the cholesky decomposition in lines and with a total complexity oc the memory requirement is for multiple test cases repeat from line for each test case practice for multiple test cases one may reorder the computations in lines to avoid referring to all ec matrices repeatedly. implementation the implementation follows closely the implementation for the binary case detailed in section with the slight complications that k is now a block diagonal matrix of size cn cn and the w matrix is no longer diagonal see eq. care has to be taken to exploit the structure of these matrices to reduce the computational burden. the newton iteration from eq. requires the inversion of k w which we first re-write as w k kk w using the matrix inversion lemma eq. in the following the inversion of the above matrix k w is our main concern. first however we apply the c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification matrix inversion lemma eq. to the w w d ri rdr d ro where d diag r d is a cn n matrix of stacked in unit matrices c dc in and o is the zero matrix. introducing the above in k w and applying the matrix inversion lemma eq. again we have w d ro we use the fact that normalizes over classes rdr e ero rer e erp and rer c ec is a block diagonal matrix c ec. the newton iterations can now be computed by inserting eq. and in eq. as detailed in algorithm the predictions use an equivalent route to compute the gaussian posterior and the final step of deriving predictive class probabilities is done by monte carlo as shown in algorithm where e d d d kd expectation propagation the expectation propagation algorithm is a general approximation tool with a wide range of applications. in this section we present only its application to the specific case of a gp model for binary classification. we note that opper and winther presented a similar method for binary gpc based on the fixed-point equations of the thouless-anderson-palmer type of mean-field approximation from statistical physics. the fixed points for the two methods are the same although the precise details of the two algorithms are different. the ep algorithm naturally lends itself to sparse approximations which will not be discussed in detail here but touched upon in section the object of central importance is the posterior distribution over the latent variables pfx y. in the following notation we suppress the explicit dependence on hyperparameters see section for their treatment. the posterior is given by bayes rule as the product of a normalization term the prior and the likelihood pfx y pfx where the prior pfx is gaussian and we have utilized the fact that the likelihood factorizes over the training cases. the normalization term is the marginal likelihood pyifi z pyx pfx pyifi df ny ny z z who are disturbed by our sloppy treatment of the inverse of singular matrices are invited to insert the matrix between and in eq. and verify that eq. coincides with the limit c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn expectation propagation so far everything is exactly as in the regression case discussed in chapter however in the case of classification the likelihood pyifi is not gaussian a property that was used heavily in arriving at analytical solutions for the regression framework. in this section we use the probit likelihood page for binary classification and this makes the posterior in eq. analytically intractable. to overcome this hurdle in the ep framework we approximate the likelihood by a local likelihood approximation in the form of an un-normalized gaussian function in the latent variable fi pyifi pyifi tifi zi i i zin i i site parameters which defines the site parameters zi i and i remember that the notation n is used for a normalized gaussian distribution. notice that we are approximating the likelihood i.e. a probability distribution which normalizes over the targets yi by an un-normalized gaussian distribution over the latent variables fi. this is reasonable because we are interested in how the likelihood behaves as a function of the latent fi. in the regression setting we utilized the gaussian shape of the likelihood but more to the point the gaussian distribution for the outputs yi also implied a gaussian shape as a function of the latent variable fi. in order to compute the posterior we are of course primarily interested in how the likelihood behaves as a function of the property that the likelihood should normalize over yi any value of fi is not simultaneously achievable with the desideratum of gaussian dependence on fi in the ep approximation we abandon exact normalization for tractability. the product of the local likelihoods ti is ny i n tifi zi i zi i where is the vector of i and is diagonal with ii the posterior pfx y by qfx y i we approximate ny qfx y zep with and tifi zi i pfx i n where we have used eq. to compute the product by definition we know that the distribution must normalize correctly over f. notice that we use the tilde-parameters and z for the local likelihood approximations that although each likelihood approximation is local the posterior approximation produced by the ep algorithm is global because the latent variables are coupled through the prior. for computing the marginal likelihood normalization becomes crucial see section c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification and plain and for the parameters of the approximate posterior. the normalizing term of eq. zep qyx is the ep algorithm s approximation to the normalizing term z from eq. and eq. kl divergence how do we choose the parameters of the local approximating distributions ti? one of the most obvious ideas would be to minimize the kullback-leibler divergence section between the posterior and its approximation yqfx direct minimization of this kl divergence for the choose to minimize the reversed kl divergence ypfx with joint distribution on f turns out to be intractable. can alternatively respect to the distribution qfx y this has been used to carry out variational inference for gpc see e.g. seeger instead the key idea in the ep algorithm is to update the individual ti approximations sequentially. conceptually this is done by iterating the following four steps we start from some current approximate posterior from which we leave out the current ti giving rise to a marginal cavity distribution. secondly we combine the cavity distribution with the exact likelihood pyifi to get the desired marginal. thirdly we choose a gaussian approximation to the non-gaussian marginal and in the final step we compute the ti which makes the posterior have the desired marginal from step three. these four steps are iterated until convergence. in more detail we optimize the ti approximations sequentially using the approximation so far for all the other variables. in particular the approximate posterior for fi contains three kinds of terms the prior pfx the local approximate likelihoods tj for all cases j i the exact likelihood for case i pyifi our goal is to combine these sources of information and choose parameters of ti such that the marginal posterior is as accurate as possible. we will first combine the prior and the local likelihood approximations into the cavity distribution z pfxy q ifi tjfj zj j j and subsequently combine this with the exact likelihood for case i. conceptually one can think of the combination of prior and the n approximate likelihoods in eq. in two ways either by explicitly multiplying out the terms or by removing approximate likelihood i from the approximate posterior in eq. here we will follow the latter approach. the marginal for fi from qfx y is obtained by using eq. in eq. to give i ii. this marginal eq. contains one approximate term qfix y n i i where c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn expectation propagation ti too many so we need to divide it by ti to get the cavity distribution cavity distribution q ifi n i i i i where i i i i and i i i note that the cavity distribution and its parameters carry the subscript i indicating that they include all cases except number i. the easiest way to verify eq. is to multiply the cavity distribution by the local likelihood approximation ti from eq. using eq. to recover the marginal in eq. notice that despite the appearance of eq. the cavity mean and variance are course not dependent on i and i see exercise to proceed we need to find the new gaussian marginal which best approximates the product of the cavity distribution and the exact likelihood it is well known that when qx is gaussian the distribution qx which min imizes is the one whose first and second moments match that i q ifipyifi. qfi zin i of px see eq. as qfi is un-normalized we choose additionally to impose the condition that the zero-th moments constants should match when choosing the parameters of qfi to match the right hand side of eq. this process is illustrated in figure the derivation of the moments is somewhat lengthy so we have moved the details to section the desired posterior marginal moments are zi i i in i i i n zi yi in i where zi yi i i the final step is to compute the parameters of the approximation ti which achieves a match with the desired moments. in particular the product of the cavity distribution and the local approximation must have the desired moments leading to i zi zi p i i i i i i i i i i i i i which is easily verified by multiplying the cavity distribution by the local approximation using eq. to obtain eq. note that the desired marginal posterior variance i given by eq. is guaranteed to be smaller than the cavity variance such that i is always this completes the update for a local likelihood approximation ti. we then have to update the approximate posterior using eq. but since only a cases where the likelihood is log concave one can show that i but for a general likelihood there may be no such guarantee. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification figure approximating a single likelihood term by a gaussian. panel dashdotted the exact likelihood corresponding target being yi as a function of the latent fi dotted gaussian cavity distribution n i i solid posterior dashed posterior approximation. panel shows an enlargement of panel single site has changed one can do this with a computationally efficient rankone update see section the ep algorithm is used iteratively updating each local approximation in turn. it is clear that several passes over the data are required since an update of one local approximation potentially influences all of the approximate marginal posteriors. predictions the procedure for making predictions in the ep framework closely resembles the algorithm for the laplace approximation in section ep gives a gaussian approximation to the posterior distribution eq. the approximate predictive mean for the latent variable f becomes eqf y x k k k k k the approximate latent predictive variance is analogous to the derivation from eq. and eq. with playing the r ole of w vqf y x kx x k qy y x eq y x the approximate predictive distribution for the binary target becomes y x df where qf y x is the approximate latent predictive gaussian with mean and variance given by eq. and eq. this integral is readily evaluated using eq. giving the predictive probability z qy y x k kx x k p c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn expectation propagation marginal likelihood the ep approximation to the marginal likelihood can be found from the normalization of eq. z ny zep qyx pfx tifi zi i i df using eq. and eq. in an analogous way to the treatment of the regression setting in equations and we arrive at logzep log yi i log nx nx i log i i nx i i i where denotes the hyperparameters of the covariance function. this expression has a nice intuitive interpretation the first two terms are the marginal likelihood for a regression model for each component of which has independent gaussian noise of variance ii is diagonal cf. eq. the remaining three terms come from the normalization constants zi. the first of these penalizes the cavity leave-one-out distributions for not agreeing with the classification labels see eq. in other words we can see that the marginal likelihood combines two desiderata the means of the local likelihood approximations should be well predicted by a gp and the corresponding latent function when ignoring a particular training example should be able to predict the corresponding classification label well. implementation the implementation for the ep algorithm follows the derivation in the previous section closely except that care has to be taken to achieve numerical stability in similar ways to the considerations for laplace s method in section in addition we wish to be able to specifically handle the case were some site variances i may tend to infinity this corresponds to ignoring the corresponding likelihood terms and can form the basis of sparse approximations touched upon in section in this limit everything remains well-defined although this is not obvious e.g. from looking at eq. it turns out to be slightly more convenient to use natural parameters i i and i i for the site and cavity parameters i i rather than importance is i i i i i and i i themselves. the symmetric matrix of central i i s diag s which plays a r ole equivalent to eq. expressions involving the inverse of b are computed via cholesky factorization which is numerically stable since b i s k s natural parameters c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification input k matrix y targets k initialization and eq. repeat o for i to n do i i i i i i i i and i i o i i i i i compute the marginal moments i and i i ii s i o compute approximate cavity parameters i and i using eq. using eq. update site parameters i and i using eq. update and by eq. and eq. si is column i of end for l choleskyin s v l s k v and k s k re-compute the approximate posterior parameters and using eq. and eq. until convergence compute log zep using eq. and and the existing l return site param. log zep log marg. likelihood algorithm expectation propagation for binary classification. the targets y are used only in line in lines the parameters of the approximate posterior are re-computed they already exist this is done because of the large number of rank-one updates in line which would eventually cause loss of numerical precision in the computational complexity is dominated by the rank-one updates in line which takes per variable i.e. for an entire sweep over all variables. similarly re-computing in lines is the eigenvalues of b are bounded below by one. the parameters of the gaussian approximate posterior from eq. are computed as s k kk s k k s k. b s after updating the parameters of a site we need to update the approximate posterior eq. taking the new site parameters into account. for the inverse covariance matrix of the approximate posterior we have from eq. k s and thus i where ei is a unit vector in direction i and we have used that s diag using the matrix inversion lemma eq. on eq. we obtain the new new k sold new i old i new old i old new i old new i i old ii sis i in time where si is the i th column of old. the posterior mean is then calculated from eq. in the ep algorithm each site is updated in turn and several passes over all sites are required. pseudocode for the ep-gpc algorithm is given in algorithm c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn expectation propagation input site param. x y targets z s k s k ll s v s l choleskyin s f kx z vf kx x vv vf kx f k function x test input k s eq. using eq. b in s o o eq. using eq. eq. return class probability class algorithm predictions for expectation propagation. the natural site parameters and of the posterior can be computed using algorithm are input. for multiple test inputs lines are applied to each test input. computational complexity is operations once and plus operations per test case although the cholesky decomposition in line could be avoided by storing it in algorithm note the close similarity to algorithm on page there is no formal guarantee of convergence but several authors have reported that ep for gaussian process models works relatively for the predictive distribution we get the mean from eq. which is evaluated using eqf y x k s s k k s and the predictive variance from eq. similarly by s b s k vqf y x kx x k s kx x k s k b s pseudocode for making predictions using ep is given in algorithm finally we need to evaluate the approximate log marginal likelihood from eq. there are several terms which need careful consideration principally due to the fact the i values may be arbitrarily small cannot safely be inverted. we start with the fourth and first terms of eq. x log s st log s i i x log log s log lii i i where t is a diagonal matrix of cavity precisions tii i i and l is the cholesky factorization of b. in eq. we have factored out the matrix s from both determinants and the terms cancel. continuing with the part of the has been conjectured not proven by l. csat o communication that ep is guaranteed to converge if the likelihood is log concave. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification fifth term from eq. which is quadratic in together with the second term s s s s s s s k s k s b s where in eq. we apply the matrix inversion lemma eq. to both parenthesis to be inverted. the remainder of the fifth term in eq. is evaluated using the identity it s i it s t s i where i is the vector of cavity means i. the third term in eq. requires in no special treatment and can be evaluated as written. experiments in this section we present the results of applying the algorithms for gp classification discussed in the previous sections to several data sets. the purpose is firstly to illustrate the behaviour of the methods and secondly to gain some insights into how good the performance is compared to some other commonlyused machine learning methods for classification. section illustrates the action of a gp classifier on a toy binary prediction problem with a input space and shows the effect of varying the length-scale in the se covariance function. in section we illustrate and compare the behaviour of the two approximate gp methods on a simple onedimensional binary task. in section we present results for a binary gp classifier on a handwritten digit classification task and study the effect of varying the kernel parameters. in section we carry out a similar study using a multi-class gp classifier to classify digits from all ten classes in section we discuss the methods from both experimental and theoretical viewpoints. a toy problem figure illustrates the operation of a gaussian process classifier on a binary problem using the squared exponential kernel with a variable length-scale and the logistic response function. the laplace approximation was used to make the plots. the data points lie within the square as shown in panel notice in particular the lone white point amongst the black points in the ne corner and the lone black point amongst the white points in the sw corner. in panel the length-scale is a relatively short value. in this case the latent function is free to vary relatively quickly and so the classifications c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn experiments figure panel shows the location of the data points in the two-dimensional space the two classes are labelled as open circles and closed circles panels show contour plots of the predictive probability eq for signal variance f and length-scales of and respectively. the decision boundaries between the two classes are shown by the thicker black lines. the maximum value attained is and the minimum is provided by thresholding the predictive probability eq at agrees with the training labels at all data points. in contrast in panel the lengthscale is set to now the latent function must vary more smoothly and so the two lone points are misclassified. panel was obtained with as would be expected the decision boundaries are more complex for shorter length-scales. methods for setting the hyperparameters based on the data are discussed in chapter c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification figure one-dimensional toy classification dataset panel shows the dataset where points from class have been plotted at and class at together with the predictive probability for laplace s method and the ep approximation. also shown is the probability py of the data generating process. panel shows the corresponding distribution of the latent function f showing curves for the mean and standard deviations corresponding to confidence regions. one-dimensional example although laplace s method and the ep approximation often give similar results we here present a simple one-dimensional problem which highlights some of the differences between the methods. the data shown in figure consists of data points in three groups generated from a mixture of three gaussians centered on points points and points where the middle component has label and the two other components label all components have standard deviation thus the two left-most components are well separated whereas the two right-most components overlap. both approximation methods are shown with the same value of the hyperparameters and f chosen to maximize the approximate marginal likelihood for laplace s method. notice in figure that there is a considerable difference in the value of the predictive probability for negative inputs. the laplace approximation seems overly cautious given the very clear separation of the data. this effect can be explained as a consequence of the intuition that the influence of well-explained data points is effectively reduced see the discussion around eq. because the points in the left hand cluster are relatively well-explained by the model they don t contribute as strongly to the posterior and thus the predictive probability never gets very close to notice in figure the confidence region for the latent function for laplace s method actually includes functions that are negative at x which does not seem appropriate. for the positive examples centered around x on the right-hand side of figure this effect is not visible because the points around the transition between the classes at x are not so well-explained this is because the points near the boundary are competing against the points from the other class attempting to pull the latent function in opposite directions. consequently the datapoints in this region all contribute strongly. xpredictive probability xlatent function fxlaplaceep c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn experiments another sign of this effect is that the uncertainty in the latent function which is closely related to the effective local density of the data is very small in the region around x the small uncertainty reveals a high effective density which is caused by all data points in the region contributing with full weight. it should be emphasized that the example was artificially constructed specifically to highlight this effect. finally figure also shows clearly the effects of uncertainty in the latent function on eq in the region between x to x the latent mean in panel increases slightly but the predictive probability decreases in this region in panel this is caused by the increase in uncertainty for the latent function when the widely varying functions are squashed through the nonlinearity it is possible for both classes to get high probability and the average prediction becomes less extreme. binary handwritten digit classification example handwritten digit and character recognition are popular real-world tasks for testing and benchmarking classifiers with obvious application e.g. in postal services. in this section we consider the discrimination of images of the digit from images of the digit as an example of binary classification the specific choice was guided by the experience that this is probably one of the most difficult binary subtasks. classification of the digits is described in the following section. we use the us postal service database of handwritten digits which consists of segmented greyscale images normalized so that the intensity of the pixels lies in the data was originally split into a training set of cases and a testset of the remaining cases and has often been used in this configuration. unfortunately the data in the two partitions was collected in slightly different ways such that the data in the two sets did not stem from the same since the basic underlying assumption for most machine learning algorithms is that the distribution of the training and test data should be identical the original data partitions are not really suitable as a test bed for learning algorithms the interpretation of the results being hampered by the change in distribution. secondly the original test set was rather small sometimes making it difficult to differentiate the performance of different algorithms. to overcome these two problems we decided to pool the two partitions and randomly split the data into two identically sized partitions of cases each. a side-effect is that it is not trivial to compare to results obtained using the original partitions. all experiments reported here use the repartitioned data. the binary vs. data has training cases divided on vs. while the test set has cases split usps dataset usps repartitioned we present results of both laplace s method and ep using identical experimental setups. the squared exponential covariance function kx squared exponential covariance function is well known e.g. that the original test partition had more difficult cases than the training set. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification figure binary laplace approximation vs. discrimination using the usps data. panel shows a contour plot of the log marginal likelihood as a function of log and log f the marginal likelihood has an optimum at log and log f with an optimum value of log pyx panel shows a contour plot of the amount of information excess of a simple base-line model see text about the test cases in bits as a function of the same variables. the statistical uncertainty of the finite number of test cases is about bits confidence interval. panel shows a histogram of the latent means for the training and test sets respectively at the values of the hyperparameters with optimal marginal likelihood panel panel shows the number of test errors of when predicting using the sign of the latent mean. f exp was used so there are two free parameters namely f process standard deviation which controls its vertical scaling and the length-scale controls the input length-scale. let log f denote the vector of hyperparameters. we first present the results of laplace s method in figure and discuss these at some length. we then briefly compare these with the results of the ep method in figure hyperparameters lengthscale logllog magnitude log flog marginal likelihood lengthscale logllog magnitude log finformation about test targets in means ffrequencytraining set latent means means ffrequencytest set latent lengthscale logllog magnitude log of test misclassifications c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn experiments figure the ep algorithm on vs. digit discrimination task from the usps data. panel shows a contour plot of the log marginal likelihood as a function of the hyperparameters log and log f the marginal likelihood has an optimum at log at the maximum value of log f but the log marginal likelihood is essentially flat as a function of log f in this region so a good point is at log f where the log marginal likelihood has a value of panel shows a contour plot of the amount of information excess of the baseline model about the test cases in bits as a function of the same variables. zero bits corresponds to no information and one bit to perfect binary generalization. the test cases allows the information to be determined within bits. panel shows a histogram of the latent means for the training and test sets respectively at the values of the hyperparameters with optimal marginal likelihood panel a. panel shows the number of test errors of when predicting using the sign of the latent mean. in figure we show a contour plot of the approximate log marginal likelihood log qyx as a function of log and log f obtained from runs on a grid of evenly-spaced values of log and evenly-spaced values of log f notice that there is a maximum of the marginal likelihood laplace results lengthscale logllog magnitude log flog marginal likelihood lengthscale logllog magnitude log finformation about test targets in means ffrequencytraining set latent means means ffrequencytest set latent lengthscale logllog magnitude log of test misclassifications c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification test log predictive probability base-line method interpretation of information score error rate near log and log f as will be explained in chapter we would expect that hyperparameters that yield a high marginal likelihood would give rise to good predictions. notice that an increase of unit on the log scale means that the probability is times larger so the marginal likelihood in figure is fairly well peaked. there are at least two ways we can measure the quality of predictions at the test points. the first is the test log predictive probability py in figure we plot the average over the test set of the test log predictive probability for the same range of hyperparameters. we express this as the amount of information in bits about the targets by using log to the base further we off-set the value by subtracting the amount of information that a simple base-line method would achieve. as a base-line model we use the best possible model which does not use the inputs in this case this model would just produce a predictive distribution reflecting the frequency of the two classes in the training set i.e. bits essentially bit. the classes had been perfectly balanced and the training and test partitions also exactly balanced we would arrive at exactly bit. thus our scaled information score used in figure would be zero for a method that did random guessing and bit for a method which did perfect classification complete confidence. the information score measures how much information the model was able to extract from the inputs about the identity of the output. note that this is not the mutual information between the model output and the test targets but rather the kullback-leibler divergence between them. figure shows that there is a good qualitative agreement between the marginal likelihood and the test information compare panels and the second perhaps most commonly used method for measuring the quality of the predictions is to compute the number of test errors made when using the predictions. this is done by computing eq eq. for each test point thresholding at to get hard predictions and counting the number of errors. figure shows the number of errors produced for each entry in the grid of values for the hyperparameters. the general trend in this table is that the number of errors is lowest in the top left-hand corner and increases as one moves right and downwards. the number of errors rises dramatically in the far bottom righthand corner. however note in general that the number of errors is quite small are cases in the test set. the qualitative differences between the two evaluation criteria depicted in figure panels and may at first sight seem alarming. and although panels and show similar trends one may worry about using to select the hyperparameters if one is interested in minimizing the test misclassification rate. indeed a full understanding of all aspects of these plots is quite involved but as the following discussion suggests we can explain the major trends. first bear in mind that the effect of increasing is to make the kernel function broader so we might expect to observe effects like those in figure c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn experiments where large widths give rise to a lack of flexibility. keeping constant the effect of increasing f is to increase the magnitude of the values obtained for f. by itself this would lead to harder predictions predictive probabilities closer to or but we have to bear in mind that the variances associated will also increase and this increased uncertainty for the latent variables tends to soften the predictive probabilities i.e. move them closer to the most marked difference between figure and is the behaviour in the the top left corner where classification error rate remains small but the test information and marginal likelihood are both poor. in the left hand side of the plots the length scale is very short. this causes most points to be deemed far away from most other points. in this regime the prediction is dominated by the class-label of the nearest neighbours and for the task at hand this happens to give a low misclassification rate. in this parameter region the test latent variables f are very close to zero corresponding to probabilities very close to consequently the predictive probabilities carry almost no information about the targets. in the top left corner the predictive probabilities for all test cases lie in the interval notice that a large amount of information implies a high degree of correct classification but not vice versa. at the optimal marginal likelihood values of the hyperparameters there are misclassifications which is slightly higher that the minimum number attained which is errors. in exercise readers are encouraged to investigate further the behaviour of f and the predictive probabilities etc. as functions of log and log f for themselves. in figure we show the results on the same experiment using the ep method. the findings are qualitatively similar but there are significant differences. in panel the approximate log marginal likelihood has a different shape than for laplace s method and the maximum of the log marginal likelihood is about units on a natural log scale larger the marginal probability is times higher. also note that the marginal likelihood has a ridge log that extends into large values of log f for these very large latent amplitudes also panel the probit likelihood function is well approximated by a step function it transitions from low to high values in the domain once we are in this regime it is of course irrelevant exactly how large the magnitude is thus the ridge. notice however that this does not imply that the prediction will always be hard since the variance of the latent function also grows. figure shows a good qualitative agreement between the approximate log marginal likelihood and the test information compare panels and the best value of the test information is significantly higher for ep than for laplace s method. the classification error rates in panel show a fairly similar behaviour to that of laplace s method. in figure we show the latent means for training and test cases. these show a clear separation on the training set and much larger magnitudes than for laplace s method. the absolute values of the entries in f are quite large often well in excess of which may suggest very hard predictions close to zero or one ep results c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification figure map vs. averaged predictions for the ep algorithm for the s vs. s digit discrimination using the usps data. the optimal values of the hyperparameters from figure log and log f are used. the map predictions are hard mostly being very close to zero or one. on the other hand the averaged predictions eq from eq. are a lot less extreme. in panel the cases that were misclassified are indicated by crosses classified cases are shown by points. note that only of the misclassified points have confident predictions outside notice that all points fall in the triangles below and above the horizontal line confirming that averaging does not change the most probable class and that it always makes the probabilities less extreme closer to panel shows histograms of averaged and map predictions where we have truncated values over since the sigmoid saturates for smaller arguments. however when taking the uncertainties in the latent variables into account and computing the predictions using averaging as in eq. the predictive probabilities are softened in figure we can verify that the averaged predictive probabilities are much less extreme than the map predictions. in order to evaluate the performance of the two approximate methods for gp classification we compared to a linear probit model a support vector machine a least-squares classifier and a nearest neighbour approach all of which are commonly used in the machine learning community. in figure we show error-reject curves for both misclassification rate and the test information measure. the error-reject curve shows how the performance develops as a function of the fraction of test cases that is being rejected. to compute these we first modify the methods that do not naturally produce probabilistic predictions to do so as described below. based on the predictive probabilities we reject test cases for which the maximum predictive probability is smaller than a threshold. varying the threshold produces the error-reject curve. the gp classifiers applied in figure used the hyperparameters which optimized the approximate marginal likelihood for each of the two methods. for the gp classifiers there were two free parameters f and the linear probit model logistic models are probably more common but we chose the probit here since the other likelihood based methods all used probit can be error-reject curve linear probit model map averagedfrequency c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn experiments figure panel shows the error-reject curve and panel the amount of information about the test cases as a function of the rejection rate. the probabilistic one nearest neighbour method has much worse performance than the other methods. gaussian processes with ep behaves similarly to svm s although the classification rate for svm for low rejection rates seems to be a little better. laplace s method is worse than ep and svm. the gp least squares classifier described in section performs the best. implemented as gp model using laplace s method which is equivalent to not computationally as efficient as iteratively reweighted least squares the covariance function kx has a single hyperparameter which was set by maximizing the log marginal likelihood. this gives log pyx at thus the marginal likelihood for the linear covariance function is about units on a natural log scale lower than the maximum log marginal likelihood for the laplace approximation using the squared exponential covariance function. the support vector machine classifier section for further details on the svm used the same se kernel as the gp classifiers. for the svm the r ole of is identical and the trade-off parameter c in the svm formulation f we carried out cross validation eq. plays a similar r ole to on a grid in parameter space to identify the best combination of parameters w.r.t. the error rate this turned out to be at c our experiments were conducted using the svmtorch software and bengio in order to compute probabilistic predictions we squashed the test-activities through a cumulative gaussian using the methods proposed by platt we made a parameterized linear transformation of the test-activities and fed this through the cumulative the parameters of the linear transformation were chosen to maximize the log predictive probability evaluated on the hold-out sets of the cross validation. the probabilistic one nearest neighbour method is a simple natural extension to the classical one nearest neighbour method which provides probabilistic predictions. it computes the leave-one-out one nearest neighbour prediction on the training set and records the fraction of cases where the loo predictions were correct. on test cases the method then pre used a logistic whereas we use a cumulative gaussian. support vector machine probabilistic one nearest neighbour ratemisclassification ratetest information probit c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification dicts the one nearest neighbour class with probability and the other class with probability rejections are based on thresholding on the distance to the nearest neighbour. the least-squares classifier is described in section in order to produce probabilistic predictions the method of platt was used described above for the svm using the predictive means only predictive variances were except that instead of the cross validation leave-one-out cross-validation was used and the kernel parameters were also set using loo-cv. figure shows that the three best methods are the ep approximation for gpc the svm and the least-squares classifier presenting both the error rates and the test information helps to highlight differences which may not be apparent from a single plot alone. for example laplace s method and ep seem very similar on error rates but quite different in test information. notice also that the error-reject curve itself reveals interesting differences e.g. notice that although the method has an error rate comparable to other methods at zero rejections things don t improve very much when rejections are allowed. refer to section for more discussion of the results. handwritten digit classification example we apply the multi-class laplace approximation developed in section to the handwritten digit classification problem from the usps dataset having n training cases and n cases for testing see page we used a squared exponential covariance function with two hyperparameters a single signal amplitude f common to all latent functions and a single length-scale parameter common to all latent functions and common to all input dimensions. the behaviour of the method was investigated on a grid of values for the hyperparameters see figure note that the correspondence between the log marginal likelihood and the test information is not as close as for laplace s method for binary classification in figure on page the maximum value of the log marginal likelihood attained is and for the hyperparameters corresponding to this point the error rate is and the test information bits. as with the binary classification problem the test information is standardized by subtracting off the negative entropy of the targets which is bits. the classification error rate in figure shows a clear minimum and this is also attained at a shorter length-scale than where the marginal likelihood and test information have their maxima. this effect was also seen in the experiments on binary classification. to gain some insight into the level of performance we compared these results with those obtained with the probabilistic one nearest neighbour method a multiple logistic regression model and a svm. the first uses an course one could also have tried a variant where the full latent predictive distribution was averaged over but we did not do that here. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn experiments figure digit classification using the laplace approximation. panel shows the approximate log marginal likelihood reaching a maximum value of log pyx at log and log f in panel information about the test cases is shown. the maximum possible amount of information about the test targets corresponding to perfect classification would be bits entropy of the targets. at the point of maximum marginal likelihood the test information is bits. in panel the test set misclassification rate is shown in percent. at the point of maximum marginal likelihood the test error rate is internal leave-one-out assessment on the training set to estimate its probability of being correct for the test set it then predicts the nearest neighbour with probability and all other classes with equal probability we obtained a test information of bits and a test set classification error rate of we also compare to multiple linear logistic regression. one way to implement this method is to view it as a gaussian process with a linear covariance lengthscale logllog magnitude log flog marginal likelihood lengthscale logllog magnitude log finformation about the test targets in lengthscale logllog magnitude log ftest set misclassification c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification function although it is equivalent and computationally more efficient to do the laplace approximation over the weights of the linear model. in our case there are weights inputs and one bias whereas there are latent function values in the gp. the linear covariance function kx has a single hyperparameter for all latent functions. optimizing the log marginal likelihood w.r.t. gives log pyx at using this value for the hyperparameter the test information is bits and the test set error rate is finally a support vector machine classifier was trained using the same se kernel as the gaussian process classifiers. section for further details on the svm. as in the binary svm case there were two free parameters length-scale of the kernel and the trade-off parameter c eq. f we carried out cross-validation on a grid which plays a similar r ole to in parameter space to identify the best combination of parameters w.r.t. the error rate this turned out to be at c our experiments were conducted using the svmtorch software and bengio which implements multi-class svm classification using the one-versus-rest method described in section the test set error rate for the svm is we did not attempt to evaluate the test information for the multi-class svm. discussion in the previous section we presented several sets of experiments comparing the two approximate methods for inference in gpc models and comparing them to other commonly-used supervised learning methods. in this section we discuss the results and attempt to relate them to the properties of the models. for the binary examples from figures and we saw that the two approximations showed quite different qualitative behaviour of the approximated log marginal likelihood although the exact marginal likelihood is of course identical. the ep approximation gave a higher maximum value of the log marginal likelihood about units on the log scale and the test information was somewhat better than for laplace s method although the test set error rates were comparable. however although this experiment seems to favour the ep approximation it is interesting to know how close these approximations are to the exact intractable solutions. in figure we show the results of running a sophisticated markov chain monte carlo method called annealed importance sampling carried out by kuss and rasmussen the usps dataset for these experiments was identical to the one used in figures and so the results are directly comparable. it is seen that the mcmc results indicate that the ep method achieves a very high level of accuracy i.e. that the difference between ep and laplace s method is caused almost exclusively by approximation errors in laplace s method. the main reason for the inaccuracy of laplace s method is that the high dimensional posterior is skew and that the symmetric approximation centered on the mode is not characterizing the posterior volume very well. the posterior monte carlo results c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn discussion figure the log marginal likelihood panel and test information panel for the usps s vs. s binary classification task computed using markov chain monte carlo comparing this to the laplace approximation figure and figure shows that the ep approximation is surprisingly accurate. the slight wiggliness of the contour lines are caused by finite sample effects in the mcmc runs. is a combination of the gaussian prior centered on the origin and the likelihood terms which cut off half-spaces which do not agree with the training set labels. therefore the posterior looks like a correlated gaussian restricted to the orthant which agrees with the labels. its mode will be located close to the origin in that orthant and it will decrease rapidly in the direction towards the origin due to conflicts from the likelihood terms and decrease only slowly in the opposite direction of the prior. seen in this light it is not surprising that the laplace approximation is somewhat inaccurate. this explanation is corroborated further by kuss and rasmussen it should be noted that all the methods compared on the binary digits classification task except for the linear probit model are using the squared distance between the digitized digit images measured directly in the image space as the sole input to the algorithm. this distance measure is not very well suited for the digit discrimination task for example two similar images that are slight translations of each other may have a huge squared distance although of course identical labels. one of the strengths of the gp formalism is that one can use prior distributions over in this case functions and do inference based on these. if however the prior over functions depends only on one particular aspect of the data squared distance in image space which is not so well suited for discrimination then the prior used is also not very appropriate. it would be more interesting to design covariance functions by hyperparameters which are more appropriate for the digit discrimination task e.g. reflecting on the known invariances in the images such as the tangent-distance ideas from simard et al. see also sch olkopf and smola ch. and section the results shown here follow the common approach of using a generic suitablility of the covariance function lengthscale logllog magnitude log flog marginal likelihood lengthscale logllog magnitude log finformation about test targets in c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification covariance function with a minimum of hyperparameters but this doesn t allow us to incorporate much prior information about the problem. for an example in the gp framework for doing inference about multiple hyperparameters with more complex covariance functions which provide clearly interpretable information about the data see the carbon dioxide modelling problem discussed on page appendix moment derivations consider the integral of a cumulative gaussian with respect to a gaussian z z or in matrix notation initially for the special case v writing out in full substituting z y x m and w x and interchanging the order of the integrals z x n dy dy dx dw dz w z dw dz dw dz v v x m dx where z z x z z m w z m z z m z w h z m v v z z dz m i.e. an integral over a joint gaussian. the inner integral corresponds to marginalizing over w eq. yielding which assumed v if v is negative we can substitute the symmetry z into eq. to get exp m m dx where z qx z x m collecting the two cases eq. and eq. we arrive at z for general v we wish to compute the moments of x m z v v m v c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn exercises where z is given in eq. perhaps the easiest way to do this is to differentiate w.r.t. on both sides of eq. z x x m x x m v dx v dx z z z n v where we have used n z we recognize the first term in the integral in the top line of eq. as z times the first moment of q which we are seeking. multiplying through by and rearranging we obtain first moment eqx similarly the second moment can be obtained by differentiating eq. twice z h x eqx x m i v dx zn second moment where the first and second terms of the integral in the top line of eq. are multiples of the first and second moments. the second central moment after reintroducing eq. into eq. and simplifying is given by eq z n exercises for binary gpc show the equivalence of using a noise-free latent process combined with a probit likelihood and a latent process with gaussian noise combined with a step-function likelihood. hint introduce explicitly additional noisy latent variables fi which differ from fi by gaussian noise. write down the step function likelihood for a single case as a function of fi integrate out the noisy variable to arrive at the probit likelihood as a function of the noise-free process. consider a multinomial random variable y having c states with yc if the variable is in state c and otherwise. state c occurs with probability c. show that covy ey diag observe that covy being a covariance matrix must necessarily be positive semidefinite. using this fact show that the matrix w diag from eq. is positive semidefinite. by showing that the vector of all ones is an eigenvector of covy with eigenvalue zero verify that the matrix is indeed positive semidefinite and not positive definite. section for definitions of positive semidefinite and positive definite matrices. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn classification figure the decision regions for the three-class softmax function in space. consider the softmax function pcc expfc where c and are the corresponding activations. to more easily visualize the decision boundaries let and thus and similarly for the other classes. the decision boundary relating to is the curve the decision regions for the three classes are illustrated in figure let f have a gaussian distribution centered on the origin and let softmaxf. we now consider the effect of this distribution on df. for a gaussian with given covariance structure this integral is easily approximated by drawing samples from pf. show that the classification can be made to fall into any of the three categories depending on the covariance matrix. thus by considering displacements of the mean of the gaussian by from the origin into each of the three regions we have shown that overall classification depends not only on the mean of the gaussian but also on its covariance. show that this conclusion is still valid when it is recalled that z is derived from f as z t f where t so that covz t covft consider the update equation for f new given by eq. when some of the training points are well-explained under f so that ti i and wii c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn exercises for these points. break f into two subvectors that corresponds to points that are not well-explained and to those that are. re-write w from eq. as ki w k and let k be partitioned as and similarly for the other matrices. using the partitioned matrix inverse equations section show that log f new f new f new see section for the consequences of this result. show that the expressions in eq. for the cavity mean i and variance i do not depend on the approximate likelihood terms i and i for the corresponding case despite the appearance of eq. consider the usps vs. prediction problem discussed in section use the implementation of the laplace binary gpc provided to investigate how f and the predictive probabilities etc. vary as functions of log and log f c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter covariance functions we have seen that a covariance function is the crucial ingredient in a gaussian process predictor as it encodes our assumptions about the function which we wish to learn. from a slightly different viewpoint it is clear that in supervised learning the notion of similarity between data points is crucial it is a basic assumption that points with inputs x which are close are likely to have similar target values y and thus training points that are near to a test point should be informative about the prediction at that point. under the gaussian process view it is the covariance function that defines nearness or similarity. an arbitrary function of input pairs x and will not in general be a valid covariance the purpose of this chapter is to give examples of some commonly-used covariance functions and to examine their properties. section defines a number of basic terms relating to covariance functions. section gives examples of stationary dot-product and other non-stationary covariance functions and also gives some ways to make new ones from old. section introduces the important topic of eigenfunction analysis of covariance functions and states mercer s theorem which allows us to express the covariance function certain conditions in terms of its eigenfunctions and eigenvalues. the covariance functions given in section are valid when the input domain x is a subset of rd. in section we describe ways to define covariance functions when the input domain is over structured objects such as strings and trees. preliminaries a stationary covariance function is a function of x thus it is invariant to translations in the input for example the squared exponential co be a valid covariance function it must be positive semidefinite see eq. stochastic process theory a process which has constant mean and whose covariance function is invariant to translations is called weakly stationary. a process is strictly stationary if all of its finite dimensional distributions are invariant to translations sec. similarity valid covariance functions stationarity c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn isotropy dot product covariance kernel gram matrix covariance matrix positive semidefinite covariance functions variance function given in equation is stationary. if further the covariance function is a function only of then it is called isotropic it is thus invariant to all rigid motions. for example the squared exponential covariance function given in equation is isotropic. as k is now only a function of r these are also known as radial basis functions if a covariance function depends only on x and through x we call it a dot product covariance function. a simple example is the covariance function x which can be obtained from linear regression by putting kx n priors on the coefficients of xd d and a prior of n on the bias constant function see eq. another important example x where p is a is the inhomogeneous polynomial kernel kx positive integer. dot product covariance functions are invariant to a rotation of the coordinates about the origin but not translations. a general name for a function k of two arguments mapping a pair of inputs x x x into r is a kernel. this term arises in the theory of integral operators where the operator tk is defined as kx d z x where denotes a measure see section for further explanation of this a real kernel is said to be symmetric if kx x clearly covariance functions must be symmetric from the definition. given a set of input points n we can compute the gram matrix k whose entries are kij kxi xj. if k is a covariance function we call the matrix k the covariance matrix. a real n n matrix k which satisfies qv vkv for all vectors v rn is called positive semidefinite if qv only when v the matrix is positive definite. qv is called a quadratic form. a symmetric matrix is psd if and only if all of its eigenvalues are non-negative. a gram matrix corresponding to a general kernel function need not be psd but the gram matrix corresponding to a covariance function is psd. a kernel is said to be positive semidefinite if kx d d z for all f equivalently a kernel function which gives rise to psd gram matrices for any choice of n n and d is positive semidefinite. to see this let f be the weighted sum of delta functions at each xi. since such functions are limits of functions in eq. implies that the gram matrix corresponding to any d is psd. for a one-dimensional gaussian process one way to understand the characteristic length-scale of the process this exists is in terms of the number of upcrossings of a level u. adler theorem states that the expected speaking readers will usually be able to substitute dx or pxdx for d upcrossing rate c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn examples of covariance functions number of upcrossings enu of the level u on the unit interval by a zero-mean stationary almost surely continuous gaussian process is given by s enu exp if does not exist that the process is not mean square differentiable then if such a process has a zero at then it will almost surely have an infinite number of zeros in the arbitrarily small interval and lindsey p. mean square continuity and differentiability we now describe mean square continuity and differentiability of stochastic processes following adler sec. let be a sequence of points and x be a fixed point in rd such that x as k then a process fx is continuous in mean square at x if efxk fx as mean square continuity k if this holds for all x a where a is a subset of rd then fx is said to be continuous in mean square over a. a random field is continuous in mean square at x if and only if its covariance function kx is continuous at the point x x for stationary covariance functions this reduces to checking continuity at note that ms continuity does not necessarily imply sample function continuity for a discussion of sample function continuity and differentiability see adler ch. the mean square derivative of fx in the ith direction is defined as mean square differentiability fx xi l. i. m h fx hei fx h when the limit exists where l.i.m denotes the limit in mean square and ei is the unit vector in the ith direction. the covariance function of fx xi is given by xi i. these definitions can be extended to higher order derivatives. for stationary processes if the partial derivative exists and is finite at x then the kth order partial derivative kfx xik exists for all x rd as a mean square limit. notice that it is the properties of the kernel k around that determine the smoothness properties differentiability of a stationary process. examples of covariance functions in this section we consider covariance functions where the input domain x is a subset of the vector space rd. more general input spaces are considered in section we start in section with stationary covariance functions then consider dot-product covariance functions in section and other varieties of non-stationary covariance functions in section we give an overview of some commonly used covariance functions in table and in section c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions we describe general methods for constructing new kernels from old. there exist several other good overviews of covariance functions see e.g. abrahamsen stationary covariance functions in this section section it will be convenient to allow kernels to be a map from x x x into c than r. if a zero-mean process f is complexvalued then the covariance function is defined as kx efxf where denotes complex conjugation. a stationary covariance function is a function of x sometimes in this case we will write k as a function of a single argument i.e. k the covariance function of a stationary process can be represented as the fourier transform of a positive finite measure. bochner s theorem theorem s theorem a complex-valued function k on rd is the covariance function of a weakly stationary mean square continuous complexvalued random process on rd if and only if it can be represented as where is a positive finite measure. k rd is d z spectral density power spectrum the statement of bochner s theorem is quoted from stein p. a proof can be found in gihman and skorohod p. if has a density ss then s is known as the spectral density or power spectrum corresponding to k. the construction given by eq. puts non-negative power into each frequency s this is analogous to the requirement that the prior covariance matrix p on the weights in equation be non-negative definite. z z in the case that the spectral density ss exists the covariance function and the spectral density are fourier duals of each other as shown in eq. this is known as the wiener-khintchine theorem see e.g. chatfield k notice that the variance of the process is ss ds so the power spectrum ss k is d is ds must be integrable to define a valid gaussian process. to gain some intuition for the definition of the power spectrum given in eq. it is important to realize that the complex exponentials is x are eigenfunctions of a stationary kernel with respect to lebesgue measure section for further details. thus ss is loosely speaking the amount of power allocated on average to the eigenfunction is x with frequency s. ss must eventually decay sufficiently fast as so that it is integrable the appendix for details of fourier transforms. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn examples of covariance functions rate of this decay of the power spectrum gives important information about the smoothness of the associated stochastic process. for example it can determine the mean-square differentiability of the process section for further details. if the covariance function is isotropic that it is a function of r where r then it can be shown that ss is a function of s only theorem in this case the integrals in eq. can be simplified by changing to spherical polar coordinates and integrating out the angular variables e.g. bracewell ch. to obtain z z kr ss ds dr where is a bessel function of order note that the dependence on the dimensionality d in equation means that the same isotropic functional form of the spectral density can give rise to different isotropic covariance functions in different dimensions. similarly if we start with a particular isotropic covariance function kr the form of spectral density will in general depend on d e.g. the mat ern class spectral density given in eq. and in fact kr may not be valid for all d. a necessary condition for the spectral density to exist is thatr rd dr see stein sec. for more details. we now give some examples of commonly-used isotropic covariance functions. the covariance functions are given in a normalized form where we can multiply k by a constant f to get any desired process variance. squared exponential covariance function the squared exponential covariance function has already been introduced in chapter eq. and has the form kser exp with parameter defining the characteristic length-scale. using eq. we see that the mean number of level-zero upcrossings for a se process in is which confirms the r ole of as a length-scale. this covariance function is infinitely differentiable which means that the gp with this covariance function has mean square derivatives of all orders and is thus very smooth. the spectral density of the se covariance function is ss exp stein argues that such strong smoothness assumptions are unrealistic for modelling many physical processes and recommends the mat ern class below. however the squared exponential is probably the most widely-used kernel within the kernel machines field. squared exponential characteristic length-scale c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions infinitely divisible the se kernel is infinitely divisible in that is a valid kernel for all t the effect of raising k to the power of t is simply to rescale infinite network construction for se covariance function mat ern class we now digress briefly to show that the squared exponential covariance function can also be obtained by expanding the input x into a feature space defined by gaussian-shaped basis functions centered densely in x-space. for simplicity of exposition we consider scalar inputs with basis functions cx where c denotes the centre of the basis function. from sections and we recall that with a gaussian prior on the weights w n pi this gives rise to a gp with covariance function kxp xq p cxp cxq. now allowing an infinite number of basis functions centered everywhere on an interval scaling down the variance of the prior on the weights with the number of basis functions we obtain the limit nx z cmax cmin nx lim n p n cxp cxq p cxp cxqdc. plugging in the gaussian-shaped basis functions eq. and letting the integration limits go to infinity we obtain z p kxp xq p which we recognize as a squared exponential covariance function with a times longer length-scale. the derivation is adapted from mackay it is straightforward to generalize this construction to multivariate x. see also eq. for a similar construction where the centres of the basis functions are sampled from a gaussian distribution the constructions are equivalent when the variance of this gaussian tends to infinity. the mat ern class of covariance functions the mat ern class of covariance functions is given by kmaternr r k r with positive parameters and where k is a modified bessel function and stegun sec. this covariance function has a spectral density ss c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn examples of covariance functions figure panel covariance functions and random functions drawn from gaussian processes with mat ern covariance functions eq. for different values of with the sample functions on the right were obtained using a discretization of the x-axis of equally-spaced points. in d dimensions. note that the scaling is chosen so that for we obtain the se covariance function e see eq. stein named this the mat ern class after the work of mat ern for the mat ern class the process fx is k-times ms differentiable if and only if k. the mat ern covariance functions become especially simple when is half-integer p where p is a non-negative integer. in this case the covariance function is a product of an exponential and a polynomial of order p the general expression can be derived from and stegun eq. giving px r i i! i!p i! r k exp it is possible that the most interesting cases for machine learning are and for which exp exp k k since for the process becomes very rough below and for in the absence of explicit prior knowledge about the existence of higher order derivatives it is probably very hard from finite noisy training examples to distinguish between values of even to distinguish between finite values of and the smooth squared exponential in this case. for example a value of was used in et al. ornstein-uhlenbeck process and exponential covariance function the special case obtained by setting in the mat ern class gives the exponential covariance function kr exp r the corresponding process exponential distance rcovariance kr xoutput fx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions figure panel covariance functions and random functions drawn from gaussian processes with the covariance function eq. for different values of with the sample functions are only differentiable when se case. the sample functions on the right were obtained using a discretization of the x-axis of equally-spaced points. is ms continuous but not ms differentiable. in d this is the covariance function of the ornstein-uhlenbeck process. the ou process and ornstein was introduced as a mathematical model of the velocity of a particle undergoing brownian motion. more generally in d setting p for integer p gives rise to a particular form of a continuous-time arp gaussian process for further details see section the form of the mat ern covariance function and samples drawn from it for and are illustrated in figure the covariance function the family of covariance functions which includes both the exponential and squared exponential is given by kr for although this function has a similar number of parameters to the mat ern class it is stein notes in a sense less flexible. this is because the corresponding process is not ms differentiable except when it is infinitely ms differentiable. the covariance function and random samples from the process are shown in figure a proof of the positive definiteness of this covariance function can be found in schoenberg rational quadratic covariance function the rational quadratic covariance function krqr ornstein-uhlenbeck process rational quadratic distancecovariance xoutput fx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn examples of covariance functions figure panel covariance functions and random functions drawn from gaussian processes with rational quadratic covariance functions eq. for different values of with the sample functions on the right were obtained using a discretization of the x-axis of equally-spaced points. with can be seen as a scale mixture infinite sum of squared exponential covariance functions with different characteristic length-scales of covariance functions are also a valid covariance see section parameterizing now in terms of inverse squared length scales and putting a gamma distribution on p exp we can add up the contributions through the following integral p d d exp z z krqr exp scale mixture where we have set the rational quadratic is also discussed by mat ern p. using a slightly different parameterization in our notation the limit of the rq covariance for eq. is the se covariance function with characteristic length-scale eq. figure illustrates the behaviour for different values of note that the process is infinitely ms differentiable for every in contrast to the mat ern covariance function in figure the previous example is a special case of kernels which can be written as superpositions of se kernels with a distribution p of length-scales kr r exp d this is in fact the most general representation for an isotropic kernel which defines a valid covariance function in any dimension d see sec. piecewise polynomial covariance functions with compact support a family of piecewise polynomial functions with compact support provide another interesting class of covariance functions. compact support means that note that there are several common ways to parameterize the gamma distribution our choice is convenient here is the shape and is the mean. piecewise polynomial covariance functions with compact support distancecovariance xoutput fx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions figure panel covariance functions and random functions drawn from gaussian processes with piecewise polynomial covariance functions with compact support from eq. with specified parameters. positive definiteness restricted dimension the covariance between points become exactly zero when their distance exceeds a certain threshold. this means that the covariance matrix will become sparse by construction leading to the possibility of computational the challenge in designing these functions is how to guarantee positive definiteness. multiple algorithms for deriving such covariance functions are discussed by wendland ch. these functions are usually not positive definite for all input dimensions but their validity is restricted up to some maximum dimension d. below we give examples of covariance functions kppdqr which are positive definite in rd rj c q where j b d the properties of three of these covariance functions are illustrated in figure these covariance functions are continuously differentiable and thus the corresponding processes are q-times mean-square differentiable see section it is interesting to ask to what extent one could use the compactly-supported covariance functions described above in place of the other covariance functions mentioned in this section while obtaining inferences that are similar. one advantage of the compact support is that it gives rise to sparsity of the gram matrix which could be exploited for example when using iterative solutions to gpr problem see section the product of the inverse covariance matrix with a vector e.g. for prediction is computed using a conjugate gradient algorithm then products of the covariance matrix with vectors are the basic computational unit and these can obviously be carried out much faster if the matrix is sparse. distance rcovariance xoutput fx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn examples of covariance functions further properties of stationary covariance functions the covariance functions given above decay monotonically with r and are always positive. however this is not a necessary condition for a covariance function. for example yaglom shows that kr c r j r is a valid covariance function for and this function has the form of a damped oscillation. anisotropic versions of these isotropic covariance functions can be created by setting for some positive semidefinite m. if m is diagonal this implements the use of different length-scales on different dimensions for further discussion of automatic relevance determination see section general m s have been considered by mat ern p. poggio and girosi and also in vivarelli and williams in the latter work a low-rank m was used to implement a linear dimensionality reduction step from the input space to lower-dimensional feature space. more generally one could assume the form where is a d k matrix whose columns define k directions of high relevance and is a diagonal matrix positive entries capturing the axisaligned relevances see also figure on page thus m has a factor analysis form. for appropriate choices of k this may represent a good trade-off between flexibility and required number of parameters. m kx the kernel ktx stationary kernels can also be defined on a periodic domain and can be readily constructed from stationary kernels on r. given a stationary kernel m z kx ml is periodic with period l as shown in section and sch olkopf and smola eq. anisotropy factor analysis distance periodization if dot product covariance functions x can as we have already mentioned above the kernel kx we call this the homogeneous be obtained from linear regression. linear kernel otherwise it is inhomogeneous. of course this can be generalized x by using a general covariance matrix p on the to kx components of x as described in eq. it is also the case that kx x is a valid covariance function for positive integer p because of the general result that a positive-integer power of a given covariance function is also a valid covariance function as described in section however it is also interesting to show an explicit feature space construction for the polynomial covariance function. we consider the homogeneous polynomial case as the inhomogeneous case can simply be obtained by considering x to be extended the bias term could also be included in the general expression. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions by concatenating a constant. we write dx dx dx dp d dp kx dx dx d appears in the monomial under the constraint that pd notice that this sum apparently contains dp terms but in fact it is less than this as the order of the indices in the monomial xdp is unimportant e.g. for p and are the same monomial. we can remove the redundancy by defining a vector m whose entry md specifies the number of times index mi p. thus mx the feature corresponding to vector m is proportional to the monomial as usual we define giving the feature map d the degeneracy of mx is xmd p! r mx p! md! xmd d dotfor example for p in d we have product kernels are sometimes used in a normalized form given by eq. for regression problems the polynomial kernel is a rather strange choice as the prior variance grows rapidly with for however such kernels have proved effective in high-dimensional classification problems take x to be a vectorized binary image where the input data are binary or greyscale normalized to on each dimension olkopf and smola sec. other non-stationary covariance functions above we have seen examples of non-stationary dot product kernels. however there are also other interesting kernels which are not of this form. in this section we first describe the covariance function belonging to a particular type of neural network this construction is due to neal consider a network which takes an input x has one hidden layer with nh units and then linearly combines the outputs of the hidden units with a bias b to obtain fx. the mapping can be written fx b vjhx uj where the vjs are the hidden-to-output weights and hx u is the hidden unit transfer function we shall assume is bounded which depends on the input-to-hidden weights u. for example we could choose hx u tanhx u. this architecture is important because it has been shown by hornik that networks with one hidden layer are universal approximators as the number of nhx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn examples of covariance functions hidden units tends to infinity for a wide class of transfer functions excluding polynomials. let b and the v s have independent zero-mean distributions v respectively and let the weights uj for each hidden unit of variance be independently and identically distributed. denoting all weights by w we obtain neal b and ewfx b euhx uj v j b nh v euhx u where eq. follows because all of the hidden units are identically distributed. the final term in equation becomes u by letting v scale as the sum in eq. is over nh identically and independently distributed random variables. as the transfer function is bounded all moments of the distribution will be bounded and hence the central limit theorem can be applied showing that the stochastic process will converge to a gaussian process in the limit as nh r z by evaluating euhx u we can obtain the covariance function of the neural network. for example if we choose the error function hz erfz and choose u n then we obtain x dt as the transfer function let hx u e x sin knnx where x xd is an augmented input vector. this is a true neural network covariance function. the sigmoid kernel kx tanha bx has sometimes been proposed but in fact this kernel is never positive definite and is thus not a valid covariance function see e.g. sch olkopf and smola p. figure shows a plot of the neural network covariance function and samples from the prior. we have set diag samples from a gp with this covariance function can be viewed as superpositions of the functions where controls the variance of thus the amount of offset of these functions from the origin and controls u and thus the scaling on the x-axis. in figure we observe that the sample functions with larger vary more quickly. notice that the samples display the non-stationarity of the covariance function in that for large values of or x they should tend to a constant value consistent with the construction as a superposition of sigmoid functions. another interesting construction is to set hx u exp g where g sets the scale of this gaussian basis function. with u n ui neural network covariance function modulated squared exponential c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions covariance sample functions figure panel a plot of the covariance function knnx for panel samples drawn from the neural network covariance function with and as shown in the legend. the samples were obtained using a discretization of the x-axis of equally-spaced points. we obtain kgx z xx g exp exp g uu u du exp m s s u and m u e exp u e g u g g m where g. this is u scaling in general a non-stationary covariance function but if appropriately we recover the squared exponential kgx exp u kgx comprises a squared exponential covariance function modulated by the gaussian decay envelope function exp m cf. the vertical rescaling construction described in section g. for a finite value of m exp one way to introduce non-stationarity is to introduce an arbitrary non-linear mapping warping ux of the input x and then use a stationary covariance function in u-space. note that x and u need not have the same dimensionality as each other. this approach was used by sampson and guttorp to model patterns of solar radiation in southwestern british columbia using gaussian processes. another interesting example of this warping construction is given in mackay where the one-dimensional input variable x is mapped to the two-dimensional ux sinx to give rise to a periodic random function of x. if we use the squared exponential kernel in u-space then x kx exp as x warping periodic random function xinput x xoutput fx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn examples of covariance functions figure panel shows the chosen length-scale function panel shows three samples from the gp prior using gibbs covariance function eq. this figure is based on fig. in gibbs we have described above how to make an anisotropic covariance function by scaling different dimensions differently. however we are not free to make these length-scales d be functions of x as this will not in general produce a valid covariance function. gibbs derived the covariance function varying length-scale kx dy dx dx dx exp dx where each ix is an arbitrary positive function of x. note that kx x for all x. this covariance function is obtained by considering a grid of n gaussian basis functions with centres cj and a corresponding length-scale on input dimension d which varies as a positive function dcj. taking the limit as n the sum turns into an integral and after some algebra eq. is obtained. an example of a variable length-scale function and samples from the prior corresponding to eq. are shown in figure notice that as the lengthscale gets shorter the sample functions vary more rapidly as one would expect. the large length-scale regions on either side of the short length-scale region can be quite strongly correlated. if one tries the converse experiment by creating a length-scale function which has a longer length-scale region between two shorter ones then the behaviour may not be quite what is expected on initially transitioning into the long length-scale region the covariance drops off quite sharply due to the prefactor in eq. before stabilizing to a slower variation. see gibbs sec. for further details. exercises and invite you to investigate this further. paciorek and schervish have generalized gibbs construction to obtain non-stationary versions of arbitrary isotropic covariance functions. let ks be a lxinput xoutput fx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions covariance function constant linear polynomial squared exponential mat ern exponential rational quadratic neural network d expression pd exp r sin exp r r k r x x s nd table summary of several commonly-used covariance functions. the covariances are written either as a function of x and or as a function of r two columns marked s and nd indicate whether the covariance functions are stationary and nondegenerate respectively. degenerate covariance functions have finite rank see section for more discussion of this issue. stationary isotropic covariance function that is valid in every euclidean space rd for d let be a d d matrix-valued function which is positive definite for all x and let i set of gibbs ix functions define a diagonal then define the quadratic form qij xj i xj. paciorek and schervish show that knsxi xj i j wiener process is a valid non-stationary covariance function. in chapter we described the linear regression model in feature space fx o hagan suggested making w a function of x to allow for different values of w to be appropriate in different regions. thus he put a gaussian process prior on w of the form covwx for some positive definite matrix giving rise to a prior on fx with covariance kf finally we note that the wiener process with covariance function kx minx is a fundamental non-stationary process. see section and texts such as grimmett and stirzaker ch. for further details. making new kernels from old in the previous sections we have developed many covariance functions some of which are summarized in table in this section we show how to combine or modify existing covariance functions to make new ones. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn sum product vertical rescaling convolution direct sum tensor product additive model functional anova examples of covariance functions the sum of two kernels is a kernel. proof consider the random process fx where and are independent. then kx this construction can be used e.g. to add together kernels with different characteristic length-scales. the product of two kernels is a kernel. proof consider the random process fx where and are independent. then kx a simple extension of this argument means that kpx is a valid covariance function for p n. let ax be a given deterministic function and consider gx axfx where fx is a random process. then covgx axkx such a construction can be used to normalize kernels by choosing ax k x kx x x so that kx pkx kx this ensures that kx x for all x. we can also obtain a new process by convolution blurring. consider an arbitrary fixed kernel hx z and the map gx r hx zfz dz. then clearly covgx hx zkz dz if shirani has the form fx c fx c form kx are covariance functions over different spaces and and then the direct sum kx and the tensor product kx are also covariance functions on the product space by virtue of the sum and product constructions. the direct sum construction can be further generalized. consider a function fx where x is d-dimensional. an additive model and tibi.e. a linear combination of functions of one variable. if the individual fi s are taken to be independent stochastic processes then the covariance function of f will have the form of a direct sum. if we now admit interactions of two variables so that ijji fijxi xj and the various fi s and fij s are independent stochastic processes then the covariance function will have the j. indeed this process can be extended further to provide a functional decomposition ranging from a simple additive model up to full interaction of all d input variables. sum can also be truncated at some stage. wahba ch. and stitson et al. suggest using tensor products for kernels with interactions so that in the example above kijxi xj j would have the form kixi j. note that if d is large then the large number of pairwise higher-order terms may be problematic plate has investigated using a combination of additive gp models plus a general covariance function that permits full interactions. pi kijxi xj i ikjxj i i and are gaussian processes then the product f will not in general be a gaussian process but there exists a gp with this covariance function. stands for analysis of variance a statistical technique that analyzes the interac tions between various attributes. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions eigenfunction analysis of kernels we first define eigenvalues and eigenfunctions and discuss mercer s theorem which allows us to express the kernel certain conditions in terms of these quantities. section gives the analytical solution of the eigenproblem for the se kernel under a gaussian measure. section discusses how to compute approximate eigenfunctions numerically for cases where the exact solution is not known. it turns out that gaussian process regression can be viewed as bayesian linear regression with a possibly infinite number of basis functions as discussed in chapter one possible basis set is the eigenfunctions of the covariance function. a function that obeys the integral equation kx d z eigenvalue eigenfunction mercer s theorem is called an eigenfunction of kernel k with eigenvalue with respect to the two measures of particular interest to us will be lebesgue measure over a compact subset c of rd or when there is a density px so that d can be written pxdx. in general there are an infinite number of eigenfunctions which we label we assume the ordering is chosen such that the eigenfunctions are orthogonal with respect to and can be chosen to be normalized so thatr ix jx d ij where ij is the kronecker delta. mercer s theorem e.g. k onig allows us to express the kernel k in terms of the eigenvalues and eigenfunctions. theorem s theorem. let be a finite measure space and k l be a kernel such that tk is positive definite eq. let i be the normalized eigenfunctions of tk associated with the eigenvalues i then the eigenvalues i are absolutely summable x kx i ix i holds almost everywhere where the series converges absolutely and uniformly almost everywhere. this decomposition is just the infinite-dimensional analogue of the diagonalization of a hermitian matrix. note that the sum may terminate at some value n n the eigenvalues beyond n are zero or the sum may be infinite. we have the following definition et al. p. further explanation of measure see appendix c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn eigenfunction analysis of kernels definition a degenerate kernel has only a finite number of non-zero values. a degenerate kernel is also said to have finite rank. if a kernel is not degenerate it is said to be nondegenerate. as an example a n-dimensional linear regression model in feature space eq. gives rise to a degenerate kernel with at most n non-zero eigenvalues. course if the measure only puts weight on a finite number of points n in x-space then the eigendecomposition is simply that of a n n matrix even if the kernel is nondegenerate. degenerate nondegenerate kernel the statement of mercer s theorem above referred to a finite measure if we replace this with lebesgue measure and consider a stationary covariance function then directly from bochner s theorem eq. we obtain kx is d d z rd is is z rd the complex exponentials is x are the eigenfunctions of a stationary kernel w.r.t. lebesgue measure. note the similarity to eq. except that the summation has been replaced by an integral. the rate of decay of the eigenvalues gives important information about the smoothness of the kernel. for example ritter et al. showed that in with uniform on processes which are r-times mean-square differentiable have i i asymptotically. this makes sense as rougher processes have more power at high frequencies and so their eigenvalue spectrum decays more slowly. the same phenomenon can be read off from the power spectrum of the mat ern class as given in eq. hawkins gives the exact eigenvalue spectrum for the ou process on widom gives an asymptotic analysis of the eigenvalues of stationary kernels taking into account the effect of the density d pxdx bach and jordan table use these results to show the effect of varying px for the se kernel. an exact eigenanalysis of the se kernel under the gaussian density is given in the next section. an analytic example for the case that px is a gaussian and for the squared-exponential kernel kx exp there are analytic results for the eigenvalues and eigenfunctions as given by zhu et al. sec. putting px n we find that the eigenvalues k and eigenfunctions k convenience let k are given by a k kx bk c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions figure the first eigenfunctions of the squared exponential kernel w.r.t. a gaussian density. the value of k is equal to the number of zero-crossings of the function. the dashed line is proportional to the density px. where hkx dk gradshteyn and ryzhik sec. a b and dxk exp is the kth order hermite polynomial a a b c b ba. hints on the proof of this result are given in exercise a plot of the first three eigenfunctions for a and b is shown in figure the result for the eigenvalues and eigenfunctions is readily generalized to the multivariate case when the kernel and gaussian density are products of the univariate expressions as the eigenfunctions and eigenvalues will simply be products too. for the case that a and b are equal on all d dimensions the degeneracy of the eigenvalue which is okd as th eigenvalue has a value given by we see that a d pk d a and this can be used to determine the rate of decay of the spectrum. d d numerical approximation of eigenfunctions the standard numerical method for approximating the eigenfunctions and eigenvalues of eq. is to use a numerical routine to approximate the integral e.g. baker ch. for example letting d pxdx in eq. one could use the approximation c z i kx ix dx n kxl ixl nx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn kernels for non-vectorial inputs where the xl s are sampled from px. plugging in xl for l n into eq. we obtain the matrix eigenproblem kui mat i ui i nuij where the where k is the n n gram matrix with entries kij kxi xj mat is the ith i ui we have ixj matrix eigenvalue and ui is the corresponding eigenvector so that u n factor arises from the differing normalizations of the eigenvector and eigenfunction. thus is an obvious estimator for i for i n. for fixed n one would expect that the larger eigenvalues would be better estimated than the smaller ones. the theory of the numerical solution of eigenvalue problems shows that for a fixed i will converge to i in the limit that n theorem n mat it is also possible to study the convergence further for example it is quite easy using the properties of principal components analysis in feature space to show that for any l l n en i and en i where en denotes expectation with respect to samples of size n drawn from px. for further details see shawe-taylor and williams pl pn pn mat i pl n mat i mat i n i n nystr om method kernel pca the nystr om method for approximating the ith eigenfunction baker and press et al. section is given by n mat i where kxn which is obtained from eq. by dividing both sides by i. equation extends the approximation ixj nuij from the sample points xn to all x. there is an interesting relationship between the kernel pca method of sch olkopf et al. and the eigenfunction expansion discussed above. the eigenfunction expansion has least potentially an infinite number of nonzero eigenvalues. in contrast the kernel pca algorithm operates on the n n matrix k and yields n eigenvalues and eigenvectors. eq. clarifies the relationship between the two. however note that eq. is identical to scaling factors to sch olkopf et al. eq. which describes the projection of a new point onto the ith eigenvector in the kernel pca feature space. kernels for non-vectorial inputs so far in this chapter we have assumed that the input x is a vector measuring the values of a number of attributes features. however for some learning problems the inputs are not vectors but structured objects such as strings trees or general graphs. for example we may have a biological problem where we want to classify proteins as strings of amino acid are initially made up of different amino acids of which a few may later be modified bringing the total number up to or c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions or our input may be parse-trees derived from a linguistic analysis. or we may wish to represent chemical compounds as labelled graphs with vertices denoting atoms and edges denoting bonds. to follow the discriminative approach we need to extract some features from the input objects and build a predictor using these features. a classification problem the alternative generative approach would construct class-conditional models over the objects themselves. below we describe two approaches to this feature extraction problem and the efficient computation of kernels from them in section we cover string kernels and in section we describe fisher kernels. there exist other proposals for constructing kernels for strings for example watkins describes the use of pair hidden markov models that generate output symbols for two strings conditional on the hidden state for this purpose. string kernels we start by defining some notation for strings. let a be a finite alphabet of characters. the concatenation of strings x and y is written xy and denotes the length of string x. the string s is a substring of x if we can write x usv for some empty u s and v. let sx denote the number of times that substring s appears in string x. then we define the kernel between two strings x and as kx x s a ws sx where ws is a non-negative weight for substring s. for example we could set ws where so that shorter substrings get more weight than longer ones. a number of interesting special cases are contained in the definition setting ws for gives the bag-of-characters kernel. this takes the feature vector for a string x to be the number of times that each character in a appears in x. in text analysis we may wish to consider the frequencies of word occurrence. if we require s to be bordered by whitespace then a bag-of-words representation is obtained. although this is a very simple model of text ignores word order it can be surprisingly effective for document classification and retrieval tasks see e.g. hand et al. sec. the weights can be set differently for different words e.g. using the term frequency inverse document frequency weighting scheme developed in the information retrieval area and buckley if we only consider substrings of length k then we obtain the k-spectrum kernel et al. bag-of-characters bag-of-words k-spectrum kernel c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn kernels for non-vectorial inputs importantly there are efficient methods using suffix trees that can compute a string kernel kx in time linear in some restrictions on the weights et al. vishwanathan and smola work on string kernels was started by watkins and haussler there are many further developments of the methods we have described above for example lodhi et al. go beyond substrings to consider subsequences of x which are not necessarily contiguous and leslie et al. describe mismatch string kernels which allow substrings s and of x and respectively to match if there are at most m mismatches between them. we expect further developments in this area tailoring engineering the string kernels to have properties that make sense in a particular domain. the idea of string kernels where we consider matches of substrings can easily be extended to trees e.g. by looking at matches of subtrees and duffy leslie et al. have applied string kernels to the classification of protein domains into superfamilies. the results obtained were significantly better than methods based on either searches or a generative hidden markov model classifier. similar results were obtained by jaakkola et al. using a fisher kernel in the next section. saunders et al. have also described the use of string kernels on the problem of classifying natural language newswire stories from the database into ten classes. fisher kernels score vector as explained above our problem is that the input x is a structured object of arbitrary size e.g. a string and we wish to extract features from it. the fisher kernel by jaakkola et al. does this by taking a generative model px where is a vector of parameters and computing the feature vector log px is sometimes called the score vector. in string x. then a markov model gives px take for example a markov model for strings. let xk be the kth symbol a where a. here gives the probability that will be the jth symbol in the alphabet a and a is a stochastic matrix with ajk giving the probability that kxi j. given such a model it is straightforward to compute the score vector for a given x. it is also possible to consider other generative models px for example we might try a kth-order markov model where xi is predicted by the preceding k symbols. see leslie et al. and saunders et al. for an interesting discussion of the similarities of the features used in the k-spectrum kernel and the score vector derived from an order k markov model see also exercise classification of proteins database httpscop.mrc-lmb.cam.ac.ukscop. iterative basic local alignment search tool see c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions fisher information matrix fisher kernel top kernel another interesting choice is to use a hidden markov model as the generative model as discussed by jaakkola et al. see also exercise for a linear kernel derived from an isotropic gaussian model for x rd. we define a kernel kx based on the score vectors for x and one simple choice is to set kx where m is a strictly positive definite matrix. alternatively we might use the squared exponential kernel kx exp for some the structure of px as varies has been studied extensively in information geometry e.g. amari it can be shown that the manifold of log px is riemannian with a metric tensor which is the inverse of the fisher information matrix f where f ex setting m f in eq. gives the fisher kernel if f is difficult to compute then one might resort to setting m i. the advantage of using the fisher information matrix is that it makes arc length on the manifold invariant to reparameterizations of the fisher kernel uses a class-independent model px tsuda et al. have developed the tangent of posterior odds kernel based on py log py which makes use of class-conditional distributions for the c and c classes. exercises the ou process with covariance function kx exp is the unique stationary first-order markovian gaussian process appendix b for further details. consider training inputs xn xn on r with corresponding function values f fxn. let xl denote the nearest training input to the left of a test point x and similarly let xu denote the nearest training input to the right of x then the markovian property means that pfx pfx fxu. demonstrate this by choosing some x-points on the line and computing the predictive distribution pfx using eq. and observing that non-zero contributions only arise from xl and xu. note that this only occurs in the noise-free case if one allows the training points to be corrupted by noise and then all points will contribute in general. computer exercise write code to draw samples from the neural network covariance function eq. in and consider the cases when is either or non-zero. explain the form of the plots obtained when c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn exercises consider the random process fx where u n show that this non-linear transform of a process with an inhomogeneous linear covariance function has the same covariance function as the erf neural network. however note that this process is not a gaussian process. draw samples from the given process and compare them to your results from exercise derive gibbs non-stationary covariance function eq. computer exercise write code to draw samples from gibbs non-stationary covariance function eq. in and investigate various forms of length-scale function show that the se process is infinitely ms differentiable and that the ou process is not ms differentiable. prove that the eigenfunctions of a symmetric kernel are orthogonal w.r.t. the measure let kx and assume px for all x. has the same show that the eigenproblem r kx ixdx i eigenvalues asr kx ixdx i and that the eigenfunc tions are related by ix ix. also give the matrix version of this problem introduce a diagonal matrix p to take the r ole of px. the significance of this connection is that it can be easier to find eigenvalues of symmetric matrices than general matrices. apply the construction in the previous exercise to the eigenproblem for the se kernel and gaussian density given in section with px exp exp bx exp using equation in gradshteyn and ryzhik exp thus consider the modified kernel given by kx z x dx y verify that kx exp and and thus confirm equations computer exercise the analytic form of the eigenvalues and eigenfunctions for the se kernel and gaussian density are given in section compare these exact results to those obtained by the nystr om approximation for various values of n and choice of samples. let x n consider the fisher kernel derived from this model with respect to variation of regard as a constant. show that log px x and that f thus the fisher kernel for this model with is the linear kernel kx x c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn covariance functions consider a k order markov model for strings on a finite alphabet. let this model have parameters denoting the probability pxi txi xk sk of course as these are probabilities they enforcing this constraint obey the constraint that p can be achieved automatically by setting p where the parameters are now independent as suggested in et al. the current parameter values are denoted let the current values of i.e. that show that log px log where is be set so that p the number of instances of the substring sk in x. thus following leslie et al. show that log px where is the number of instances of the substring sk in x. as is the expected number of occurrences of the string sk given the count the fisher score captures the degree to which this string is over- or under-represented relative to the model. for the k-spectrum kernel the relevant feature is sk c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter model selection and adaptation of hyperparameters in chapters and we have seen how to do regression and classification using a gaussian process with a given fixed covariance function. however in many practical applications it may not be easy to specify all aspects of the covariance function with confidence. while some properties such as stationarity of the covariance function may be easy to determine from the context we typically have only rather vague information about other properties such as the value of free parameters e.g. length-scales. in chapter several examples of covariance functions were presented many of which have large numbers of parameters. in addition the exact form and possible free parameters of the likelihood function may also not be known in advance. thus in order to turn gaussian processes into powerful practical tools it is essential to develop methods that address the model selection problem. we interpret the model selection problem rather broadly to include all aspects of the model including the discrete choice of the functional form for the covariance function as well as values for any hyperparameters. in section we outline the model selection problem. in the following sections different methodologies are presented in section bayesian principles are covered and in section cross-validation is discussed in particular the leave-one-out estimator. in the remaining two sections the different methodologies are applied specifically to learning in gp models for regression in section and classification in section model selection c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters the model selection problem in order for a model to be a practical tool in an application one needs to make decisions about the details of its specification. some properties may be easy to specify while we typically have only vague information available about other aspects. we use the term model selection to cover both discrete choices and the setting of continuous parameters of the covariance functions. in fact model selection can help both to refine the predictions of the model and give a valuable interpretation to the user about the properties of the data e.g. that a non-stationary covariance function may be preferred over a stationary one. a multitude of possible families of covariance functions exists including squared exponential polynomial neural network etc. see section for an overview. each of these families typically have a number of free hyperparameters whose values also need to be determined. choosing a covariance function for a particular application thus comprises both setting of hyperparameters within a family and comparing across different families. both of these problems will be treated by the same methods so there is no need to distinguish between them and we will use the term model selection to cover both meanings. we will refer to the selection of a covariance function and its parameters as training of a gaussian in the following paragraphs we give example choices of parameterizations of distance measures for stationary covariance functions. enable interpretation hyperparameters training covariance functions such as the squared exponential can be parameterized in terms of hyperparameters. for example kxp xq where n is a vector containing all the and denotes the parameters in the symmetric matrix m. possible choices for the matrix m include f n pq f xqmxp characteristic length-scale automatic relevance determination diag diag where is a vector of positive values and is a d k matrix k d. the properties of functions with these covariance functions depend on the values of the hyperparameters. for many covariance functions it is easy to interpret the meaning of the hyperparameters which is of great importance when trying to understand your data. for the squared exponential covariance function eq. with distance measure from eq. the d hyperparameters play the r ole of characteristic length-scales loosely speaking how far do you need to move a particular axis in input space for the function values to become uncorrelated. such a covariance function implements automatic relevance determination since the inverse of the length-scale determines how relevant an input is if the length-scale has a very large value the contrasts the use of the word in the svm literature where training usually refers to finding the support vectors for a fixed kernel. the noise level parameter n is not considered a hyperparameter however it plays an analogous role and is treated in the same way so we simply consider it a hyperparameter. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn the model selection problem figure functions with two dimensional input drawn at random from noise free squared exponential covariance function gaussian processes corresponding to the three different distance measures in eq. respectively. the parameters were and in panel the two inputs are equally important while in the function varies less rapidly as a function of than in the column gives the direction of most rapid variation covariance will become almost independent of that input effectively removing it from the inference. ard has been used successfully for removing irrelevant input by several authors e.g. williams and rasmussen we call the parameterization of in eq. the factor analysis distance due to the analogy with the factor analysis model which seeks to explain the data through a low rank plus diagonal decomposition. for high dimensional datasets the k columns of the matrix could identify a few directions in the input space with specially high relevance and their lengths give the inverse characteristic length-scale for those directions. in figure we show functions drawn at random from squared exponential covariance function gaussian processes for different choices of m. in panel we get an isotropic behaviour. in panel the characteristic length-scale is different along the two input axes the function varies rapidly as a function of but less rapidly as a function of in panel the direction of most rapid variation is perpendicular to the direction as this figure illustrates factor analysis distance y y y c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters there is plenty of scope for variation even inside a single family of covariance functions. our task is based on a set of training data to make inferences about the form and parameters of the covariance function or equivalently about the relationships in the data. it should be clear from the above example that model selection is essentially open ended. even for the squared exponential covariance function there is a huge variety of possible distance measures. however this should not be a cause for despair rather seen as a possibility to learn. it requires however a systematic and practical approach to model selection. in a nutshell we need to be able to compare two more methods differing in values of particular parameters or the shape of the covariance function or compare a gaussian process model to any other kind of model. although there are endless variations in the suggestions for model selection in the literature three general principles cover most compute the probability of the model given the data estimate the generalization error and bound the generalization error. we use the term generalization error to mean the average error on unseen test examples the same distribution as the training cases. note that the training error is usually a poor proxy for the generalization error since the model may fit the noise in the training set leading to low training error but poor generalization performance. in the next section we describe the bayesian view on model selection which involves the computation of the probability of the model given the data based on the marginal likelihood. in section we cover cross-validation which estimates the generalization performance. these two paradigms are applied to gaussian process models in the remainder of this chapter. the probably approximately correct framework is an example of a bound on the generalization error and is covered in section bayesian model selection in this section we give a short outline description of the main ideas in bayesian model selection. the discussion will be general but focusses on issues which will be relevant for the specific treatment of gaussian process models for regression in section and classification in section it is common to use a hierarchical specification of models. at the lowest level are the parameters w. for example the parameters could be the parameters in a linear model or the weights in a neural network model. at the second level are hyperparameters which control the distribution of the parameters at the bottom level. for example the weight decay term in a neural network or the ridge term in ridge regression are hyperparameters. at the top level we may have a set of possible model structures hi under consideration. we will first give a mechanistic description of the computations needed for bayesian inference and continue with a discussion providing the intuition about what is going on. inference takes place one level at a time by applying hierarchical models c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bayesian model selection the rules of probability theory see e.g. mackay for this framework and mackay for the context of neural networks. at the bottom level the posterior over the parameters is given by bayes rule pwy x pyx whipw pyx level inference where pyx whi is the likelihood and pw is the parameter prior. the prior encodes as a probability distribution our knowledge about the parameters prior to seeing the data. if we have only vague prior information about the parameters then the prior distribution is chosen to be broad to reflect this. the posterior combines the information from the prior and the data the likelihood. the normalizing constant in the denominator of eq. pyx is independent of the parameters and called the marginal likelihood evidence and is given by pyx pyx whipw dw. z z at the next level we analogously express the posterior over the hyperparameters where the marginal likelihood from the first level plays the r ole of the likelihood p xhi pyx where p is the hyper-prior prior for the hyperparameters. the normalizing constant is given by pyxhi pyxhi pyx level inference at the top level we compute the posterior for the model level inference pyx where pyx p phiy x pyxhiphi i pyxhiphi. we note that the implementation of bayesian inference calls for the evaluation of several integrals. depending on the details of the models these integrals may or may not be analytically tractable and in general one may have to resort to analytical approximations or markov chain monte carlo methods. in practice especially the evaluation of the integral in eq. may be difficult and as an approximation one may shy away from using the hyperparameter posterior in eq. and instead maximize the marginal likelihood in eq. w.r.t. the hyperparameters this approximation is known as type ii maximum likelihood of course one should be careful with such an optimization step since it opens up the possibility of overfitting especially if there are many hyperparameters. the integral in eq. can then be approximated using a local expansion around the maximum laplace approximation. this approximation will be good if the posterior for is fairly well peaked which is more often the case for the ml-ii c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters figure the marginal likelihood pyxhi is the probability of the data given the model. the number of data points n and the inputs x are fixed and not shown. the horizontal axis is an idealized representation of all possible vectors of targets y. the marginal likelihood for models of three different complexities are shown. note that since the marginal likelihood is a probability distribution it must normalize to unity. for a particular dataset indicated by y and a dotted line the marginal likelihood prefers a model of intermediate complexity over too simple or too complex alternatives. hyperparameters than for the parameters themselves see mackay for an illuminating discussion. the prior over models hi in eq. is often taken to be flat so that a priori we do not favour one model over another. in this case the probability for the model is proportional to the expression from eq. it is primarily the marginal likelihood from eq. involving the integral over the parameter space which distinguishes the bayesian scheme of inference from other schemes based on optimization. it is a property of the marginal likelihood that it automatically incorporates a trade-off between model fit and model complexity. this is the reason why the marginal likelihood is valuable in solving the model selection problem. in figure we show a schematic of the behaviour of the marginal likelihood for three different model complexities. let the number of data points n and the inputs x be fixed the horizontal axis is an idealized representation of all possible vectors of targets y and the vertical axis plots the marginal likelihood pyxhi. a simple model can only account for a limited range of possible sets of target values but since the marginal likelihood is a probability distribution over y it must normalize to unity and therefore the data sets which the model does account for have a large value of the marginal likelihood. conversely for a complex model it is capable of accounting for a wider range of data sets and consequently the marginal likelihood doesn t attain such large values as for the simple model. for example the simple model could be a linear model and the complex model a large neural network. the figure illustrates why the marginal likelihood doesn t simply favour the models that fit the training data the best. this effect is called occam s razor after william of occam whose principle plurality should not be assumed without necessity he used to encourage simplicity in explanations. see also rasmussen and ghahramani for an investigation into occam s razor in statistical models. occam s razor ymarginal likelihood pyxhiall possible data setssimpleintermediatecomplex c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn cross-validation notice that the trade-off between data-fit and model complexity is automatic there is no need to set a parameter externally to fix the trade-off. do not confuse the automatic occam s razor principle with the use of priors in the bayesian method. even if the priors are flat over complexity the marginal likelihood will still tend to favour the least complex model able to explain the data. thus a model complexity which is well suited to the data can be selected using the marginal likelihood. in the preceding paragraphs we have thought of the specification of a model as the model structure as well as the parameters of the priors etc. if it is unclear how to set some of the parameters of the prior one can treat these as hyperparameters and do model selection to determine how to set them. at the same time it should be emphasized that the priors correspond to assumptions about the data. if the priors are grossly at odds with the distribution of the data inference will still take place under the assumptions encoded by the prior see the step-function example in section to avoid this situation one should be careful not to employ priors which are too narrow ruling out reasonable explanations of the cross-validation in this section we consider how to use methods of cross-validation for model selection. the basic idea is to split the training set into two disjoint sets one which is actually used for training and the other the validation set which is used to monitor performance. the performance on the validation set is used as a proxy for the generalization error and model selection is carried out using this measure. in practice a drawback of hold-out method is that only a fraction of the full data set can be used for training and that if the validation set it small the performance estimate obtained may have large variance. to minimize these problems cv is almost always used in the k-fold cross-validation setting the data is split into k disjoint equally sized subsets validation is done on a single subset and training is done using the union of the remaining k subsets the entire procedure being repeated k times each time with a different subset for validation. thus a large fraction of the data can be used for training and all cases appear as validation cases. the price is that k models must be trained instead of one. typical values for k are in the range to an extreme case of k-fold cross-validation is obtained for k n the number of training cases also known as leave-one-out cross-validation often the computational cost of loo-cv training n models is prohibitive but in certain cases such as gaussian process regression there are computational shortcuts. is known as cromwell s dictum after oliver cromwell who on august wrote to the synod of the church of scotland i beseech you in the bowels of christ consider it possible that you are mistaken. automatic trade-off cross-validation k-fold cross-validation leave-one-out cross-validation c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters other loss functions model parameters cross-validation can be used with any loss function. although the squared error loss is by far the most common for regression there is no reason not to allow other loss functions. for probabilistic models such as gaussian processes it is natural to consider also cross-validation using the negative log probability loss. craven and wahba describe a variant of cross-validation using squared error known as generalized cross-validation which gives different weightings to different datapoints so as to achieve certain invariance properites. see wahba sec. for further details. model selection for gp regression we apply bayesian inference in section and cross-validation in section to gaussian process regression with gaussian noise. we conclude in section with some more detailed examples of how one can use the model selection principles to tailor covariance functions. marginal likelihood bayesian principles provide a persuasive and consistent framework for inference. unfortunately for most interesting models for machine learning the required computations over parameter space are analytically intractable and good approximations are not easily derived. gaussian process regression models with gaussian noise are a rare exception integrals over the parameters are analytically tractable and at the same time the models are very flexible. in this section we first apply the general bayesian inference principles from section to the specific gaussian process model in the simplified form where hyperparameters are optimized over. we derive the expressions for the marginal likelihood and interpret these. since a gaussian process model is a non-parametric model it may not be immediately obvious what the parameters of the model are. generally one may regard the noise-free latent function values at the training inputs f as the parameters. the more training cases there are the more parameters. using the weight-space view developed in section one may equivalently think of the parameters as being the weights of the linear model which uses the basis-functions which can be chosen as the eigenfunctions of the covariance function. of course we have seen that this view is inconvenient for nondegenerate covariance functions since these would then have an infinite number of weights. we proceed by applying eq. and eq. for the level of inference which we find that we have already done back in chapter the predictive distribution from eq. is given for the weight-space view in eq. and eq. and equivalently for the function-space view in eq. the marginal likelihood evidence from eq. was computed in eq. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection for gp regression figure panel shows a decomposition of the log marginal likelihood into its constituents data-fit and complexity penalty as a function of the characteristic length-scale. the training data is drawn from a gaussian process with se covariance function and parameters f n the same as in figure and we are fitting only the length-scale parameter two other parameters have been set in accordance with the generating process. panel shows the log marginal likelihood as a function of the characteristic length-scale for different sizes of training sets. also shown are the confidence intervals for the posterior length-scales. and we re-state the result here log pyx yk y y log n log where ky kf ni is the covariance matrix for the noisy targets y kf is the covariance matrix for the noise-free latent f and we now explicitly write the marginal likelihood conditioned on the hyperparameters parameters of the covariance function from this perspective it becomes clear why we call eq. the log marginal likelihood since it is obtained through marginalization over the latent function. otherwise if one thinks entirely in terms of the function-space view the term marginal may appear a bit mysterious and similarly the hyper from the parameters of the covariance the three terms of the marginal likelihood in eq. have readily interpretable r oles the only term involving the observed targets is the data-fit yk y log is the complexity penalty depending only on the covariance function and the inputs and n is a normalization constant. in figure we illustrate this breakdown of the log marginal likelihood. the data-fit decreases monotonically with the length-scale since the model becomes less and less flexible. the negative complexity penalty increases with the length-scale because the model gets less complex with growing length-scale. the marginal likelihood itself peaks at a value close to for length-scales somewhat longer than the marginal likelihood decreases rapidly the reason that we like to stick to the term marginal likelihood is that it is the likelihood of a non-parametric model i.e. a model which requires access to all the training data when making predictions this contrasts the situation for a parametric model which absorbs the information from the training data into its parameter this difference makes the two likelihoods behave quite differently as a function of marginal likelihood interpretation probabilitycharacteristic lengthscaleminus complexity penaltydata fitmarginal lengthscalelog marginal conf int c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters figure contour plot showing the log marginal likelihood as a function of the characteristic length-scale and the noise level for the same data as in figure and figure the signal variance hyperparameter was set to f the optimum is close to the parameters used when generating the data. note the two ridges one for small noise and length-scale and another for long length-scale and noise n the contour lines spaced units apart in log probability density. log scale! due to the poor ability of the model to explain the data compare to figure for smaller length-scales the marginal likelihood decreases somewhat more slowly corresponding to models that do accommodate the data but waste predictive mass at regions far away from the underlying function compare to figure in figure the dependence of the log marginal likelihood on the characteristic length-scale is shown for different numbers of training cases. generally the more data the more peaked the marginal likelihood. for very small numbers of training data points the slope of the log marginal likelihood is very shallow as when only a little data has been observed both very short and intermediate values of the length-scale are consistent with the data. with more data the complexity term gets more severe and discourages too short length-scales. marginal likelihood gradient to set the hyperparameters by maximizing the marginal likelihood we seek the partial derivatives of the marginal likelihood w.r.t. the hyperparameters. using eq. and eq. we obtain k k k j k where k log pyx yk k j j j tr the complexity of computing the marginal likelihood in eq. is dominated by the need to invert the k matrix log determinant of k is easily computed as a by-product of the inverse. standard methods for matrix inversion of positive definite symmetric matrices require time for inversion of an n by n matrix. once k is known the computation of the derivatives in eq. requires only time per thus the computational that matrix-by-matrix products in eq. should not be computed directly in the first term do the vector-by-matrix multiplications first in the trace term compute only the diagonal terms of the product. lengthscalenoise standard deviation c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection for gp regression head of computing derivatives is small so using a gradient based optimizer is advantageous. estimation of by optimzation of the marginal likelihood has a long history in spatial statistics see e.g. mardia and marshall as n increases one would hope that the data becomes increasingly informative about however it is necessary to contrast what stein sec. calls fixed-domain asymptotics one gets increasingly dense observations within some region with increasing-domain asymptotics the size of the observation region grows with n. increasing-domain asymptotics are a natural choice in a time-series context but fixed-domain asymptotics seem more natural in spatial machine learning settings. for further discussion see stein sec. figure shows an example of the log marginal likelihood as a function of the characteristic length-scale and the noise standard deviation hyperparameters for the squared exponential covariance function see eq. the signal variance f was set to the marginal likelihood has a clear maximum around the hyperparameter values which were used in the gaussian process from which the data was generated. note that for long length-scales and a noise level of n the marginal likelihood becomes almost independent of the length-scale this is caused by the model explaining everything as noise and no longer needing the signal covariance. similarly for small noise and a length-scale of the marginal likelihood becomes almost independent of the noise level this is caused by the ability of the model to exactly interpolate the data at this short length-scale. we note that although the model in this hyperparameter region explains all the data-points exactly this model is still disfavoured by the marginal likelihood see figure there is no guarantee that the marginal likelihood does not suffer from multiple local optima. practical experience with simple covariance functions seem to indicate that local maxima are not a devastating problem but certainly they do exist. in fact every local maximum corresponds to a particular interpretation of the data. in figure an example with two local optima is shown together with the corresponding free predictions of the model at each of the two local optima. one optimum corresponds to a relatively complicated model with low noise whereas the other corresponds to a much simpler model with more noise. with only data points it is not possible for the model to confidently reject either of the two possibilities. the numerical value of the marginal likelihood for the more complex model is about higher than for the simple model. according to the bayesian formalism one ought to weight predictions from alternative explanations according to their posterior probabilities. in practice with data sets of much larger sizes one often finds that one local optimum is orders of magnitude more probable than other local optima so averaging together alternative explanations may not be necessary. however care should be taken that one doesn t end up in a bad local optimum. above we have described how to adapt the parameters of the covariance function given one dataset. however it may happen that we are given several datasets all of which are assumed to share the same hyperparameters this is known as multi-task learning see e.g. caruana in this case one can multiple local maxima multi-task learning c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters n standard deviation where figure panel shows the marginal likelihood as a function of the hyperparameters and f standard deviation for a data set of observations in panels and there are two local optima indicated with the global optimum has low noise and a short length-scale the local optimum has a high noise and a long length scale. in and the inferred underlying functions confidence intervals are shown for each of the two solutions. in fact the data points were generated by a gaussian process with f n in eq. simply sum the log marginal likelihoods of the individual problems and optimize this sum w.r.t. the hyperparameters and picard cross-validation the predictive log probability when leaving out training case i is log pyix y i log i i log negative log validation density loss where the notation y i means all targets except number i and i and i are computed according to eq. and respectively in which the training sets are taken to be i y i. accordingly the loo log predictive probability is lloox y log pyix y i nx lengthscalenoise standard deviation xoutput y xoutput y c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection for gp regression pseudo-likelihood see and eddy for a discussion of this and related approaches. lloo in eq. is sometimes called the log pseudo-likelihood. notice that in each of the n loo-cv rotations inference in the gaussian process model fixed hyperparameters essentially consists of computing the inverse covariance matrix to allow predictive mean and variance in eq. and to be evaluated there is no parameter-fitting such as there would be in a parametric model. the key insight is that when repeatedly applying the prediction eq. and the expressions are almost identical we need the inverses of covariance matrices with a single column and row removed in turn. this can be computed efficiently from the inverse of the complete covariance matrix using inversion by partitioning see eq. a similar insight has also been used for spline models see e.g. wahba sec. the approach was used for hyperparameter selection in gaussian process models in sundararajan and keerthi the expressions for the loo-cv predictive mean and variance are i yi and i where careful inspection reveals that the mean i is in fact independent of yi as indeed it should be. the computational expense of computing these quantities is once for computing the inverse of k plus for the entire loocv procedure k is known. thus the computational overhead for the loo-cv quantities is negligible. plugging these expressions into eq. and produces a performance estimator which we can optimize w.r.t. hyperparameters to do model selection. in particular we can compute the partial derivatives of lloo w.r.t. the hyperparameters eq. and use conjugate gradient optimization. to this end we need the partial derivatives of the loo-cv predictive mean and variances from eq. w.r.t. the hyperparameters izjk i j where k and zj k k obtained by using the chain-rule and eq. to give j ii i j ii the partial derivatives of eq. are lloo j nx nx log pyix y i i j i log pyix y i i j i i izj the computational complexity is for computing the inverse of k and per hyperparameter for the derivative eq. thus the computational burden of the derivatives is greater for the loo-cv method than for the method based on marginal likelihood eq. of the matrix-by-matrix product k k j for each hyperparameter is un avoidable. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters loo-cv with squared error loss in eq. we have used the log of the validation density as a crossvalidation measure of fit equivalently the negative log validation density as a loss function. one could also envisage using other loss functions such as the commonly used squared error. however this loss function is only a function of the predicted mean and ignores the validation set variances. further since the mean prediction eq. is independent of the scale of the covariances you can multiply the covariance of the signal and noise by an arbitrary positive constant without changing the mean predictions one degree of freedom is left by a loo-cv procedure based on squared error loss any other loss function which depends only on the mean predictions. but of course the full predictive distribution does depend on the scale of the covariance function. also computation of the derivatives based on the squared error loss has similar computational complexity as the negative log validation density loss. in conclusion it seems unattractive to use loo-cv based on squared error loss for hyperparameter selection. comparing the pseudo-likelihood for the loo-cv methodology with the marginal likelihood from the previous section it is interesting to ask under which circumstances each method might be preferable. their computational demands are roughly identical. this issue has not been studied much empirically. however it is interesting to note that the marginal likelihood tells us the probability of the observations given the assumptions of the model. this contrasts with the frequentist loo-cv value which gives an estimate for the predictive probability whether or not the assumptions of the model may be fulfilled. thus wahba sec. has argued that cv procedures should be more robust against model mis-specification. examples and discussion in the following we give three examples of model selection for regression models. we first describe a modelling task which illustrates how special covariance functions can be designed to achieve various useful effects and can be evaluated using the marginal likelihood. secondly we make a short reference to the model selection carried out for the robot arm problem discussed in chapter and again in chapter finally we discuss an example where we deliberately choose a covariance function that is not well-suited for the problem this is the so-called mis-specified model scenario. mauna loa atmospheric carbon dioxide we will use a modelling problem concerning the concentration of in the atmosphere to illustrate how the marginal likelihood can be used to set multiple hyperparameters in hierarchical gaussian process models. a complex covariance function is derived by combining several different kinds of simple covariance functions and the resulting model provides an excellent fit to the data as well the special case where we know either the signal or the noise variance there is no indeterminancy. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection for gp regression figure the observations of monthly averages of the atmospheric concentration of made between and the end of together with predictive confidence region for a gaussian process regression model years into the future. rising trend and seasonal variations are clearly visible. note also that the confidence interval gets wider the further the predictions are extrapolated. as insights into its properties by interpretation of the adapted hyperparameters. although the data is one-dimensional and therefore easy to visualize a total of hyperparameters are used which in practice rules out the use of cross-validation for setting parameters except for the gradient-based loo-cv procedure from the previous section. the data and whorf consists of monthly average atmospheric concentrations parts per million by volume derived from in situ air samples collected at the mauna loa observatory hawaii between and some missing the data is shown in figure our goal is the model the concentration as a function of time x. several features are immediately apparent a long term rising trend a pronounced seasonal variation and some smaller irregularities. in the following we suggest contributions to a combined covariance function which takes care of these individual properties. this is meant primarily to illustrate the power and flexibility of the gaussian process framework it is possible that other choices would be more appropriate for this data set. to model the long term smooth rising trend we use a squared exponential covariance term with two hyperparameters controlling the amplitude and characteristic length-scale smooth trend exp note that we just use a smooth trend actually enforcing the trend a priori to be increasing is probably not so simple and not desirable. we can use the periodic covariance function from eq. with a period of one year to model the seasonal variation. however it is not clear that the seasonal trend is data is available from seasonal component concentration ppm c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters figure panel long term trend dashed left hand scale predicted by the squared exponential contribution superimposed is the medium term trend full line right hand scale predicted by the rational quadratic contribution the vertical dashdotted line indicates the upper limit of the training data. panel shows the seasonal variation over the year for three different years. the concentration peaks in mid may and has a low in the beginning of october. the seasonal variation is smooth but not of exactly sinusoidal shape. the peak-to-peak amplitude increases from about ppm in to about ppm in but the shape does not change very much. the characteristic decay length of the periodic component is inferred to be years so the seasonal trend changes rather slowly as also suggested by the gradual progression between the three years shown. exactly periodic so we modify eq. by taking the product with a squared exponential component the product construction from section to allow a decay away from exact periodicity exp where gives the magnitude the decay-time for the periodic component and the smoothness of the periodic component the period has been fixed to one the seasonal component in the data is caused primarily by different rates of uptake for plants depending on the season and it is probably reasonable to assume that this pattern may itself change slowly over time partially due to the elevation of the level itself if this effect turns out not to be relevant then it can be effectively removed at the fitting stage by allowing to become very large. to model the medium term irregularities a rational quadratic term medium term irregularities is used eq. where is the magnitude is the typical length-scale and is the shape parameter determining diffuseness of the length-scales see the discussion on page one could also have used a squared exponential form for this component but it turns out that the rational quadratic works better higher marginal likelihood probably because it can accommodate several length-scales. concentration ppmyear concentration ppmjfmamjjasond concentration c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection for gp regression figure the time course of the seasonal effect plotted in a months vs. year plot wrap-around continuity between the edges. the labels on the contours are in ppmv of the training period extends up to the dashed line. note the slow development the height of the may peak may have started to recede but the low in october may currently be deepening further. the seasonal effects from three particular years were also plotted in figure finally we specify a noise model as the sum of a squared exponential con tribution and an independent component xq exp pq where is the magnitude of the correlated noise component is its lengthscale and is the magnitude of the independent noise component. noise in the series could be caused by measurement inaccuracies and by local short-term weather phenomena so it is probably reasonable to assume at least a modest amount of correlation in time. notice that the correlated noise component the first term of eq. has an identical expression to the long term component in eq. when optimizing the hyperparameters we will see that one of these components becomes large with a long length-scale long term trend while the other remains small with a short length-scale the fact that we have chosen to call one of these components signal and the other one noise is only a question of interpretation. presumably we are less interested in very short-term effect and thus call it noise if on the other hand we were interested in this effect we would call it signal. the final covariance function is kx with hyperparameters we first subtract the empirical mean of the data ppm and then fit the hyperparameters by optimizing the marginal likelihood using a conjugate gradient optimizer. to avoid bad local minima caused by swapping r oles of the rational quadratic and squared exponential terms a few random restarts are tried picking the run with the best marginal likelihood which was log pyx noise terms parameter estimation c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters we now examine and interpret the hyperparameters which optimize the marginal likelihood. the long term trend has a magnitude of ppm and a length scale of years. the mean predictions inside the range of the training data and extending for years into the future are depicted in figure in the same plot right hand axis we also show the medium term effects modelled by the rational quadratic component with magnitude ppm typical length years and shape the very small shape value allows for covariance at many different length-scales which is also evident in figure notice that beyond the edge of the training data the mean of this contribution smoothly decays to zero but of course it still has a contribution to the uncertainty see figure the hyperparameter values for the decaying periodic contribution are magnitude ppm decay-time years and the smoothness of the periodic component is the quite long decay-time shows that the data have a very close to periodic component in the short term. in figure we show the mean periodic contribution for three years corresponding to the beginning middle and end of the training data. this component is not exactly sinusoidal and it changes its shape slowly over time most notably the amplitude is increasing see figure for the noise components we get the amplitude for the correlated component ppm a length-scale of months and an independent noise magnitude of ppm. thus the correlation length for the noise component is indeed inferred to be short and the total magnitude of the noise ppm indicating that the data can be explained very is just well by the model. note also in figure that the model makes relatively confident predictions the confidence region being ppm wide at a year prediction horizon. in conclusion we have seen an example of how non-trivial structure can be inferred by using composite covariance functions and that the ability to leave hyperparameters to be determined by the data is useful in practice. of course a serious treatment of such data would probably require modelling of other effects such as demographic and economic indicators too. finally one may want to use a real time-series approach just a regression from time to level as we have done here to accommodate causality etc. nevertheless the ability of the gaussian process to avoid simple parametric assumptions and still build in a lot of structure makes it as we have seen a very attractive model in many application domains. robot arm inverse dynamics we have discussed the use of gpr for the sarcos robot arm inverse dynamics problem in section this example is also further studied in section where a variety of approximation methods are compared because the size of the training set examples precludes the use of simple gpr due to its storage and time complexity. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection for gp regression figure mis-specification example. fit to datapoints drawn from a step function with gaussian noise with standard deviation n the gaussian process models are using a squared exponential covariance function. panel shows the mean and confidence interval for the noisy signal in grey when the hyperparameters are chosen to maximize the marginal likelihood. panel shows the resulting model when the hyperparameters are chosen using leave-one-out cross-validation note that the marginal likelihood chooses a high noise level and long length-scale whereas loo-cv chooses a smaller noise level and shorter length-scale. it is not immediately obvious which fit it worse. one of the techniques considered in section is the subset of datapoints method where we simply discard some of the data and only make use of m n training examples. given a subset of the training data of size m selected at random we adjusted the hyperparameters by optimizing either the marginal likelihood or lloo. as ard was used this involved adjusting d hyperparameters. this process was repeated times with different random subsets of the data selected for both m and m the results show that the predictive accuracy obtained from the two optimization methods is very similar on both standardized mean squared error and mean standardized log loss criteria but that the marginal likelihood optimization is much quicker. step function example illustrating mis-specification in this section we discuss the mis-specified model scenario where we attempt to learn the hyperparameters for a covariance function which is not very well suited to the data. the mis-specification arises because the data comes from a function which has either zero or very low probability under the gp prior. one could ask why it is interesting to discuss this scenario since one should surely simply avoid choosing such a model in practice. while this is true in theory for practical reasons such as the convenience of using standard forms for the covariance function or because vague prior information one inevitably ends up in a situation which resembles some level of mis-specification. as an example we use data from a noisy step function and fit a gp model with a squared exponential covariance function figure there is misspecification because it would be very unlikely that samples drawn from a gp yinput x yinput x c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters figure same data as in figure panel shows the result of using a covariance function which is the sum of two squared-exponential terms. although this is still a stationary covariance function it gives rise to a higher marginal likelihood than for the squared-exponential covariance function in figure and probably also a better fit. in panel the neural network covariance function eq. was used providing a much larger marginal likelihood and a very good fit. with the stationary se covariance function would look like a step function. for short length-scales samples can vary quite quickly but they would tend to vary rapidly all over not just near the step. conversely a stationary se covariance function with a long length-scale could model the flat parts of the step function but not the rapid transition. note that gibbs covariance function eq. would be one way to achieve the desired effect. it is interesting to note the differences between the model optimized with marginal likelihood in figure and one optimized with loo-cv in panel of the same figure. see exercise for more on how these two criteria weight the influence of the prior. for comparison we show the predictive distribution for two other covariance functions in figure in panel a sum of two squared exponential terms were used in the covariance. notice that this covariance function is still stationary but it is more flexible than a single squared exponential since it has two magnitude and two length-scale parameters. the predictive distribution looks a little bit better and the value of the log marginal likelihood improves from in figure to in figure we also tried the neural network covariance function from eq. which is ideally suited to this case since it allows saturation at different values in the positive and negative directions of x. as shown in figure the predictions are also near perfect and the log marginal likelihood is much larger at model selection for gp classification in this section we compute the derivatives of the approximate marginal likelihood for the laplace and ep methods for binary classification which are needed for training. we also give the detailed algorithms for these and briefly discuss the possible use of cross-validation and other methods for training binary gp yinput x yinput x c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection for gp classification classifiers. derivatives of the marginal likelihood for laplace s approximation recall from section that the approximate log marginal likelihood was given in eq. as log qyx fk f log py f log kw and f is the maximum of the posterior eq. where b i w found by newton s method in algorithm and w is the diagonal matrix w log py f. we can now optimize the approximate marginal likelihood qyx w.r.t. the hyperparameters to this end we seek the partial derivatives of qyx j. the covariance matrix k is a function of the hyperparameters but f and therefore w are also implicitly functions of since when changes the optimum of the posterior f also changes. thus fi j log qyx log qyx log qyx nx fi j j by the chain rule. using eq. and eq. the explicit term is given by fk k j k f tr k j log qyx j when evaluating the remaining term from eq. we utilize the fact that f is the maximum of the posterior so that f at f f where the log posterior is defined in eq. thus the implicit derivatives of the two first terms of eq. vanish leaving only log qyx fi log fi w tr w w fi log py f. f i ii k j in order to evaluate the derivative f j we differentiate the self-consistent eq. f k log py f to obtain f log py f j where we have used the chain rule j f j f and the identity log py f f w the desired derivatives are obtained by plugging eq. into eq. k j log py fk log py f f j f c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters a y input x y targets pyf function compute covariance matrix from x and locate posterior mode using algorithm compute k w log pyf l choleskyi w log z llw c lw r w kw af log pyf p logdiagl diagk log pyf r w k solve ll b i w kw w kw eq. eq. o o eq. eq. for j dim do c k j aca b c log pyf b krb j log z s trrc compute derivative matrix from x and eq. end for return log z marginal likelihood log z derivatives algorithm compute the approximate log marginal likelihood and its derivatives w.r.t. the hyperparameters for binary laplace gpc for use by an optimization routine such as conjugate gradient optimization. in line algorithm on page is called to locate the posterior mode. in line only the diagonal elements of the matrix product should be computed. in line the notation j means the partial derivative w.r.t. the j th hyperparameter. an actual implementation may also return the value of f to be used as an initial guess for the subsequent call an alternative the zero initialization in line of algorithm details of the implementation the implementation of the log marginal likelihood and its partial derivatives w.r.t. the hyperparameters is shown in algorithm it is advantageous to rewrite the equations from the previous section in terms of well-conditioned symmetric positive definite matrices whose solutions can be obtained by cholesky factorization combining numerical stability with computational speed. in detail the matrix of central importance turns out to be r k w w kw where the right hand side is suitable for numerical evaluation as in line of algorithm reusing the cholesky factor l from the newton scheme above. remember that w is diagonal so eq. does not require any real matrix-bymatrix products. rewriting eq. is straightforward and for eq. we apply the matrix inversion lemma to kw to obtain i kr which is used in the implementation. the computational complexity is dominated by the cholesky factorization in line which takes operations per iteration of the newton scheme. in addition the computation of r in line is also all other computations being at most per hyperparameter. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection for gp classification input x y targets log zep compute k k s l choleskyi s ll s b s r bb s ll s for j dim do k compute covariance matrix from x and run the ep algorithm k s b from under eq. r bb s b s solve ll b i s trrc end for c k j j log zep compute derivative matrix from x and eq. return log zep marginal likelihood log zep derivatives algorithm compute the log marginal likelihood and its derivatives w.r.t. the hyperparameters for ep binary gp classification for use by an optimization routine such as conjugate gradient optimization. s is a diagonal precision matrix with entries sii i. in line algorithm on page is called to compute parameters of the ep approximation. in line only the diagonal of the matrix product should be computed and the notation j means the partial derivative w.r.t. the j th hyperparameter. the computational complexity is dominated by the cholesky factorization in line and the solution in line both of which are derivatives of the marginal likelihood for ep optimization of the ep approximation to the marginal likelihood w.r.t. the hyperparameters of the covariance function requires evaluation of the partial derivatives from eq. luckily it turns out that implicit terms in the derivatives caused by the solution of ep being a function of the hyperparameters is exactly zero. we will not present the proof here see seeger consequently we only have to take account of the explicit dependencies log log zep j j s k j s s k k j j s b in algorithm the derivatives from eq. are implemented using log zep tr where b s j b s k cross-validation whereas the loo-cv estimates were easily computed for regression through the use of rank-one updates it is not so obvious how to generalize this to classification. opper and winther sec. use the cavity distributions of their mean-field approach as loo-cv estimates and one could similarly use the cavity distributions from the closely-related ep algorithm discussed in c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn model selection and adaptation of hyperparameters section although technically the cavity distribution for site i could depend on the label yi the algorithm uses all cases when converging to its fixed point this effect is probably very small and indeed opper and winther sec. report very high precision for these loo-cv estimates. as an alternative k-fold cv could be used explicitly for some moderate value of k. other methods for setting hyperparameters alignment above we have considered setting hyperparameters by optimizing the marginal likelihood or cross-validation criteria. however some other criteria have been proposed in the literature. for example cristianini et al. define the alignment between a gram matrix k and the corresponding vector of targets y as yky nkkkf ak y trices ki so that k where kkkf denotes the frobenius norm of the matrix k as defined in eq. lanckriet et al. show that if k is a convex combination of gram mai iki with i for all i then the optimization of the alignment score w.r.t. the i s can be achieved by solving a semidefinite programming problem. example for an example of model selection refer to section although the experiments there were done by exhaustively evaluating the marginal likelihood for a whole grid of hyperparameter values the techniques described in this chapter could be used to locate the same solutions more efficiently. exercises the optimization of the marginal likelihood w.r.t. the hyperparameters is generally not possible in closed form. consider however the situation where one hyperparameter gives the overall scale of the covariance kyx kyx where ky is the covariance function for the noisy targets including noise contributions and kyx may depend on further hyperparameters show that the marginal likelihood can be optimized w.r.t. in closed form. p given by p consider the difference between the log marginal likelihood given by i log pyiyj j i and the loo-cv using log probability which is i log pyiyj j i. from the viewpoint of the marginal likelihood the loo-cv conditions too much on the data. show that the expected loo-cv loss is greater than the expected marginal likelihood. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter relationships between gps and other models in this chapter we discuss a number of concepts and models that are related to gaussian process prediction. in section we cover reproducing kernel hilbert spaces which define a hilbert space of sufficiently-smooth functions corresponding to a given positive semidefinite kernel k. as we discussed in chapter there are many functions that are consistent with a given dataset d. we have seen how the gp approach puts a prior over functions in order to deal with this issue. a related viewpoint is provided by regularization theory in section where one seeks a trade-off between data-fit and the rkhs norm of function. this is closely related to the map estimator in gp prediction and thus omits uncertainty in predictions and also the marginal likelihood. in section we discuss splines a special case of regularization which is obtained when the rkhs is defined in terms of differential operators of a given order. there are a number of other families of kernel machines that are related to gaussian process prediction. in section we describe support vector machines in section we discuss least-squares classification and in section we cover relevance vector machines reproducing kernel hilbert spaces here we present a brief introduction to reproducing kernel hilbert spaces. the theory was developed by aronszajn a more recent treatise is saitoh information can also be found in wahba sch olkopf and smola and wegman the collection of papers edited by weinert provides an overview of the uses of rkhss in statistical signal processing. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models we start with a formal definition of a rkhs and then describe two specific bases for a rkhs firstly through mercer s theorem and the eigenfunctions of k and secondly through the reproducing kernel map. definition kernel hilbert space. let h be a hilbert space of real functions f defined on an index set x then h is called a reproducing kernel hilbert space endowed with an inner product h ih norm kfkh phf fih if there exists a function k x x r with the following properties for every x kx as a function of belongs to h and k has the reproducing property hf k xih fx. see e.g. sch olkopf and smola and wegman note also that as kx and are in h we have that hkx kx reproducing property the rkhs uniquely determines k and vice versa as stated in the following theorem theorem theorem aronszajn let x be an index set. then for every positive definite function k on x x there exists a unique rkhs and vice versa. the hilbert space has the dot product hf fxgxdx function is the representer of evaluation i.e. fx kernels contains many non-smooth functions. in is not a rkhs the delta are the analogues of delta functions within the smoother rkhs. note that the delta function is not itself in in contrast for a rkhs the kernel k is the representer of evaluation and is itself in the rkhs. inner product hf gih the above description is perhaps rather abstract. for our purposes the key intuition behind the rkhs formalism is that the squared norm can be thought of as a generalization to functions of the n-dimensional quadratic form fk we have seen in earlier chapters. consider a real positive semidefinite kernel kx with an eigenfunction i ix relative to a measure recall from mercer s theorem that the eigenfunctions are orthonormal w.r.t. i.e. we have expansion kx r ix jx d ij. we now consider a hilbert space comprised of linear combinations of the eigenfunctions i.e. fx functions fx and gx i i we assert that the inner product hf gih in the hilbert space between ix withpn ix is defined as pn thus this hilbert space is equipped with a norm kfkh where hf fih i i. note that for kfkh to be finite the sequence of coefficients must decay quickly effectively this imposes a smoothness condition on the space. nx figi i hf gih c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn reproducing kernel hilbert spaces we now need to show that this hilbert space is the rkhs corresponding to the kernel k i.e. that it has the reproducing property. this is easily achieved as hf k xih fx. similarly nx fi i ix i nx i ix i hkx kx i notice also that kx is in the rkhs as it has normpn linear combinations of the eigenfunctions with the restrictionpn i i kx x we have now demonstrated that the hilbert space comprised of i i fulfils the two conditions given in definition as there is a unique rkhs associated with k this hilbert space must be that rkhs. the advantage of the abstract formulation of the rkhs is that the eigenbasis will change as we use different measures in mercer s theorem. however the rkhs norm is in fact solely a property of the kernel and is invariant under this change of measure. this can be seen from the fact that the proof of the rkhs properties above is not dependent on the measure see also kailath sec. ii.b. a finite-dimensional example of this measure invariance is explored in exercise notice the analogy between the rkhs norm hf fih if we sample the coefficients fi in the eigenexpansion fx i i and the quadratic form fk if we express k and f in terms of the eigenvectors of k we obtain exactly the same form the sum has only n terms if f has length n. ix from n i then nx ef i i nx thus if n is infinite the sample functions are not in h probability as the expected value of the rkhs norm is infinite see wahba p. and kailath sec. ii.b for further details. however note that although sample functions of this gaussian process are not in h the posterior mean after observing some data will lie in the rkhs due to the smoothing properties of averaging. another view of the rkhs can be obtained from the reproducing kernel map construction. we consider the space of functions f defined as n fx nx ikx xi n n xi x i ro c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regularizer ridge regression representer theorem now let gx relationships between gps and other models jkx j. then we define the inner product hf gih i jkxi j. nx nx clearly condition of definition is fulfilled under the reproducing kernel map construction. we can also demonstrate the reproducing property as hk x f ikx xi fx. regularization the problem of inferring an underlying function fx from a finite possibly noisy dataset without any additional assumptions is clearly ill posed for example in the noise-free case any function that passes through the given data points is acceptable. under a bayesian approach our assumptions are characterized by a prior over functions and given some data we obtain a posterior over functions. the problem of bringing prior assumptions to bear has also been addressed under the regularization viewpoint where these assumptions are encoded in terms of the smoothness of f. we consider the functional jf qy f where y is the vector of targets we are predicting and f fxn is the corresponding vector of function values and is a scaling parameter that trades off the two terms. the first term is called the regularizer and represents smoothness assumptions on f as encoded by a suitable rkhs and the second term is a data-fit term assessing the quality of the prediction fxi for the observed datum yi e.g. the negative log likelihood. indeed recalling that pn ridge regression in section can be seen as a particular case i i where fi is the of regularization. coefficient of eigenfunction ix we see that we are penalizing the weighted squared coefficients. this is taking place in feature space rather than simply in input space as per the standard formulation of ridge regression eq. so it corresponds to kernel ridge regression. the representer theorem shows that each minimizer f h of jf has the ikx the representer theorem was first stated by kimeldorf and wahba for the case of squared o sullivan et al. showed that the representer theorem could be extended to likelihood form fx pn the rkhs contains a null space of unpenalized functions then the given form is correct modulo a term that lies in this null space. this is explained further in section proved the representer theorem for the special case of cubic splines and squared error. this was result extended to general rkhss in kimeldorf and wahba c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regularization functions arising from generalized linear models. the representer theorem can be generalized still further see e.g. sch olkopf and smola sec. if the data-fit term is convex section then there will be a unique minimizer f of jf. for gaussian process prediction with likelihoods that involve the values of f at the n training points only that qy f is the negative log likelihood up to some terms not involving f the analogue of the representer theorem is obvious. this is because the predictive distribution of fx f at test point x given the data y is pf pf df. as derived in eq. we thus ef due to the formulae for the conditional distribution of a multivariate gaussian. ikx xi where k ef kx have the regularization approach has a long tradition in inverse problems dating back at least as far as tikhonov see also tikhonov and arsenin for the application of this approach in the machine learning literature see e.g. poggio and girosi in section we consider rkhss defined in terms of differential operators. in section we demonstrate how to solve the regularization problem in the specific case of squared error and in section we compare and contrast the regularization approach with the gaussian process viewpoint. null space regularization defined by differential operators for x rd define z x mfx xjd d z for example for m and d dx. now set kp with non-negative coefficients am. notice that kp is translation and rotation invariant. in this section we assume that if this is not the case and ak is the first non-zero coefficient then there is a null space of functions that are unpenalized. for example if k then constant and linear functions are in the null space. this case is dealt with in section kp penalizes f in terms of the variability of its function values and derivatives up to order m. how does this correspond to the rkhs formulation of section the key is to recognize that the complex exponentials is x are eigenfunctions of the differential operator if x rd. in this case kp sm z mx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models where fs is the fourier transform of fx. comparing eq. with eq. we see that the kernel has the power spectrum ss pm sm pm sm is x z and thus by fourier inversion we obtain the stationary kernel kx ds. a slightly different approach to obtaining the kernel is to use calculus of variations to minimize jf with respect to f. the euler-lagrange equation leads to nx fx igx xi with mx green s function kernel differential operatorpm where gx is known as a green s function. notice that the green s function also depends on the boundary conditions. for the case of x rd by fourier transforming eq. we recognize that g is in fact the kernel k. the and the integral operator k are in fact inverses as shown by eq. see poggio and girosi for further details. arfken provides an introduction to calculus of variations and green s functions. rkhss for regularizers defined by differential operators are sobolev spaces see e.g. adams for further details on sobolev spaces. we now give two specific examples of kernels derived from differential oper ators. example set and am for m in d using the fourier pair e we obtain kx e note that this is the covariance function of the ornstein-uhlenbeck process see section example by setting am we obtain and using the power series ey ykk! kx is exp sds exp z as shown by yuille and grzywacz this is the squared exponential covariance function that we have seen earlier. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn regularization obtaining the regularized solution the representer theorem tells us the general form of the solution to eq. we now consider a specific functional jf n nx which uses a squared error data-fit term to the negative log likelihood of a gaussian noise model with variance n. substituting fx pn ikx xi and using hk xi k xjih kxi xj we obtain j k n k n n yk yy. n minimizing j by differentiating w.r.t. the vector of coefficients we obtain ni so that the prediction for a test point x is fx kx ni this should look very familiar it is exactly the form of the predictive mean obtained in eq. in the next section we compare and contrast the regularization and gp views of the problem. the solution fx regularization network in poggio and girosi ikx xi that minimizes eq. was called a regularization network the relationship of the regularization view to gaus sian process prediction the regularization method returns f argminf jf. for a gaussian process predictor we obtain a posterior distribution over functions. can we make a connection between these two views? in fact we shall see in this section that f can be viewed as the maximum a posteriori function under the posterior. following szeliski and poggio and girosi we consider exp jf kp exp qy f the first term on the rhs is a gaussian process prior on f and the second is proportional to the likelihood. as f is the minimizer of jf it is the map function. f. thus we obtain kp pm to get some intuition for the gaussian process prior imagine fx being represented on a grid in x-space so that f is now an dimensional vector mdmf where dm is an appropriate finite-difference approximation of the differential operator om. observe that this prior term is a quadratic form in f. amdmfdmf fp m amd to go into more detail concerning the map relationship we consider three cases when qy f is quadratic to a gaussian likelihood c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models when qy f is not quadratic but convex and when qy f is not convex. in case we have seen in chapter that the posterior mean function can be obtained exactly and the posterior is gaussian. as the mean of a gaussian is also its mode this is the map solution. the correspondence between the gp posterior mean and the solution of the regularization problem f was made in kimeldorf and wahba in case we have seen in chapter for classification problems using the logistic probit or softmax response functions that qy f is convex. here the map solution can be found by finding f map solution to the n-dimensional problem defined at the training points and then extending it to other x-values through the posterior mean conditioned on f. in case there will be more than one local minimum of jf under the regularization approach. one could check these minima to find the deepest one. however in this case the argument for map is rather weak if there are multiple optima of similar depth and suggests the need for a fully bayesian treatment. while the regularization solution gives a part of the gaussian process solu tion there are the following limitations it does not characterize the uncertainty in the predictions nor does it handle well multimodality in the posterior. the analysis is focussed at approximating the first level of bayesian inference concerning predictions for f. it is not usually extended to the next level e.g. to the computation of the marginal likelihood. the marginal likelihood is very useful for setting any parameters of the covariance function and for model comparison chapter in addition we find the specification of smoothness via the penalties on derivatives to be not very intuitive. the regularization viewpoint can be thought of as directly specifying the inverse covariance rather than the covariance. as marginalization is achieved for a gaussian distribution directly from the covariance not the inverse covariance it seems more natural to us to specify the covariance function. also while non-stationary covariance functions can be obtained from the regularization viewpoint e.g. by replacing the lebesgue measure in eq. with a non-uniform measure calculation of the corresponding covariance function can then be very difficult. spline models in section we discussed regularizers which had in eq. we now consider the case when in particular we consider the regularizer to be of the form as defined in eq. in this case polynomials of degree c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn spline models up to m are in the null space of the regularization operator in that they are not penalized at all. in the case that x rd we can again use fourier techniques to obtain the green s function g corresponding to the euler-lagrange equation the result as shown by duchon and meinguet is d log cmdx d gx if d and d even otherwise where cmd is a constant p. gives the explicit form. note that the constraint d has to be imposed to avoid having a green s function that is singular at the origin. explicit calculation of the green s function for other domains x is sometimes possible for example see wahba sec. for splines on the sphere. because of the null space a minimizer of the regularization functional has the form nx fx kx igx xi jhjx where hkx are polynomials that span the null space. the exact values of the coefficients and for a specific problem can be obtained in an analogous manner to the derivation in section in fact the solution is equivalent to that given in eq. to gain some more insight into the form of the green s function we consider the equation in fourier space leading to gs s m. gs plays a r ole like that of the power spectrum in eq. but notice thatr gsds is infinite which would imply that the corresponding process has infinite variance. the problem is of course that the null space is unpenalized for example any arbitrary constant function can be added to f without changing the regularizer. tions of fx of the form gx because of the null space we have seen that one cannot obtain a simple connection between the spline solution and a corresponding gaussian process problem. however by introducing the notion of an intrinsic random function one can define a generalized covariance see cressie sec. and stein section for details. the basic idea is to consider linear aifx i for which gx is second-order stationary and where hj ka for j k. a careful description of the equivalence of spline and irf prediction is given in kent and mardia the power-law form of gs s m means that there is no characteristic length-scale for random functions drawn from this prior. thus we obtain the self-similar property characteristic of fractals for further details see szeliski and mandelbrot some authors argue that the lack of a characteristic length-scale is appealing. this may sometimes be the case but if we believe there is an appropriate length-scale set of length-scales irf c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models for a given problem but this is unknown in advance we would argue that a hierarchical bayesian formulation of the problem described in chapter would be more appropriate. splines were originally introduced for one-dimensional interpolation and smoothing problems and then generalized to the multivariate setting. schoenberg considered the problem of finding the function that minimizes spline interpolation natural polynomial spline smoothing spline z b a dx where f denotes the m th derivative of f subject to the interpolation constraints fxi fi xi b for i n and for f in an appropriate sobolev space. he showed that the solution is the natural polynomial spline which is a piecewise polynomial of order in each interval i n and of order m in the two outermost intervals. the pieces are joined so that the solution has continuous derivatives. schoenberg also proved that the solution to the univariate smoothing problem eq. is a natural polynomial spline. a common choice is m leading to the cubic spline. one possible way of writing this solution is nx fx jxj ix where x if x otherwise. it turns out that the coefficients and can be computed in time on using an algorithm due to reinsch see green and silverman sec. for details. splines were first used in regression problems. however by using generalized linear modelling and nelder they can be extended to classification problems and other non-gaussian likelihoods as we did for gp classification in section early references in this direction include silverman and o sullivan et al. there is a vast literature in relation to splines in both the statistics and numerical analysis literatures for entry points see citations in wahba and green and silverman a gaussian process spline construction in this section we will further clarify the relationship between splines and gaussian processes by giving a gp construction for the solution of the univariate cubic spline smoothing problem whose cost functional is nx yi z dx where the observed data are yii n xn and is a smoothing parameter controlling the trade-off between the first term the c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn spline models data-fit and the second term the regularizer or complexity penalty. recall that the solution is a piecewise polynomial as in eq. following wahba we consider the random function gx jxj fx where n where kspx and v minx z i and fx is a gaussian process with covariance f kspx u du to complete the analogue of the regularizer in eq. we need to remove any penalty on polynomial terms in the null space by making the prior vague notice that the covariance has the form of i.e. by taking the limit contributions from explicit basis functions hx x and a regular covariance function kspx a problem which we have already studied in section indeed we have computed the limit where the prior becomes vague the result is given in eq. plugging into the mean equation from eq. we get the predictive mean fx kx y h hx n ij evalwhere ky is the covariance matrix corresponding to uated at the training points h is the matrix that collects the hxi vectors at all training points and y y is given below eq. it is not difficult to show that this predictive mean function is a piecewise cubic polynomial since the elements of kx are cubic polynomials. showing that the mean function is a first order polynomial in the outer intervals and is left as exercise y h f kspxi xj so far ksp has been produced rather mysteriously from the hat we now provide some explanation. shepp defined the l-fold integrated wiener process as l! zudu wlx l where zu denotes the gaussian white noise process with covariance note that is the standard wiener process. it is easy to show that kspx is the covariance of the once-integrated wiener process by writing and using eq. and taking the expectation using the covariance of the white noise process. note that wl is the solution to the stochastic differential equation x z see appendix b for further details on sdes. thus pieces are joined at the datapoints the points where the minx from the covari z ul ance function is non-differentiable. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models spline covariance squared exponential cov. figure panel shows the application of the spline covariance to a simple dataset. the full line shows the predictive mean which is a piecewise cubic polynomial and the grey area indicates the confidence area. the two thin dashed and dash-dotted lines are samples from the posterior. note that the posterior samples are not as smooth as the mean. for comparison a gp using the squared exponential covariance function is shown in panel the hyperparameters in both cases were optimized using the marginal likelihood. the regularizerr for the cubic spline we set l to obtain the sde z corresponding to we can also give an explicit basis-function construction for the covariance function ksp. consider the family of random functions given by fn n ix i n n n where is a vector of parameters with n i. note that the sum has the form of evenly spaced ramps whose magnitudes are given by the entries in the vector. thus efn n i n i n taking the limit n we obtain eq. a derivation which is also found in sec. notice that the covariance function ksp given in eq. corresponds to a gaussian process which is ms continuous but only once ms differentiable. thus samples from the prior will be quite rough although noted in section the posterior mean eq. is smoother. the constructions above can be generalized to the regularizerr dx by replacing u with um eq. and setting hx x xm in eq. and similarly in thus we can use a gaussian process formulation as an alternative to the usual spline fitting procedure. note that the trade-off parameter from eq. xoutput y xoutput y c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn support vector machines figure panel shows a linearly separable binary classification problem and a separating hyperplane. panel shows the maximum margin hyperplane. n f and f the hyperparameters n can be set is now given as the ratio using the techniques from section by optimizing the marginal likelihood given in eq. kohn and ansley give details of an on algorithm on kalman filtering for the computation of the spline and the marginal likelihood. in addition to the predictive mean the gp treatment also yields an explicit estimate of the noise level and predictive error bars. figure shows a simple example. notice that whereas the mean function is a piecewise cubic polynomial samples from the posterior are not smooth. in contrast for the squared exponential covariance functions shown in panel both the mean and functions drawn from the posterior are infinitely differentiable. support vector machines since the mid s there has been an explosion of interest in kernel machines and in particular the support vector machine the aim of this section is to provide a brief introduction to svms and in particular to compare them to gaussian process predictors. we consider svms for classification and regression problems in sections and respectively. more comprehensive treatments can be found in vapnik cristianini and shawe-taylor and sch olkopf and smola support vector classification for support vector classifiers the key notion that we need to introduce is that of the maximum margin hyperplane for a linear classifier. then by using the kernel trick this can be lifted into feature space. we consider first the separable case and then the non-separable case. we conclude this section with a comparison between gp classifiers and svms. xixjw c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models the separable case figure illustrates the case where the data is linearly separable. for a linear classifier with weight vector w and offset let the decision boundary be defined by w x and let w clearly there is a whole version space of weight vectors that give rise to the same classification of the training points. the svm algorithm chooses a particular weight vector that gives rise to the maximum margin of separation. functional margin geometrical margin let the training set be pairs of the form yi for i n where yi for a given weight vector we can compute the quantity i yiw x which is known as the functional margin. notice that i if a training point is correctly classified. if the equation fx w x defines a discriminant function that the output is sgnfx then the hyperplane cw x defines the same discriminant function for any c thus we have the freedom to choose the scaling of w so that mini i and in this case w is known as the canonical form of the hyperplane. the geometrical margin is defined as i iw. for a training point xi that is correctly classified this is simply the distance from xi to the hyperplane. to see this let c so that w ww is a unit vector in the direction of w and is the corresponding offset. then w x computes the length of the projection of x onto the direction orthogonal to the hyperplane and w x computes the distance to the hyperplane. for training points that are misclassified the geometrical margin is the negative distance to the hyperplane. the geometrical margin for a dataset d is defined as d mini i. thus for a canonical separating hyperplane the margin is we wish to find the maximum margin hyperplane i.e. the one that maximizes d. optimization problem by considering canonical hyperplanes we are thus led to the following op timization problem to determine the maximum margin hyperplane minimize subject to yiw xi over w for all i n. it is clear by considering the geometry that for the maximum margin solution there will be at least one data point in each class for which yiw see figure let the hyperplanes that pass through these points be denoted h and h respectively. this constrained optimization problem can be set up using lagrange multipliers and solved using numerical methods for quadratic problems. the form of the solution is w x quadratic programming problem is an optimization problem where the objective func tion is quadratic and the constraints are linear in the unknowns. i iyixi c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn support vector machines support vectors kernel trick soft margin where the i s are non-negative lagrange multipliers. notice that the solution is a linear combination of the xi s. the key feature of equation is that i is zero for every xi except those which lie on the hyperplanes h or h these points are called the support vectors. the fact that not all of the training points contribute to the final solution is referred to as the sparsity of the solution. the support vectors lie closest to the decision boundary. note that if all of the other training points were removed moved around but not crossing h or h the same maximum margin hyperplane would be found. the quadratic programming problem for finding the i s is convex i.e. there are no local minima. notice the similarity of this to the convexity of the optimization problem for gaussian process classifiers as described in section to make predictions for a new input x we compute sgnw x sgn iyixi x nx in the qp problem and in eq. the training points and the test point x enter the computations only in terms of inner products. thus by using the kernel trick we can replace occurrences of the inner product by the kernel to obtain the equivalent result in feature space. the non-separable case for linear classifiers in the original x space there will be some datasets that are not linearly separable. one way to generalize the svm problem in this case is to allow violations of the constraint yiw xi but to impose a penalty when this occurs. this leads to the soft margin support vector machine problem the minimization of nx c yifi with respect to w and where fi fxi w xi and z if z and otherwise. here c is a parameter that specifies the relative importance of the two terms. this convex optimization problem can again be solved using qp methods and yields a solution of the form given in eq. in this case the support vectors with i are not only those data points which lie on the separating hyperplanes but also those that incur penalties. this can occur in two ways the data point falls in between h and h but on the correct side of the decision surface or the data point falls on the wrong side of the decision surface. in a feature space of dimension n if n n then there will always be separating hyperplane. however this hyperplane may not give rise to good generalization performance especially if some of the labels are incorrect and thus the soft margin svm formulation is often used in practice. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models figure a comparison of the hinge error g and g the error function used in svr. for both the hard and soft margin svm qp problems a wide variety of algorithms have been developed for their solution see sch olkopf and smola ch. for details. basic interior point methods involve inversions of n n matrices and thus scale as as with gaussian process prediction. however there are other algorithms such as the sequential minimal optimization algorithm due to platt which often have better scaling in practice. above we have described svms for the two-class classification problem. there are many ways of generalizing svms to the multi-class problem see sch olkopf and smola sec. for further details. comparing support vector and gaussian process classifiers for the soft margin classifier we obtain a solution of the form w p i iyi and thus i ixi ij i jxi xj. kernelizing this we obtain fk k f. thus the soft margin objective nx function can be written as fk c yifi. fk nx for the binary gp classifier to obtain the map value f of pfy we minimize the quantity log pyifi cf. eq. final two terms in eq. are constant if the kernel is fixed. for log-concave likelihoods as those derived from the logistic or probit response functions there is a strong similarity between the two optimization problems in that they are both convex. let g e z g the offset has been absorbed into the kernel so it is not an explicit extra param eter. exp z log z c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn support vector machines hinge error function log and ghingez z where z yifi. we refer to ghinge as the hinge error function due to its shape. as shown in figure all three data fit terms are monotonically decreasing functions of z. all three functions tend to infinity as z and decay to zero as z the key difference is that the hinge function takes on the value for z while the other two just decay slowly. it is this flat part of the hinge function that gives rise to the sparsity of the svm solution. thus there is a close correspondence between the map solution of the gp classifier and the svm solution. can this correspondence be made closer by considering the hinge function as a negative log likelihood? the answer to this is no sollich if cghingez defined a negative log likelihood then exp cghingef exp cghinge f should be a constant independent of f but this is not the case. to see this consider the quantity c f exp f. cannot be chosen so as to make c independent of the value of f for c by comparison for the logistic and probit likelihoods the analogous expression is equal to sollich suggests choosing exp which ensures that c equality only when f he also gives an ingenious interpretation a don t know class to soak up the unassigned probability mass that does yield the svm solution as the map solution to a certain bayesian problem although we find this construction rather contrived. exercise invites you to plot c as a function of f for various values of c. one attraction of the gp classifier is that it produces an output with a clear probabilistic interpretation a prediction for py one can try to interpret the function value fx output by the svm probabilistically and platt suggested that probabilistic predictions can be generated from the svm by computing b for some constants a b that are fitted using some unbiased version of the training set using cross-validation. one disadvantage of this rather ad hoc procedure is that unlike the gp classifiers it does not take into account the predictive variance of fx eq. seeger sec. shows that better error-reject curves can be obtained on an experiment using the mnist digit classification problem when the effect of this uncertainty is taken into account. support vector regression the svm was originally introduced for the classification problem then extended to deal with the regression case. the key concept is that of the error function. this is defined as gz if otherwise. this function is plotted in figure as in eq. we can interpret exp gz as a likelihood model for the regression residuals the squared c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models error function corresponding to a gaussian model. however we note that this is quite an unusual choice of model for the distribution of residuals and is basically motivated by the desire to obtain a sparse solution below as in support vector classifier. if then the error model is a laplacian distribution which corresponds to least absolute values regression cited in rousseeuw this is a heavier-tailed distribution than the gaussian and provides some protection against outliers. girosi showed that the laplacian distribution can be viewed as a continuous mixture of zeromean gaussians with a certain distribution over their variances. pontil et al. extended this result by allowing the means to uniformly shift in in order to obtain a probabilistic model corresponding to the error function. see also section for work on robustification of the gp regression problem. for the linear regression case with an error function and a gaussian prior on w the map value of w is obtained by minimizing gyi fi nx w.r.t. w. the is fx solution fx ikxi x c ixi x where the coefficients are obtained from a qp problem. the problem can also be kernelized to give the as for support vector classification many of the coefficients i are zero. the data points which lie inside the tube have i while those on the edge or outside have non-zero i. least-squares classification in chapter we have argued that the use of logistic or probit likelihoods provides the natural route to develop a gp classifier and that it is attractive in that the outputs can be interpreted probabilistically. however there is an even simpler approach which treats classification as a regression problem. our starting point is binary classification using the linear predictor fx wx. this is trained using linear regression with a target y for patterns that have label and target y for patterns that have label y y give slightly more flexibility than just using targets of as shown in duda and hart section choosing y y appropriately allows us to obtain the same solution as fisher s linear discriminant using the decision criterion fx also they show that using targets y y with the least-squares error function gives a minimum squared-error approximation to the bayes discriminant function pcx pc as n following rifkin and klautau we call such methods least-squares classification note that under a probabilistic interpretation the squared-error criterion is rather an we have assumed that the constant is included in the input vector x. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn least-squares classification odd choice as it implies a gaussian noise model yet only two values of the target and y are observed. it is natural to extend the least-squares classifier using the kernel trick. this has been suggested by a number of authors including poggio and girosi and suykens and vanderwalle experimental results reported in rifkin and klautau indicate that performance comparable to svms can be obtained using kernel lsc as they call it the regularized least-squares classifier rlsc. consider a single random variable y which takes on the value with probability p and value with probability p. then the value of f which minimizes the squared error function e pf pf is f which is a linear rescaling of p to the interval if the targets are and we obtain f p. hence we observe that lsc will estimate p correctly in the large data limit. if we now consider not just a single random variable but wish to estimate pcx a linear rescaling of it then as long as the approximating function fx is sufficiently flexible we would expect that in the limit n it would converge to pcx. more technical detail on this issue see section on consistency. hence lsc is quite a sensible procedure for classification although note that there is no guarantee that fx will be constrained to lie in the interval y. if we wish to guarantee a probabilistic interpretation we could squash the predictions through a sigmoid as suggested for svms by platt and described on page when generalizing from the binary to multi-class situation there is some freedom as to how to set the problem up. sch olkopf and smola sec. identify four methods namely one-versus-rest c binary classifiers are trained to classify each class against all the rest all pairs cc binary classifiers are trained error-correcting output coding each class is assigned a binary codeword and binary classifiers are trained on each bit separately and multi-class objective functions the aim is to train c classifiers simultaneously rather than creating a number of binary classification problems. one also needs to specify how the outputs of the various classifiers that are trained are combined so as to produce an overall answer. for the method one simple criterion is to choose the classifier which produces the most positive output. rifkin and klautau performed extensive experiments and came to the conclusion that the one-versus-rest scheme using either svms or rlsc is as accurate as any other method overall and has the merit of being conceptually simple and straightforward to implement. probabilistic least-squares classification the lsc algorithm discussed above is attractive from a computational point of view but to guarantee a valid probabilistic interpretation one may need to use a separate post-processing stage to squash the predictions through a sigmoid. however it is not so easy to enforce a probabilistic interpretation method is also sometimes called one-versus-all. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models during the training stage. one possible solution is to combine the ideas of training using leave-one-out cross-validation covered in section with the use of a sigmoid function as in platt we will call this method the probabilistic least-squares classifier in section we saw how to compute the gaussian leave-one-out predictive probabilities and that training of hyperparameters can be based on the sum of the log loo probabilities. using this idea we express the loo probability by squashing a linear function of the gaussian predictive probability through a cumulative gaussian pyix y i fi i yi i i dfi z i where the integral is given in eq. and the leave-one-out predictive mean i and variance i are given in eq. the objective function is the sum of the log loo probabilities eq. which can be used to set the hyperparameters as well as the two additional parameters of the linear transformation and in eq. introducing the likelihood in eq. into the objective eq. and taking derivatives we obtain log pyix y i i yi i n i j i j log pyix y i i i i i j i j where ri i loo parameters i j and linear transformation parameters we have i and the partial derivatives of the gaussian i j are given in eq. finally for the lloo j nx nx nx nx lloo lloo n n yi i i i i i these partial derivatives can be used to train the parameters of the gp. there are several options on how to do predictions but the most natural would seem to be to compute predictive mean and variance and squash it through the sigmoid parallelling eq. applying this model to the usps vs. binary classification task discussed in section we get a test set error rate of which compares favourably with the results reported for other methods in figure however the test set information is only which is very poor. test information is dominated by a single test case which is predicted confidently to belong to the wrong class. visual inspection of the digit reveals that indeed it looks as though the testset label is wrong for this case. this observation highlights the danger of not explicitly allowing for data mislabelling in the model for this kind of data. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relevance vector machines relevance vector machines although usually not presented as such the relevance vector machine introduced by tipping is actually a special case of a gaussian process. the covariance function has the form nx kx jx j where j are hyperparameters and the n basis functions jx are usually but not necessarily taken to be gaussian-shaped basis functions centered on each of the n training data points jx exp where is a length-scale hyperparameter controlling the width of the basis function. notice that this is simply the construction for the covariance function corresponding to an n-dimensional set of basis functions given in section with p diag n the covariance function in eq. has two interesting properties firstly it is clear that the feature space corresponding to the covariance function is finite dimensional i.e. the covariance function is degenerate and secondly the covariance function has the odd property that it depends on the training data. this dependency means that the prior over functions depends on the data a property which is at odds with a strict bayesian interpretation. although the usual treatment of the model is still possible this dependency of the prior on the data may lead to some surprising effects as discussed below. training the rvm is analogous to other gp models optimize the marginal likelihood w.r.t. the hyperparameters. this optimization often leads to a significant number of the j hyperparameters tending towards infinity effectively removing or pruning the corresponding basis function from the covariance function in eq. the basic idea is that basis functions that are not significantly contributing to explaining the data should be removed resulting in a sparse model. the basis functions that survive are called relevance vectors. empirically it is often observed that the number of relevance vectors is smaller than the number of support vectors on the same problem the original rvm algorithm was not able to exploit the sparsity very effectively during model fitting as it was initialized with all of the is set to finite values meaning that all of the basis functions contributed to the model. however careful analysis of the rvm marginal likelihood by faul and tipping showed that one can carry out optimization w.r.t. a single i analytically. this has led to the accelerated training algorithm described in tipping and faul which starts with an empty model all is set to infinity and adds basis functions sequentially. as the number of relevance vectors is much less than the number of training cases it will often be much faster to train and make predictions using a rvm than a non-sparse relevance vectors c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn relationships between gps and other models gp. also note that the basis functions can include additional hyperparameters e.g. one could use an automatic relevance determination form of basis function by using different length-scales on different dimensions in eq. these additional hyperparameters could also be set by optimizing the marginal likelihood. the use of a degenerate covariance function which depends on the data imagine a test point x which lies far away has some undesirable effects. from the relevance vectors. at x all basis functions will have values close to zero and since no basis function can give any appreciable signal the predictive distribution will be a gaussian with a mean close to zero and variance close to zero to the inferred noise level. this behaviour is undesirable and could lead to dangerously false conclusions. if the x is far from the relevance vectors then the model shouldn t be able to draw strong conclusions about the output are extrapolating but the predictive uncertainty becomes very small this is the opposite behaviour of what we would expect from a reasonable model. here we have argued that for localized basis functions the rvm has undesirable properties but as argued in rasmussen and qui nonero-candela it is actually the degeneracy of the covariance function which is the core of the problem. although the work of rasmussen and qui nonero-candela goes some way towards fixing the problem there is an inherent conflict degeneracy of the covariance function is good for computational reasons but bad for modelling reasons. exercises sis vectors f pn pn we motivate the fact that the rkhs norm does not depend on the density px using a finite-dimensional analogue. consider the n-dimensional vector f and let the n n matrix be comprised of non-colinear columns n. then f can be expressed as a linear combination of these ci i c for some coefficients let the s be eigenvectors of the covariance matrix k w.r.t. a diagonal matrix p with non-negative entries so that kp where is a diagonal matrix containing the eigenvalues. note that in. show that i i c fk and thus observe that fk can be expressed as c for any valid p and corresponding hint you may find it useful to set p k p etc. plot eq. as a function of f for different values of c. show that there is no value of c and which makes c equal to for all values of f. try setting exp as suggested in sollich and observe what effect this has. show that the predictive mean for the spline covariance gp in eq. is a linear function of x when x is located either to the left or to the right of all training points. hint consider the eigenvectors corresponding to the two largest eigenvalues of the training set covariance matrix from eq. in the vague limit. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter theoretical perspectives this chapter covers a number of more theoretical issues relating to gaussian processes. in section we saw how gpr carries out a linear smoothing of the datapoints using the weight function. the form of the weight function can be understood in terms of the equivalent kernel which is discussed in section as one gets more and more data one would hope that the gp predictions would converge to the true underlying predictive distribution. this question of consistency is reviewed in section where we also discuss the concepts of equivalence and orthogonality of gps. when the generating process for the data is assumed to be a gp it is particularly easy to obtain results for learning curves which describe how the accuracy of the predictor increases as a function of n as described in section an alternative approach to the analysis of generalization error is provided by the pac-bayesian analysis discussed in section here we seek to relate high probability the error observed on the training set to the generalization error of the gp predictor. gaussian processes are just one of the many methods that have been developed for supervised learning problems. in section we compare and contrast gp predictors with other supervised learning methods. the equivalent kernel in this section we consider regression problems. we have seen in section that the posterior mean for gp regression can be obtained as the function which minimizes the functional jf n nx where kfkh is the rkhs norm corresponding to kernel k. our goal is now to understand the behaviour of this solution as n c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives let y be the probability measure from which the data pairs yi are generated. observe that eh nx z n d y. let eyx be the regression function corresponding to the probability then writing y f f we obtain measure the variance around is denoted z z d d y d z as the cross term vanishes due to the definition of as the second term on the right hand side of eq. is independent of f an idealization of the regression problem consists of minimizing the functional z j n n d the form of the minimizing solution is most easily understood in terms of the eigenfunctions ix of the kernel k w.r.t. to wherer ix jxd form a complete orthonormal basis we write fx i ix where i ixd thus x ij see section assuming that the kernel is nondegenerate so that the s fi ix. similarly x j n n i f i i this is readily minimized by differentiation w.r.t. each fi to obtain fi i i nn i. the generalized fourier seriesp nn as n so that in this limit we would notice that the term expect that fx will converge to there are two caveats we have assumed that is sufficiently well-behaved so that it can be represented by i ix and we assumed that the kernel is nondegenerate. if the kernel is degenerate a polynomial kernel then f should converge to the best approximation to within the span of the s. in section we will say more about rates of convergence of f to clearly in general this will depend on the smoothness of the kernel k and the measure y. from a bayesian perspective what is happening is that the prior on f is being overwhelmed by the data as n looking at eq. we also see n n i then fi is effectively zero. this means that we cannot find that if out about the coefficients of eigenfunctions with small eigenvalues until we get sufficient amounts of data. ferrari trecate et al. demonstrated this by c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn the equivalent kernel showing that regression performance of a certain nondegenerate gp could be approximated by taking the first m eigenfunctions where m was chosen so that m using the fact that i r and defining nn. of course as more data is obtained then m has to be increased. nn we eff obtain x z h x i i ix i eff z fx i i i eff ix d with r hnx notice that in the limit n that the term in square brackets in eq. is the equivalent kernel for the smoothing problem we denote it by hnx notice the similarity to the vector-valued weight function hx defined in section the difference is that there the prediction was obtained as a linear combination of a finite number of observations yi with weights given by hix while here we have a noisy function yx instead eff the equivalent kernel tends towards the delta function. the form of the equivalent kernel given in eq. is not very useful in practice as it requires knowledge of the eigenvaluesfunctions for the combination of k and however in the case of stationary kernels we can use fourier methods to compute the equivalent kernel. consider the functional j n dx where has dimensions of the number of observations per unit of x-space etc. as appropriate. using a derivation similar to eq. we obtain hs sf sf n s f n where sf is the power spectrum of the kernel k. the term n corresponds to the power spectrum of a white noise process as the delta function covariance function of white noise corresponds to a constant in the fourier domain. this analysis is known as wiener filtering see e.g. papoulis sec. equation is the same as eq. except that the discrete eigenspectrum has been replaced by a continuous one. as can be observed in figure the equivalent kernel essentially gives a weighting to the observations locally around x. thus identifying with npx we can obtain an approximation to the equivalent kernel for stationary kernels when the width of the kernel is smaller than the length-scale of variations in px. this form of analysis was used by silverman for splines in one dimension. some specific examples of equivalent kernels we first consider the ou process in this has kr exp relative to our previous notation and r x and power spectrum equivalent kernel wiener filtering c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives ss let vn n using eq. we obtain hs where this again has the form of fourier transform of an vn e in ou covariance and can be inverted to obtain hr particular notice that as n increases thus vn decreases the inverse lengthscale of hr increases asymptotically for large n. this shows that the width of equivalent kernel for the ou covariance function will scale as n asymptotically. similarly the width will scale as px asymptotically. a similar analysis can be carried out for the gaussian process in section which has a power spectrum it is in the mat ern class with in this case we can show the fourier relationships given by papoulis p. that the width of the equivalent kernel scales as n asymptotically. analyzes end-effects if the domain of interest is a bounded open interval. for analysis of the equivalent kernel has also been carried out for spline models. silverman gives the explicit form of the equivalent kernel in the case of a one-dimensional cubic spline to the regularizer kp thomas-agnan gives a general expression for the equivalent r kernel for the spline regularizer kp in one dimension and also the regularizer kp in two dimensions the equivalent kernel is sponding to a roughness penalty of r dx the width of the equivalent silverman has also shown that for splines of order m in will scale as n asymptotically. in fact it can be shown that this is true for splines in d dimensions too see exercise given in terms of the kelvin function kei et al. stein another interesting case to consider is the squared exponential kernel where ss exp thus hses b where b n we are unaware of an exact result in this case but the following approximation due to sollich and williams is simple but effective. for large large n b will be small. thus for small s we have that hse but for large s it is approximately the change takes place around the point sc where b c as grows quickly with s the transition of hse between and can be expected to be rapid and thus be well-approximated by a step function. by using the standard result for the fourier transform of the step function we obtain c i.e. hsex scx fact that hs has the same form as sf is particular to the ou covariance function and is not generally the case. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn asymptotic analysis for d where sincz sinzz. a similar calculation in d using eq. gives sc hser r scr. notice that sc scales as so that the width of the equivalent kernel will decay very slowly as n increases. notice that the plots in figure show the sinc-type shape although the sidelobes are not quite as large as would be predicted by the sinc curve the transition is smoother than a step function in fourier space so there is less ringing asymptotic analysis in this section we consider two asymptotic properties of gaussian processes consistency and equivalenceorthogonality. consistency in section we have analyzed the asymptotics of gp regression and have seen how the minimizer of the functional eq. converges to the regression function as n we now broaden the focus by considering loss functions other than squared loss and the case where we work directly with eq. rather than the smoothed version eq. the set up is as follows let l be a pointwise loss function. consider a procedure that takes training data d and this loss function and returns a function fdx. for a measurable function f the risk loss is defined as z rlf ly fx d y. l denote the function that minimizes this risk. for squared loss f let f lx eyx. for loss with classification problems we choose f lx to be the class c at x such that pccx pcjx for all j c ties arbitrarily. definition we will say that a procedure that returns fd is consistent for a given measure y and loss function l if consistency rlfd rlf l as n where convergence is assessed in a suitable manner e.g. in probability. if fdx is consistent for all borel probability measures y then it is said to be versally consistent. ing fx a simple example of a consistent procedure is the kernel regression method. as described in section one obtains a prediction at test point x by j nadaraya-watson estimator. let h be the width of the kernel and d be the dimension of the input wiyi where wi ipn c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives space. it can be shown that under suitable regularity conditions if h and nhd as n then the procedure is consistent see e.g. orfi et al. theorem for the regression case with squared loss and devroye et al. theorem for the classification case using loss. an intuitive understanding of this result can be obtained by noting that h means that only datapoints very close to x will contribute to the prediction bias while the condition nhd means that a large number of datapoints will contribute to the prediction noisevariance. combination of eigenfunctionsp it will first be useful to consider why we might hope that gpr and gpc should be universally consistent. as discussed in section the key property is that a non-degenerate kernel will have an infinite number of eigenfunctions forming an orthonormal set. thus from generalized fourier analysis a linear ci ix should be able to represent a sufficiently well-behaved target function f l. however we have to estimate the infinite number of coefficients from the noisy observations. this makes it clear that we are playing a game involving infinities which needs to be played with care and there are some results and freedman freedman gr unwald and langford which show that in certain circumstances bayesian inference in infinite-dimensional objects can be inconsistent. however there are some positive recent results on the consistency of gpr and gpc. choudhuri et al. show that for the binary classification case under certain assumptions gpc is consistent. the assumptions include smoothness on the mean and covariance function of the gp smoothness on eyx and an assumption that the domain is a bounded subset of rd. their result holds for the class of response functions which are c.d.f.s of a unimodal symmetric density this includes the probit and logistic functions. for gpr choi and schervish show that for a one-dimensional input space of finite length under certain assumptions consistency holds. here the assumptions again include smoothness of the mean and covariance function of the gp and smoothness of eyx. an additional assumption is that the noise has a normal or laplacian distribution an unknown variance which is inferred. there are also some consistency results relating to the functional j nf n n nx m where n as n note that to agree with our previous formulations we would set n but other decay rates on n are often considered. in the splines literature cox showed that for regression problems using the regularizer the definitions in eq. consistency can be obtained under certain technical conditions. cox and o sullivan considered a wide range of problems regression problems with squared loss and classification using logistic loss where the solution is obtained by minimizing the regularized risk using a spline smoothness term. l h h is the rkhs corresponding to the spline they showed that if f c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn asymptotic analysis regularizer then as n and n at an appropriate rate one gets convergence of fd to f l. more recently zhang theorem has shown that for the classification problem with a number of different loss functions logistic loss hinge loss and quadratic loss and for general rkhss with a nondegenerate kernel that if n nn and y is sufficiently regular then the classification error of fd will converge to the bayes optimal error in probability as n similar results have also been obtained by steinwart with various rates on the decay of n depending on the smoothness of the kernel. bartlett et al. have characterized the loss functions that lead to universal consistency. above we have focussed on regression and classification problems. however similar analyses can also be given for other problems such as density estimation and deconvolution see wahba chs. for references. also we have discussed consistency using a fixed decay rate for n. however it is also possible to analyze the asymptotics of methods where n is set in a data-dependent way e.g. by see wahba sec. and references therein for further details. consistency is evidently a desirable property of supervised learning procedures. however it is an asymptotic property that does not say very much about how a given prediction procedure will perform on a particular problem with a given dataset. for instance note that we only required rather general properties of the kernel function non-degeneracy for some of the consistency results. however the choice of the kernel can make a huge difference to how a procedure performs in practice. some analyses related to this issue are given in section equivalence and orthogonality the presentation in this section is based mainly on stein ch. for two probability measures and defined on a measurable space is said to be absolutely continuous w.r.t. if for all a f implies if is absolutely continuous w.r.t. and is absolutely continuous w.r.t. the two measures are said to be equivalent written and are said to be orthogonal written if there exists an a f such that and that in this case we have and where ac is the complement of a. the dichotomy theorem for gaussian processes to hajek and independently feldman states that two gaussian processes are either equivalent or orthogonal. equivalence and orthogonality for gaussian measures with corresponding probability densities can be characterized in terms of the validation is discussed in section section for background on measurable spaces. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives z symmetrized kullback-leibler divergence klsym between them given by the measures are equivalent if klsym and orthogonal otherwise. for two finite-dimensional gaussian distributions n and n we have sec. df log klsym trk k k this expression can be simplified considerably by simultaneously diagonalizing and two finite-dimensional gaussian distributions are equivalent if the null spaces of their covariance matrices coincide and are orthogonal otherwise. things can get more interesting if we consider infinite-dimensional distributions i.e. gaussian processes. consider some closed subset r rd. choose some finite number n of x-points in r and let f fn denote the values corresponding to these inputs. we consider the klsym-divergence as above but in the limit n klsym can now diverge if the rates of decay of the eigenvalues of the two processes are not the same. for example consider zero-mean periodic processes with period where the eigenvalue i j indicates the amount of power in the sincos terms of frequency j for process i then using eq. we have klsym j j j x also p. some corresponding results for the equivalence or orthogonality of non-periodic gaussian processes are given in stein pp. stein gives an example of two equivalent gaussian processes on r those with covariance functions exp r and exp is easy to check that for large s these have the same power spectrum. we now turn to the consequences of equivalence for the model selection problem. suppose that we know that either gp or gp is the correct model. then if gp gp then it is not possible to determine which model is correct with probability however under a bayesian setting all this means is if we have prior probabilities and on these two hypotheses then after observing some data d the posterior probabilities pgp id i will not be or but could be heavily skewed to one model or the other. the other important observation is to consider the predictions made by gp or gp consider the case where gp is the correct model and gp gp then stein sec. shows that the predictions of gp are asymptotically optimal in the sense that the expected relative prediction error between gp and gp tends to as n under some technical conditions. stein s corollary shows that this conclusion remains true under additive noise if the un-noisy gps are equivalent. one caveat about equivalence is although the predictions of gp are asymptotically optimal when gp is the correct model and gp gp one would see differing predictions for finite n. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn average-case learning curves average-case learning curves in section we have discussed the asymptotic properties of gaussian process predictors and related methods. in this section we will say more about the speed of convergence under certain specific assumptions. our goal will be to obtain a learning curve describing the generalization error as a function of the training set size n. this is an average-case analysis averaging over the choice of target functions from a gp and over the x locations of the training points. in more detail we first consider a target function f drawn from a gaussian process. n locations are chosen to make observations at giving rise to the training set d y. the yis are noisy observations of the underlying function f. given a loss function l which measures the difference between the prediction for f and f itself we obtain an estimator fd for f. below we will use the squared loss so that the posterior mean fdx is the estimator. then the generalization error f and d is given by egdf lfx fdx dx generalization error z z as this is an expected loss it is technically a risk but the term generalization error is commonly used. egdf depends on both the choice of f and on x. that y depends on the choice of f and also on the noise if present. the first level of averaging we consider is over functions f drawn from a gp prior to obtain egx egdfpf df. it will turn out that for regression problems with gaussian process priors and predictors this average can be readily calculated. the second level of averaging assumes that the x-locations of the training set are drawn i.i.d. from px to give z egn pxn dxn. a plot of egn against n is known as a learning curve. learning curve rather than averaging over x an alternative is to minimize egx w.r.t. x. this gives rise to the optimal experimental design problem. we will not say more about this problem here but it has been subject to a large amount of investigation. an early paper on this subject is by ylvisaker these questions have been addressed both in the statistical literature and in theoretical numerical analysis for the latter area the book by ritter provides a useful overview. we now proceed to develop the average-case analysis further for the specific case of gp predictors and gp priors for the regression case using squared loss. let f be drawn from a zero-mean gp with covariance function and noise level similarly the predictor assumes a zero-mean process but covariance c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives at a particular test location x averaging over function and noise level f we have efx ef x efx eyyk where kiy kif assumed noise. the above expression reduces to x variance of the gp. i for i i.e. the covariance matrix including the if so that the predictor is correctly specified then the predictive z z averaging the error over px we obtain egx efx x dx tr k z tr dx z dx k dx for some choices of px and covariance functions these integrals will be analytically tractable reducing the computation of egx to a n n matrix computation. to obtain egn we need to perform a final level of averaging over x. in general this is difficult even if egx can be computed exactly but it is sometimes possible e.g. for the noise-free ou process on the real line see section we use the definition kx the form of egx can be simplified considerably if we express the covariance functions in terms of their eigenfunction expansions. in the case that i let be a diagonal matrix of the eigenvalues and be the n n design matrix as defined in section then from eq. we obtain i i ix andr kx ixpx dx egx tr tr tr ni n where the second line follows through the use of the matrix inversion lemma eq. directly if we use eq. as shown in sollich or opper and vivarelli using the fact that ex ni a na ve approximation would replace inside the trace with its expectation in fact opper and vivarelli showed that this gives a lower bound so that egn tr n n i nx i n n i examining the asymptotics of eq. we see that for each eigenvalue where i nn onto the bound on the generalization error. as we saw nn we add c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn pac-bayesian analysis p in section more eigenfunctions come into play as n increases so the rate of decay of egn is slower than sollich derives a number of more accurate approximations to the learning curve than eq. for the noiseless case with there is a simple lower bound egn i due to micchelli and wahba this bound is obtained by demonstrating that the optimal n pieces of information are the projections of the random function f onto the first n eigenfunctions. as observations which simply consist of function evaluations will not in general provide such information this is a lower bound. plaskota generalized this result to give a bound on the learning curve if the observations are noisy. some asymptotic results for the learning curves are known. for example in ritter sec. covariance functions obeying sacks-ylvisaker of order r in are considered. he shows that for an optimal sampling of the input space the generalization error goes as on for the noisy problem. similar rates can also be found in sollich for random designs. for the noise-free case ritter p. gives the rate as on one can examine the learning curve not only asymptotically but also for small n where typically the curve has a roughly linear decrease with n. williams and vivarelli explained this behaviour by observing that the introduction of a datapoint reduces the variance locally around a stationary covariance function. the addition of another datapoint at will also create a hole there and so on. with only a small number of datapoints it is likely that these holes will be far apart so their contributions will add thus explaining the initial linear trend. sollich has also investigated the mismatched case where this can give rise to a rich variety of behaviours in the learning curves including plateaux. stein chs. has also carried out some analysis of the mismatched case. although we have focused on gp regression with squared loss we note that malzahn and opper have developed more general techniques that can be used to analyze learning curves for other situations such as gp classification. pac-bayesian analysis in section we gave an average-case analysis of generalization taking the average with respect to a gp prior over functions. in this section we present a different kind of analysis within the probably approximately correct framework due to valiant seeger has presented a pacbayesian analysis of generalization in gaussian process classifiers and we get to this in a number of stages we first present an introduction to the pac framework then describe the pac-bayesian approach speaking a stochastic process which possesses r ms derivatives but not r is said to satisfy sacks-ylvisaker conditions of order r in this gives rise to a spectrum i i asymptotically. the ou process obeys sacks-ylvisaker conditions of order pac c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives and then finally the application to gp classification our presentation is based mainly on seeger the pac framework consider a fixed measure y. given a loss function l there exists a function which minimizes the expected risk. by running a learning algorithm on a data set d of size n drawn i.i.d. from y we obtain an estimate fd of p which attains an expected risk rlfd. we are not able to evaluate rlfd as p we do not know however we do have access to the empirical distribution of i xi yi and can compute the empirical the training set y n i lyi fdxi. because the training set had been used to risk rlfd compute fd we would expect rlfd to underestimate and the aim of the pac analysis is to provide a bound on rlfd based on rlfd. n a pac bound has the following format pdrlfd rlfd gapfdd where pd denotes the probability distribution of datasets drawn i.i.d. from y and is called the confidence parameter. the bound states that averaged over draws of the dataset d from y rlfd does not exceed the sum of rlfd and the gap term with probability of at least the accounts for the probably in pac and the approximately derives from the fact that the gap term is positive for all n. it is important to note that pac analyses are distribution-free i.e. eq. must hold for any measure there are two kinds of pac bounds depending on whether gapfdd actually depends on the particular sample d than on simple statistics like n. bounds that do depend on d are called data dependent and those that do not are called data independent. the pac-bayesian bounds given below are data dependent. has mean m. an estimate of m is given by the sample mean x p it is important to understand the interpretation of a pac bound and to clarify this we first consider a simpler case of statistical inference. we are given a dataset d xn drawn i.i.d. from a distribution that i xin. under certain assumptions we can obtain put bounds on the sampling distribution p xm which relates to the choice of dataset d. however if we wish to perform probabilistic inference for m we need to combine p xm with a prior distribution pm and use bayes theorem to obtain the the situation is similar somewhat more complex for pac bounds as these concern the sampling distribution of the expected and empirical risks of fd w.r.t. d. is also possible to consider pac analyses of other empirical quantities such as the cross-validation error section which do not have this bias. introductory treatments of frequentist statistics the logical hiatus of going from the sampling distribution to inference on the parameter of interest is often not well explained. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn pac-bayesian analysis we might wish to make a conditional statement like pdrlfd r gapfdd rlfd r where r is a small value but such a statement cannot be inferred directly from the pac bound. this is because the gap might be heavily anti-correlated with rlfd so that the gap is large when the empirical risk is small. pac bounds are sometimes used to carry out model selection given a learning machine which depends on a or continuous parameter vector one can seek to minimize the generalization bound as a function of however this procedure may not be well-justified if the generalization bounds are loose. let the slack denote the difference between the value of the bound and the generalization error. the danger of choosing to minimize the bound is that if the slack depends on then the value of that minimizes the bound may be very different from the value of that minimizes the generalization error. see seeger sec. for further discussion. pac-bayesian analysis we now consider a bayesian set up with a prior distribution pw over the parameters w and a posterior distribution qw. speaking the analysis does not require qw to be the posterior distribution just some other distribution but in practice we will consider q to be an posterior distribution. we also limit our discussion to binary classification with labels although more general cases can be considered see seeger sec. r qf x and the predictive classifier outputs sgnqf the predictive distribution for f at a test point x given qw is qf the gibbs classifier has also been studied in learning theory given a test point x one draws a sample w from qw and predicts the label using sgnfx w. the main reason for introducing the gibbs classifier here is that the pacbayesian theorems given below apply to gibbs classifiers. for a given parameter vector w giving rise to a classifier cx w the ex pected risk and empirical risk are given by rlw ly cx w d y rlw lyi cxi w. as the gibbs classifier draws samples from qw we consider the averaged risks rlq rlwqw dw rlq rlwqw dw. nx n z z z theorem s pac-bayesian theorem for any probability measures p and q over w and for any bounded loss function l for which ly cx for any classifier c and input x we have n s pd rlq rlq klqp log log n q o predictive classifier gibbs classifier mcallester s pac-bayesian theorem c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives the proof can be found in mcallester the kullback-leibler divergence klqp is defined in section an example of a loss function which obeys the conditions of the theorem is the loss. for the special case of loss seeger gives the following tighter seeger s pacbayesian theorem n bound. theorem s pac-bayesian theorem for any distribution over x and for any probability measures p and q over w the following bound holds for i.i.d. samples drawn from the data distribution log n here klber is the kl divergence between two bernoulli distributions in eq. thus the theorem bounds high probability the kl divergence between rlq and rlq. o pd klber rlqrlq n q the pac-bayesian theorems above refer to a gibbs classifier. if we are interested in the predictive classifier sgnqf then seeger shows that if qf is symmetric about its mean then the expected risk of the predictive classifier is less than twice the expected risk of the gibbs classifier. however this result is based on a simple bounding argument and in practice one would expect that the predictive classifier will usually give better performance than the gibbs classifier. recent work by meir and zhang provides some pac bounds directly for bayesian algorithms the predictive classifier whose predictions are made on the basis of a data-dependent posterior distribution. pac-bayesian analysis of gp classification to apply this bound to the gaussian process case we need to compute the kl divergence klqp between the posterior distribution qw and the prior distribution pw. although this could be considered w.r.t. the weight vector w in the eigenfunction expansion in fact it turns out to be more convenient to consider the latent function value fx at every possible point in the input space x as the parameter. we divide this infinite vector into two parts the values corresponding to the training points xn denoted f and those at the remaining points in x-space test points f the key observation is that all methods we have described for dealing with gp classification problems produce a posterior approximation qfy which is defined at the training points. is an approximation for laplace s method and for ep mcmc methods sample from the exact posterior. this posterior over f is then extended to the test points by setting qf f qfypf of course for the prior distribution we have a similar decomposition pf f c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn comparison with other supervised learning methods pfpf thus the kl divergence is given by z z klqp qfypf log qfypf qfy log qfy pfpf df df pf df as shown e.g. in seeger notice that this has reduced a rather scary infinite-dimensional integration to a more manageable n-dimensional integration in the case that qfy is gaussian for the laplace and ep approximations this kl divergence can be computed using eq. for the laplace approximation with pf n k and qfy n f a this gives klqp fk f log log seeger has evaluated the quality of the bound produced by the pacbayesian method for a laplace gpc on the task of discriminating handwritten and from the mnist handwritten digits he reserved a test set of examples and used training sets of size and the classifications were replicated ten times using draws of the training sets from a pool of examples. we quote example results for n where the training error was the test error was and the pac-bayesian bound on the generalization error for was figures denote a confidence interval. the classification results are for the gibbs classifier for the predictive classifier the test error rate was thus the generalization error is around while the pac bound is many pac bounds struggle to predict error rates below so this is an impressive and highly non-trivial result. further details and experiments can be found in seeger comparison with other supervised learn ing methods the focus of this book is on gaussian process methods for supervised learning. however there are many other techniques available for supervised learning such as linear regression logistic regression decision trees neural networks support vector machines kernel smoothers k-nearest neighbour classifiers etc. and we need to consider the relative strengths and weaknesses of these approaches. supervised learning is an inductive process given a finite training set we wish to infer a function f that makes predictions for all possible input values. the additional assumptions made by the learning algorithm are known as its inductive bias e.g. mitchell p. sometimes these assumptions are explicit but for other algorithms for decision tree induction they can be rather more implicit. httpyann.lecun.comexdbmnist. inductive bias c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives however for all their variety supervised learning algorithms are based on the idea that similar input patterns will usually give rise to similar outputs output distributions and it is the precise notion of similarity that differentiates the algorithms. for example some algorithms may do feature selection and decide that there are input dimensions that are irrelevant to the predictive task. some algorithms may construct new features out of those provided and measure similarity in this derived space. as we have seen many regression techniques can be seen as linear smoothers section and these techniques vary in the definition of the weight function that is used. one important distinction between different learning algorithms is how they relate to the question of universal consistency section for example a linear regression model will be inconsistent if the function that minimizes the risk cannot be represented by a linear function of the inputs. in general a model with a finite-dimensional parameter vector will not be universally consistent. examples of such models are linear regression and logistic regression with a finite-dimensional feature vector and neural networks with a fixed number of hidden units. in contrast to these parametric models we have non-parametric models as k-nearest neighbour classifiers kernel smoothers and gaussian processes and svms with nondegenerate kernels which do not compress the training data into a finite-dimensional parameter vector. an intermediate position is taken by semi-parametric models such as neural networks where the number of hidden units k is allowed to increase as n increases. in this case universal consistency results can be obtained et al. ch. under certain technical conditions and growth rates on k. although universal consistency is a good thing it does not necessarily mean that we should only consider procedures that have this property for example if on a specific problem we knew that a linear regression model was consistent for that problem then it would be very natural to use it. in the s there was a large surge in interest in artificial neural networks which are feedforward networks consisting of an input layer followed by one or more layers of non-linear transformations of weighted combinations of the activity from previous layers and an output layer. one reason for this surge of interest was the use of the backpropagation algorithm for training anns. initial excitement centered around that fact that training non-linear networks was possible but later the focus came onto the generalization performance of anns and how to deal with questions such as how many layers of hidden units to use how many units there should be in each layer and what type of non-linearities should be used etc. for a particular ann the search for a good set of weights for a given training set is complicated by the fact that there can be local optima in the optimization problem this can cause significant difficulties in practice. in contrast for gaussian process regression and classification the posterior for the latent variables is convex. neural networks bayesian neural networks one approach to the problems raised above was to put anns in a bayesian framework as developed by mackay and neal this gives rise c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn comparison with other supervised learning methods to posterior distributions over weights for a given architecture and the use of the marginal likelihood section for model comparison and selection. in contrast to gaussian process regression the marginal likelihood for a given ann model is not analytically tractable and thus approximation techniques such as the laplace approximation and markov chain monte carlo methods have to be used. neal s observation that certain anns with one hidden layer converge to a gaussian process prior over functions section led us to consider gps as alternatives to anns. mackay sec. raises an interesting question whether in moving from neural networks to gaussian processes we have thrown the baby out with the bathwater? this question arises from his statements that neural networks were meant to be intelligent models that discovered features and patterns in data while gaussian processes are simply smoothing devices our answer to this question is that gps give us a computationally attractive method for dealing with the smoothing problem for a given kernel and that issues of feature discovery etc. can be addressed through methods to select the kernel function chapter for more details on how to do this. note that using a distance function with m having a low-rank form m as in eq. features are described by the columns of however some of the non-convexity of the neural network optimization problem now returns as optimizing the marginal likelihood in terms of the parameters of m may well have local optima. as we have seen from chapters and linear regression and logistic regression with gaussian priors on the parameters are a natural starting point for the development of gaussian process regression and gaussian process classification. however we need to enhance the flexibility of these models and the use of non-degenerate kernels opens up the possibility of universal consistency. kernel smoothers and classifiers have been described in sections and at a high level there are similarities between gp prediction and these methods as a kernel is placed on every training example and the prediction is obtained through a weighted sum of the kernel functions but the details of the prediction and the underlying logic differ. note that the gp prediction view gives us much more e.g. error bars on the predictions and the use of the marginal likelihood to set parameters in the kernel section on the other hand the computational problem that needs to be solved to carry out gp prediction is more demanding than that for simple kernel-based methods. kernel smoothers and classifiers are non-parametric methods and consistency can often be obtained under conditions where the width h of the kernel tends to zero while nhd the equivalent kernel analysis of gp regression shows that there are quite close connections between the kernel regression method and gpr but note that the equivalent kernel automatically reduces its width as n grows in contrast the decay of h has to be imposed for kernel regression. also for some kernel smoothing and classification algorithms the width of the kernel is increased in areas of low observation density for example this would occur in algorithms that consider the k nearest neighbours of a test point. again notice from the equivalent kernel analysis that the width linear and logistic regression kernel smoothers and classifiers c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives regularization networks splines svms and rvms of the equivalent kernel is larger in regions of low density although the exact dependence on the density will depend on the kernel used. the similarities and differences between gp prediction and regularization networks splines svms and rvms have been discussed in chapter appendix learning curve for the ornstein uhlenbeck process we now consider the calculation of the learning curve for the ou covariance function kr exp on the interval assuming that the training x s are drawn from the uniform distribution our treatment is based on williams and vivarelli we first calculate egx for a fixed design and then integrate over possible designs to obtain egn. in the absence of noise the ou process is markovian discussed in appendix b and exercise we consider the interval with points xn xn placed on this interval. also let and due to the markovian nature of the process the prediction at a test point x depends only on the function values of the training points immediately to the left and right of x. thus in the i-th interval from the bounding points are xi and let this interval have length i. using eq. we have z z nx xi egx f dx f dx where we have in interval i i n that where k is the gram matrix f is the predictive variance at input x. using the markovian property f kxk k i and kx is the corresponding vector of length thus k i k i k i k i k where i i and f i z thus xi ikx x. f i xi i i thanks manfred opper for pointing out that the upper bound developed in williams and vivarelli is exact for the noise-free ou process. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn exercises where z k z kzk zdz. for kr exp these equations reduce to e e and e thus f i ie i e i this calculation is not correct in the first and last intervals where only f and xn are relevant for the interval we have that and thus z f e xdx z xi z and a similar result holds forr f putting this all together we obtain xn egx n e n n ie i e i choosing a regular grid so that n and i for i n it is straightforward to show exercise that eg scales as on in agreement with the general sacks-ylvisaker result p. when it is recalled that the ou process obeys sacks-ylvisaker conditions of order a similar calculation is given in plaskota sec. for the wiener process on that this is also markovian but non-stationary. we have now worked out the generalization error for a fixed design x. however to compute egn we need to average egx over draws of x from the uniform distribution. the theory of order statistics david eq. tells us that p for all the i i n. taking the expectation of egx then turns into the problem of evaluating the one-dimensional integrals r e p and r e e exercise asks you to compute these integrals numerically. exercises consider a spline regularizer with sf c we noted in section this is not strictly a power spectrum as the spline is an improper prior but it can be used as a power spectrum in eq. for the c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn theoretical perspectives purposes of this analysis. the equivalent kernel corresponding to this spline is given by z is x hx n by changing variables in the integration to ds where c show that the width of hx scales as n equation gives the form of the equivalent kernel for a spline regularizer. show that is only finite if d. transform the integration to polar coordinates. this observation was made by p. whittle in the discussion of silverman and shows the need for the condition d for spline smoothing. computer exercise space n points out evenly along the interval n to be even so that one of the sample points falls at calculate the weight function section corresponding to gaussian process regression with a particular covariance function and noise level and plot this for the point x now compute the equivalent kernel corresponding to the covariance function e.g. the examples in section plot this on the same axes and compare results. hint recall that the equivalent kernel is defined in terms of integration eq. so that there will be a scaling factor of hint if you wish to use large n use the ngrid method described in section consider egx as given in eq. and choose a regular grid design x so that n and i for i n show that egx scales as on asymptotically. hint when expanding exp i be sure to extend the expansion to sufficient order. compute numerically the expectation of egx eq. over random designs for the ou process example discussed in section make use of the fact eq. that p for all the i i n. investigate the scaling behaviour of egn w.r.t. n. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter approximation methods for large datasets as we have seen in the preceding chapters a significant problem with gaussian process prediction is that it typically scales as for large problems n both storing the gram matrix and solving the associated linear systems are prohibitive on modern workstations this boundary can be pushed further by using high-performance computers. an extensive range of proposals have been suggested to deal with this problem. below we divide these into five parts in section we consider reducedrank approximations to the gram matrix in section a general strategy for greedy approximations is described in section we discuss various methods for approximating the gp regression problem for fixed hyperparameters in section we describe various methods for approximating the gp classification problem for fixed hyperparameters and in section we describe methods to approximate the marginal likelihood and its derivatives. many not all of these methods use a subset of size m n of the training examples. reduced-rank approximations of the gram matrix in the gp regression problem we need to invert the matrix k ni at least to solve a linear system niv y for v. if the matrix k has rank q that it can be represented in the form k qq where q is an n q matrix then this matrix inversion can be speeded up using the matrix inversion lemma eq. as niq qq notice that the inversion of an n n matrix has now been transformed to the inversion of a q q nin n in n q numerical reasons this is not the best way to solve such a linear system but it does illustrate the savings that can be obtained with reduced-rank representations. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets in the case that the kernel is derived from an explicit feature expansion with n features then the gram matrix will have rank minn n so that exploitation of this structure will be beneficial if n n. even if the kernel is non-degenerate it may happen that it has a fast-decaying eigenspectrum e.g. section so that a reduced-rank approximation will be accurate. if k is not of rank n we can still consider reduced-rank approximations to k. the optimal reduced-rank approximation of k w.r.t. the frobenius norm eq. is uq qu q where q is the diagonal matrix of the leading q eigenvalues of k and uq is the matrix of the corresponding orthonormal eigenvectors and van loan theorem unfortunately this is of limited interest in practice as computing the eigendecomposition is an operation. however it does suggest that if we can more cheaply obtain an approximate eigendecomposition then this may give rise to a useful reducedrank approximation to k. kmm we now consider selecting a subset i of the n datapoints set i has size m n. the remaining n m datapoints form the set r. a mnemonic i is for the included datapoints and r is for the remaining points. we sometimes call the included set the active set. without loss of generality we assume that the datapoints are ordered so that set i comes first. thus k can be partitioned as k kn mm kn mn m the top m n block will also be referred to as kmn and its transpose as knm. in section we saw how to approximate the eigenfunctions of a kernel using the nystr om method. we can now apply the same idea to approximating the eigenvaluesvectors of k. we compute the eigenvectors and eigenvalues of kmm and denote them these are extended to all n points using eq. to give and i i kmn m i n m r m i un i i m i m knmum i n i has been chosen so that un in general we have where the scaling of un a choice of how many of the approximate eigenvaluesvectors to include in our approximation of k choosing the first p we get k pp un i i i un i i below we will set p m to obtain nystr om approximation k knmk mmkmn using equations and which we call the nystr om approximation to k. computation of k takes time as the eigendecomposition of kmm is and the computation of each un is omn. fowlkes et al. have applied the nystr om method to approximate the top few eigenvectors in a computer vision problem where the matrices in question are larger than in size. i c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn reduced-rank approximations of the gram matrix the nystr om approximation has been applied above to approximate the elements of k. however using the approximation for the ith eigenfunction where kmx kx xm ix restatement of eq. using the current notation and i it is easy to see that in general we obtain an approximation for the kernel kx m i i i pn i ix as kx mx mx i m ix i m m kmxk i kmxum i i clearly eq. is obtained by evaluating eq. for all pairs of datapoints in the training set. by multiplying out eq. using kmn m it is easy to show that kmm kmm kmn m kmn m kn mm kn mm but that kn mn m kn mmk kn mn m kn mn m is in fact the schur complement of kmm and van loan p. it is easy to see that kn mn m kn mn m is positive semi-definite if a vector f is partitioned as f n m and f has a gaussian distribution with zero mean and covariance k then fn mfm has the schur complement as its covariance matrix see eq. mmkmn m. the difference m f the nystr om approximation was derived in the above fashion by williams and seeger for application to kernel machines. an alternative view which gives rise to the same approximation is due to smola and sch olkopf also sch olkopf and smola sec. here the starting point is that we wish to approximate the kernel centered on point xi as a linear combination of kernels from the active set so that cijkxj x kxi x kxi x x j i for some coefficients that are to be determined so as to optimize the approximation. a reasonable criterion to minimize is kkxi x kxi nx ec tr k trckmn trckmmc where the coefficients are arranged into a n m matrix c. minimizing ec w.r.t. c gives copt knmk mm thus we obtain the approximation k knmk mmkmn in agreement with eq. also it can be shown that ecopt trk k. smola and sch olkopf suggest a greedy algorithm to choose points to include into the active set so as to minimize the error criterion. as it takes c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets omn operations to evaluate the change in e due to including one new datapoint exercise it is infeasible to consider all members of set r for inclusion on each iteration instead smola and sch olkopf suggest finding the best point to include from a randomly chosen subset of set r on each iteration. recent work by drineas and mahoney analyzes a similar algorithm to the nystr om approximation except that they use biased sampling with replacement column i of k with probability ii and a pseudoinverse of the inner m m matrix. for this algorithm they are able to provide probabilistic bounds on the quality of the approximation. earlier work by frieze et al. had developed an approximation to the singular value decomposition of a rectangular matrix using a weighted random subsampling of its rows and columns and probabilistic error bounds. however this is rather different from the nystr om approximation see drineas and mahoney sec. for details. fine and scheinberg suggest an alternative low-rank approximation to k using the incomplete cholesky factorization golub and van loan sec. the idea here is that when computing the cholesky decomposition of k pivots below a certain threshold are if the number of pivots greater than the threshold is k the incomplete cholesky factorization takes time greedy approximation many of the methods described below use an active set of training points of size m selected from the training set of size n m. we assume that it is impossible to search for the optimal subset of size m due to combinatorics. the points in the active set could be selected randomly but in general we might expect better performance if the points are selected greedily w.r.t. some criterion. in the statistics literature greedy approaches are also known as forward selection strategies. a general recipe for greedy approximation is given in algorithm the algorithm starts with the active set i being empty and the set r containing the indices of all training examples. on each iteration one index is selected from r and added to i. this is achieved by evaluating some criterion and selecting the data point that optimizes this criterion. for some algorithms it can be too expensive to evaluate on all points in r so some working set j r can be chosen instead usually at random from r. greedy selection methods have been used with the subset of regressors subset of datapoints and the projected process methods described below. a technical detail symmetric permutations of the rows and columns are required to stabilize the computations. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximations for gpr with fixed hyperparameters input m desired size of active set initialization i r n for j m do create working set j r compute j for all j j i argmaxj j j update model to include data from example i i i r ri end for return i algorithm general framework for greedy subset selection. j is the criterion function evaluated on data point j. approximations for gpr with fixed hy perparameters we present six approximation schemes for gpr below namely the subset of regressors the nystr om method the subset of datapoints the projected process approximation the bayesian committee machine and the iterative solution of linear systems. section provides a summary of these methods and a comparison of their performance on the sarcos data which was introduced in section subset of regressors pn silverman sec. showed that the mean gp predictor can be obtained from a finite-dimensional generalized linear regression model fx ikx xi with a prior n k to see this we use the mean prediction for linear regression model in feature space given by eq. i.e. fx n setting kx k and fx n y with a p p k we obtain n kx kx n kk ni ni in agreement with eq. note however that the predictive of this model is different from full gpr. a simple approximation to this model is to consider only a subset of regres sors so that mx fsrx ikx xi with m n k mm. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets again using eq. we obtain fsrx kmx nkmm vfsrx nkmx nkmm sr marginal likelihood nkmm thus the posterior mean for m is given by m this method has been proposed for example in wahba chapter and in poggio and girosi eq. via the regularization framework. the name subset of regressors was suggested to us by g. wahba. the computations for equations and take time to carry out the necessary matrix computations. after this the prediction of the mean for a new test point takes time om and the predictive variance takes under the subset of regressors model we have f n k where k is y k log k nin defined as in eq. thus the log marginal likelihood under this model is log psryx notice that the covariance function defined by the sr model has the form kx kxk which is exactly the same as that from the nystr om approximation for the covariance function eq. in fact if the covariance function kx in the predictive mean and variance equations and is replaced systematically with kx we obtain equations and as shown in appendix nin n if the kernel function decays to zero for for fixed then kx x will be near zero when x is distant from points in the set i. this will be the case even when the kernel is stationary so that kx x is independent of x. thus we might expect that using the approximate kernel will give poor predictions especially underestimates of the predictive variance when x is far from points in the set i. ysr pm an interesting idea suggested by rasmussen and qui nonero-candela to mitigate this problem is to define the sr model with m basis functions where the extra basis function is centered on the test point x so that ikx xi kx x this model can then be used to make predictions and it can be implemented efficiently using the partitioned matrix inverse equations and the effect of the extra basis function centered on x is to maintain predictive variance at the test point. so far we have not said how the subset i should be chosen. one simple method is to choose it randomly from x another is to run clustering on to obtain centres. alternatively a number of greedy forward selection algorithms for i have been proposed. luo and wahba choose the next kernel so as to minimize the residual sum of squares knm after optimizing m. smola and bartlett take a similar approach but choose as their criterion the quadratic form knm mkmm m y k nin n c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximations for gpr with fixed hyperparameters comparison with rvm where the right hand side follows using eq. and the matrix inversion lemma. alternatively qui nonero-candela suggests using the approximate log marginal likelihood log psryx eq. as the selection criterion. in fact the quadratic term from eq. is one of the terms comprising log psryx. for all these suggestions the complexity of evaluating the criterion on a new example is omn by making use of partitioned matrix equations. thus it is likely to be too expensive to consider all points in r on each iteration and we are likely to want to consider a smaller working set as described in algorithm note that the sr model is obtained by selecting some subset of the datapoints of size m in a random or greedy manner. the relevance vector machine described in section has a similar flavour in that it automatically selects a greedy fashion which datapoints to use in its expansion. however note one important difference which is that the rvm uses a diagonal prior on the s while for the sr method we have m n k mm. the nystr om method williams and seeger suggested approximating the gpr equations by replacing the matrix k by k in the mean and variance prediction equations and and called this the nystr om method for approximate gpr. notice that in this proposal the covariance function k is not systematically replaced by k it is only occurrences of the matrix k that are replaced. as for the sr model the time complexity is to carry out the necessary matrix computations and then on for the predictive mean of a test point and omn for the predictive variance. experimental evidence in williams et al. suggests that for large m the sr and nystr om methods have similar performance but for small m the nystr om method can be quite poor. also the fact that k is not systematically replaced by k means that embarrassments can occur like the approximated predictive variance being negative. for these reasons we do not recommend the nystr om method over the sr method. however the nystr om method can be effective when the eigenvalue of k is much smaller than n. subset of datapoints the subset of regressors method described above approximated the form of the predictive distribution and particularly the predictive mean. another simple approximation to the full-sample gp predictor is to keep the gp predictor but only on a smaller subset of size m of the data. although this is clearly wasteful of data it can make sense if the predictions obtained with m points are sufficiently accurate for our needs. clearly it can make sense to select which points are taken into the active set i and typically this is achieved by greedy algorithms. however one has to be c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets wary of the amount of computation that is needed especially if one considers each member of r at each iteration. lawrence et al. suggest choosing as the next point site for inclusion into the active set the one that maximizes the differential entropy score j hpfj hpnewfj where hpfj is the entropy of the gaussian at site j r is a function of the variance at site j as the posterior is gaussian see eq. and hpnewfj is the entropy at this site once the observation at site j has been included. let the posterior variance of fj before inclusion be vj. as pfjyi yj pfjyin we have j using the fact that the entropy of a gaussian with variance v is we obtain v j ivm j vj j is a monotonic function of vj so that it is maximized by choosing the site with the largest variance. lawrence et al. call their method the informative vector machine if coded na vely the complexity of computing the variance at all sites in r on a single iteration is as we need to evaluate eq. at ni can be done once in each site the matrix inversion of kmm then stored. however as we are incrementally growing the matrices kmm and kmn m in fact the cost is omn per inclusion leading to an overall complexity of when using a subset of size m. for example once a site has been chosen for inclusion the matrix kmm ni is grown by including an extra row and column. the inverse of this expanded matrix can be found using eq. although it would be better practice numerically to use a cholesky decomposition approach as described in lawrence et al. the scheme evaluates j over all j r at each step to choose the inclusion site. this makes sense when m is small but as it gets larger it can make sense to select candidate inclusion sites from a subset of r. lawrence et al. call this the randomized greedy selection method and give further ideas on how to choose the subset. the differential entropy score j is not the only criterion that can be used for site selection. for example the information gain criterion klpnewfjpfj can also be used seeger et al. the use of greedy selection heuristics here is similar to the problem of active learning see e.g. mackay projected process approximation the sr method has the unattractive feature that it is based on a degenerate gp the finite-dimensional model given in eq. the sd method is a nondegenerate process model but it only makes use of m datapoints. the projected process approximation is also a non-degenerate process model but it can make use of all n datapoints. we call it a projected process approximation as it represents only m n latent function values but computes a likelihood involving all n datapoints by projecting up the m latent points to n dimensions. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximations for gpr with fixed hyperparameters one problem with the basic gpr algorithm is the fact that the likelihood term requires us to have f-values for the n training points. however say we only represent m of these values explicitly and denote these as fm. then the remaining f-values in r denoted fn m have a conditional distribution pfn mfm the mean of which is given by efn mfm kn mmk say we replace the true likelihood term for the points in r by n mefn mfm ni. including also the likelihood contribution of the points in set i we have qyfm n ni mmfm which can also be written as qyfm n ni. the key feature here is that we have absorbed the information in all n points of d into the m points in i. the form of qyfm in eq. might seem rather arbitrary but in fact it can be shown that if we consider minimizing klqfypfy the kldivergence between the approximating distribution qfy and the true posterior pfy over all q distributions of the form qfy pfrfm where r is positive and depends on fm only this is the form we obtain. see seeger lemma and sec. for detailed derivations and also csat o sec. to make predictions we first have to compute the posterior distribution mmkmn so that effm p then qfmy. define the shorthand p k we have qyfm p p n qfmy combining this with the prior pfm exp f mk p p which can be recognized as a gaussian n a with a n n ap y kmm mm p p f mk nk nkmm kmnknm mm n we obtain yp n n k mm nkmm kmnknmk mm thus the predictive mean is given by eqfx kmx kmx which turns out to be just the same as the predictive mean under the sr model as given in eq. however the predictive variance is different. the argument is the same as in eq. and yields mm nkmm kmnknm vqfx kx x kmx kmx mmkmx mmcovfmyk mmkmx kx x kmx nkmx mmkmx nkmm kmnknm is no a priori reason why the m points chosen have to be a subset of the n points in d they could be disjoint from the training set. however for our derivations below we will consider them to be a subset. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets notice that predictive variance is the sum of the predictive variance under the sr model term in eq. plus kx x kmx mmkmx which is the predictive variance at x given fm. thus eq. is never smaller than the sr predictive variance and will become close to kx x when x is far away from the points in set i. as for the sr model it takes time to carry out the necessary matrix computations. after this the prediction of the mean for a new test point takes time om and the predictive variance takes we have qyfm n ni and pfm n kmm. by integrating out fm we find that y n k nin. thus the marginal likelihood for the projected process approximation is the same as that for the sr model eq. again the question of how to choose which points go into the set i arises. csat o and opper present a method in which the training examples are presented sequentially an on-line fashion. given the current active set i one can compute the novelty of a new input point if this is large then this point is added to i otherwise the point is added to r. to be precise the novelty of an input x is computed as kx x kmxk mmkx which can be recognized as the predictive variance at x given non-noisy observations at the points in i. if the active set gets larger than some preset maximum size then points can be deleted from i as specified in section of csat o and opper later work by csat o et al. replaced the dependence of the algorithm described above on the input sequence by an expectation-propagation type algorithm section as an alternative method for selecting the active set seeger et al. suggest using a greedy subset selection method as per algorithm computation of the information gain criterion after incorporating a new site takes omn and is thus too expensive to use as a selection criterion. however an approximation to the information gain can be computed cheaply seeger et al. eq. and seeger sec. for further details and this allows the greedy subset algorithm to be run on all points in r on each iteration. bayesian committee machine tresp introduced the bayesian committee machine as a way of speeding up gaussian process regression. let f be the vector of function values at the test locations. under gpr we obtain a predictive gaussian distribution for pf for the bcm we split the dataset into p parts qp where di yi and make the approximation that ypf x qp pyif xi. under this approximation we have pf pyif xi c pp qf pf py where c is a normalization constant. using the fact that the terms in the numerator and denomination are all gaussian distributions over f it is easy c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximations for gpr with fixed hyperparameters to show exercise that the predictive mean and covariance for f are given by eqf px px where k is the covariance matrix evaluated at the test points. here ef and covf are the mean and covariance of the predictions for f given di as given in eqs. and note that eq. has an interesting form in that the predictions from each part of the dataset are weighted by the inverse predictive covariance. we are free to choose how to partition the dataset d. this has two aspects the number of partitions and the assignment of data points to the partitions. if we wish each partition to have size m then p nm. tresp used a random assignment of data points to partitions but schwaighofer and tresp recommend that clustering the data with p-means clustering can lead to improved performance. however note that compared to the greedy schemes used above clustering does not make use of the target y values only the inputs x. although it is possible to make predictions for any number of test points n this slows the method down as it involves the inversion of n n matrices. schwaighofer and tresp recommend making test predictions on blocks of size m so that all matrices are of the same size. in this case the computational complexity of bcm is for predicting m test points or omn per test point. the bcm approach is transductive rather than inductive in the sense that the method computes a test-set dependent model making use of the test set input locations. note also that if we wish to make a prediction at just one test point it would be necessary to hallucinate some extra test points as eq. generally becomes a better approximation as the number of test points increases. iterative solution of linear systems one straightforward method to speed up gp regression is to note that the linear system niv y can be solved by an iterative method for example conjugate gradients golub and van loan sec. for further details on the cg method. conjugate gradients gives the exact solution round-off errors if run for n iterations but it will give an approximate solution if terminated earlier say after k iterations with time complexity this method has been suggested by wahba et al. the context of numerical weather prediction and by gibbs and mackay the context of general gp regression. cg methods have also been used in the context c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets method m sd sr pp bcm smse msll mean runtime table test results on the inverse dynamics problem for a number of different methods. ten repetitions were used the mean loss is shown one standard deviation. of laplace gpc where linear systems are solved repeatedly to obtain the map solution f sections and for details. one way that the cg method can be speeded up is by using an approximate rather than exact matrix-vector multiplication. for example recent work by yang et al. uses the improved fast gauss transform for this purpose. comparison of approximate gpr methods above we have presented six approximation methods for gpr. of these we retain only those methods which scale linearly with n so the iterative solution of linear systems must be discounted. also we discount the nystr om approximation in preference to the sr method leaving four alternatives subset of regressors subset of data projected process and bayesian committee machine table shows results of the four methods on the robot arm inverse dynamics problem described in section which has d input variables training examples and test examples. as in section we used the squared exponential covariance function with a separate length-scale parameter for each of the input dimensions. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximations for gpr with fixed hyperparameters method sd sr pp bcm storage omn omn omn variance initialization mean om om om omn omn table a comparison of the space and time complexity of the four methods using random selection of subsets. initialization gives the time needed to carry out preliminary matrix computations before the test point x is known. mean variance refers to the time needed to compute the predictive mean at x for the sd method a subset of the training data of size m was selected at random and the hyperparameters were set by optimizing the marginal likelihood on this subset. as ard was used this involved the optimization of d hyperparameters. this process was repeated times giving rise to the mean and standard deviation recorded in table for the sr pp and bcm methods the same subsets of the data and hyperparameter vectors were used as had been obtained from the sd note that the m result is not available for bcm as this gave an out-of-memory error. these experiments were conducted on a ghz twin processor machine with gb of ram. the code for all four methods was written in a summary of the time complexities for the four methods are given in table thus for a test set of size n and using full and variance predictions we find that the sd method has time complexity for the sr and pp methods it is and for the bcm method it is omnn assuming that n m these reduce to and omnn respectively. these complexities are in broad agreement with the timings in table the results from table are plotted in figure as we would expect the general trend is that as m increases the smse and msll scores decrease. notice that it is well worth doing runs with small m so as to obtain a learning curve with respect to m this helps in getting a feeling of how useful runs at large m will be. both in terms of smse and msll we see surprisingly that sd is inferior to the other methods all of which have similar performance. these results were obtained using a random selection of the active set. some experiments were also carried out using active selection for the sd method and for the sr method but these did not lead to significant improvements in performance. for bcm we also experimented with the use of p-means clustering instead of random assignment to partitions again this did not lead to significant improvements in performance. overall on this dataset our con the bcm case it was only the hyperparameters that were re-used the data was parti tioned randomly into blocks of size m. thank anton schwaighofer for making his bcm code available to us. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets figure panela plot of smse against m. panelb shows the msll for the four methods. the error bars denote one standard deviation. for clarity in both panels the bcm results are slightly displaced horizontally w.r.t. the sr results. clusion is that for fixed m sr or pp are the methods of choice as bcm has longer running times for similar performance. however notice that if we compare on runtime then sd for m is competitive with the sr pp and bcm results for m on both time and performance. in the above experiments the hyperparameters for all methods were set by optimizing the marginal likelihood of the sd model of size m. this means that we get a direct comparison of the different methods using the same hyperparameters and subsets. however one could alternatively optimize the marginal likelihood for each method section and then compare results. notice that the hyperparameters which optimize the approximate marginal likelihood may depend on the method. for example figure shows that the maximum in the marginal likelihood occurs at shorter length-scales as the amount of data increases. this effect has also been observed by v. tresp and a. schwaighofer comm. when comparing the sd marginal likelihood eq. with the full marginal likelihood computed on all n datapoints eq. schwaighofer and tresp report some experimental comparisons between the bcm method and some other approximation methods for a number of synthetic regression problems. in these experiments they optimized the kernel hyperparameters for each method separately. their results are that for fixed m bcm performs as well as or better than the other methods. however these results depend on factors such as the noise level in the data generating process they report comm. that for relatively large noise levels bcm no longer displays an advantage. based on the evidence currently available we are unable to provide firm recommendations for one approximation method over another further research is required to understand the factors that affect performance. and c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximations for gpc with fixed hyperparameters approximations for gpc with fixed hy perparameters the approximation methods for gpc are similar to those for gpr but need to deal with the non-gaussian likelihood as well either by using the laplace approximation see section or expectation propagation see section in this section we focus mainly on binary classification tasks although some of the methods can also be extended to the multi-class case. pm for the subset of regressors method we again use the model fsrx ikx xi with m n k mm. the likelihood is non-gaussian but the optimization problem to find the map value of m is convex and can be obtained using a newton iteration. using the map value m and the hessian at this point we obtain a predictive mean and variance for fx which can be fed through the sigmoid function to yield probabilistic predictions. as usual the question of how to choose a subset of points arises lin et al. select these using a clustering method while zhu and hastie propose a forward selection strategy. the subset of datapoints method for gpc was proposed in lawrence et al. using an ep-style approximation of the posterior and the differential entropy score section to select new sites for inclusion. note that the ep approximation lends itself very naturally to sparsification a sparse model results when some site precisions eq. are zero making the corresponding likelihood term vanish. a computational gain can thus be achieved by ignoring likelihood terms whose site precisions are very small. the projected process approximation can also be used with nongaussian likelihoods. csat o and opper present an online method where the examples are processed sequentially while csat o et al. give an expectation-propagation type algorithm where multiple sweeps through the training data are permitted. the bayesian committee machine has also been generalized to deal with non-gaussian likelihoods in tresp as in the gpr case the dataset is broken up into blocks but now approximate inference is carried out using the laplace approximation in each block to yield an approximate predictive mean eqf and approximate predictive covariance covqf these predictions are then combined as before using equations and approximating the marginal likelihood and its derivatives we consider approximations first for gp regression and then for gp classification. for gpr both the sr and pp methods give rise to the same approximate marginal likelihood as given in eq. for the sd method a very simple c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets y log mkmm m approximation the datapoints not in the active set is given by log psdymxm where ym is the subvector of y corresponding to the active set eq. is simply the log marginal likelihood under the model ym n kmm for the bcm a simple approach would be to sum eq. evaluated on each partition of the dataset. this ignores interactions between the partitions. tresp and schwaighofer comm. have suggested a more sophisticated bcm-based method which approximately takes these interactions into account. for gpc under the sr approximation one can simply use the laplace or ep approximations on the finite-dimensional model. for sd one can again ignore all datapoints not in the active set and compute an approximation to log pymxm using either laplace or ep. for the projected process method seeger p. suggests the following lower bound z qf pyfpf qf df z z z z nx log pyx log pyfpf df log pyfpf qf qf log df qf log qyf df klqfpf qfi log pyifi dfi klqfmpfm where qf is a shorthand for qfy and eq. follows from the equation on the previous line using jensen s inequality. the kl divergence term can be readily evaluated using eq. and the one-dimensional integrals can be tackled using numerical quadrature. we are not aware of work on extending the bcm approximations to the marginal likelihood to gpc. given the various approximations to the marginal likelihood mentioned above we may also want to compute derivatives in order to optimize it. clearly it will make sense to keep the active set fixed during the optimization although note that this clashes with the fact that methods that select the active set might choose a different set as the covariance function parameters change. for the classification case the derivatives can be quite complex due to the fact that site parameters as the map values f see section change as changes. have already seen an example of this in section for the non-sparse laplace approximation. seeger sec. describes some experiments comparing sd and pp methods for the optimization of the marginal likelihood on both regression and classification problems. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn appendix equivalence of sr and gpr using the nystr om approximate kernel appendix equivalence of sr and gpr us- ing the nystr om approximate kernel in section we derived the subset of regressors predictors for the mean and variance as given in equations and the aim of this appendix is to show that these are equivalent to the predictors that are obtained by replacing kx systematically with kx in the gpr prediction equations and first for the mean. the gpr predictor is efx kx replacing all occurrences of kx with kx we obtain ni ni e fx kx k kmx kmx n kmx n kmx n kmx mm mm mmkmn mmkmnknmk ni mmkmn knmq y kmnknmq kmny nkmmq kmny nkmm kmnknm which agrees with eq. equation where q follows from eq. by use of the matrix inversion lemma eq. and nkmm kmnknmq for eq. follows from eq. using im the predictive variance we have v f kx x kx k ni kx kmx kmx kmx mmkmx mmkmnknmk mmkmx kmx mmkmn ni kmx q k mmkmx kmx nkmmk nkmx mmkmx mmkmx mmkmx in agreement with eq. the step between eqs. and is obtained from eqs. and above and eq. follows from eq. using im nkmm kmnknmq exercises verify that the mean and covariance of the bcm predictions and are correct. if you are stuck see tresp for details. mm show that ecopt trk k where k knmk mmkmn. now consider adding one datapoint into set i so that kmm grows to using eq. using eq. and the fact that copt knmk c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn approximation methods for large datasets show that the change in e due to adding the extra datapoint can be computed in time omn. if you need help see sch olkopf and smola sec. for further details. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn chapter further issues and conclusions in the previous chapters of the book we have concentrated on giving a solid grounding in the use of gps for regression and classification problems including model selection issues approximation methods for large datasets and connections to related models. in this chapter we provide some short descriptions of other issues relating to gaussian process prediction with pointers to the literature for further reading. so far we have mainly discussed the case when the output target y is a single label but in section we describe how to deal with the case that there are multiple output targets. similarly for the regression problem we have focussed on i.i.d. gaussian noise in section we relax this condition to allow the noise process to have correlations. the classification problem is characterized by a non-gaussian likelihood function however there are other non-gaussian likelihoods of interest as described in section we may not only have observations of function values by also on derivatives of the target function. in section we discuss how to make use of this information in the gpr framework. also it may happen that there is noise on the observation of the input variable x in section we explain how this can be handled. in section we mention how more flexible models can be obtained using mixtures of gaussian process models. as well as carrying out prediction for test inputs one might also wish to try to find the global optimum of a function within some compact set. approaches based on gaussian processes for this problem are described in section the use of gaussian processes to evaluate integrals is covered in section by using a scale mixture of gaussians construction one can obtain a multivariate student s t distribution. this construction can be extended to give a student s t process as explained in section one key aspect of the bayesian framework relates to the incorporation of prior knowledge into the problem c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn further issues and conclusions formulation. in some applications we not only have the dataset d but also additional information. for example for an optical character recognition problem we know that translating the input pattern by one pixel will not change the label of the pattern. approaches for incorporating this knowledge are discussed in section in this book we have concentrated on supervised learning problems. however gps can be used as components in unsupervised learning models as described in section finally we close with some conclusions and an outlook to the future in section multiple outputs throughout this book we have concentrated on the problem of predicting a single output variable y from an input x. however it can happen that one may wish to predict multiple output variables channels simultaneously. for example in the robot inverse dynamics problem described in section there are really seven torques to be predicted. a simple approach is to model each output variable as independent from the others and treat them separately. however this may lose information and be suboptimal. one way in which correlation can occur is through a correlated noise process. even if the output channels are a priori independent if the noise process is correlated then this will induce correlations in the posterior processes. such a situation is easily handled in the gp framework by considering the joint block-diagonal prior over the function values of each channel. another way that correlation of multiple channels can occur is if the prior already has this structure. for example in geostatistical situations there may be correlations between the abundances of different ores e.g. silver and lead. this situation requires that the covariance function models not only the correlation structure of each channel but also the cross-correlations between channels. some work on this topic can be found in the geostatistics literature under the name of cokriging see e.g. cressie sec. one way to induce correlations between a number of output channels is to obtain them as linear combinations of a number of latent channels as described in teh et al. see also micchelli and pontil a related approach is taken by boyle and frean who introduce correlations between two processes by deriving them as different convolutions of the same underlying white noise process. noise models with dependencies the noise models used so far have almost exclusively assumed gaussianity and independence. non-gaussian likelihoods are mentioned in section below. inside the family of gaussian noise models it is not difficult to model dependencies. this may be particularly useful in models involving time. we simply add terms to the noise covariance function with the desired structure including cokriging coloured noise c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn arma non-gaussian likelihoods hyperparameters. in fact we already used this approach for the atmospheric carbon dioxide modelling task in section also murray-smith and girard have used an autoregressive moving-average noise model also eq. in a gp regression task. non-gaussian likelihoods our main focus has been on regression with gaussian noise and classification using the logistic or probit response functions. however gaussian processes can be used as priors with other likelihood functions. for example diggle et al. were concerned with modelling count data measured geographically using a poisson likelihood with a spatially varying rate. they achieved this by placing a gp prior over the log poisson rate. goldberg et al. stayed with a gaussian noise model but introduced heteroscedasticity i.e. allowing the noise variance to be a function of x. this was achieved by placing a gp prior on the log variance function. neal robustified gp regression by using a student s t-distributed noise model rather than gaussian noise. chu and ghahramani have described how to use gps for the ordinal regression problem where one is given ranked preference information as the target data. derivative observations since differentiation is a linear operator the derivative of a gaussian process is another gaussian process. thus we can use gps to make predictions about derivatives and also to make inference based on derivative information. in general we can make inference based on the joint gaussian distribution of function values and partial derivatives. a covariance function k on function values implies the following covariance between function values and partial derivatives and between partial derivatives kxi xj xdj fj xdj fi xdi fj xej xj xdi xej see e.g. papoulis ch. or adler sec. with n datapoints in d dimensions the complete joint distribution of f and its d partial derivatives involves quantities but in a typical application we may only have access to or interest in a subset of these we simply remove the rows and columns from the joint matrix which are not needed. observed function values and derivatives may often have different noise levels which are incorporated by adding a diagonal contribution with differing hyperparameters. inference and predictions are done as usual. this approach was used in the context of learning in dynamical systems by solak et al. in figure the posterior process with and without derivative observations are compared. noise-free derivatives may be a useful way to enforce known constraints in a modelling problem. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn further issues and conclusions figure in panel we show four data points in a one dimensional noise-free regression problem together with three functions sampled from the posterior and the confidence region in light grey. in panel the same observations have been augmented by noise-free derivative information indicated by small tangent segments at the data points. the covariance function is the squared exponential with unit process variance and unit length-scale. prediction with uncertain inputs it can happen that the input values to a prediction problem can be uncertain. for example for a discrete time series one can perform multi-step-ahead predictions by iterating one-step-ahead predictions. however if the one-stepahead predictions include uncertainty then it is necessary to propagate this uncertainty forward to get the proper multi-step-ahead predictions. one simple approach is to use sampling methods. alternatively it may be possible to use analytical approaches. girard et al. showed that it is possible to compute the mean and variance of the output analytically when using the se covariance function and gaussian input noise. more generally the problem of regression with uncertain inputs has been studied in the statistics literature under the name of errors-in-variables regression. see dellaportas and stephens for a bayesian treatment of the problem and pointers to the literature. mixtures of gaussian processes in chapter we have seen many ideas for making the covariance functions more flexible. another route is to use a mixture of different gaussian process models each one used in some local region of input space. this kind of model is generally known as a mixture of experts model and is due to jacobs et al. in addition to the local expert models the model has a manager that assigns points to the experts. rasmussen and ghahramani used gaussian process models as local experts and based their manager on another type of stochastic process the dirichlet process. inference in this model required mcmc methods. xoutput yx xoutput yx c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn global optimization global optimization often one is faced with the problem of being able to evaluate a continuous function gx and wishing to find the global optimum or minimum of this function within some compact set a rd. there is a very large literature on the problem of global optimization see neumaier for a useful overview. given a dataset d n one appealing approach is to fit a gp regression model to this data. this will give a mean prediction and predictive variance for every x a. jones examines a number of criteria that have been suggested for where to make the next function evaluation based on the predictive mean and variance. one issue with this approach is that one may need to search to find the optimum of the criterion which may itself be multimodal optimization problem. however if evaluations of g are expensive or time-consuming it can make sense to work hard on this new optimization problem. for historical references and further work in this area see jones and ritter sec. evaluation of integrals another interesting and unusual application of gaussian processes is for the evaluation of the integrals of a deterministic function f. one evaluates the function at a number of locations and then one can use a gaussian process as a posterior over functions. this posterior over functions induces a posterior over the value of the integral each possible function from the posterior would give rise to a particular value of the integral. for some covariance functions the squared exponential one can compute the expectation and variance of the value of the integral analytically. it is perhaps unusual to think of the value of the integral as being random it does have one particular deterministic value but it is perfectly in line of bayesian thinking that you treat all kinds of uncertainty using probabilities. this idea was proposed under the name of bayes-hermite quadrature by o hagan and later under the name of bayesian monte carlo in rasmussen and ghahramani another approach is related to the ideas of global optimization in the section above. one can use a gp model of a function to aid an mcmc sampling procedure which may be advantageous if the function of interest is computationally expensive to evaluate. rasmussen combines hybrid monte carlo with a gp model of the log of the integrand and also uses derivatives of the function in section to get an accurate model of the integrand with very few evaluations. combining gps with mcmc c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn scale mixture noise entanglement further issues and conclusions student s t process a student s t process can be obtained by applying the scale mixture of gaussians construction of a student s t distribution to a gaussian process hagan o hagan et al. we divide the covariances by the scalar and put a gamma distribution on with shape and mean so that exp kx p z where k is any valid covariance function. now the joint prior distribution of any finite number n of function values y becomes n py yk y y which is recognized as the zero mean multivariate student s t distribution with degrees of freedom py t we could state a definition analogous to definition on page for the gaussian process and write cf. eq. the marginal likelihood can be directly evaluated using eq. and training can be achieved using the methods discussed in chapter regarding and as hyperparameters. the predictive distribution for test cases are also t distributions the derivation of which is left as an exercise below. f t notice that the above construction is clear for noise-free processes but that the interpretation becomes more complicated if the covariance function kx contains a noise contribution. the noise and signal get entangled by the common factor and the observations can no longer be written as the sum of independent signal and noise contributions. allowing for independent noise contributions removes analytic tractability which may reduce the usefulness of the t process. exercise using the scale mixture representation from eq. derive the posterior predictive distribution for a student s t process. exercise consider the generating process implied by eq. and write a program to draw functions at random. characterize the difference between the student s t process and the corresponding gaussian process in the limit and explain why the t process is perhaps not as exciting as one might have hoped. invariances it can happen that the input is apparently in vector form but in fact it has additional structure. a good example is a pixelated image where the array c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn invariances of pixels can be arranged into a vector in raster-scan order. imagine that the image is of a handwritten digit then we know that if the image is translated by one pixel it will remain the same digit. thus we have knowledge of certain invariances of the input pattern. in this section we describe a number of ways in which such invariances can be exploited. our discussion is based on sch olkopf and smola ch. prior knowledge about the problem tells us that certain transformations of the input would leave the class label invariant these include simple geometric transformations such as translations rescalings and rather less obvious ones such as line thickness given enough data it should be possible to learn the correct input-output mapping but it would make sense to try to make use of these known invariances to reduce the amount of training data needed. there are at least three ways in which this prior knowledge has been used as described below. the first approach is to generate synthetic training examples by applying valid transformations to the examples we already have. this is simple but it does have the disadvantage of creating a larger training set. as kernel-machine training algorithms typically scale super-linearly with n this can be problematic. a second approach is to make the predictor invariant to small transformations of each training case this method was first developed by simard et al. for neural networks under the name of tangent prop for a single training image we consider the manifold of images that are generated as various transformations are applied to it. this manifold will have a complex structure but locally we can approximate it by a tangent space. the idea in tangent prop is that the output should be invariant to perturbations of the training example in this tangent space. for neural networks it is quite straightforward to modify the training objective function to penalize deviations from this invariance see simard et al. for details. section in sch olkopf and smola describes some ways in which these ideas can be extended to kernel machines. the third approach to dealing with invariances is to develop a representation of the input which is invariant to some or all of the transformations. for example binary images of handwritten digits are sometimes skeletonized to remove the effect of line thickness. if an invariant representation can be achieved for all transformations it is the most desirable but it can be difficult or perhaps impossible to achieve. for example if a given training pattern can belong to more than one class an ambiguous handwritten digit then it is clearly not possible to find a new representation which is invariant to transformations yet leaves the classes distinguishable. digit recognition problem is only invariant to small rotations we must avoid turning a into a changing the thickness of the pen we write with within reasonable bounds does not change the digit we write. synthetic training examples tangent prop invariant representation c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn further issues and conclusions latent variable models gtm gplvm our main focus in this book has been on supervised learning. however gps have also been used as components for models carrying out non-linear dimensionality reduction a form of unsupervised learning. the key idea is that data which is apparently high-dimensional a pixelated image may really lie on a low-dimensional non-linear manifold which we wish to model. let z rl be a latent hidden variable and let x rd be a visible variable. we suppose that our visible data is generated by picking a point in z-space and mapping this point into the data space through a non linear mapping and then optionally adding noise. thus px pxzpzdz. if the mapping from z to x is linear and z has a gaussian distribution then this is the factor analysis model and the mean and covariance of the gaussian in x-space can easily be determined. however if the mapping is non-linear then the integral cannot be computed exactly. in the generative topographic mapping model et al. the integral was approximated using a grid of points in z-space. in the original gtm paper the non-linear mapping was taken to be a linear combination of non-linear basis functions but in bishop et al. this was replaced by a gaussian process mapping between the latent and visible spaces. more recently lawrence has introduced a rather different model known as the gaussian process latent variable model instead of having a prior thus a posterior distribution over the latent space we consider that each data point xi is derived from a corresponding latent point zi through a non-linear mapping added noise. if a gaussian process is used for this non-linear mapping then one can easily write down the joint distribution pxz of the visible variables conditional on the latent variables. optimization routines can then be used to find the locations of the latent points that optimize pxz. this has some similarities to the work on regularized principal manifolds olkopf and smola ch. except that in the gplvm one integrates out the latent-to-visible mapping rather than optimizing it. conclusions and future directions in this section we briefly wrap up some of the threads we have developed throughout the book and discuss possible future directions of work on gaussian processes. in chapter we saw how gaussian process regression is a natural extension of bayesian linear regression to a more flexible class of models. for gaussian noise the model can be treated analytically and is simple enough that the gp model could be often considered as a replacement for the traditional linear analogue. we have also seen that historically there have been numerous ideas along the lines of gaussian process models although they have only gained a sporadic following. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn conclusions and future directions one may indeed speculate why are gps not currently used more widely in applications? we see three major reasons firstly that the application of gaussian processes requires the handling of large matrices. while these kinds of computations were tedious years ago and impossible further in the past even na ve implementations suffice for moderate sized problems on an anno pc. another possibility is that most of the historical work on gps was done using fixed covariance functions with very little guide as to how to choose such functions. the choice was to some degree arbitrary and the idea that one should be able to infer the structure or parameters of the covariance function as we discuss in chapter is not so well known. this is probably a very important step in turning gps into an interesting method for practitioners. the viewpoint of placing gaussian process priors over functions is a bayesian one. although the adoption of bayesian methods in the machine learning community is quite widespread these ideas have not always been appreciated more widely in the statistics community. although modern computers allow simple implementations for up to a few thousand training cases the computational constraints are still a significant limitation for applications where the datasets are significantly larger than this. in chapter we have given an overview of some of the recent work on approximations for large datasets. although there are many methods and a lot of work is currently being undertaken both the theoretical and practical aspects of these approximations need to be understood better in order to be a useful tool to the practitioner. the computations required for the gaussian process classification models developed in chapter are a lot more involved than for regression. although the theoretical foundations of gaussian process classification are well developed it is not yet clear under which circumstances one would expect the extra work and approximations associated with treating a full probabilistic latent variable model to pay off. the answer may depend heavily on the ability to learn meaningful covariance functions. the incorporation of prior knowledge through the choice and parameterization of the covariance function is another prime target for future work on gps. in chapter we have presented many families of covariance functions with widely differing properties and in chapter we presented principled methods for choosing between and adapting covariance functions. particularly in the machine learning community there has been a tendency to view gaussian processes as a black box what exactly goes on in the box is less important as long as it gives good predictions. to our mind we could perhaps learn something from the statisticians here and ask how and why the models work. in fact the hierarchical formulation of the covariance functions with hyperparameters the testing of different hypotheses and the adaptation of hyperparameters gives an excellent opportunity to understand more about the data. we have attempted to illustrate this line of thinking with the carbon dioxide prediction example developed at some length in section although this problem is comparatively simple and very easy to get an intuitive understanding of the principles of trying out different components in the covariance structure c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn further issues and conclusions and adapting their parameters could be used universally. indeed the use of the isotropic squared exponential covariance function in the digit classification examples in chapter is not really a choice which one would expect to provide very much insight to the classification problem. although some of the results presented are as good as other current methods in the literature one could indeed argue that the use of the squared exponential covariance function for this task makes little sense and the low error rate is possibly due to the inherently low difficulty of the task. there is a need to develop more sensible covariance functions which allow for the incorporation of prior knowledge and help us to gain real insight into the data. going beyond a simple vectorial representation of the input data to take into account structure in the input domain is also a theme which we see as very important. examples of this include the invariances described in section arising from the structure of images and the kernels described in section which encode structured objects such as strings and trees. as this brief discussion shows we see the current level of development of gaussian process models more as a rich principled framework for supervised learning than a fully-developed set of tools for applications. we find the gaussian process framework very appealing and are confident that the near future will show many important developments both in theory methodology and practice. we look forward very much to following these developments. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn appendix a mathematical background joint marginal and conditional probability let the n or continuous random variables yn have a joint probability yn or py for technically one ought to distinguish between probabilities discrete variables and probability densities for continuous variables. throughout the book we commonly use the term probability to refer to both. let us partition the variables in y into two groups ya and yb where a and b are two disjoint sets whose union is the set n so that py pya yb. each group may contain one or more variables. the marginal probability of ya is given by z pya pya yb dyb. the integral is replaced by a sum if the variables are discrete valued. notice that if the set a contains more than one variable then the marginal probability is itself a joint probability whether it is referred to as one or the other depends on the context. if the joint distribution is equal to the product of the marginals then the variables are said to be independent otherwise they are dependent. the conditional probability function is defined as pyayb pya yb pyb defined for pyb as it is not meaningful to condition on an impossible event. if ya and yb are independent then the marginal pya and the conditional pyayb are equal. can deal with more general cases where the density function does not exist by using the distribution function. joint probability marginal probability independence conditional probability c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bayes rule gaussian definition conditioning and marginalizing products mathematical background theorem using the definitions of both pyayb and pybya we obtain bayes pyayb pyapybya pyb since conditional distributions are themselves probabilities one can use all of the above also when further conditioning on other variables. for example in supervised learning one often conditions on the inputs throughout which would lead e.g. to a version of bayes rule with additional conditioning on x in all four probabilities in eq. see eq. for an example of this. gaussian identities the multivariate gaussian normal distribution has a joint probability density given by pxm m where m is the mean vector length d and is the positive definite covariance matrix size d d. as a shorthand we write x n let x and y be jointly gaussian random vectors x y a x y n c c b n x y a c c b then the marginal distribution of x and the conditional distribution of x given y are x n x a and xy x cb y a cb or xy x a cy y a see e.g. von mises sec. and eqs. the product of two gaussians gives another gaussian n an b z c where c ca b and c b notice that the resulting gaussian has a precision variance equal to the sum of the precisions and a mean equal to the convex sum of the means weighted by the precisions. the normalizing constant looks itself like a gaussian a or b z b ba b to prove eq. simply write out the expressions by introducing eq. and eq. into eq. and expand the terms inside the exp to c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn matrix identities generating multivariate gaussian samples verify equality. hint it may be helpful to expand c using the matrix inversion lemma eq. c a aab b bab to generate samples x n k with arbitrary mean m and covariance matrix k using a scalar gaussian generator is readily available in many programming environments we proceed as follows first compute the cholesky decomposition known as the matrix square root l of the positive definite symmetric covariance matrix k ll where l is a lower triangular matrix see section then generate u n i by multiple separate calls to the scalar gaussian generator. compute x m lu which has the desired distribution with mean m and covariance leuul ll k the independence of the elements of u. in practice it may be necessary to add a small multiple of the identity matrix to the covariance matrix for numerical reasons. this is because the eigenvalues of the matrix k can decay very rapidly section for a closely related analytical result and without this stabilization the cholesky decomposition fails. the effect on the generated samples is to add additional independent noise of variance from the context can usually be chosen to have inconsequential effects on the samples while ensuring numerical stability. matrix identities the matrix inversion lemma also known as the woodbury sherman morri- matrix inversion lemma son formula e.g. press et al. p. states that u w v z z v assuming the relevant inverses all exist. here z is n n w is m m and u and v are both of size n m consequently if z is known and a low rank m n perturbation is made to z as in left hand side of eq. considerable speedup can be achieved. a similar equation exists for determinants u w v v let the invertible n n matrix a and its inverse a be partitioned into p q r s p q r s a a where p and p are matrices and s and s are matrices with n the submatrices of a are given in press et al. p. as p p p rp q p r m rp s m where m rp determinants inversion of a partitioned matrix c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn mathematical background or equivalently p n q n qs r s s s s qs where n qs matrix derivatives derivatives of the elements of an inverse matrix k k k k where k positive definite symmetric matrix we have is a matrix of elementwise derivatives. for the log determinant of a log k derivative of inverse derivative of log determinant matrix norms the frobenius norm kakf of a matrix a is defined as f traa and van loan p. cholesky decomposition the cholesky decomposition of a symmetric positive definite matrix a decomposes a into a product of a lower triangular matrix l and its transpose ll a where l is called the cholesky factor. the cholesky decomposition is useful for solving linear systems with symmetric positive definite coefficient matrix a. to solve ax b for x first solve the triangular system ly b by forward substitution and then the triangular system lx y by back substitution. using the backslash operator we write the solution as x llb where the notation ab is the vector x which solves ax b. both the forward and backward substitution steps require operations when a is of size n n. the computation of the cholesky factor l is considered numerically extremely stable and takes time so it is the method of choice when it can be applied. solving linear systems computational cost c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn entropy and kullback-leibler divergence ny nx note also that the determinant of a positive definite symmetric matrix can be calculated efficiently by ii or log log lii where l is the cholesky factor from a. entropy and kullback-leibler divergence the entropy hpx of a distribution px is a non-negative measure of the amount of uncertainty in the distribution and is defined as hpx px log px dx. z the integral is substituted by a sum for discrete variables. entropy is measured in bits if the log is to the base and in nats in the case of the natural log. the entropy of a gaussian in d dimensions measured in nats is hn the kullback-leibler divergence relative entropy klpq be e. tween two distributions px and qx is defined as px log px klpq qx dx. log d z it is easy to show that klpq with equality if p q everywhere. for the case of two bernoulli random variables p and q this reduces to klberpq p log p q p log p q where we use p and q both as the name and the parameter of the bernoulli distributions. for two gaussian distributions n and n we have sec. consider a general distribution px on rd and a gaussian distribution qx n then klpq dx z log px log px dx. log tr z log d determinant entropy divergence of bernoulli random variables divergence of gaussians minimizing klpq divergence leads to moment matching c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn mathematical background equation can be minimized w.r.t. and by differentiating w.r.t. these parameters and setting the resulting expressions to zero. the optimal q is the one that matches the first and second moments of p. the kl divergence can be viewed as the extra number of nats needed on average to code data generated from a source px under the distribution qx as opposed to px. limits the limit of a rational quadratic is a squared exponential lim measure and integration here we sketch some definitions concerning measure and integration fuller treatments can be found e.g. in doob and bartle let be the set of all possible outcomes of an experiment. for example for a d-dimensional real-valued variable rd. let f be a of subsets of which contains all the events in whose occurrences we may be then is a countably additive measure if it is real and non-negative and for all mutually disjoint sets f we have ai x finite measure probability measure lebesgue measure if then is called a finite measure and if it is called a probability measure. the lebesgue measure defines a uniform measure over subsets of euclidean space. here an appropriate is the borel bd where b is the generated by the open subsets of r. for example on the line r the lebesgue measure of the interval b is b a. we now restrict to be rd and wish to give meaning to integration of a function f rd r with respect to a measure z fx d we assume that f is measurable i.e. that for any borel-measurable set a r f bd. there are two cases that will interest us when is the lebesgue measure and when is a probability measure. for the first case expression reduces to the usual integral notationr fxdx. restriction to a of subsets is important technically to avoid paradoxes such as the banach-tarski paradox. informally we can think of the as restricting consideration to reasonable subsets. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn fourier transforms for a probability measure on x the non-negative function px is called the density of the measure if for all a bd we have px dx. if such a density exists it is uniquely determined almost everywhere i.e. except for sets with measure zero. not all probability measures have densities only distributions that assign zero probability to individual points in x-space can have if px exists then we have z fx d fxpx dx. z a z if does not have a density expression still has meaning by the standard construction of the lebesgue integral. for rd the probability measure can be related to the distribution function f rd which is defined as f xd zd. the distribution function is more general than the density as it is always defined for a given probability measure. a simple example of a random variable which has a distribution function but no density is obtained by the following construction a coin is tossed and with probability p it comes up heads if it comes up heads x is chosen from uniform distribution on otherwise probability p x is set to this distribution has a point mass atom at x point mass example lp spaces let be a measure on an input set x for some function f x r and p we define kfklpx d if the integral exists. for p we define kfkl ess sup x x where ess sup denotes the essential supremum i.e. the smallest number that upper bounds almost everywhere. the function space lpx is defined for any p in p as the space of functions for which kfklpx fourier transforms for sufficiently well-behaved functions on rd we have fx is x ds fs fxe is x dx z z measure has a density if and only if it is absolutely continuous with respect to lebesgue measure on rd i.e. every set that has lebesgue measure zero also has zero. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn mathematical background where fs is called the fourier transform of fx see e.g. bracewell we refer to the equation on the left as the synthesis equation and the equation on the right as the analysis equation. there are other conventions for fourier transforms particularly those involving s. however this tends to destroy symmetry between the analysis and synthesis equations so we use the definitions given above. here we have defined fourier transforms for fx being a function on rd. for related transforms for periodic functions functions defined on the integer lattice and on the regular n-polygon see section convexity convex sets convex function below we state some definitions and properties of convex sets and functions taken from boyd and vandenberghe a set c is convex if the line segment between any two points in c lies in c i.e. if for any c and for any with we have a function f x r is convex if its domain x is a convex set and if for all x and with we have c. f where x is a improper subset of rd. f is concave if f is convex. a function f is convex if and only if its domain x is a convex set and its hessian is positive semidefinite for all x x c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn appendix b gaussian markov processes a discrete-time autoregressive process of order p can be written as ar process px particularly when the index set for a stochastic process is one-dimensional such as the real line or its discretization onto the integer lattice it is very interesting to investigate the properties of gaussian markov processes in this appendix we use xt to define a stochastic process with continuous time parameter t. in the discrete time case the process is denoted x etc. we assume that the process has zero mean and is unless otherwise stated stationary. xt akxt k where zt n and all zt s are i.i.d. notice the order-p markov property that given the history xt xt xt depends only on the previous p x s. this relationship can be conveniently expressed as a graphical model part of an process is illustrated in figure the name autoregressive stems from the fact that xt is predicted from the p previous x s through a regression equation. if one stores the current x and the p previous values as a state vector then the arp scalar process can be written equivalently as a vector process. figure graphical model illustrating an process. moving from the discrete time to the continuous time setting the question arises as to how generalize the markov notion used in the discrete-time ar process to define a continuoous-time ar process. it turns out that the correct generalization uses the idea of having not only the function value but also p of its derivatives at time t giving rise to the stochastic differential equation sde stochastic differential equation c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn gaussian markov processes apx ap where x denotes the ith derivative of xt and zt is a white gaussian noise process with covariance this white noise process can be considered the derivative of the wiener process. to avoid redundancy in the coefficients we assume that ap a considerable amount of mathematical machinery is required to make rigorous the meaning of such equations see e.g. ksendal as for the discrete-time case one can write eq. as a first-order vector sde by defining the state to be xt and its first p derivatives. we begin this chapter with a summary of some fourier analysis results in section fourier analysis is important to linear time invariant systems such as equations and because ist is an eigenfunction of the corresponding difference differential operator. we then move on in section to discuss continuous-time gaussian markov processes on the real line and their relationship to the same sde on the circle. in section we describe discrete-time gaussian markov processes on the integer lattice and their relationship to the same difference equation on the circle. in section we explain the relationship between discrete-time gmps and the discrete sampling of continuous-time gmps. finally in section we discuss generalizations of the markov concept in higher dimensions. much of this material is quite standard although the relevant results are often scattered through different sources and our aim is to provide a unified treatment. the relationship between the second-order properties of the sdes on the real line and the circle and difference equations on the integer lattice and the regular polygon is to our knowledge novel. fourier analysis we follow the treatment given by kammler we consider fourier analysis of functions on the real line r of periodic functions of period l on the circle tl of functions defined on the integer lattice z and of functions on pn the regular n-polygon which is a discretization of tl. for sufficiently well-behaved functions on r we have fx isx ds fs fxe isx dx. we refer to the equation on the left as the synthesis equation and the equation on the right as the analysis equation. for functions on tl we obtain the fourier series representations fxe ikxl dx ikxl fk fx x ak coefficients in equations and are not intended to have a close relationship. an approximate relationship might be established through the use of finite-difference approximations to derivatives. z k z z l l c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn fourier analysis z l n where fk denotes the coefficient of ikxl in the expansion. we use square to denote that the argument is discrete so that xt and xt are brackets equivalent notations. similarly for z we obtain fn isnl ds fs x n l fne isnl. note that fs is periodic with period l and so is defined only for s l to avoid aliasing. often this transform is defined for the special case l but the general case emphasizes the duality between equations and finally for functions on pn we have the discrete fourier transform fn iknn fk n fne iknn n note that there are other conventions for fourier transforms particularly those involving s. however this tends to destroy symmetry between the analysis and synthesis equations so we use the definitions given above. in the case of stochastic processes the most important fourier relationship is between the covariance function and the power spectrum this is known as the wiener-khintchine theorem see e.g. chatfield sampling and periodization we can obtain relationships between functions and their transforms on r tl z pn through the notions of sampling and periodization. definition h-sampling given a function f on r and a spacing parameter h we construct a corresponding discrete function on z using similarly we can discretize a function defined on tl onto pn but in this case we must take h ln so that n steps of size h will equal the period l. fnh n z. definition periodization by summation let fx be a function on r that rapidly approaches as x we can sum translates of the function to produce the l-periodic function gx fx ml for l analogously when is defined on z and rapidly approaches as n we can construct a function on pn by n-summation by setting mn. x m x m h-sampling periodization by summation c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn gaussian markov processes let be obtained by h-sampling from fx with corresponding fourier transforms and fs. then we have z fnh isnh ds isnl ds. by breaking up the domain of integration in eq. we obtain z l x x z l z z l ml x x m m x m m m isnh ds using the change of variable s ml. now set hl and use inm for n m integers to obtain fs ml isnl ds which implies that fs ml p with l alternatively setting l one obtains similarly if f is defined on tl and f nl m f sm h n is obtained by sampling then h fn mn. thus we see that sampling in x-space causes periodization in fourier space. periodic function gx now consider the periodization of a function fx with x r to give the lm fx ml. let gk be the fourier coefficients of gx. we obtain z l z gk l l gxe ikxl dx l fxe ikxl dx z l x k m f l l fx mle ikxl dx assuming that fx is sufficiently well-behaved that the summation and integration operations can be exchanged. a similar relationship can be obtained for the periodization of a function defined on z. thus we see that periodization in x-space gives rise to sampling in fourier space. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn continuous-time gaussian markov processes continuous-time gaussian markov processes we first consider continuous-time gaussian markov processes on the real line and then relate the covariance function obtained to that for the stationary solution of the sde on the circle. our treatment of continuous-time gmps on r follows papoulis ch. continuous-time gmps on r we wish to find the power spectrum and covariance function for the stationary process corresponding to the sde given by eq. recall that the covariance function of a stationary process kt and the power spectrum ss form a fourier transform pair. the fourier transform of the stochastic process xt is a stochastic process z xs given by z xs xte ist dt xt ist ds where the integrals are interpreted as a mean-square limit. let denote complex conjugation and h. denote expectation with respect to the stochastic process. then for a stationary gaussian process we have h x hxtx dt z z z d k the delta function r e istdt this shows that and are using the change of variables t and the integral representation of uncorrelated for i.e. that the fourier basis are eigenfunctions of the differential operator. also from eq. we obtain x isk ist ds. now if we fourier transform eq. we obtain isk xs zs z px where zs denotes the fourier transform of the white noise. taking the product of equation with its complex conjugate and taking expectations we obtain px px ak h x z c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn process arp process let az pp spectrum of white noise is we obtain akzk. then using eq. and the fact that the power gaussian markov processes srs note that the denominator is a polynomial of order p in the relationship of stationary solutions of pth-order sdes to rational spectral densities can be traced back at least as far as doob above we have assumed that the process is stationary. however this depends on the coefficients ap. to analyze this issue we assume a solution of the form xt e t when the driving term this leads to the condition ak k must lie in the left for stationarity that the roots of the polynomialpp half plane o p. example process. in this case we have the sde where for stationarity. this gives rise to the power spectrum ss is is taking the fourier transform we obtain kt e to the power spectrum ss this process is known as the ornstein-uhlenbeck process and ornstein and was introduced as a mathematical model of the velocity of a particle undergoing brownian motion. it can be shown that the ou process is the unique stationary first-order gaussian markov process. example arp process. in general the covariance transform corresponding ak isk can be quite complicated. for example papoulis p. gives three forms of is the covariance function for the process depending on whether greater than equal to or less than however if the coefficients ap are chosen in a particular way then one can obtain iskpp ss ance function is of the formpp for some it can be shown p. that the corresponding ktke for some coefficients p e for the ou process. for e these are special cases of the mat ern for p we have already seen that kt p we obtain kt class of covariance functions described in section c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn wiener process continuous-time gaussian markov processes example wiener process. although our derivations have focussed on stationary gaussian markov processes there are also several important non-stationary processes. one of the most important is the wiener process that satisfies the sde zt for t with the initial condition this process has covariance function kt s mint s. an interesting variant of the wiener process known as the brownian bridge tied-down wiener process is obtained by conditioning on the wiener process passing through this has covariance kt s mint s st for s t see e.g. grimmett and stirzaker for further information on these processes. markov processes derived from sdes of order p are p times ms differentiable. this is easy to see heuristically from eq. given that a process gets rougher the more times it is differentiated eq. tells us that x is like the white noise process i.e. not ms continuous. so for example the ou process also the wiener process are ms continuous but not ms differentiable. the solution of the corresponding sde on the cir cle the analogous analysis to that on the real line is carried out on tl using xt intl xn l xte intldt. x n z l as xt is assumed stationary we obtain an analogous result to eq. i.e. that the fourier coefficients are independent sn px h xm x if m n otherwise. p similarly the covariance function on the cirle is given by kt s hxtx x n int sl. let l then plugging in the expression n lk xnein lt into the sde eq. and equating terms in we obtain akin lk xn zn. as in the real-line case we form the product of equation with its complex conjugate and take expectations to give note that stn is equal to n stn i.e. that it is a sampling of sr at intervals where srs is the power spectrum of the continuous process on the real line given in equation let kth denote the covariance function on the l c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn gaussian markov processes circle and krh denote the covariance function on the real line for the sde. then using eq. we find that x m ktt krt ml. order sde example sde. on r for the ou process we have krt by summing the series geometric progressions we obtain ktt e e e l sinh e for l t l. eq. is also given to scaling factors in grenander et al. eq. where it is obtained by a limiting argument from the discrete-time gmp on pn see section discrete-time gaussian markov processes we first consider discrete-time gaussian markov processes on z and then relate the covariance function obtained to that of the stationary solution of the difference equation on pn chatfield and diggle provide good coverage of discrete-time arma models on z. discrete-time gmps on z assuming that the process is stationary the covariance function ki denotes hxtxtii t z. that because of stationarity ki k i. we first use a fourier approach to derive the power spectrum and hence the covariance function of the arp process. defining we can rewrite akxt k the fourier pair for xt is x t istl ds xs l xte istl. akxt k we obtain px ake i xs zs eq. aspp z l xt plugging this intopp where l as above taking the product of eq. with its complex conjugate and taking expectations we obtain szs c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn discrete-time gaussian markov processes above we have assumed that the process is stationary. however this depends on the coefficients ap. to analyze this issue we assume a solution of the form xt zt when the driving term this leads to the condition akzp k must lie inside for stationarity that the roots of the polynomial pp the unit circle. see hannan theorem p. for further details. as well as deriving the covariance function from the fourier transform of the power spectrum it can also be obtained by solving a set of linear equations. our first observation is that xs is independent of zt for s t. multiplying equation through by zt and taking expectations we obtain hxtzti and hxt izti for i by multiplying equation through by xt j for j and taking expectations we obtain the yule-walker equations yule-walker equations process kj aiki aikj i j the first p of these equations form a linear system that can be used to solve for kp in terms of and ap and eq. can be used to obtain kj for j p recursively. example process. the simplest example of an ar process is the process defined as xt this gives rise to the yule-walker equations and x where the linear system for can easily be solved to give kj a is the variance of the process. notice that for the process to x be stationary we require the corresponding power spectrum obtained from equation is ss cos ls similarly to the continuous case the covariance function for the discrete-time process has three different forms depending on these are described in diggle example the solution of the corresponding difference equa tion on pn we now consider variables x xn arranged around the circle with n p. by appropriately modifying eq. we obtain xt akxmodt kn px px px c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn gaussian markov processes pn the zt s are i.i.d. and n thus z zn has density pz exp t equation shows that x and z are related by a linear transformation and thus z px exp n xt px akxmodt kn this is an n-dimensional multivariate gaussian. for an arp process the inverse covariance matrix has a circulant structure consisting of a diagonal band entries wide and appropriate circulant entries in the corners. thus pxtx xt pxtxmodt xmodt pn xmodtpn which geman and geman call the two-sided markov property. notice that it is the zeros in the inverse covariance matrix that indicate the conditional independence structure see also section the properties of eq. have been studied by a number of authors e.g. whittle the name of circulant processes kashyap and chellappa the name of circular autoregressive models and grenander et al. cyclic markov process. as above we define the fourier transform pair xn inmn xm n xne inmn n n px by similar arguments to those above we obtain ak imn zm where and thus spm imn as in the continuous-time case we see that spm is obtained by sampling the power spectrum of the corresponding process on the line so that spm ml n thus using eq. we have x kpn m kzn mn. process example process. for this process xt the diagonal entries in the inverse covariance are and the non-zero offdiagonal entries are by summing the covariance function kzn we obtain x a kpn x an a n n n c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn the relationship between discrete-time and sampled continuous-time gmps we now illustrate this result for n in this case the covariance matrix has diagonal entries of the inverse covariance matrix has the structure described above. multiplying these two matrices together we do indeed obtain the identity matrix. and off-diagonal entries of x x the relationship between discrete-time and sampled continuous-time gmps we now consider the relationship between continuous-time and discrete-time gmps. in particular we ask the question is a regular sampling of a continuoustime arp process a discrete-time arp process? it turns out that the answer will in general be negative. first we define a generalization of ar processes known as autoregressive moving-average processes. arma processes the arp process defined above is a special case of the more general armap q process which is defined as px qx xt aixt i bjzt j. observe that the arp process is in fact also an armap process. a spectral analysis of equation similar to that performed in section gives where bz density of the form ss ss bjzj. in continuous time a process with a rational spectral we require q p as ssds is known as a armap q process. for this to define a valid covariance function discrete-time observation of a continuous-time process let xt be a continuous-time process having covariance function kt and power spectrum ss. let xh be the discrete-time process obtained by sampling xt at interval h so that xhn xnh for n z. clearly the covariance function of this process is given by khn knh. by eq. this means that x m shs ss m h where shs is defined using l c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn gaussian markov processes theorem let x be a continuous-time stationary gaussian process and xh be the discretization of this process. if x is an arma process then xh is also an arma process. however if x is an ar process then xh is not necessarily an ar process. the proof is given in ihara theorem it is easy to see using the covariance functions given in sections and that the discretization of a continuous-time process is indeed a discrete-time process. however ihara shows that in general the discretization of a continuous-time process is not a discrete-time process. markov processes in higher dimensions we have concentrated above on the case where t is one-dimensional. in higher dimensions it is interesting to ask how the markov property might be generalized. let s be an infinitely differentiable closed surface separating rd into a bounded part s and an unbounded part s. loosely a random field xt is said to be quasi-markovian if xt for t s and xu for u s are independent given xs for s s. wong showed that the only isotropic quasi-markov gaussian field with a continuous covariance function is the degenerate case xt where is a gaussian variate. however if instead of conditioning on the values that the field takes on in s one conditions on a somewhat larger set then gaussian random fields with non-trivial markovtype structure can be obtained. for example random fields with an inverse kj pseudo-markovian of order p. for example the d-dimensional tensor-product e iti is pseudo-markovian of order d. for further discussion of markov properties of random fields see the appendix in adler power spectrum of the formp and cs sp of the ou process kt qd d with for some c are said to be k skd skd d if instead of rd we wish to define a markov random field on a graphical structure example the lattice zd things become more straightforward. we follow the presentation in jordan let g e be a graph where x is a set of nodes that are in one-to-one correspondence with a set of random variables and e be the set of undirected edges of the graph. let c be the set of all maximal cliques of g. a potential function cxc is a function on the possible realizations xc of the maximal clique xc. potential functions are assumed to be positive real-valued functions. the probability distribution px corresponding to the markov random field is given by y function obtained by summingintegratingq px c c z cxc where z is a normalization factor in statistical physics as the partition c c cxc over all possible as a precise formulation of this definition involving see adler p. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn markov processes in higher dimensions signments of values to the nodes x. under this definition it is easy to show that a local markov property holds i.e. that for any variable x the conditional distribution of x given all other variables in x depends only on those variables that are neighbours of x. a useful reference on markov random fields is winkler a simple example of a gaussian markov random field has the form x x px exp i i ijj n where ni denotes the set of neighbours of node xi and on one might choose a four-connected neighbourhood i.e. those nodes to the north south east and west of a given node. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn appendix c datasets and code the datasets used for experiments in this book and implementations of the algorithms presented are available for download at the website of the book httpwww.gaussianprocess.orggpml the programs are short stand-alone implementations and not part of a larger package. they are meant to be simple to understand and modify for a desired purpose. some of the programs allow specification of covariance functions from a selection provided or to link in user defined covariance code. for some of the plots code is provided which produces a similar plot as this may be a convenient way of conveying the details. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography abrahamsen p. a review of gaussian random fields and correlation functions. techhttppublications.nr.no p. nical report norwegian computing center oslo norway. rapport.pdf. abramowitz m. and stegun i. a. handbook of mathematical functions. dover new york. pp. adams r. sobolev spaces. academic press new york. p. adler r. j. the geometry of random fields. wiley chichester. pp. amari s. differential-geometrical methods in statistics. springer-verlag berlin. p. ansley c. f. and kohn r. estimation filtering and smoothing in state space models with p. incompletely specified initial conditions. annals of statistics arat o m. linear stochastic systems with constant coefficients. springer-verlag berlin. lecture p. notes in control and information sciences arfken g. mathematical methods for physicists. academic press san diego. pp. xv aronszajn n. theory of reproducing kernels. trans. amer. math. soc. pp. bach f. r. and jordan m. i. kernel independent component analysis. journal of machine p. learning research baker c. t. h. the numerical treatment of integral equations. clarendon press oxford. pp. barber d. and saad d. does extra knowledge necessarily improve generalisation? neural p. computation bartle r. g. the elements of integration and lebesgue measure. wiley new york. p. bartlett p. l. jordan m. i. and mcauliffe j. d. convexity classification and risk bounds. technical report department of statistics university of california berkeley. available from accepted for publication in journal of the american statistical association. p. berger j. o. statistical decision theory and bayesian analysis. springer new york. second pp. edition. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography bishop c. m. neural networks for pattern recognition. clarendon press oxford. p. bishop c. m. svensen m. and williams c. k. i. developments of the generative topographic p. mapping. neurocomputing bishop c. m. svensen m. and williams c. k. i. gtm the generative topographic p. mapping. neural computation blake i. f. and lindsey w. c. level-crossing problems for random processes. ieee trans p. information theory blight b. j. n. and ott l. a bayesian approach to model inadequacy for polynomial regression. p. biometrika boyd s. and vandenberghe l. convex optimization. cambridge university press cambridge p. uk. boyle p. and frean m. dependent gaussian processes. in saul l. k. weiss y. and bottou l. editors advances in neural information processing systems pages mit press. p. bracewell r. n. the fourier transform and its applications. mcgraw-hill singapore interpp. national edition. caruana r. multitask learning. machine learning p. chatfield c. the analysis of time series an introduction. chapman and hall london pp. edition. choi t. and schervish m. j. posterior consistency in nonparametric regression problems under gaussian process priors. technical report department of statistics cmu. p. choudhuri n. ghosal s. and roy a. nonparametric binary regression using a gaussian p. process prior. unpublished. sghosalpapers.html. chu w. and ghahramani z. gaussian processes for ordinal regression. journal of machine p. learning research collins m. and duffy n. convolution kernels for natural language. in diettrich t. g. becker s. and ghahramani z. editors advances in neural information processing systems mit press. p. collobert r. and bengio s. gression problems. bengioprojectssvmtorch.html. journal of machine learning research svmtorch support vector machines for large-scale rehttpwww.idiap.ch pp. cornford d. nabney i. t. and williams c. k. i. modelling frontal discontinuities in wind p. fields. journal of nonparameteric statsitics cox d. d. multivariate smoothing spline functions. siam journal on numerical analysis p. cox d. d. and o sullivan f. asymptotic analysis of penalized likelihood and related estip. mators. annals of statistics c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography craven p. and wahba g. smoothing noisy data with spline functions. numer. math. p. cressie n. a. c. statistics for spatial data. wiley new york. pp. cristianini n. and shawe-taylor j. an introduction to support vector machines. cambridge p. university press. cristianini n. shawe-taylor j. elisseeff a. and kandola j. on kernel-target alignment. in diettrich t. g. becker s. and ghahramani z. editors advances in neural information processing systems mit press. p. csat o l. gaussian processes iterative sparse approximations. phd thesis aston university p. uk. csat o l. and opper m. sparse on-line gaussian processes. neural computation pp. csat o l. opper m. and winther o. tap gibbs free energy belief propagation and sparsity. in diettrich t. g. becker s. and ghahramani z. editors advances in neural information processing systems pages mit press. pp. daley r. atmospheric data analysis. cambridge university press cambridge uk. p. david h. a. order statistics. wiley new york. davis p. j. circulant matrices. wiley new york. dawid a. p. properties of diagnostic data distributions. biometrics pp. p. p. dellaportas p. and stephens d. a. bayesian analysis of errors-in-variables regression models. p. biometrics devroye l. gy orfi l. and lugosi g. a probabilistic theory of pattern recognition. springer pp. new york. diaconis p. and freedman d. on the consistency of bayes estimates. annals of statistics p. diggle p. j. time series a biostatistical introduction. clarendon press oxford. pp. diggle p. j. tawn j. a. and moyeed r. a. model-based geostatistics discussion. p. applied statistics doob j. l. the elementary gaussian processes. annals of mathematical statistics p. doob j. l. measure theory. springer-verlag new york. p. drineas p. and mahoney m. w. on the nystr om method for approximating a gram matrix for improved kernel-based learning. technical report yale university. p. httpcs-www.cs.yale.eduhomesmmahoney. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography duchon j. splines minimizing rotation-invariant semi-norms in sobolev spaces. in schempp w. and zeller k. editors constructive theory of functions of several variables pages springer-verlag. p. duda r. o. and hart p. e. pattern classification and scene analysis. john wiley new york. p. edgeworth f. y. on observations relating to several quantities. hermathena p. faul a. c. and tipping m. e. analysis of sparse bayesian learning. in dietterich t. g. becker s. and ghahramani z. editors advances in neural information processing systems pages cambridge massachussetts. mit press. p. feldman j. equivalence and perpendicularity of gaussian processes. pacific j. math. p. erratum in pacific j. math. ferrari trecate g. williams c. k. i. and opper m. finite-dimensional approximation of gaussian processes. in kearns m. s. solla s. a. and cohn d. a. editors advances in neural information processing systems pages mit press. p. fine s. and scheinberg k. efficient svm training using low-rank kernel representations. pp. journal of machine learning research fowlkes c. belongie s. and malik j. efficient spatiotemporal grouping using the nystr om method. in proceedings of the ieee conference on computer vision and pattern recognition cvpr p. freedman d. on the bernstein-von mises theorem with infinite-dimensional parameters. p. annals of statistics frieze a. kannan r. and vempala s. fast monte-carlo algorithms for finding low-rank approximations. in conference on the foundations of computer science pages p. geisser s. and eddy w. f. a predictive approach to model selection. journal of the americal p. statistical association geman s. and geman d. stochastic relaxation gibbs distributions and the bayesian restorap. tion of images. ieee trans. pattern analysis and machine intellligence gibbs m. n. bayesian gaussian processes for regression and classification. phd thesis p. department of physics university of cambridge. gibbs m. n. and mackay d. j. c. efficient implementation of gaussian processes. unpublished manuscript. cavendish laboratory cambridge uk. httpwww.inference.phy.cam.ac.uk mackaybayesgp.html. p. gibbs m. n. and mackay d. j. c. variational gaussian process classifiers. ieee transactions p. on neural networks gihman i. i. and skorohod a. v. the theory of stochastic processes volume springer p. verlag berlin. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography girard a. rasmussen c. e. qui nonero-candela j. and murray-smith r. gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting. in becker s. thrun s. and obermayer k. editors advances in neural information processing systems mit press. p. girosi f. models of noise and robust estimates. technical report ai memo mit ai p. laboratory. girosi f. jones m. and poggio t. regularization theory and neural networks architectures. p. neural computation goldberg p. w. williams c. k. i. and bishop c. m. regression with input-dependent noise a gaussian process treatment. in jordan m. i. kearns m. j. and solla s. a. editors advances p. in neural information processing systems mit press cambridge ma. golub g. h. and van loan c. f. matrix computations. johns hopkins university press pp. baltimore. second edition. gradshteyn i. s. and ryzhik i. m. tables of integrals series and products. academic press. pp. corrected and enlarged edition prepared by a. jeffrey. green p. j. and silverman b. w. nonparametric regression and generalized linear models. p. chapman and hall london. grenander u. chow y. and keenan d. m. hands a pattern theoretic study of biological pp. shapes. springer-verlag new york. grimmett g. r. and stirzaker d. r. probability and random processes. oxford university pp. press oxford england second edition. gr unwald p. d. and langford j. suboptimal behaviour of bayes and mdl in classification under misspecification. in proc. seventeenth annual conference on computational learning theory p. gy orfi l. kohler m. krzy zak a. and walk h. a distribution-free theory of nonparametric p. regression. springer new york. hajek j. on a property of normal distributions of any stochastic process russian. czechoslovak math. j. translated in selected trans. math. statist. probab. also available in collected works of jaroslav hajek eds. m. hu skov a r. beran v. dupa c wiley p. hand d. j. mannila h. and smyth p. principles of data mining. mit press. hannan e. j. multiple time series. wiley new york. p. p. hansen l. k. liisberg c. and salamon p. the error-reject tradeoff. open sys. information p. dyn. hastie t. j. and tibshirani r. j. generalized additive models. chapman and hall. pp. haussler d. convolution kernels on discrete structures. technical report p. dept of computer science university of california at santa cruz. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography hawkins d. l. some practical problems in implementing a certain sieve estimator of the gaussian mean function. communications in statistics simulation and computation p. hoerl a. e. and kennard r. w. ridge regression biased estimation for nonorthogonal p. problems. technometrics hornik k. some new results on neural network approximation. neural networks p. ihara s. information theory for continuous systems. world scientific singapore. p. jaakkola t. s. diekhans m. and haussler d. a discriminative framework for detecting pp. remote protein homologies. journal of computational biology jaakkola t. s. and haussler d. probabilistic kernel regression models. in heckerman d. and whittaker j. editors workshop on artificial intelligence and statistics morgan kaufmann. p. jacobs r. a. jordan m. i. nowlan s. j. and hinton g. e. adaptive mixtures of local p. experts. neural computation johnson n. l. kotz s. and balakrishnan n. continuous univariate distributions volume p. john wiley and sons new york second edition. jones d. r. a taxonomy of global optimization methods based on response surfaces. j. p. global optimization jordan m. i. an introduction to probabilistic graphical models. draft book. journel a. g. and huijbregts c. j. mining geostatistics. academic press. p. p. kailath t. rkhs approach to detection and estimation problems part i deterministic p. signals in gaussian noise. ieee trans. information theory kammler d. w. a first course in fourier analysis. prentice-hall upper saddle river nj. p. kashyap r. l. and chellappa r. stochastic models for closed boundary analysis represenp. tation and reconstruction. ieee trans. on information theory keeling c. d. and whorf t. p. atmospheric records from sites in the sio air sampling network. in trends a compendium of data on global change. carbon dioxide information analysis p. center oak ridge national laboratory oak ridge tenn. u.s.a. kent j. t. and mardia k. v. the link between kriging and thin-plate splines. in kelly p. f. p. editor probability statsitics and optimization pages wiley. kimeldorf g. and wahba g. a correspondence between bayesian estimation of stochastic p. processes and smoothing by splines. annals of mathematical statistics kimeldorf g. and wahba g. some results on tchebycheffian spline functions. j. mathematical p. analysis and applications c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography kohn r. and ansley c. f. a new algorithm for spline smoothing based on smoothing a p. stochastic process. siam j. sci. stat. comput. kolmogorov a. n. interpolation und extrapolation von station aren zuf aligen folgen. izv. akad. p. nauk sssr k onig h. eigenvalue distribution of compact operators. birkh auser. kullback s. information theory and statistics. dover new york. p. pp. kuss m. and rasmussen c. e. assessing approximations for gaussian process classification. in weiss y. sch olkopf b. and platt j. editors advances in neural information processing systems mit press. pp. lanckriet g. r. g. cristianini n. bartlett p. l. el ghaoui l. and jordan m. i. learning the kernel matrix with semidefinite programming. journal of machine learning research p. lauritzen s. l. time series analysis in a discussion of contributions made by p. t. n. thiele. international statistical review lawrence n. gaussian process latent variable models for visualization of high dimensional data. in thrun s. saul l. and sch olkopf b. editors advances in neural information processing p. systems pages mit press. lawrence n. seeger m. and herbrich r. fast sparse gaussian process methods the informative vector machine. in becker s. thrun s. and obermayer k. editors advances in neural pp. information processing systems pages mit press. leslie c. eskin e. weston j. and stafford noble w. mismatch string kernels for svm in becker s. thrun s. and obermayer k. editors advances in neural pp. protein classification. information processing systems mit press. lin x. wahba g. xiang d. gao f. klein r. and klein b. smoothing spline anova models for large data sets with bernoulli observations and the randomized gacv. annals of statistics p. lindley d. v. making decisions. john wiley and sons london uk second edition. p. lodhi h. shawe-taylor j. cristianini n. and watkins c. j. c. h. text classification using string kernels. in leen t. k. diettrich t. g. and tresp v. editors advances in neural information p. processing systems mit press. luo z. and wahba g. hybrid adaptive splines. j. amer. statist. assoc. p. mackay d. j. c. a practical bayesian framework for backpropagation networks. neural pp. computation mackay d. j. c. bayesian interpolation. neural computation pp. xiii xvi mackay d. j. c. information-based objective functions for active data selection. neural p. computation c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography mackay d. j. c. the evidence framework applied to classification networks. neural comp. putation mackay d. j. c. introduction to gaussian processes. in bishop c. m. editor neural networks pp. and machine learning. springer-verlag. mackay d. j. c. comparison of approximate methods for handling hyperparameters. neural p. computation mackay d. j. c. information theory inference and learning algorithms. cambridge univerpp. xiv sity press cambridge uk. malzahn d. and opper m. a variational approach to learning curves. in diettrich t. g. becker s. and ghahramani z. editors advances in neural information processing systems mit press. p. mandelbrot b. b. the fractal geometry of nature. w. h. freeman san francisco. p. mardia k. v. and marshall r. j. maximum likelihood estimation for models of residual p. covariance in spatial regression. biometrika mat ern b. spatial variation. meddelanden fr an statens skogsforskningsinstitut pp. alm anna f orlaget stockholm. second edition springer-verlag berlin. matheron g. the intrinsic random functions and their applications. advances in applied p. probability maxwell j. c. letter to lewis campbell reproduced in l. campbell and w. garrett the life p. v of james clerk maxwell macmillan mcallester d. pac-bayesian stochastic model selection. machine learning p. mccullagh p. and nelder j. generalized linear models. chapman and hall. pp. meinguet j. multivariate interpolation at arbitrary points made simple. journal of applied p. mathematics and physics meir r. and zhang t. generalization error bounds for bayesian mixture algorithms. journal p. of machine learning research micchelli c. a. and pontil m. kernels for multi-task learning. in saul l. k. weiss y. and bottou l. editors advances in neural information processing systems pages mit p. press. micchelli c. a. and wahba g. design problems for optimal surface interpolation. in ziegler p. z. editor approximation theory and applications pages academic press. minka t. p. a family of algorithms for approximate bayesian inference. phd thesis maspp. sachusetts institute of technology. minka t. p. a comparison of numerical optimizers for logistic regression. httpresearch.microsoft.com minkapaperslogreg. p. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography minka t. p. and picard r. w. learning how to learn is learning with point sets. httpresearch.microsoft.com minkapaperspoint-sets.html. mitchell t. m. machine learning. mcgraw-hill new york. p. pp. murray-smith r. and girard a. gaussian process priors with arma noise models. in irish signals and systems conference pages maynooth. httpwww.dcs.gla.ac.uk rod p. neal r. m. bayesian learning for neural networks. springer new york. lecture notes in pp. xiii statistics neal r. m. monte carlo implementation of gaussian process models for bayesian regression and classification. technical report department of statistics university of toronto. httpwww.cs.toronto.edu radford. p. neal r. m. regression and classification using gaussian process priors. in bernardo j. m. berger j. o. dawid a. p. and smith a. f. m. editors bayesian statistics pages oxford pp. university press. discussion. neal r. m. annealed importance sampling. statistics and computing p. neumaier a. introduction to global optimization. httpwww.mat.univie.ac.at neum p. gloptintro.html. o hagan a. curve fitting and optimal design for prediction. journal of the royal statistical pp. society b discussion. o hagan a. bayes-hermite quadrature. journal of statistical planning and inference pp. o hagan a. kennedy m. c. and oakley j. e. uncertainty analysis and other inference tools for complex computer codes. in bernardo j. m. berger j. o. dawid a. p. and smith a. f. m. editors bayesian statistics pages oxford university press. discussion. p. ksendal b. stochastic differential equations. springer-verlag berlin. p. opper m. and vivarelli f. general bounds on bayes errors for regression with gaussian processes. in kearns m. s. solla s. a. and cohn d. a. editors advances in neural information processing systems pages mit press. p. opper m. and winther o. gaussian processes for classification mean-field algorithms. pp. neural computation o sullivan f. yandell b. s. and raynor w. j. automatic smoothing of regression functions in generalized linear models. journal of the american statistical association pp. paciorek c. and schervish m. j. nonstationary covariance functions for gaussian process regression. in thrun s. saul l. and sch olkopf b. editors advances in neural information processing pp. systems mit press. papoulis a. probability random variables and stochastic processes. mcgraw-hill new york. pp. third edition. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography plaskota l. noisy information and computational complexity. cambridge university press pp. cambridge. plate t. a. accuarcy versus interpretability in flexible modeling implementing a tradeoff p. using gaussian process models. behaviourmetrika platt j. c. fast training of support vector machines using sequential minimal optimization. in sch olkopf b. burges c. j. c. and smola a. j. editors advances in kernel methods pages p. mit press. platt j. c. probabilities for sv machines. in smola a. bartlett p. sch olkopf b. and schuurmans d. editors advances in large margin classifiers pages mit press. pp. poggio t. and girosi f. networks for approximation and learning. proceedings of ieee pp. poggio t. voorhees h. and yuille a. a regularized solution to edge detection. technical p. report ai memo mit ai laboratory. pontil m. mukherjee s. and girosi f. on the noise model of support vector machine p. regression. technical report ai memo mit ai laboratory. press w. h. teukolsky s. a. vetterling w. t. and flannery b. p. numerical recipes in c. pp. cambridge university press second edition. qui nonero-candela j. learning with uncertainty gaussian processes and relevance vector machines. phd thesis informatics and mathematical modelling technical univeristy of denmark. p. rasmussen c. e. evaluation of gaussian processes and other methods for non-linear regression. phd thesis dept. of computer science university of toronto. httpwww.kyb.mpg.de p. rasmussen c. e. gaussian processes to speed up hybrid monte carlo for expensive bayesian integrals. in bernardo j. m. bayarri m. j. berger j. o. dawid a. p. heckerman d. smith a. f. m. and west m. editors bayesian statistics pages oxford university press. p. rasmussen c. e. and ghahramani z. occam s razor. in leen t. dietterich t. g. and tresp v. editors advances in neural information processing systems pages mit press. p. rasmussen c. e. and ghahramani z. in diettrich t. g. becker s. and ghahramani z. editors advances in neural information processing systems mit press. p. infinite mixtures of gaussian process experts. rasmussen c. e. and ghahramani z. bayesian monte carlo. in suzanna becker s. t. and obermayer k. editors advances in neural information processing systems pages mit press. p. rasmussen c. e. and qui nonero-candela j. healing the relevance vector machine through pp. augmentation. in proc. international conference on machine learning. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography rifkin r. and klautau a. in defense of one-vs-all classification. journal of machine learning pp. research ripley b. spatial statistics. wiley new york. p. ripley b. pattern recognition and neural networks. cambridge university press cambridge p. uk. ritter k. average-case analysis of numerical problems. springer verlag. pp. ritter k. wasilkowski g. w. and wo zniakowski h. multivariate integration and approximation of random fields satisfying sacks-ylvisaker conditions. annals of applied probability p. rousseeuw p. j. least median of squares regression. journal of the american statistical p. association sacks j. welch w. j. mitchell t. j. and wynn h. p. design and analysis of computer pp. experiments. statistical science saitoh s. theory of reproducing kernels and its applications. longman harlow england. p. salton g. and buckley c. term-weighting approaches in automatic text retrieval. information p. processing and management sampson p. d. and guttorp p. nonparametric estimation of nonstationary covariance strucp. ture. journal of the american statistical association santner t. j. williams b. j. and notz w. the design and analysis of computer experiments. p. springer new york. saunders c. gammerman a. and vovk v. ridge regression learning algorithm in dual variables. in shavlik j. editor proceedings of the fifteenth international conference on machine learning morgan kaufmann. p. saunders c. shawe-taylor j. and vinokourov a. string kernels fisher kernels and finite state automata. in becker s. thrun s. and obermayer k. editors advances in neural information processing systems mit press. p. schoenberg i. j. metric spaces and positive definite functions. trans. american mathematical p. society schoenberg i. j. spline functions and the problem of graduation. proc. nat. acad. sci. usa pp. sch olkopf b. and smola a. j. learning with kernels. mit press. pp. xvi sch olkopf b. smola a. j. and m uller k.-r. nonlinear component analysis as a kernel p. eigenvalue problem. neural computation c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography schwaighofer a. and tresp v. transductive and inductive methods for approximate gaussian process regression. in becker s. thrun s. and obermayer k. editors advances in neural information processing systems mit press. pp. scott d. w. multivariate density estimation. wiley new york. p. seeger m. bayesian model selection for support vector machines gaussian processes and other kernel classifiers. in solla s. a. leen t. k. and m uller k.-r. editors advances in neural pp. information processing systems mit press cambridge ma. seeger m. pac-bayesian generalisation error bounds for gaussian process classification. pp. journal of machine learning research seeger m. bayesian gaussian process models pac-bayesian generalisation error bounds and sparse approximations. phd thesis school of informatics university of edinburgh. httpwww.cs.berkeley.edu mseeger. pp. seeger m. expectation propagation for exponential families. httpwww.cs.berkeley.edu p. mseegerpapersepexpfam.ps.gz. seeger m. and jordan m. i. sparse gaussian process classification with multiple classes. p. technical report tr department of statistics university of california at berkeley. seeger m. williams c. k. i. and lawrence n. fast forward selection to speed up sparse gaussian process regression. in bishop c. and frey b. j. editors proceedings of the ninth international workshop on artificial intelligence and statistics. society for artificial intelligence and statistics. pp. shawe-taylor j. and williams c. k. i. the stability of kernel principal components analysis and its relation to the process eigenspectrum. in becker s. thrun s. and obermayer k. editors advances in neural information processing systems mit press. p. shepp l. a. radon-nikodym derivatives of gaussian measures. annals of mathematical statisp. tics silverman b. w. density ratios empirical likelihood and cot death. applied statistics p. silverman b. w. spline smoothing the equivalent variable kernel method. annals of pp. statistics silverman b. w. some aspects of the spline smoothing approach to non-parametric regression pp. curve fitting discussion. j. roy. stat. soc. b simard p. victorri b. le cun y. and denker j. tangent prop a formalism for specifying selected invariances in an adaptive network. in moody j. e. hanson s. j. and lippmann r. p. editors advances in neural information processing systems pages morgan kaufmann. pp. smola a. j. and bartlett p. l. sparse greedy gaussian process regression. in leen t. k. diettrich t. g. and tresp v. editors advances in neural information processing systems pages p. mit press. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography smola a. j. and sch olkopf b. sparse greedy matrix approximation for machine learning. in proceedings of the seventeenth international conference on machine learning. morgan kaufmann. pp. solak e. murray-smith r. leithead w. e. leith d. and rasmussen c. e. derivative observations in gaussian process models of dynamic systems. in becker s. s. t. and obermayer k. editors advances in neural information processing systems pages mit press. p. sollich p. learning curves for gaussian processes. in kearns m. s. solla s. a. and cohn pp. d. a. editors neural information processing systems vol. mit press. sollich p. bayesian methods for support vector machines evidence and predictive class pp. probabilities. machine learning sollich p. and williams c. k. i. using the equivalent kernel to understand gaussian process in saul l. k. weiss y. and bottou l. editors advances in neural information p. regression. processing systems mit press. stein m. l. a kernel approximation to the kriging predictor of a spatial process. ann. inst. p. statist. math stein m. l. interpolation of spatial data. springer-verlag new york. pp. steinwart i. consistency of support vector machines and other regularized kernel classifiers. p. ieee trans. on information theory stitson m. o. gammerman a. vapnik v. n. vovk v. watkins c. j. c. h. and weston j. support vector regression with anova decomposition kernels. in sch olkopf b. burges c. j. c. and smola a. j. editors advances in kernel methods. mit press. p. sundararajan s. and keerthi s. s. predictive approaches for choosing hyperparameters in p. gaussian processes. neural computation suykens j. a. k. and vanderwalle j. least squares support vector machines. neural processing p. letters szeliski r. regularization uses fractal priors. in proceedings of the national conference pp. on artificial intelligence teh y. w. seeger m. and jordan m. i. semiparametric latent factor models. in cowell r. g. and ghahramani z. editors proceedings of tenth international workshop on artificial inte lligence and statistics pages society for artificial intelligence and statistics. p. thomas-agnan c. computing a family of reproducing kernels for statistical applications. p. numerical algorithms thompson p. d. optimum smoothing of two-dimensional fields. tellus p. tikhonov a. n. solution of incorrectly formulated problems and the regularization method. p. soviet. math. dokl. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography tikhonov a. n. and arsenin v. y. solutions of ill-posed problems. w. h. winston washington p. d.c. tipping m. e. sparse bayesian learning and the relevance vector machine. journal of machine p. learning research tipping m. e. and faul a. c. fast marginal likelihood maximisation for sparse bayesian models. in bishop c. m. and frey b. j. editors proceedings of ninth international workshop on p. artificial intelligence and statistics. society for artificial intelligence and statistics. tresp v. a bayesian committee machine. neural computation pp. tsuda k. kawanabe m. r atsch g. sonnenburg s. and m uller k.-r. a new discriminative p. kernel from probabilistic models. neural computation uhlenbeck g. e. and ornstein l. s. on the theory of brownian motion. phys. rev. pp. valiant l. g. a theory of the learnable. communications of the acm p. vapnik v. n. the nature of statistical learning theory. springer verlag new york. pp. vapnik v. n. statistical learning theory. john wiley and sons. p. vijayakumar s. d souza a. and schaal s. incremental online learning in high dimensions. pp. accepted for publication in neural computation. vijayakumar s. d souza a. shibata t. conradt j. and schaal s. statistical learning for p. humanoid robots. autonomous robot vijayakumar s. and schaal s. lwpr an on algorithm for incremental real time learning in high dimensional space. in proc. of the seventeenth international conference on machine learning pages p. vishwanathan s. v. n. and smola a. j. fast kernels for string and tree matching. in becker s. thrun s. and obermayer k. editors advances in neural information processing systems mit press. p. vivarelli f. and williams c. k. i. discovering hidden features with gaussian processes regression. in kearns m. s. solla s. a. and cohn d. a. editors advances in neural information processing systems mit press. p. von mises r. mathematical theory of probability and statistics. academic press. p. wahba g. improper priors spline smoothing and the problem of guarding against model p. errors in regression. journal of the royal statistical society b wahba g. a comparison of gcv and gml for choosing the smoothing parameter in the p. generalized spline smoothing problem. annals of statistics c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography wahba g. society for industrial and applied mathematics philadelphia pa. cbms-nsf regional conference series in applied mathematics. pp. spline models for observational data. wahba g. johnson d. r. gao f. and gong j. adaptive tuning of numerical weather prediction models randomized gcv in three-and four-dimensional data assimilation. monthly weather review p. watkins c. j. c. h. dynamic alignment kernels. technical report dept of p. computer science royal holloway university of london. watkins c. j. c. h. dynamic alignment kernels. in smola a. j. bartlett p. l. and sch olkopf b. editors advances in large margin classifiers pages mit press cambridge ma. p. wegman e. j. reproducing kernel hilbert spaces. in kotz s. and johnson n. l. editors pp. encyclopedia of statistical sciences volume pages wiley new york. weinert h. l. editor reproducing kernel hilbert spaces. hutchinson ross stroudsburg p. pennsylvania. wendland h. scattered data approximation. cambridge monographs on applied and compup. tational mathematics. cambridge university press. whittle p. prediction and regulation by linear least-square methods. english universities pp. press. widom h. asymptotic behavior of the eigenvalues of certain integral equations. trans. of the p. american mathematical society widom h. asymptotic behavior of the eigenvalues of certain integral equations ii. archive p. for rational mechanics and analysis wiener n. extrapolation interpolation and smoothing of stationary time series. mit press p. cambridge mass. williams c. k. i. computation with infinite neural networks. neural computation p. williams c. k. i. and barber d. bayesian classification with gaussian processes. transactions on pattern analysis and machine intelligence ieee pp. williams c. k. i. and rasmussen c. e. gaussian processes for regression. in touretzky d. s. mozer m. c. and hasselmo m. e. editors advances in neural information processing systems pages mit press. pp. williams c. k. i. rasmussen c. e. schwaighofer a. and tresp v. observations on the nystr om method for gaussian process prediction. technical report university of edinburgh. httpwww.dai.ed.ac.ukhomesckiwonline pubs.html. p. williams c. k. i. and seeger m. using the nystr om method to speed up kernel machines. in leen t. k. diettrich t. g. and tresp v. editors advances in neural information processing systems pages mit press. pp. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn bibliography williams c. k. i. and vivarelli f. upper and lower bounds on the learning curve for gaussian pp. proccesses. machine learning winkler g. image analysis random fields and dynamic monte carlo methods. springer p. berlin. wong e. stochastic processes in information and dynamical systems. mcgraw-hill new york. p. wood s. and kohn r. a bayesian approach to robust binary nonparametric regression. j. p. american statistical association yaglom a. m. correlation theory of stationary and related random functions volume i basic p. results. springer verlag. yang c. duraiswami r. and david l. efficient kernel machines using the improved fast gauss transform. in saul l. k. weiss y. and bottou l. editors advances in neural information processing systems mit press. p. ylvisaker d. designs on random fields. in srivastava j. n. editor a survey of statistical p. design and linear models pages north-holland. yuille a. and grzywacz n. m. a mathematical analysis of motion coherence theory. interp. national journal of computer vision zhang t. statistical behaviour and consistency of classification methods based on convex p. risk minimization discussion. annals of statistics zhu h. williams c. k. i. rohwer r. j. and morciniec m. gaussian regression and optimal finite dimensional linear models. in bishop c. m. editor neural networks and machine learning. springer-verlag berlin. p. zhu j. and hastie t. j. kernel logistic regression and the import vector machine. in diettrich t. g. becker s. and ghahramani z. editors advances in neural information processing systems pages mit press. p. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn author index abrahamsen p. abramowitz m. adams r. adler r. j. amari s. ansley c. f. ansley c. f. see kohn r. arat o m. arfken g. xv aronszajn n. arsenin v. y. see tikhonov a. n. bach f. r. baker c. t. h. balakrishnan n. see johnson n. l. barber d. barber d. see williams c. k. i. bartle r. g. bartlett p. l. bartlett p. l. see lanckriet g. r. g. bartlett p. l. see smola a. j. belongie s. see fowlkes c. bengio s. see collobert r. berger j. o. bishop c. m. bishop c. m. see goldberg p. w. blake i. f. blight b. j. n. boyd s. boyle p. bracewell r. n. buckley c. see salton g. caruana r. chatfield c. chellappa r. see kashyap r. l. choi t. choudhuri n. chow y. see grenander u. chu w. collins m. collobert r. conradt j. see vijayakumar s. cornford d. cox d. d. craven p. cressie n. a. c. cristianini n. cristianini n. see lanckriet g. r. g. cristianini n. see lodhi h. csat o l. daley r. david h. a. david l. see yang c. davis p. j. dawid a. p. dellaportas p. denker j. see simard p. devroye l. diaconis p. diekhans m. see jaakkola t. s. diggle p. j. doob j. l. drineas p. d souza a. see vijayakumar s. duchon j. duda r. o. duffy n. see collins m. duraiswami r. see yang c. eddy w. f. see geisser s. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn author index edgeworth f. y. el ghaoui l. see lanckriet g. r. g. elisseeff a. see cristianini n. eskin e. see leslie c. faul a. c. faul a. c. see tipping m. e. feldman j. ferrari trecate g. fine s. flannery b. p. see press w. h. fowlkes c. frean m. see boyle p. freedman d. freedman d. see diaconis p. frieze a. gammerman a. see saunders c. gammerman a. see stitson m. o. gao f. see lin x. gao f. see wahba g. geisser s. geman d. see geman s. geman s. ghahramani z. see chu w. ghahramani z. see rasmussen c. e. ghosal s. see choudhuri n. gibbs m. n. gihman i. i. girard a. girard a. see murray-smith r. girosi f. girosi f. see poggio t. girosi f. see pontil m. goldberg p. w. golub g. h. gong j. see wahba g. gradshteyn i. s. green p. j. grenander u. grimmett g. r. gr unwald p. d. grzywacz n. m. see yuille a. guttorp p. see sampson p. d. gy orfi l. gy orfi l. see devroye l. hajek j. hand d. j. hannan e. j. hansen l. k. hart p. e. see duda r. o. hastie t. j. hastie t. j. see zhu j. haussler d. haussler d. see jaakkola t. s. hawkins d. l. herbrich r. see lawrence n. hinton g. e. see jacobs r. a. hoerl a. e. hornik k. huijbregts c. j. see journel a. g. ihara s. jaakkola t. s. jacobs r. a. johnson d. r. see wahba g. johnson n. l. jones d. r. jones m. see girosi f. jordan m. i. jordan m. i. see bach f. r. jordan m. i. see bartlett p. l. jordan m. i. see jacobs r. a. jordan m. i. see lanckriet g. r. g. jordan m. i. see seeger m. jordan m. i. see teh y. w. journel a. g. kailath t. kammler d. w. kandola j. see cristianini n. kannan r. see frieze a. kashyap r. l. kawanabe m. see tsuda k. keeling c. d. keenan d. m. see grenander u. keerthi s. s. see sundararajan s. kennard r. w. see hoerl a. e. kennedy m. c. see o hagan a. kent j. t. kimeldorf g. klautau a. see rifkin r. klein b. see lin x. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn author index klein r. see lin x. kohler m. see gy orfi l. kohn r. kohn r. see ansley c. f. kohn r. see wood s. kolmogorov a. n. k onig h. kotz s. see johnson n. l. krzy zak a. see gy orfi l. kullback s. kuss m. lanckriet g. r. g. langford j. see gr unwald p. d. lauritzen s. l. lawrence n. lawrence n. see seeger m. le cun y. see simard p. leith d. see solak e. leithead w. e. see solak e. leslie c. liisberg c. see hansen l. k. lin x. lindley d. v. lindsey w. c. see blake i. f. lodhi h. lugosi g. see devroye l. luo z. mackay d. j. c. xiii xiv xvi mackay d. j. c. see gibbs m. n. mahoney m. w. see drineas p. malik j. see fowlkes c. malzahn d. mandelbrot b. b. mannila h. see hand d. j. mardia k. v. mardia k. v. see kent j. t. marshall r. j. see mardia k. v. mat ern b. matheron g. maxwell j. c. v mcallester d. mcauliffe j. d. see bartlett p. l. mccullagh p. meinguet j. meir r. micchelli c. a. minka t. p. mitchell t. j. see sacks j. mitchell t. m. morciniec m. see zhu h. moyeed r. a. see diggle p. j. mukherjee s. see pontil m. m uller k.-r. see sch olkopf b. m uller k.-r. see tsuda k. murray-smith r. murray-smith r. see girard a. murray-smith r. see solak e. nabney i. t. see cornford d. neal r. m. xiii nelder j. see mccullagh p. neumaier a. notz w. see santner t. j. nowlan s. j. see jacobs r. a. oakley j. e. see o hagan a. o hagan a. ksendal b. opper m. opper m. see csat o l. opper m. see ferrari trecate g. opper m. see malzahn d. ornstein l. s. see uhlenbeck g. e. o sullivan f. see cox d. d. o sullivan f. ott l. see blight b. j. n. paciorek c. papoulis a. picard r. w. see minka t. p. plaskota l. plate t. a. platt j. c. poggio t. poggio t. see girosi f. pontil m. pontil m. see micchelli c. a. press w. h. qui nonero-candela j. qui nonero-candela j. see girard a. qui nonero-candela j. see rasmussen c. e. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn author index rasmussen c. e. rasmussen c. e. see girard a. rasmussen c. e. see kuss m. rasmussen c. e. see solak e. rasmussen c. e. see williams c. k. i. r atsch g. see tsuda k. raynor w. j. see o sullivan f. rifkin r. ripley b. ritter k. rohwer r. j. see zhu h. rousseeuw p. j. roy a. see choudhuri n. ryzhik i. m. see gradshteyn i. s. saad d. see barber d. sacks j. saitoh s. salamon p. see hansen l. k. salton g. sampson p. d. santner t. j. saunders c. schaal s. see vijayakumar s. scheinberg k. see fine s. schervish m. j. see choi t. schervish m. j. see paciorek c. schoenberg i. j. sch olkopf b. xvi sch olkopf b. see smola a. j. schwaighofer a. schwaighofer a. see williams c. k. i. scott d. w. seeger m. seeger m. see lawrence n. seeger m. see teh y. w. seeger m. see williams c. k. i. shawe-taylor j. shawe-taylor j. see cristianini n. shawe-taylor j. see lodhi h. shawe-taylor j. see saunders c. shepp l. a. shibata t. see vijayakumar s. silverman b. w. silverman b. w. see green p. j. simard p. skorohod a. v. see gihman i. i. smola a. j. smola a. j. see sch olkopf b. xvi smola a. j. see vishwanathan s. v. n. smyth p. see hand d. j. solak e. sollich p. sonnenburg s. see tsuda k. stafford noble w. see leslie c. stegun i. a. see abramowitz m. stein m. l. steinwart i. stephens d. a. see dellaportas p. stirzaker d. r. see grimmett g. r. stitson m. o. sundararajan s. suykens j. a. k. svensen m. see bishop c. m. szeliski r. tawn j. a. see diggle p. j. teh y. w. teukolsky s. a. see press w. h. thomas-agnan c. thompson p. d. tibshirani r. j. see hastie t. j. tikhonov a. n. tipping m. e. tipping m. e. see faul a. c. tresp v. tresp v. see schwaighofer a. tresp v. see williams c. k. i. tsuda k. uhlenbeck g. e. valiant l. g. van loan c. f. see golub g. h. vandenberghe l. see boyd s. vanderwalle j. see suykens j. a. k. vapnik v. n. vapnik v. n. see stitson m. o. vempala s. see frieze a. vetterling w. t. see press w. h. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn author index victorri b. see simard p. vijayakumar s. vinokourov a. see saunders c. vishwanathan s. v. n. vivarelli f. vivarelli f. see opper m. vivarelli f. see williams c. k. i. von mises r. voorhees h. see poggio t. vovk v. see saunders c. vovk v. see stitson m. o. wahba g. wahba g. see craven p. wahba g. see kimeldorf g. wahba g. see lin x. wahba g. see luo z. wahba g. see micchelli c. a. walk h. see gy orfi l. wasilkowski g. w. see ritter k. watkins c. j. c. h. watkins c. j. c. h. see lodhi h. watkins c. j. c. h. see stitson m. o. wegman e. j. welch w. j. see sacks j. wendland h. weston j. see leslie c. weston j. see stitson m. o. whittle p. whorf t. p. see keeling c. d. widom h. wiener n. williams b. j. see santner t. j. williams c. k. i. williams c. k. i. see bishop c. m. williams c. k. i. see cornford d. williams c. k. i. see ferrari trecate g. williams c. k. i. see goldberg p. w. williams c. k. i. see seeger m. williams c. k. i. see shawe-taylor j. williams c. k. i. see sollich p. williams c. k. i. see vivarelli f. williams c. k. i. see zhu h. winkler g. winther o. see csat o l. winther o. see opper m. wong e. wood s. wo zniakowski h. see ritter k. wynn h. p. see sacks j. xiang d. see lin x. yaglom a. m. yandell b. s. see o sullivan f. yang c. ylvisaker d. yuille a. yuille a. see poggio t. zhang t. zhang t. see meir r. zhu h. zhu j. c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn subject index arma process see autoregressive moving-average covariance cokriging consistency convex function set predictive covariance function see also kernel anova compact support dot product exponential gaussian see covariance function squared ex alignment anisotropy ar process see autoregressive process ard see automatic relevance determination process automatic relevance determination autoregressive moving-average process noise model autoregressive process bayes classifier bayes theorem bayesian committee machine bcm see bayesian committee machine bias inductive binary classification bits blitzkrieging see fast gaussian processes bochner s theorem brownian bridge brownian motion see wiener process canonical hyperplane cholesky decomposition christ bowels of classification binary least-squares probabilistic multi-class probabilistic classifier gibbs predictive ponential inhomogeneous polynomial mat ern neural network ornstein-uhlenbeck covariance ou see uhlenbeck periodic polynomial piecewise function ornstein radial basis function see covariance function squared exponential rational quadratic rbf see covariance function squared expo se see covariance function squared exponen nential tial squared exponential covariance matrix cromwell s dictum cross-validation generalized leave-one-out c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn subject index dataset robot inverse dynamics usps decision region decision surface degenerate see kernel degenerate degrees of freedom derivative observations dirichlet process discriminative approach eigenfunction eigenvalue entropy ep see expectation propagation error function equivalent kernel error generalization error function hinge error-reject curve errors-in-variables regression evidence see marginal likelihood expectation propagation experimental design optimal factor analysis feature space fisher information matrix fisher kernel fourier transform fractal gamma distribution gaussian distribution gaussian markov process gaussian process gaussian process classification gaussian process latent variable model gaussian process regression generalization error generative approach generative topographic mapping geostatistics gmp see gaussian markov process gp see gaussian process gpc see gaussian process classification gplvm see gaussian process latent variable model gpr see gaussian process regression gram matrix green s function gtm see generative topographic mapping hidden markov model hinge error function hyperparameters hyperplane canonical index set informative vector machine integrals evaluation of intrinsic random function invariances irls see iteratively reweighted least squares isotropy iteratively reweighted least squares ivm see informative vector machine jitter kernel see also covariance function bag-of-characters degenerate equivalent fisher k-spectrum nondegenerate positive definite string tangent of posterior odds kernel classifier kernel pca kernel ridge regression see regression ridge kernel kernel smoother kernel trick kriging kullback-leibler divergence laplace approximation latent variable model learning curve learning supervised c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn subject index least-squares classification leave-one-out leave-one-out cross-validation length-scale characteristic likelihood mean square continuity mean square differentiability mean standardized log loss mean-field approximation measure mercer s theorem mixture of experts ml-ii see type ii maximum likelihood model non-parametric parametric semi-parametric moore-aronszajn theorem ms continuity see mean square continuity ms differentiability see mean square differentiabil ity msll see mean standardized log loss multi-class classification multi-task learning multiple outputs nadaraya-watson estimator nats neural network newton s method noise model correlated heteroscedastic norm frobenius null space nystr om approximation nystr om method occam s razor one-versus-rest operator integral optimal experimental design outputs multiple see probabilistic one nearest neighbour pac-bayesian theorem mcallester seeger penalized maximum likelihood estimate plsc see probabilistic least-squares classification positive definite matrix positive semidefinite matrix logistic multiple-logistic non-gaussian probit linear classifier linear regression link function log odds ratio logistic function logistic regression loo see leave-one-out loss negative log probability squared zero-one loss function loss matrix lsc see least-squares classification map see maximum a posteriori margin functional geometrical marginal likelihood marginalization property markov chain monte carlo markov random field matrix covariance fisher information gram inversion lemma loss partitioned inversion of positive definite positive semidefinite maximum a posteriori see penalized maximum like lihood maximum likelihood penalized mcmc see markov chain monte carlo mean function c. e. rasmussen c. k. i. williams gaussian processes for machine learning the mit press massachusetts institute of technology. www.gaussianprocess.orggpml isbn subject index stationarity stochastic differential equation student s t process subset of datapoints subset of regressors supervised learning support vector support vector machine soft margin support vector regression svm see support vector machine svr see support vector regression tangent of posterior odds kernel top kernel see tangent of posterior odds kernel transduction type ii maximum likelihood uncertain inputs upcrossing rate usps dataset weight function weight vector wiener process integrated tied-down wiener-khintchine theorem yule-walker equations posterior process pp see projected process approximation prediction classification averaged map probabilistic classification probabilistic least-squares classification probabilistic one nearest neighbour probability conditional joint marginal probit regression projected process approximation pseudo-likelihood quadratic form quadratic programming regression errors-in-variables gaussian process linear polynomial ridge kernel regularization regularization network reject option relative entropy see kullback leibler divergence relevance vector machine representer theorem reproducing kernel hilbert space response function ridge regression see regression ridge risk rkhs see reproducing kernel hilbert space rvm see relevance vector machine scale mixture sd see subset of datapoints sde see stochastic differential equation smse see standardized mean squared error softmax splines sr see subset of regressors standardized mean squared error