the work computer age statistical inference was first published by cambridge university press. in the work bradley efron and trevor hastie cambridge university press s catalogue entry for the work can be found at http www. cambridge. org nb the copy of the work as displayed on this website can be purchased through cambridge university press and other standard distribution channels. this copy is made available for personal use only and must not be adapted sold or re-distributed. corrected november the twenty-first century has seen a breathtaking expansion of statistical methodology both in scope and in influence. big data data science and machine learning have become familiar terms in the news as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. how did we get here? and where are we going?this book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the beginning with classical inferential theories bayesian frequentist fisherian individual chapters take up a series of influential topics survival analysis logistic regression empirical bayes the jackknife and bootstrap random forests neural networks markov chain monte carlo inference after model selection and dozens more. the distinctly modern approach integrates methodology and algorithms with statistical inference. the book ends with speculation on the future direction of statistics and data science.efron hastiecomputer age statistical inference how and why is computational statistics taking over the world? in this serious work of synthesis that is also fun to read efron and hastie give their take on the unreasonable effectiveness of statistics and machine learning in the context of a series of clear historically informed examples. andrew gelman columbia university computer age statistical inference is written especially for those who want to hear the big ideas and see them instantiated through the essential mathematics that defines statistical analysis. it makes a great supplement to the traditional curricula for beginning graduate students. rob kass carnegie mellon university this is a terrific book. it gives a clear accessible and entertaining account of the interplay between theory and methodological development that has driven statistics in the computer age. the authors succeed brilliantly in locating contemporary algorithmic methodologies for analysis of big data within the framework of established statistical theory. alastair young imperial college london this is a guided tour of modern statistics that emphasizes the conceptual and computational advances of the last century. authored by two masters of the field it offers just the right mix of mathematical analysis and insightful commentary. hal varian google efron and hastie guide us through the maze of breakthrough statistical methodologies following the computing evolution why they were developed their properties and how they are used. highlighting their origins the book helps us understand each method s roles in inference andor prediction. galit shmueli national tsing hua university a masterful guide to how the inferential bases of classical statistics can provide a principled disciplinary frame for the data science of the twenty-first century. stephen stigler university of chicago author of seven pillars of statistical wisdom a refreshing view of modern statistics. algorithmics are put on equal footing with intuition properties and the abstract arguments behind them. the methods covered are indispensable to practicing statistical analysts in today s big data and big computing landscape. robert gramacy the university of chicago booth school of businessbradley efron is max h. stein professor professor of statistics and professor of biomedical data science at stanford university. he has held visiting faculty appointments at harvard uc berkeley and imperial college london. efron has worked extensively on theories of statistical inference and is the inventor of the bootstrap sampling technique. he received the national medal of science in and the guy medal in gold of the royal statistical society in trevor hastie is john a. overdeck professor professor of statistics and professor of biomedical data science at stanford university. he is coauthor of elements of statistical learning a key text in the field of modern data analysis. he is also known for his work on generalized additive models and principal curves and for his contributions to the r computing environment. hastie was awarded the emmanuel and carol parzen prize for statistical innovation in institute of mathematical statistics monographseditorial boardd. r. cox of oxfordb. hambly of oxfords. holmes universityj. wellner of washingtoncover illustration pacific ocean wave north shore oahu hawaii. brian sytnyk getty images.cover designed by zoe naylor.printed in the united kingdomcomputer age statistical inferencealgorithms evidence and data sciencebradley efron trevor efron hastie jkt c m y k computer age statistical inference algorithms evidence and data science bradley efron trevor hastie stanford university to donna and lynda viii contents preface acknowledgments notation part i classic statistical inference algorithms and inference a regression example hypothesis testing notes frequentist inference frequentism in practice frequentist optimality notes and details bayesian inference two examples uninformative prior distributions flaws in frequentist inference a bayesianfrequentist comparison list notes and details fisherian inference and maximum likelihood estimation likelihood and maximum likelihood fisher information and the mle conditional inference permutation and randomization notes and details parametric models and exponential families ix xv xviii xix x contents univariate families the multivariate normal distribution fisher s information bound for multiparameter families the multinomial distribution exponential families notes and details part ii early computer-age methods empirical bayes robbins formula the missing-species problem a medical example indirect evidence notes and details james stein estimation and ridge regression the james stein estimator the baseball players ridge regression indirect evidence notes and details generalized linear models and regression trees logistic regression generalized linear models poisson regression regression trees notes and details survival analysis and the em algorithm life tables and hazard rates censored data and the kaplan meier estimate the log-rank test the proportional hazards model missing data and the em algorithm notes and details the jackknife and the bootstrap the jackknife estimate of standard error the nonparametric bootstrap resampling plans contents the parametric bootstrap notes and details influence functions and robust estimation bootstrap confidence intervals neyman s construction for one-parameter problems the percentile method bias-corrected confidence intervals second-order accuracy bootstrap-t intervals objective bayes intervals and the confidence distribution notes and details cross-validation and cp estimates of prediction error prediction rules cross-validation covariance penalties training validation and ephemeral predictors notes and details objective bayes inference and mcmc objective prior distributions conjugate prior distributions model selection and the bayesian information criterion gibbs sampling and mcmc example modeling population admixture notes and details postwar statistical inference and methodology part iii twenty-first-century topics large-scale hypothesis testing and fdrs large-scale testing false-discovery rates empirical bayes large-scale testing local false-discovery rates choice of the null distribution relevance notes and details sparse modeling and the lasso xi xii contents forward stepwise regression the lasso fitting lasso models least-angle regression fitting generalized lasso models post-selection inference for the lasso connections and extensions notes and details random forests and boosting random forests boosting with squared-error loss gradient boosting adaboost the original boosting algorithm connections and extensions notes and details neural networks and deep learning neural networks and the handwritten digit problem fitting a neural network autoencoders deep learning learning a deep network notes and details support-vector machines and kernel methods optimal separating hyperplane soft-margin classifier svm criterion as loss plus penalty computations and the kernel trick function fitting using kernels example string kernels for protein classification svms concluding remarks kernel smoothing and local regression notes and details inference after model selection simultaneous confidence intervals accuracy after model selection selection bias combined bayes frequentist estimation notes and details contents empirical bayes estimation strategies bayes deconvolution g-modeling and estimation likelihood regularization and accuracy two examples generalized linear mixed models deconvolution and f notes and details epilogue references author index subject index xiii xiv preface statistical inference is an unusually wide-ranging discipline located as it is at the triple-point of mathematics empirical science and philosophy. the discipline can be said to date from with the publication of bayes rule the philosophical side of the subject the rule s early advocates considered it an argument for the existence of god. the most recent quarter of this history from the to the present is the computer age of our book s title the time when computation the traditional bottleneck of statistical applications became faster and easier by a factor of a million. the book is an examination of how statistics has evolved over the past sixty years an aerial view of a vast subject but seen from the height of a small plane not a jetliner or satellite. the individual chapters take up a series of influential topics generalized linear models survival analysis the jackknife and bootstrap false-discovery rates empirical bayes mcmc neural nets and a dozen more describing for each the key methodological developments and their inferential justification. needless to say the role of electronic computation is central to our story. this doesn t mean that every advance was computer-related. a land bridge had opened to a new continent but not all were eager to cross. topics such as empirical bayes and james stein estimation could have emerged just as well under the constraints of mechanical computation. others like the bootstrap and proportional hazards were pureborn children of the computer age. almost all topics in twenty-first-century statistics are now computer-dependent but it will take our small plane a while to reach the new millennium. dictionary definitions of statistical inference tend to equate it with the entire discipline. this has become less satisfactory in the big data era of immense computer-based processing algorithms. here we will attempt not always consistently to separate the two aspects of the statistical enterprise algorithmic developments aimed at specific problem areas for instance xv xvi preface random forests for prediction as distinct from the inferential arguments offered in their support. very broadly speaking algorithms are what statisticians do while inference says why they do them. a particularly energetic brand of the statistical enterprise has flourished in the new century data science emphasizing algorithmic thinking rather than its inferential justification. the later chapters of our book where large-scale prediction algorithms such as boosting and deep learning are examined illustrate the data-science point of view. the epilogue for a little more on the sometimes fraught statisticsdata science marriage. there are no such subjects as biological inference or astronomical inference or geological inference. why do we need statistical inference the answer is simple the natural sciences have nature to judge the accuracy of their ideas. statistics operates one step back from nature most often interpreting the observations of natural scientists. without nature to serve as a disinterested referee we need a system of mathematical logic for guidance and correction. statistical inference is that system distilled from two and a half centuries of data-analytic experience. the book proceeds historically in three parts. the great themes of classical inference bayesian frequentist and fisherian reviewed in part i were set in place before the age of electronic computation. modern practice has vastly extended their reach without changing the basic outlines. analogy with classical and modern literature might be made. part ii concerns early computer-age developments from the through the as a transitional period this is the time when it is easiest to see the effects or noneffects of fast computation on the progress of statistical methodology both in its theory and practice. part iii twenty-first-century topics brings the story up to the present. ours is a time of enormously ambitious algorithms machine learning being the somewhat disquieting catchphrase. their justification is the ongoing task of modern statistical inference. neither a catalog nor an encyclopedia the book s topics were chosen as apt illustrations of the interplay between computational methodology and inferential theory. some missing topics that might have served just as well include time series general estimating equations causal inference graphical models and experimental design. in any case there is no implication that the topics presented here are the only ones worthy of discussion. also underrepresented are asymptotics and decision theory the math stat side of the field. our intention was to maintain a technical level of discussion appropriate to masters statisticians or first-year phd stu preface xvii dents. inevitably some of the presentation drifts into more difficult waters more from the nature of the statistical ideas than the mathematics. readers who find our aerial view circling too long over some topic shouldn t hesitate to move ahead in the book. for the most part the chapters can be read independently of each other there is a connecting overall theme. this comment applies especially to nonstatisticians who have picked up the book because of interest in some particular topic say survival analysis or boosting. useful disciplines that serve a wide variety of demanding clients run the risk of losing their center. statistics has managed for the most part to maintain its philosophical cohesion despite a rising curve of outside demand. the center of the field has in fact moved in the past sixty years from its traditional home in mathematics and logic toward a more computational focus. our book traces that movement on a topic-by-topic basis. an answer to the intriguing question what happens next? won t be attempted here except for a few words in the epilogue where the rise of data science is discussed. acknowledgments we are indebted to cindy kirby for her skillful work in the preparation of this book and galit shmueli for her helpful comments on an earlier draft. at cambridge university press a huge thank you to steven holt for his excellent copy editing clare dennison for guiding us through the production phase and to diana gillooly our editor for her unfailing support. bradley efron trevor hastie department of statistics stanford university may xviii notation xix throughout the book the numbered sign indicates a technical note or reference element which is elaborated on at the end of the chapter. there next to the number the page number of the referenced location is given in parenthesis. for example lowess in the notes on page was referenced via a on page matrices such as are represented in bold font as are certain vectors such as y a data vector with n elements. most other vectors such as coefficient vectors are typically not bold. we use a dark green typewriter font to indicate data set names such as prostate variable names such as prog from data sets and r commands such as glmnet or locfdr. no bibliographic references are given in the body of the text important references are given in the endnotes of each chapter. part i classic statistical inference algorithms and inference statistics is the science of learning from experience particularly experience that arrives a little bit at a time the successes and failures of a new experimental drug the uncertain measurements of an asteroid s path toward earth. it may seem surprising that any one theory can cover such an amorphous target as learning from experience. in fact there are two main statistical theories bayesianism and frequentism whose connections and disagreements animate many of the succeeding chapters. first however we want to discuss a less philosophical more operational division of labor that applies to both theories between the algorithmic and inferential aspects of statistical analysis. the distinction begins with the most basic and most popular statistical method averaging. suppose we have observed numbers xn applying to some phenomenon of interest perhaps the automobile accident rates in the n d states. the mean xi nx d nx summarizes the results in a single number. how accurate is that number? the textbook answer is given in terms of the standard errorbse d nx here averaging is the algorithm while the standard error provides an inference of the algorithm s accuracy. it is a surprising and crucial aspect of statistical theory that the same data that supplies an estimate can also assess its inference concerns more than accuracy speaking broadly algorithms say what the statistician does while inference says why he or she does it. of coursebse is itself an algorithm which could be is subject algorithms and inference to further inferential analysis concerning its accuracy. the point is that the algorithm comes first and the inference follows at a second level of statistical consideration. in practice this means that algorithmic invention is a more free-wheeling and adventurous enterprise with inference playing catch-up as it strives to assess the accuracy good or bad of some hot new algorithmic methodology. if the inferencealgorithm race is a tortoise-and-hare affair then modern electronic computation has bred a bionic hare. there are two effects at work here computer-based technology allows scientists to collect enormous data sets orders of magnitude larger than those that classic statistical theory was designed to deal with huge data demands new methodology and the demand is being met by a burst of innovative computer-based statistical algorithms. when one reads of big data in the news it is usually these algorithms playing the starring roles. our book s title computer age statistical inference emphasizes the tortoise s side of the story. the past few decades have been a golden age of statistical methodology. it hasn t been quite a golden age for statistical inference but it has not been a dark age either. the efflorescence of ambitious new algorithms has forced an evolution not a revolution in inference the theories by which statisticians choose among competing methods. the book traces the interplay between methodology and inference as it has developed since the the beginning of our discipline s computer age. as a preview we end this chapter with two examples illustrating the transition from classic to computer-age practice. a regression example figure concerns a study of kidney function. data points yi have been observed for n d healthy volunteers with xi the ith volunteer s age in years and yi a composite measure tot of overall function. kidney function generally declines with age as evident in the downward scatter of the points. the rate of decline is an important question in kidney transplantation in the past potential donors past age were prohibited though given a shortage of donors this is no longer enforced. the solid line in figure is a linear regression y d o c o fit to the data by least squares that is by minimizing the sum of squared a regression example figure kidney fitness tot vs age for volunteers. the line is a linear regression fit showing standard errors at selected values of age. deviations nx over all choices of the least squares algorithm which dates back to gauss and legendre in the early gives o d as the least squares estimates. we can read off of the fitted line an estimated value of kidney fitness for any chosen age. the top line of table shows estimate at age down to at age d and o how accurate are these estimates? this is where inference comes in an extended version of formula also going back to the provides the standard errors shown in line of the table. the vertical bars in figure are two standard errors giving them about chance of containing the true expected value of tot at each age. that coverage depends on the validity of the linear regression model we might instead try a quadratic regression y d o or a cubic etc. all of this being well within the reach of pre-computer statistical theory. c o c o algorithms and inference table regression analysis of the kidney data linear regression estimates their standard errors lowess estimates their bootstrap standard errors. age linear regression std error lowess bootstrap std error figure local polynomial fit to the kidney-fitness data with bootstrap standard deviations. a modern computer-based algorithm lowess produced the somewhat bumpy regression curve in figure the lowess algorithm moves its attention along the x-axis fitting local polynomial curves of differing degrees to nearby y points. in the here and throughout the book the numbered sign indicates a technical note or reference element which is elaborated on at the end of the chapter. here and in all our examples we are employing the language r itself one of the key developments in computer-based statistical methodology. a regression example determines the definition of local. repeated passes over the x-axis refine the fit reducing the effects of occasional anomalous points. the fitted curve in figure is nearly linear at the right but more complicated at the left where points are more densely packed. it is flat between ages and a potentially important difference from the uniform decline portrayed in figure there is no formula such as to infer the accuracy of the lowess curve. instead a computer-intensive inferential engine the bootstrap was used to calculate the error bars in figure a bootstrap data set is produced by resampling pairs yi from the original with replacement so perhaps might show up twice in the bootstrap sample might be missing present once etc. applying lowess to the bootstrap sample generates a bootstrap replication of the original calculation. figure bootstrap replications of figure shows the first bootstrap lowess replications bouncing around the original curve from figure the variability of the replications at any one age the bootstrap standard deviation determined the original curve s accuracy. how and why the bootstrap works is discussed in chapter it has the great virtue of assessing estimation accu algorithms and inference racy for any algorithm no matter how complicated. the price is a hundredor thousand-fold increase in computation unthinkable in but routine now. the bottom two lines of table show the lowess estimates and their standard errors. we have paid a price for the increased flexibility of lowess its standard errors roughly doubling those for linear regression. hypothesis testing our second example concerns the march of methodology and inference for hypothesis testing rather than estimation leukemia patients with all lymphoblastic leukemia and with aml myeloid leukemia a worse prognosis have each had genetic activity measured for a panel of genes. the histograms in figure compare the genetic activities in the two groups for gene figure scores for gene leukemia data. top all d bottom aml d a two-sample t-statistic d with p-value d the aml group appears to show greater activity the mean values being all d and aml d all scores mean scores mean hypothesis testing is the perceived difference genuine or perhaps as people like to say a statistical fluke the classic answer to this question is via a two-sample t-statistic t d aml allbsd wherebsd is an estimate of the numerator s standard dividing by bsd allows us gaussian assumptions discussed in chapter to compare the observed value of t with a standard null distribution in this case a student s t distribution with degrees of freedom. we obtain t d from which would classically be considered very strong evidence that the apparent difference is genuine in standard terminology with two-sided significance level a small significance level p-value is a statement of statistical surprise something very unusual has happened if in fact there is no difference in gene expression levels between all and aml patients. we are less surprised by t d if gene is just one candidate out of thousands that might have produced interesting results. that is the case here. figure shows the histogram of the two-sample t-statistics for the panel of genes. now t d looks less unusual other genes have t exceeding about of them. this doesn t mean that gene is significant at the level. there are two powerful complicating factors large numbers of candidates here will produce some large tvalues even if there is really no difference in genetic expression between all and aml patients. the histogram implies that in this study there is something wrong with the theoretical null distribution student s t with degrees of freedom the smooth curve in figure it is much too narrow at the center where presumably most of the genes are reporting non-significant results. we will see in chapter that a low false-discovery rate i.e. a low chance of crying wolf over an innocuous gene requires t exceeding in the allaml study. only of the genes make the cut. falsediscovery-rate theory is an impressive advance in statistical inference incorporating bayesian frequentist and empirical bayesian el formally a standard error is the standard deviation of a summary statistic andbsd might better be calledbse but we will follow the distinction less than punctiliously here. algorithms and inference figure two-sample t-statistics for genes leukemia data. the smooth curve is the theoretical null density for the t-statistic. ements. it was a necessary advance in a scientific world where computerbased technology routinely presents thousands of comparisons to be evaluated at once. there is one more thing to say about the algorithminference statistical cycle. important new algorithms often arise outside the world of professional statisticians neural nets support vector machines and boosting are three famous examples. none of this is surprising. new sources of data satellite imagery for example or medical microarrays inspire novel methodology from the observing scientists. the early literature tends toward the enthusiastic with claims of enormous applicability and power. in the second phase statisticians try to locate the new metholodogy within the framework of statistical theory. in other words they carry out the statistical inference part of the cycle placing the new methodology within the known bayesian and frequentist limits of performance. offers a nice example chapter this is a healthy chain of events good both for the hybrid vigor of the statistics profession and for the further progress of algorithmic technology. t statisticsfrequency notes notes legendre published the least squares algorithm in causing gauss to state that he had been using the method in astronomical orbit-fitting since given gauss astonishing production of major mathematical advances this says something about the importance attached to the least squares idea. chapter includes its usual algebraic formulation as well as gauss formula for the standard errors line of table our division between algorithms and inference brings to mind tukey s exploratoryconfirmatory system. however the current algorithmic world is often bolder in its claims than the word exploratory implies while to our minds inference conveys something richer than mere confirmation. lowess was devised by william cleveland and is available in the r statistical computing language. it is applied to the kidney data in efron the kidney data originated in the nephrology laboratory of dr. brian myers stanford university and is available from this book s web site. frequentist inference before the computer age there was the calculator age and before big data there were small data sets often a few hundred numbers or fewer laboriously collected by individual scientists working under restrictive experimental constraints. precious data calls for maximally efficient statistical analysis. a remarkably effective theory feasible for execution on mechanical desk calculators was developed beginning in by pearson fisher neyman hotelling and others and grew to dominate twentieth-century statistical practice. the theory now referred to as classical relied almost entirely on frequentist inferential ideas. this chapter sketches a quick and simplified picture of frequentist inference particularly as employed in classical applications. we begin with another example from dr. myers nephrology laboratory kidney patients have had their glomerular filtration rates measured with the results shown in figure gfr is an important indicator of kidney function with low values suggesting trouble. is a key component of andbse d typically reported as tot in figure the mean and standard error are nx d denotes a frequentist inference for the accuracy of the estimate nx d and suggests that we shouldn t take the very seriously even the being open to doubt. where the inference comes from and what exactly it means remains to be said. statistical inference usually begins with the assumption that some probability model has produced the observed data x in our case the vector of n d gfr measurements x d xn. let x d xn indicate n independent draws from a probability distribution f written f x frequentist inference figure glomerular filtration rates for kidney patients mean standard error f being the underlying distribution of possible gfr scores here. a realization x d x of has been observed and the statistician wishes to infer some property of the unknown distribution f suppose the desired property is the expectation of a single random draw x from f denoted also equals the expectation of the average nx dp xi of random d ef fxg vector the obvious estimate of is o d nx the sample average. if n were enormous say we would expect o to nearly equal but otherwise there is room for error. how much error is the inferential question. the estimate o is calculated from x according to some known algorithm say t in our example being the averaging function nx d p xi o is a the fact that eff nxg equals effxg is a crucial though easily proved probabilistic o d t result. realization of frequentist inference d ef f o g o d t the output of t applied to a theoretical sample x from f we have chosen t we hope to make o a good estimator of the desired property of f we can now give a first definition of frequentist inference the accuracy of an observed estimate o d t is the probabilistic accuracy of o d t as an estimator of this may seem more a tautology than a definition but it contains a powerful idea o is just a single number but o takes on a range of values whose spread can define measures of accuracy. bias and variance are familiar examples of frequentist inference. define to be the expectation of o d t under model n o then the bias and variance attributed to estimate o bias d var d ef of parameter are and again what keeps this from tautology is the attribution to the single number o of the probabilistic properties of o following from model if all of this seems too obvious to worry about the bayesian criticisms of chapter may come as a shock. frequentism is often defined with respect to an infinite sequence of future trials. we imagine hypothetical data sets x x x generated by the same mechanism as x providing corresponding values o o o as in the frequentist principle is then to attribute for o the accuracy properties of the ensemble of o if the o s have empirical variance of say then o d p is claimed to have standard error etc. this amounts to a more picturesque restatement of the previous definition. frequentism in practice our working definition of frequentism is that the probabilistic properties of a procedure of interest are derived and then applied verbatim to the procedure s output for the observed data. this has an obvious defect it requires calculating the properties of estimators o d t obtained from in essence frequentists ask themselves what would i see if i reran the same situation again again and again. frequentism in practice statistics o the true distribution f even though f is unknown. practical frequentism uses a collection of more or less ingenious devices to circumvent the defect. the plug-in principle. a simple formula relates the standard error of nx dp xi to varf the variance of a single x drawn from f d varf cvarf dx but having observed x d xn we can estimate varf without bias by plugging formula into givesbse the usual estimate for the standard error of an average nx. in other words the frequentist accuracy estimate for nx is itself estimated from the observed d t more complicated taylor-series approximations. than nx can often be related back to the plug-in formula by local linear approximations sometimes known as the delta method. for example o d has d with bse as in large sample calculations as sample size n goes to o nx d thinking of as a constant gives infinity validate the delta method which fortunately often performs well in small samples. parametric families and maximum likelihood theory. theoretical expressions for the standard error of a maximum likelihood estimate are discussed in chapters and in the context of parametric families of distributions. these combine fisherian theory taylor-series approximations and the plug-in principle in an easy-to-apply package. simulation and the bootstrap. modern computation has opened up the possibility of numerically implementing the infinite sequence of future trials definition except for the infinite part. an estimate of of f perhaps the mle is found and values o d t simulated from of for k d b say b d the empirical standard deviation of the o s is then the frequentist estimate of standard error for o d t and similarly with other measures of accuracy. this is a good description of the bootstrap chapter that the most familiar example is the observed proportion p of heads in n flips of a coin having true probability the actual standard error is but we can only report the plug-in estimate pn frequentist inference table three estimates of location for the gfr data and their estimated standard errors last two standard errors using the bootstrap b d estimate standard error mean winsorized mean median of for f comes first rather than at the end of here the plugging-in of the process. the classical methods above are restricted to estimates o d t that are smoothly defined functions of various sample means. simulation calculations remove this restriction. table shows three location estimates for the gfr data the mean the winsorized and the median along with their standard errors the last two computed by the bootstrap. a happy feature of computer-age statistical inference is the tremendous expansion of useful and usable statistics t in the statistician s working toolbox the lowess algorithm in figures and providing a nice example. pivotal statistics. a pivotal statistic o d t is one whose distribution does not depend upon the underlying probability distribution f in such a case the theoretical distribution of o d t applies exactly to o removing the need for devices above. the classic example concerns student s two-sample t-test. in a two-sample problem the statistician observes two sets of numbers d d and wishes to test the null hypothesis that they come from the same distribution opposed to say the second set tending toward larger values than the first. it is assumed that the distribution for is normal or gaussian n i d the notation indicating independent draws from a normal all observations below the percentile of the observations are moved up to that point similarly those above the percentile are moved down and finally the mean is taken. each draw having probability density frequentism in practice with expectation and variance likewise n we wish to test the null hypothesis i d the obvious test statistic o d the difference of the means has distribution o c w d c n under we could plug in the unbiased estimate of d c but student provided a more elegant solution instead of o using the two-sample t-statistic wherebsd d c t d we test under t is pivotal having the same distribution s t distribution with c degrees of freedom no matter what the value of the nuisance parameter for c d as in the leukemia example student s distribution gives the hypothesis test that rejects if jtj exceeds has probability exactly of mistaken rejection. similarly t d is an exact confidence interval for the difference covering the true value in of repetitions of probability model occasionally one sees frequentism defined in careerist terms e.g. a statistician who always rejects null hypotheses at the level will over time make only errors of the first kind. this is not a comforting criterion for the statistician s clients who are interested in their own situations not everyone else s. here we are only assuming hypothetical repetitions of the specific problem at hand. frequentist inference what might be called the strong definition of frequentism insists on exact frequentist correctness under experimental repetitions. pivotality unfortunately is unavailable in most statistical situations. our looser definition of frequentism supplemented by devices such as those presents a more realistic picture of actual frequentist practice. frequentist optimality the popularity of frequentist methods reflects their relatively modest mathematical modeling assumptions only a probability model f exactly a family of probabilities chapter and an algorithm of choice t this flexibility is also a defect in that the principle of frequentist correctness doesn t help with the choice of algorithm. should we use the sample mean to estimate the location of the gfr distribution? maybe the winsorized mean would be better as table suggests. the years saw the development of two key results on frequentist optimality that is finding the best choice of t given model f the first of these was fisher s theory of maximum likelihood estimation and the fisher information bound in parametric probability models of the type discussed in chapter the mle is the optimum estimate in terms of minimum standard error. in the same spirit the neyman pearson lemma provides an optimum hypothesis-testing algorithm. this is perhaps the most elegant of frequentist constructions. in its simplest formulation the np lemma assumes we are trying to decide between two possible probability density functions for the observed data x a null hypothesis density and an alternative density a testing rule t says which choice or we will make having observed data x. any such rule has two associated frequentist error probabilities choosing when actually generated x and vice versa d d let l.x be the likelihood ratio ft d ft d l.x d the list of devices is not complete. asymptotic calculations play a major role as do more elaborate combinations of pivotality and the plug-in principle see the discussion of approximate bootstrap confidence intervals in chapter and define the testing rule tc.x by tc.x d frequentist optimality if log l.x c if log l.x c there is one such rule for each choice of the cutoff c. the neyman pearson lemma says that only rules of form can be optimum for any other rule t there will be a rule tc.x having smaller errors of both c and c figure neyman pearson alpha beta curve for cutoffs c d n n and sample size n d red dots correspond to figure graphs c c as a function of the cutoff c for the case where x d is obtained by independent sampling from a normal distribution n for versus n for the np lemma says that any rule not of form must have its point lying above the curve. here we are ignoring some minor definitional difficulties that can occur if and are discrete. b frequentist inference frequentist optimality theory both for estimation and for testing anchored statistical practice in the twentieth century. the larger data sets and more complicated inferential questions of the current era have strained the capabilities of that theory. computer-age statistical inference as we will see often displays an unsettling ad hoc character. perhaps some contemporary fishers and neymans will provide us with a more capacious optimality theory equal to the challenges of current practice but for now that is only a hope. frequentism cannot claim to be a seamless philosophy of statistical inference. paradoxes and contradictions abound within its borders as will be shown in the next chapter. that being said frequentist methods have a natural appeal to working scientists an impressive history of successful application and as our list of five devices suggests the capacity to encourage clever methodology. the story that follows is not one of abandonment of frequentist thinking but rather a broadening of connections with other methods. notes and details the name frequentism seems to have been suggested by neyman as a statistical analogue of richard von mises frequentist theory of probability the connection being made explicit in his paper frequentist probability and frequentist statistics. behaviorism might have been a more descriptive since the theory revolves around the long-run behavior of statistics t but in any case frequentism has stuck replacing the older disparaging term objectivism. neyman s attempt at a complete frequentist theory of statistical inference inductive behavior is not much quoted today but can claim to be an important influence on wald s development of decision theory. r. a. fisher s work on maximum likelihood estimation is featured in chapter fisher arguably the founder of frequentist optimality theory was not a pure frequentist himself as discussed in chapter and efron r. a. fisher in the century. that we are well into the twenty-first century the author s talents as a prognosticator can be frequentistically evaluated. delta method. the delta method uses a first-order taylor series to suppose o o s. c o of a statistic o approximate the variance of a function s. has meanvariance and consider the approximation s. that name is already spoken for in the psychology literature. o hence varfs. s and use an estimate for notes and details o js we typically plug-in o for bayesian inference the human mind is an inference machine it s getting windy the sky is darkening i d better bring my umbrella with me. unfortunately it s not a very dependable machine especially when weighing complicated choices against past experience. bayes theorem is a surprisingly simple mathematical guide to accurate inference. the theorem rule now years old marked the beginning of statistical inference as a serious scientific subject. it has waxed and waned in influence over the centuries now waxing again in the service of computer-age applications. bayesian inference if not directly opposed to frequentism is at least orthogonal. it reveals some worrisome flaws in the frequentist point of view while at the same time exposing itself to the criticism of dangerous overuse. the struggle to combine the virtues of the two philosophies has become more acute in an era of massively complicated data sets. much of what follows in succeeding chapters concerns this struggle. here we will review some basic bayesian ideas and the ways they impinge on frequentism. the fundamental unit of statistical inference both for frequentists and for bayesians is a family of probability densities d x x f x the observed data is a in the sample space x while the unobserved parameter is a point in the parameter space the statistician observes x from and infers the value of perhaps the most familiar case is the normal family d e both x and may be scalars vectors or more complicated objects. other names for the generic x and occur in specific situations for instance x for x in chapter we will also call f a family of probability distributions. bayesian inference exactly the one-dimensional normal translation with vari ance with both x and equaling the entire real line another central example is the poisson family d e where x is the nonnegative integers and is the nonnegative real line the density specifies the atoms of probability on the discrete points of x probability family f the knowledge of a prior density bayesian inference requires one crucial assumption in addition to the represents prior information concerning the parameter available to the statistician before the observation of x. for instance in an application of the normal model it could be known that is positive while past experience shows it never exceeding in which case we might take to be the uniform density d on the interval exactly what constitutes prior knowledge is a crucial question we will consider in ongoing discussions of bayes theorem. bayes theorem is a rule for combining the prior knowledge in with the current evidence in x. let denote the posterior density of that is our update of the prior density after taking account of observation x. bayes rule provides a simple expression for in terms of and f. bayes rule d where f is the marginal density of x f dz integral in would be a sum if were discrete. the rule is a straightforward exercise in conditional and yet has far-reaching and sometimes surprising consequences. in bayes formula x is fixed at its observed value while varies over just the opposite of frequentist calculations. we can emphasize this standard notation is x n for a normal distribution with expectation and variance so has x n is the ratio of the joint probability of the pair x and f the marginal probability of x. bayesian inference by rewriting as d where is the likelihood function that is with x fixed and varying. having computed the constant cx can be determined numerically from the requirement that integrate to obviating the calculation of f note multiplying the likelihood function by any fixed constant has no effect on since can be absorbed into cx. so for the poisson family we can take d e ignoring the x factor which acts as a constant in bayes rule. the luxury of ignoring factors depending only on x often simplifies bayesian calculations. for any two points and in the ratio of posterior densities is by division in d longer involving the marginal density f that is the posterior odds ratio is the prior odds ratio times the likelihood ratio a memorable restatement of bayes rule. two examples a simple but genuine example of bayes rule in action is provided by the story of the physicist s twins thanks to sonograms a physicist found out she was going to have twin boys. what is the probability my twins will be identical rather than fraternal? she asked. the doctor answered that one-third of twin births were identicals and two-thirds fraternals. in this situation the unknown parameter state of nature is either identical or fraternal with prior probability or x the possible sonogram results for twin births is either same sex or different sexes and x d same sex was observed. can ignore sex since that does not affect the calculation. a crucial fact is that identical twins are always same-sex while fraternals have probability of same or different so same sex in the sonogram is twice as likely if the twins are identical. applying bayes two examples rule in ratio form answers the physicist s question g.identicalj same g.fraternalj same d g.identical g.fraternal fidentical.same ffraternal.same d d that is the posterior odds are even and the physicist s twins have equal probabilities of being identical or here the doctor s prior odds ratio to in favor of fraternal is balanced out by the sonogram s likelihood ratio of to in favor of identical. figure analyzing the twins problem. there are only four possible combinations of parameter and outcome x in the twins problem labeled a b c and d in figure cell b has probability since identicals cannot be of different sexes. cells c and d have equal probabilities because of the random sexes of fraternals. finally a c b must have total probability and c c d total probability according to the doctor s prior distribution. putting all this together we can fill in the probabilities for all four cells as shown. the physicist knows she is in the first column of the table where the conditional probabilities of identical or fraternal are equal just as provided by bayes rule in presumably the doctor s prior distribution came from some enormous state or national database say three million previous twin births one million identical pairs and two million fraternals. we deduce that cells a c and d must have had one million entries each in the database while cell b was empty. bayes rule can be thought of as a big book with one page they turned out to be fraternal. identical twins are fraternal same sex different physicist sonogram shows doctor b a c d bayesian inference for each possible outcome x. book has only two pages in figure the physicist turns to the page same sex and sees two million previous twin births half identical and half fraternal correctly concluding that the odds are equal in her situation. given any prior distribution and any family of densities bayes rule will always provide a version of the big book. that doesn t mean that the book s contents will always be equally convincing. the prior for the twins problems was based on a large amount of relevant previous experience. such experience is most often unavailable. modern bayesian practice uses various strategies to construct an appropriate prior in the absence of prior experience leaving many statisticians unconvinced by the resulting bayesian inferences. our second example illustrates the difficulty. table scores from two tests taken by students mechanics and vectors. mechanics vectors mechanics vectors table shows the scores on two tests mechanics and vectors achieved by n d students. the sample correlation coefficient between the two scores is o o d nm.vi nv d with m and v short for mechanics and vectors nm and nv their averages. we wish to assign a bayesian measure of posterior accuracy to the true correlation coefficient true meaning the correlation for the hypothetical population of all students of which we observed only if we assume that the joint v distribution is bivariate normal discussed in chapter then the density of o as a function of has a known form two examples o z cosh w dw d o f o in terms of our general bayes notation parameter is observation x is o and family f is given by with both and x equaling the interval formula looks formidable to the human eye but not to the computer eye which makes quick work of it. figure student scores data posterior density of correlation for three possible priors. in this case as in the majority of scientific situations we don t have a trove of relevant past experience ready to provide a prior g. one expedient going back to laplace is the principle of insufficient reason that is we take to be uniformly distributed over g. d for a flat prior. the solid black curve in figure shows the resulting posterior density which is just the likelihood f plotted as a function of scaled to have integral bayesian inference jeffreys prior gjeff. d yields posterior density g.j o shown by the dashed red curve. it suggests somewhat bigger values for the unknown parameter formula arises from a theory of uninformative priors discussed in the next section an improvement on the principle of insufficient reason is an improper density in thatr g. d d but it still provides proper pos terior densities when deployed in bayes rule the dotted blue curve in figure is posterior density g.j o from the triangular-shaped prior obtained g. d jj this is a primitive example of a shrinkage prior one designed to favor smaller values of its effect is seen in the leftward shift of the posterior density. shrinkage priors will play a major role in our discussion of largescale estimation and testing problems where we are hoping to find a few large effects hidden among thousands of negligible ones. uninformative prior distributions given a convincing prior distribution bayes rule is easier to use and produces more satisfactory inferences than frequentist methods. the dominance of frequentist practice reflects the scarcity of useful prior information in day-to-day scientific applications. but the bayesian impulse is strong and almost from its inception years ago there have been proposals for the construction of priors that permit the use of bayes rule in the absence of relevant experience. one approach perhaps the most influential in current practice is the employment of uninformative priors. uninformative has a positive connotation here implying that the use of such a prior in bayes rule does not tacitly bias the resulting inference. laplace s principle of insufficient reason i.e. assigning uniform prior distributions to unknown parameters is an obvious attempt at this goal. its use went unchallenged for more than a century perhaps because of laplace s influence more than its own virtues. venn the venn diagram in the and fisher in the attacking the routine use of bayes theorem pointed out that laplace s principle could not be applied consistently. in the student correlation example for instance a uniform prior distribution for would not be uniform if we changed parameters to d e posterior probabilities such as uninformative prior distributions n o o d pr n o o pr would depend on whether or was taken to be uniform a priori. neither choice then could be considered uninformative. a more sophisticated version of laplace s principle was put forward by jeffreys beginning in the it depends interestingly enough on the frequentist notion of fisher information for a one-parameter family where the parameter space is an interval of the real line the fisher information is defined to be d log the poisson family d and d the jeffreys prior is by definition i d because equals approximately the variance equivalent definition is of the mle an formula does in fact transform correctly under parameter changes avoiding the venn fisher criticism. it is known that o approximate standard deviation in family has d d yielding jeffreys prior from the constant factor c having no effect on bayes rule the red triangles in figure indicate the credible interval z for based on jeffreys prior. that is the posterior probability equals o d d with probability for or it is not an accident that this nearly equals the standard neyman confidence interval based on o jeffreys prior tends to induce this nice connection between f the bayesian and frequentist worlds at least in one-parameter families. multiparameter probability families chapter make everything more bayesian inference difficult. suppose for instance the statistician observes independent versions of the normal model with possibly different values of xi n for i d in standard notation. jeffreys prior is flat for any one of the problems which is reasonable for dealing with them separately but the joint jeffreys prior d constant also flat can produce disastrous overall results as discussed in chapter computer-age applications are often more like than except with hundreds or thousands of cases rather than to consider simultaneously. uninformative priors of many sorts including jeffreys are highly popular in current applications as we will discuss. this leads to an interplay between bayesian and frequentist methodology the latter intended to control possible biases in the former exemplifying our general theme of computer-age statistical inference. flaws in frequentist inference bayesian statistics provides an internally consistent coherent program of inference. the same cannot be said of frequentism. the apocryphal story of the meter reader makes the point an engineer measures the voltages on a batch of tubes using a voltmeter that is normally calibrated x n x being any one measurement and the true batch voltage. the measurements range from to with an average of nx d which he reports back as an unbiased estimate of the next day he discovers a glitch in his voltmeter such that any voltage exceeding would have been reported as x d his frequentist statistician tells him that nx d is no longer unbiased for the true expectation since no longer completely describes the probability family. statistician says that is a little too small. the fact that the glitch didn t affect any of the actual measurements doesn t let him off the hook nx from the actual nx would not be unbiased for in future realizations of probability model. a bayesian statistician comes to the meter reader s rescue. for any prior density the posterior density d where x is the vector of measurements depends only on the data x actually flaws in frequentist inference observed and not on other potential data sets x that might have been seen. the flat jeffreys prior d constant yields posterior expectation nx d for irrespective of whether or not the glitch would have affected readings above figure z-values against null hypothesis d for months through a less contrived version of the same phenomenon is illustrated in figure an ongoing experiment is being run. each month i an independent normal variate is observed with the intention of testing the null hypothesis w d versus the alternative the plotted points are test statistics n xi zi d ix xj zi i a z-value based on all the data up to month i at month the scheduled end of the experiment d just exceeding the upper point for a n distribution. victory! the investigators get to claim significant rejection of at level i n iz bayesian inference unfortunately it turns out that the investigators broke protocol and peeked at the data at month in the hope of being able to stop an expensive experiment early. this proved a vain hope d not being anywhere near significance so they continued on to month as originally planned. this means they effectively used the stopping rule stop and declare significance if either or exceeds some computation shows that this rule had probability not of rejecting if it were true. victory has turned into defeat according to the honored frequentist criterion. once again the bayesian statistician is more lenient. the likelihood function for the full data set x d d e is the same irrespective of whether or not the experiment might have stopped early. the stopping rule doesn t affect the posterior distribution which depends on x only through the likelihood figure unbiased effect-size estimates for genes prostate cancer study. the estimate for gene is d what is its effect size? the lenient nature of bayesian inference can look less benign in multi effect size estimatesfrequency a bayesianfrequentist comparison list parameter settings. figure concerns a prostate cancer study comparing patients with healthy controls. each man had his genetic activity measured for a panel of n d genes. a statistic x was computed for each comparing the patients with controls say i d n xi n where represents the true effect size for gene i. most of the genes probably not being involved in prostate cancer would be expected to have effect sizes near but the investigators hoped to spot a few large values either positive or negative. the histogram of the xi values does in fact reveal some large values d being the winner. question what estimate should we give for even though was individually unbiased for a frequentist would worry that focusing attention on the largest of values would produce an upward bias and that our estimate should downwardly correct selection bias regression to the mean and the winner s curse are three names for this phenomenon. bayesian inference surprisingly is immune to selection bias. irrespec- tive of whether gene was prespecified for particular attention or only came to attention as the winner the bayes estimate for given all the data stays the same. this isn t obvious but follows from the fact that any data-based selection process does not affect the likelihood function in what does affect bayesian inference is the prior for the full vector of effect sizes. the flat prior constant results in the dangerous overestimate d d a more appropriate uninformative prior appears as part of the empirical bayes calculations of chapter gives d the operative point here is that there is a price to be paid for the desirable properties of bayesian inference. attention shifts from choosing a good frequentist procedure to choosing an appropriate prior distribution. this can be a formidable task in high-dimensional problems the very kinds featured in computer-age inference. a bayesianfrequentist comparison list bayesians and frequentists start out on the same playing field a family of probability distributions but play the game in orthogonal the statistic was the two-sample t-statistic transformed to normality see the endnotes. bayesian inference directions as indicated schematically in figure bayesian inference proceeds vertically with x fixed according to the posterior distribution while frequentists reason horizontally with fixed and x varying. advantages and disadvantages accrue to both strategies some of which are compared next. figure bayesian inference proceeds vertically given x frequentist inference proceeds horizontally given bayesian inference requires a prior distribution when past experience provides as in the twins example there is every good reason to employ bayes theorem. if not techniques such as those of jeffreys still permit the use of bayes rule but the results lack the full logical force of the theorem the bayesian s right to ignore selection bias for instance must then be treated with caution. frequentism replaces the choice of a prior with the choice of a method or algorithm t designed to answer the specific question at hand. this adds an arbitrary element to the inferential process and can lead to meterreader kinds of contradictions. optimal choice of t reduces arbitrary behavior but computer-age applications typically move outside the safe waters of classical optimality theory lending an ad-hoc character to frequentist analyses. modern data-analysis problems are often approached via a favored meth a bayesianfrequentist comparison list odology such as logistic regression or regression trees in the examples of chapter this plays into the methodological orientation of frequentism which is more flexible than bayes rule in dealing with specific algorithms one always hopes for a reasonable bayesian justification for the method at hand. having chosen only a single probability distribution is in play for bayesians. frequentists by contrast must struggle to balance the behavior of t over a family of possible distributions since in figure is unknown. the growing popularity of bayesian applications begun with uninformative priors reflects their simplicity of application and interpretation. the simplicity argument cuts both ways. the bayesian essentially bets it all on the choice of his or her prior being correct or at least not harmful. frequentism takes a more defensive posture hoping to do well or at least not poorly whatever might be. a bayesian analysis answers all possible questions at once for example estimating efgfrg or prfgfr or anything else relating to figure frequentism focuses on the problem at hand requiring different estimators for different questions. this is more work but allows for more intense inspection of particular problems. in situation for example estimators of the form x c might be investigated for different choices of the constant c hoping to reduce expected mean-squared error. the simplicity of the bayesian approach is especially appealing in dynamic contexts where data arrives sequentially and updating one s beliefs is a natural practice. bayes rule was used to devastating effect before the us presidential election updating sequential polling results to correctly predict the outcome in all states. bayes theorem is an excellent tool in general for combining statistical evidence from disparate sources the closest frequentist analog being maximum likelihood estimation. in the absence of genuine prior information a whiff of hangs over bayesian results even those based on uninformative priors. classical frequentism claimed for itself the high ground of scientific objectivity especially in contentious areas such as drug testing and approval where skeptics as well as friends hang on the statistical details. figure is soothingly misleading in its schematics and x will here we are not discussing the important subjectivist school of bayesian inference of savage de finetti and others covered in chapter bayesian inference typically be high-dimensional in the chapters that follow sometimes very high-dimensional straining to the breaking point both the frequentist and the bayesian paradigms. computer-age statistical inference at its most successful combines elements of the two philosophies as for instance in the empirical bayes methods of chapter and the lasso in chapter there are two potent arrows in the statistician s philosophical quiver and faced say with parameters and data points there s no need to go hunting armed with just one of them. notes and details thomas bayes if transferred to modern times might well be employed as a successful professor of mathematics. actually he was a mid-eighteenthcentury nonconformist english minister with substantial mathematical interests. richard price a leading figure of letters science and politics had bayes theorem published in the transactions of the royal society years after bayes death his interest being partly theological with the rule somehow proving the existence of god. bellhouse s biography includes some of bayes other mathematical accomplishments. harold jeffreys was another part-time statistician working from his day job as the world s premier geophysicist of the inter-war period fierce opponent of the theory of continental drift. what we called uninformative priors are also called noninformative or objective. jeffreys brand of bayesianism had a dubious reputation among bayesians in the period with preference going to subjective analysis of the type advocated by savage and de finetti. the introduction of markov chain monte carlo methodology was the kind of technological innovation that changes philosophies. mcmc being very well suited to jeffreys-style analysis of big data problems moved bayesian statistics out of the textbooks and into the world of computer-age applications. berger makes a spirited case for the objective bayes approach. correlation coefficient density. formula for the correlation coefficient density was r. a. fisher s debut contribution to the statistics literature. chapter of johnson and kotz gives several equivalent forms. the constant c in is often taken to be with n the sample size. ters from to in a smoothly differentiable way. the new family q jeffreys prior and transformations. suppose we change notes and details satisfies log q d and qgjeff. d then q says that transforms correctly to qgjeff. d log but this just the meter-reader fable is taken from edwards book likelihood where he credits john pratt. it nicely makes the point that frequentist inferences which are calibrated in terms of possible observed data sets x may be inappropriate for the actual observation x. this is the difference between working in the horizontal and vertical directions of figure two-sample t-statistic. applied to gene i s data in the prostate study the two-sample t-statistic ti has theoretical null hypothesis distribution a student s t distribution with degrees of freedom xi where and are the cumulative distribuin is tion functions of standard normal and variables. section of efron motivates approximation selection bias. senn discusses the immunity of bayesian inferences to selection bias and other paradoxes crediting phil dawid for the original idea. the article catches the possible uneasiness of following bayes theorem too literally in applications. the students in table were randomly selected from a larger data set of in mardia et al. gave o d welch and peers initiated the study of priors whose credible intervals such as in figure match frequentist confidence intervals. in one-parameter problems jeffreys priors provide good matches but not ususally in multiparameter situations. in fact no single multiparameter prior can give good matches for all one-parameter subproblems a source of tension between bayesian and frequentist methods revisited in chapter fisherian inference and maximum likelihood estimation sir ronald fisher was arguably the most influential anti-bayesian of all time but that did not make him a conventional frequentist. his key dataanalytic methods analysis of variance significance testing and maximum likelihood estimation were almost always applied frequentistically. their fisherian rationale however often drew on ideas neither bayesian nor frequentist in nature or sometimes the two in combination. fisher s work held a central place in twentieth-century applied statistics and some of it particularly maximum likelihood estimation has moved forcefully into computer-age practice. this chapter s brief review of fisherian methodology sketches parts of its unique philosophical structure while concentrating on those topics of greatest current importance. likelihood and maximum likelihood fisher s seminal work on estimation focused on the likelihood function or more exactly its logarithm. for a family of probability densities the log likelihood function is d the notation emphasizing that the parameter vector is varying while the observed data vector x is fixed. the maximum likelihood estimate is the value of in parameter space that maximizes mle w d arg max it can happen that doesn t exist or that there are multiple maximizers but here we will assume the usual case where exists uniquely. more careful references are provided in the endnotes. definition is extended to provide maximum likelihood estimates likelihood and maximum likelihood for a function d t of according to the simple plug-in rule o d t most often with being a scalar parameter of particular interest such as the regression coefficient of an important covariate in a linear model. maximum likelihood estimation came to dominate classical applied estimation practice. less dominant now for reasons we will be investigating in subsequent chapters the mle algorithm still has iconic status being often the method of first choice in any novel situation. there are several good reasons for its ubiquity. the mle algorithm is automatic in theory and almost in practice a single numerical algorithm produces without further statistical input. this contrasts with unbiased estimation for instance where each new situation requires clever theoretical calculations. the mle enjoys excellent frequentist properties. in large-sample situations maximum likelihood estimates tend to be nearly unbiased with the least possible variance. even in small samples mles are usually quite efficient within say a few percent of the best possible performance. the mle also has reasonable bayesian justification. looking at bayes rule d we see that is the maximizer of the posterior density if the prior is flat that is constant. because the mle depends on the family f only through the likelihood function anomalies of the meter-reader type are averted. figure displays two maximum likelihood estimates for the gfr data of figure here the is the vector x d xn n d we assume that x was obtained as a random sample of size n from a density for i d n xi iid abbreviating independent and identically distributed. two families are considered for the component density the normal with d d e now x is what we have been calling x before while we will henceforth use x as a symbol for the individual components of x. fisherian inference and mle figure glomerular filtration data of figure and two maximum-likelihood density estimates normal black and gamma blue. and the with d d e x otherwise under iid sampling we have since d ny log d nx hx d nx d dnx o lxi maximum likelihood estimates were found by maximizing for the normal model the gamma distribution is usually defined with d as the lower limit of x. here we are allowing the lower limit to vary as a free parameter. fisher information and the mle there is no closed-form solution for gamma model where numerical maximization gave d the plotted curves in figure are the two mle densities f the gamma model gives a better fit than the normal but neither is really satisfactory. more ambitious maximum likelihood fit appears in figure most mles require numerical minimization as for the gamma model. when introduced in the maximum likelihood was criticized as computationally difficult invidious comparisons being made with the older method of moments which relied only on sample moments of various kinds. there is a downside to maximum likelihood estimation that remained nearly invisible in classical applications it is dangerous to rely upon in problems involving large numbers of parameters. if the parameter vector has components each component individually may be well estimated by maximum likelihood while the mle o d t for a quantity of particular interest can be grossly misleading. for the prostate data of figure model gives mle d xi for each of the genes. this seems reasonable but if we are interested in the maximum coordinate value d t d max the mle is o d almost certainly a flagrant overestimate. regularized versions of maximum likelihood estimation more suitable for highdimensional applications play an important role in succeeding chapters. i fisher information and the mle fisher was not the first to suggest the maximum likelihood algorithm for parameter estimation. his paradigm-shifting work concerned the favorable inferential properties of the mle and in particular its achievement of the fisher information bound. only a brief heuristic review will be provided here with more careful derivations referenced in the endnotes. we with a one-parameter family of densities g d ff x x the multiparameter case is considered in the next chapter. f fisherian inference and mle where is an interval of the real line possibly infinite while the samthe continuous case with the probability of set a equalingr ple space x may be multidimensional. in the poisson example f can represent a discrete density but for convenience we assume here a f dx etc. the log likelihood function is lx. d log f and the mle o d arg maxflx. with replacing in in the one-dimensional case. dots will indicate differentiation with respect to e.g. for the score function p lx. d log f d p f the score function has expectation z lx. dx dz p x x z x d f dx p f dx d d where we are assuming the regularity conditions necessary for differentiating under the integral sign at the third step. the fisher information i is defined to be the variance of the score function p lx. dx i dz x the notation p lx. indicating that p lx. has mean and variance i. the term information is well chosen. the main result for maximum likelihood estimation sketched next is that the mle o has an approximately normal distribution with mean and variance o n and that no nearly unbiased estimator of can do better. in other words bigger fisher information implies smaller variance for the mle. the second derivative of the log likelihood function r lx. d log f d r f f f f p fisher information and the mle has expectation e r nr lx. o d i f term having expectation as in we can write lx. where j is the variance of r lx. now suppose that x d xn is an iid sample from f as in so that the total score function p lx. d nx lx. as in is p lxi lx. d nx p o lx. c r o d a first-order taylor series gives the approximation based on the full sample x satisfies the maximizing condition the mle o p lx. lxi d p lx p and similarly o lx. or o c p lx. lx. under reasonable regularity conditions and the central limit theorem imply that p lx. while the law of large numbers has n lx. approaching the constant i putting all of this together produces fisher s fundamental theo rem for the mle that in large samples o n this is the same as result since the total fisher information in an iid sample is ni as can be seen by taking expectations in in the case of normal sampling xi n for i d n fisherian inference and mle this gives with known we compute the log likelihood n nx lx. d nx lx. d n yielding the familiar result o d nx and since i d o p lx. d and r n from this brings us to an aspect of fisherian inference neither bayesian nor frequentist. fisher believed there was a logic of inductive inference that would produce the correct answer to any statistical question in the same way ordinary logic solves deductive problems. his principal tactic was to logically reduce a complicated inferential question to a simple form where the solution should be obvious to all. fisher s favorite target for the obvious was where a single scalar observation o is normally distributed around the unknown parameter of interest with known variance then everyone should agree in the absence of prior information that o is the best estimate of that has about chance of lying in the interval o fisher was astoundingly resourceful at reducing statistical problems to the form sufficiency efficiency conditionality and ancillarity were all brought to bear with the maximum likelihood approximation being the most influential example. fisher s logical system is not in favor these days but its conclusions remain as staples of conventional statistical practice. suppose that q d t is any unbiased estimate of based on an iid sample x d xn from f that is p n etc. then the cram er rao lower bound described in the endnotes says that the variance of q exceeds the fisher information bound d eft n q o var a loose interpretation is that the mle has variance at least as small as the best unbiased estimate of the mle is generally not unbiased but conditional inference p its bias is small order compared with standard deviation of order n making the comparison with unbiased estimates and the cram er rao bound appropriate. conditional inference a simple example gets across the idea of conditional inference an i.i.d. sample xi n has produced estimate o d nx. the investigators originally disagreed on an affordable sample size n and flipped a fair coin to decide i d n probability probability n d p if you answered n d won. question what is the standard deviation of nx? d then you like fisher are an advocate of conditional inference. the unconditional frequentist answer says that nx could have been n or n with equal probability yielding standard deviation c d some less obvious less trivial examples follow in this section and in chapter where conditional inference plays a central role. the data for a typical regression problem consists of pairs yi i d n where xi is a p-dimensional vector of covariates for the ith subject and yi is a scalar response. in figure xi is age and yi the kidney fitness measure tot. let x be the n p matrix having xi as its ith row and y the vector of responses. a regression algorithm uses x and y to construct a function rxy predicting y for any value of x as in where o how accurate is rxy this question is usually answered under the assumption that x is fixed not random in other words by conditioning on the observed value of x. the standard errors in the second line of table are conditional in this sense they are frequentist standard deviations c o of o assuming that the values for age are fixed as observed. correlation analysis between age and tot would not make this assumption. were obtained using least squares. and o fisher argued for conditional inference on two grounds. fisherian inference and mle more relevant inferences. the conditional standard deviation in situation seems obviously more relevant to the accuracy of the observed o for estimating it is less obvious in the regression example though arguably still the case. simpler inferences. conditional inferences are often simpler to execute and interpret. this is the case with regression where the statistician doesn t have to worry about correlation relationships among the covariates and also with our next example a fisherian classic. table shows the results of a randomized trial on ulcer patients comparing new and old surgical treatments. was the new surgery significantly better? fisher argued for carrying out the hypothesis test conditional on the marginals of the table with the marginals fixed the number y in the upper left cell determines the other three cells by subtraction. we need only test whether the number y d is too big under the null hypothesis of no treatment difference instead of trying to test the numbers in all four table forty-five ulcer patients randomly assigned to either new or old surgery with results evaluated as either success or failure. was the new surgery significantly better? success failure new old an ancillary statistic fisher s terminology is one that contains no direct information by itself but does determine the conditioning framework for frequentist calculations. our three examples of ancillaries were the sample size n the covariate matrix x and the table s marginals. contains no information is a contentious claim. more realistically the two advantages of conditioning relevance and simplicity are thought to outweigh the loss of information that comes from treating the ancillary statistic as nonrandom. chapter makes this case specifically for standard survival analysis methods. section gives the details of such tests in the surgery example the difference was not significant. conditional inference our final example concerns the accuracy of a maximum likelihood estimate o rather than o o n o the plug-in version of fisher suggested using n where i.x is the observed fisher information o d lx. i.x d lx the expectation of i.x is ni so in large samples the distribution converges to before convergence however fisher suggested that gives a better idea of o as a check a simulation was run involving i.i.d. samples x of size n d s accuracy. drawn from a cauchy density f d c within each group was then calculated. samples x of size n d were drawn d and the observed information bound computed for each. the o values were grouped according to deciles of and the observed empirical variance of o this amounts to calculating a somewhat crude estimate of the conditional variance of the mle o given the observed information bound figure shows the results. we see that the conditional variance is close to as fisher predicted. the conditioning effect is quite substantial the unconditional variance is here while the conditional variance ranges from to the observed fisher information i.x acts as an approximate ancillary enjoying both of the virtues claimed by fisher it is more relevant than the unconditional information nio and it is usually easier to calculate. once o has been found i.x is obtained by numerical second differentiation. unlike i no probability calculations are required. there is a strong bayesian current flowing here. a narrow peak for the log likelihood function i.e. a large value of i.x also implies a narrow posterior distribution for given x. conditional inference of which figure is an evocative example helps counter the central bayesian criticism of frequentist inference that the frequentist properties relate to data sets possibly much different than the one actually observed. the maximum fisherian inference and mle figure conditional variance of mle for cauchy samples of size plotted versus the observed information bound observed information bounds are grouped by quantile intervals for variance calculations percentages the broken red horizontal line is the unconditional variance likelihood algorithm can be interpreted both vertically and horizontally in figure acting as a connection between the bayesian and frequentist worlds. the equivalent of result for multiparameter families section i.x plays an important role in succeeding chapters with the matrix of second derivatives np d i.x d log information boundmle variance permutation and randomization permutation and randomization fisherian methodology faced criticism for its overdependence on normal sampling assumptions. consider the comparison between the all and aml patients in the gene leukemia example of figure the twosample t-statistic had value with two-sided significance level according to a student-t null distribution with degrees of freedom. all of this depended on the gaussian or normal assumptions as an alternative significance-level calculation fisher suggested using permutations of the data points. the values are randomly divided into disjoint sets of size and and the two-sample t-statistic is recomputed. this is done some large number b times yielding b. the two-sided permutation significance level tion t-values t for the original value t is then the proportion of the t i values exceeding t in absolute value t t j jtjg i figure permutation t for gene in the leukemia data of figure of these ticks exceeded in absolute value the observed t t-statistic giving permutation significance level for testing all vs aml valuesfrequency statistic fisherian inference and mle figure shows the histogram of b d t i values for the gene data in figure of these exceeded t d in absolute value yielding significance level against the null hypothesis of no allaml difference remarkably close to the normal-theory significance level were a little lucky here. why should we believe the permutation significance level fisher provided two arguments. suppose we assume as a null hypothesis that the n d observed measurements x are an iid sample obtained from the same distribution xi for i d n is no normal assumption here say that is n let o indicate the order statistic of x i.e. the numbers ordered from smallest to largest with their aml or all labels removed. then it can be shown that all ways of obtaining x by dividing o into disjoint subsets of sizes and are equally likely under null hypothesis a small value of the permutation significance level indicates that the actual division of amlall measurements was not random but rather resulted from negation of the null hypothesis this might be considered an example of fisher s logic of inductive inference where the conclusion should be obvious to all. it is certainly an example of conditional inference now with conditioning used to avoid specific assumptions about the sampling density in experimental situations fisher forcefully argued for randomization that is for randomly assigning the experimental units to the possible treatment groups. most famously in a clinical trial comparing drug a with drug b each patient should be randomly assigned to a or b. randomization greatly strengthens the conclusions of a permutation test. in the amlall situation where randomization wasn t feasible we wind up almost certain that the aml group has systematically larger numbers but cannot be certain that it is the different disease states causing the difference. perhaps the aml patients are older or heavier or have more of some other characteristic affecting gene experimental randomization almost guarantees that age weight etc. will be wellbalanced between the treatment groups. fisher s rct clinical trial was and is the gold standard for statistical inference in medical trials. permutation testing is frequentistic a statistician following the procedure has chance of rejecting a valid null hypothesis at level etc. notes and details randomization inference is somewhat different amounting to a kind of forced frequentism with the statistician imposing his or her preferred probability mechanism upon the data. permutation methods are enjoying a healthy computer-age revival in contexts far beyond fisher s original justification for the t-test as we will see in chapter notes and details on a linear scale that puts bayesian on the left and frequentist on the right fisherian inference winds up somewhere in the middle. fisher rejected bayesianism early on but later criticized as wooden the hard-line frequentism of the neyman wald decision-theoretic school. efron locates fisher along the bayes frequentist scale for several different criteria see in particular figure of that paper. bayesians of course believe there is only one true logic of inductive inference. fisher disagreed. his most ambitious attempt to enjoy the bayesian omelette without breaking the bayesian eggs was fiducial inference. the simplest example concerns the normal translation model x n where x has a standard n distribution the fiducial distribution of given x then being n among fisher s many contributions fiducial inference was the only outright popular bust. nevertheless the idea has popped up again in the current literature under the name confidence distribution see efron and xie and singh a brief discussion appears in chapter for an unbiased estimator q lx. d x dz p z t x d t we have z x d p f d x d d t x t d x here x is x n the sample space of x d xn and we are givesr suming the conditions necessary for differentiating under the integral sign lx. has expectation p lx. d x d p attributed to the important bayesian theorist l. j. savage. fisherian inference and mle and then applying the cauchy schwarz inequality x or f x p lx. d x x o n q i x p lx. d x var this verifies the cram er rao lower bound the optimal variance for an unbiased estimator is one over the fisher information. optimality results are a sign of scientific maturity. fisher information and its estimation bound mark the transition of statistics from a collection of ad-hoc techniques to a coherent discipline. have lost some ground recently where as discussed in chapter ad-hoc algorithmic coinages have outrun their inferential justification. fisher s information bound was a major mathematical innovation closely related to and predating heisenberg s uncertainty principle and shannon s information bound see dembo et al. unbiased estimation has strong appeal in statistical applications where biased its opposite carries a hint of self-interested data manipulation. in large-scale settings such as the prostate study of figure one can however strongly argue for biased estimates. we saw this for gene where the usual unbiased estimate d is almost certainly too large. biased estimation will play a major role in our subsequent chapters. maximum likelihood estimation is effectively unbiased in most situa tions. under repeated sampling the expected mean squared error mse d e has order-of-magnitude variance d and d the latter usually becoming negligible as sample size n increases. exceptions where bias is substantial can occur if o d t when is high-dimensional as in the james stein situation of chapter section of efron provides a detailed analysis. section of cox and hinkley gives a careful and wide-ranging account of the mle and fisher information. lehmann covers the same ground somewhat more technically in his chapter o d variance c parametric models and exponential families we have been reviewing classic approaches to statistical inference frequentist bayesian and fisherian with an eye toward examining their strengths and limitations in modern applications. putting philosophical differences aside there is a common methodological theme in classical statistics a strong preference for low-dimensional parametric models that is for modeling data-analysis problems using parametric families of probability densities d x x f where the dimension of parameter is small perhaps no greater than or or the inverted nomenclature nonparametric suggests the predominance of classical parametric methods. two words explain the classic preference for parametric models mathematical tractability. in a world of sliderules and slow mechanical arithmetic mathematical formulation by necessity becomes the computational tool of choice. our new computation-rich environment has unplugged the mathematical bottleneck giving us a more realistic flexible and far-reaching body of statistical techniques. but the classic parametric families still play an important role in computer-age statistics often assembled as small parts of larger methodologies with the generalized linear models of chapter this presents a brief review of the most widely used parametric models ending with an overview of exponential families the great connecting thread of classical theory and a player of continuing importance in computer-age applications. this chapter covers a large amount of technical material for use later and may be reviewed lightly at first reading. parametric models univariate families univariate parametric families in which the sample space x of observation x is a subset of the real line are the building blocks of most statistical analyses. table names and describes the five most familiar univariate families normal poisson binomial gamma and beta. chi-squared distribution with n degrees of freedom n is also included since it is dis tributed as the normal distribution n is a shifted and scaled version of the n used in n c table five familiar univariate densities and their sample spaces x parameter spaces and expectations and variances chi-squared distribution with n degrees of freedom is name notation expectation variance density x normal n poisson binomial bi.n gamma beta p e e x x n ng x x c relationships abound among the table s families. for instance independent gamma variables and yield a beta variate according to c the binomial and poisson are particularly close cousins. a bi.n distribution number of heads in n independent flips of a coin with the notation in indicates that if x n and y n then x and c have the same distribution. the multivariate normal distribution figure comparison of the binomial distribution lines with the poisson dots. in the legend we show the mean and standard deviation for each distribution. ity of heads approaches a distribution bi.n as n grows large and small the notation indicating approximate equality of the two distributions. figure shows the approximation already working quite effectively for n d and d the five families in table have five different sample spaces making them appropriate in different situations. beta distributions for example are natural candidates for modeling continuous data on the unit interval choices of the two parameters provide a variety of possible shapes as illustrated in figure later we will discuss general exponential families unavailable in classical theory that greatly expand the catalog of possible shapes. the multivariate normal distribution classical statistics produced a less rich catalog of multivariate distributions ones where the sample space x exists in rp p-dimensional eu parametric models figure three beta densities with indicated. clidean space p by far the greatest amount of attention focused on the multivariate normal distribution. a random vector x d xp mean vector normally distributed or not has d efxg and p p covariance d e of vectors u and v is the matrix having elements outer product uv ui vj we will use the convenient notation x for and reducing to the familiar form x in the univariate case. denoting the entries of by for i and j equaling p the diagonal elements are variances i d var.xi the notation d defines the ij th element of a matrix. the multivariate normal distribution the off-diagonal elements relate to the correlations between the coordinates of x cor.xi xj d i the multivariate normal distribution extends the univariate definition be a vector n in table to begin with let z d zp of p independent n variates with probability density function f d p e i d p e z z pp according to line of table the multivariate normal family is obtained by linear transformations of z let be a p-dimensional vector and t a p p nonsingular matrix and define the random vector following the usual rules of probability transformations yields the density of x d where is the p p symmetric positive definite matrix e j and j j its determinant the p-dimensional multivariate normal distribution with mean and covariance is denoted x figure illustrates the bivariate normal distribution with d and having d d and d d the bell-shaped mountain on the left is a plot of density the right panel shows a scatterplot of points drawn from this distribution. concentric ellipses illustrate curves of constant density d constant classical multivariate analysis was the study of the multivariate normal distribution both of its probabilistic and statistical properties. the notes reference some important lengthy multivariate texts. here we will just recall a couple of results useful in the chapters to follow. x d c t z d t t parametric models figure left bivariate normal density with d d and d right sample of pairs from this bivariate normal density. is partitioned into suppose that x d xp d and d c d p with and similarly partitioned np is is etc.. then the conditional distribution of given is itself normal if d d then reduces to c n here is familiar as the linear regression coefficient of as a equals the squared proportion tion of while of the variance of explained by hence we can write the variance term in as bayesian statistics also makes good use of the normal family. it helps to begin with the univariate case x n where now we assume that c fisher s information bound the expectation vector itself has a normal prior distribution n a n a and n bayes theorem and some algebra show that the posterior distribution of having observed x is normal m c a m a c n a c the posterior expectation d m c m is a shrinkage estimator of if say a equals then d m c m is shrunk half the way back from the unbiased estimate d x toward the prior mean m while the posterior variance of is only one-half that of the multivariate version of the bayesian setup is and now with m and p-vectors and a and positive definite matrices. as indicated in the notes the posterior distribution of given x is then np.m a c a.a c np which reduces to when p d m a.a c fisher s information bound for multiparameter families the multivariate normal distribution plays its biggest role in applications as a large-sample approximation for maximum likelihood estimates. we suppose that the parametric family of densities normal or not is smoothly defined in terms of its p-dimensional parameter vector terms of is a subset of rp. the mle definitions and results are direct analogues of the single-parameter calculations beginning at in chapter the score function p is now defined as the gradient of log d np log o d d the p-vector of partial derivatives of log with respect to the coordinates of it has mean zero p d parametric models using outer product notation by definition the fisher information matrix i for is the p p covariance matrix of p np i d the key result is that the mle d arg has an approximately normal distribution with covariance matrix log d log p approximation is justified by large-sample arguments say with x an iid sample in rp xn n going to infinity. suppose the statistician is particularly interested in the first coordinate of let d denote the other p coordinates of which are now nuisance parameters as far as the estimation of goes. according to the mle which is the first coordinate of has n where the notation indicates the upper leftmost entry of we can partition the information matrix i into the two parts corre sponding to and i d i of dimension and i i d the endnotes show that the subtracted term on the right side of is nonnegative implying that i if were known to the statistician rather than requiring estimation then would be a one-parameter family with fisher information for estimating giving n the multinomial distribution comparing with shows that the variance of the mle must always in the presence of nuisance parameters. maximum likelihood and in fact any form of unbiased or nearly unbiased estimation pays a nuisance tax for the presence of other parameters. modern applications often involve thousands of others think of regression fits with too many predictors. in some circumstances biased estimation methods can reverse the situation using the others to actually improve estimation of a target parameter see chapter on empirical bayes techniques and chapter on regularized regression models. the multinomial distribution second in the small catalog of well-known classic multivariate distributions is the multinomial. the multinomial applies to situations in which the observations take on only a finite number of discrete values say l of them. the ulcer surgery of table is repeated in table now with the cells labeled and here there are l d possible outcomes for each patient success failure success failure. table the ulcer study of table now with the cells numbered through as shown. success failure new old a number n of cases has been observed n d in table let x d xl be the vector of counts for the l possible outcomes xl d having outcome lg for the ulcer data. it is convenient to code the outcomes x d in terms of the coordinate vectors el of length l el d with a in the lth place. unless is a vector of zeros a condition that amounts to approximate independence of and parametric models figure the simplex is an equilateral triangle set at an angle to the coordinate axes in the multinomial probability model assumes that the n cases are independent of each other with each case having probability for outcome el d prfelg l d l indicate the vector of probabilities. the count vector x then follows the multinomial distribution let denoted d ly n xl d xl l n observations l outcomes probability vector the parameter space for is the simplex sl x multl.n lx w and d sl d figure shows an equilateral triangle sitting at an angle to the coordinate axes and the midpoint of the triangle d the multinomial distribution corresponds to a multinomial distribution putting equal probability on the three possible outcomes. figure sample space x for x numbers indicate the sample space x for x is the subset of nsl set of nonnegative vectors summing to n having integer components. figure illustrates the case n d and l d now with the triangle of figure multiplied by and set flat on the page. the point indicates x d with probability in the dichotomous case l d the multinomial distribution reduces to the binomial with equaling in line of table and equaling n x. the mean vector and covariance matrix of multl.n for any value of l are according to etc. x is the diagonal matrix with diagonal elements so var.xl d and covariance xj d generalizes the binomial mean and variance there is a useful relationship between the multinomial distribution and the poisson. suppose sl are independent poissons having possibly different parameters sl l d l or more concisely s with s d sl and d the independence being assumed in notation then the conditional distribution of s parametric models given the sum sc dp sl is multinomial dp sjsc multl.sc going in the other direction suppose n poi.n. then the uncondi tional or marginal distribution of multl.n is poisson if n poi.n multl.n calculations involving x multl.n are sometimes complicated by the multinomial s correlations. the approximation x removes the correlations and is usually quite accurate if n is large. there is one more important thing to say about the multinomial family it contains all distributions on a sample space x composed of l discrete categories. in this sense it is a model for nonparametric inference on x the nonparametric bootstrap calculations of chapter use the multinomial in this way. nonparametrics and the multinomial have played a larger role in the modern environment of large difficult to model data sets. exponential families classic parametric families dominated statistical theory and practice for a century and more with an enormous catalog of their individual properties means variances tail areas etc. being compiled. a surprise though a slowly emerging one beginning in the was that all of them were examples of a powerful general construction exponential families. what follows here is a brief introduction to the basic theory with further development to come in subsequent chapters. to begin with consider the poisson family line of table the ratio of poisson densities at two parameter values and is which can be re-expressed as d e d e where we have defined d and d looking at we can describe the poisson family in three steps. exponential families start with any one poisson distribution for any value of let d and calculate for x d q d e finally divide q by exp. to get the poisson density in other words we tilt with the exponential factor e x to get q and then renormalize q to sum to notice that gives as the renormalizing constant since e d e figure poisson densities for d heavy green curve with dots for d figure graphs the poisson density for d each poisson density is a renormalized exponential tilt of any other poisson density. so for instance is obtained from via the tilt e x with d d alternate expressions for as an exponential family are available for example exp. x where d log d exp. and d isn t necessary for to be a member of the family. parametric models the poisson is a one-parameter exponential family in that and x in expression are one-dimensional. a p-parameter exponential family has the form f d e where and y are p-vectors and a is contained in rp. here is the canonical or natural parameter vector and y d t is the sufficient statistic vector. the normalizing function which makes f integrate sum to one satisfies for a e dx and it can be shown that the parameter space a for which the integral is finite is a convex set in rp. as an example the gamma family on line of table is a two-parameter exponential family with and y d t given by e dz x and d log x d log c log d c logf the parameter space a is f and why are we interested in exponential tilting rather than some other transformational form? the answer has to do with repeated sampling. suppose x d xn is an iid sample from a p-parameter exponential family then letting yi d t denote the sufficient vector corresponding to xi f d ny e d en. where ny dpn yi this is still a p-parameter exponential family now with natural parameter n sufficient statistic ny and normalizer n no matter how large n may be the statistician can still compress all the inferential information into a p-dimensional statistic ny. only exponential families enjoy this property. even though they were discovered and developed in quite different contexts and at quite different times all of the distributions discussed in this exponential families chapter exist in exponential families. this isn t quite the coincidence it seems. mathematical tractability was the prized property of classic parametric distributions and tractability was greatly facilitated by exponential structure even if that structure went unrecognized. in one-parameter exponential families the normalizer is also known as the cumulant generating function. derivatives of yield the cumulants of the first two giving the mean and variance p d e fyg r d var fyg and similarly in p-parametric families and p d j d e fyg r d d cov fyg j k the p-dimensional expectation parameter denoted d e fyg is a one-to-one function of the natural parameter let v indicate the p p covariance matrix v d cov then the p p derivative matrix of with respect to is d d j k d v v d this following from the inverse mapping being d d as a one-parameter example the poisson in table has d log d y d x and d d d d v the maximum likelihood estimate for the expectation parameter is simply y ny under repeated sampling which makes it immediate to calculate in most situations. less immediate is the mle for the natural parameter the one-to-one mapping d p has inverse d p so o d p the simplified dot notation leads to more compact expressions p d d and r d d parametric models e.g. o d log y for the poisson. the trouble is that p is usually unavailable in closed form. numerical approximation algorithms are necessary to calculate o in most cases. all of the classic exponential families have closed-form expressions for f yielding pleasant formulas for the mean and covariance v modern computational technology allows us to work with general exponential families designed for specific tasks without concern for mathematical tractability. figure a seven-parameter exponential family fit to the gfr data of figure compared with gamma fit of figure as an example we again consider fitting the gfr data of figure for our exponential family of possible densities we take and sufficient statistic vector y.x d y in can represent all polynomials in x the gfr so at power gives the n family which we already know fits poorly from figure the heavy curve in figure shows the mle fit fo now following the gfr histogram quite closely. chapter discusses lindsey s method a simplified algorithm for calculating the mle o any intercept in the polynomial is absorbed into the term in family notes and details a more exotic example concerns the generation of random graphs on a fixed set of n nodes. each possible graph has a certain total number e of edges and t of triangles. a popular choice for generating such graphs is the two-parameter exponential family having y d t so that larger values of and yield more connections. notes and details the notion of sufficient statistics ones that contain all available inferential information was perhaps fisher s happiest contribution to the classic corpus. he noticed that in the exponential family form the fact that the parameter interacts with the data x only through the factor exp. y makes y.x sufficient for estimating in a trio of authors working independently in different countries pitman darmois and koopmans showed that exponential families are the only ones that enjoy fixed-dimensional sufficient statistics under repeated independent sampling. until the late such distributions were called pitman darmois koopmans families the long name suggesting infrequent usage. generalized linear models chapter show the continuing impact of sufficiency on statistical practice. peter bickel has pointed out that data compression a lively topic in areas such as image transmission is a modern less stringent version of sufficiency. our only nonexponential family so far was the cauchy translational model. efron and hinkley analyze the cauchy family in terms of curved exponential families a generalization of model properties of classical distributions of properties and lots of distributions are covered in johnson and kotz s invaluable series of reference books two classic multivariate analysis texts are anderson and mardia et al. we have dzdx d t formula from z d t jt p and d f so follows from t t d d and jt j d j t t d formula let d be partitioned as in then direct multiplication showing that d i the identity matrix. if is parametric models symmetric then d by redefining x to be x we can set and equal to zero in the quadratic form in the exponent of is d x x c x but using this matches the quadratic form from c except for an added term that does not involve for a multivariate normal distribution this is sufficient to show that the conditional distribution of given is indeed formulas and suppose that the continuous univariate random variable z has density of the form f d q.z where q.z d c c a b and constants a then by completing the square f d b a and we see that z n the key point is that form specifies z as normal with mean and variance uniquely determined by a and b. the multivariate version of this fact was used in the derivation of formula by redefining and x as m and x m we can take m d in setting b d a.a c density for is of form with d c but bayes rule says that the density of is proportional to also of form now with d c a c a little algebra shows that the quadratic and linear coefficients of match in verifying we verify the multivariate result using a different argument. the vector x has joint distribution n a a a c m notes and details now we employ and a little manipulation to get formula this is the matrix identity now with equaling multivariate gaussian and nuisance parameters. the cautionary message here that increasing the number of unknown nuisance parameters decreases the accuracy of the estimate of interest can be stated more positively if some nuisance parameters are actually known then the mle of the parameter of interest becomes more accurate. suppose for example we wish to estimate from a sample of size n in a bivariate normal model x the mle has variance in notation but if is known then the mle of becomes p with variance being the correlation xi where the xi are iid observations having prfxi d eig d as in the mean and covariance of each xi are formula x d pn efxig d lx el d g dx and covfxig d efxi x g efxigefx i i el e l d formula follows from efxg dp efxig and cov.x dp cov.xi formula the densities of s and sc dp sl are d ly scc e l the conditional density of s given sc is the ratio and d e sc ql ly sl d which is formula and the convexity of a. suppose and are any two points in a i.e. values of having the integral in finite. for any value of c in the interval and any value of y we have c ce e c ce because of the convexity in c of the function on the right by showing that its second derivative is positive. integrating both sides of y parametric models over x with respect to shows that the integral on the right must be finite that is c c c is in a verifying a s convexity. formula in the univariate case differentiating both sides of with respect to gives dividing by e shows that p d e fyg. differentiating again gives ye dxi p dz r c p e dz x dx x or successive derivatives of yield the higher cumulants of y its skewness kurtosis etc. mle for the gradient with respect to of log f is r d e e d var fyg y d y p d y e fy r represents a hypothetical realization y.x where y f we achieve the mle o at ro d or d y eo fy in other words the mle o is the value of that makes the expectation match the observed y. thus implies that the mle of pae fy rameter is y. drawn from part ii early computer-age methods empirical bayes the constraints of slow mechanical computation molded classical statistics into a mathematically ingenious theory of sharply delimited scope. emerging after the second world war electronic computation loosened the computational stranglehold allowing a more expansive and useful statistical methodology. some revolutions start slowly. the journals of the continued to emphasize classical themes pure mathematical development typically centered around the normal distribution. change came gradually but by the a new statistical technology computer enabled was firmly in place. key developments from this period are described in the next several chapters. the ideas for the most part would not startle a pre-war statistician but their computational demands factors of or times those of classical methods would. more factors of a thousand lay ahead as will be told in part iii the story of statistics in the twenty-first century. empirical bayes methodology this chapter s topic has been a particularly slow developer despite an early start in the the roadblock here was not so much the computational demands of the theory as a lack of appropriate data sets. modern scientific equipment now provides ample grist for the empirical bayes mill as will be illustrated later in the chapter and more dramatically in chapters robbins formula table shows one year of claims data for a european automobile insurance company of the policy holders made no claims during the year made a single claim made two claims each etc. with table continuing to the one person who made seven claims. of course the insurance company is concerned about the claims each policy holder will make in the next year. bayes formula seems promising here. we suppose that xk the number empirical bayes table counts yx of number of claims x made in a single year by automobile insurance policy holders. robbins formula estimates the number of claims expected in a succeeding year for instance for a customer in the x d category. parametric maximum likelihood analysis based on a gamma prior gives less noisy estimates. claims x counts yx formula gamma mle of claims to be made in a single year by policy holder k follows a poisson distribution with parameter prfxk d xg d pk d e x for x d is the expected value of xk. a good customer from the company s point of view has a small value of though in any one year his or her actual number of accidents xk will vary randomly according to probability density k suppose we knew the prior density g. for the customers values. then bayes rule would yield efjxg d d p d for the expected value of of a customer observed to make x claims in a single year. this would answer the insurance company s question of what number of claims x to expect the next year from the same customer since efjxg is also efxjxg being the expectation of x. formula is just the ticket if the prior g. is known to the company but what if it is not? a clever rewriting of provides a way forward. using becomes r r efjxg d g. d r xx g. d r c g. d d c xx g. d r o f d yxn with n dp f c o o f d version of robbins formula o oefjxg d c o f d etc. this yields an empirical x yx the total count the marginal density of x integrating p over the prior g. is robbins formula f dz p d dz h i xx e g. d comparing with gives robbins formula efjxg d c c the surprising and gratifying fact is that even with no knowledge of the prior density g. the insurance company can estimate efjxg from formula the obvious estimate of the marginal density f is the proportion of total counts in category x f d c the final expression not requiring n table gives d customers who made zero claims in one year had expectation of a claim the next year those with one claim had expectation and so on. robbins formula came as a to the statistical world of the the expectation efkjxkg for a single customer unavailable without the prior g. somehow becomes available in the context of a large study. the terminology empirical bayes is apt here bayesian formula for a single subject is estimated empirically frequentistically from a collection of similar cases. the crucial point and the surprise is that large data sets of parallel situations carry within them their own bayesian information. large parallel data sets are a hallmark of twenty-first-century scientific investigation promoting the popularity of empirical bayes methods. formula goes awry at the right end of table where it is destabilized by small count numbers. a parametric approach gives more dependable results now we assume that the prior density g. for the customers values has a gamma form g. d for but with parameters and unknown. estimates are obtained by perhaps it shouldn t have estimation methods similar to were familiar in the actuarial literature. empirical bayes maximum likelihood fitting to the counts yx yielding a parametrically estimated marginal density o f d or equivalently oyx d figure auto accident data logcounts vs claims for auto insurance policies. the dashed line is a gamma mle fit. the bottom row of table gives parametric estimates d c oyx which are seen to be less eccentric for large x. figure compares the log scale the raw counts yx with their parametric cousins oyx. the missing-species problem the very first empirical bayes success story related to the butterfly data of table even in the midst of world war ii alexander corbet a leading naturalist had been trapping butterflies for two years in malaysia malaya species were so rare that he had trapped only one specimen each species had been trapped twice each table going on to show that species were trapped three times each and so on. some of the more the missing-species problem common species had appeared hundreds of times each but of course corbet was interested in the rarer specimens. table butterfly data number y of species seen x times each in two years of trapping species trapped just once trapped twice each etc. x y x y corbet then asked a seemingly impossible question if he trapped for one additional year how many new species would he expect to capture? the question relates to the absent entry in table x d the species that haven t been seen yet. do we really have any evidence at all for answering corbet? fortunately he asked the right man r. a. fisher who produced a surprisingly satisfying solution for the missing-species problem. suppose there are s species in all seen or unseen and that xk the number of times species k is trapped in one time follows a poisson distribution with parameter as in xk poi.k for k d s the entries in table are yx d d xg for x d the number of species trapped exactly x times each. now consider a further trapping period of t time units t d in corbet s question and let xk.t be the number of times species k is trapped in the new period. fisher s key assumption is that xk.t poi.kt independently of xk. that is any one species is trapped independently over at a rate proportional to its parameter the probability that species k is not seen in the initial trapping period one time unit equals two years in corbet s situation. this is the definition of a poisson process. but is seen in the new period that is xk d and xk.t is empirical bayes t e e e e.t d sx z e t t so that e.t the expected number of new species seen in the new trapping period is it is convenient to write as an integral e e.t d s e g. d e t gives z expanding e e.t d s where g. is the empirical density putting probability on each of the values. we will think of g. as a continuous prior density on the possible values. t t c t g. d ex d efyxg d sx h notice that the expected value ex of yx is the sum of the probabilities of being seen exactly x times in the initial period x z k i e xx e d s g. d comparing with provides a surprising result e.t d c we don t know the ex values but as in robbins formula we can esti mate them by the yx values yielding an answer to corbet s question oe.t d c corbet specified t d d c d this may have been discouraging there were no new trapping results reported. the missing-species problem table expectation and its standard error for the number of new species captured in t additional fractional units of trapping time. t e.t bsd.t formulas and do not require the butterflies to arrive independently. if we are willing to add the assumption that the xk s are mutually independent we can calculate bsd.t d as an approximate standard error for oe.t table shows oe.t andbsd.t for t d in particular yxt d formula becomes unstable for t this is our price for substituting the nonparametric estimates yx for ex in fisher actually answered corbet using a parametric empirical bayes model in which the prior g. for the poisson parameters was assumed to be of the gamma form it can be shown that then e.t is given by e.t d c t where d c taking d maximum likelihood estimation gave d and d figure shows that the parametric estimate of e.t using and is just slightly greater than the nonparametric estimate over the range t fisher s parametric estimate however gives reasonable results for t d for instance for a future trapping period of units years. reasonable does not necessarily mean dependable. the gamma prior is a mathematical convenience not a fact of nature projections into the far future fall into the category of educated guessing. the missing-species problem encompasses more than butterflies. there are words in total in the recognized shakespearean canon of which are so rare they appear just once each appear twice each etc. empirical bayes figure butterfly data expected number of new species in t units of additional trapping time. nonparametric fit standard deviation gamma model table shakespeare s word counts distinct words appeared once each in the canon distinct words twice each etc. the canon has words in total counting repeats. as in table which goes on to the five words appearing times each. all told distinct words appear those that appear more than times each this being the observed size of shakespeare s vocabulary. but what of the words shakespeare knew but didn t use? these are the missing species in table tet gamma model the missing-species problem suppose another quantity of previously unknown shakespeare manuscripts was discovered comprising t words t d would represent a new canon just as large as the old one. how many previously unseen distinct words would we expect to discover? employing formulas and gives for the expected number of distinct new words if t d this is a very conservative lower bound on how many words shakespeare knew but didn t use. we can imagine t rising toward infinity revealing ever more unseen vocabulary. formula fails for t and fisher s gamma assumption is just that but more elaborate empirical bayes calculations give a firm lower bound of on shakespeare s unseen vocabulary exceeding the visible portion! missing mass is an easier version of the missing-species problem in which we only ask for the proportion of the total sum of values corresponding to the species that went unseen in the original trapping period the numerator has expectation m d x z unseen all d s g. d x dx all x as in while the expectation of the denominator is efxsg d e d efng xs all all all where n is the total number of butterflies trapped. the obvious missingmass estimate is then om d for the shakespeare data om d d we have seen most of shakespeare s vocabulary as weighted by his usage though not by his vocabulary count. all of this seems to live in the rarefied world of mathematical abstraction but in fact some previously unknown shakespearean work might have empirical bayes been discovered in a short poem shall i die? was found in the archives of the bodleian library and controversially attributed to shakespeare by some but not all experts. the poem of words provided a new trapping period of length only t d d and a prediction from of eftg d new species i.e. distinct words not appearing in the canon. in fact there were nine such words in the poem. similar empirical bayes predictions for the number of words appearing once each in the canon twice each etc. showed reasonable agreement with the poem s counts but not enough to stifle doubters. shall i die? is currently grouped with other canonical apocrypha by a majority of experts. a medical example the reader may have noticed that our examples so far have not been particularly computer intensive all of the calculations could have been originally were done by this section discusses a medical study where the empirical bayes analysis is more elaborate. cancer surgery sometimes involves the removal of surrounding lymph nodes as well as the primary target at the site. figure concerns n d surgeries each reporting n d nodes removed and x d nodes found positive positive meaning malignant. the ratios pk d xknk k d n are described in the histogram. a large proportion of them or were zero the remainder spreading unevenly between zero and one. the denominators nk ranged from to with a mean of and standard deviation of we suppose that each patient has some true probability of a node being not so collecting the data. corbet s work was pre-computer but shakespeare s word counts were done electronically. twenty-first-century scientific technology excels at the production of the large parallel-structured data sets conducive to empirical bayes analysis. a medical example figure nodes study ratio p d xn for patients n d number of nodes removed x d number positive. positive say probability for patient k and that his or her nodal results occur independently of each other making xk binomial xk bi.nk this gives pk d xknk with mean and variance pk so that is estimated more accurately when nk is large. a bayesian analysis would begin with the assumption of a prior density g. for the values g. for k d n d we don t know g. but the parallel nature of the nodes data set similar cases suggests an empirical bayes approach. as a first try for the nodes study we assume that logfg. is a fourth-degree polynomial in logfg d c j ji p g is determined by the parameter vector d since given can be calculated from the requirement that empirical bayes z c g d d dz f dz xk g d j j exp d nk xk for a given choice of let f be the marginal probability of the observed value xk for patient k the maximum likelihood estimate of is the maximizer nx o d arg max log f figure estimated prior density g. for the nodes study of patients have have figure graphs go the empirical bayes estimate for the prior distribution of the values. the huge spike at zero in figure is now reduced prfk d compared with the of the pk values sd a medical example z go d d compared with less than small values are still the rule though for instance z go d d the vertical bars in figure indicate one standard error for the estimation of g. the curve seems to have been estimated very accurately at least if we assume the adequacy of model chapter describes the computations involved in figure the posterior distribution of given xk and nk is estimated according to bayes rule to be og.jxk nk d go xk fo nk xk with fo from figure empirical bayes posterior densities of for three patients given x d number of positive nodes n d number of nodes. figure graphs og.jxk nk for three choices of nk and if we take as indicating poor prognosis suggesting more aggressive follow-up therapy then the first patient is almost surely on safe ground the third patient almost surely needs more follow-up therapy and the situation of the second is uncertain. x empirical bayes indirect evidence a good definition of a statistical argument is one in which many small pieces of evidence often contradictory are combined to produce an overall conclusion. in the clinical trial of a new drug for instance we don t expect the drug to cure every patient or the placebo to always fail but eventually perhaps we will obtain convincing evidence of the new drug s efficacy. the clinical trial is collecting direct statistical evidence in which each subject s success or failure bears directly upon the question of interest. direct evidence interpreted by frequentist methods was the dominant mode of statistical application in the twentieth century being strongly connected to the idea of scientific objectivity. bayesian inference provides a theoretical basis for incorporating indirect evidence for example the doctor s prior experience with twin sexes in section the assertion of a prior density g. amounts to a claim for the relevance of past data to the case at hand. empirical bayes removes the bayes scaffolding. in place of a reassuring prior g. the statistician must put his or her faith in the relevance of the other cases in a large data set to the case of direct interest. for the second patient in figure the direct estimate of his value is o d d the empirical bayes estimate is a little less eb dz o og.jxk d nk d d a small difference but we will see bigger ones in succeeding chapters. the changes in twenty-first-century statistics have largely been demand driven responding to the massive data sets enabled by modern scientific equipment. philosophically as opposed to methodologically the biggest change has been the increased acceptance of indirect evidence especially as seen in empirical bayes and objective uninformative bayes applications. false-discovery rates chapter provide a particularly striking shift from direct to indirect evidence in hypothesis testing. indirect evidence in estimation is the subject of our next chapter. notes and details robbins introduced the term empirical bayes as well as rule as part of a general theory of empirical bayes estimation. was also the publication year for good and toulmin s solution to the missingspecies problem. good went out of his way to credit his famous bletchley notes and details colleague alan turing for some of the ideas. the auto accident data is taken from table of carlin and louis who provide a more complete discussion. empirical bayes estimates such as in do not depend on independence among the species but accuracies such as do and similarly for the error bars in figures and corbet s enormous efforts illustrate the difficulties of amassing large data sets in pre-computer times. dependable data is still hard to come by but these days it is often the statistician s job to pry it out of enormous databases. efron and thisted apply formula to the shakespeare word counts and then use linear programming methods to bound shakespeare s unseen vocabulary from below at words. was actually less wordy than his contemporaries marlow and donne. shall i die the possibly shakespearean poem recovered in is analyzed by a variety of empirical bayes techniques in thisted and efron comparisons are made with other elizabethan authors none of whom seem likely candidates for authorship. the shakespeare word counts are from spevack s concordance. first concordance was compiled by hand in the mid listing every word shakespeare wrote and where it appeared a full life s labor. the nodes example figure is taken from gholami et al. formula for any positive numbers c and d we have so combining gamma prior with poisson density gives marginal density z d d d d c r d c x d where d c assuming independence among the counts yx is exactly true if the customers act independently of each other and n the total number of them is itself poisson the log likelihood function for the accident data is yx xmaxx here xmax is some notional upper bound on the maximum possible number empirical bayes of accidents for a single customer since yx d for x the choice of xmax is irrelevant. the values in maximize formula if n dp yx the total number trapped is assumed to be poisson and if the n observed values xk are mutually independent then a useful property of the poisson distribution implies that the counts yx are themselves approximately independent poisson variates for x d poi.ex yx in notation formula and varfyxg d ex then give ext n oe.t o dx var substituting yx for ex produces section of efron shows that is an upper bound on varf oe.t if n is considered fixed rather than poisson. formula combining the case x d in with e g. d r e g. d yields e.t d substituting the gamma prior for g. and using three times gives formula james stein estimation and ridge regression if fisher had lived in the era of apps maximum likelihood estimation might have made him a billionaire. arguably the twentieth century s most influential piece of applied mathematics maximum likelihood continues to be a prime method of choice in the statistician s toolkit. roughly speaking maximum likelihood provides nearly unbiased estimates of nearly minimum variance and does so in an automatic way. that being said maximum likelihood estimation has shown itself to be an inadequate and dangerous tool in many twenty-first-century applications. again speaking roughly unbiasedness can be an unaffordable luxury when there are hundreds or thousands of parameters to estimate at the same time. the james stein estimator made this point dramatically in and made it in the context of just a few unknown parameters not hundreds or thousands. it begins the story of shrinkage estimation in which deliberate biases are introduced to improve overall performance at a possible danger to individual estimates. chapters and will carry on the story in its modern implementations. the james stein estimator suppose we wish to estimate a single parameter from observation x in the bayesian situation and n a n in which case has posterior distribution n c b.x m b as given in we take d for convenience. the bayes estimator of b d a.a c d m c b.x m james stein estimation and ridge regression has expected squared error e compared with for the mle d x d b d if say a d in then b d and has only half the risk of the mle. e the same calculation applies to a situation where we have n indepen dent versions of say d and x d xn with and n a independently for i d n that the differ from each other and that this situation is not the same as let indicate the vector of individual bayes estimates d m d m m d m c b.x m n bayes i and similarly d x using the total squared error risk of is nx d e d n compared with bayes i e e d n b again has only b times the risk of this is fine if we know m and a equivalently m and b in if not we might try to estimate them from x d xn marginally gives then om d nx is an unbiased estimate of m moreover xi n a c s d nx ob d the james stein estimator unbiasedly estimates b as long as n the james stein estimator is the plug-in version of xi om d om c ob for i d n i or equivalently d om c ob.x om with om d om om om at this point the terminology empirical bayes seems especially apt bayesian model leads to the bayes estimator which itself is estimated empirically frequentistically from all the data x and then applied to the individual cases. of course cannot perform as well as the actual bayes rule but the increased risk is surprisingly modest. the expected squared risk of under model is d nb c b e if say n d and a d then equals compared with true bayes risk from much less than risk for a defender of maximum likelihood might respond that none of this is surprising bayesian model specifies the parameters to be clustered more or less closely around a central point m while makes no such assumption and cannot be expected to perform as well. wrong! removing the bayesian assumptions does not rescue as james and stein proved in james stein theorem suppose that independently for i d n with n then n n d e e for all choices of rn expectations in are with fixed and x varying according to in the language of decision theory equation says that is inadmissible its total squared error risk exceeds that of no matter what may be. this is a strong frequentist form of defeat for not depending on bayesian assumptions. the james stein theorem came as a rude shock to the statistical world of first of all the defeat came on mle s home field normal observations with squared error loss. fisher s logic of inductive inference chapter claimed that d x was the obviously correct estimator in the univariate case an assumption tacitly carried forward to multiparameter linear james stein estimation and ridge regression regression problems where versions of were predominant. there are still some good reasons for sticking with in low-dimensional problems as discussed in section but shrinkage estimation as exemplified by the james stein rule has become a necessity in the high-dimensional situations of modern practice. the baseball players the james stein theorem doesn t say by how much beats if the improvement were infinitesimal nobody except theorists would be interested. in favorable situations the gains can in fact be substantial as suggested by one such situation appears in table the batting of major league players have been observed over the season. the column labeled mle reports the player s observed average over his first at bats truth is the average over the remainder of the season further at bats on average. we would like to predict truth from the early-season observations. the column labeled js in table is from a version of the james stein estimator applied to the mle numbers. we suppose that each player s mle value pi batting average in the first tries is a binomial proportion pi pi here pi is his true average how he would perform over an infinite number of tries truthi is itself a binomial proportion taken over an average of more tries per player. at this point there are two ways to proceed. the simplest uses a normal approximation to pi is the binomial variance where n d np c opjs d d p.pi i with np d the average of the pi values. letting xi d pi applying and transforming back to opjs i gives james stein estimates i batting average d hits at bats that is the success rate. for example player hits successfully times in his first tries for batting average d this data is based on major league performances but is partly artificial see the endnotes. np the baseball players table eighteen baseball players mle is batting average in first at bats truth is average in remainder of season james stein estimator js is based on arcsin transformation of mles. sum of squared errors for predicting truth mle js player mle js truth x npi c a second approach begins with the arcsin transformation xi d c sin n d labeled x in table a classical device that produces approximate normal deviates of variance n c xi n where is transformation applied to truthi. using gives i which is finally inverted back to the binomial scale opjs i d n n c n c i formulas and yielded nearly the same estimates for the baseball players the js column in table is from james and stein s theorem requires normality but the james stein estimator often james stein estimation and ridge regression d while works perfectly well in less ideal situations. that is the case in table d in other words the james stein estimator reduced total predictive squared error by about figure eighteen baseball players top line mle middle james stein bottom true values. only points are visible since there are ties. the james stein rule describes a shrinkage estimator each mle value xi being shrunk by factor ob toward the grand mean om d nx ob d in figure illustrates the shrinking process for the baseball players. to see why shrinking might make sense let us return to the original bayes model and take m d for simplicity so that the xi are marginally n a c even though each xi is unbiased for its nx d na parameter as a group they are overdispersed d n.a c compared with e e i nx i the sum of squares of the mles exceeds that of the true values by expected amount n shrinkage improves group estimation by removing the excess. llllllllllllllllllmlelllllllllllllllllljames steinlllllllllllllllllltruebatting averages ridge regression bayes i bayes i nx e a a c d bxi have d nb c d na in fact the james stein rule overshrinks the data as seen in the bottom two lines of figure a property it inherits from the underlying bayes model the bayes estimates i d na by factor a.a c we could use the bxi which gives the correct expected sum of squares na but a larger expected sum of squared estimation errors overshrinking e.p less extreme shrinking rule d p efp. pothesis of no differences among the values. gavep.pi d null indicating that in a classical sense we have accepted the null hy the most extreme shrinkage rule would be all the way that is to for i d n i d nx for the baseball data the james stein estimator is a databased rule for compromising between the null hypothesis of no differences and the mle s tacit assumption of no relationship at all among the values. in this sense it blurs the classical distinction between hypothesis testing and estimation. ridge regression linear regression perhaps the most widely used estimation technique is based on a version of in the usual notation we observe an n-dimensional vector y d yn from the linear model y d x c here x is a known p structure matrix is an unknown p-dimensional parameter vector while the noise vector d has its components uncorrelated and with constant variance where i is the n n identity matrix. often is assumed to be multivariate normal but that is not required for most of what follows. james stein estimation and ridge regression the least squares estimate o early is the minimizer of the total sum of squared errors going back to gauss and legendre in the ky x o d arg min it is given by o d s where s is the p p inner product matrix xi s d x y o is unbiased for and has covariance matrix o is the mle of before a great deal could be feasibly in the normal case o of effort went into designing matrices x such that s calculated which is now no longer a concern. a great advantage of the linear model is that it reduces the number of unknown parameters to p p c including no matter how large n may be. in the kidney data example of section n d while p d in modern applications however p has grown larger and larger sometimes into the thousands or more as we will see in part iii causing statisticians again to confront the limitations of high-dimensional unbiased estimation. ridge regression is a shrinkage method designed to improve the estimation of in linear models. by transformations we can standardize so that the columns of x each have mean and sum of squares that is si i d for i d p puts the regression coefficients p on comparable scales. for convenience we also assume ny d a ridge regression estimate o is defined for to be y d c while o o d c o is a shrunken version of o the bigger the more d o extreme the shrinkage o equals the vector of zeros. ridge regression effects can be quite dramatic. as an example consider the diabetes data partially shown in table in which prediction variables measured at baseline age sex bmi mass index map arterial blood pressure and six blood serum measurements have o ridge regression table first of n d patients in the diabetes study we wish to predict disease progression at one year prog from the baseline measurements age sex glu. age sex bmi map tc ldl hdl tch ltg glu prog been obtained for n d patients. we wish to use the variables to predict prog a quantitative assessment of disease progression one year after baseline. in this case x is the matrix of standardized predictor variables and y is prog with its mean subtracted off. figure ridge coefficient trace for the standardized diabetes data. james stein estimation and ridge regression table ordinary least squares estimate o compared with ridge regression estimate o with d the columns and are their estimated standard errors. was taken to be the usual ols estimate based on model o o age sex bmi map tc ldl hdl tch ltg glu figure vertically plots the coordinates of o as the ridge parameter increases from to four of the coefficients change rapidly at first. table compares o positive coefficients predict increased disease progression. notice that ldl the bad cholesterol measurement goes from being a strongly positive predictor in o there is a bayesian rationale for ridge regression. assume that the noise that is the usual estimate o with o vector is normal as in so that rather than just then the bayesian prior to a mildly negative one in o o np i np n j o o d c o makes e with m d ridge regression amounts to an the same as the ridge regression estimate o a d and d increased prior belief that lies near the last two columns of table compare the standard deviations of and o o ridging has greatly reduced the variability of the estimated ridge regression d x o regression coefficients. this does not guarantee that the corresponding estimate of d x o will be more accurate than the ordinary least squares estimate d x we have introduced bias and the squared bias term counteracts some of the advantage of reduced variability. the cp calculations of chapter suggest that the two effects nearly offset each other for the diabetes data. however if interest centers on the coefficients of then ridging can be crucial as table emphasizes. by current standards p d is a small number of predictors. data sets with p in the thousands and more will show up in part iii. in such situations the scientist is often looking for a few interesting predictor variables hidden in a sea of uninteresting ones the prior belief is that most of the i values lie near zero. biasing the maximum likelihood estimates o i toward zero then becomes a necessity. o there is still another way to motivate the ridge regression estimator o d arg min fky x c differentiating the term in brackets with respect to shows that o d c y as in if d then describes the ordinary least squares algorithm penalizes choices of having k k large biasing o various terminologies are used to describe algorithms such as penalized least squares penalized likelihood maximized a-posteriori probability and generically regularization describes almost any method that tamps down statistical variability in high-dimensional estimation or prediction problems. toward the origin. a wide variety of penalty terms are in current use the most influential one involving the norm k dpp j jj q d arg min fky x c the so-called lasso estimator chapter despite the bayesian provenance most regularization research is carried out frequentistically with various penalty terms investigated for their probabilistic behavior regarding estimation prediction and variable selection. if we apply the james stein rule to the normal model we get a different shrinkage rule for o say q js james stein estimation and ridge regression o q js d s o o q js be the corresponding estimator of d efyg in letting d x the james stein theorem guarantees that e no matter what is as long as p there is no such guarantee for ridge regression and no foolproof way to choose the ridge parameter on the other hand q js does not stabilize the coordinate standard deviations as in the column of table the main point here is that at present there is no optimality theory for shrinkage estimation. fisher provided an elegant theory for optimal unbiased estimation. it remains to be seen whether biased estimation can be neatly codified. d indirect evidence there is a downside to shrinkage estimation which we can examine by returning to the baseball data of table one thousand simulations were run each one generating simulated batting averages p i truthi i d these gave corresponding james stein estimates with np np table shows the root mean square error for the mle and js estimates over simulations for each of the players ij truthi and op js ij truthi loses to op as foretold by the james stein theorem the js estimates are easy victors in terms of total squared error over all players. however op js i for of the players losing badly in the case of player js for player appear in figure strikingly all of the op js values lie histograms comparing the simulations of p i with those of op d p mle i i i of course we are assumimg is known in if it is estimated some of the improvement erodes away. indirect evidence table simulation study comparing root mean square errors for mle and js estimators as estimates of truth. total mean square errors and asterisks indicate four players for whom rmsjs exceeded rmsmle these have two largest and two smallest truth values is clemente. column is for the limited translation version of js that bounds shrinkage to within one standard deviation of the mle. player truth rmsmle rmsjs js i below d player could have had a legitimate complaint if the james stein estimate were used to set his next year s salary. the four losing cases for op are the players with the two largest and two smallest values of the truth. shrinkage estimators work against cases that are genuinely outstanding a positive or negative sense. player was roberto clemente. a better informed bayesian that is a baseball fan would know that clemente had led the league in batting over the previous several years and shouldn t be thrown into a shrinkage pool with ordinary hitters. of course the james stein estimates were more accurate for of the players. shrinkage estimation tends to produce better results in general at the possible expense of extreme cases. nobody cares much about cold war batting averages but if the context were the efficacies of new anticancer drugs the stakes would be higher. james stein estimation and ridge regression figure comparing mle estimates with js estimates for clemente simulations at bats each. compromise methods are available. the column of table refers to a limited translation version of opjs in which shrinkage is not allowed to diverge more than one unit from opi in formulaic terms i d min opjs opjs i opi c i opi this mitigates the clemente problem while still gaining most of the shrinkage advantages. the use of indirect evidence amounts to learning from the experience of others each batter learning from the others in the baseball examples. which others? is a key question in applying computer-age methods. chapter returns to the question in the context of false-discovery rates. notes and details the bayesian motivation emphasized in chapters and is anachronistic originally the work emerged mainly from frequentist considerations and was justified frequentistically as in robbins stein proved the inadmissibility of the neat version of appearing in james and stein james was stein s graduate student is itself inadmissable being everywhere improvable by changing ob in mlep james stein to max. ob this in turn is inadmissable but further gains tend to the minuscule. notes and details in a series of papers in the early efron and morris emphasized the empirical bayes motivation of the james stein rule efron and morris giving the limited translation version the baseball data in its original form appears in table of efron here the original at bats recorded for each player have been artificially augmented by adding binomial draws truthi for player i. this gives a somewhat less optimistic view of the james stein rule s performance. stein s paradox in statistics efron and morris title for their scientific american article catches the statistics world s sense of discomfort with the james stein theorem. why should our estimate for player a go up or down depending on the other players performances? this is the question of direct versus indirect evidence raised again in the context of hypothesis testing in chapter unbiased estimation has great scientific appeal so the argument is by no means settled. ridge regression was introduced into the statistics literature by hoerl and kennard it appeared previously in the numerical analysis literature as tikhonov regularization. of freedom z formula if z has a chi-squared distribution with degrees is z in table it has density f d for z dz d n d s a c d yielding dz e z e verifying but standard results starting from show that s c with d n in formula first consider the simpler situation where m in is known to equal zero in which case the james stein estimator is with ob d where s dpn oc d ob d and c d b d c d obxi i for convenient notation let i james stein estimation and ridge regression the conditional distribution e e i n b gives i x o d b c oc c x o d nb c oc c o d b n oc c d nb c b e e and so and adding over the n coordinates the marginal distribution s c calculation n and yields after a little by orthogonal transformations in situation where m is not assumed to be zero can be represented as the sum of two parts a js estimate in n dimensions but with m d as in and a mle estimate of the remaining one coordinate. using this gives d c b c d nb c b e which is the james stein theorem. stein derived a simpler proof of the js theorem that appears in section of efron transformations to form the linear regression model is equivariant under scale changes of the variables xj what this means is that the space of fits using linear combinations of the xj is the same as the space of linear combinations using scaled versions qxj d xj with sj furthermore the least squares fits are the same and the coefficient estimates map in the obvious way oq j d sj ridge regression we see that the penalty term k dp not so for ridge regression. changing the scales of the columns of x will generally lead to different fits. using the penalty version of j treats all the coefficients as equals. this penalty is most natural if all the variables are measured on the same scale. hence we typically use for sj the standard deviation of variable xj which leads to furthermore with ridge regression we typically do not penalize the intercept. this can be achieved o j j by centering and scaling each of the variables qxj d where notes and details nxj d nx xij and sj dhx nxj with the n-vector of we now work with qx d qxp rather than x and the intercept is estimated separately as ny. we calculate the covariance matrix of o d c standard deviations in table from the first equality in to be c the entries in table are square roots of the diagonal elements of substituting the ordinary least squares estimate d for penalized likelihood and map. with fixed and known in the nn.x minimizing ky x is the normal linear model y same as maximizing the log density function log f d ky x c constant in this sense the term in penalizes the likelihood log f connected with in proportion to the magnitude k under the prior distribution the log posterior density of given y log of is ky x c plus a term that doesn t depend on that makes the maximizer of also the maximizer of the posterior density of given y or the map. formula let d and d where s is a matrix square root of s d s then o in and the m d form of the james stein rule is i p transforming back to the scale gives js d generalized linear models and regression trees indirect evidence is not the sole property of bayesians. regression models are the frequentist method of choice for incorporating the experience of others. as an example figure returns to the kidney fitness data of section a potential new donor aged has appeared and we wish to assess his kidney fitness without subjecting him to an arduous series of medical tests. only one of the previously tested volunteers was age his tot score being upper large dot in figure most dtot d the former is the only direct evidence we have while the applied statisticians though would prefer to read off the height of the least squares regression line at age d green dot on the regression line figure kidney data a new volunteer donor is aged which prediction is preferred for his kidney function? logistic regression regression line lets us incorporate indirect evidence for age from all previous cases. increasingly aggressive use of regression techniques is a hallmark of modern statistical practice aggressive applying to the number and type of predictor variables the coinage of new methodology and the sheer size of the target data sets. generalized linear models this chapter s main topic have been the most pervasively influential of the new methods. the chapter ends with a brief review of regression trees a completely different regression methodology that will play an important role in the prediction algorithms of chapter logistic regression an experimental new anti-cancer drug called xilathon is under development. before human testing can begin animal studies are needed to determine safe dosages. to this end a bioassay or dose response experiment was carried out groups of n d mice each were injected with increasing amounts of xilathon dosages let yi d mice dying in ith group the points in figure show the proportion of deaths pi d yi lethality generally increasing with dose. the counts yi are modeled as independent binomials bi.ni for i d n yi n d and all ni equaling here is the true death rate in group i estimated unbiasedly by pi the direct evidence for the regression curve in figure uses all the doses to give a better picture of the true dose response relation. logistic regression is a specialized technique for regression analysis of count or proportion data. the logit parameter is defined as o n d log dose would usually be labeled on a log scale each one say larger than its predecessor. glms and regression trees figure dose response study groups of mice exposed to increasing doses of experimental drug. the points are the observed proportions that died in each group. the fitted curve is the maximum-likelihoood estimate of the linear logistic regression model. the open circle on the curve is the the estimated dose for mortality. with increasing from to as increases from to a linear logistic regression dose response analysis begins with binomial model and assumes that the logit is a linear function of dose d c d log maximum likelihood gives estimates o and fitted curve since the inverse transformation of is we obtain from the linear logistic regression curve o d o c o c e c e pictured in figure table compares the standard deviation of the estimated regression llllllllllldoseproportion of logistic regression table standard deviation estimates for in figure the first row is for the linear logistic regression fit the second row is based on the individual binomial estimates pi. x sd sd pi curve at x d discussed in the next section with the usual binomial standard deviation estimate pi pi obtained by considering the doses regression has reduced error by better than the price being possible bias if model goes seriously wrong. one advantage of the logit transformation is that isn t restricted to the range so model never verges on forbidden territory. a better reason has to do with the exploitation of exponential family properties. we can rewrite the density function for bi.n y as n y d n y with the logit parameter and d c is a one-parameter exponential as described in section with the natural parameter called there. let y d yn denote the full data set n d in figure using and the independence of the yi gives the probability density of y as a function of ni ny f d ny d e e ni yi ni yi for the separate-dose standard error pi was taken equal to the fitted value from the curve in figure it is not necessary for in on page to be a probability density function only that it not depend on the parameter where glms and regression trees d nx yi and d nx xi yi formula expresses f as the product of three factors f d g only the first of which involves both the parameters and the data. this implies that is a sufficient statistic no matter how large n might be we will have n in the thousands just the two numbers contain all of the experiment s information. only the logistic parameterization makes this a more intuitive picture of logistic regression depends on d.pi the deviance between an observed proportion pi and an estimate c pi log pi d d pi log the is zero if d pi otherwise it increases as departs further from pi. the logistic regression mle value o also turns out to be the choice of minimizing the total deviance between the n points pi and their corresponding estimates d o d arg min nx d the solid line in figure is the linear logistic curve coming closest to the points when distance is measured by total deviance. in this way the notion of least squares is generalized to binomial regression as discussed in the next section. a more sophisticated notion of distance between data and models is one of the accomplishments of modern statistics. table reports on the data for a more structured logistic regression analysis. human muscle cell colonies were infused with mouse nuclei in five different ratios cultured over time periods ranging from one to five where the name logistic regression comes from is explained in the endnotes along with a description of its nonexponential family predecessor probit analysis. deviance is analogous to squared error in ordinary regression theory as discussed in what follows. it is twice the kullback leibler distance the preferred name in the information-theory literature. logistic regression table cell infusion data human cell colonies infused with mouse nuclei in five ratios over to days and observed to see whether they did or did not thrive. green numbers are estimates from the logistic regression model. for example of colonies in the lowest ratiodays category thrived with observed proportion d and logistic regression estimate d ratio time days and observed to see whether they thrived. for example of the colonies having the third ratio and shortest time period thrived. let denote the true probability of thriving for ratio i during time period j and its logit a two-way additive logistic regression was fit to the d c i c j i d j d the green numbers in table show the maximum likelihood estimates ic o j d c e straintsp i d p j d necessary to avoid definitional difficulties model has nine free parameters into account the con compared with just two in the dose response experiment. the count can easily go much higher these days. table reports on a logistic regression applied to the spam data. a researcher george labeled n d of his email mes using the statistical computing language r see the endnotes. glms and regression trees table logistic regression analysis of the spam data model estimated regression coefficients standard errors and z d estimatese for keyword predictors. the notation char means the relative number of times appears etc. the last three entries measure characteristics such as length of capital-letter strings. the word george is special since the recipient of the email is named george and the goal here is to build a customized spam filter. intercept make address all our over remove internet order mail receive will people report addresses free business email you credit your font money hp hpl george estimate se z-value lab labs telnet data technology parts pm direct cs meeting original project re edu table conference char char char char! char char cap.ave cap.long cap.tot estimate se sages as either spam or ham say yi d if email i is spam if email i is ham z-value ham refers to nonspam or good email this is a playful connection to the processed logistic regression of the messages were spam. the p d predictor variables represent the most frequently used words and tokens in george s corpus of email trivial words such as articles and are in fact the relative frequencies of these chosen words in each email by the length of the email. the goal of the study was to predict whether future emails are spam or ham using these keywords that is to build a customized spam filter. let xij denote the relative frequency of keyword j in email i and represent the probability that email i is spam. letting be the logit transform we fit the additive logistic model table shows o i for each word for example for make as well as the estimated standard error and the z-value estimatese. j xij d c it looks like certain words such as free and your are good spam predictors. however the table as a whole has an unstable appearance with occasional very large estimates o i accompanied by very large standard the dangers of high-dimensional maximum likelihood estimation are apparent here. some sort of shrinkage estimation is called for as discussed in chapter regression analysis either in its classical form or in modern formulations requires covariate information x to put the various cases into some sort of geometrical relationship. given such information regression is the statistician s most powerful tool for bringing other results to bear on a case of primary interest for instance the volunteer in figure empirical bayes methods do not require covariate information but may be improvable if it exists. if for example the player s age were an important covariate in the baseball example of table we might first regress the mle values on age and then shrink them toward the regression line rather than toward the grand mean np as in in this way two different sorts of indirect evidence would be brought to bear on the estimation of each player s ability. spam that was fake ham during wwii and has been adopted by the machine-learning community. the x matrix was standardized so disparate scalings are not the cause of these discrepancies. some of the features have mostly zero observations which may account for their unstable estimation. glms and regression trees generalized linear logistic regression is a special case of generalized linear models a key methodology having both algorithmic and inferential influence. glms extend ordinary linear regression that is least squares curvefitting to situations where the response variables are binomial poisson gamma beta or in fact any exponential family form. n we begin with a one-parameter exponential family d o as in with and x replaced by and y and replaced by for clearer notation in what follows. here is the natural parameter and y the sufficient statistic both being one-dimensional in usual applications takes its values in an interval of the real line. each coordinate yi of an observed data set y d yi yn is assumed to come from a member of family yi independently for i d n table lists and y for the first four families in table as well as their deviance and normalizing functions. by itself model requires n parameters usually too many for effective individual estimation. a key glm tactic is to specify the in terms of a linear regression equation. let x be an structure matrix with ith row say x i and an unknown vector of p parameters the n d is then specified by d x in the dose response experiment of figure and model x is n with ith row xi and parameter vector d ny the probability density function f of the data vector y is pn f d ny d e which can be written as f d e some of the more technical points raised in this section are referred to in later chapters and can be scanned or omitted at first reading. generalized linear models table exponential family form for first four cases in table natural parameter sufficient statistic y deviance between family members and and normalizing function y x log x log x h i log c log i log i n c log x normal n known poisson binomial bi.n gamma known where y and d nx z d x i a p-parameter exponential family with natural parameter vector and sufficient statistic vector z. the main point is that all the information from a p-parameter glm is summarized in the p-dimensional vector z no matter how large n may be making it easier both to understand and to analyze. we have now reduced the n model to the pparameter exponential family with p usually much smaller than n in this way avoiding the difficulties of high-dimensional estimation. the moments of the one-parameter constituents determine the estimation properties in model let denote the expectation and variance of univariate density y d for the poisson. the n y obtained for instance from glm then has mean vector and covariance matrix y glms and regression trees where is the vector with ith component with d x i and is the n n diagonal matrix having diagonal elements the maximum likelihood estimate o of the parameter vector can be shown to satisfy the simple equation y d x for the normal case where yi linear regression d x o and becomes x the familiar solution n in that is for ordinary x o d with o d yi x otherwise is a nonlinear function of and must be solved by numerical iteration. this is made easier by the fact that for glms log f the likelihood function we wish to maximize is a concave function of the mle o has approximate expectation and covariance similar to the exact ols result o generalizing the binomial definition the deviance between den sities and is defined to be d dy o z x log y the integral sum for discrete distributions being over their common sample space y. is always nonnegative equaling zero only if and are the same in general does not equal deviance does not depend on how the two densities are named for example having the same expression as the binomial entry in table in what follows it will sometimes be useful to label the family by its expectation parameter d rather than by the natural parameter d meaning the same thing as only the names attached to the individual family members being changed. in this notation it is easy to show a fundamental result sometimes known as hoeffding s lemma the maximum likelihood estimate of given y is y itself and the log likelihood log decreases from its maximum log fy.y by an amount that depends on the deviance d.y d fy.ye generalized linear models returning to the glm framework parameter vector gives d x which in turn gives the vector of expectation parameters d for instance d for the poisson family. multiplying hoeffding s lemma over the n cases y d yn yields d ny f d ny minimizes the total deviancepn this has an important consequence the mle o is the choice of that d.yi as in figure glm maximum likelihood fitting is least total deviance in the same way that ordinary linear regression is least sum of squares. d.yi fyi e the inner circle of figure represents normal theory the preferred venue of classical applied statistics. exact inferences t-tests f distributions most of multivariate analysis were feasible within the circle. outside the circle was a general theory based mainly on asymptotic approximations involving taylor expansions and the central limit theorem. figure three levels of statistical modeling. a few useful exact results lay outside the normal theory circle relating normal theoryexact calculationsexponential familiespartly exactgeneral theoryasymptoticsfigure three levels of statistical modeling glms and regression trees to a few special families the binomial poisson gamma beta and others less well known. exponential family theory the second circle in figure unified the special cases into a coherent whole. it has a partly exact flavor with some ideal counterparts to normal theory convex likelihood surfaces least deviance regression but with some approximations necessary as in even the approximations though are often more convincing than those of general theory exponential families fixed-dimension sufficient statistics making the asymptotics more transparent. logistic regression has banished its predecessors as probit analysis almost entirely from the field and not only because of estimating efficiencies and computational advantages are actually rather modest but also because it is seen as a clearer analogue to ordinary least squares our dependable standby. glm research development has been mostly frequentist but with a substantial admixture of likelihoodbased reasoning and a hint of fisher s logic of inductive inference. helping the statistician choose between competing methodologies is the job of statistical inference. in the case of generalized linear models the choice has been made at least partly in terms of aesthetics as well as philosophy. poisson regression the third most-used member of the glm family after normal theory least squares and logistic regression is poisson regression. n independent poisson variates are observed i d n where d log is assumed to follow a linear model yi d x where x is a known n p structure matrix and an unknown p-vector i for i d n where x of regression coefficients. that is d x is the ith row of x. in the chapters that follow we will see poisson regression come to the rescue in what at first appear to be awkward data-analytic situations. here we will settle for an example involving density estimation from a spatially truncated sample. i table shows galaxy counts from a small portion of the sky galaxies have had their redshifts r and apparent magnitudes m measured. poisson regression table counts for a truncated sample of galaxies binned by redshift and magnitude. redshift magnitude distance from earth is an increasing function of r while apparent brightness is a decreasing of m. in this survey counts were limited to galaxies having r and m the upper limit reflecting the difficulty of measuring very dim galaxies. the range of log r has been divided into equal intervals and likewise equal intervals for m. table gives the counts of the galaxies in the d bins. lower right corner of the table is empty because distant galaxies always appear dim. the multinomialpoisson connection helps motivate model picturing the table as a multinomial observation on categories in which the sample size n was itself poisson. we can imagine table as a small portion of a much more extensive table hypothetically available if the data were not truncated. experience suggests that we might then fit an appropriate bivariate normal density to the data as in figure it seems like it might be awkward to fit part of a bivariate normal density to truncated data but poisson regression offers an easy solution. an object of the second magnitude is less bright than one of the first and so on a classification system owing to the greeks. glms and regression trees let r be the listing the values of r in each bin of the table column order and likewise m for the m values for instance m d repeated times and define the matrix x as x d r m r rm where r is the vector whose components are the square of r s etc. the log density of a bivariate normal distribution in m is of the form c c c c agreeing with log d x i as specified by we can use a poisson glm with yi the ith bin s count to estimate the portion of our hypothesized bivariate normal distribution in the truncation region figure left galaxy data binned counts. right poisson glm density estimate. the left panel of figure is a perspective picture of the raw counts in table on the right is the fitted density from the poisson regression. irrespective of density estimation poisson regression has done a useful job of smoothing the raw bin counts. contours of equal value of the fitted log density o c o c o c o c o c o are shown in figure one can imagine the contours as truncated portions of ellipsoids of the type shown in figure the right panel of figure makes it clear that we are nowhere near the center of the hypothetical bivariate normal density which must lie well beyond our dimness limit. dimmerfarthercountdimmerfartherdensity poisson regression figure contour curves for poisson glm density estimate for the galaxy data. the red dot shows the point of maximum density. the poisson deviance residual z between an observed count y and a fitted value is z d sign.y tist glm theory says that s dp with d the poisson deviance from table zj k the deviance residual between the count yij in the ij th bin of table and the fitted value k from the poisson glm was calculated for all bins. standard frequenj k should be about if the bivariate normal model is actually the fit was poor s d in practice we might try adding columns to x in e.g. or r improving the fit where it was worst near the boundaries of the table. chapter demonstrates some other examples of poisson density estimation. in general poisson glms reduce density estimation to regression model fitting a familiar and flexible inferential technology. j k this is a modern version of the classic chi-squared goodness-of-fit test. l glms and regression trees regression trees the data set d for a regression problem typically consists of n pairs yi d d f.xi yi i d ng where xi is a vector of predictors or covariates taking its value in some space x and yi is the response assumed to be univariate in what follows. the regression algorithm perhaps a poisson glm inputs d and outputs a rule rd for any value of x in x rd produces an estimate oy for a possible future value of y oy d rd in the logistic regression example rd is there are three principal uses for the rule rd for prediction given a new observation of x but not of its corresponding y we use oy d rd to predict y. in the spam example the keywords of an incoming message could be used to predict whether or not it is chapter for estimation the rule rd describes a regression surface os over x the right panel of figure shows os for the galaxy example. os can be thought of as estimating s the true regression surface often defined in the form of conditional expectation os d frd x g x s d fefyjxg x g x x g. a dichotomous situation where y is coded as or s d fprfy d x for estimation but not necessarily for prediction we want os to accurately portray s. the right panel of figure shows the estimated galaxy density still increasing monotonically in dimmer at the top end of the truncation region but not so in farther perhaps an important clue for directing future search the flat region in the kidney function regression curve of figure makes almost no difference to prediction but is of scientific interest if accurate. prediction of dichotomous outcomes is often called classification. physicists call a regression-based search for new objects bump hunting. regression trees for explanation the predictors for the diabetes data of section age sex bmi. were selected by the researcher in the hope of explaining the etiology of diabetes progression. the relative contribution of the different predictors to rd is then of interest. how the regression surface is composed is of prime concern in this use but not in use or above. the three different uses of rd raise different inferential questions. use calls for estimates of prediction error. in a dichotomous situation such as the spam study we would want to know both error probabilities prf oy d spamjy d hamg and prf oy d hamjy d spamg for estimation the accuracy of rd as a function of x perhaps in standard deviation terms sd.x d sd. oyjx would tell how closely os approximates s. use explanation requires more elaborate inferential tools saying for example which of the regression coefficients i in can safely be set to zero. figure left a hypothetical regression tree based on two predictors and right corresponding regression surface. regression trees use a simple but intuitively appealing technique to form a regression surface recursive partitioning. the left panel of figure illustrates the method for a hypothetical situation involving two predictor variables and r and m in the galaxy example. at the top of glms and regression trees the tree the sample population of n cases has been split into two groups those with equal to or less than value go to the left those with to the right. the leftward group is itself then divided into two groups depending on whether or not the division stops there leaving two terminal nodes and on the tree s right side two other splits give terminal nodes and a prediction value oyrj is attached to each terminal node rj the prediction oy applying to a new observation x d is calculated by starting x at the top of the tree and following the splits downward until a terminal node and its attached prediction oyrj is reached. the corresponding regression surface os is shown in the right panel of figure the oyrj happen to be in ascending order. various algorithmic rules are used to decide which variable to split and which splitting value t to take at each step of the tree s construction. here is the most common method suppose at step k of the algorithm groupk of nk cases remains to be split those cases having mean and sum of squares yi and k mk d x d x dividing groupk into groupkleft and groupkright produces means mkleft and mkright and corresponding sums of squares kright. the algorithm proceeds by choosing the splitting variable xk and the threshold tk to minimize kleft and c kright kleft in other words it splits groupk into two groups that are as different from each other as possible. cross-validation estimates of prediction error chapter are used to decide when the splitting process should stop. if groupk is not to be further divided it becomes terminal node rk with prediction value oyrk d mk. none of this would be feasible without electronic computation but even quite large prediction problems can be short work for modern computers. figure shows a regression tree of the spam data table there are seven terminal nodes labeled or for decision ham or spam. the leftmost node say is a and contains ham cases and spam with and in the full data set. starting at the top of the tree is reached if it has a low proportion of symbols using the r program rpart in classification mode employing a different splitting rule than the version based on regression trees figure regression tree on the spam data d ham d spam. error rates ham spam captions indicate leftward moves. char a low proportion of the word remove and a low proportion of exclamation marks char!. regression trees are easy to interpret too many dollar signs means spam! seemingly suiting them for use explanation. unfortunately they are also easy to overinterpret with a reputation for being unstable in practice. discontinuous regression surfaces os as in figure disqualify them for use estimation. their principal use in what follows will be as key parts of prediction algorithms use the tree in figure has apparent error rates of and this can be much improved upon by bagging aggregation chapters and and by other computer-intensive techniques. compared with generalized linear models regression trees represent a break from classical methodology that is more stark. first of all they are totally nonparametric bigger but less structured data sets have promoted nonparametrics in twenty-first-century statistics. regression trees are more computer-intensive and less efficient than glms but as will be seen in part iii the availability of massive data sets and modern computational equip regression tree spam data rates nonspam spam indicate leftward moves glms and regression trees ment has diminished the appeal of efficiency in favor of easy assumptionfree application. notes and details computer-age algorithms depend for their utility on statistical computing languages. after a period of evolution the language s et al. and its open-source successor r core team have come to dominate applied generalized linear models are available from a single r command e.g. for logistic regression and hastie and similarly for regression trees and hundreds of other applications. the classic version of bioassay probit analysis assumes that each test animal has its own lethal dose level x and that the population distribution of x is normal prfx xg d c for unknown parameters and standard normal cdf then the number of animals dying at dose x is binomial bi.nx as in with d c or d c replacing the standard normal cdf with the logistic cdf c e resembles changes into logistic regression the usual goal of bioassay was to estimate the dose lethal to of the test population it is indicated by the open circle in figure cox the classic text on logistic regression lists berkson as an early practitioner. wedderburn is credited with generalized linear models in mccullagh and nelder s influential text of that name first edition birch developed an important and suggestive special case of glm theory. the twenty-first century has seen an efflorescence of computer-based regression techniques as described extensively in hastie et al. the discussion of regression trees here is taken from their section including our figure they use the spam data as a central example it is publicly previous computer packages such as sas and spss continue to play a major role in application areas such as the social sciences biomedical statistics and the pharmaceutical industry. notes and details available at ftp.ics.uci.edu. breiman et al. propelled regression trees into wide use with their cart algorithm. sufficiency as in the fisher neyman criterion says that if f d h when does not depend on then s.x is sufficient for equation from we have the log likelihood function with sufficient statistic z d x ing with respect to l d z y and dpn i differentiat y x p l d z p d x where we have used d so says p matrix r l with respect to is l d x verifying the mle equation i d x i but concavity of the log likelihood. from the second derivative but z d x r d cov y has cov d x a positive definite p p matrix verifying the concavity of l in fact applies to any exponential family not only glms. formula the sufficient statistic z has mean vector and co variance matrix z v using the with d e fzg and v d x first-order taylor series for o as a function of z is c v o rather taken literally gives in the ols formula we have than since the natural parameter for the normal entry in table is formula this formula attributed to hoeffding is a key result in the interpretation of glm fitting. applying definition glms and regression trees to family gives d g d if is the mle o d d log d y d y then d y the maximum likelihood equation y i d for any choice of but the right-hand side of is log verifying table the galaxy counts are from loh and spillar s redshift survey as discussed in efron and petrosian criteria abbreviating left and right by l and r we have d kl c kr k c nkl nkr nk mkr with nkl and nkr the subgroup sizes showing that minimizing is the same as maximizing the last term in intuitively a good split is one that makes the left and right groups as different as possible the ideal being all on the left and all on the right making the terminal nodes pure. in some cases is undefined for example when y d for a poisson response d log.y which is undefined. but in we assume that d similarly for binary y and the binomial family. survival analysis and the em algorithm survival analysis had its roots in governmental and actuarial statistics spanning centuries of use in assessing life expectancies insurance rates and annuities. in the years between and survival analysis was adapted by statisticians for application to biomedical studies. three of the most popular post-war statistical methodologies emerged during this period the kaplan meier estimate the log-rank and cox s proportional hazards model the succession showing increased computational demands along with increasingly sophisticated inferential justification. a connection with one of fisher s ideas on maximum likelihood estimation leads in the last section of this chapter to another statistical method that has gone platinum the em algorithm. life tables and hazard rates an insurance company s life table appears in table showing its number of clients is life insurance policy holders by age and the number of deaths during the past year in each age for example five deaths among the clients aged the column labeled os is of great interest to the company s actuaries who have to set rates for new policy holders. it is an estimate of survival probability probability of a person aged beginning of the table surviving past age etc. os is calculated according to an ancient but ingenious algorithm. let x represent a typical lifetime so fi d prfx d ig also known as the mantel haenszel or cochran mantel haenszel test. the insurance company is fictitious but the deaths y are based on the true rates for us men per social security administration data. survival analysis and the em algorithm h d hazard rate yn os d age n age oh oh table insurance company life table at each age n d number of policy holders y d number of deaths o survival probability estimate os os n y y is the probability of dying at age i and si dx fj d prfx ig is the probability of surviving past age i the hazard rate at age i is by life tables and hazard rates definition sij d jy hi d fi d prfx d ijx ig the probability of dying at age i given survival past age i a crucial observation is that the probability sij of surviving past age j given survival past age i is the product of surviving each intermediate year hk d prfx jjx igi kdi first you have to survive year i probability hi then year i c probability etc. up to year j probability hj notice that si equals os in table is an estimate of sij for i d first each hi was estimated as the binomial proportion of the number of deaths yi among the ni clients and then we set o hi d yi o d jy hk the insurance company doesn t have to wait years to learn the probability of a living past to be in the table. one year s data hazard rates are more often described in terms of a continuous positive random variable t called time having density function f and reverse cdf or survival function s.t dz f dx d prft tg the hazard rate t h.t d f satisfies prft t c dt tg for dt in analogy with the analog of is h.t of course the estimates can go badly wrong if the hazard rates change over time. t survival analysis and the em algorithm prft d exp h.x dx so in particular the reverse cdf is given by s.t d exp h.x dx a one-sided exponential density f d for t has s.t d and constant hazard rate h.t d the name memoryless is quite appropriate for density having survived to any time t the probability of surviving dt units more is always the same about dt no matter what t is. if human lifetimes were exponential there wouldn t be old or young people only lucky or unlucky ones. censored data and the kaplan meier estimate table reports the survival data from a randomized clinical trial run by ncog northern california oncology group comparing two treatments for head and neck cancer arm a chemotherapy versus arm b chemotherapy plus radiation. the response for each patient is survival time in days. the c sign following some entries indicates censored data that is survival times known only to exceed the reported value. these are patients lost to followup mostly because the ncog experiment ended with some of the patients still alive. this is what the experimenters hoped to see of course but it complicates the comparison. notice that there is more censoring in arm b. in the absence of censoring we could run a simple two-sample test maybe wilcoxon s test to see whether the more aggressive treatment of arm b was increasing the survival times. kaplan meier curves provide a graphical comparison that takes proper account of censoring. next section describes an appropriate censored data two-sample test. kaplan meier curves have become familiar friends to medical researchers a lingua franca for reporting clinical trial results. life table methods are appropriate for censored data. table puts the arm a results into the same form as the insurance study of table now censored data and kaplan meier table censored survival times in days from two arms of the ncog study of headneck cancer. arm a chemotherapy arm b chemotherapycradiation with the time unit being months. of the patients in arm a d was observed to die in the first month after treatment this left at risk d of whom died in the second month d of the remaining died in their third month after treatment and one was lost to followup this being noted in the l column of the table leaving d patients at risk at the beginning of month etc. os here is calculated as in except starting at time instead of there is nothing wrong with this estimate but binning the ncog survival data by months is arbitrary. why not go down to days as the data was originally presented in table a kaplan meier survival curve is the limit of life table survival estimates as the time unit goes to zero. observations zi for censored data problems are of the form zi d di where ti equals the observed survival time while di indicates whether or not there was censoring di d if death observed if death not observed the patients were enrolled at different calendar times as they entered the study but for each patient time zero in the table is set at the beginning of his or her treatment. survival analysis and the em algorithm table arm a of the ncog headneck cancer study binned by month n d number at risk y d number of deaths l d lost to followup h d hazard rate yn os d life table survival estimate. month n y l h os month n y l h os di d corresponds to a c in table let t.n denote the ordered survival censored or not with corresponding indicator d.k for t.k. the kaplan meier estimate for survival probability s.j d prfx t.j is then the life table estimate n k os.j dy kj n k c assuming no ties among the survival times which is convenient but not crucial for what follows. censored data and kaplan meier os jumps downward at death times tj and is constant between observed deaths. figure ncog kaplan meier survival curves lower arm a only upper arm b vertical lines indicate approximate confidence intervals. the kaplan meier curves for both arms of the ncog study are shown in figure arm b the more aggressive treatment looks better its survival estimate occurs at days compared with days for arm a. the answer to the inferential question is b really better than a or is this just random variability? is less clear-cut. the accuracy of os.j can be estimated from greenwood s formula for its standard deviation back in life table notation os.j d os.j sd kj yk nk.nk yk the vertical bars in figure are approximate confidence limits for the two curves based on greenwood s formula. they overlap enough to cast doubt on the superiority of arm b at any one choice of days but the twosample test of the next section which compares survival at all timepoints will provide more definitive evidence. life tables and the kaplan meier estimate seem like a textbook example of frequentist inference as described in chapter a useful probabilistic a chemotherapy onlyarm b chemotherapy radiation survival analysis and the em algorithm result is derived and then implemented by the plug-in principle there is more to the story though as discussed below. life table curves are nonparametric in the sense that no particular relationship is assumed between the hazard rates hi. a parametric approach can greatly improve the curves accuracy. reverting to the life table form of table we assume that the death counts yk are independent binomials and that the logits d hkg satisfy some sort of regression equation yk as in a cubic regression for instance would set xk d k for the kth row of x with x for table bi.nk hk d x figure parametric hazard rate estimates for the ncog study. arm a black curve has about times higher hazard than arm b for all times more than a year after treatment. standard errors shown at and months. the parametric hazard-rate estimates in figure were instead based on a cubic-linear spline xk k where equals k for k and for k the vector per montharm a chemotherapy onlyarm b chemotherapy radiation the log-rank test d x describes a curve that is cubic for k linear for k and joined smoothly at the logistic regression maximum likelihood estimate o produced hazard rate curves o c e hk d o as in the black curve in figure traces o red curve is that for arm b fit separately. hk for arm a while the k comparison in terms of hazard rates is more informative than the survival curves of figure both arms show high initial hazards peaking at five months and then a long slow arm b hazard is always below arm a in a ratio of about to after the first year. approximate confidence limits obtained as in don t overlap indicating superiority of arm b at and months after treatment. in addition to its frequentist justification survival analysis takes us into the fisherian realm of conditional inference section the yk s in model are considered conditionally on the nk s effectively treating the nk values in table as ancillaries that is as fixed constants by themselves containing no statistical information about the unknown hazard rates. we will examine this tactic more carefully in the next two sections. the log-rank test a randomized clinical trial interpreted by a two-sample test remains the gold standard of medical experimentation. interpretation usually involves student s two-sample t-test or its nonparametric cousin wilcoxon s test but neither of these is suitable for censored data. the log-rank test employs an ingenious extension of life tables for the nonparametric twosample comparison of censored survival data. table compares the results of the ncog study for the first six after treatment. at the of month there were patients at risk in arm b none of whom died compared with at risk and death in arm a. this left at risk in arm b at the beginning of month and in arm a with and deaths during the month respectively. the cubic linear spline is designed to show more detail in the early months where there is more available patient data and where hazard rates usually change more quickly. a month is defined here as days. the beginning of month is each patient s initial treatment time at which all patients ever enrolled in arm b were at risk that is available for observation. survival analysis and the em algorithm table life table comparison for the first six months of the ncog study. for example at the beginning of the sixth month after treatment there were remaining arm b patients of whom died during the month compared with at risk and dying in arm a. the conditional expected number of deaths in arm a assuming the null hypothesis of equal hazard rates in both arms was using expression month arm b arm a at risk died at risk died expected number arm a deaths to followup were assumed to occur at the end of each month there was such at the end of month reducing the number at risk in arm a to for month the month data is displayed in two-by-two tabular form in table showing the notation used in what follows na for the number at risk in arm a nd for the number of deaths etc. y indicates the number of arm a deaths. if the marginal totals na nb nd and ns are given then y determines the other three table entries by subtraction so we are not losing any information by focusing on y. table two-by-two display of data for the ncog study. e is the expected number of arm a deaths assuming the null hypothesis of equal hazard rates column of table arm a arm b died y d e d d nd survived ns d na d nb d n d consider the null hypothesis that the hazard rates for month are the log-rank test the same in arm a and arm b w d under y has mean e and variance v e d nand v d nanb nd ns as calculated according to the hypergeometric distribution. e d and v d in table we can form a two-by-two table for each of the n d months of the ncog study calculating yi ei and vi for month i. the log-rank statistic z is then defined to be z d nx ei nx vi the idea here is simple but clever. each month we test the null hypothesis of equal hazard rates w hai d hbi the numerator yi ei has expectation under but if hai is greater than hbi that is if treatment b is superior then the numerator has a positive expectation. adding up the numerators gives us power to detect a general superiority of treatment b over a against the null hypothesis of equal hazard rates hai d hbi for all i. for the ncog study binned by months ei d vi d yi d nx nx nx giving log-rank test statistic z d asymptotic calculations based on the central limit theorem suggest z n under the null hypothesis that the two treatments are equally effective i.e. that hai d hbi for i d n in the usual interpretation z d is significant at the one-sided level providing moderately strong evidence in favor of treatment b. an impressive amount of inferential guile goes into the log-rank test. survival analysis and the em algorithm working with hazard rates instead of densities or cdfs is essential for survival data. conditioning at each period on the numbers at risk na and nb in table finesses the difficulties of censored data censoring only changes the at-risk numbers in future periods. also conditioning on the number of deaths and survivals nd and ns in table leaves only the univariate statistic y to interpret at each period which is easily done through the null hypothesis of equal hazard rates adding the discrepancies yi ei in the numerator of than say adding the individual z values zi d ei or adding the i values accrues power for the natural alternative hypothesis hai hbi for all i while avoiding destabilization from small values of vi. i each of the four tactics had been used separately in classical applications. putting them together into the log-rank test was a major inferential accomplishment foreshadowing a still bigger step forward the proportional hazards model our subject in the next section. conditional inference takes on an aggressive form in the log-rank test. let di indicate all the data except yi available at the end of the ith period. for month in the ncog study includes all data for months in table and the marginals na nb nd and ns in table but not the y value for month the key assumption is that under the null hypothesis of equal hazard rates yijdi vi ind here meaning that the yi s can be treated as independent quantities with means and variances in particular we can add the variances vi to get the denominator of partial likelihood argument described in the endnotes justifies adding the variances. the purpose of all this fisherian conditioning is to simplify the inference the conditional distribution yijdi depends only on the hazard rates hai and hbi nuisance parameters relating to the survival times and censoring mechanism of the data in table are hidden away. there is a price to pay in testing power though usually a small one. the lost-to-followup values l in table have been ignored even though they might contain useful information say if all the early losses occurred in one arm. the proportional hazards model the proportional hazards model the kaplan meier estimator is a one-sample device dealing with data coming from a single distribution. the log-rank test makes two-sample comparisons. proportional hazards ups the ante to allow for a full regression analysis of censored data. now the individual data points zi are of the form zi d ti di where ti and di are observed survival time and censoring indicator as in and ci is a known p vector of covariates whose effect on survival we wish to assess. both of the previous methods are included here for the log-rank test ci indicates treatment say ci equals or for arm a or arm b while ci is absent for kaplan meier. table pediatric cancer data first of children. sex d male d female race d white d nonwhite age in years entry d calendar date of entry in days since july far d home distance from treatment center in miles t d survival time in days d d if death observed if not. sex race age entry far t d survival analysis and the em algorithm medical studies regularly produce data of form an example the pediatric cancer data is partially listed in table the first of n d cases are shown. there are five explanatory covariates in the table s caption sex race age at entry calendar date of entry into the study and far the distance of the child s home from the treatment center. the response variable t is survival in days from time of treatment until death. happily only of the children were observed to die d some left the study for various reasons but most of the d d cases were those children still alive at the end of the study period. of particular interest was the effect of far on survival. we wish to carry out a regression analysis of this heavily censored data set. the proportional hazards model assumes that the hazard rate hi for the ith individual is hi d i here is a baseline hazard we need not specify and is an unknown p-parameter vector we want to estimate. for concise notation let model says that individual i s hazard is a constant nonnegative factor times the baseline hazard. equivalently from the ith survival function si is a power of the baseline survival function d ec i i si d larger values of lead to more quickly declining survival curves i.e. to worse survival in let j be the number of observed deaths j d here occurring at times t.j again for convenience assuming no just before time t.j there is a risk set of individuals still under observation whose indices we denote by rj rj d fi w ti t.j let ij be the index of the individual observed to die at time t.j the key to proportional hazards regression is the following result. more precisely assuming only one event a death occurred at t.j with none of the other individuals being lost to followup at exact time t.j the proportional hazards model lemma under the proportional hazards model the conditional probability given the risk set rj that individual i in rj is the one observed to die at time t.j is prfij d ij x k i ec rjg d ec rj to put it in words given that one person dies at time t.j the probability i among the set of individuals it is individual i is proportional to exp.c at risk. for the purpose of estimating the parameter vector in model we multiply factors to form the partial likelihood l. d jy ij c x rj k ec l. is then treated as an ordinary likelihood function yielding an approximately unbiased mle-like estimate o d arg max fl. o l o with an approximate covariance obtained from the second-derivative matrix of l. d log l. as in section table shows the proportional hazards analysis of the pediatric cancer data with the covariates age entry and far standardized to have mean and standard deviation for the neither sex nor race seems to make much difference. we see that age is a mildly significant factor with older children doing better the estimated regression coefficient is negative. however the dramatic effects are date of entry and far. individuals who entered the study later survived longer perhaps the treatment protocol was being improved while children living farther away from the treatment center did worse. justification of the partial likelihood calculations is similar to that for the log-rank test but there are some important differences too the proportional hazards model is semiparametric semi because we don t have to specify in rather than nonparametric as before and the table was obtained using the r program coxph. survival analysis and the em algorithm table proportional hazards analysis of pediatric cancer data entry and far standardized. age significantly negative older children doing better entry very significantly negative showing hazard rate declining with calendar date of entry far very significantly positive indicating worse results for children living farther away from the treatment center. last two columns show limits of approximate confidence intervals for exp. sex race age entry far sd z-value p-value exp. lower upper emphasis on likelihood has increased the fisherian nature of the inference moving it further away from pure frequentism. still more fisherian is the emphasis on likelihood inference in rather than the direct frequentist calculations of the conditioning argument here is less obvious than that for the kaplan meier estimate or the log-rank test. has its convenience possibly come at too high a price? in fact it can be shown that inference based on the partial likelihood is highly efficient assuming of course the correctness of the proportional hazards model missing data and the em algorithm censored data the motivating factor for survival analysis can be thought of as a special case of a more general statistical topic missing data. what s missing in table for example are the actual survival times for the c cases which are known only to exceed the tabled values. if the data were not missing we could use standard statistical methods for instance wilcoxon s test to compare the two arms of the ncog study. the em algorithm is an iterative technique for solving missing-data inferential problems using only standard methods. a missing-data situation is shown in figure n d points have been independently sampled from a bivariate normal distribution missing data and the em algorithm figure forty points from a bivariate normal distribution the last with missing means variances and correlation however the second coordinates of the last points have been lost. these are represented by the circled points in figure with their values arbitrarily set to we wish to find the maximum likelihood estimate of the parameter vector d the standard maximum likelihood estimates d d d d d survival analysis and the em algorithm are unavailable for and because of the missing data. the em algorithm begins by filling in the missing data in some way say by setting d for the missing values giving an artificially complete data set then it proceeds as follows. the standard method is applied to the filled-in to produce o d this is the m maximizing each of the missing values is replaced by its conditional expectation d o given the nonmissing data this is the e expectation step. in our case the missing values are replaced by c o is suitably small. the e and m steps are repeated at the j th stage giving a new artificially complete data set data.j and an updated estimate o the iteration stops when k o table shows the em algorithm at work on the bivariate normal example of figure in exponential families the algorithm is guaranteed to converge to the mle o based on just the observed data o moreover the likelihood fo increases with every step j convergence can be sluggish as it is here for and the em algorithm ultimately derives from the fake-data principle a property of maximum likelihood estimation going back to fisher that can only briefly be summarized here. let x d u represent the complete data of which o is observed while u is unobserved or missing. write the density for x as f d f be the mle of based just on o. and let o suppose we now generate simulations of u by sampling from the conditional distribution fo fo u for k d k stars indicating creation by the statistician and not by observation giving fake complete-data values x d fx d u x x let in this example data and and as in table stay the same in subsequent steps of the algorithm. are available as the complete-data estimates in missing data and the em algorithm table em algorithm for estimating means standard deviations and the correlation of the bivariate normal distribution that gave the data in figure step f goes to o yields mle o whose notional likelihoodqk it then turns out that o as k goes to infinity. in other words maximum likelihood estimation is self-consistent generating artificial data from the mle density fo doesn t change the mle. moreover any value o not equal to the mle o cannot be self-consistent carrying through using fo leads to hypothetical mle o having fo fo etc. a more general version of the em modern technology allows social scientists to collect huge data sets perhaps hundreds of responses for each of thousands or even millions of individuals. inevitably some entries of the individual responses will be missing. imputation amounts to employing some version of the fake-data principle to fill in the missing values. imputation s goal goes beyond findbe replaced by e with e indicating expectation with respect to o simulation is unnecessary in exponential families where at each stage data can as in survival analysis and the em algorithm ing the mle to the creation of graphs confidence intervals histograms and more using only convenient standard complete-data methods. finally returning to survival analysis the kaplan meier estimate is itself self-consistent. consider the arm a censored observation in table we know that that patient s survival time exceeded suppose we distribute his probability mass of the arm a sample to the right in accordance with the conditional distribution for x defined by the arm a kaplan meier survival curve. it turns out that redistributing all the censored cases does not change the original kaplan meier survival curve kaplan meier is self-consistent leading to its identification as the nonparametric mle of a survival function. notes and details the progression from life tables kaplan meier curves and the log-rank test to proportional hazards regression was modest in its computational demands until the final step. kaplan meier curves lie within the capabilities of mechanical calculators. not so for proportional hazards which is emphatically a child of the computer age. as the algorithms grew more intricate their inferential justification deepened in scope and sophistication. this is a pattern we also saw in chapter in the progression from bioassay to logistic regression to generalized linear models and will reappear as we move from the jackknife to the bootstrap in chapter censoring is not the same as truncation. for the truncated galaxy data of section we learn of the existence of a galaxy only if it falls into the observation region the censored individuals in table are known to exist but with imperfect knowledge of their lifetimes. there is a version of the kaplan meier curve applying to truncated data which was developed in the astronomy literature by lynden-bell the methods of this chapter apply to data that is left-truncated as well as right-censored. in a survival time study of a new hiv drug for instance subject i might not enter the study until some time after his or her initial diagnosis in which case ti would be left-truncated at as well as possibly later right-censored. this only modifies the composition of the various risk sets. however other missing-data situations e.g. left- and right-censoring require more elaborate less elegant treatments. formula let the interval be partitioned into a large number of subintervals of length dt with tk the midpoint of subinterval k. notes and details as in using prft h.ti dt o nx h.ti dt o h.ti dt d exp exp which as dt goes to kaplan meier estimate. in the life table formula k d let the time unit be small enough to make each bin contain at most one value t.k then at t.k o h.k d d.k n k c giving expression greenwood s formula in the life table formulation of sec tion gives from nk o hk bi.nk hk we get n n log osj var var log log osj d jx o log hk o jx hk hk hk nk o d jx d jx var o hk n log osj o jx where we have used the delta-method approximation varflog xg varfxg plugging in hk d yknk yields yk var nk.nk yk then the inverse approximation varfxg d varflog xg gives greenwood s formula the censored data situation of section does not enjoy independence between the o hk values. however successive conditional independence given the nk values is enough to verify the result as in the partial likelihood calculations below. note the confidence intervals in figure were obtained survival analysis and the em algorithm by exponentiating the intervals log osj h n log osj var parametric life tables analysis. figure and the analysis behind it is developed in efron where it is called partial logistic regression in analogy with partial likelihood. the log-rank test. this chapter featured an all-star cast including four of the most referenced papers of the post-war era kaplan and meier cox on proportional hazards dempster et al. codifying and naming the em algorithm and mantel and haenszel on the log-rank test. gives a careful and early analysis of the mantel haenszel idea. the not very helpful name log-rank does at least remind us that the test depends only on the ranks of the survival times and will give the same result if all the observed survival times ti are monotonically transformed say to exp.ti or t it is often referred to as the mantel haenszel or cochran mantel haenszel test in older literature. kaplan meier and proportional hazards are also rank-based procedures. i hypergeometric distribution. hypergeometric calculations as for table are often stated as follows n marbles are placed in an urn na labeled a and nb labeled b nd marbles are drawn out at random y is the number of these labeled a. elementary not simple calculations then produce the conditional distribution of y given the table s marginals na nb n nd and ns n nd prfyjmarginalsg d na y nb nd y for max.na ns y min.nd na and expressions for the mean and variance. if na and nb go to infinity such that nan pa and nb pa then v nd pa the variance of y bi.nd pa. log-rank statistic z why is nominator for z? let ui d yi ei in so z s numerator ispn vi the correct ui with uijdi vi under the null hypothesis of equal hazard rates. this implies that unconditionally efuig d for j i uj is a function of di yj and d nx i varfuig nx e ui d e nx nx vi ej are so efuj uijdig d and again unconditionally efuj uig d therefore assuming equal hazard rates notes and details the last approximation replacing unconditional variances varfuig with conditional variances vi is justified in crowley as is the asymptotic normality the infinitesimal interval t.j c d t is hi d t so rj the probability pi that death occurs in lemma for i i d t pi d y pi d pi pk and the probability of event ai that individual i dies while the others don t is but the ai are disjoint events so given that has occurred the probability that it is individual i who died is eci x eck pj pi this becoming exactly as d t rj rj partial likelihood cox introduced partial likelihood as inferential justification for the proportional hazards model which had been questioned in the literature. let dj indicate all the observable information available just before time t.j including all the death or loss times for individuals having ti t.j that dj determines the risk set rj by successive conditioning we write the full likelihood f as f d f d jy jy rj f f letting d where is a nuisance parameter vector having to do survival analysis and the em algorithm with the occurrence and timing of events between observed deaths jy l. f d f where l. is the partial likelihood the proportional hazards model simply ignores the bracketed factor in l. d log l. is treated as a genuine likelihood maximized to give o and assigned covariance matrix as in section efron shows this tactic is highly efficient for the estimation of o l. fake-data principle. for any two values of the parameters and define dz log u d u this being the limit as k of d lim log u k kx the fake-data log likelihood under if were the true value of using f u d f definition gives d log d log log d with d the deviance which is always positive unless ujo has the same distribution under and which we will assume doesn t happen. suppose we begin the em algorithm at d and find the value maximizing then and d implies in that is we have increased the likelihood of the observed data. now take d o d arg max f then the right side of is lo for any not equaling d o o negative implying lo putting this successively computing by fake-data mle calculations increases f at every step and the only stable point of the algorithm is at d o kaplan meier self-consistency. this property was verified in efron where the name was coined. generating the fake data is equivalent to the e step of the algorithm the m step being the maximization of lj the jackknife and the bootstrap a central element of frequentist inference is the standard error. an algorithm has produced an estimate of a parameter of interest for instance the mean nx d for the all scores in the top panel of figure how accurate is the estimate? in this case formula for the standard of a sample mean gives estimated standard error so one can t take the third digit of nx d very seriously and even the is dubious. bse d direct standard error formulas like exist for various forms of averaging such as linear regression and for hardly anything else. taylor series approximations device of section extend the formulas to smooth functions of averages as in before computers applied statisticians needed to be taylor series experts in laboriously pursuing the accuracy of even moderately complicated statistics. the jackknife was a first step toward a computation-based nonformulaic approach to standard errors. the bootstrap went further toward automating a wide variety of inferential calculations including standard errors. besides sparing statisticians the exhaustion of tedious routine calculations the jackknife and bootstrap opened the door for more complicated estimation algorithms which could be pursued with the assurance that their accuracy would be easily assessed. this chapter focuses on standard errors with more adventurous bootstrap ideas deferred to chapter we end with a brief discussion of accuracy estimation for robust statistics. we will use the terms standard error and standard deviation interchangeably. the jackknife and the bootstrap the jackknife estimate of standard error the basic applications of the jackknife apply to one-sample problems where the statistician has observed an independent and identically distributed sample x d xn from an unknown probability distribution f on some space x x can be anything the real line the plane a function a real-valued statistic o has been computed by applying some algorithm to x xi for i d n f o d s.x and we wish to assign a standard error to o the standard deviation of o let x.i be the sample with xi removed d s.x under sampling model that is we wish to estimate x.i d xn and denote the corresponding value of the statistic of interest as o d s.x.i n n d nx then the jackknife estimate of standard error for o o nx is bsejack d with o o in the case where o is the mean nx of real values xn x is an interval of the real line o be expressed as o d xi d nx o d xi and is their average excluding xi which can o equation gives o bsejack d nx o fudge factor in definition was inserted to makebsejack agree exactly the same as the classic formula this is no coincidence. the with when o if x is an interval of the real line we might take f to be the usual cumulative is nx. distribution function but here we will just think of f as any full description of the probability distribution for an xi on x the jackknife estimate of standard error the advantage ofbsejack is that definition can be applied in an automatic way to any statistic o d s.x. all that is needed is an algorithm that computes for the deleted data sets x.i computer power is being substituted for theoretical taylor series calculations. later we will see that the underlying inferential ideas plug-in estimation of frequentist standard errors haven t changed only their implementation. as an example consider the kidney function data set of section here the data consists of n d points yi with x d age and y d tot in figure the generic xi in now represents the pair yi and f describes a distribution in the plane. suppose we are interested in the correlation between age and tot estimated by the usual sample correlation o d s.x nx s.x d nx nx applying gavebsejack d for the accuracy of o computed to be o d for the kidney data. casebsetaylor d nonparametric bootstrap computations section also gave estimated standard error the classic taylor series formula looks quite formidable in this nx.yi ny o c c c d nx where nxh.yi nykn need be assumed. it gavebse d it is worth emphasizing some features of the jackknife formula it is nonparametric no special form of the underlying distribution f inputs the data set x and the function s.x and outputsbsejack. it is completely automatic a single master algorithm can be written that the algorithm works with data sets of size not n. there is a hidden assumption of smooth behavior across sample sizes. this can be worrisome for statistics like the sample median that have a different definition for odd and even sample size. the jackknife and the bootstrap the jackknife standard error is upwardly biased as an estimate of the the connection of the jackknife formula with taylor series meth true standard error. ods is closer than it appears. we can write bsejack dpn i where di d p o o n.n as discussed in section the di are approximate directional derivatives measures of how fast the statistic s.x is changing as we decrease the weight on data point xi. so jack is proportional to the sum of squared derivatives of s.x in the n component directions. taylor series expressions such as amount to doing the derivatives by formula rather than numerically. figure the lowess curve for the kidney data of figure vertical bars indicate standard errors jackknife blue dashed bootstrap red solid. the jackknife greatly overestimates variability at age the principal weakness of the jackknife is its dependence on local derivafigure can result in erratic behavior forbsejack. figure illustrates tives. unsmooth statistics s.x such as the kidney data lowess curve in the point. the dashed blue vertical bars indicate jackknife standard er the nonparametric bootstrap rors for the lowess curve evaluated at ages for the most part these agree with the dependable bootstrap standard errors solid red bars described in section but things go awry at age where the local derivatives greatly overstate the sensitivity of the lowess curve to global changes in the sample x. the nonparametric bootstrap from the point of view of the bootstrap the jackknife was a halfway house between classical methodology and a full-throated use of electronic computation. term computer-intensive statistics was coined to describe the bootstrap. the frequentist standard error of an estimate o d s.x is ideally the standard deviation we would observe by repeatedly sampling new versions of x from f this is impossible since f is unknown. instead the bootstrap ingenious device number in section substitutes an estimate of for f and then estimates the frequentist standard by direct simulation a feasible tactic only since the advent of electronic computation. the bootstrap estimate of standard error for a statistic o d s.x computed from a random sample x d xn begins with the notion of a bootstrap sample d x x n x where each x i is drawn randomly with equal probability and with replacement from xng. each bootstrap sample provides a bootstrap replication of the statistic of some large number b of bootstrap samples are independently drawn d in figure the corresponding bootstrap replications are calculated say o for b d b d s.x the resulting bootstrap estimate of standard error for o standard deviation of the o bseboot d o bx with o values o is the empirical d bx b o is intended to avoid confusion with the original data x which stays the star notation x fixed in bootstrap computations and likewise o vis-a-vis o. o d s.x f x the jackknife and the bootstrap motivation forbseboot begins by noting that o is obtained in two steps first x is generated by iid sampling from probability distribution f and then o is calculated from x according to algorithm o we don t know f but we can estimate it by the empirical probability distribution of that puts probability on each point xi weight on each point yi in figure notice that a bootstrap sample x is an iid sample drawn from of since then each x independently has equal probability of being any member of xng. it can be shown that of maximizes the probability of obtaining the observed sample x under all possible choices of f in i.e. it is the nonparametric mle of f bootstrap replications o are obtained by a process analogous to of in the real world we only get to see the single value o but the bootstrap world is more generous we can generate as many bootstrap replications o as we want or have time for and directly estimate their suggests correctly in most cases thatbseboot approaches the true standard of approaches f as n grows large variability as in the fact that error of o the true standard deviation of o i.e. its standard error can be thought of as a function of the probability distribution f that generates the data say sd.f hypothetically sd.f inputs f and outputs the standard deviation of o which we can imagine being evaluated by independently running some enormous number of times n and then computing the empirical standard deviation of the resulting o x o values with o d nx n o nx o o sd.f d bseboot d sd. of the bootstrap standard error of o is the plug-in estimate more exactly sd. of is the ideal bootstrap estimate of standard error what we would get by letting the number of bootstrap replications b go to infinity. in practice we have to stop at some finite value of b as discussed in what follows. the nonparametric bootstrap and multisample versions will be taken up later. as with the jackknife there are several important points worth empha sizing aboutbseboot. ten that inputs the data x and the function and outputsbseboot. it is completely automatic. once again a master algorithm can be we have described the one-sample nonparametric bootstrap. parametric bootstrapping shakes the original data more violently than from x. the bootstrap is more ing producing nonlocal deviations of x dependable than the jackknife for unsmooth statistics since it doesn t b d is usually sufficient for evaluatingbseboot. larger values depend on local derivatives. or will be required for the bootstrap confidence intervals of chapter there is nothing special about standard errors. we could just as well use the bootstrap replications to estimate the expected absolute error efj o fisher s mle formula is applied in practice via or any other accuracy measure. bsefisher d that is by plugging in o for after a theoretical calculation of se. the bootstrap operates in the same way at though the plugging in is done before rather than after the calculation. the connection with fisherian theory is more obvious for the parametric bootstrap of section the jackknife is a completely frequentist device both in its assumptions and in its applications errors and biases. the bootstrap is also basically frequentist but with a touch of the fisherian as in the relation with its versatility has led to applications in a variety of estimation and prediction problems with even some bayesian connections. unusual applications can also pop up for the jackknife see the jackknifeafter-bootstrap comment in the chapter endnotes. from a classical point of view the bootstrap is an incredible computational spendthrift. classical statistics was fashioned to minimize the hard labor of mechanical computation. the bootstrap seems to go out of its way to multiply it by factors of b d or or more. it is nice to report that all this computational largesse can have surprising data analytic payoffs. the students of table actually each took five tests mechanics vectors algebra analytics and statistics. table shows the jackknife and the bootstrap table correlation matrix for the student score data. the eigenvalues are and the eigenratio statistic o d and its bootstrap standard error estimate is d mechanics vectors algebra analytics statistics mechanics vectors algebra analysis statistics the sample correlation matrix and also its eigenvalues. the eigenratio statistic fidence interval calculations. the jackknife gave a bigger estimate o d largest eigenvaluesum eigenvalues measures how closely the five scores can be predicted by a single linear combination essentially an iq score for each student o d here indicating strong predictive power for the iq score. how accurate is ror estimate d was times more bootstraps b d bootstrap replications yielded bootstrap standard erthan necessary forbseboot but will be needed for chapter s bootstrap conbsejack d for coverage. these are based on an assumpstandard errors are usually used to suggest approximate confidence intervals often o tion of normality for o the histogram of the bootstrap replications of o as seen in figure disabuses belief in even approximate normality. compared with classical methods a massive amount of computation has gone into the histogram but this will pay off in chapter with more accurate confidence limits. we can claim a double reward here for bootstrap methods much wider applicability and improved inferences. the bootstrap histogram invisible to classical statisticians nicely illustrates the advantages of computer-age statistical inference. resampling plans there is a second way to think about the jackknife and the bootstrap as algorithms that reweight or resample the original data vector x d resampling plans figure histogram of b d bootstrap replications o for the eigenratio statistic for the student score data. the vertical black line is at o d the long left tail shows that normality is a dangerous assumption in this case. at the price of a little more abstraction resampling is by definition a vector of xn nects the two algorithms and suggests a class of other possibilities. a resampling vector p d pn nonnegative weights summing to nx p d pn with pi and pi d that is p is a member of the simplex sn resampling plans operate by holding the original data set x fixed and seeing how the statistic of interest o we denote the value of o changes as the weight vector p varies across sn. for a vector putting weight pi on xi as o d s.p the star notation now indicating any reweighting not necessarily from bootstrapping o d s.x describes the behavior of o mean s.x d nx we have s.p d pn in the real world while o d s.p describes it in the resampling world. for the sample pi xi. the unbiased estimate of errorbootstrap variance s.x dpn the jackknife and the bootstrap i can be seen to have s.p d n n pi i pi xi nx nx figure resampling simplex for sample size n d the center point is the green circles are the jackknife points p.i triples indicate bootstrap resampling numbers the bootstrap probabilities are for for each corner point and for each of the six starred points. letting d the resampling vector putting equal weight on each value xi we require in the definition of that d s.x d o the original estimate. the ith jackknife value o corresponds to resampling plans resampling vector p.i d with in the ith place. figure illustrates the resampling simplex applying to sample size n d with the center point being and the open circles the three possible jackknife vectors p.i with n d sample points there are only distinct bootstrap vectors also shown in figure let ni d the number of bootstrap draws in x ure are for example for x the bootstrap resampling vectors are of the form equaling xi. the triples in the having once and d xig j d nn p where the ni are nonnegative integers summing to n. according to definition of bootstrap sampling the vector n d nn follows a multinomial distribution with n draws on n equally likely categories n multn.n this gives bootstrap probability n nn nn on p figure is misleading in that the jackknife vectors p.i appear only as n grows large slightly closer to than are the bootstrap vectors p they are in fact an order of magnitude closer. subtracting from gives euclidean distance kp.i d ni bi n.n n for the bootstrap notice that ni in has a binomial distribution a hidden assumption of definition is that o d s.x has the same value for any permutation of x so for instance d d n the jackknife and the bootstrap with mean and variance then p d ni has mean and variance adding over the n coordinates gives the expected root mean square distance for bootstrap vector p i dp p n times further than an order of magnitude the function s.p has approximate directional derivative di d s.p.i kp.i o ideal bootstrap standard error estimate in the direction from toward p.i along the dashed lines in figure di measures the slope of function s.p at in the direction of p.i formula showsbsejack as proportional to the root out thatbsejack equalsbseboot for the fudge factor in if s.p is a linear function of p as it is for the sample mean it turns mean square of the slopes. most statistics are not linear and then the local jackknife resamples may provide a poor approximation to the full resampling behavior of s.p. this was the case at one point in figure we can easily evaluate the with only possible resampling points p d o pk pk o o bseboot d with o d s.p k and pk the probability from in fig! ure this rapidly becomes impractical. the number of distinct bootstrap samples for n points turns out to be for n d this is already while n d gives distinct at random which is what alpossible resamples. choosing b vectors p gorithm effectively is doing makes the un-ideal bootstrap standard error estimate almost as accurate as for b as small as or even less. n the luxury of examining the resampling surface provides a major advantage to modern statisticians both in inference and methodology. a variety of other resampling schemes have been proposed a few of which follow. resampling plans the infinitesimal jackknife looking at figure again the vector pi d c d c lies proportion of the way from to p.i then s qdi d lim i nx bseij d exactly defines the direction derivative at in the direction of p.i the infinitesimal jackknife estimate of standard error is usually evaluated numerically by setting to some small value in than d in we will meet the infinitesimal jackknife again in chapters and multisample bootstrap the median difference between the aml and the all scores in figure is mediff d d how accurate is an appropriate form of bootstrapping draws times with replacement from the aml patients times with replacement from the all patients and computes as the difference between the medians of the two bootstrap samples. one bootstrap sample of size from all the patients would result in random sample sizes for the groups adding inappropriate variability to the frequentist standard error estimate. givebseboot d the estimate is units above zero agreea histogram of b d values appears in figure they ing surprisingly well with the usual two-sample t-statistic on mean differences and its permutation histogram figure permutation testing can be considered another form of resampling. the jackknife and the bootstrap figure b d bootstrap replications for the median bseboot d the observed value mediff d difference between the aml and all scores in figure giving black line is more than standard errors above zero. moving blocks bootstrap suppose x d xn instead of being an iid sample is a time series. that is the x values occur in a meaningful order perhaps with nearby observations highly correlated with each other. let bm be the set of contiguous blocks of length m for example d xng presumably m is chosen large enough that correlations between xi and xj jj ij m are neglible. the moving block bootstrap first selects nm having constructed b such samplesbseboot is calculated blocks from bm and assembles them in random order to construct a bootstrap sample x as in the bayesian bootstrap let gn be independent one-sided exponential variates in table each having density for x the parametric bootstrap nx the bayesian bootstrap uses resampling vectors p it can be shown that p d gn is then uniformly distributed over the resampling simplex sn for n d uniformly distributed over the triangle in fig ure prescription is motivated by assuming a jeffreys-style uninformative prior distribution on the unknown distribution f gi has mean vector and covariance matrix distribution for p p n c this is almost identical to the mean and covariance of bootstrap resamples multn.n p p n the bayesian bootstrap and the ordinary bootstrap tend to agree at least for smoothly defined statistics o there was some bayesian disparagement of the bootstrap when it first appeared because of its blatantly frequentist take on estimation accuracy. and yet connections like have continued to pop up as we will see in chapter d s.p the parametric bootstrap in our description of bootstrap resampling of x o there is no need to insist that of be the nonparametric mle of f suppose d we are willing to assume that the observed data vector x comes from a parametric family f as in let be the mle of the bootstrap parametric resamples from f and proceeds as in to calculatebseboot. f x o f the jackknife and the bootstrap as an example suppose that x d xn is an iid sample of size n from a normal distribution xi n i d n then d nx and a parametric bootstrap sample is x where d x x n i d n n x i more adventurously if f were a family of time series models for x algorithm would still apply without any iid structure x would be a time series sampled from model f and o d s.x the b d b andbseboot from would give resampled statistic of interest. b independent realizations x o figure the gfr data of figure curves show the mle fits from polynomial poisson models for degrees of freedom df d the points on the curves show the fits computed at the centers x.j of the bins with the responses being the counts in the bins. the dashes at the base of the plot show the nine gfr values appearing in table as an example of parametric bootstrapping figure expands the gfr investigation of figure in addition to the seventh-degree polynomial fit we now show lower-degree polynomial fits for the parametric bootstrap and degrees of freedom df d obviously gives a poor fit df d give nearly identical curves df d gives only a slightly better fit to the raw data. the plotted curves were obtained from the poisson regression method used in section which we refer to as lindsey s method the x-axis was partitioned into k d bins with endpoints and centerpoints say x. d x.k d d etc. count vector y d yk was computed yk d in binkg y gives the heights of the bars in figure an independent poisson model was assumed for the counts for k d k yk the parametric model of degree df assumed that the values were described by an exponential polynomial of degree df in the x.k values j xj the mle o d the plotted curves in figure trace the mle values o df in model was o o d dfx log. d dfx o j xj y k how accurate are the curves? parametric bootstraps were used to assess their standard errors. that is poisson resamples were generated according to for k d k poi. and bootstrap mle values k calculated as above but now based on count rather than y. all of this was done b d times yielding vector y the results appear in table showing bseboot for df d bootstrap standard errors a single r command accomplishes this. the jackknife and the bootstrap table bootstrap estimates of standard error for the gfr density. poisson regression models df d as in figure each b d bootstrap replications nonparametric standard errors based on binomial bin counts. degrees of freedom gfr nonparametric standard error degrees of freedom evaluated at nine values of gfr. variability generally increases with increasing df as expected. choosing a best model is a compromise between standard error and possible definitional bias as suggested by figure with perhaps df d or the winner. if we kept increasing the degrees of freedom eventually df d we would exactly match the bar heights yk in the histogram. at this point the parametric bootstrap would merge into the nonparametric bootstrap. nonparametric is another name for very highly parameterized. the huge sample sizes associated with modern applications have encouraged nonparametric methods on the sometimes mistaken ground that estimation efficiency is no longer of concern. it is costly here as the nonparametric column of table figure returns to the student score eigenratio calculations of figure the solid histogram shows parametric bootstrap replications with f the five-dimensional bivariate normal distribution o here nx and o are the usual mle estimates for the expectation togram with bseboot d compared with the nonparametric estimate vector and covariance matrix based on the five-component student score vectors. it is narrower than the corresponding nonparametric bootstrap his these are the binomial standard errors yk.n ykn n d the nonparametric results look much more competitive when estimating cdf s rather than densities. the parametric bootstrap figure eigenratio example student score data. solid histogram b d parametric bootstrap replications o the five-dimensional normal mle line histogram the nonparametric replications of figure mle o d is vertical red line. from the different histogram bin limits from figure changing the details of the nonparametric histogram. parametric families act as regularizers smoothing out the raw data and de-emphasizing outliers. in fact the student score data is not a good candidate for normal modeling having at least one notable casting doubt on the smaller estimate of standard error. the classical statistician could only imagine a mathematical device that given any statistic o d s.x would produce a formula for its standard error as formula does for nx. the electronic computer is such a device. as harnessed by the bootstrap it automatically produces a numerical estimate of standard error not a formula with no further cleverness required. chapter discusses a more ambitious substitution of computer power for mathematical analysis the bootstrap computation of confidence intervals. as revealed by examining scatterplots of the five variates taken two at a time. fast and painless plotting is another advantage for twenty-first-century data analysts. standard errorsnonparametric the jackknife and the bootstrap influence functions and robust estimation the sample mean played a dominant role in classical statistics for reasons heavily weighted toward mathematical tractibility. beginning in the an important counter-movement robust estimation aimed to improve upon the statistical properties of the mean. a central element of that theory the influence function is closely related to the jackknife and infinitesimal jackknife estimates of standard error. we will only consider the case where x the sample space is an interval of the real line. the unknown probability distribution f yielding the iid sample x d xn in is now the cdf of a density function f on x a parameter of interest i.e. a function of f is to be estimated by the plug-in principle o of is the empirical probability distribution putting probability on each sample point xi. for the mean d t of where as in section d t dz of d nx dr xd of riemann stieltjes notation dr xdf and o xf dx and o d t xi x n the influence function of t evaluated at point x in x is defined to be if.x d lim t c x t where x is the one-point probability distribution putting probability on x. in words if.x measures the differential effect of modifying f by putting additional probability on x. for the mean dr xf we cal culate that if.x d x a fundamental theorem says that o d t of is approximately nx c if.xi o n with the approximation becoming exact as n goes to infinity. this implies that o is approximately the mean of the n iid variates if.xi and that the variance of o is approximately varfif.xg n o o n var influence functions and robust estimation varfif.xg being the variance of if.x for any one draw of x from f for the sample mean using in gives the familiar equality varfnxg d n varfxg the sample mean suffers from an unbounded influence function which grows ever larger as x moves farther from this makes nx unstable against heavy-tailed densities such as the cauchy robust estimation theory seeks estimators o of bounded influence that do well against heavytailed densities without giving up too much efficiency against light-tailed densities such as the normal. of particular interest have been the trimmed mean and its close cousin the winsorized mean. d or equivalently let x. denote the th percentile of distribution f satisfying f dz x. f dx z the th trimmed mean of f is defined as d the mean of the central portion of f trimming off the lower and upper portions. this is not the same as the th winsorized mean xf dx x. where dz x w d x w dx if x x. if x. x if x removes the outer portions of f while moves them into x. or in practice empirical versions o are used substituting the empirical density o f with probability at each xi for f and o there turns out to be an interesting relationship between the two the influence function of is a function of if d w this is pictured in figure where we have plotted empirical influence the jackknife and the bootstrap figure empirical influence functions for the leukemia all scores of figure the two dashed curves are if for the trimmed means for d and d the solid curve is if.x for the sample mean nx functions in of for f in definition relating to the leukemia all scores of figure and are plotted along with that is for the mean. table trimmed means and their bootstrap standard deviations for the leukemia all scores of figure b d bootstrap replications for each trim value. the last column gives empirical influence function estimates of the standard error which are also the infinitesimal jackknife estimates these fail for the median. trim mean median trimmed bootstrap mean sd functionmeantrimmed mean a mean a notes and details the upper panel of figure shows a moderately heavy right tail for the all distribution. would it be more efficient to estimate the center of vides an answer bseboot was calculated for nx and o the distribution with a trimmed mean rather than nx? the bootstrap protrim. d and the last being the sample median. it appears that o is moderately better than nx. this brings up an important question discussed in chapter if we use something like table to select an estimator how does the selection process affect the accuracy of the resulting estimate? we might also use the square root of formula to estimate the standard errors of the various estimators plugging in the empirical influence function for if.x. this turns out to be the same as using the infinitesimal jackknife these appear in the last column of table predictably this approach fails for the sample median whose influence function is a square wave sharply discontinuous at the median if.x d robust estimation offers a nice illustration of statistical progress in the computer age. trimmed means go far back into the classical era. influence functions are an insightful inferential tool for understanding the tradeoffs in trimmed mean estimation. and finally the bootstrap allows easy assessment of the accuracy of robust estimation including some more elaborate ones not discussed here. notes and details quenouille introduced what is now called the jackknife estimate of bias. tukey realized that quenouille-type calculations could be repurposed for nonparametric standard-error estimation inventing formula and naming it the jackknife as a rough and ready tool. miller s important paper a trustworthy jackknife asked when formula could be trusted. for the median. the bootstrap began as an attempt to better understand the jackknife s successes and failures. its name celebrates baron munchausen s success in pulling himself up by his own bootstraps from the bottom of a lake. burgeoning computer power soon overcame the bootstrap s main drawback prodigous amounts of calculation propelling it into general use. meanwhile theoretical papers were published asking when the bootstrap itself could be trusted. but not all of the time in common practice. the jackknife and the bootstrap a main reference for the chapter is efron s monograph the jackknife the bootstrap and other resampling plans. its chapter shows the equality of three nonparametric standard error estimates jaeckel s infinitesimal jackknife the empirical influence function estimate based on and what is known as the nonparametric delta method. bootstrap packages various bootstrap packages in r are available on the cran contributedpackages web site bootstrap being an ambitious one. algorithm shows a simple r program for nonparametric bootstrapping. aside from bookkeeping it s only a few lines long. algorithm an r program for the nonparametric bootstrap. boot function b func x is data vector or matrix each row a case b is number of bootstrap replications func is r function that inputs a data vector or matrix and returns a numeric number or vector other arguments for func x as.matrixx n nrowx get size of output fmat for in n replace true fmatb funcxi dropfmat the jackknife standard error. the monograph also contains efron and stein s result on the bias of the jackknife variance estimate the square of formula modulo certain sample size considerations the expectation of the jackknife variance estimate is biased upward for the true variance. for the sample mean nx the jackknife yields exactly the usual variance i while the ideal bootstrap estimate estimate gives nx notes and details as with the jackknife we could append a fudge factor to get perfect agreement with but there is no real gain in doing so. bootstrap sample sizes. letbseb indicate the bootstrap standard error estimate based on b replications the ideal bootstrap creasing b past a certain point is itself a statistic whose value b in any actual application there are diminishing returns from invaries with the observed sample x in leaving an irreducible remainder of randomness in any standard error estimate. section of efron and tibshirani shows that b d will almost always be plenty standard errors but not for bootstrap confidence intervals chapter smaller numbers or even less can still be quite useful in complicated situations where resampling is expensive. an early complaint bootstrap estimates are random is less often heard in an era of frequent and massive simulations. the bayesian bootstrap. rubin suggested the bayesian bootstrap section of efron used as an objective bayes justification for what we will call the percentile-method bootstrap confidence intervals in chapter jackknife-after-bootstrap. for the eigenratio example displayed in just the original replications. how accurate is this value? bootstrapping the bootstrap seems like too much work perhaps times resamples. it turns out though figure b d nonparametric bootstrap replications gavebseboot d that we can use the jackknife to estimate the variability ofbseboot based on now the deleted sample estimate in isbseboot.i the key idea is applying definition to this subset givesbseboot.i for the estimate of figure the jackknife-after-bootstrap calculations gavebsejack d for bseboot d in other words isn t very accurate which is to be expected for the standard error of a complicated statistic estimated from only n d observations. an infinitesimal jackknife version of this technique will play a major role in chapter among the original to consider those bootstrap samples x that do not include the point xi. about of the original b samples will be in this subset. section of efron and tibshirani shows that a fundamental theorem. tukey can justly be considered the founding father of robust statistics his paper being especially influential. huber s celebrated paper brought the subject into the realm of highconcept mathematical statistics. robust statistics the approach based on influence functions the book by hampel et al. conveys the breadth of a subject only lightly scratched in our section hampel the jackknife and the bootstrap introduced the influence function as a statistical tool. boos and serfling verified expression qualitative notions of robustness more than specific theoretical results have had a continuing influence on modern data analysis. bootstrap confidence intervals the jackknife and the bootstrap represent a different use of modern computer power rather than extending classical methodology from ordinary least squares to generalized linear models for example they extend the reach of classical inference. chapter focused on standard errors. here we will take up a more ambitious inferential goal the bootstrap automation of confidence intervals. the familiar standard intervals o d from a poisson model o for approximate coverage are immensely useful in practice but often the standard interval d o not very accurate. if we observe o poi. is a mediocre ap proximation to the exact standard intervals are symmetric around o this being their main weakness. poisson distributions grow more variable as increases which is why interval extends farther to the right of o d than to the left. correctly capturing such effects in an automatic way is the goal of bootstrap confidence interval theory. neyman s construction for one-parameter the student score data of table comprised n d pairs problems xi d vi i d using the neyman construction of section as explained there see also table in section bootstrap confidence intervals where mi and vi were student i s scores on the mechanics and vectors tests. the sample correlation coefficient o between mi and vi was computed to be o d question what can we infer about the true correlation between m and v? figure displayed three possible bayesian answers. confidence intervals provide the frequentist solution by far the most popular in applied practice. figure the solid curve is the normal correlation coefficient density fo for o student score data o endpoints of the confidence interval for with corresponding densities shown by dashed curves. these yield tail areas at o d the mle estimate for the d are the d and o suppose first that we assume a bivariate normal model for the o for sample correpairs vi in that case the probability density f lation o given true correlation has known form the solid curve in figure graphs f for d that is for set equal to the observed value o in more careful notation the curve graphs fo as a function of the dummy r taking values in this is an example of a parametric bootstrap distribution here with being o. neyman s construction two other curves f appear in figure for equaling these were numerically calculated as the solutions to o o d and d z o z o in words o above o below o fo dr d and fo dr d d while o is the smallest value of putting probability at least is the largest value with probability at least o o i is a confidence interval for the true correlation statement holding true with probability for every possible value of we have just described neyman s construction of confidence intervals o for one-parameter problems f we will consider the more difficult situation where there are nuisance parameters in addition to the parameter of interest one of the jewels of classical frequentist inference it depends on a pivotal argument ingenious device number of section to show that it produces genuine confidence intervals i.e. ones that contain the true parameter value at the claimed probability level in figure the argument appears in the chapter endnotes. for the poisson calculation it was necessary to define exactly what the smallest value of putting probability at least above o meant. o at this was done assuming that for any half of the probability f o d counted as above and similarly for calculating the upper limit. transformation invariance confidence intervals enjoy the important and useful property of transformation invariance. in the poisson example suppose our interest shifts from parameter to parameter d log the exact interval for then transforms to the exact interval for simply by taking logs of the endpoints d to state things generally suppose we observe o from a family of densio o for of coverage level and construct a confidence interval c. ties f bootstrap confidence intervals d in our examples. now let parameter be a monotonic increasing function of say d log in and likewise d m. then c. for o maps point by point into a level- confidence interval o for the point estimate. d m. d m. for o this just says that the event f is the same as the event so if the former always occurs with probability then so must the c. c latter. dn o figure the situation in figure after transformation to d m. according to the curves are nearly with standard deviation d p d transformation invariance has an historical resonance with the normal o correlation coefficient. fisher s derivation of f in was a mathematical triumph but a difficult one to exploit in an era of mechanical computation. most ingeniously fisher suggested instead working with the transformed parameter d m. where c d m. d log s f and likewise with statistic d m. imation the percentile method o then to a surprisingly good approx see figure which shows neyman s construction on the scale. in other words we are back in fisher s favored situation the sim ple normal translation problem where n n d n is the obviously correct confidence for closely approximating neyman s construction. the endpoints of are then transformed back to the scale according to the inverse transformation d c o seen in figure but without the in giving the interval c. volved computations. bayesian confidence statements are inherently transformation invariant. the fact that the neyman intervals are also invariant unlike the standard intervals has made them more palatable to bayesian statisticians. transformation invariance will play a major role in justifying the bootstrap confidence intervals introduced next. the percentile method our goal is to automate the calculation of confidence intervals given the bootstrap distribution of a statistical estimator o we want to automatically produce an appropriate confidence interval for the unseen parameter to this end a series of four increasingly accurate bootstrap confidence interval algorithms will be described. for coverage withbse taken to be the bootstrap standard error bseboot the limitations of this approach become obvious in o figure where the histogram shows b d nonparametric bootstrap replications o of the sample correlation coefficient for the student the first and simplest method is to use the standard interval this is an anachronism. fisher hated the term confidence interval after it was later coined by neyman for his comprehensive theory. he thought of as an example of the logic of inductive inference. bootstrap confidence intervals figure histogram of b d nonparametric bootstrap replications o for the student score sample correlation the solid curve is the ideal parametric bootstrap distribution fo as in figure observed correlation o d small triangles show histogram s and quantiles. score data obtained as in section the standard intervals are justified by taking literally the asymptotic normality of o o n the true standard error. relation will generally hold for large enough sample size n but we can see that for the student score data asymptotic normality has not yet set in with the histogram being notably long-tailed to the left. we can t expect good performance from the standard method in this case. parametric bootstrap distribution is just as nonnormal as shown by the smooth curve. o the percentile method uses the shape of the bootstrap distribution to improve upon the standard intervals having generated b bootstrap replications o either nonparametrically as in section or parametrically as in section we use the obvious percentiles of their distribution to define the percentile confidence limits. the histogram in figure has its and percentiles equal to and o bootstrap correlationsfrequency the percentile method and these are the endpoints of the central nonparametric percentile interval. figure a central confidence interval via the percentile method based on the nonparametric replications o figure of we can state things more precisely in terms of the bootstrap cdf the proportion of bootstrap samples less than t n o o. t og.t d of the bootstrap distribution is given by the b the th percentile point o inverse function of og og.t o d og o is the value putting proportion of the bootstrap sample to its left. the level- upper endpoint of the percentile interval say o is by definition in this notation the central percentile interval is d og d o o o o bootstrap confidence intervals the construction is illustrated in figure the percentile intervals are transformation invariant. let d m. as o in and likewise d m. monotonically increasing with o bootstrap replications for b d b. the bootstrap d m. percentiles transform in the same way d m so that as in d m verifying transformation invariance. o o in what sense does the percentile method improve upon the standard intervals? one answer involves transformation invariance. suppose there exists a monotone transformation d m. and d m. o such that n for every with constant. fisher s transformation almost accomplishes this for the normal correlation coefficient. it would then be true that parametric bootstrap replications would also follow that is the bootstrap cdf would be normal with mean and variance the th percentile of would equal n d d c z. where z. denotes the th percentile of a standard normal distribution z. d d d etc.. in other words the percentile method would provide fisher s obviously correct intervals for for coverage for example. but because of transformation invariance the percentile intervals for our original parameter would also be exactly correct. some comments concerning the percentile method are pertinent. o the percentile method o it only assumes its existence. the method does not require actually knowing the transformation to normality d m. if a transformation to form exists then the percentile intervals are not only accurate but also correct in the fisherian sense of giving the logically appropriate inference. the justifying assumption for the standard intervals o n becomes more accurate as the sample size n increases with decreasing as n but the convergence can be slow in cases like that of the normal correlation coefficient. the broader assumption that m. up convergence irrespective of whether or not it holds exactly. section makes this point explicit in terms of asymptotic rates of convergence. the standard method works fine once it is applied on an appropriate scale as in figure the trouble is that the method is not transformation invariant leaving the statistician the job of finding the correct scale. the percentile method can be thought of as a transformation-invariant version of the standard intervals an automatic fisher that substitutes massive computations for mathematical ingenuity. p n for some transformation speeds the method requires bootstrap sample sizes on the order of b d the percentile method is not the last word in bootstrap confidence intervals. two improvements the bc and bca methods will be discussed in the next section. table compares the various intervals as applied to the student score correlation o d table bootstrap confidence limits for student score correlation o d n d parametric exact limits from neyman s construction as in figure the bc and bca methods are discussed in the next two sections a two constants required for bca are parametric and nonparametric. parametric nonparametric standard percentile bc bca exact the label computer-intensive inference seems especially apt as ap bootstrap confidence intervals plied to bootstrap confidence intervals. neyman and fisher s constructions are expanded from a few special theoretically tractable cases to almost any situation where the statistician has a repeatable algorithm. automation the replacement of mathematical formulas with wide-ranging computer algorithms will be a major theme of succeeding chapters. for implies o d o d o z o fo z bias-corrected confidence intervals the ideal form for the percentile method o that the transformation d m. yields an unbiased estimator of constant variance. the improved methods of this section and the next take into account the possibility of bias and changing variance. we begin with bias. if n and n for all d m. as hypothesized in then n says o indicating bootstrap probability in which case the monotonicity of gives that is o we can check that. for a parametric family of densities f for the normal correlation coefficient density n d numerical integration gives o is median for o and likewise o o o which is not far removed from but far enough to have a small impact on proper inference. it suggests that o is biased upward relative to o that s why less than half of the bootstrap probability lies below o and by implication that o is upwardly biased for estimating accordingly confidence intervals should be adjusted a little bit downward. the biascorrected percentile method for short is a data-based algorithm for making such adjustments. d d o d o d median unbiasedness unlike the usual mean unbiasedness definition has the advantage of being transformation invariant. bias-corrected confidence intervals having simulated b bootstrap replications o parameto. ric or nonparametric let be the proportion of replications less than o o d n o o o b estimate of and define the bias-correction value d is the inverse function of the standard normal cdf. the bc where level- confidence interval endpoint is defined to be c z. o d og z. d og where og is the bootstrap cdf and z. d if d the median unbiased situation then d and o d og d o the percentile limit otherwise a bias correction is made. taking d for the normal correlation example value we would get from an infinite number of parametric bootstrap replications gives bias correction value notice that the bc limits are indeed shifted downward from the parametric percentile limits in table nonparametric bootstrapping gave about in this case making the bc limits nearly the same as the percentile limits. a more general transformation argument motivates the bc definition suppose there exists a monotone transformation d m. and d m. o such that for any n with and fixed constants. then the bc endpoints are accurate i.e. have the claimed coverage probabilities and are also obviously correct in the fisherian sense. see the chapter endnotes for proof and discussion. as before the statistican does not need to know the transformation that leads to n only that it exists. it is a broader target than n making the bc method better justified than the percentile method irrespective of whether or not such a transformation exists. there is no extra computational burden the bootstrap replications f o b d bg parametric or nonparametric provide og and giving o from bootstrap confidence intervals second-order accuracy p coverage errors of the standard confidence intervals typically decrease at n in the sample size n having calculated o order for an iid sample x d xn we can expect the actual coverage p probability to be d o o c o n pr n where depends on the problem at hand defines first-order accuracy. it can connote painfully slow convergence to the nominal coverage level requiring sample size to cut the error in half. a second-order accurate method say o makes errors of order only n o pr o c the improvement is more than theoretical. in practical problems like that of table second-order accurate methods bca defined in the following is one such often provide nearly the claimed coverage probabilities even in small-size samples. neither the percentile method nor the bc method is second-order accurate as in table they tend to be more accurate than the standard intervals. the difficulty for o lies in the ideal form o has constant standard error instead we now postulate the existence of a monotone transformation d m. and d m. n where it is assumed d m. o less restrictive than d c n here the acceleration a is a small constant describing how the standard deviation of varies with if a d we are back in situation but if not an amendment to the bc formula is required. the bca method bias-corrected and accelerated takes its level- confidence limit to be o d og c c z. c z. a still more elaborate transformation argument shows that if there exists a monotone transformation d m. and constants and a yielding this assumes d on the right side of which can always be achieved by further transforming to second-order accuracy then the bca limits have their claimed coverage probabilities and moreover are correct in the fisherian sense. bca makes three corrections to the standard intervals for nonnormality of o using the bootstrap percentiles rather than just the bootstrap standard error for bias the bias correction value and for nonconstant standard error a. notice that if a d then bca reduces to bc if d then bc reduces to the percentile method and if og the bootstrap histogram is normal then reduces to the standard interval all three of the corrections for nonnormality bias and acceleration can have substantial effects in practice and are necessary to achieve second-order accuracy. a great deal of theoretical effort was devoted to verifying the second-order accuracy and bca intervals under reasonably general d actual tail areas above and below table nominal central confidence intervals for poisson parameter having observed o o d defined as in figure of probability split at for instance lower standard limit actually puts probability above rather than nominal value bias correction value and acceleration a both equal nominal limits tail areas above below standard bc bca exact the advantages of increased accuracy are not limited to large sample sizes. table returns to our original example of observing o d from poisson model o poi. according to neyman s construction the exact limits give tail areas in both the above and below directions as in figure and this is nearly matched by the bca limits. however the standard limits are much too conservative at the left end and anti-conservative at the right. the mathematical side of statistics has also been affected by electronic computation where it is called upon to establish the properties of general-purpose computer algorithms such as the bootstrap. asymptotic analysis in particular has been aggressively developed the verification of second-order accuracy being a nice success story. bootstrap confidence intervals table nominal confidence intervals for the parametric and nonparametric eigenratio examples of figures and standard bc bca parametric d a d nonparametric d a d bootstrap confidence limits continue to provide better inferences in the vast majority of situations too complicated for exact analysis. one such situation is examined in table it relates to the eigenratio example illustrated in figures in this case the nonnormality and bias corrections stretch the bootstrap intervals to the left but the acceleration effect pulls right partially canceling out the net change from the standard intervals. the percentile and bc methods are completely automatic and can be applied whenever a sufficiently large number of bootstrap replications are available. the same cannot be said of bca. a drawback of the bca method is that the acceleration a is not a function of the bootstrap distribution and must be computed separately. often this is straightforward for one-parameter exponential families such as the poisson a equals in one-sample nonparametric problems a can be estimated from the jackknife resamples o pn o o o o oa d the abc method computes a in multiparameter exponential families as does the resampling-based r algorithm accel. confidence intervals require the number of bootstrap replications b to be on the order of rather than the or fewer needed for standard errors the corrections made to the standard intervals are more delicate than standard errors and require greater accuracy. there is one more cautionary note to sound concerning nuisance parameters biases can easily get out of hand when the parameter vector is bootstrap-t intervals high-dimensional. suppose we observe xi and wish to set a confidence interval for dpn will be sharply biased upward if n is at all large. to be specific if n d and o d we compute for i d n dpn i the mle o n i this o centile d d equal a ludicrously small bootstrap per og a warning sign against the bc or bca intervals which work most dependably for and jaj small say a more general warning would be against blind trust in maximum likelihood estimates in high dimensions. computing is a wise precaution even if it is not used for bc or bca purposes in case it alerts one to dangerous biases. the standard method estimated by the delta method except confidence intervals for classical applications were most often based on in a few especially simple situations such as the poisson. second-order accurate intervals are very much a computer-age development with both the algorithms and the inferential theory presupposing high-speed electronic computation. bootstrap-t intervals the initial breakthrough on exact confidence intervals came in the form of student s t distribution in suppose we independently observe data from two possibly different normal distributions x d xnx and y d yny xi n and yi n and wish to form a central confidence interval for d o d ny nx the obvious estimate is also obca a is zero in this model. bootstrap confidence intervals but its distribution depends on the nuisance parameter student s masterstroke was to base inference about on the pivotal quantity o t d is an unbiased estimate of d cpny c ny i nx represent the th percentile of a tdf distribution yields nx c ny t then has the student s t distribution with df d nx c ny degrees of freedom if d no matter what may be. t letting t df d as the upper level- interval of a student s t confidence limit. applied to the difference between the aml and all scores in figure the central student s t interval for d efamlg efallg was calculated to be o d o o o df here nx d ny d and df d student s theory depends on the normality assumptions of the bootstrap-t approach is to accept pretend that t in is pivotal but to estimate its distribution via bootstrap resampling. nonparametric bootstrap samples are drawn separately from x and y x x d nx from which we calculate o x and y andbse ny y y d and giving o o d t playing the role of as appropriate in the bootstrap world. and cor b d bg provide estimated percentiles t with o cations ft responding confidence limits o t d o t for the aml all example the t distribution differed only slightly from a distribution the resulting interval was nearly bootstrap-t intervals the same as lending credence to the original normality assumptions. figure b d nonparametric replications of bootstrap-t statistic for the student score correlation small triangles show and percentile points. the histogram is sharply skewed to the right the solid curve is student s t density for degrees of freedom. returning to the student score correlation example of table we can apply bootstrap-t methods by still taking t d bse the approximate standard error o pivotal but now with the true correlation o p the sample correlation and figure shows the histogram of b d nonparametric bootstrap replications t o to be notionally d these gave bootstrap percentiles o o d t t might be compared with for a standard distribution and interval from somewhat out of place compared with the other entries in the right panel of table bootstrap-t intervals are not transformation invariant. this means they can perform poorly or well depending on the scale of application. if performed on fisher s scale they agree well with exact intervals for t distributiondf bootstrap confidence intervals mula forbse. the correlation coefficient. a practical difficulty is the requirement of a for nevertheless the idea of estimating the actual distribution of a proposed pivotal quantity has great appeal to the modern statistical spirit. calculating the percentiles of the original student t distribution was a multi-year project in the early twentieth century. now we can afford to calculate our own special t table for each new application. spending such computational wealth wisely while not losing one s inferential footing is the central task and goal of twenty-first-century statisticians. objective bayes intervals and the confidence distribution interval estimates are ubiquitous. they play a major role in the scientific discourse of a hundred disciplines from physics astronomy and biology to medicine and the social sciences. neyman-style frequentist confidence intervals dominate the literature but there have been influential bayesian and fisherian developments as well as discussed next. g. bayes rule produces the posterior density of given a one-parameter family of densities f o is the marginal densityr f o d g. o the bayes cred g.j o o o and a prior density spans the central region of g.j o say where f ible interval c.j o d o b. o with c.j o z b.o a.o g.j o d d and with posterior probability in each tail region. confidence intervals of course require no prior information making them eminently useful in day-to-day applied practice. the bayesian equivalents are credible intervals based on uninformative priors section matching priors those whose credible intervals nearly match neyman confidence intervals have been of particular interest. jeffreys prior g. d i i dz o log f o d o f objective bayes intervals provides a generally accurate matching prior for one-parameter problems. figure illustrates this for the student score correlation where the credible interval is a near-exact match to the neyman interval of figure difficulties begin with multiparameter families we wish to construct an interval estimate for a one-dimensional function d t of the p-dimensional parameter vector and must somehow remove the effects of the nuisance parameters. in a few rare situations including the normal theory correlation coefficient this can be done exactly. pivotal methods do the job for student s t construction. bootstrap confidence intervals greatly extend the reach of such methods at a cost of greatly increased computation. bayesians get rid of nuisance parameters by integrating them out of the posterior density d now representing all the data x equaling y for the student t setup that is we the marginal density of d t given x and call it h.jx. a credible interval for c.jx is then constructed as in with h.jx playing the role of g.j o this leaves us the knotty problem of choosing an uninformative multidimensional prior we will return to the question after first discussing fiducial methods a uniquely fisherian device. interpretation of pivotality. we rewrite the student t pivotal t d as t d o where t has a student s t distribution with df degrees of freedom t andbse were fixed at their calculated tdf. having observed the data y fiducial theory assigns the distribution implied by as if o values while t was distributed as tdf. then o confidence limit is the th percentile of s fiducial distribution. fiducial constructions begin with what seems like an obviously incorrect o the student t level we seem to have achieved a bayesian posterior conclusion without any prior the historical development here is confused by fisher s refusal to accept neyman s confidence interval theory as well as his disparagement of bayesian ideas. as events worked out all of fisher s immense prestige was not enough to save fiducial theory from the scrapheap of failed statistical methods. often a difficult calculation as discussed in chapter enjoying the bayesian omelette without breaking the bayesian eggs in l. j. savage s words. bootstrap confidence intervals andbse exhaust the information about available from the and yet in arthur koestler s words the history of ideas is filled with barren truths and fertile errors. fisher s underlying rationale went something like this o data after which there remains an irreducible component of randomness described by t. this is an idea of substantial inferential appeal and one that can be rephrased in more general terms discussed next that bear on the question of uninformative priors. by definition an upper confidence limit o satisfies n o pr o d n o o pr c o d now we have indicated the observed data x in the notation and so we can consider o as a one-to-one function between in and a point in its parameter space that o is smoothly increasing in letting go to zero in determines the confidence density of say qgx. qgx. d d the local derivative of probability at location for the unknown parameter the derivative being taken at d o integrating qgx. recovers as a function of let d o d o and for any two values in then d d qgx. d dz z d d d as in there is nothing controversial about as long as we remember that the random quantity in is not but rather the interval which varies as a function of x. forgetting this leads to the textbook error of attributing bayesian properties to frequentist results there is probability that is in its confidence interval etc. this is exactly what the fiducial argument whether or not one accepts there is an immediate connection with matching priors. fiducial and confidence densities agree as can be seen in the student t situation at least in the somewhat limited catalog of cases fisher thought appropriate for fiducial calculations. objective bayes intervals suppose prior gives a perfect match to the confidence interval system o then by definition its posterior density h.jx must satisfy h.jx d d dz ox z ox for but this implies h.jx equals qgx. for all that is the confidence density qgx. is the posterior density of given x for any matching prior. qgx. d figure confidence density for poisson parameter having observed o between and as in table and areas in each tail. d there is area under the curve figure graphs the confidence density for o served o function of poi. having ob d this was obtained by numerically differentiating as a d poi. including splitting the atom of probability at according to table has area between and and area in each tail. whatever its provenance the graph delivers a striking picture of the uncertainty in the unknown value of bootstrap confidence intervals bootstrap confidence intervals provide easily computable confidence denog. be the bootstrap cdf and og. its density function let tained by differentiating a smoothed version of og. when og is based on b bootstrap replications. the percentile confidence limits o have d og. giving d og qgx. d og. is helpful to picture this in figure for the percentile method the bootstrap density is the confidence density. reweighting og. for the bca intervals the confidence density is obtained by qgx. d cw. where w. d z c az c az c with z d og. here is the standard normal density its cdf and c the constant that makes qgx. integrate to in the usual case where the bootstrap cdf is estimated from replications o b d b parametric or nonparametric the bca confidence density is a reweighted version of og. define bx o o w wb d w then the bca confidence density is the discrete density putting weight wb on o figure returns to the student score data n d students five scores each modeled normally as in figure for i d xi this is a p d parametric family expectations variances covariances. the parameter of interest was taken to be d maximum eigenvalue of it had mle o d this being the maximum eigenvalue of the mle sample covariance matrix o each sum of squares by rather than b d parametric bootstrap o gave percentile and b d would have been enough for most purposes but b d gave a sharper picture of the different curves. objective bayes intervals figure confidence densities for the maximum eigenvalue parameter using a multivariate normal model for the student score data. the dashed red curve is the percentile method solid black the bca a d the dotted blue curve is the bayes posterior density for using jeffreys prior bca confidence densities as shown. in this case the weights wb increased with o pushing the bca density to the right. also shown is the bayes posterior density for starting from jeffreys multiparameter prior density d ji where i is the fisher information matrix it isn t truly uninformative here moving its credible limits upward from the second-order accurate bca confidence limits. formula is discussed further in chapter bayesian data analysis has the attractive property that after examining the data we can express our remaining uncertainty in the language of probability. fiducial and confidence densities provide something similar for confidence intervals at least partially freeing the frequentist from the interpretive limitations of neyman s intervals. maximum eigenvalueposterior bootstrap confidence intervals notes and details fisher s theory of fiducial inference preceded neyman s approach formalized in which was presented as an attempt to put interval estimation on a firm probabilistic basis as opposed to the mysteries of fiducialism. the result was an elegant theory of exact and optimal intervals phrased in hard-edged frequentistic terms. readers familiar with the theory will know that neyman s construction a favorite name in the physics literature as pictured in figure requires some conditions on the famo ily of densities f to yield optimal intervals a sufficient condition being monotone likelihood ratios. bootstrap confidence intervals efron are neither exact nor optimal but aim instead for wide applicability combined with near-exact accuracy. second-order acuracy of bca intervals was established by hall bca is emphatically a child of the computer age routinely requiring b d or more bootstrap replications per use. shortcut methods are available. the abc method and efron needs only as much computation at the expense of requiring smoothness properties for d t and a less automatic coding of the exponential family setting for individual situations. in other words it is less convenient. z denote the central interval of density f neyman s construction. for any given value of let d o d and o f o be the indicator function for o if otherwise. o o o satisfying z f d o o d and let i i o has a two-point probability distribution by definition i d o d i probability probability o a pivotal statistic one whose distribution does not de this makes i pend upon neyman s construction takes the confidence interval c. to observed value o o dn w i o d o to be c o corresponding notes and details o has the desired coverage property n i o d o d then c. pr d pr o n c and o o o for any choice of the true parameter the normal theory correlation density of f are increasing functions of this makes our previous construction agree with the construction applies quite generally as long as we are able to define acceptance regions of the sample space having the desired target probability content for every choice of this can be challenging in multiparameter families. fisherian correctness. fisher arguing against the neyman paradigm pointed out that confidence intervals could be accurate without being correct having observed xi interval based on just the first observations would provide exact coverage while giving obviously incorrect inferences for if we can reduce the situation to form the percentile method intervals satisfy fisher s logic of inductive inference for correctness as at bootstrap sample sizes. why we need bootstrap sample sizes on the order of b d for confidence interval construction can be seen in the estimation of the bias correction value the delta-method standard error of d n for i d the standard is calculated to be this is with the standard normal density. with about equaling at b d a none-too-small error for use in the bc formula or the bca formula bca accuracy and correctness. the bca confidence limit o and b is transformation invariant. define c z. z d c so o o and d m. d m. d m og since isfies percentiles. therefore d og oh d oh og c z. d m.r the bootstrap cdf z g. for a monotone increasing transformation oh of for the bootstrap o og. f d m f d m o equals d m. verifying transformation invariance. that d o bootstrap confidence intervals oh and is also transformation invariant as is a as discussed pre viously. exact confidence intervals are transformation invariant adding considerably to their inferential appeal. for approximate intervals transformation invariance means that if we can demonstrate good behavior on any one n scale then it remains good on all scales. the model to the scale can be re-expressed as c a o d c c a.z where z is a standard normal variate z d c u taking logarithms n where d c a d c and u is the random variable c a.z represents the simplest kind of translation model where the unknown value of rigidly shifts the distribution of u the obvious confidence limit for d u where u is the percentile of u is then accurate and also correct according to fisher s vague logic of inductive inference. it is an algebraic exercise given in section of efron to reverse the transformations and recover o setting a d shows the accuracy and correctness of o the acceleration a. this a appears in as the rate of change of s standard deviation as a function of its expectation. in one-parameter exponential families it turns out that this is one-third of that is the transformation to normality d m. also decreases the instability of the standard deviation though not to zero. the variance of the score function p lx. determines the standard deviation of the mle o in one-parameter exponential families one-sixth the skewness of p lx. gives a. the skewness connection can be seen at work in estimate in multivariate exponential families the skewness must be evaluated in the least favorable direction discussed further in chapter the r algorithm accel web site to estimate a. the peruses b parametric bootstrap replications centile and bc intervals require only the replications o while bca also requires knowledge of the underlying exponential family. see sections and of efron o o notes and details d p tral chi-square variable with noncentrality parameter d p equation model makes o i a nonceni and n degrees of freedom written as o with o d and n d the parametric bootstrap distribution is r numerical evaluation gives efron concerns confidence intervals for parameters d t in model where third-order accurate confidence intervals can be calculated. the acceleration a equals zero for such problems making the bc intervals second-order accurate. in practice the bc intervals usually perform well and are a reasonable choice if the accleration a is unavailable. d leading to bca confidence density define og. i d z d so that z. d z c az c z. c z. z c az here we are thinking of and as functionally related by d o differentiation yields and d z z c az c az c d d d dz dz d og. which together give d verifying the name confidence density seems to appear first in efron though the idea is familiar in the fiducial literature. an ambitious frequentist theory of confidence distributions is developed in xie and singh jeffreys prior. formula is discussed further in chapter in the more general context of uninformative prior distributions. the theory of matching priors was initiated by welch and peers another important reference being tibshirani cross-validation and cp estimates of prediction error prediction has become a major branch of twenty-first-century commerce. questions of prediction arise naturally how credit-worthy is a loan applicant? is a new email message spam? how healthy is the kidney of a potential donor? two problems present themselves how to construct an effective prediction rule and how to estimate the accuracy of its predictions. in the language of chapter the first problem is more algorithmic the second more inferential. chapters on machine learning concern prediction rule construction. here we will focus on the second question having chosen a particular rule how do we estimate its predictive accuracy? two quite distinct approaches to prediction error assessment developed in the the first depending on the classical technique of crossvalidation was fully general and nonparametric. a narrower more efficient model-based approach was the second emerging in the form of mallows cp estimate and the akaike information criterion both theories will be discussed here beginning with cross-validation after a brief overview of prediction rules. prediction rules prediction problems typically begin with a training set d consisting of n pairs yi d d f.xi yi i d ng where xi is a vector of p predictors and yi a real-valued response. on the basis of the training set a prediction rule rd is constructed such that a prediction oy is produced for any point x in the predictor s sample space x oy d rd for x x d if if prediction rules the inferential task is to assess the accuracy of the rule s predictions. practice there are usually several competing rules under consideration and the main question is determining which is best. in the spam data of section xi comprised p d keyword counts while yi indicated whether or not message i was spam. the rule rd in table was an mle logistic regression fit. given a new message s count vector say rd provided an estimated probability of it being spam which could be converted into a prediction according to the diabetes data of table section involved the p d predictors x d sex glu obtained at baseline and a response y measuring disease progression one year later. given a new patient s baseline measurements we would like to predict his or her progression table suggests two possible prediction rules ordinary least squares and ridge regression using ridge parameter d either of which will produce a prediction in this case we might assess prediction error in terms of squared error in both of these examples rd was a regression estimator suggested by a probability model. one of the charms of prediction is that the rule rd need not be based on an explicit model. regression trees as pictured in figure are widely prediction algorithms that do not require model specifications. prediction perhaps because of its model-free nature is an area where algorithmic developments have run far ahead of their inferential justification. quantifying the prediction error of a rule rd requires specification of the discrepancy d.y oy between a prediction oy and the actual response y. the two most common choices are squared error d.y oy d and classification error d.y oy d if y oy if y d oy when as with the spam data the response y is dichotomous. of a dichotomous response is often called classification. random forests one of the most popular machine learning prediction algorithms is an elaboration of regression trees. see chapter cross-validation and cp estimates for the purpose of error estimation we suppose that the pairs yi in the training set d of have been obtained by random sampling from some probability distribution f on c space f for i d n yi the true error rate errd of rule rd is the expected discrepancy of d rd from given a new pair drawn from f independently of d errd d ef d rd is held fixed in expectation only varying. figure concerns the supernova data an example we will return to in the next section. absolute magnitudes yi have been measured for n d relatively nearby type ia supernovas with the data scaled such that i d yi is a reasonable model. for each supernova a vector xi of p d spectral energies has been observed n i d xi d xi table shows yi for i d frequency measurements have been standardized to have mean and variance while y has been adjusted to have mean on the basis of the training set d d f.xi yi i d we wish to construct a rule rd that given the frequency vector for a newly observed type ia supernova accurately its absolute magnitude to this end a lasso estimate q was fit with y in the vector and x the matrix having ith row xi was selected to minimize a cp estimate of prediction error section yielding prediction rule d x q in this case constructing rd itself involves error rate estimation. type ia supernovas were used as standard candles in the discovery of dark energy and the cosmological expansion of the universe on the grounds that they have constant absolute magnitude. this isn t exactly true. our training set is unusual in that the supernovas are close enough to earth to have y ascertained directly. this allows the construction of a prediction rule based on the frequency vector x which is observable for distant supernovas leading to improved calibration of the cosmological expansion. prediction rules figure the supernova data observed absolute magnitudes yi log scale plotted versus predictions oyi obtained from lasso rule for n d nearby type ia supernovas. predictions based on spectral power measurements of which had nonzero coefficients in q the plotted points in figure are oyi yi for i d n d these gave apparent error nx oyi d err d comparing this withp.yi d yields an impressive-looking n r squared value d d things aren t really that good cross-validation and cp methods allow us to correct apparent errors for the fact that rd was chosen to make the predictions oyi fit the data yi. prediction and estimation are close cousins but they are not twins. as discussed earlier prediction is less model-dependent which partly accounts for the distinctions made in section the prediction criterion err lllllllllllllllllllllllllllllllllllllll magnitude yiabsolute magnitude yiapparent mean squarederror cross-validation and cp estimates table supernova data frequency measurements and response variable absolute magnitude for the first of n d type ia supernovas. in terms of notation frequency measurements are x and magnitude y. mag is an expectation over the y space. this emphasizes good overall performance without much concern for behavior at individual points x in x shrinkage usually improves prediction. consider a bayesian model like that of section n a and n for i d n the bayes shrinkage estimator which is ideal for estimation d bxi b d a.a c is also ideal for prediction. suppose that in addition to the observations xi there are independent unobserved replicates one for each of the n xi values yi n for i d n that we wish to predict. the bayes predictor oyi d bxi has overall bayes prediction error nx oyi e n d b c which cannot be improved upon. the mle rule oyi d xi has bayes prediction error which is always worse than cross-validation as far as prediction is concerned it pays to overshrink as illustrated in figure for the james stein version of situation this is fine for prediction but less fine for estimation if we are concerned about extreme cases see table prediction rules sacrifice the extremes for the sake of the middle a particularly effective tactic in dichotomous situations where the cost of individual errors is bounded. the most successful machine learning prediction algorithms discussed in chapters carry out a version of local bayesian shrinkage in selected regions of x cross-validation having constructed a prediction rule rd on the basis of training set d we wish to know its prediction error err d ef for a new case obtained independently of d. a first guess is the apparent error nx err d n d.yi oyi the average discrepancy in the training set between yi and its prediction oyi d rd err usually underestimates err since rd has been to fit the observed responses yi. the ideal remedy discussed in section would be to have an inde pendent validation set test set dval of nval additional cases nval cross-validation attempts to mimiccerrval without the need for a validation set. define d to be the reduced training set in which pair yi has been omitted and let rd.i indicate the rule constructed on the basis err dp linear regression using ordinary least squares fitting provides a classical illustration i oyi p where p is i oyi must be increased top the degrees of freedom to obtain an unbiased estimate of the noise variance dval d j d nval nvalx d rd cerrval d this would provide an unbiased estimate of err cross-validation and cp estimates of d the cross-validation estimate of prediction error is oy.i d rd.i cerrcv d d.yi oy.i nx n now yi is not involved in the construction of the prediction rule for yi.cerrcv is the leave one out version of cross-validation. a more tions for the yi in group j thencerrcv is evaluated as in besides common tactic is to leave out several pairs at a time d is randomly partitioned into j groups of size about nj each d the training set with group j omitted provides rule rd.j which is used to provide predic reducing the number of rule constructions necessary from n to j grouping induces larger changes among the j training sets improving the predictive performance on rules rd that include discontinuities. argument here is similar to that for the jackknife section cross-validation was applied to the supernova data pictured in figure the cases were split randomly into j d groups of three cases each. this gave cerrcv d larger than err d the calculation now yields the smaller value d d we can apply cross-validation to the spam data of section having n d cases p d predictors and dichotomous response y. for this example each of the predictors was itself dichotomized to be either or depending on whether the original value xij equaled zero or not. a logistic regression section regressing yi on the dichotomized predictors gave apparent classification error i.e. wrong predictions among the cases. cross-validation with j d groups of size or each increased this to err d cerrcv d an increase of glmnet is an automatic model building program that among other things constructs a lasso sequence of logistic regression models adding cross-validation figure spam data. apparent error rate err and cross-validated estimate for a sequence of prediction rules generated by glmnet. the degrees of freedom are the number of nonzero regression coefficients df d corresponds to ordinary logistic regression which gave apparent err cross-validated rate the minimum cross-validated error rate is variables one at a time in their order of apparent predictive power see chapter the blue curve in figure tracks the apparent error err as a function of the number of predictors employed. aside from numerical artifacts err is monotonically decreasing declining to err d glmnet produced prediction error estimatescerrcv for each of the sucfor the full model that employs all predictors i.e. for the usual logistic regression model as in cessive models shown by the red curve. these are a little noisy themselves but settle down between and above the corresponding err estimates. the minimum value cerrcv d occurred for the model using predictors. the difference between and is too small to take seriously given the noise in the cerrcv estimates. there is a more subtle objection the choice of best prediction rule based on comparative cerrcv estimates is not itself cross-validated. each case yi is involved in choosing its of freedommisclassification error ratelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllogisticregression own best prediction socerrcv at the apparently optimum choice cannot be cross-validation and cp estimates taken entirely at face value. nevertheless perhaps the principal use of cross-validation lies in choosing among competing prediction rules. whether or not this is fully justified it is often the only game in town. that being said minimum predictive error no matter how effectuated is a notably weaker selection principle than minimum variance of estimation. as an example consider an iid normal sample xi having mean nx and median mx. both are unbiased for estimating but nx is much more efficient n i d var.mx var.nx e e d suppose we wish to predict a future observation independently selected from the same n distribution. in this case there is very little advantage to nx the noise in n dominates its prediction error. perhaps the proliferation of prediction algorithms to be seen in part iii reflects how weakly changes in strategy affect prediction error. table ratio of predictive errors for the mean of an independent sample of size from n nx and mx are the mean and median from xi n for i d ratio in this last example suppose that our task was to predict the average of further draws from the n distribution. table shows the ratio of predictive errors as a function of the superiority of the mean compared to the median reveals itself as gets larger. in this supersimplified example the difference between prediction and estimation lies in predicting the average of one versus an infinite number of future observations. doescerrcv actually estimate errd as defined in it seems like the answer must be yes but there is some doubt expressed in the literature for cross-validation reasons demonstrated in the following simulation we take the true distribution f in to be the discrete distribution of that puts weight on each of the yi pairs of the supernova a random sample with replacement of size from of gives simulated data set d and gives cerr based on the lassocp recipe used originally. the same diction rule rd cross-validation procedure as before applied to d cv. because this is a simulation we can also compute the actual mean-squared error applied to the true distribution of rate of rule rd d rd err d with cross-validation estimatecerr supernova data.cerr figure simulation experiment comparing true error err cv simulations based on the cv and err are negatively correlated. figure plots ble cerr cv for simulations using squared error discrepancy d.y oy d summary statistics are given in cv has performed well overall averaging quite near the true err both estimates being greater than the average apparent error however the figure shows something unsettling there is a simulation based on of is the same as nonparametric bootstrap analysis chapter cross-validation and cp estimates cv and apparent table true error err error simulations based on supernova data. correlation between err cross-validated errorcerr cerr andcerr cv. cv mean st dev negative correlation betweencerr cv and err large values ofcerr cv go with smaller values of the true prediction error and vice versa. our original definition of err errd d ef rd for rd ifcerr took rd fixed as constructed from d only f random. in other words errd was the expected prediction error for the specific rule rd as as it is all we can say is thatcerr we would expect to see a positive is err correlation in figure makes cross-validation a strongly frequentist devicecerrcv is estimating the cv is estimating the expected predictive error where d as well as is random in definition this average prediction error of the algorithm producing rd not of rd itself. cv is tracking err covariance penalties cross-validation does its work nonparametrically and without the need for probabilistic modeling. covariance penalty procedures require probability models but within their ambit they provide less noisy estimates of prediction error. some of the most prominent covariance penalty techniques will be examined here including mallows cp akaike s information criterion and stein s unbiased risk estimate the covariance penalty approach treats prediction error estimation in a regression framework the predictor vectors xi in the training set d d f.xi yi i d ng are considered fixed at their observed values not random as in an unknown vector of expectations d efyig has yielded the observed vector of responses y according to some given probability model which to begin with we assume to have the simple form covariance penalties y that is the yi are uncorrelated with yi having unknown mean and variance we take as known though in practice it must usually be estimated. a regression rule has been used to produce an estimate of vector d r.y y is included in the notation since the predictors xi are considered fixed and known. for instance we might take d r.y d x y x where x is the n p matrix having xi as the ith row as suggested by the linear regression model d x in covariance penalty calculations the estimator also functions as a predictor. we wonder how accurate d r.y will be in predicting a new vector of observations from model independent of y to begin with prediction error will be assessed in terms of squared discrepancy for component i where indicates expectation with random but held fixed. overall prediction error is the erri d nx erri d the apparent error for component i is n erri d a simple but powerful lemma underlies the theory of covariance penalties. lemma let e indicate expectation over both y in and in then eferrig d eferrig c cov. yi is sometimes called insample error as opposed to outsample error err though in practice the two tend to behave similarly. cross-validation and cp estimates where the last term is the covariance between the ith components of and y cov. yi d e f. i i c does not require ef d proof letting d yi and i d the elementary equality i d d c and likewise d taking expectations gives i becomes eferrig d cov. yi c e. while gives eferrig d c e. the middle term on the right side of equaling zero because of the independence of and taking the difference between and verifies the lemma. note the lemma remains valid if varies with i. the lemma says that on average the apparent error erri understimates the true prediction error erri by the covariance penalty cov. yi makes intuitive sense since yi measures the amount by which yi influences its own prediction covariance penalty estimates of prediction error take the formcerri d erri yi wheredcov. yi approximates yi overall prediction error d errc dcov. yi where err dp erri as before. the form ofdcov. yi in depends on the context assumed for is estimated by nx n the prediction problem. covariance penalties suppose that d r.y in is linear where c is a known n and m a known n n matrix. then the covariance matrix between and y is d c c my giving cov. yi d i mi i the ith diagonal element of m and since err dp cov. y d cerri d erri i i nx d c tr.m formula is mallows cp estimate of prediction error. for ols has tr.m d p the number of x n n estimation m d x nx d predictors so c n y yielded err dp.yi d the covariance penalty with for the supernova data the ols predictor d x n d d p d was giving cp estimate of prediction error x x n d c d for ols regression the degrees of freedom p the rank of matrix x in determines the covariance penalty in comparing this with leads to a general definition of degrees of freedom df for a regression rule d r.y df d nx dcov. yi this definition provides common ground for comparing different types of regression rules. rules with larger df are more flexible and tend toward better apparent fits to the data but require bigger covariance penalties for fair comparison. we are not counting the intercept as an predictor since y and all the xi were standardized to have mean all our models assuming zero intercept. cross-validation and cp estimates for lasso estimation and it can be shown that formula with p equaling the number of nonzero regression coefficients holds to a good approximation. the lasso rule used in figure for the supernova data had p d err was for this rule almost the same as for the ols rule above but the cp penalty is less d giving d c d compared with for ols. this estimate does not account for the databased selection of the choice p d see item below. if we are willing to add multivariate normality to model y we can drop the assumption of linearity in this case it can be shown that for any differentiable estimator d r.y the covariance in formula is given by cov. yi d times the partial derivative of with respect to yi. measure of yi s influence on its own prediction. the sure formula s unbiased risk estimator is with corresponding estimate for overall prediction error cerri d erri nx d errc sure was applied to the rule d for the kidney fitness data of figure the open circles in figure plot the component-wise degrees of freedom n by numerical differentiation versus agei. their sum i d n d nx d notice that the factor in cancels out in covariance penalties estimates the total degrees of freedom as in implying that is about as flexible as a sixth-degree polynomial fit with df figure analysis of the fit to kidney data of figure open circles are sure coordinate-wise df estimates plotted versus agei giving total degrees of freedom the solid curve tracks bootstrap coordinate-wise estimates with their sum giving total df d the parametric of section can be used to estimate the covariances cov. yi in the lemma the data vector y is assumed to be generated from a member of a given parametric family d f yielding d r.y parametric bootstrap replications of y and are obtained by analogy with y d r.y there is also a nonparametric bootstrap competitor to cross-validation the estimate see the chapter endnote estimatedegrees of freedomsure cross-validation and cp estimates a large number b of replications then yield bootstrap estimates f y bx dcov. yi d i b d r.y i i y i b d parametric bootstrap replications the dot notation indicating averages over the b replications. were obtained from the normal model taking in to be the estimate from as in figure a standard linear regression of y as a polynomial function of age gave d covariances were computed as in yielding coordinate-wise degrees of freedom estimates y dfi ddcov. yi the solid curve in figure plots dfi as a function of agei. these are seen to be similar to but less noisy than the sure estimates. they totaled nearly the same as the overall covariance penalty term in equaled by about over err d the advantage of parametric bootstrap estimates of covariance penalties is their applicability to any prediction rule d r.y no matter how exotic. applied to the lasso estimates for the supernova data b d replications yielded total df d for the rule that always used p d predictors compared with the theoretical approximation df d another replications now letting choose the apparently each time increased the df estimate to so the adaptive choice best p of p cost about extra degrees of freedom. these calculations exemplify modern computer-intensive inference carrying through error estimation for complicated adaptive prediction rules on a totally automatic basis. covariance penalties can apply to measures of prediction error other than squared error d.yi d we will discuss two examples of a general theory. first consider classification where yi equals or and it isn t necessary for the in to equal d r.y. the calculation was rerun taking in from with r.y still from with almost identical results. in general one might take in to be from a more flexible less biased estimator than r.y. d r.y covariance penalties similarly the predictor with dichotomous error if yi if yi d d.yi d as in in this situation the apparent error is the observed proportion of prediction mistakes in the training set err d now the true prediction error for case i is erri d the conditional probability given that an independent replicate of yi will be incorrectly predicted. the lemma holds as stated in leading to the prediction error estimate d n c n nx cov. yi some algebra yields cov. yi d d d prf d d with d prfyi d showing again the covariance penalty measuring the self-influence of yi on its own prediction. as a second example suppose that the observations yi are obtained from different members of a one-parameter exponential family d yi z fyi fy.y according to the apparent errorp d.yi is then for i d n d.y d nx and that error is measured by the deviance y d err d fy.y log f d y log n f n more generally is some predictor of prfyi d and is the indicator function i. cross-validation and cp estimates in this case the general theory gives overall covariance penalty nx penalty d n cov where o is the natural parameter in family corresponding to o d log for poisson observations. moreover if is obtained as the mle of in a generalized linear model with p degrees of freedom penalty n c constant to a good approximation. the corresponding version of can then be written the constant log.fy not depending on the term in brackets is the akaike information criterion if the statistician is comparing possible prediction rules r for a given data set y the aic says to select the rule maximizing the penalized maximum likelihood n p.j where is rule j s mle and p.j its degrees of freedom. comparison with the smallest value ofcerr.j with shows that for glms the aic amounts to selecting the rule is available then the error estimate cerrcv can be improved by bootstrap with the predictor vectors xi considered fixed as observed a parametric model generates the data set d d f.xi yi i d ng as estimatecerrcv in from which we calculate the prediction rule rd and the error cross-validation does not require a probability model but if such a model d rd substituting the estimated density f for as in provides perhaps better known as bagging see chapter training validation and ephemeral predictors parametric bootstrap replicates ofcerrcv rd bx cerr f d err d cv cv some large number b of replications can then be averaged to give the smoothed estimate b err averages out the considerable noise incerrcv often significantly reducleft after excess randomness is squeezed out ofcerrcv example of rao a surprising result referenced in the endnotes shows that err approximates the covariance penalty estimate speaking broadly is what s ing its blackwellization to use classical terminology. improvements can be quite substantial. covariance penalty estimates when believable parametric models are available should be preferred to cross-validation. training validation and ephemeral predictors good practice suggests splitting the full set of observed predictor response pairs y into a training set d of size n and a validation set dval of size nval the validation set is put into a vault while the training set is used to develop an effective prediction rule rd finally dval is removed from the vault and used to calculatecerrval an honest estimate of the predictive error rate of rd. this is a good idea and seems foolproof at least if one has enough data to afford setting aside a substantial portion for a validation set during the training process. nevertheless there remains some peril of underestimating the true error rate arising from ephemeral predictors those whose predictive powers fade away over time. a contrived but not completely fanciful example illustrates the danger. the example takes the form of an imaginary microarray study involving subjects patients and healthy controls coded i d yi d patient control a related tactic pertaining to grouped cross-validation is to repeat calculation for several different randomly selected splits into j groups and then average the resulting cerrcv estimates. cross-validation and cp estimates each subject is assessed on a microarray measuring the genetic activity of p d genes these being the predictors xi d xi xi one subject per day is assessed alternating patients and controls. figure orange bars indicate transient episodes and the reverse for imaginary medical study xij the measurements xij are independent of each other and of the yi s for i d and j d most of the equal zero but each gene s measurements can experience transient episodes of two possible types in type n if yi d if yi d d while type reverses signs. the episodes are about days long randomly and independently located between days and with an average of two episodes per gene. the orange bars in figure indicate the episodes. for the purpose of future diagnoses we wish to construct a prediction rule oy d rd to this end we randomly divide the subjects into a ephemeral predictors training set d of size n d and a validation set dval of size nval d the popular machine learning prediction program random forests chapter is applied. random forests forms rd by averaging the predictions of a large number of randomly subsampled regression trees figure test error and cross-validated training error for random forest prediction rules using the imaginary medical study top panel training set randomly selected days test set the remaining days. bottom panel training set the first days test set the last days. the top panel of figure shows the results with blue points indicating test-set error and black the training-set error. both converge to as the number of random forest trees grows large. this seems to confirm an success rate for prediction rule rd one change has been made for the bottom panel now the training set is the data for days through and the test set days through set random days test set the remainder treesprediction days test days treesprediction cross-validation and cp estimates cerrval is now nearly double. the cross-validated training-set prediction error still converges to but the reason isn t hard to see. any predictive power must come from the transient episodes which lose efficacy outside of their limited span. in the first example the test days are located among the training days and inherit their predictive accuracy from them. this mostly fails in the second setup where the test days are farther removed from the training days. the orange bars crossing the line can help lower cerrval in this situaan obvious but often ignored dictum is thatcerrval is more believable if tion. the test set is further separated from the training set. further has a clear meaning in studies with a time or location factor but not necessarily in general. for j cross-validation separation is improved by removing contiguous blocks of nj cases for each group rather than by random selection but the amount of separation is still limited making cerrcv less believable than a suitably constructedcerrval. the distinction between transient ephemeral predictors and dependable ones is sometimes phrased as the difference between correlation and causation. for prediction purposes if not for scientific exegesis we may be happy to settle for correlations as long as they are persistent enough for our purposes. we return to this question in chapter in the discussion of large-scale hypothesis testing. a notorious cautionary tale of fading correlations concerns google flu trends a machine-learning algorithm for predicting influenza outbreaks. introduced in the algorithm based on counts of internet search terms outperformed traditional medical surveys in terms of speed and predictive accuracy. four years later however the algorithm failed badly overestimating what turned out to be a nonexistent flu epidemic. perhaps one lesson here is that the google algorithmists needed a validation set years not weeks or months removed from the training data. error rate estimation is mainly frequentist in nature but the very large data sets available from the internet have encouraged a disregard for inferential justification of any type. this can be dangerous. the heterogeneous nature of found data makes statistical principles of analysis more not less relevant. notes and details the evolution of prediction algorithms and their error estimates nicely illustrates the influence of electronic computation on statistical theory and notes and details practice. the classical recipe for cross-validation recommended splitting the full data set in two doing variable selection model choice and data fitting on the first half and then testing the resulting procedure on the second half. interest revived in with the independent publication of papers by geisser and by stone featuring leave-one-out cross-validation of predictive error rates. a question of bias versus variance arises here. a rule based on only cases is less accurate than the actual rule based on all n leave-one-out cross-validation minimizes this type of bias at the expense of increased variability of error rate estimates for jumpy rules of a discontinuous nature. current best practice is described in section of hastie et al. where j cross-validation with j perhaps is recommended possibly averaged over several random data splits. nineteen seventy-three was another good year for error estimation featuring mallows cp estimator and akaike s information criterion. efron extended cp methods to a general class of situations below established the connection with aic and suggested bootstrapping methods for covariance penalties. the connection between cross-validation and covariance penalties was examined in efron where the rao blackwelltype relationship mentioned at the end of section was demonstrated. the sure criterion appeared in charles stein s paper. ye suggested the general degrees of freedom definition standard candles and dark energy. adam riess saul perlmutter and brian schmidt won the nobel prize in physics for discovering increasing rates of expansion of the universe attributed to an einsteinian concept of dark energy. they measured cosmic distances using type ia supernovas as standard candles. the type of analysis suggested by figure is intended to improve the cosmological distance scale. data-based choice of a lasso estimate. the regularization parameter for a lasso estimator controls the number of nonzero coefficients of q with larger yielding fewer nonzeros. efron et al. cients. substituting this for p in provides a quick version of and zou et al. showed that a good approximation for the degrees of freedom df of a lasso estimate is the number of its nonzero coeffithis was minimized at df d for the supernova example in figure stein s unbiased risk estimate. the covariance formula is obtained directly from integration by parts. the computation is clear from the one-dimensional version of n d cross-validation and cp estimates cov. y dz z d d e dy dy e d r broad regularity conditions for sure are given in stein the rule. bootstrap competitors to cross-validation are discussed in efron and efron and tibshirani the most successful of these the rule is generally less variable than leave-one-out cross-validation. we suppose that nonparametric bootstrap data sets d b d b have been formed each by sampling with replacement produces n times from the original n members of d data set d rule d rd giving predictions r y let i b i d if pair yi is not in d d i will equal the remaining equaling the and if it is. e of the n b i b out of bootstrap estimate of prediction error is i cerrout d nx bx i b i d nx bx yi oy i i b i the average discrepancy in the omitted cases. of the cases each time. the rule compensates for the upward bias cerrout is similar to a grouped cross-validation estimate that omits about incerrout by incorporating the downwardly biased apparent error cerrout has resurfaced in the popular random forests prediction algorithm d c err chapter where a closely related procedure gives the out of bag estimate of err. google flu trends. harford s article big data a big mistake? concerns the enormous found data sets available in the internet age and the dangers of forgetting the principles of statistical inference in their analysis. google flu trends is his primary cautionary example. objective bayes inference and markov chain monte carlo from its very beginnings bayesian inference exerted a powerful influence on statistical thinking. the notion of a single coherent methodology employing only the rules of probability to go from assumption to conclusion was and is immensely attractive. for years however two impediments stood between bayesian theory s philosophical attraction and its practical application. in the absence of relevant past experience the choice of a prior distribution introduces an unwanted subjective element into scientific inference. bayes rule looks simple enough but carrying out the numerical calculation of a posterior distribution often involves intricate higherdimensional integrals. the two impediments fit neatly into the dichotomy of chapter the first being inferential and the second a renewed cycle of bayesian enthusiasm took hold in the at first concerned mainly with coherent inference. building on work by bruno de finetti and l. j. savage a principled theory of subjective probability was constructed the bayesian statistician by the careful elicitation of prior knowledge utility and belief arrives at the correct subjective prior distribution for the problem at hand. subjective bayesianism is particularly appropriate for individual decision making say for the business executive trying to choose the best investment in the face of uncertain information. it is less appropriate for scientific inference where the sometimes skeptical world of science puts a premium on objectivity. an answer came from the school of objective bayes inference. following the approach of laplace and jeffreys as discussed in section their goal was to fashion objective or uninformative prior distributions that in some sense were unbiased in their effects upon the data analysis. the exponential family material in this chapter provides technical support but is not required in detail for a general understanding of the main ideas. objective bayes inference and mcmc in what came as a surprise to the bayes community the objective school has been the most successful in bringing bayesian ideas to bear on scientific data analysis. of the articles in the december issue of the annals of applied statistics employed bayesian analysis predominantly based on objective priors. this is where electronic computation enters the story. commencing in the dramatic steps forward were made in the numerical calculation of high-dimensional bayes posterior distributions. markov chain monte carlo is the generic name for modern posterior computation algorithms. these proved particularly well suited for certain forms of objective bayes prior distributions. taken together objective priors and mcmc computations provide an attractive package for the statistician faced with a complicated data analysis situation. statistical inference becomes almost automatic at least compared with the rigors of frequentist analysis. this chapter discusses both parts of the package the choice of prior and the subsequent computational methods. criticisms arise both from the frequentist viewpoint and that of informative bayesian analysis which are brought up here and also in chapter objective prior distributions a flat or uniform distribution over the space of possible parameter values seems like the obvious choice for an uninformative prior distribution and has been so ever since laplace s advocacy in the late eighteenth century. for a finite parameter space say d flat has the obvious meaning d k for all if k is infinite or if is continuous we can still take d constant bayes rule gives the same posterior distribution for any choice of the constant d with f dz objective prior distributions notice that cancels out of the fact that is improper that is it integrates to infinity doesn t affect the formal use of bayes rule in as long as f is finite. notice also that amounts to taking the posterior density of to be proportional to the likelihood function d x fixed and varying over this brings us close to fisherian inference with its emphasis on the direct interpretation of likelihoods but fisher was adamant in his insistance that likelihood was not probability. figure the solid curve is flat-prior posterior density having observed x d from poisson model x it is shifted about units right from the confidence density of figure jeffreys prior gives a posterior density nearly the same as the confidence density. the solid curve in figure shows for the poisson situation of table x with x d observed is shifted almost exactly units right of the confidence density from figure is itself in this fisher s withering criticism of flat-prior bayes inference focused on its the reader may wish to review chapter particularly section for these constructions. densityflat objective bayes inference and mcmc lack of transformation invariance. if we were interested in d rather than gflat.jx would not be the transformation to the log scale of jeffreys prior or which does transform correctly is for x d is then a close match to the confidence density in figure d p coverage matching priors a variety of improvements and variations on jeffreys prior have been suggested for use as general-purpose uninformative prior distributions as briefly discussed in the chapter endnotes. all share the drawback seen in figure the posterior distribution can have unintended effects on the resulting inferences for a real-valued parameter of interest d t this is unavoidable it is mathematically impossible for any single prior to be uninformative for every choice of d t the label uninformative for a prior sometimes means gives bayes posterior intervals that closely match confidence intervals. perhaps surprisingly this definition has considerable resonance in the bayes community. such priors can be constructed for any given scalar parameter of interest d t for instance the maximum eigenvalue parameter of figure in brief the construction proceeds as follows. the p-dimensional parameter vector is transformed to a form that makes the first coordinate say where is a nuisance parameter. the transformation is chosen so that the fisher information matrix for has the diagonal form i i is always possible. finally the prior for is taken proportional to where is an arbitrary density. in other words i g. d conjugate prior distributions g. combines the one-dimensional jeffreys prior for with an arbitrary independent prior for the orthogonal nuisance parameter vector the main thing to notice about is that g. represents different priors on the original parameter vector for different functions d t no single prior can be uninformative for all choices of the parameter of interest calculating g. can be difficult. one alternative is to go directly to the bca confidence density which can be interpreted as the posterior distribution from an uninformative prior its integrals agree closely with confidence interval endpoints. coverage matching priors are not much used in practice and in fact none of the eight annals of applied statistics objective bayes papers mentioned earlier were of type a form of almost uninformative priors the conjugates is more popular mainly because of the simpler computation of their posterior distributions. conjugate prior distributions a mathematically convenient class of prior distributions the conjugate priors applies to samples from an exponential section d e here we have indexed the family with the expectation parameter d ef fxg rather than the canonical parameter on the right-hand side of can be thought of as a one-to-one function of so-called link function e.g. d for the poisson family. the observed data is a random sample x d xn from xn having density function the average nx dp xi being sufficient. d en we will concentrate on one-parameter families though the theory extends to the multiparameter case. figure relates to a two-parameter situation. objective bayes inference and mcmc the family of conjugate priors for allows the statistician to choose two parameters and d v v the variance of an x from v d varf fxgi c is the constant that makes integrate to with respect to lebesgue measure on the interval of possible values. the interpretation is that represents the average of hypothetical prior observations from the utility of conjugate priors is seen in the following theorem. theorem define nc d c n and nx then the posterior density of given x d xn is nc c n nc nxc d d gncnxc moreover the posterior expectation of given x is nx d nc c n nc the intuitive interpretation is quite satisfying we begin with a hypothetical prior sample of size sufficient statistic observe x a sample of size n and update our prior distribution to a distribution gncnxc of the same form. moreover equals the average of a hypothetical sample with copies of xn that is we have n i.i.d. observations from a poisson distribution table formula gives conjugate prior as an example suppose xi d c not depending on so in the notation of table is a gamma distribution the posterior distribution is d gncnxc gam.nc nxc nc gnc nxc conjugate prior distributions where indicates a standard gamma distribution d table conjugate priors for four familiar one-parameter exponential families using notation in table the last column shows the posterior distribution of given n observations xi starting from prior in line is the standard gamma distribution with the same as gamma parameter in table the chapter endnotes give the density of the inverse gamma distribution and corresponding results for chi-squared variates. name normal xi distribution n known poisson binomial gamma known n gam.nc nxc be.nc nxc nxc n nc table describes the conjugate prior and posterior distributions for four familiar one-parameter families. the binomial case where is the success probability in table is particularly evocative indepeni xi d nnx successes. prior amounts to assuming proportion d prior successes in flips. formula becomes dent coin flips xn give say s dp d c s c n for the posterior expectation of the choice d for instance gives bayesian estimate for pulling the mle sn a little bit toward the size of the number of hypothetical prior observations determines how informative or uninformative the prior is. recent objective bayes literature has favored choosing small d being popular. the hope here is to employ a proper prior that has a finite integral while still not injecting much unwarranted information into the analysis. the choice of is also by convention. one possibility is to set objective bayes inference and mcmc d nx in which case the posterior expectation equals the mle nx. another possibility is choosing equal to a null value for instance d for effect size estimation in table vasoconstriction data volume of air inspired in cases without vasoconstriction d and with vasoconstriction d y d y d as a miniature example of objective bayes inference we consider the vasoconstriction data of table n d measurements of lung volume have been obtained without vasoconstriction d and with d here we will think of the yi as binomial variates following logistic regression model yi i d d c log with the xi as fixed covariates values in table letting xi d xi nential family results in a two-parameter expo the mle o has approximate covariance matrix ov as given in i o h f d en nx yi xi yi having o d n nx and d c e xi nx n conjugate prior distributions in figure the posterior distributions are graphed in terms of rather than or making the contours of equal density roughly circular and centered at zero. d ov o figure vasoconstriction data contours of equal posterior density of from four uninformative priors as described in the text. numbers indicate probability content within contours light dashed contours from panel a flat prior. panel a of figure illustrates the flat prior posterior density of given the data y in model the heavy lines are contours of equal density with the one labeled containing of the posterior probability etc. panel b shows the corresponding posterior density a. flat jeffreys lc. conjugate ld. bootstrap objective bayes inference and mcmc contours obtained from jeffreys multiparameter prior in this case gjeff. d jv v the covariance matrix of o as calculated from for comparison purposes the light dashed curves show some of the flat prior contours from panel a. the effect of gjeff. is to reduce the flat prior bulge toward the upper left corner. panel c relates to the conjugate besides reducing the flat prior bulge pulls the contours slightly downward. with o replacing gave resamples y the contours of toward the left. panel d shows the parametric bootstrap distribution model and mle replications o o considerably accentuate the bulge d ov figure posterior densities for first coordinate of in for the vasoconstriction data. dashed red curve raw distribution of b d parametric replications from model solid black curve bca density d a d dotted blue curve posterior density using jeffreys multiparameter prior the role of nx in is taken by o in so has o d d this makes d the factor v in is absent in the conjugate prior for opposed to bootstrapjeffreys model selection and the bayesian information criterion this doesn t necessarily imply that a bootstrap analysis would give much different answers than the three similar objective bayes results. for any particular real-valued parameter of interest the raw bootstrap distribution weight on each replication would be reweighted according to the bca formula in order to produce accurate confidence intervals. figure compares the raw bootstrap distribution the bca confidence density and the posterior density obtained from jeffreys prior for equal to the first coordinate of in the bca density is shifted to the right of jeffreys critique of objective bayes inference despite its simplicity or perhaps because of it objective bayes procedures are vulnerable to criticism from both ends of the statistical spectrum. from the subjectivist point of view objective bayes is only partially bayesian it employs bayes theorem but without doing the hard work of determining a convincing prior distribution. this introduces frequentist elements into its practice clearly so in the case of jeffreys prior along with frequentist incoherencies. for the frequentist objective bayes analysis can seem dangerously untethered from the usual standards of accuracy having only tenuous largesample claims to legitimacy. this is more than a theoretical objection. the practical advantages claimed for bayesian methods depend crucially on the fine structure of the prior. can we safely ignore stopping rules or selective inference choosing the largest of many estimated parameters for special attention for a prior not based on some form of genuine experience? in an era of large complicated and difficult data-analytic problems objective bayes methods are answering a felt need for relatively straightforward paths to solution. granting their usefulness it is still reasonable to hope for better or at least for more careful comparisons with competing methods as in figure model selection and the bayesian information criterion data-based model selection has become a major theme of modern statistical inference. in the problem s simplest form the statistician observes data x and wishes to choose between a smaller model and a larger model chapter discusses the frequentist assessment of bayes and objective bayes estimates. objective bayes inference and mcmc the classic textbook example takes x d xn as an inde pendent normal sample xi n for i d n with the null hypothesis d and the general two-sided alter native w d w can include d in with no effect on what follows. from a frequentist viewpoint choosing between and in amounts to running a hypothesis test of w d perhaps augmented with a confidence interval for bayesian model selection aims for more an evaluation of the posterior probabilities of and given x. a full bayesian specification requires prior probabilities for the two models and conditional prior densities for within each model and d density for x say dz and dz let be the density of x given each model induces a marginal bayes theorem in its ratio form then gives posterior probabilities d prf and d prf and d d prf d prf d satisfying d b.x where b.x is the bayes factor b.x d leading to the elegant statement that the posterior odds ratio is the prior odds ratio times the bayes factor. all of this is of more theoretical than applied use. prior specifications are usually unavailable in practical settings is why model selection and the bic standard hypothesis testing is so popular. the objective bayes school has concentrated on estimating the bayes factor b.x with the understanding that the prior odds ratio in would be roughly evaluated depending on the specific circumstances perhaps set to the laplace choice d table jeffreys scale of evidence for the interpretation of bayes factors. bayes factor evidence for negative barely worthwhile positive strong very strong jeffreys suggested a scale of evidence for interpreting bayes factors re produced in table b.x d for instance constitutes positive but not strong evidence in favor of the bigger model. jeffreys scale is a bayesian version of fisher s interpretive scale for the outcome of a hypothetic test with coverage value minus the significance level famously constituting significant evidence against the null hypothesis. table shows fisher s scale as commonly interpreted in the biomedical and social sciences. table fisher s scale of evidence against null hypothesis and in favor of as a function of coverage level minus the p-value. coverage evidence for null borderline moderate substantial strong very strong overwhelming even if we accept the reduction of model selection to assessing the bayes factor b.x in and even if we accept jeffreys scale of interpretation this still leaves a crucial question how to compute b.x in objective bayes inference and mcmc practice without requiring informative choices of the priors and in a popular objective bayes answer is provided by the bayesian informa tion criterion for a given model m we define log.n bic.m d log f p where is the mle p the degrees of freedom of free parameters in m and n the sample size. then the bic approximation to bayes factor b.x is log bbic.x d d log f w d log f the subscripts indexing the mles and degrees of freedom in and this can be restated in somewhat more familiar terms. letting w be wilks likelihood ratio statistic log.n we have log bbic.x d fw d log.ng with d d w approximately follows a d distribution under model d implying bbic.x will tend to be less than one favoring if it is true ever more strongly as n increases. we can apply bic selection to the vasoconstriction data of table taking to be model and to be the submodel having d in this case d d in direct calculation gives w d and positive but not strong evidence against according to jeffreys scale. by comparison the usual frequentist z-value for testing d is coverage level between substantial and strong evidence against on fisher s scale. the bic was named in reference to akaike s information criterion bbic d aic.m d log f p which suggests as in basing model selection on the sign of d fw model selection and the bic the bic penalty d log.n in grows more severe than the aic penalty as n gets larger increasingly favoring selection of rather than the distinction is rooted in bayesian notions of coherent behavior as discussed in what follows. where does the bic penalty term d log.n in come from? a first answer uses the simple normal model xi n has prior d equal a delta function at zero. suppose we take d in to be the gaussian conjugate prior the discussion following in section suggests setting m d and a d corresponding to prior information equivalent to one of the n actual observations. in this case we can calculate the actual bayes factor b.x n a n w log.n c log b.x d n c nearly equaling log bbic.x d for large n. justifications of the bic formula as an approximate bayes factor follow generalizations of this kind of argument as discussed in the chapter endnotes. z n under the difference between bic and frequentist hypothesis testing grows more drastic for large n. suppose is a regression model and is augmented with one additional covariate d d let z be a standard z-value for testing the hypothesis that is no improvement over table shows bbic.x as a function of z and n. at n d fisher s and jeffreys scales give roughly similar assessments of the evidence against jeffreys nomenclature is more conservative. at the other end of the table at n d the inferences are contradictory z d with p-value and coverage level is overwhelming evidence for on fisher s scale but barely worthwhile for jeffreys bayesian coherency the axiom that inferences should be consistent over related situations lies behind the contradiction. suppose n d in the simple normal model that is we observe only the single variable and wish to decide between w d and w let denote our prior density for this situation. x n objective bayes inference and mcmc table bic bayes factors corresponding to z-values for testing one additional covariate coverage value minus the significance level of a two-sided hypothesis test as interpreted by fisher s scale of evidence right. jeffreys scale of evidence table is in rough agreement with fisher for n d but favors the null much more strongly for larger sample sizes. n cover z-value fisher null borderline moderate substantial strong very strong overwhelming n.p xi and d p p the case n in is logically identical to letting x.n d gives x.n n p p with becoming w d and w coherency p requires that in have the same prior as in since d the prior for sample size n satisfies n this implies that g.n d g.n this being sample size coherency. farther away from the null value d at rate stays fixed. for any fixed value of the sufficient statistic x.n being p z in table this results in the bayes factor b.x.n decreasing at rate n the frequentistbayesian contradiction seen in table goes beyond the specifics of the bic algorithm. the effect of is to spread the prior density g.n p n while the prior g.n n a general information criterion takes the form gic.m d log f p cn model selection and the bic where cn is any sequence of positive numbers cn d for bic and cn d for aic the difference d d d will be positive if w for d d as in table will favor if w with approximate probability if is actually true this equals for the aic choice cn d for bic n d it equals the choice cn d agreeing with the usual frequentist makes prf rejection level. the bic is consistent prf goes to zero as n if is true. this isn t true of for instance where we will have prf no matter how large n may be but consistency is seldom compelling as a practical argument. confidence intervals help compensate for possible frequentist overfitting. with z d and n d the confidence interval for in model is whether or not such a small effect is interesting depends on the scientific context. the fact that bic says not interesting speaks to its inherent small-model bias. the prostate cancer study data of section provides a more challenging model selection problem. figure shows the histogram of n d observations xi each measuring the effects of one gene. the histogram has bins each of width with centers cj ranging from to yj the height of the histogram at cj is the number of xi in bin j yj d bin jg for j d we assume that the yj follow a poisson regression model as in sec tion yj j d and wish to fit a log polynomial glm model to the the model selection question is what degree polynomial? degree corresponds to normal densities but the long tails seen in figure suggest otherwise. models of degree through are assessed in figure four model selection measures are compared aic bic with n d objective bayes inference and mcmc figure log polynomial models of degree through applied to the prostate study histogram of figure model selection criteria aic bic with n d number of bins or number of genes gic using classic fisher hypothesis choice cn d all four selected the fourth-degree model as best. the number of yj values and also n d the number of genes and gic with cn d the choice based on classic fisherian hypothesis testing. is almost the same as bic n d since d a fourth-degree polynomial model was the winner under all four criteria. the untethered criticism made against objective bayes methods in general is particularly applicable to bic. the concept of sample size is not well defined as the prostate study example shows. sample size coherency the rationale for bic s strong bias toward smaller models is less convincing in the absence of priors based on genuine experience if there is no prospect of the sample size changing. whatever its vulnerabilities bic model selection has nevertheless become a mainstay of objective bayes model selection not least because of its freedom from the choice of bayesian priors. polynomial degreeaic and bicbic and gicaic gibbs sampling and mcmc gibbs sampling and mcmc miraculously blessed with visions of the future a bayesian statistician of the would certainly be pleased with the prevalence of bayes methodology in twenty-first-century applications. but his pleasure might be tinged with surprise that the applications were mostly of the objective uninformative type rather than taken from the elegant de finetti savage school of subjective inference. the increase in bayesian applications and the change in emphasis from subjective to objective had more to do with computation than philosophy. better computers and algorithms facilitated the calculation of formerly intractable bayes posterior distributions. technology determines practice and the powerful new algorithms encouraged bayesian analyses of large and complicated models where subjective priors those based on actual past experience were hard to come by. add in the fact that the algorithms worked most easily with simple convenience priors like the conjugates of section and the stage was set for an objective bayes renaissance. at first glance it s hard to see why bayesian computations should be daunting. from parameter vector data x density function f and prior density g. bayes rule directly produces the posterior density g.jx d g.f where f is the marginal density f dz the posterior probability of any set a in the parameter space is then pfajxg dz g.f d g.f d g.f d a this is easy to write down but usually difficult to evaluate if is multidimensional. modern bayes methods attack the problem through the application of computer power. even if we can t integrate g.jx perhaps we can sample from it. if so a sufficiently large sample say would provide estimates g.jx n o. a opfajxg d b objective bayes inference and mcmc and similarly for posterior moments correlations etc. we would in this way be employing the same general tactic as the bootstrap applied now for bayesian rather than frequentist purposes toward the same goal as the bootstrap of freeing practical applications from the constraints of mathematical tractability. the two most popular computational gibbs sampling and markov chain monte carlo are based on markov chain algorithms that is the posterior samples are produced in sequence each one depending only on and not on its more distant predecessors. we begin with gibbs sampling. the central idea of gibbs sampling is to reduce the generation of multidimensional vectors d to a series of univariate calculations. let denote with component k removed and g.k the conditional density of given and the data x x g.k x the algorithm begins at some arbitrary initial value having computed the components of are generated according to conditional distributions g.k for k d k x k as an example we take x to be the n d observations for y d in the vasoconstriction data of table and assume that these are a normal sample xi the sufficient statistics for estimating the bivariate parameter d are the sample mean and variance n i d n d nx d nx xi and t d nx having independent normal and gamma distributions nx n and t with d the latter being in the notation of table the two methods are often referred to collectively as mcmc because of mathematical connections with metropolis-hasting algorithm referring to the second type of procedure. gibbs sampling and mcmc for our bayes prior distribution we take the conjugates n d in terms of table d for the gamma while for the normal. simple specification would take n multiplying the normal and gamma functional forms in table yields and density function t d exp and prior density d exp h c n h c c indicating positive constants that do not affect the posterior computations. the posterior density t is then calculated to be t d where q d c t c nc c here nc d c n and d c nnxnc. in order to make use of gibbs sampling we need to know the full conditional distributions nx t and nx t as in this case k d d and d this is where the conjugate expressions in table come into play. inspection of density shows that n nc and nx t q nx t b d gibbs samples d were generated starting from d t d the prior specifications were chosen to be uninformative or mildly informative d d nx d or and d t which case d nx and q d c c from d we see that corresponds to about hypothetical prior observations. the resulting posterior distributions for are shown by the histograms in figure as a point of frequentist comparison b d parametric bootstrap replications involve no prior assumptions d t objective bayes inference and mcmc figure posterior distributions for variance parameter model volume of air inspired for vasoconstriction group y d from table solid teal histogram b d gibbs samples with d black line histogram b d samples with d red line histogram parametric bootstrap samples suggests even the d prior has substantial posterior effect. are seen to be noticeably more dispersed than even the d bayes posterior distribution the likely choice for an objective bayes analysis. bayes techniques even objective ones have regularization effects that may or may not be appropriate. a similar independent gibbs sample of size was obtained for the y d vasoconstriction measurements in table with specifications as in k d let c d where y d and y d runs. and denote the bth gibbs samples from the figure shows the posterior distribution of twenty-eight of the gibbs sampling and mcmc figure b gibbs samples for bayes t-statistic comparing y d with y d values for vasoconstriction data. b values were less than giving a bayesian t-test estimate pf d usual t-test yielded one-sided p-value against the null hypothesis d an appealing feature of gibbs sampling is that having obtained the posterior distribution of any parameter d t is obtained directly from the b values d t gibbs sampling requires the ability to sample from the full conditional distributions a more general markov chain monte carlo method commonly referred to as mcmc makes clearer the basic idea. suppose the space of possible values is finite say and we wish to simulate samples from a posterior distribution putting probability p.i on p d p.m the mcmc algorithm begins with the choice of a candidate probability distribution q.i j for moving from to in theory q.i j can be almost anything for instance q.i j d for j i. the simulated samples are obtained by a random walk if equals objective bayes inference and mcmc then equals with q.i j d q.i j min p.j i p.i j for j i while with probability q.i i d j i q.i j d d markov chain theory then says that under quite general conditions the empirical distribution of the random walk values will approach the desired distribution p as b gets large. a heuristic argument for why this happens begins by supposing that was in fact generated by sampling from the target distribution p prf d ig d p.i and then was obtained according to transition probabilities a little algebra shows that implies the so-called balance equations. this results in n d i pr p.i j d p.j i o d p.i i cx p.j i j i d p.i q.i j d p.i mx in other words if has distribution p then so will and likewise p is the equilibrium distribution of the markov chain random walk defined by transition probabilities q. under reasonable conditions must asymptotically attain distribution p no matter how is initially selected. example modeling population admixture mcmc has had a big impact in statistical genetics where bayesian modeling is popular and useful for representing the complex evolutionary processes. here we illustrate its use in demography and modeling admixture estimating the contributions from ancestral populations in an individual in bayes applications p.i d g..i d g..i however f is not needed since it cancels out of a considerable advantage in complicated situations when f is often unavailable and a prime reason for the popularity of mcmc. example modeling population admixture genome. for example we might consider human ancestry and for each individual wish to estimate the proportion of their genome coming from european african and asian origins. the procedure we describe here is unsupervised a type of soft clustering but we will see it can be very informative with regard to such questions. we have a sample of n individuals and we assume each arose from possible admixture among j parent populations each with their own characteristic vector of allele frequencies. for us j d and let qi denote a probability vector for individual i representing the proportions of their heritage coming from populations j section we have genomic measurements for each individual in our case snps polymorphisms at each of m well-spaced loci and hence can assume they are in linkage equilibrium. at each snp we have a measurement that identifies the two alleles per chromosome where each can be either the wild-type a or the mutation a. that is we have the genotype gi m at snp m for individual i a three-level factor with levels faa aa aag which we code as table shows some examples. table a subset of the genotype data on individuals each with genotype measurements at snps. in this case the ethnicity is known for each individual one of japanese african european or african american. for example individual has genotype aa for has aa and has aa. subject let pj be the m of minor allele frequencies actually in population j we have available a sample of n individuals and for each sample we have their genomic information measured at each of the m loci. some of the individuals might appear to have pure ancestral origins but many do not. our goal is to estimate qi i d n and pj j objective bayes inference and mcmc i m x for this purpose it is useful to pose a generative model. we first create a pair of variables xi m d i m corresponding to each gi m to which we allocate the two alleles arbitrary order. for example if gi m d d to aa then we might set x vice versa. if gi m d they are both and if gi m d they are both i m let zi m represent the ancestral origin for individual i of each of these allele copies xi m at locus m again a two-vector with elements zi m d d and x i m i m then our generative model goes as follows. i m qi independently at each m for each copy c d that is we select the ancestral origin of each chromosome at locus m according to the individual s mixture proportions qi. d j for each copy c d what this means is that for each of the two ancestral picks at locus m for each arm of the chromosome we draw a binomial with the appropriate allele frequency. pj m if z.c i m z.c i m x i m to complete the bayesian specification we need to supply priors for the qi and also for pj m. although one can get fancy here we resort to the recommended flat priors which are qi a flat three-component dirichlet independently for each subject i and pj m independently for each population j and each locus m beta distribution see in the end notes. we use the least-informative values d d in practice these could get updated as well but for the purposes of this demonstration we leave them fixed at these values. let x be the n m array of observed alleles for all n samples. we wish to estimate the posterior distribution pr.p qjx referring collectively to all the elements of p and q. for this purpose we use gibbs sampling which amounts to the following sequence. initialize p sample z.b from the conditional distribution pr.zjx p sample p q.b from the conditional distribution pr.p qjx z.b. gibbs is effective when one can sample efficiently from these conditional distributions which is the case here. example modeling population admixture in step we can sample p and q separately. it can be seen that for each m we should sample pj m from pj mjx z c j m c j m d c w x d c w x i m i m d and z.c d and z.c i m i m d jg d jg where z d z.b and j m j m this follows from the conjugacy of the two-component dirichlet with the binomial distribution table updating qi involves simulating from qijx z c c mi c mi where mij is the number of allele copies in individual i that originated to z d z.b in population j mij d m w z.c i m d jg figure barycentric coordinate plot for the estimated posterior means of the qi based on mcmc sampling. step can be performed by simulating z.c i m independently for each i m llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllleuropeanjapaneseafricanafrican american objective bayes inference and mcmc and c from pr.z.c i m d jjx p q d qij pr.x i m qi pr.x i m jp z.c d j i m jp z.c i m d the probabilities on the right refer back to our generative distribution described earlier. figure shows a triangle plot that summarizes the result of running the mcmc algorithm on our subjects. we used a burn in of complete iterations and then a further to estimate the distribution of the parameters of interest in this case the qi. each dot in the figure represents a three-component probability vector and is the posterior mean of the sampled qi for each subject. the points are colored according to the known ethnicity. although this algorithm is unsupervised we see that the ethnic groups cluster nicely in the corners of the simplex and allow us to identify these clusters. the african american group is spread between the african and european clusters a little movement toward the japanese. markov chain methods are versatile tools that have proved their value in requiring some individual ingenuity with each application. bayesian applications. there are some drawbacks. the algorithms are not universal in the sense of maximum likelihood as a result applications especially of gibbs sampling have favored a small set of convenient priors mainly jeffreys and conjugates that simplify the calculations. this can cast doubt on the relevance of the ing the convergence of estimates such as n dp slow. resulting bayes inferences. successive realizations are highly correlated with each other the correlation makes it difficult to assign a standard error to n. actual applications ignore an initial of the values a burn-in period and go on to large enough b such that estimates like n appear to settle down. however neither the choice of nor that of b may be clear. objective bayes offers a paradigm of our book s theme the effect of electronic computation on statistical inference ingenious new algorithms facilitated bayesian applications over a wide class of applied problems and in doing so influenced the dominant philosophy of the whole area. notes and details notes and details the books by savage and de finetti summarizing his earlier work served as foundational texts for the subjective bayesian school of inference. highly influential they championed a framework for bayesian applications based on coherent behavior and the careful elucidation of personal probabilities. a current leading text on bayesian methods carlin and louis does not reference either savage or de finetti. now jeffreys again following earlier works claims foundational status. the change of direction has not gone without protest from the subjectivists see adrian smith s discussion of o hagan but is nonetheless almost a complete rout. metropolis et al. as part of nuclear weapons research developed the first mcmc algorithm. a vigorous line of work on markov chain methods for solving difficult probability problems has continued to flourish under such names as particle filtering and sequential monte carlo see gerber and chopin and its enthusiastic discussion. modeling population admixture et al. is one of several applications of hierarchical bayesian models and mcmc in genetics. other applications include haplotype estimation and motif finding as well as estimation of phylogenetic trees. the examples in this section were developed with the kind help of hua tang and david golan both from the stanford genetics department. hua suggested the example and provided helpful guidance david provided the data and ran the mcmc algorithm using the structure program in the pritchard lab. uninformative priors. a large catalog of possible uninformative priors has been proposed thoroughly surveyed by kass and wasserman one approach is to use the likelihood from a small part of the data say just one or two data points out of n as the prior as with the intrinsic priors of berger and pericchi or o hagan s fractional bayes factors. another approach is to minimize some mathematical measure of prior information as with bernardo s reference priors or jaynes maximum entropy criterion. kass and wasserman list a dozen more possibilities. coverage matching priors. welch and peers showed that for a multiparameter family and real-valued parameter of interest d t there exist priors such that the bayes credible interval of coverage has frequentist coverage with n the sample size. in other words the credible intervals are second-order accurate confidence intervals. tibshirani building on stein s work produced the objective bayes inference and mcmc nice formulation stein s paper developed the least-favorable family the one-parameter subfamily of that does not inappropriately increase the amount of fisher information for estimating cox and reid s orthogonal parameters form is formally equivalent to the least favorable family construction. least favorable family versions of reference priors and intrinsic priors have been proposed to avoid the difficulty with general-purpose uninformative priors seen in figure they do so but at the price of requiring a different prior for each choice of d t which begins to sound more frequentistic than bayesian. conjugate families theorem. theorem is rigorously derived in diaconis and ylvisaker families other than have conjugate-like properties but not the neat posterior expectation result using d d and v d for the poisson. has density e inverse gamma and chi-square distributions. a variate an inverse gamma variate has density poisson formula this follows immediately from so d is the gamma conjugate density in table the gamma results can be restated in terms of chi-squared variates d xi m m has conjugate prior an inverse chi-squared distribution. vasoconstriction data. efron and gous use this data to illustrate a theory connecting bayes factors with fisherian hypothesis testing. it is part of a larger data set appearing in finney also discussed in kass and raftery jeffreys and fisher s scales of evidence. jeffreys scale as it appears in table is taken from the slightly amended form in kass and raftery efron and gous compare it with fisher s scale for the contradictory results of table fisher and jeffreys worked in different scientific contexts small-sample agricultural experiments versus notes and details hard-science geostatistics which might explain jeffreys more stringent conception of what constitutes significant evidence. the bayesian information criterion. the bic was proposed by schwarz kass and wasserman provide an extended discussion of the bic and model selection. proofs of ultimately depend on sample size coherency as in efron and gous quotation marks are used here to indicate the basically qualitative nature of bic if we think of the data points as being collected in pairs then n becomes in etc. so it doesn t pay to put too fine a point on the criterion. moreover the convergence is geometric in the normpjp.b mcmc convergence. suppose we begin the mcmc random walk by choosing according to some arbitrary starting distribution let p.b be the distribution of obtained after b steps of the random walk. markov chain theory says that under certain broad conditions on q.i j p.b will converge to the target distribution p successive discrepancies eventually decreasing by a multiplicative factor. a proof appears in tanner and wong unfortunately the factor won t be known in most applications and the actual convergence may be quite slow. k dirichlet distribution. the dirichlet is a multivariate generalization of the beta distribution typically used to represent prior distributions for the multinomial distribution. for x d xk j xj d the density is defined as ky x j with xj where dq d j j statistical inference and methodology in the postwar era the fundamentals of statistical inference frequentist bayesian fisherian were set in place by the end of the first half of the twentieth century as discussed in part i of this book. the postwar era witnessed a massive expansion of statistical methodology responding to the data-driven demands of modern scientific technology. we are now at the end of part ii early computer-age methods having surveyed the march of new statistical algorithms and their inferential justification from the through the this was a time of opportunity for the discipline of statistics when the speed of computation increased by a factor of a thousand and then another thousand. as we said before a land bridge had opened to a new continent but not everyone was eager to cross. we saw a mixed picture the computer played a minor or negligible role in the development of some influential topics such as empirical bayes but was fundamental to others such as the bootstrap. fifteen major topics were examined in chapters through what follows is a short scorecard of their inferential affinities bayesian frequentist or fisherian as well as an assessment of the computer s role in their development. none of this is very precise but the overall picture illustrated in figure is evocative. empirical bayes robbins original development of formula was frequentistic but most statistical researchers were frequentists in the postwar era so that could be expected. the obvious bayesian component of empirical bayes arguments is balanced by their frequentist emphasis on unbiased estimation of bayesian estimators as well as the restriction to using only current data for inference. electronic computation played hardly any role in the theory s development indicated by blue coloring in the figure. of course mod postwar inference and methodology figure bayesian frequentist and fisherian influences as described in the text on major topics through colors indicate the importance of electronic computation in their development red crucial violet very important green important light blue less important blue negligible. ern empirical bayes applications are heavily computational but that is the case for most methods now. james stein and ridge regression the frequentist roots of james stein estimation are more definitive especially given the force of the james stein theorem nevertheless the empirical bayes interpretation lends james stein some bayesian credibility. electronic computation played no role in its development. this was less true for ridge regression colored light blue in the figure where the matrix calculation would have been daunting in the preelectronic age. the bayesian justification of ridge regression lllbayesianfrequentistfisherianlkaplan meierllog ranklglmlproportional hazardspartial likelihoodlbootstraplempiricalbayeslobjectivebayesmcmcljackknifelcvljames steinlregression treeslridgeregressionlbiclmissing dataemlaic cp postwar inference and methodology carries more weight than for james stein given the absence of a strong frequentist theorem. generalized linear models glm development began with a pronounced fisherian emphasis on modeling but settled down to more or less standard frequentist regression theory. a key operational feature low-dimensional sufficient statistics limited its computational demands but glm theory could not have developed before the age of electronic computers indicated by green coloring. regression trees model building by means of regression trees is a computationally intensive enterprise indicated by its red color in figure its justification has been mainly in terms of asymptotic frequentist properties. survival analysis the kaplan meier estimate log-rank test and proportional hazards model move from the frequentist pole of the diagram toward the fisherian pole as the conditioning arguments in sections through become more elaborate. the role of computation in their development increases in the same order. kaplan meier estimates can be done by hand were while it is impossible to contemplate proportional hazards analysis without the computer. partial likelihood the enabling argument for the theory is a quintessential fisherian device. missing data and the em algorithm the imputation of missing data has a bayesian flavor of indirect evidence but the fake data principle has fisherian roots. fast computation was important to the method s development particularly so for the em algorithm. jackknife and bootstrap the purpose of the jackknife was to calculate frequentist standard errors and biases. electronic computation was of only minor importance in its more explicitly quasilikelihoods an extension to a wider class of exponential family models. postwar inference and methodology development. by contrast the bootstrap is the archetype for computerintensive statistical inference. it combines frequentism with fisherian devices plug-in estimation of accuracy estimates as in and correctness arguments for bootstrap confidence intervals cross-validation the renaissance of interest in cross-validation required fast computation especially for assessing modern computer-intensive prediction algorithms. as pointed out in the text following figure cross-validation is a strongly frequentist procedure. bic aic and cp these three algorithms were designed to avoid computation bic for bayesian model selection section aic and cp for unbiased estimation of frequentist prediction error and objective bayes and mcmc in addition to their bayesian provenance objective bayes methods have some connection with fiducial ideas and the bootstrap as discussed in section argument can be made that they are at least as frequentist as they are bayesian see the notes below though that has not been acted upon in coloring the figure. gibbs sampling and mcmc the enabling algorithms epitomize modern computer-intensive inference. notes figure is an updated version of figure in efron r. a. fisher in the century. there the difficulty of properly placing objective bayes is confessed with erich lehmann arguing for a more frequentist location in fact the concept of uninformative prior is philosophically close to wald s least favorable distribution and the two often coincide. figure shows a healthy mixture of philosophical and computational tactics at work with all three edges not the center of the triangle in play. all new points will be red as we move into the twenty-first century in part iii. our triangle will have to struggle to accommodate some major developments based on machine learning a philosophically atheistic approach to statistical inference. part iii twenty-first-century topics large-scale hypothesis testing and false-discovery rates by the final decade of the twentieth century electronic computation fully dominated statistical practice. almost all applications classical or otherwise were now performed on a suite of computer platforms sas spss minitab matlab s r and others. the trend accelerates when we enter the twenty-first century as statistical methodology struggles most often successfully to keep up with the vastly expanding pace of scientific data production. this has been a twoway game of pursuit with statistical algorithms chasing ever larger data sets while inferential analysis labors to rationalize the algorithms. part iii of our book concerns topics in statistics. the word topics is intended to signal selections made from a wide catalog of possibilities. part ii was able to review a large portion certainly not all of the important developments during the postwar period. now deprived of the advantage of hindsight our survey will be more illustrative than definitive. for many statisticians microarrays provided an introduction to largescale data analysis. these were revolutionary biomedical devices that enabled the assessment of individual activity for thousands of genes at once and in doing so raised the need to carry out thousands of simultaneous hypothesis tests done with the prospect of finding only a few interesting genes among a haystack of null cases. this chapter concerns large-scale hypothesis testing and the false-discovery rate the breakthrough in statistical inference it elicited. actually what historians might call the long twenty-first century since we will begin in large-scale hypothesis testing and fdrs large-scale testing the prostate cancer data figure came from a microarray study of n d men prostate cancer patients and normal controls. each man s gene expression levels were measured on a panel of n d genes yielding a matrix of measurements xij xij d activity of ith gene for j th man for each gene a two-sample t statistic ti was computed comparing gene i s expression levels for the patients with those for the controls. under the null hypothesis that the patients and the controls responses come from the same normal distribution of gene i expression levels ti will follow a standard student t distribution with degrees of freedom the transformation zi d the inverse function of where is the cdf of a distribution and a standard normal cdf makes zi standard normal under the null hypothesis w zi n of course the investigators were hoping to spot some non-null genes ones for which the patients and controls respond differently. it can be shown that a reasonable model for both null and non-null genes zi n being the effect size for gene i. null genes have d while the investigators hoped to find genes with large positive or negative effects. figure shows the histogram of the zi values. the red curve is the scaled n density that would apply if in fact all of the genes were null that is if all of the equaled we can see that the curve is a little too high near the center and too low in the tails. good! even though most of the genes appear null the discrepancies from the curve suggest that there are some non-null cases the kind the investigators hoped to find. large-scale testing refers exactly to this situation having observed a large number n of test statistics how should we decide which if any of the null hypotheses to reject? classical testing theory involved only a single case n d a theory of multiple testing arose in the multiple this is model with zi now replacing the notation xi it is ce with c chosen to make the area under the curve equal the area of p the histogram. large-scale testing figure histogram of n d z-values one for each gene in the prostate cancer study. if all genes were null the histogram would track the red curve. for which genes can we reject the null hypothesis? meaning n between and perhaps the microarray era produced data sets with n in the hundreds thousands and now even millions. this sounds like piling difficulty upon difficulty but in fact there are some inferential advantages to the large-n framework as we will see. the most troubling fact about large-scale testing is how easy it is to be fooled. running separate hypothesis tests at significance level will produce about five significant results even if each case is actually null. the classical bonferroni bound avoids this fallacy by strengthening the threshold of evidence required to declare an individual case significant non-null. for an overall significance level perhaps d with n simultaneous tests the bonferroni bound rejects the ith null hypothesis only if it attains individual significance level for d n d and w zi n the one-sided bonferroni threshold for significance is d with for n d only four of the prostate study genes surpass this threshold. classic hypothesis testing is usually phrased in terms of significance levels and p-values. if test statistic z has cdf under the null hypothesis valuescounts large-scale hypothesis testing and fdrs p d is the right-sided p-value larger z giving smaller p-value. significance level refers to a prechosen threshold value e.g. d the null hypothesis is rejected at level if we observe p table on page coverage level means one minus the significance level shows fisher s scale for interpreting p-values. a level- test for a single null hypothesis satisfies by definition d prfreject true for a collection of n null hypotheses the family-wise error rate is the probability of making even one false rejection bonferroni s procedure controls fwer at level let be the indices of the true having say members. then fwer d prfreject any true n pi pi n pr n o fwer d pr d n the top line following from boole s inequality doesn t require even independence among the pi. the bonferroni bound is quite conservative for n d and d we reject only those cases having pi one can do only a little better under the fwer constraint. holm s procedure which offers modest improvement over bonferroni goes as follows. order the observed p-values from smallest to largest p.i p.n with denoting the corresponding null hypotheses. let be the smallest index i such that p.i i c reject all null hypotheses for i and accept all with i the left-sided p-value is p d we will avoid two-sided p-values in this discussion. false-discovery rates it can be shown that holm s procedure controls fwer at level while being slightly more generous than bonferroni in declaring rejections. false-discovery rates the fwer criterion aims to control the probability of making even one false rejection among n simultaneous hypothesis tests. originally developed for small-scale testing say n fwer usually proved too conservative for scientists working with n in the thousands. a quite different and more liberal criterion false-discovery rate control has become standard. figure a decision rule d has rejected r out of n null hypotheses a of these decisions were incorrect i.e. they were false discoveries while b of them were true discoveries. the false-discovery proportion fdp equals ar. figure diagrams the outcome of a hypothetical decision rule d applied to the data for n simultaneous hypothesis-testing problems null and d n non-null. an omniscient oracle has reported the rule s results r null hypotheses have been rejected a of these were cases of false discovery i.e. valid null hypotheses for a false-discovery proportion of fdp.d d ar define fdp d if r d fdp is unobservable without the oracle we cannot see a but under certain assumptions we can control its expectation. null actual non-null null a non-null a b b n r r n decision large-scale hypothesis testing and fdrs define fdr.d d e ffdp.dg a decision rule d controls fdr at level q with q a prechosen value between and if fdr.d q it might seem difficult to find such a rule but in fact a quite simple but ingenious recipe does the job. ordering the observed p-values from smallest to largest as in define imax to be the largest index for which p.i i n q and let dq be the that rejects for i imax accepting otherwise. a proof of the following theorem is referenced in the chapter endnotes. theorem hochberg fdr control ing to valid null hypotheses are independent of each other then if the p-values correspond fdr.dq d q where d in other words dq controls fdr at level the null proportion is unknown estimable so the usual claim is that dq controls fdr at level q. not much is sacrificed large-scale testing problems are most often fishing expeditions in which most of the cases are null putting near identification of a few non-null cases being the goal. the choice q d is typical practice. the popularity of fdr control hinges on the fact that it is more generous than fwer in declaring holm s procedure rejects null hypothesis if p.i thresholdholm s d n i c while dq has threshold p.i thresholddq d q n i sometimes denoted bhq after its inventors benjamini and hochberg see the chapter endnotes. the classic term significant for a non-null identification doesn t seem quite right for fdr control especially given the bayesian connections of section and we will sometimes use interesting instead. false-discovery rates in the usual range of interest large n and small i the ratio thresholddq thresholdholm s d q increases almost linearly with i. i i n figure ordered p-values p.i d plotted versus i for the largest z-values from the prostate data in figure the fdr control boundary dq q d rejects for the smallest values p.i while holm s fwer procedure d rejects for only the smallest values. upward slope of holm s boundary is too small to see here. figure illustrates the comparison for the right tail of the prostate data of figure with pi d and d q d the fdr procedure rejects for the largest z-values while fwer control rejects only the most extreme z-values hypothesis testing has been a traditional stronghold of frequentist decision theory with type error control being strictly enforced very often at the level. it is surprising that a new control criterion fdr has taken hold in large-scale testing situations. a critic noting fdr s relaxed rejection standards in figure might raise some pointed questions. ip valueholmsfdri large-scale hypothesis testing and fdrs is controlling a rate fdr as meaningful as controlling a probabil ity type error? the fdr significance for gene say one with isn t this unlikely in situations such as the prostate study? how should q be chosen? the control theorem depends on independence among the p-values. d depends on the results of all the other genes the more other zi values exceed the more interesting gene becomes that increases s index i in the ordered list making it more likely that lies below the dq threshold does this make inferential sense? a bayesempirical bayes restatement of the dq algorithm helps answer these questions as discussed next. empirical bayes large-scale testing in practice single-case hypothesis testing has been a frequentist preserve. its methods demand little from the scientist only the choice of a test statistic and the calculation of its null distribution while usually delivering a clear verdict. by contrast bayesian model selection whatever its inferential virtues raises the kinds of difficult modeling questions discussed in section it then comes as a pleasant surprise that things are different for largescale testing bayesian methods at least in their empirical bayes manifestation no longer demand heroic modeling efforts and can help untangle the interpretation of simultaneous test results. this is particularly true for the fdr control algorithm dq of the previous section. a simple bayesian framework for simultaneous testing is provided by the two-groups model each of the n cases genes for the prostate study is either null with prior probability or non-null with probability d the resulting observation z then has density either or d prfnullg d prfnon-nullg p for the prostate study is nearly and is the standard normal density d while the non-null density remains to be estimated. density if null density if non-null let and be the cdf values corresponding to and empirical bayes large-scale testing with survival curves d and d being the probability that a null z-value exceeds and similarly for finally define s.z to be the mixture survival curve the mixture density determines s.z s.z d c f d c dz f dz suppose now that observation zi for case i is seen to exceed some threshold value perhaps d bayes rule gives prfcase i is nulljzi d the correspondence with on page being d d and d f fdr is the bayes false-discovery rate as contrasted with the frequentist quantity fdr in typical applications is assumed in the prostate study and is assumed to be near the denominator in is unknown but and this is the crucial point it has an obvious estimate in large-scale testing situations namely os d where d the definition of the two-group model each zi has marginal density f making os the usual empirical estimate of plugging into yields an empirical bayes estimate of the bayes falsediscovery rate d os the connection with fdr control is almost immediate. first of all from definitions and we have pi d also for the ith from the largest z-value we have os d in putting these together condition p.i becomes os q but see section large-scale hypothesis testing and fdrs or os q which can be written as cfdr.z.i in other words the dq algorithm which rejects those null hypotheses p.i is in fact rejecting those cases for which the empirical bayes posterior probability of nullness is too small as defined by the bayesian nature of fdr control offers a clear advantage to the investigating scientist who gets a numerical assessment of the probability that he or she will be wasting time following up any one of the selected cases. we can now respond to the four questions at the end of the previous section fdr control does relate to a probability the bayes posterior probabil ity of nullness. the choice of q for dq amounts to setting the maximum tolerable amount of bayes risk of after taking d in nearly unbiased for there most often the zi and hence the pi will be correlated with each other. even under correlation however os in is still unbiased for and is a price to be paid for correlation which increases the variance of in the bayes two-groups model all of the non-null zi are i.i.d. observations from the non-null density with survival curve the number of null cases zi exceeding some threshold has fixed expectation n therefore an increase in the number of observed values zi exceeding must come from a heavier right tail for implying a greater posterior probability of non-nullness this point is made more clearly in the local false-discovery framework of the next section. it emphasizes the learning from the experience of others aspect of empirical bayes inference section the question of which others? is returned to in section figure illustrates the two-group model the n cases are the algorithm as stated just before the fdr control theorem is actually a little more liberal in allowing rejections. for a case of particular interest the calculation can be reversed if the case has ordered index i then according to the value q d npi i puts it exactly on the boundary of rejection making this its q-value. the largest z-value for the prostate data has zi d pi d and q-value that being both the frequentist boundary for rejection and the empirical bayes probability of nullness. empirical bayes large-scale testing figure a diagram of the two-groups model here the statistician observes values zi from a mixture density f d c and decides to reject or accept the null hypothesis depending on whether zi exceeds or is less than the threshold value w d randomly dispatched to the two arms in proportions and at which point they produce z-values according to either or suppose we are using a simple decision rule d that rejects the ith null hypothesis if zi exceeds some threshold and accepts otherwise reject accept if zi if zi the oracle of figure knows that d a of the null case zvalues exceeded and similarly d b of the non-null cases leading to d c d r total rejections. the false-discovery proportion is fdp d but this is unobservable since we see only the clever inferential strategy of false-discovery rate theory substitutes the expectation of e d n large-scale hypothesis testing and fdrs d for in giving dfdp d n d os using and starting from the two-groups model is an obvious empirical frequentist estimate of the bayesian probability as well as of fdp. if placed in the bayes fisher frequentist triangle of figure falsediscovery rates would begin life near the frequentist corner but then migrate at least part of the way toward the bayes corner. there are remarkable parallels with the james stein estimator of chapter both theories began with a striking frequentist theorem which was then inferentially rationalized in empirical bayes terms. both rely on the use of indirect evidence learning from the experience of others. the difference is that james stein estimation always aroused controversy while fdr control has been quickly welcomed into the pantheon of widely used methods. this could reflect a change in twenty-first-century attitudes or perhaps only that the dq rule better conceals its bayesian aspects. local false-discovery rates tail-area statistics were synonymous with classic one-at-a-time hypothesis testing and the dq algorithm carried over p-value interpretation to large-scale testing theory. but tail-area calculations are neither necessary nor desirable from a bayesian viewpoint where having observed test statistic zi equal to some value we should be more interested in the probability of nullness given zi d than given zi to this end we define the local false-discovery rate d prfcase i is nulljzi d as opposed to the tail-area false-discovery rate the main point of what follows is that reasonably accurate empirical bayes estimates of fdr are available in large-scale testing problems. as a first try suppose that a proposed region for rejecting null hy potheses is a small interval centered at d d c d with d perhaps we can redraw figure now with local false-discovery rates and the null non-null and total number of z-values in the local false-discovery proportion is unobservable but we can replace with n its approximate expectation as in yielding the d d n estimate would be needlessly noisy in practice z-value distributions tend to be smooth allowing the use of regression estimates for bayes theorem gives fdr.z d in the two-groups model in now the indicator of null or non-null states and x now z. drawing a smooth curve o f through the histogram of the z-values yields the more efficient estimate d o f cfdr.z figure shows cfdr.z for the prostate study data of figure the null proportion can be estimated see section or set equal to where o f in has been estimated as described below. the curve hovers near for the of the cases having jzij sensibly suggesting that there is no involvement with prostate cancer for most genes. it declines quickly for jzij reaching the conventionally interesting threshold for zi and zi this was attained for genes in the right tail and in the left these being reasonable candidates to flag for followup investigation. the curve o f used in was obtained from a fourth-degree log polynomial poisson regression fit to the histogram in figure as in figure log polynomials of degree through were fit by maximum likelihood giving total residual deviances shown in table an enormous improvement in fit is seen in going from degree to but nothing significant after that with decreases less than the null value suggested by equation makes argument of the previous section clearer having more other z-values fall into increases and making it more likely that zi d represents a non-null case. large-scale hypothesis testing and fdrs figure local false-discovery rate estimatecfdr.z the left indicated by dashes havecfdr.zi light dashed curves are the left and right tail-area estimates cfdr.z for prostate study of figure genes on the right and on table total residual deviances from log polynomial poisson regressions of the prostate data for polynomial degrees through degree is preferred. degree deviance the points in figure represent the log bin counts from the histogram in figure zero counts with the solid curve showing the mle polynomial fit. also shown is the standard normal log density log d c constant it fits reasonably well for jzj emphasizing the null status of the gene the cutoffcfdr.z for declaring a case interesting is not completely majority. arbitrary. definitions and and a little algebra show that it valuefdr and fdrlocal fdr local false-discovery rates figure points are log bin counts for figure s histogram. the solid black curve is a fourth-degree log-polynomial fit used to calculatecfdr.z in figure the dashed red curve the log null density provides a reasonable fit for jzj is equivalent to if we assume as is reasonable in most large-scale testing situations this makes the bayes factor quite large strong evidence against the null hypothesis in jeffreys scale table there is a simple relation between the local and tail-area false-discovery rates d e ffdr.zjz so is the average value of fdr.z for z greater than in interesting situations fdr.z will be a decreasing function for large values of z as on the right side of figure making this accounts valuelog degree large-scale hypothesis testing and fdrs for the conventional significance cutoff cfdr.z being smaller than cfdr.z as with left-sided and right-sided tail-area cfdr estimates sincecfdr.z ap the bayesian interpretation of local false-discovery rates carries with it the advantages of bayesian coherency. we don t have to change definitions plies without change to both also we don t need a separate theory for true-discovery rates since d is the conditional probability that case i is non-null given zi d choice of the null distribution the null distribution in the two-groups model plays a crucial role in large-scale testing just as it does in the classic single-case theory. something different however happens in large-scale problems with thousands of z-values to examine at once it can become clear that the conventional theoretical null is inappropriate for the situation at hand. put more positively large-scale applications may allow us to empirically determine a more realistic null distribution. the police data of figure illustrates what can happen. possible racial bias in pedestrian stops was assessed for n d new york city police officers in each officer was assigned a score zi large positive scores suggesting racial bias. the zi values were summary scores from a complicated logistic regression model intended to compensate for differences in the time of day location and context of the stops. logistic regression theory suggested the theoretical null distribution w zi n for the absence of racial bias. the trouble is that the center of the z-value histogram in figure which should track the n curve applying to the presumably large fraction of null-case officers is much too wide. the situation for the prostate data in figure an mle fitting algorithm discussed below produced the empirical null w zi n going further z in the two-groups model could be multidimensional. then tail-area false-discovery rates would be unavailable but would still legitimately define fdr.z. choice of the null distribution figure police data histogram of z scores for n d new york city police officers with large zi suggesting racial bias. the center of the histogram is too wide compared with the theoretical null distribution zi n an mle fit to central data gave n as empirical null. there is a lot at stake here. based on the empirical null only the four circled points at the far right of figure the fifth point had as appropriate here. this is reinforced by a qq plot of the zi values shown in figure where we see most of the cases falling nicely along a n line with just a few outliers at both extremes. four officers reached the probably racially biased cutoffcfdr.zi cfdr d while all the others exceeded the theoretical n null was much more severe assigningcfdr to the officers having zi one can imagine the difference in newspaper headlines. from a classical point of view it seems heretical to question the theoretical null distribution especially since there is no substitute available in single-case testing. once alerted by data sets like the police study however it is easy to list reasons for doubt asymptotics taylor series approximations go into theoretical null calculations such as which can lead to inaccuracies particularly in the crucial tails of the null distribution. correlations false-discovery rate methods are correct on the average z valuesfrequency large-scale hypothesis testing and fdrs figure qq plot of police data z scores most scores closely follow the n line with a few outliers at either end. the circled points are cases having local false-discovery estimate cfdr.zi based on the empirical null. using the theoretical n null gives cases withcfdr.zi on the left and on the right. even with correlations among the n z-values. however severe correlation destabilizes the z-value histogram which can become randomly wider or narrower than theoretically predicted undermining theoretical null results for the data set at hand. unobserved covariates the police study was observational individual encounters were not assigned at random to the various officers but simply observed as they happened. observed covariates such as the time of day and the neighborhood were included in the logistic regression model but one can never rule out the possibility of influential unobserved covariates. effect size considerations the hypothesis-testing setup where a large fraction of the cases are truly null may not be appropriate. an effect size model with and zi n might apply with the prior not having an atom at d the nonatomic choice n provides a good fit to the qq plot in figure quantilessample quantileslllllllllintercept choice of the null distribution empirical null estimation our point of view here is that the theoretical null zi n is not completely wrong but needs adjustment for the data set at hand. to this end we assume the two-groups model with normal but not necessarily n say n in order to compute the local false-discovery rate fdr.z d we want to estimate the three numerator parameters the mean and standard deviation of the null density and the proportion of null cases. denominator f is estimated as in section our key assumptions are that is large say and that most of the zi near are null cases. the algorithm locfdr begins by selecting a set near z d in which it is assumed that all the zi in are null in terms of the two-groups model the assumption can be stated as d for z modest violations of which are to be expected produce small biases in the empirical null estimates. maximum likelihood based on the number and values of the zi observed in yield the empirical null estimates applied to the police data locfdr chose d and pro o duced estimates d two small simulation studies described in table give some idea of the variabilities and biases inherent in the locfdr estimation process. the third method somewhere between the theoretical and empirical null estimates but closer to the former relies on permutations. the vector z of z-values for the prostate data of figure was obtained from a study of men cancer patients and controls. randomly permuting the men s data that is randomly choosing of the to be controls and the remaining to be patients and then carrying through steps in which any actual cancercontrol differences have gives a vector z been suppressed. a histogram of the z i values combining several permutations provides the permutation null. here we are extending fisher s original permutation idea section to large-scale testing. ten permutations of the prostate study data produced an almost perfect large-scale hypothesis testing and fdrs o for two table means and standard deviations of simulation studies of empirical null estimation using locfdr. n d cases each trial with as shown trials two-groups model with non-null density equal to n side or n side. true mean st dev n permutation null. is as expected from the classic theory of permutation t-tests. permutation methods reliably overcome objection to the theoretical null distribution over-reliance on asymptotic approximations but cannot cure objections and whatever the cause of disparity the operational difference between the theoretical and empirical null distribution is clear with the latter the significance of an outlying case is judged relative to the dispersion of the majority not by a theoretical yardstick as with the former. this was persuasive for the police data but the story isn t one-sided. estimating the null distribution adds substantially to the variability ofcfdr orcfdr. for situations such as the prostate data when the theoretical null looks nearly it is reasonable to stick with it. the very large data sets of twenty-first-century applications encourage self-contained methodology that proceeds from just the data at hand using a minimum of theoretical constructs. false-discovery rate empirical bayes analysis of large-scale testing problems with data-based estimation of o and o f comes close to the ideal in this sense. relevance false-discovery rates return us to the purview of indirect evidence sections and our interest in any one gene in the prostate cancer study depends on its own z score of course but also on the other genes scores learning from the experience of others in the language used before. the crucial question we have been avoiding is which others? our tacit answer has been all the cases that arrive in the same data set all the genes the locfdr algorithm gave d for the prostate data. relevance in the prostate study all the officers in the police study. why this can be a dangerous tactic is shown in our final example. a dti tensor imaging study compared six dyslexic children with six normal controls. each dti scan recorded fluid flows at n voxels i.e. at three-dimensional brain coordinates. a score zi comparing dyslexics with normal controls was calculated for each voxel i calibrated such that the theoretical null distribution of no difference was w zi n as at figure histogram of z scores for the dti study comparing dyslexic versus normal control children at brain locations. a fdr analysis based on the empirical null distribution gave voxels withcfdr.zi those having zi by red dashes. figure shows the histogram of all zi values normal-looking near the center and with a heavy right tail locfdr gave empirical null parameters d the voxels with zi havingcfdr values using the the z scorefrequency large-scale hypothesis testing and fdrs voxels with zi havingcfdri oretical null yielded only modestly different results now the figure a plot of zi scores from a dti study axis and voxel distances xi from the back of the brain axis. the starred points are the voxels with cfdr.zi which occur mostly for xi in the interval in figure the voxel scores zi graphed vertically are plotted versus xi the voxel s distance from the back of the brain. waves of differing response are apparent. larger values occur in the interval x up. most of the voxels havingcfdri occur at the top of this wave. where the entire z-value distribution low medium and high is pushed figure raises the problem of fair comparison. perhaps the voxels with xi between and should be compared only with each other and not with all cases. doing so gave d only voxels havingcfdri those with zi all of this is a question of relevance which other voxels i are relevant to the assessment of significance for voxel one might argue that this is a question for the scientist who gathers the data and not for the statistical analyst but that is unlikely to be a fruitful avenue at least not without xz relevance a lot of back-and-forth collaboration. standard bayesian analysis solves the problem by dictate the assertion of a prior is also an assertion of its relevance. empirical bayes situations expose the dangers lurking in such assertions. relevance was touched upon in section where the limited translation rule was designed to protect extreme cases from being shrunk too far toward the bulk of ordinary ones. one could imagine having a relevance function zi that given the covariate information xi and response zi for casei somehow adjusts an ensemble false-discovery rate estimate to correctly apply to the case of interest but such a theory barely exists. summary combine frequentist and bayesian thinking. large-scale testing particularly in its false-discovery rate implementation is not at all the same thing as the classic fisher neyman pearson theory frequentist single-case hypothesis testing depends on the theoretical long-run behavior of samples from the theoretical null distribution. with data available from say n d simultaneous tests the statistician has his or her own long run in hand diminishing the importance of theoretical modeling. in particular the data may cast doubt on the theoretical null providing a more appropriate empirical null distribution in its place. classic testing theory is purely frequentist whereas false-discovery rates on its own score zi while cfdr.zi or cfdr.zi also depends on the in classic testing the attained significance level for case i depends only applications of single-test theory usually hope for rejection of the null hypothesis a familiar prescription being power at size the opposite is true for large-scale testing where the usual goal is to accept most of the null hypotheses leaving just a few interesting cases for further study. sharp null hypotheses such as d are less important in large-scale applications where the statistician is happy to accept a hefty proportion of uninterestingly small but nonzero effect sizes false-discovery rate hypothesis testing involves a substantial amount of estimation blurring the line beteen the two main branches of statistical inference. served z-values for other cases. large-scale hypothesis testing and fdrs notes and details the story of false-discovery rates illustrates how developments in scientific technology in this case can influence the progress of statistical inference. a substantial theory of simultaneous inference was developed between and mainly aimed at the frequentist control of family-wise error rates in situations involving a small number of hypothesis tests maybe up to good references are miller and westfall and young benjamini and hochberg s seminal paper introduced false-discovery rates at just the right time to catch the wave of large-scale data sets now involving thousands of simultaneous tests generated by microarray applications. most of the material in this chapter is taken from efron where the empirical bayes nature of fdr theory is emphasized. the police data is discussed and analyzed at length in ridgeway and macdonald model section of efron discusses the following result for the non-null distribution of z-values a transformation such as that produces a z-value a standard normal random variable z n under the null hypothesis gives to a good approximation z n student s t with degrees of freedom under reasonable alternatives. for the specific situation in as in holm s procedure. methods of fwer control including holm s procedure are surveyed in chapter of efron they display a large amount of mathematical ingenuity and provided the background against which fdr theory developed. fdr control theorem. benjamini and hochberg s striking control theorem was rederived by storey et al. using martingale theory. the basic idea of false discoveries as displayed in figure goes back to soric formula integrating fdr.z d gives e ffdr.zjz dz dz f dz d d thresholds for fdr and fdr. suppose the survival curves and satisfy the lehmann alternative relationship log d log notes and details for large values of z where is a positive constant less than is a reasonable condition for the non-null density to produce larger positive values of z than does the null density differentiating gives d after some rearrangement. but fdr.z d c is algebraically equivalent to fdr.z fdr.z d and similarly for fdr.z yielding fdr.z fdr.z fdr.z fdr.z d for large z both fdr.z and fdr.z go to zero giving the asymptotic relationship fdr.z this motivates the suggested relative thresholdscfdr.zi compared if d for instance fdr.z will be about twice fdr.z where z is large. with cfdr.zi correlation effects. the poisson regression method used to estimate o f in figure proceeds as if the components of the n of zi values z are independent. approximation that the kth bin count yk requires independence. if not it can be shown that var.yk increases above the poisson value as var.yk c nx x d here ck is a fixed constant depending on f while is the root mean square correlation between all pairs zi and zj cov.zi zj n.n j i estimates likecfdr.z in figure remain nearly unbiased under correlation but their sampling variability increases as a function of chapters and of efron discuss correlation effects in detail. often can be estimated. let x be the matrix of gene expression levels measured for the control subject in the prostate study. rows large-scale hypothesis testing and fdrs i and j provide an unbiased estimate of cor.zi zj modern computation is sufficiently fast to evaluate all n.n pairs that isn t necessary sampling is faster from which estimate o is obtained. it equaled for the control subjects and for the matrix of the cancer patients. correlation is not much of a worry for the prostate study but other microarray studies show much larger o values. sections and of efron discuss how correlations can undercut inferences based on the theoretical null even when it is correct for all the null cases. the program locfdr. available from cran this is an r program that provides fdr and fdr estimates using both the theoretical and empirical null distributions. ml estimation of the empirical null. let be the zero set the set of zi observed to be in their indices and the number of zi in also define d e p dz according to then has density and d dz d prfzi and likelihood f d n p the first factor being the binomial probability of seeing of the zi in and the second the conditional probability of those zi falling within o while the second factor is numerically maximized to give o d is obtained from the first and then d o o this is a partial likelihood argument as in section locfdr centers at the median of the n zi values with width about twice the interquartile range estimate of the permutation null. an impressive amount of theoretical effort concerned the permutation t-test in a single-test two-sample situation permuting the data and computing the t statistic gives after a great many repetitions a histogram dependably close to that of the standard t distribution see hoeffding this was fisher s justification for using the standard t-test on nonnormal data. the argument cuts both ways. permutation methods tend to recreate the notes and details theoretical null even in situations like that of figure where it isn t appropriate. the difficulties are discussed in section of efron relevance theory. suppose that in the dti example shown in figure we want to consider only voxels with x d as relevant to an observed zi with xi d now there may not be enough relevant cases to how the complete-data estimatescfdr.zi orcfdr.zi can be efficiently modadequately estimate fdr.zi or fdr.zi section of efron shows ified to conform to this situation. sparse modeling and the lasso the amount of data we are faced with keeps growing. from around the late we started to see wide data sets where the number of variables far exceeds the number of observations. this was largely due to our increasing ability to measure a large amount of information automatically. in genomics for example we can use a high-throughput experiment to automatically measure the expression of tens of thousands of genes in a sample in a short amount of time. similarly sequencing equipment allows us to genotype millions of snps polymorphisms cheaply and quickly. in document retrieval and modeling we represent a document by the presence or count of each word in the dictionary. this easily leads to a feature vector with components one for each distinct vocabulary word although most would be zero for a small document. if we move to bi-grams or higher the feature space gets really large. in even more modest situations we can be faced with hundreds of variables. if these variables are to be predictors in a regression or logistic regression model we probably do not want to use them all. it is likely that a subset will do the job well and including all the redundant variables will degrade our fit. hence we are often interested in identifying a good subset of variables. note also that in these wide-data situations even linear models are over-parametrized so some form of reduction or regularization is essential. in this chapter we will discuss some of the popular methods for model selection starting with the time-tested and worthy forward-stepwise approach. we then look at the lasso a popular modern method that does selection and shrinkage via convex optimization. the lars algorithm ties these two approaches together and leads to methods that can deliver paths of solutions. finally we discuss some connections with other modern big- and wide data approaches and mention some extensions. forward stepwise regression forward stepwise regression stepwise procedures have been around for a very long time. they were originally devised in times when data sets were quite modest in size in particular in terms of the number of variables. originally thought of as the poor cousins of best-subset selection they had the advantage of being much cheaper to compute in fact possible to compute for large p. we will review best-subset regression first. i suppose we have a set of n observations on a response yi and a vecd xi xip and we plan to fit a linear tor of p predictors x regression model. the response could be quantitative so we can think of fitting a linear model by least squares. it could also be binary leading to a linear logistic regression model fit by maximum likelihood. although we will focus on these two cases the same ideas transfer exactly to other generalized linear models the cox model and so on. the idea is to build a model using a subset of the variables in fact the smallest subset that adequately explains the variation in the response is what we are after both for inference and for prediction purposes. suppose our loss function for fitting the linear model is l sum of squares negative log-likelihood. the method of best-subset regression is simple to describe and is given in algorithm step is easy to state but requires a lot of computation. for algorithm best-subset regression. start with m d and the null model d o estimated by the mean of the yi. at step m d pick the single variable j that fits the response best in terms of the loss l evaluated on the training data in a univariate regression d o for each subset size m mg m min.n p identify the best subset am of size m when fitting a linear model om.x d o o am with m of the p variables in terms of the loss l. use some external data or other means to select the best amongst these o j set d fjg. c x c x j am m models. p much larger than about it becomes prohibitively expensive to perform exactly a so-called n-p complete problem because of its combinatorial complexity are subsets. note that the subsets need not be nested sparse modeling and the lasso the best subset of size m d say need not include both or any of the variables in the best subset of size m d in step there are a number of methods for selecting m. originally the cp criterion of chapter was proposed for this purpose. here we will favor k-fold cross-validation since it is applicable to all the methods discussed in this chapter. it is interesting to digress for a moment on how cross-validation works here. we are using it to select the subset size m on the basis of prediction performance future data. with k d we divide the n training observations randomly into equal size groups. leaving out say group k d we perform steps on the and for each of the chosen models we summarize the prediction performance on the data. we do this k d times each time with group k left out. we then average the performance measures for each m and select the value of m corresponding to the best performance. notice that for each m the models om.x might involve different subsets of variables! this is not a concern since we are trying to find a good value of m for the method. having identified om we rerun steps on the entire training set and deliver the chosen model o om.x. as hinted above there are problems with best-subset regression. a primary issue is that it works exactly only for relatively small p. for example we cannot run it on the spam data with variables least not in on a macbook pro!. we may also think that even if we could do the computations with such a large search space the variance of the procedure might be too high. as a result more manageable stepwise procedures were invented. forward stepwise regression algorithm is a simple modification of bestsubset with the modification occurring in step forward stepwise regression produces a nested sequence of models am it starts with the null model here an intercept and adds variables one at a time. even with large p identifying the best variable to add at each step is manageable and can be distributed if clusters of machines are available. most importantly it is feasible for large p. figure shows the coefficient profiles for forward-stepwise linear regression on the spam training data. here there are input variables prevalence of particular words in the document and an official test split of observations. the response is coded as if the email was spam else the figure caption gives the details. we saw the spam data earlier in table figure and figure fitting the entire forward-stepwise linear regression path as in the figure forward stepwise regression algorithm forward stepwise regression. start with m d and the null model d o estimated by the mean of the yi. at step m d pick the single variable j that fits the response best in terms of the loss l evaluated on the training data in a univariate regression d o for each subset size m mg m min.n p identify the variable k that when augmented with to form am leads to the model om.x d o o am that performs best in terms of the loss l. use some external data or other means to select the best amongst these o j set d fjg. c x c x j am m models. n p has essentially the same cost as a single least squares fit on all the variables. this is because the sequence of models can be updated each time a variable is added. however this is a consequence of the linear model and squared-error loss. suppose instead we run a forward stepwise logistic regression. here updating does not work and the entire fit has to be recomputed by maximum likelihood each time a variable is added. identifying which variable to add in step in principle requires fitting an c model p m times and seeing which one reduces the deviance the most. in practice we can use score tests which are much cheaper to evaluate. these amount to using the quadratic approximation to the log-likelihood from the final iteratively reweighted least-squares iteration for fitting the model with m terms. the score test for a variable not in the model is equivalent to testing for the inclusion of this variable in the weighted least-squares fit. hence identifying the next variable is almost back to the previous cases requiring p m simple regression updates. figure shows the test misclassification error for forward-stepwise linear regression and logistic regression on the spam data as a function of the number of steps. they both level off at around steps and have a similar shape. however the logistic regression gives more accurate although forward-stepwise methods are possible for large p they get tedious for very large p the thousands especially if the data could sup for this example we can halve the gap between the curves by optimizing the prediction threshold for linear regression. sparse modeling and the lasso figure forward stepwise linear regression on the spam data. each curve corresponds to a particular variable and shows the progression of its coefficient as the model grows. these are plotted against the training and the vertical gray bars correspond to each step. starting at the left at step the first selected variable explains d adding the second increases to etc. what we see is that early steps have a big impact on the while later steps hardly have any at all. the vertical black line corresponds to step figure and we see that after that the step-wise improvements in are negligible. port a model with many variables. however if the ideal active set is fairly small even with many thousands of variables forward-stepwise selection is a viable option. forward-stepwise selection delivers a sequence of models as seen in the previous figures. one would generally want to select a single model and as discussed earlier we often use cross-validation for this purpose. figure illustrates using stepwise linear regression on the spam data. here the sequence of models are fit using squared-error loss on the binary response variable. however cross-validation scores each model for misclassification error the ultimate goal of this modeling exercise. this highlights one of the advantages of cross-validation in this context. a con stepwise on training datacoefficients the lasso figure forward-stepwise regression on the spam data. shown is the misclassification error on the test data as a function of the number of steps. the brown dots correspond to linear regression with the response coded as and a prediction greater than zero is classified as one less than zero as the blue dots correspond to logistic regression which performs better. we see that both curves essentially reach their minima after steps. venient and smooth loss function is used to fit the sequence of models. however we can use any performance measure to evaluate the sequence of models here misclassification error is used. in terms of the parameters of the linear model misclassification error would be a difficult and discontinuous loss function to use for parameter estimation. all we need to use it for here is pick the best model size. there appears to be little benefit in going beyond terms. the lasso the stepwise model-selection methods of the previous section are useful if we anticipate a model using a relatively small number of variables even if the pool of available variables is very large. if we expect a moderate number of variables to play a role these methods become cumbersome. datasteptest misclassification errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward stepwise linear regressionforward stepwise logistic regression sparse modeling and the lasso figure ten-fold cross-validated misclassification errors for forward-stepwise regression on the spam data as a function of the step number. since each error is an average of numbers we can compute a standard error included in the plot are pointwise standard-error bands. the brown curve is the misclassification error on the test data. another black mark against forward-stepwise methods is that the sequence of models is derived in a greedy fashion without any claimed optimality. the methods we describe here are derived from a more principled procedure indeed they solve a convex optimization as defined below. we will first present the lasso for squared-error loss and then the more i subject to k t general case later. consider the constrained linear regression problem where k dpp nx x j jj the norm of the coefficient vector. since both the loss and the constraint are convex in this is a convex optimization problem and it is known as the lasso. the constraint k t restricts the coefficients of the model by pulling them toward zero this has the effect of reducing their variance and prevents overfitting. ridge regression is an earlier great uncle of the lasso and solves a similar problem to ex minimize n datasteptest and cv misclassification errorlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllltest fold cv error the lasso figure an example with to illustrate the difference between ridge regression and the lasso. in both plots the red contours correspond to the squared-error loss function with the unrestricted least-squares estimate o regions show the constraints with the lasso on the left and ridge on the right. the solution to the constrained problem corresponds to the value of where the expanding loss contours first touch the constraint region. due to the shape of the lasso constraint this will often be at a corner an edge more generally as here which means in this case that the minimizing has d for the ridge constraint this is unlikely to happen. in the center. the blue cept the constraint is k t ridge regression bounds the quadratic norm of the coefficient vector. it also has the effect of pulling the coefficients toward zero in an apparently very similar way. ridge regression is discussed in section both the lasso and ridge regression are shrinkage methods in the spirit of the james stein estimator of chapter a big difference however is that for the lasso the solution typically has many of the j equal to zero while for ridge they are all nonzero. hence the lasso does variable selection and shrinkage while ridge only shrinks. figure illustrates this for in higher dimensions the norm has sharp edges and corners which correspond to coefficient estimates zero in since the constraint in the lasso treats all the coefficients equally it usually makes sense for all the elements of x to be in the same units. if not we here we use the bound form of ridge regression while in section we use the lagrange form. they are equivalent in that for every lagrange solution there is a corresponding bound solution. sparse modeling and the lasso typically standardize the predictors beforehand so that each has variance one. two natural boundary values for t in are t d and t d the former corresponds to the constant model fit is the mean of the and the latter corresponds to the unrestricted least-squares fit. in fact if n p and o is the least-squares estimate then we can replace by k o and any value of t k o is a non-binding constraint. figure figure the lasso linear regression regularization path on the spam data. each curve corresponds to a particular variable and shows the progression of its coefficient as the regularization bound t grows. these curves are plotted against the training rather than t to make the curves comparable with the forward-stepwise curves in figure some values of t are indicated at the top. the vertical gray bars indicate changes in the active set of nonzero coefficients typically an inclusion. here we see clearly the role of the penalty as t is relaxed coefficients become nonzero but in a smoother fashion than in forward stepwise. shows the regularization for the lasso linear regression problem on we typically do not restrict the intercept in the model. also known as the homotopy path. on training the lasso the spam data that is the solution path for all values of t. this can be computed exactly as we will see in section because the coefficient profiles are piecewise linear in t. it is natural to compare this coefficient profile with the analogous one in figure for forward-stepwise regression. because of the control of k o we don t see the same range as in forward stepwise and observe somewhat smoother behavior. figure contrasts figure lasso versus forward-stepwise regression on the spam data. shown is the misclassification error on the test data as a function of the number of variables in the model. linear regression is coded brown logistic regression blue hollow dots forward stepwise solid dots lasso. in this case it appears stepwise and lasso achieve the same performance but lasso takes longer to get there because of the shrinkage. the prediction performance on the spam data for lasso regularized models regression and logistic regression versus forward-stepwise models. the results are rather similar at the end of the path here forward stepwise can achieve classification performance similar to that of lasso regularized logistic regression with about half the terms. lasso logistic regression indeed any likelihood-based linear model is fit by penalized maximum datasteptest misclassification errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward stepwise linear regressionforward stepwise logistic regressionlasso linear regressionlasso logistic regression sparse modeling and the lasso likelihood minimize n nx l.yi c xi subject to k t here l is the negative of the log-likelihood function for the response distribution. fitting lasso models the lasso objectives or are differentiable and convex in and and the constraint is convex in hence solving these problems is a convex optimization problem for which standard packages are available. it turns out these problems have special structure that can be exploited to yield efficient algorithms for fitting the entire path of solutions as in figures and we will start with problem which we rewrite in the more convenient lagrange form minimize ky x c here we have centered y and the columns of x beforehand and hence the intercept has been omitted. the lagrange and constraint versions are equivalent in the sense that any solution o to with corresponds to a solution to with t d k o here large values of will encourage solutions with small norm coefficient vectors and vice-versa d corresponds to the ordinary least squares fit. the solution to satisfies the subgradient condition j d p o i c d hxj y x o j j d p. this notation means sj d sign. n o where sj sign. j if o j and sj if o j d we use the inner-product notation ha bi d a b in which leads to more evocative expressions. these subgradient conditions are the modern way of characterizing solutions to problems of this kind and are equivalent to the karush kuhn tucker optimality conditions. from these conditions we can immediately learn some properties of a lasso solution. o ij d for all members of the active set i.e. each of the jhxj y x n variables in the model nonzero coefficient has the same covariance with the residuals absolute value. least-angle regression o jhxk y x ij for all variables not in the active set with n coefficients zero. these conditions are interesting and have a big impact on computation. suppose we have the solution o at and we decrease by a small amount to the coefficients and hence the residuals change in such a way that the covariances all remain tied at the smaller value if in the process the active set has not changed and nor have the signs of their coefficients then we get an important consequence o is linear for to see this suppose a indexes the active set which is the same at and and let sa be the constant sign vector. then we have a.y x a.y x x x o d o d by subtracting and solving we get o o axa d and the remaining coefficients indices not in a are all zero. this shows that the full coefficient vector o is linear for in fact the coefficient profiles for the lasso are continuous and piecewise linear over the entire range of with knots occurring whenever the active set changes or the signs of the coefficients change. another consequence is that we can easily determine the smallest value for such that the solution o d from this can be jhxj yij. seen to be d maxj these two facts plus a few more details enable us to compute the exact solution path for the squared-error-loss lasso that is the topic of the next section. n least-angle regression we have just seen that the lasso coefficient profile o is piecewise linear in and that the elements of the active set are tied in their absolute o covariance with the residuals. with d y x the covariance between xj and the evolving residual is cj d jhxj hence these also change in a piecewise linear fashion with cj d for j a and cj for j a. this inspires the least-angle regression algorithm given in algorithm which exploits this linearity to fit the entire lasso regularization path. n sparse modeling and the lasso algorithm least-angle regression. a a xa d fjg and xa standardize the predictors to have mean zero and unit norm. start with the residual d y ny d p d find the predictor xj most correlated with i.e. with largest value for jhxj call this value define the active set a n the matrix consisting of this single variable. for k d k d min.n p do define the least-squares direction d and d and the remaining elements define the p-vector such that a are zero. move the coefficients from in the direction toward their least-squares solution on xa d c for keeping track of the evolving residuals d y x d jhx for keeping track of n a identify the largest value of at which a variable catches up with the active set if the variable jhx d this defines the next has index that means n knot k d d c and rk d d a y x k. set a return the sequence kgk in step d a as in we can think of the lar al xa gorithm as a democratic version of forward-stepwise regression. in forwardstepwise regression we identify the variable that will improve the fit the most and then move all the coefficients toward the new least-squares fit. as described in endnotes and this is sometimes done by computing the inner products of each variable with the residual and picking the largest in absolute value. in step of algorithm we move the coefficients for the variables in the active set a toward their least-squares fit their inner products tied but stop when a variable not in a catches up in inner product. at that point it is invited into the club and the process continues. step can be performed efficiently because of the linearity of the evolving inner products for each variable not in a we can determine exactly when time it would catch up and hence which catches up first and when. since the path is piecewise linear and we know the slopes this least-angle regression figure covariance evolution on the spam data. as variables tie for maximal covariance they become part of the active set. these occasions are indicated by the vertical gray bars again plotted against the training as in figure means we know the path exactly without further computation between and the newly found the name least-angle regression derives from the fact that in step the fitted vector evolves in the direction x d xa and its inner product xa d sa. since all the columns with each active vector is given by x of x have unit norm this means the angles between each active vector and the evolving fitted vector are equal and hence minimal. a the main computational burden in algorithm is in step computing the new direction each time the active set is updated. however this is easily performed using standard updating of a qr decomposition and hence the computations for the entire path are of the same order as that of a single least-squares fit using all the variables. the vertical gray lines in figure show when the active set changes. we see the slopes change at each of these transitions. compare with the corresponding figure for forward-stepwise regression. figure shows the the decreasing covariance during the steps of the on training datacovariance with residuals sparse modeling and the lasso lar algorithm. as each variable joins the active set the covariances become tied. at the end of the path the covariances are all zero because this is the unregularized ordinary least-squares solution. it turns out that the lar algorithm is not quite the lasso path variables can drop out of the active set as the path evolves. this happens when a coefficient curve passes through zero. the subgradient equations imply that the sign of each active coefficient matches the sign of the gradient. however a simple addition to step in algorithm takes care of the issue lasso modification if a nonzero coefficient crosses zero before the next variable enters drop it from a and recompute the joint least-squares direction using the reduced set. figure was computed using the lars package in r with the lasso option set to accommodate step in this instance there was no need for dropping. dropping tends to occur when some of the variables are highly correlated. lasso and degrees of freedom we see in figure panel that forward-stepwise regression is more aggressive than the lasso in that it brings down the training mse faster. we can use the covariance formula for df from chapter to quantify the amount of fitting at each step. in the right panel we show the results of a simulation for estimating the df of forward-stepwise regression and the lasso for the spam data. recall the covariance formula cov.yi oyi nx df d these covariances are of course with respect to the sampling distribution of the yi which we do not have access to since these are real data. so instead we simulate from fitted values from the full least-squares fit by adding gaussian errors with the appropriate standard deviation. is the parametric bootstrap calculation it turns out that each step of the lar algorithm spends one df as is evidenced by the brown curve in the right plot of figure forward stepwise spends more df in the earlier stages and can be erratic. under some technical conditions on the x matrix guarantee that fitting generalized lasso models figure left training mean-squared error on the spam data for forward-stepwise regression and the lasso as a function of the size of the active set. forward stepwise is more aggressive than the lasso in that it the training data more quickly. right simulation showing the degrees of freedom or df of forward-stepwise regression versus lasso. the lasso uses one df per step while forward stepwise is greedier and uses more especially in the early steps. since these df were computed using random simulated data sets we include standard-error bands on the estimates. lar delivers the lasso path one can show that the df is exactly one per step. more generally for the lasso if we definecdf d j of the active set at we have that e cdf d df in other words the size size of the active set is an unbiased estimate of df. ordinary least squares with a predetermined sequence of variables spends one df per variable. intuitively forward stepwise spends more because it pays a price some extra df for searching. although the lasso does search for the next variable it does not fit the new model all the way but just until the next variable enters. at this point one new df has been spent. fitting generalized lasso models so far we have focused on the lasso for squared-error loss and exploited the piecewise-linearity of its coefficient profile to efficiently compute the entire path. unfortunately this is not the case for most other loss functions training datasteptraining msellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward stepwiselasso sparse modeling and the lasso so obtaining the coefficient path is potentially more costly. as a case in point we will use logistic regression as an example in this case in l represents the negative binomial log-likelihood. writing the loss explicitly and using the lagrange form for the penalty we wish to solve nx yi log c yi minimize here we assume the yi and are the fitted probabilities n c d e c e i i similar to the solution satisfies the subgradient condition hxj y d j d p n where sj sign. j j d p and d however the nonlinearity of in j results in piecewise nonlinear coefficient profiles. instead we settle for a solution path on a sufficiently fine grid of values for it is once again easy to see that the largest value of we need consider is d max jhxj y j since this is the smallest value of for which o d logit. ny. a reasonable sequence is values equally spaced on the log-scale from down to where is some small fraction such as d and o an approach that has proven to be surprisingly efficient is path-wise coordinate descent. for each value solve the lasso problem for one j only holding all the others fixed. cycle around until the estimates stabilize. by starting at where all the parameters are zero we use warm starts in computing the solutions at the decreasing sequence of values. the warm starts provide excellent initializations for the sequence of solutions o the active set grows slowly as decreases. computational hedges that guess the active set prove to be particularly efficient. if the guess is good correct one iterates coordinate descent using only those variables pn yi d n pn the equation for the intercept is n fitting generalized lasso models until convergence. one more sweep through all the variables confirms the hunch. the r package glmnet employs a proximal-newton strategy at each value compute a weighted least squares approximation to the loglikelihood l at the current estimate for the solution vector o this produces a working response and observation weights as in a regular glm. solve the weighted least-squares lasso at by coordinate descent using warm starts and active-set iterations. we now give some details which illustrate why these particular strate gies are effective. consider the weighted least-squares problem wi x i c j minimize p with all but j fixed at their current values. writing ri d zi j xi we can recast as wi xij j c jj nx nx minimize j nx n a one-dimensional problem. the subgradient equation is wi xij xij j sign. j d the simplest form of the solution occurs if each variable is standardized to have weighted mean zero and variance one and the weights sum to one in that case we have a two-step solution. compute the weighted simple least-squares coefficient q j d hxj riw d nx j to produce o soft-threshold q j o j d sign. j q q if j q j jj otherwise wi xij ri sparse modeling and the lasso without the standardization the solution is almost as simple but less intuitive. hence each coordinate-descent update essentially requires an inner product followed by the soft thresholding operation. this is especially convenient for xij that are stored in sparse-matrix format since then the inner products need only visit the nonzero values. if the coefficient is zero before the step and remains zero one just moves on otherwise the model is updated. moving from the solution at which jhxj riwj d for all the nonzero coefficients o j down to the smaller one might expect all variables for which jhxj riwj would be natural candidates for the new active set. the strong rules lower the bar somewhat and include any variables for which jhxj riwj this tends to rarely make mistakes and still leads to considerable computational savings. apart from variations in the loss function other penalties are of interest as well. in particular the elastic net penalty bridges the gap between the lasso and ridge regression. that penalty is defined as c k p d where the factor in the first term is for mathematical convenience. when the predictors are excessively correlated the lasso performs somewhat poorly since it has difficulty in choosing among the correlated cousins. like ridge regression the elastic net shrinks the coefficients of correlated variables toward each other and tends to select correlated variables in groups. in this case the co-ordinate descent update is almost as simple as in o j d sign. q j q if j q j otherwise again assuming the observations have weighted variance equal to one. when d the update corresponds to a coordinate update for ridge regression. figure compares lasso with forward-stepwise logistic regression on the spam data here using all binarized variables and their pairwise interactions. this amounts to variables in all once degenerate variables have been excised. forward stepwise takes a long time to run since it enters one variable at a time and after each one has been selected a new glm must be fit. the lasso path as fit by glmnet includes many new variables at each step and is extremely fast s for the entire path. for very large post-selection inference for the lasso figure test misclassification error for lasso versus forward-stepwise logistic regression on the spam data where we consider pairwise interactions as well as main effects predictors in all. here the minimum error for lasso is versus for stepwise logistic regression and for the main-effects-only lasso logistic regression model. the stepwise models went up to variables before encountering convergence issues while the lasso had a largest active set of size and wide modern data sets of examples and millions of variables the lasso path algorithm is feasible and attractive. post-selection inference for the lasso this chapter is mostly about building interpretable models for prediction with little attention paid to inference indeed inference is generally difficult for adaptively selected models. which ends up selecting a subset a of size j a suppose we have fit a lasso regression model with a particular value for j d k of the p available variables. the question arises as to whether we can assign p-values to these selected variables and produce confidence intervals for their coefficients. a recent burst of research activity has made progress on these important problems. we give a very brief survey here with references ap data with interactionspercentage null deviance explained on training datatest misclassification errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward stepwise logistic regressionlasso logistic regression sparse modeling and the lasso pearing in the notes. we discuss post-selection inference more generally in chapter one question that arises is whether we are interested in making inferences about the population regression parameters using the full set of p predictors or whether interest is restricted to the population regression parameters using only the subset a. for the first case it has been proposed that one can view the coefficients of the selected model as an efficient but biased estimate of the full population coefficient vector. the idea is to then debias this estimate allowing inference for the full vector of coefficients. of course sharper inference will be available for the stronger variables that were selected in the first place. figure hiv data. linear regression of drug resistance in hiv-positive patients on seven sites indicators of mutations at particular genomic locations. these seven sites were selected from a total of candidates using the lasso. the naive confidence intervals use standard linear-regression inference ignoring the selection event. the light intervals are confidence intervals using linear regression but conditioned on the selection event. for the second case the idea is to condition on the selection events and hence the set a itself and then perform conditional inference on the intervalselection adjusted interval connections and extensions unrestricted not lasso-shrunk regression coefficients of the response on only the variables in a. for the case of a lasso with squared-error loss it turns out that the set of response vectors y rn that would lead to a particular subset a of variables in the active set form a convex polytope in rn we condition on the signs of the coefficients as well ignoring the signs leads to a finite union of such polytopes. this along with delicate gaussian conditioning arguments leads to truncated gaussian and t-distrubtions for parameters of interest. figure shows the results of using the lasso to select variables in an hiv study. the outcome y is a measure of the resistence to an treatment reverse transcriptase inhibitor and the predictors are indicators of whether mutations had occurred at particular genomic sites. lasso regression with cross-validation selected a value of d and the seven sites indicated in the figure had nonzero coefficients. the dark bars in the figure indicate standard confidence intervals for the coefficients of the selected variables using linear regression and ignoring the fact that the lasso was used to select the variables. three variables are significant and two more nearly so. the lighter bars are confidence intervals in a similar regression but conditioned on the selection event. we see that they are generally wider and only variable remains significant. connections and extensions there are interesting connections between lasso models and other popular approaches to the prediction problem. we will briefly cover two of these here namely support-vector machines and boosting. lasso logistic regression and the svm we show in section that ridged logistic regression has a lot in common with the linear support-vector machine. for separable data the limit as in ridged logistic regression coincides with the svm. in addition their loss functions are somewhat similar. the same holds true for regularized logistic regression versus the svm their end-path limits are the same. in fact due to the similarity of the loss functions their solutions are not too different elsewhere along the path. however the end-path behavior is a little more complex. they both converge to the maximizing margin separator that is the margin is measured with respect to the distance of points to the decision boundary or maximum absolute coordinate. sparse modeling and the lasso lasso and boosting in chapter we discuss boosting a general method for building a complex prediction model using simple building components. in its simplest form boosting amounts to the following simple iteration inititialize b d and f wd for b d b compute the residuals ri d yi f i d n fit a small regression tree to the observations ri update f b.x d f c gb.x. the smallness of the tree limits the interaction order of the model a tree with only two splits involves at most two variables. the number of terms b and the shrinkage parameter are both tuning parameters that control the rate of learning hence overfitting and need to be set for example by cross-validation. think of as estimating a function gb.x and which we can in words this algorithm performs a search in the space of trees for the one most correlated with the residual and then moves the fitted function f b a small amount in that direction a process known as forward-stagewise fitting. one can paraphrase this simple algorithm in the context of linear regression where in step the space of small trees is replaced by linear functions. inititialize d and standardize all the variables xj j d p. for b d b compute the residuals r d y x b find the predictor xj most correlated with the residual vector r and c sj being the sign of d b update b to where the correlation leaving all the other components alone. j j for small the solution paths for this least-squares boosting and the lasso are very similar. it is natural to consider the limiting case or infinitesimal forward stagewise fitting which we will abbreviate ifs. one can imagine a scenario where a number of variables are vying to win the competition in step and once they are tied their coefficients move in concert as they each get incremented. this was in fact the inspiration for the lar algorithm where a represents the set of tied variables and is the relative number of turns they each have in getting their coefficients updated. it turns out that ifs is often but not always exactly the lasso it can instead be characterized as a type of monotone lasso. notes and details not only do these connections inspire new insights and algorithms for the lasso they also offer insights into boosting. we can think of boosting as fitting a monotone lasso path in the high-dimensional space of variables defined by all possible trees of a certain size. extensions of the lasso the group lasso penaltypk the idea of using regularization to induce sparsity has taken hold and variations of these ideas have spread like wildfire in applied statistical modeling. along with advances in convex optimization hardly any branch of applied statistics has been left untouched. we don t go into detail here but refer the reader to the references in the endnotes. instead we will end this section with a list of such applications which may entice the reader to venture into this domain. applies to vectors of parameters and selects whole groups at a time. armed with these penalties one can derive lasso-like schemes for including multilevel factors in linear models as well as hierarchical schemes for including low-order interactions. the graphical lasso applies penalties in the problem of edge selection in dependence graphs. sparse principal components employ penalties to produce components with many loadings zero. the same ideas are applied to discriminant analysis and canonical correlation analysis. the nuclear norm of a matrix is the sum of its singular values a lasso penalty on matrices. nuclear-norm regularization is popular in matrix completion for estimating missing entries in a matrix. notes and details classical regression theory aimed for an unbiased estimate of each predictor variable s effect. modern wide data sets often with enormous numbers of predictors p make that an untenable goal. the methods described here by necessity use shrinkage methods biased estimation and sparsity. the lasso was introduced by tibshirani and has spawned a great deal of research. the recent monograph by hastie et al. gives a compact summary of some of the areas where the lasso and sparsity have been applied. the regression version of boosting was given in hastie et al. chapter and inspired the least-angle regression algorithm sparse modeling and the lasso et al. a new and more democratic version of forward-stepwise regression as well as a fast algorithm for fitting the lasso. these authors showed under some conditions that each step of the lar algorithm corresponds to one df zou et al. show that with a fixed the size of the active set is unbiased for the df for the lasso. hastie et al. also view boosting as fitting a lasso regularization path in the high-dimensional space of trees. friedman et al. developed the pathwise coordinate-descent algorithm for generalized lasso problems and provide the glmnet package for r et al. strong rules for lasso screening are due to tibshirani et al. hastie et al. chapter show the similarity between the svm and lasso logistic regression. we now give some particular technical details on topics covered in the chapter. forward-stepwise computations. building up the forward-stepwise model can be seen as a guided gram schmidt orthogonalization decomposition. after step r all p r variables not in the model are orthogonal to the r in the model and the latter are in qr form. then the next variable to enter is the one most correlated with the residuals. this is the one that will reduce the residual sum-of-squares the most and one requires p r n-vector inner products to identify it. the regression is then updated trivially to accommodate the chosen one which is then regressed out of the p r remaining variables. iteratively reweighted least squares generalized linear models are fit by maximum-likelihood and since the log-likelihood is differentiable and concave typically a newton algorithm is used. the newton algorithm can be recast as an iteratively reweighted linear regression algorithm and nelder at each iteration one computes a working response variable zi and a weight per observation wi of which depend on the current parameter vector o then the newton update for o is obtained by a weighted least-squares fit of the zi on the xi with weights wi et al. section forward-stepwise logistic regression computations. although the current model is in the form of a weighted least-squares fit the p r variables not in the model cannot be kept orthogonal to those in the model weights keep changing!. however since our current model will have performed a weighted qr decomposition this orthogonalization can be obtained without too much cost. we will need p r multiplications of an r n matrix with an n vector o..p r r n computations. an even simpler alternative for the selection is to use the size of the gradient of the notes and details log-likelihood which simply requires an inner product jhy xjij for each omitted variable xj all the variables are standardized to unit variance. best interpolant. if p n then another boundary solution becomes interesting for the lasso. for t sufficiently large we will be able to achieve a perfect fit to the data and hence a zero residual. there will be many such solutions so it becomes interesting to find the perfect-fit solution with smallest value of t the perfect-fit solution. this requires solving a separate convex-optimization problem. more on df. when the search is easy in that a variable stands out as far superior lar takes a big step and forward stepwise spends close to a unit df. on the other hand when there is close competition the lar steps are small and a unit df is spent for little progress while forward stepwise can spend a fair bit more than a unit df price paid for searching. in fact the dfj curve for forward stepwise can exceed p for j p et al. post-selection inference. there has been a lot of activity around post-selection inference for lasso and related methods all of it since to a large extent this was inspired by the work of berk et al. but more tailored to the particular selection process employed by the lasso. for the debiasing approach we look to the work of zhang and zhang van de geer et al. and javanmard and montanari the conditional inference approach began with lockhart et al. and then was developed further in a series of papers et al. taylor et al. fithian et al. with many more in the pipeline. selective inference software. the example in figure was produced using the r package selectiveinference et al. thanks to rob tibshirani for providing this example. end-path behavior of ridge and lasso logistic regression for separable data. the details here are somewhat technical and rely on dual norms. details are given in hastie et al. section lar and boosting. least-squares boosting moves the winning coefficient in the direction of the correlation of its variable with the residual. the direction computed in step of the lar algorithm may have some components whose signs do not agree with their correlations especially if the variables are very correlated. this can be fixed by a particular nonnegative least-squares fit to yield an exact path algorithm for ifs details can be found in efron et al. random forests and boosting in the modern world we are often faced with enormous data sets both in terms of the number of observations n and in terms of the number of variables p. this is of course good news we have always said the more data we have the better predictive models we can build. well we are there now we have tons of data and must figure out how to use it. although we can scale up our software to fit the collection of linear and generalized linear models to these behemoths they are often too modest and can fall way short in terms of predictive power. a need arose for some general purpose tools that could scale well to these bigger problems and exploit the large amount of data by fitting a much richer class of functions almost automatically. random forests and boosting are two relatively recent innovations that fit the bill and have become very popular as out-thebox learning algorithms that enjoy good predictive performance. random forests are somewhat more automatic than boosting but can also suffer a small performance hit as a consequence. these two methods have something in common they both represent the fitted model by a sum of regression trees. we discuss trees in some detail in chapter a single regression tree is typically a rather weak prediction model it is rather amazing that an ensemble of trees leads to the state of the art in black-box predictors! we can broadly describe both these methods very simply. random forest grow many deep regression trees to randomized versions of the training data and average them. here randomized is a wideranging term and includes bootstrap sampling andor subsampling of the observations as well as subsampling of the variables. boosting repeatedly grow shallow trees to the residuals and hence build up an additive model consisting of a sum of trees. the basic mechanism in random forests is variance reduction by averaging. each deep tree has a high variance and the averaging brings the vari random forests ance down. in boosting the basic mechanism is bias reduction although different flavors include some variance reduction as well. both methods inherit all the good attributes of trees most notable of which is variable selection. random forests suppose we have the usual setup for a regression problem with a training set consisting of an p data matrix x and an n-vector of responses y. a tree fits a piecewise constant surface or.x over the domain x by recursive partitioning. the model is built in a greedy fashion each time creating two daughter nodes from a terminal node by defining a binary split using one of the available variables. the model can hence be represented by a binary tree. part of the art in using regression trees is to know how deep to grow the tree or alternatively how much to prune it back. typically that is achieved using left-out data or cross-validation. figure shows a tree fit to the spam training data. the splitting variables and split points are indicated. each node is labeled as spam or ham spam see footnote on page the numbers beneath each node show misclassifiedtotal. the overall misclassification error on the test data is which compares poorly with the performance of the lasso for linear lasso for lasso with interactions. the surface or.x here is clearly complex and by its nature represents a rather high-order interaction deepest branch is eight levels and involves splits on eight different variables. despite the promise to deliver interpretable models this bushy tree is not easy to interpret. nevertheless trees have some desirable properties. the following lists some of the good and bad properties of trees. l trees automatically select variables only variables used in defining splits are in the model. l tree-growing algorithms scale well to large n growing a tree is a divide and-conquer operation. l trees handle mixed features seamlessly and can deal with missing data. l small trees are easy to interpret. m large trees are not easy to interpret. m trees do not generally have good prediction performance. trees are inherently high-variance function estimators and the bushier they are the higher the variance. the early splits dictate the architecture of random forests and boosting figure regression tree fit to the binary spam data a bigger version of figure the initial trained tree was far bushier than the one displayed it was then optimally pruned using cross-validation. the tree. on the other hand deep bushy trees localize the training data the variables that matter to a relatively small region around the target point. this suggests low bias. the idea of random forests its predecessor bagging is to grow many very bushy trees and get rid of the variance by averaging. in order to benefit from averaging the individual trees should not be too correlated. this is achieved by injecting some randomness into the tree-growing process. random forests achieve this in two ways. random forests bootstrap each tree is grown to a bootstrap resampled training data set which makes them different and somewhat decorrelates them. split-variable randomization each time a split is to be performed the search for the split variable is limited to a random subset of m of the p variables. typical values of m are p or p when m d p the randomization amounts to using only step and was an earlier ancestor of random forests called bagging. in most examples the second level of randomization pays dividends. b. algorithm random forest. given training data set d d y. fix m p and the number of trees for b d b do the following. create a bootstrap version of the training data d b by randomly sampling the n rows with replacement n times. the sample can be represented by the bootstrap frequency vector w grow a maximal-depth tree orb.x using the data in d of the p features at random prior to making each split. b sampling m b. save the tree as well as the bootstrap sampling frequencies for each of the training observations. compute the random-forest fit at any prediction point as the average bx d b compute the oobi error for each response observation yi in the training data by using the fit or obtained by averaging only those orb.xi for which observation i was not in the bootstrap sample. the overall oob error is the average of these oobi. rf algorithm gives some of the details some more are given in the technical notes. the package randomforest in r sets as a default m d p random forests are easy to use since there is not much tuning needed. p for classification trees and m d for regression trees but one can use other values. with m d the split variable is completely random so all variables get a chance. this will decorrelate the trees the most but can create bias somewhat similar to that in ridge regression. figure shows the random forests and boosting figure test misclassification error of random forests on the spam data as a function of the number of trees. the red curve selects m d of the p d features at random as candidates for the split variable each time a split is made. the blue curve uses m d and hence amounts to bagging. both bagging and random forests outperform the lasso methods and a single tree. misclassification performance of a random forest on the spam test data as a function of the number of trees averaged. we see that in this case after a relatively small number of trees the error levels off. the number b of trees averaged is not a real tuning parameter as with the bootstrap and we need a sufficient number for the estimate to stabilize but cannot overfit by having too many. random forests have been described as adaptive nearest-neighbor estimators adaptive in that they select predictors. a k-nearest-neighbor estimate finds the k training observations closest in feature space to the target point and averages their responses. each tree in the random forest drills down by recursive partitioning to pure terminal nodes often consisting of a single observation. hence when evaluating the prediction from each tree d y for some and for many of the trees this could be the same forest on the spam datanumber of treestest forestsingle treelassolasso random forests from the whole collection of b trees the number of distinct can be fairly small. since the partitioning that reaches the terminal nodes involves only a subset of the predictors the neighborhoods so defined are adaptive. out-of-bag error estimates random forests deliver cross-validated error estimates at virtually no extra cost. the idea is similar to the bootstrap error estimates discussed in chapter the computation is described in step of algorithm in making the prediction for observation pair yi we average all the random-forest trees orb.xi for which that pair is not in the corresponding bootstrap sample figure out-of-bag misclassification error estimate for the spam data versus the test error as a function of the number of trees. x b w w b i or rf d bi orb.xi where bi is the number of times observation i was not in the bootstrap we then compute the oob sample expected value e nx error estimate erroob d n l yi or rf of treesmisclassification errortest error random forests and boosting where l is the loss function of interest such as misclassification or squarederror loss. if b is sufficiently large three times the number needed for the random forest to stabilize we can see that the oob error estimate is equivalent to leave-one-out cross-validation error. standard errors is given by bvjack. we can use very similar ideas to estimate the variance of a random-forest prediction using the jackknife variance estimator in chapter if o is a statistic estimated using all n training observations then the jackknife estimate of the variance of o d n o is the estimate using all but observation i and o d at is obtained by simply plugging into this formula o o o where o the natural jackknife variance estimate for a random-forest prediction n i d n rf nx p n nx n this formula is derived under the b d setting in which case is an expectation under bootstrap sampling and hence is free of monte carlo variability. this also makes the distinction clear we are estimating the sampling variability of a random-forest prediction as distinct from any monte carlo variation. in practice b is finite and expression will have monte carlo bias and variance. all of the or rf are based on b bootstrap samples and they are hence noisy versions of their expectations. since the n quantities summed in are squared by jensen s inequality we will have positive bias it turns out that this bias dominates the monte carlo variance. hence one would want to use a much larger value of b when estimating variances than was used in the original randomforest fit. alternatively one can use the same b bootstrap samples as were used to fit the random forest along with a bias-corrected version of the jackknife variance estimate bvu n b random forests figure jackknife standard error estimates bias correction for the probability estimates in the spam test data. the points labeled red were misclassifications and tend to concentrate near the decision boundary where e d and d b bx the bootstrap estimate of the variance of a single random-forest tree. all these quantities are easily computed from the output of a random forest so they are immediately available. figure shows the predicted probabilities and their jackknife estimated standard errors for the spam test data. the estimates near the decision boundary tend to have higher standard errors. variable-importance plots a random forest is something of a black box giving good predictions but usually not much insight into the underlying surface it has fit. each random-forest tree orb will have used a subset of the predictors as splitting variables and each tree is likely to use overlapping but not necessarily predictionstandard error estimate random forests and boosting figure variable-importance plots for random forests fit to the spam data. on the left we have the m d random forest due to the split-variable randomization it spreads the importance among the variables. on the right is the m d random forest or bagging which focuses on a smaller subset of the variables. identical subsets. one might conclude that any variable never used in any of the trees is unlikely to be important but we would like a method of assessing the relative importance of variables that are included in the ensemble. variable-importance plots fit this bill. whenever a variable is used in a tree the algorithm logs the decrease in the split-criterion due to this split. these are accumulated over all the trees for each variable and summarized as relative importance measures. figure demonstrates this on the spam data. we see that the m d random forest by virtue of the split-variable randomization spreads the importance out much more than bagging which always gets to pick the best variable for splitting. in this sense small m has some similarity to ridge regression which also tends to share the coefficients evenly among correlated variables. forest m m importance boosting with squared-error loss boosting with squared-error loss boosting was originally proposed as a means for improving the performance of weak learners in binary classification problems. this was achieved through resampling training points giving more weight to those which had been misclassified to produce a new classifier that would boost the performance in previously problematic areas of feature space. this process is repeated generating a stream of classifiers which are ultimately combined through to produce the final classifier. the prototypical weak learner was a decision tree. boosting has evolved since this earliest invention and different flavors are popular in statistics computer science and other areas of pattern recognition and prediction. we focus on the version popular in statistics gradient boosting and return to this early version later in the chapter. algorithm algorithm gradient boosting with squared-error loss. shrinkage factor and the tree depth d. set the initial fit and given a training sample d d y. fix the number of steps b the the residual vector r d y. for b d b repeat fit a regression tree qgb to the data r grown best-first to depth d this means the total number of splits are d and each successive update the fitted model with a shrunken version of qgbbgb c split is made to that terminal node that yields the biggest reduction in residual sum of squares. ogb with ogb d qgb return the sequence of fitted functionsbgb b d b. update the residuals accordingly ri d ri ogb.xi i d n gives the most basic version of gradient boosting for squared-error loss. this amounts to building a model by repeatedly fitting a regression tree to the residuals. importantly the tree is typically quite small involving a small number d of splits it is indeed a weak learner. after each tree has been grown to the residuals it is shrunk down by a factor before it is added to the current model this is a means of slowing the learning process. despite the obvious similarities with a random forest boosting is different in a fundamental way. the trees in a random forest are identically each classifier predicts a class label and the class with the most votes wins. random forests and boosting figure test performance of a boosted regression-tree model fit to the als training data with n d and p d shown is the mean-squared error on the designated test observations as a function of the number of trees. here the depth d d and d boosting achieves a lower test mse than a random forest. we see that as the number of trees b gets large the test error for boosting starts to increase a consequence of overfitting. the random forest does not overfit. the dotted blue horizontal line shows the best performance of a linear model fit by the lasso. the differences are less dramatic than they appear since the vertical scale does not extend to zero. distributed the same treatment is repeatedly applied to the same data. with boosting on the other hand each tree is trying to amend errors made by the ensemble of previously grown trees. the number of terms b is important as well because unlike random forests a boosted regression model can overfit if b is too large. hence there are three tuning parameters b d and and each can change the performance of a boosted model sometimes considerably. figure shows the test performance of boosting on the als data. these data represent measurements on patients with amyotrophic lateral sclerosis gehrig s disease. the goal is to predict the rate of progression of an als functional rating score there are training of treesmean squared forestlasso boosting with squared-error loss measurements on predictors and the response with a corresponding test set of size observations. as is often the case boosting slightly outperforms a random forest here but at a price. careful tuning of boosting requires considerable extra work with time-costly rounds of cross-validation whereas random forests are almost automatic. in the following sections we explore in more detail some of the tuning parameters. the r package gbm implements gradient boosting with some added bells and whistles. by default it grows each new tree on a random sub-sample of the training data. apart from speeding up the computations this has a similar effect to bagging and results in some variance reduction in the ensemble. we can also compute a variable-importance plot as we did for random forests this is displayed in figure for the als data. only of the variables were ever used with one variable onset.delta standing out ahead of the others. this measures the amount of time that has elapsed since the patient was first diagnosed with als and hence a larger value will indicate a slower progression rate. tree depth and interaction order tree depth d is an important parameter for gradient boosted models and the right choice will depend on the data at hand. here depth d d appears to be a good choice on the test data. without test data we could use crossvalidation to make the selection. apart from a general complexity measure suppose we have a fitted boosted modelbgb using b trees. denote by tree depth also controls the interaction order of the the easiest case is with d d where each tree consists of a single split stump. d bg the indices of the trees that made the single split bj using variable j for j d p. these bj are disjoint b can be b a interaction is also known as a k-way interaction. hence an order-one interaction model has two-way interactions and an order-zero model is additive. random forests and boosting figure variable importance plot for the als data. here of the variables were used in the ensemble. there are too many variables for the labels to be visible so this plot serves as a visual guide. variable onset.delta has relative importance lowest red bar more than double the next two at around and alsfrs.score.slope. however the importances drop off slowly suggesting that the model requires a significant fraction of the variables. empty andsp bj d b. then we can write ogb.x x bgb d bx d px d px ogb.x bj o fj importance plot for boosting on the als data boosting with squared-error loss figure als test error for boosted models with different depth parameters d and all using the same shrinkage parameter d it appears that d d is inferior to the rest with d d about the best. with d d overfitting begins around trees with d d around while neither of the other two show evidence of overfitting by trees. hence boosted stumps fits an additive model but in a fully adaptive way. it selects variables and also selects how much action to devote to each variable. we return to additive models in section figure shows the three functions with highest relative importance. the first function confirms that a longer time since diagnosis negative onset.delta predicts a slower decline. last.slope.weight is the difference in body weight at the last two visits again positive is good. likewise for alsfrs.score.slope which measures the local slope of the frs score after the first two visits. in a similar way boosting with d d fits a two-way interaction model each tree involves at most two variables. in general boosting with d d k leads to a interaction model. interaction order is perhaps a more natural way to think of model complexity. of treesmean squared random forests and boosting figure three of the fitted functions for the als data in a boosted stumps model d each centered to average zero over the training data. in terms of the outcome bigger is better decline in frs. the first function confirms that a longer time since diagnosis negative value of onset.delta predicts a slower decline. the variable last.slope.weight is the difference in body weight at the last two visits again positive is good. likewise for alsfrs.score.slope which measures the local slope of the frs score after the first two visits. shrinkage the shrinkage parameter controls the rate at which boosting fits and hence overfits the data. figure demonstrates the effect of shrinkage on the als data. the under-shrunk ensemble quickly overfits the data leading to poor validation error. the blue ensemble uses a shrinkage parameter times smaller and reaches a lower validation error. the downside of a very small shrinkage parameter is that it can take many trees to adequately fit the data. on the other hand the shrunken fits are smoother take much longer to overfit and hence are less sensitive to the stopping point b. gradient boosting we now turn our attention to boosting models using other than square-error loss. we focus on the family of generalized models generated by the exponential family of response distributions chapter the most popular and relevant in this class is logistic regression where we are interested in modeling d pr.y d d x for a bernoulli response variable. function function function gradient boosting figure boosted d d models with different shrinkage parameters fit to a subset of the als data. the solid curves are validation errors the dashed curves training errors with red for d and blue for d with d the training error drops rapidly with the number of trees but the validation error starts to increase rapidly after an initial decrease. with d times smaller the training error drops more slowly. the validation error also drops more slowly but reaches a lower minimum horizontal dotted line than the d case. in this case the slower learning has paid off. the idea is to fit a model of the form d gb d bx gb.xi where is the natural parameter in the conditional distribution of y jx d x and the gb.xi are simple functions such as shallow trees. here we have indexed each function by a parameter vector for trees these would capture the identity of the split variables their split values and the constants in the terminal nodes. in the case of the bernoulli response we have d d x pr.y d d x d log of treesmean squared random forests and boosting the logit link function that relates the mean to the natural parameter. in general if d e.y jx d x is the conditional mean we have d where is the monotone link function. algorithm outlines a general strategy for building a model by forward stagewise fitting. l is the loss function such as the negative loglikelihood for bernoulli responses or squared-error for gaussian responses. although we are thinking of trees for the simple functions g.xi the ideas generalize. this algorithm is easier to state than to implement. for algorithm generalized boosting by forward-stagewise fitting define the class of functions g.xi start with d and set b and the shrinkage parameter for b d b repeat the following steps. solve nx yi c g.xii l updatebgb.x c ogb.x with ogb.x d g.xi return the sequencebgb.x b d b. d arg min squared-error loss at each step we need to solve nx minimize g.xii with ri d yi i d n. if represents a depth-d tree is still difficult to solve. but here we can resort to the usual greedy heuristic and grow a depth-d tree to the residuals by the usual top-down splitting as in step of algorithm hence in this case we have exactly the squared-error boosting algorithm for more general loss functions we rely on one more heuristic for solving step inspired by gradient descent. algorithm gives the details. the idea is to perform functional gradient descent on the loss function in the n-dimensional space of the fitted vector. however we want to be able to evaluate our new function everywhere not just at the n original values xi. hence once the gradient vector has been computed it is approximated by a depth-d tree can be evaluated everywhere. taking a step of length down adaboost the original boosting algorithm the gradient amounts to adding times the tree to the current function. gradient boosting is quite general and can be used with any differentiable algorithm gradient boosting start with d and set b and the shrinkage parameter for b d b repeat the following steps. compute the pointwise negative gradient of the loss function at the current fit ri d nx minimize g.xii i d n approximate the negative gradient by a depth-d tree by solving update ogb.x d c ogb.x with ogb.x d g.xi return the sequence ogb.x b d b. loss function. the r package gbm implements algorithm for a variety of loss functions including squared-error binomial laplace loss multinomial and others. included as well is the partial likelihood for the cox proportional hazards model figure compares the misclassification error of boosting on the spam data with that of random forests and bagging. since boosting has more tuning parameters a careful comparison must take these into account. using the mcnemar test we would conclude that boosting and random forest are not significantly different from each other but both outperform bagging. adaboost the original boosting algorithm the original proposal for boosting looked quite different from what we have presented so far. adaboost was developed for the two-class classification problem where the response is coded as the idea was to fit a sequence of classifiers to modified versions of the training data where the modifications give more weight to misclassified points. the final classification is by weighted majority vote. the details are rather specific and are given in algorithm here we distinguish a classifier c.x which returns a class label rather than a probability. algorithm gives random forests and boosting figure test misclassification for gradient boosting on the spam data compared with a random forest and bagging. although boosting appears to be better it requires crossvaldiation or some other means to estimate its tuning parameters while the random forest is essentially automatic. the algorithm. although the classifier in step can be arbitrary it was intended for weak learners such as shallow trees. steps look mysterious. its easy to check that with the reweighted points the classifier ocb just learned would have weighted error that of a coin flip. ues the ensemblebgb.x takes values in r. we also notice that although the individual classifiers ocb.x produce valponential loss function. the functions bgb.x output in step of algo it turns out that the adaboost algorithm fits a logistic regression model via a version of the general boosting algorithm using an ex rithm are estimates of the logit function to show this we first motivate the exponential loss a somewhat unusual choice and show how it is linked to logistic regression. for a response y and function f the exponential loss is defined as le f d exp a simple calculation shows that the solution to the boosting on the spam datanumber of treestest forestboosting adaboost the original boosting algorithm wi. algorithm adaboost initialize the observation weights wi d i d n. for b d b repeat the following steps. fit a classifier ocb.x to the training data using observation weights compute the weighted misclassification error for ocb pn wi i yi ocb.xi wi compute b d log errberrb exp b i yi cb.xi update the weights wi wi output the sequence of functions bgb.x d pb i sponding classifiersbcb.x d sign b d b. i d m oc.x and corre errb dpn hbgb.x n. tional population minimization problem j x e e minimize f d pr.y d f d log is given by inverting we get pr.y d d ef e and pr.y d d e e c ef c ef a perfectly reasonable symmetric model for a probability. the quantity yf is known as the margin also chapter if the margin is positive the classification using cf d sign.f is correct for y else it is incorrect if the margin is negative. the magnitude of yf is proportional to the distance of x from the classification boundary for linear models approximately otherwise. for data we can also write the binomial log-likelihood in terms of the margin. random forests and boosting using we have lb f d d log pr.y d c i.y d log pr.y d c e d log c e j also has population minimizer f equal to half the logit figure compares the exponential loss function with this binomial loss. they both asymptote to zero in the right tail the area of correct classification. in the left tail the binomial loss asymptotes to a linear function much less severe than the exponential loss. figure exponential loss used in adaboost versus the binomial loss used in the usual logistic regression. both estimate the logit function. the exponential left tail which punishes misclassifications is much more severe than the asymptotically linear tail of the binomial. the exponential loss simplifies step in the gradient boosting algo the half comes from the symmetric representation we use. rithm yi c g.xii le nx connections and extensions d nx d nx d nx exp c g.xii wi exp g.xii wi le g.xii with wi d exp this is just a weighted exponential loss with the past history encapsulated in the observation weight wi step in algorithm we give some more details in the chapter endnotes on how this reduces to the adaboost algorithm. the adaboost algorithm achieves an error rate on the spam data com parable to binomial gradient boosting. connections and extensions boosting is a general nonparametric function-fitting algorithm and shares attributes with a variety of existing methods. here we relate boosting to two different approaches generalized additive models and the lasso of chapter generalized additive models boosting fits additive low-order interaction models by a forward stagewise strategy. generalized additive models are a predecessor a semi-parametric approach toward nonlinear function fitting. a gam has the form fj where again d is the natural parameter in an exponential family. the attraction of a gam is that the components are interpretable and can be visualized and they can move us a big step up from a linear model. there are many ways to specify and fit additive models. for the fj we could use parametric functions polynomials fixed-knot regression splines or even linear functions for some terms. less parametric options d px random forests and boosting are smoothing splines and local regression section in the case of squared-error loss gaussian case there is a natural set of backfitting equations for fitting a gam o fj of j d p sj o f.xn o j here of d is the n-vector of fitted values for the current estimate of function f. hence the term in parentheses is a partial residual removing all the current function fits from y except the one about to be updated. sj is a smoothing operator derived from variable xj that gets applied to this residual and delivers the next estimate for function f. backfitting starts with all the functions zero and then cycles through these equations for j d p in a block-coordinate fashion until all the functions stabilize. the first pass through all the variables is similar to the regression boosting algorithm where each new function takes the residuals from the past fits and models them using a tree sj the difference is that boosting never goes back and fixes up past functions but fits in a forwardstagewise fashion leaving all past functions alone. of course with its adaptive fitting mechanism boosting can select the same variables as used before and thereby update that component of the fit. boosting with stumps trees see the discussion on tree depth on in section can hence be seen as an adaptive way for fitting an additive model that simultaneously performs variable selection and allows for different amounts of smoothing for different variables. boosting and the lasso in section we drew attention to the close connection between the forward-stagewise fitting of boosting shrinkage and the lasso via infinitesimal forward-stagewise regression. here we take this a step further by using the lasso as a post-processor for boosting random forests. boosting with shrinkage does a good job in building a prediction model but at the end of the day can involve a lot of trees. because of the shrinkage many of these trees could be similar to each other. the idea here is to use the lasso to select a subset of these trees reweight them and hence produce a prediction model with far fewer trees and one hopes comparable accuracy. suppose boosting has produced a sequence of fitted trees ogb.x b d b. we then solve the lasso problem notes and details figure post-processing of the trees produced by boosting on the als data. shown is the test prediction error as a function of the number of trees selected by the lasso. we see that the lasso can do as good a job with one-third the number of trees although selecting the correct number is critical. nx bx bx minimize f bgb l yi ogb.xi b c j bj for different values of this model selects some of the trees and assigns differential weights to them. a reasonable variant is to insist that the weights are nonnegative. figure illustrates this approach on the als data. here we could use one-third of the trees. often the savings are much more dramatic. notes and details random forests and boosting live at the cutting edge of modern prediction methodology. they fit models of breathtaking complexity compared with classical linear regression or even with standard glm modeling as practiced in the late twentieth century they are routinely used as prediction engines in a wide variety of industrial and scientific applications. for the more cautious they provide a terrific benchmark for how well a traditional parametrized model is performing if the random forests of treesmean squared boostlasso post fit random forests and boosting does much better you probably have some work to do by including some important interactions and the like. the regression and classification trees discussed in chapter et al. took traditional models to a new level with their ability to adapt to the data select variables and so on. but their prediction performance is somewhat lacking and so they stood the risk of falling by the wayside. with their new use as building blocks in random forests and boosting they have reasserted themselves as critical elements in the modern toolbox. random forests and bagging were introduced by breiman and boosting by schapire and freund and schapire there has been much discussion on why boosting works friedman et al. schapire and freund the statistical interpretation given here can also be found in hastie et al. and led to the gradient boosting algorithm adaboost was first described in freund and schapire hastie et al. chapter is devoted to random forests. for the examples in this chapter we used the randomforest package in r and wiener and for boosting the gbm package. the lasso post-processing idea is due to friedman and popescu which we implemented using glmnet et al. generalized additive models are described in hastie and tibshirani we now give some particular technical details on topics covered in the chapter. averaging trees. a maximal-depth tree splits every node until it is pure meaning all the responses are the same. for very large n this might be unreasonable in practice one can put a lower bound on the minimum count in a terminal node. we are deliberately vague about the response type in algorithm if it is quantitative we would fit a regression tree. if it is binary or multilevel qualitative we would fit a classification tree. in this case at the averaging stage there are at least two strategies. the original random-forest paper proposed that each tree should make a classification and then the ensemble uses a plurality vote. an alternative reasonable strategy is to average the class probabilities produced by the trees these procedures are identical if the trees are grown to maximal depth. jackknife variance estimate. the jackknife estimate of variance for a random forest and the bias-corrected version is described in wager et al. the jackknife formula is applied to the b d ver notes and details sion of the random forest but of course is estimated by plugging in finite b versions of the quantities involved. replacing or rf by its expectation is not the problem its that each of the or rf vary about their bootstrap expectations compounded by the square in expression calculating the bias requires some technical derivations which can be found in that reference. they also describe the infinitesimal jackknife estimate of variance given by withdcovi ddcov.w d nx i bx as discussed in chapter it too has a bias-corrected version given by b b i d bvu n b similar to the als data. these data were kindly provided by lester mackey and lilly fang who won the dream challenge prediction prize in et al. it includes some additional variables created by them. their winning entry used bayesian trees not too different from random forests. gradient-boosting details. in friedman s gradient-boosting algorithm et al. chapter for example a further refinement is implemented. the tree in step of algorithm is used to define the structure variables and splits but the values in the terminal nodes are left to be updated. we can think of partitioning the parameters d and then represent the tree as g.xi d t here t is a vector of d c binary basis functions that indicate the terminal node reached by input x and are the d c values of the terminal nodes of the tree. we learn by approximating the gradient in step by a tree and then the terminal-node parameters by solving the optimization problem yi c t nx minimize l solving amounts to fitting a simple glm with an offset. random forests and boosting adaboost and gradient boosting. hastie et al. chapter derive adaboost as an instance of algorithm one detail is that the trees g.xi are replaced by a simplified scaled classifier c.xi hence from in step of algorithm we need to solve wi exp c.xii nx minimize the derivation goes on to show that minimizing for any value of can be achieved by fitting to minimize the weighted misclassification a classification tree c.xi nx error wi i yi c.xi i given c.xi is estimated as in step of algorithm is non-negative the weight-update scheme in step of algorithm corresponds exactly to the weights as computed in neural networks and deep learning something happened in the mid that shook up the applied statistics community. neural networks were introduced and they marked a shift of predictive modeling towards computer science and machine learning. a neural network is a highly parametrized model inspired by the architecture of the human brain that was widely promoted as a universal approximator a machine that with enough data could learn any smooth predictive relationship. figure neural network diagram with a single hidden layer. the hidden layer derives transformations of the inputs nonlinear transformations of linear combinations which are then used to model the output. figure shows a simple example of a feed-forward neural network diagram. there are four predictors or inputs xj five hidden units a d a. the language associated with nns is colorful memory units or neurons automatically learn new features from the data through a process called xj and a single output unit o d neural networks supervised learning. each neuron al is connected to the input layer via a gp vector of parameters or weights refers to the first layer and refers to the j th variable and unit. the intercept terms are called a bias and the function g is a nonlinearity such as the sigmoid function g.t d c e the idea was that each neuron will learn a simple binary onoff function the sigmoid function is a smooth and differentiable compromise. the final or output layer also has weights and an output function h. for quantitative regression h is typically the identity function and for a binary response it is once again the sigmoid. note that without the nonlinearity in the hidden layer the neural network would reduce to a generalized linear model typically neural networks are fit by maximum likelihood usually with a variety of forms of regularization. the knee-jerk response from statisticians was what s the big deal? a neural network is just a nonlinear model not too different from many other generalizations of linear models. while this may be true neural networks brought a new energy to the field. they could be scaled up and generalized in a variety of ways many hidden units in a layer multiple hidden layers weight sharing a variety of colorful forms of regularization and innovative learning algorithms for massive data sets. and most importantly they were able to solve problems on a scale far exceeding what the statistics community was used to. this was part computing scale and expertise part liberated thinking and creativity on the part of this computer science community. new journals were devoted to the field and several popular annual conferences at ski resorts attracted their denizens and drew in members of the statistics community. after enjoying considerable popularity for a number of years neural networks were somewhat sidelined by new inventions in the mid such as boosting and svms neural networks were pass e. but then they re-emerged with a vengeance after the reincarnation now being called deep learning. this renewed enthusiasm is a result of massive improvements in computer resources some innovations and the ideal niche learning tasks such as image and video classification and speech and text processing. neural networks and the handwritten digit problem neural networks and the handwritten digit problem neural networks really cut their baby teeth on an optical character recognition task automatic reading of handwritten digits as in a zipcode. figure shows some examples taken from the mnist corpus. the idea is to build a classifier c.x based on the input image x a grid of image intensities. in fact as is often the case it is more useful to learn the probability function pr.y d jjx j d this is indeed the target for our neural network. figure figure examples of handwritten digits from the mnist corpus. each digit is represented by a grayscale image derived from normalized binary images of different shapes and sizes. the value stored for each pixel in an image is a nonnegative eight-bit representation of the amount of gray present at that location. the pixels for each image are the predictors and the class labels the response. there are training images in the full data set and in the test set. shows a neural network with three hidden layers a successful configuration for this digit classification problem. in this case the output layer has nodes one for each of the possible class labels. we use this example to walk the reader through some of the aspects of the configuration of a network and fitting it to training data. since all of the layers are functions of their previous layers and finally functions of the input vector x the network represents a somewhat complex function f w where w represents the entire collection of weights. armed with a suitable loss function we could simply barge right in and throw it at our favorite optimizer. in the early days this was not computationally feasible especially when special neural networks figure neural network diagram with three hidden layers and multiple outputs suitable for the mnist handwritten-digit problem. the input layer has p d units. such a network with hidden layer sizes and particular choices of tuning parameters achieves the state-of-the art error rate of on the official test data set. this network has close to four million weights and hence needs to be heavily regularized. structure is imposed on the weight vectors. today there are fairly automatic systems for setting up and fitting neural networks and this view is not too far from reality. they mostly use some form of gradient descent and rely on an organization of parameters that leads to a manageable calculation of the gradient. the network in figure is complex so it is essential to establish a convenient notation for referencing the different sets of parameters. we continue with the notation established for the single-layer network but with some additional annotations to distinguish aspects of different layers. from the first to the second layer we have xj c px d d neural networks and the handwritten digit problem we have separated the linear transformations of the xj from the nonlinear transformation of these and we allow for layer-specific nonlinear transformations g.k. more generally we have the transition from layer k to layer k c d d g.k.z.k z.k a.k j in fact can serve for the input layer if we adopt x and d p the number of input variables. the notation that hence each of the arrows in figure is associated with a weight parameter. it is simpler to adopt a vector notation z.k d w a.k d g.k.z.k where w represents the matrix of weights that go from layer to layer lk a.k is the entire vector of activations at layer lk and our notation assumes that g.k operates elementwise on its vector argument. we have also absorbed the bias parameters into the matrix w which assumes that we have augmented each of the activation vectors a.k with a constant element sometimes the nonlinearities g.k at the inner layers are the same function such as the function defined earlier. in section we present a network for natural color image classification where a number of different activation functions are used. depending on the response the final transformation g.k is usually special. for m classification such as here with m d one typically uses the softmax function g.k.z.k m i z.k d mpm ez.k ez.k which computes a number between zero and one and all m of them sum to this is a symmetric version of the inverse link function used for multiclass logistic regression. neural networks fitting a neural network as we have seen a neural network model is a complex hierarchical function f w of the the feature vector x and the collection of weights w. for typical choices for the g.k this function will be differentiable. given a training set fxi yign and a loss function l y f along familiar lines we might seek to solve l yi f w c nx n minimize w where j.w is a nonnegative regularization term on the elements of w and is a tuning parameter. practice there may be multiple regularization terms each with their own for example an early popular penalty is the quadratic n pkx j.w d w.k as in ridge regression also known as the weight-decay penalty it pulls the weights toward zero the biases are not penalized. lasso penalties are also popular as are mixtures of these elastic net. for binary classification we could take l to be binomial deviance in which case the neural network amounts to a penalized logistic regression section albeit a highly parametrized and penalized one. loss functions are usually convex in f but not in the elements of w so solving is difficult and at best we seek good local optima. most methods are based on some form of gradient descent with many associated bells and whistles. we briefly discuss some elements of the current practice in finding good solutions to computing the gradient backpropagation the elements of w occur in layers since f w is defined as a series of compositions starting from the input layer. computing the gradient is also done most naturally in layers chain rule for differentiation see for example in algorithm below and our notation makes this easier to describe in a recursive fashion. we will consider computing the derivative of l y f w with respect to any of the elements of w for a generic input output pair x y since the loss part of the objective is a sum fitting a neural network the overall gradient will be the sum of these individual gradient elements over the training pairs yi the intuition is as follows. given a training generic pair y we first make a forward pass through the network which creates activations at each of the nodes a.k in each of the layers including the final output layer. we would then like to compute an error term that measures the responsibility of each node for the error in predicting the true output y. for the output activations a.k these errors are easy either residuals or generalized residuals depending on the loss function. for activations at inner layers will be a weighted sum of the errors terms of nodes that use a.k as inputs. the backpropagation algorithm gives the details for computing the gradient for a single input output pair x y. we leave it to the reader to verify that this indeed implements the chain rule for differentiation. algorithm backpropagation at each of the layers lk i.e. compute f given a pair x y perform a feedforward pass computing the activations a.k w at x using the current w saving each of the intermediary quantities along the way. for each output unit in layer lk compute d y f d y f w pg.k.z.k where pg denotes the derivative of g.z wrt z. for example for l.y f d ky f for layers k d k k and for each node in layer k set becomes f pg.k.z.k pg.k.z.k d j w.k j the partial derivatives are given by w y f d a.k j one again matrix vector notation simplifies these expressions a bit neural networks becomes squared-error loss d a.k pg.k.z.k where denotes the hadamard product becomes pg.k.z.ki w becomes y f w d backpropagation was considered a breakthrough in the early days of neural networks since it made fitting a complex model computationally manageable. gradient descent algorithm computes the gradient of the loss function at a single generic pair y with n training pairs the gradient of the first part of is given by w nx yi f n w d w c with the quadratic form for the penalty a gradient-descent update is w w k d k where is the learning rate. gradient descent requires starting values for all the weights w. zero is not an option because each layer is symmetric in the weights flowing to the different neurons hence we rely on starting values to break the symmetries. typically one would use random starting weights close to zero random uniform or gaussian weights are common. there are a multitude of tricks of the trade in fitting or learning a neural network and many of them are connected with gradient descent. here we list some of these without going into great detail. stochastic gradient descent rather than process all the observations before making a gradient step it can be more efficient to process smaller batches at a time even batches fitting a neural network figure training and test misclassification error as a function of the number of epochs of training for the mnist digit classification problem. the architecture for the network is shown in figure the network was fit using accelerated gradient descent with adaptive rate control a rectified linear activation function and dropout regularization the horizontal broken line shows the error rate of a random forest a logistic regression model achieves only the scale. of size one! these batches can be sampled at random or systematically processed. for large data sets distributed on multiple computer cores this can be essential for reasons of efficiency. an epoch of training means that all n training samples have been used in gradient steps irrespective of how they have been grouped hence how many gradient steps have been made. accelerated gradient methods the idea here is to allow previous iterations to build up momentum and influence the current iterations. the iterations have the form d wt c d wt c errorlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllltraintesttest rf neural networks using wt to represent the entire collection of weights at iteration t. vt is a velocity vector that accumulates gradient information from previous iterations and is controlled by an additional momentum parameter when correctly tuned accelerated gradient descent can achieve much faster convergence rates however tuning tends to be a difficult process and is typically done adaptively. rate annealing a variety of creative methods have been proposed to adapt the learning rate to avoid jumping across good local minima. these tend to be a mixture of principled approaches combined with ad-hoc adaptations that tend to work well in practice. figure shows the performance of our neural net on the mnist digit data. this achieves state-of-the art misclassification error rates on these data under errors and outperforms random forests and a generalized linear model figure shows the misclassified digits. figure all misclassified digits in the mnist test set. the true digit class is labeled in blue the predicted in red. fitting a neural network other tuning parameters apart from the many details associated with gradient descent there are several other important structural and operational aspects of neural networks that have to be specified. number of hidden layers and their sizes with a single hidden layer the number of hidden units determines the number of parameters. in principle one could treat this number as a tuning parameter which could be adjusted to avoid overfitting. the current collective wisdom suggests it is better to have an abundant number of hidden units and control the model complexity instead by weight regularization. having deeper networks hidden layers increases the complexity as well. the correct number tends to be task specific having two hidden layers with the digit recognition problem leads to competitive performance. choice of nonlinearities there are a number of activation functions g.k in current use. apart from the sigmoid function which transforms its input to a values in other popular choices are figure activation functions. relu is a rectified linear tanh g.z d ez e ez c e relu which delivers values in neural networks rectified linear g.z d zc or the positive-part function. this has the advantage of making the gradient computations cheaper to compute. leaky rectified linear g d zc for nonnegative and close to zero. the rectified linear tends to have flat spots because of the many zero activations this is an attempt to avoid these and the accompanying zero gradients. choice of regularization typically this is a mixture of and regularization each of which requires a tuning parameter. as in lasso and regression applications the bias terms are usually not regularized. the weight regularization is typically light and serves several roles. the reduces problems with collinearity the can ignore irrelevant features and both slow the rate of overfitting especially with deep networks. early stopping neural nets are typically over-parametrized and hence are prone to overfitting. originally early stopping was set up as the primary tuning parameter and the stopping time was determined using a held-out set of validation data. in modern networks the regularization is tuned adaptively to avoid overfitting and hence it is less of a problem. for example in figure we see that the test misclassification error has flattened out and does not rise again with increasing number of epochs. autoencoders an autoencoder is a special neural network for computing a type of nonlinear principal-component decomposition. the linear principal component decomposition is a popular and effective linear method for reducing a large set of correlated variables to a typically smaller number of linear combinations that capture most of the variance in the original set. hence given a collection of n vectors xi rp to have mean zero we produce a derived set of uncorrelated features zi rq autoencoders p and typically smaller via zi d v xi. the columns of v are orthonormal and are derived such that the first component of zi has maximal variance the second has the next largest variance and is uncorrelated with the first and so on. it is easy to show that the columns of v are the leading q eigenvectors of the sample covariance matrix s d n x principal components can also be derived in terms of a best-approximating linear subspace and it is this version that leads to the nonlinear generalization presented here. consider the optimization problem x. minimize kxi nx for q p. the subspace is defined by the column space of a and for each point xi we wish to locate its best approximation in the subspace terms of euclidean distance. without loss of generality we can assume a has orthonormal columns in which case d a xi for each i separate linear regressions. plugging in reduces to kxi aa a solution is given by oa d v the matrix above of the first q principalcomponent direction vectors computed from the xi. by analogy a singlelayer autoencoder solves a nonlinear version of this problem nx a minimize adiq minimize w kxi w g.w xi nx for some nonlinear activation function g see figure panel. if g is the identity function these solutions coincide w d v figure panel represents the learned row of w as images when the autoencoder is fit to the mnist digit database. since autoencoders do not require a response class labels in this case this decomposition is unsupervised. it is often expensive to label images for example while unlabeled images are abundant. autoencoders provide a means for extracting potentially useful features from such data which can then be used with labeled data to train a classifier. in fact they are often used as warm starts for the weights when fitting a supervised neural network. once again there are a number of bells and whistles that make autoen coders more effective. neural networks figure left network representation of an autoencoder used for unsupervised learning of nonlinear principal components. the middle layer of hidden units creates a bottleneck and learns nonlinear representations of the inputs. the output layer is the transpose of the input layer so the network tries to reproduce the input data using this restrictive representation. right images representing the estimated rows of w using the mnist database the images can be seen as filters that detect local gradients in the image pixels. in each image most of the weights are zero and the nonzero weights are localized in the two-dimensional image space. regularization applied to the rows of w lead to sparse weight vectors and hence local features as was the case in our example. denoising is a process where noise is added to the input layer not the output resulting in features that do not focus on isolated values such as pixels but instead have some volume. we discuss denoising further in section with regularization the bottleneck is not necessary as in the figure or in principal components. in fact we can learn many more than p components. autoencoders can also have multiple layers which are typically learned sequentially. the activations learned in the first layer are treated as the input output features and a model like is fit to them. deep learning neural networks were reincarnated around with deep learning as a flashier name largely a result of much faster and larger computing systems plus a few new ideas. they have been shown to be particularly successful deep learning in the difficult task of classifying natural images using what is known as a convolutional architecture. initially autoencoders were considered a crucial aspect of deep learning since unlabeled images are abundant. however as labeled corpora become more available the word on the street is that supervised learning is sufficient. figure shows examples of natural images each with a class label such as beaver sunflower trout etc. there are class labels in figure examples of natural images. the database consists of color image classes with examples in each class train test. each image is green blue. here we display a randomly chosen image from each class. the classes are organized by hierarchical structure with coarse levels and five subclasses within each. so for example the first five images in the first column are aquatic mammals namely beaver dolphin otter seal and whale. neural networks all and training images and test images per class. the goal is to build a classifier to assign a label to an image. we present the essential details of a deep-learning network for this task one that achieves a respectable classification performance of errors on the designated test figure shows a typical deep-learning architecture with many figure architecture of a deep-learning network for the image classification task. the input layer and hidden layers are all represented as images except for the last hidden layer which is flattened the input layer consists of the d color green and blue versions of an input image earlier here we use the pk to refer to the number of images rather than the totality of pixels. each of these color panes is pixels in dimension. the first hidden layer computes a convolution using a bank of distinct q q learned filters producing an array of images of dimension the next pool layer reduces each non-overlapping block of numbers in each pane of the first hidden layer to a single number using a max operation. both q and are typically small each was for us. these convolve and pool layers are repeated here three times with changing dimensions our actual implementation there are layers in total. finally the derived features are flattened and a fully connected layer maps them to the classes via a softmax activation. hidden layers. these consist of two special types of layers convolve and pool. we describe each in turn. convolve layer figure illustrates a convolution layer and some details are given in classification becomes increasingly difficult as the number of classes grows. with equal representation in each class the null or random error rate for k classes is for two classes for fully the caption. if an image x is represented by a k k matrix and a filter f deep learning figure convolution layer for the input images. the input image is split into its three color components. a single filter is a q q array one q q for each of the d color panes and is used to compute an inner product with a correspondingly sized subimage in each pane and summed across the panes. we used q d and small values are typical. this is repeated over all q q subimages boundary padding and hence produces an image of the same dimension as one of the input panes. this is the convolution operation. there are different versions of this filter and hence new panes are produced. each of the filters has weights which are learned via backpropagation. xic jc f pq qx with elements qxi j dpq is a q q matrix with q k the convolved image is another k k matrix edge padding to achieve a full-sized k k output image. in our application we used but other sizes such as are popular. it is most natural to represent the structure in terms of these images as in figure but they could all be vectorized into a massive network diagram as in figures and however the weights would have special sparse structure with most being zero and the nonzero values repeated weight sharing neural networks pool layer the pool layer corresponds to a kind of nonlinear activation. it reduces each nonoverlapping block of pixels d for us to a single number by computing their maximum. why maximum? the convolution filters are themselves small image patches and are looking to identify similar patches in the target image which case the inner product will be high. the max operation introduces an element of local translation invariance. the pool operation reduces the size of each image by a factor r in each dimension. to compensate the number of tiles in the next convolution layer is typically increased accordingly. also as these tiles get smaller the effective weights resulting from the convolution operator become denser. eventually the tiles are the same size as the convolution filter and the layer becomes fully connected. learning a deep network despite the additional structure imposed by the convolution layers deep networks are learned by gradient descent. the gradients are computed by backpropagation as before but with special care taken to accommodate the tied weights in the convolution filters. however a number of additional tricks have been introduced that appear to improve the performance of modern deep learning networks. these are mostly aimed at regularization indeed our image network has around million parameters so regularization is essential to avoid overfitting. we briefly discuss some of these. dropout this is a form of regularization that is performed when learning a network typically at different rates at the different layers. it applies to all networks not just convolutional in fact it appears to work better when applied at the deeper denser layers. consider computing the activation z.k in layer k as in for a single observation during the feed-forward stage. the idea is to randomly set each of the nodes to zero with probability and inflate the remaining ones by a factor hence for this observation those nodes that survive have to stand in for those omitted. this can be shown to be a form of ridge regularization and when done correctly improves performance. the fraction omitted is a tuning parameter and for convolutional networks it appears to be better to use different values at j learning a deep network different layers. in particular as the layers become denser is increased from in the input layer to in the final fully connected layer. input distortion this is another form of regularization that is particularly suitable for tasks like image classification. the idea is to augment the training set with many distorted copies of an input image of course the same label. these distortions can be location shifts and other small affine transformations but also color and shading shifts that might appear in natural images. we show figure each column represents distorted versions of an input image including affine and color distortions. the input images are padded on the boundary to increase the size and hence allow space for some of the distortions. some distorted versions of input images in figure the distortions are such that a human would have no trouble identifying any of the distorted images if they could identify the original. this both enriches the training data with hints and also prevents overfitting to the original image. one could also apply distortions to a test image and then poll the results to produce a final classification. configuration designing the correct architecture for a deep-learning network along with the various choices at each layer appears to require experience and trial neural networks and error. we summarize the third and final architecture which we built for classifying the data set in algorithm in addition to these size parameters for each layer we must select the activation functions and additional regularization. in this case we used the leaky rectified linear functions g with increasing from in layer up to in layer in addition a type of regularization was imposed on the weights restricting all incoming weight vectors to a node to have norm bounded by one. figure shows both the progress of the optimization objective and the test misclassification error as the gradientdescent algorithm proceeds. the accelerated gradient method maintains a memory which we can see was restarted twice to get out of local minima. our network achieved a test error rate of on the test images images per class. the best reported error rate we have seen is so apparently we have some way to go! figure progress of the algorithm as a function of the number of epochs. the accelerated gradient algorithm is restarted every epochs meaning the long-term memory is forgotten and a new trail is begun starting at the current solution. the red curve shows the objective penalized log-likelihood on the training data. the blue curve shows test-set misclassification error. the vertical axis is on the log scale so zero cannot be included. misclassification costmisclassification error notes and details algorithm configuration parameters for deep-learning network used on the data. layer convolution maps each with kernel for three colors. the input image is padded from to to accommodate input distortions. layers and convolution maps each compositions of convolutions are roughly equivalent to convolutions with a bigger bandwidth and the smaller ones have fewer parameters. layer max pool layer pooling nonoverlapping blocks of pixels and hence reducing the images to size layer convolution maps each with dropout learning with rate d layer repeat of layer layer max pool layer to images. layer convolution maps each with dropout rate d layer convolution maps each with dropout rate d layer max pool layer to images. layer convolution maps each this is a pixelwise weighted sum across the images from the previous layer. layer fully connected units with dropout rate d layer final output units with softmax activation and dropout rate d notes and details the reader will notice that probability models have disappeared from the development here. neural nets are elaborate regression methods aimed solely at prediction not estimation or explanation in the language of section in place of parametric optimality criteria the machine learning community has focused on a set of specific prediction data sets like the digits mnist corpus and as benchmarks for measuring performance. there is a vast literature on neural networks with hundreds of books and thousands of papers. with the resurgence of deep learning this literature is again growing. two early statistical references on neural networks are ripley and bishop and hastie et al. devote one chapter to the topic. part of our description of backpropagation in section was neural networks guided by andrew ng s online stanford lecture notes bengio et al. provide a useful review of autoencoders. lecun et al. give a brief overview of deep learning written by three pioneers of this field yann lecun yoshua bengio and geoffrey hinton we also benefited from reading ngiam et al. dropout learning et al. is a relatively new idea and its connections with ridge regression were most usefully described in wager et al. the most popular version of accelerated gradient descent is due to nesterov learning with hints is due to abu-mostafa the material in sections and benefited greatly from discussions with rakesh achanta and hastie who produced some of the color images and diagrams and designed and fit the deep-learning network to the data. the neural information processing systems conferences started in late fall in denver colorado and post-conference workshops were held at the nearby ski resort at vail. these are still very popular today although the venue has changed over the years. the nips proceedings are refereed and nips papers count as publications in most fields especially computer science and engineering. although neural networks were initially the main topic of the conferences a modern nips conference covers all the latest ideas in machine learning. mnist is a curated database of images of handwritten digits and cortes there are training images and test images each a grayscale image. these data have been used as a testbed for many different learning algorithms so the reported best error rates might be optimistic. tuning parameters. typical neural network implementations have dozens of tuning parameters and many of these are associated with the fine tuning of the descent algorithm. we used the function in the r package to fit our model for the mnist data set. it has around such parameters although most default to factory-tuned constants that have been found to work well on many examples. arno candel was very helpful in assisting us with the software. dropout and ridge regression. dropout was originally proposed in srivastava et al. and reinterpreted in wager et al. dropout was inspired by the random selection of variables at each tree split in a random forest consider a simple version of dropout for the linear regression problem with squared-error loss. we have an n p regression matrix x and a response n-vector y. for simplicity we assume all variables have mean zero so we can ignore intercepts. consider the following random least-squares criterion notes and details px nx xij iij j li d iij d d o here the iij are i.i.d variables j with with probability with probability particular form is used so that e iij d using simple probability it can be shown that the expected score equations can be written e with d d hence the solution is given by y c x d d x c x c x d x y a generalized ridge regression. if the variables are standardized the term d becomes a scalar and the solution is identical to ridge regression. with a nonlinear activation function the interpretation changes slightly see wager et al. for details. distortion and ridge regression. we again show in a simple example that input distortion is similar to ridge regression. assume the same setup as in the previous example except a different randomized version of the criterion ln d c nij j nx px d here we have added random noise to the prediction variables and we assume this noise is i.i.d once again the expected score equations can be written y c x x c d e ij d once again because of the independence of all the nij and this leads to a ridge regression. so replacing each observation pair xi yi by the collection fx is a noisy version of xi is approximately equivalent to a ridge regression on the original data. where each x yigb i i neural networks software for deep learning. our deep learning convolutional network for the data was constructed and run by rakesh achanta in theano a python-based system et al. bergstra et al. theano has a user-friendly language for specifying the host of parameters for a deep-learning network and uses symbolic differentiation for computing the gradients needed in stochastic gradient descent. in google announced an open-source version of their tensorflow software for fitting deep networks. support-vector machines and kernel methods while linear logistic regression has been the mainstay in biostatistics and epidemiology it has had a mixed reception in the machine-learning community. there the goal is often classification accuracy rather than statistical one ifbpr.y d d x svms bypass the first step and build a inference. logistic regression builds a classifier in two steps fit a conditional probability model for pr.y d d x and then classify as a classifier directly. another rather awkward issue with logistic regression is that it fails if the training data are linearly separable! what this means is that in the feature space one can separate the two classes by a linear boundary. in cases such as this maximum likelihood fails and some parameters march off to infinity. while this might have seemed an unlikely scenario to the early users of logistic regression it becomes almost a certainty with modern wide genomics data. when p n features than observations we can typically always find a separating hyperplane. finding an optimal separating hyperplane was in fact the launching point for svms. as we will see they have more than this to offer and in fact live comfortably alongside logistic regression. svms pursued an age-old approach in statistics of enriching the feature space through nonlinear transformations and basis expansions a classical example being augmenting a linear regression with interaction terms. a linear model in the enlarged space leads to a nonlinear model in the ambient space. this is typically achieved via the kernel trick which allows the computations to be performed in the n-dimensional space for an arbitrary number of predictors p. as the field matured it became clear that in fact this kernel trick amounted to estimation in a reproducing-kernel hilbert space. finally we contrast the kernel approach in svms with the nonparame teric regression techniques known as kernel smoothing. svms and kernel methods optimal separating hyperplane figure shows a small sample of points in each belonging to one of two classes or orange. numerically we would score these classes as y d for say blue and y d for we define a two-class linear classifier via a function f d c x with the convention that we classify a point as if f and as if f the fence we flip a coin. hence the classifier itself is c.x d sign f the deci figure left panel data in two classes in three potential decision boundaries are shown each separate the data perfectly. right panel the optimal separating hyperplane line in creates the biggest margin between the two classes. sion boundary is the set fx j f d we see three different classifiers in the left panel of figure and they all classify the points perfectly. the optimal separating hyperplane is the linear classifier that creates the largest margin between the two classes and is shown in the right panel is also known as an optimal-margin classifier. the underlying hope is that by making a big margin on the training data it will also classify future observations well. some elementary geometry shows that the euclidean distance from a point to the linear decision boundary defined by f is given by f with this in mind for a separating hyperplane the quantity in this chapter the scoring leads to convenient notation. yi f is optimal separating hyperplane the distance of xi from the decision this leads to an optimization problem for creating the optimal margin classifier maximize m subject to yi c x m i d n a rescaling argument reduces this to the simpler form k minimize subject to yi c x i d n this is a quadratic program which can be solved by standard techniques in convex optimization. one noteworthy property of the solution is that dx o s o i xi s where s is the support set. we can see in figure that the margin touches three points in this case there are j j d support vectors and clearly the orientation of o is determined by them. however we still have to solve the optimization problem to identify the three points i in s and their coefficients i s. figure shows an optimalmargin classifier fit to wide data that is data where p n. these are gene-expression measurements on p d genes measured on blood samples from n d leukemia patients seen in chapter they were classified into two classes acute lymphoblastic leukemia and myeloid leukemia in cases like this we are typically guaranteed a separating in this case of the points are support points. one might be justified in thinking that this solution is overfit to this small amount of data. indeed when broken into a training and test set we see that the test data encroaches well into the margin region but in this case none are misclassified. such classifiers are very popular in the widedata world of genomics largely because they seem to work very well. they offer a simple alternative to logistic regression in a situation where the latter fails. however sometimes the solution is overfit and a modification is called for. this same modification takes care of nonseparable situations as well. since all the points are correctly classified the sign of f agrees with yi hence this quantity is always positive. if n p c we can always find a separating hyperplane unless there are exact feature ties across the class barrier! svms and kernel methods figure left panel optimal margin classifier fit to leukemia data. there are observations from two classes all and aml and gene-expression variables. of the observations are support vectors sitting on the margin. the points are plotted against their fitted classifier function o component of the data for display purposes since it has low correlation with the former. right panel here the optimal margin classifier was fit to a random subset of of the observations and then used to classify the remaining in color. although these points fall on the wrong sides of their respective margins they are all correctly classified. f labeled svm projection and the fifth principal soft-margin classifier figure shows data in that are not separable. the generalization to a soft margin allows points to violate their margin. each of the violators has a line segment connecting it to its margin showing the extent of the violation. the soft-margin classifier solves k minimize subject to yi c x i d n and nx i b here b is the budget for the total amount of overlap. once again the solution has the form except now the support set s includes any vectors on the margin as well as those that violate the margin. the bigger b the llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll all datasvm projectionpca projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll train and testsvm projectionpca projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllll svm criterion as loss plus penalty figure for data that are not separable such as here the soft-margin classifier allows margin violations. the budget b for the total measure of violation becomes a tuning parameter. the bigger the budget the wider the soft margin and the more support points there are involved in the fit. bigger the support set and hence the more points that have a say in the solution. hence bigger b means more stability and lower variance. in fact even for separable data allowing margin violations via b lets us regularize the solution by tuning b. svm criterion as loss plus penalty it turns out that one can reformulate and in more traditional terms as the minimization of a loss plus a penalty nx yi c x i c c minimize here the hinge loss lh f d yf c operates on the margin quantity yf and is piecewise linear as in figure the same margin quantity came up in boosting in section the quantity yi c i c is the cost for xi being on the wrong side of its margin cost x is zero if it s on the correct side. the correspondence between and is exact large corresponds to large b and this formulation makes explicit the form of regularization. for separable data the optimal separating hyperplane solution corresponds to the limiting minimum-norm solution as one can show that the population minimizer of the svms and kernel methods figure the hinge loss penalizes observation margins yf less than linearly and is indifferent to margins greater than the negative binomial log-likelihood has the same asymptotes but operates in a smoother fashion near the elbow at yf d hinge loss is in fact the bayes this shows that the svm is in fact directly estimating the classifier c.x the red curve in figure is the binomial deviance for logistic regression f d c x is now modeling logit pr.y d d x. with y d the deviance can also be written in terms of the margin nx and the ridged logistic regression corresponding to has the form log c e i c minimize logistic regression is discussed in section as well as sections and this form of the binomial deviance is derived in on page these loss functions have some features in common as can be seen in the figure. the binomial loss asymptotes to zero for large positive margins and to a linear loss for large negative margins matching the hinge loss in this regard. the main difference is that the hinge has a sharp elbow at while the binomial bends smoothly. a consequence of this is that the binomial solution involves all the data via weights pi pi that fade smoothly with distance from the decision boundary as apposed to the binary nature the bayes classifier c.x for a two-class problem using equal costs for misclassification errors assigns x to the class for which pr.yjx is largest. computations and the kernel trick d of support points. also as seen in section as well the population minimizer of the binomial deviance is the logit of the class probability d log pr.y d while that of the hinge loss is its sign c.x d sign interestingly as the solution direction o to the ridged logistic regression problem converges to that of the svm. these forms immediately suggest other generalizations of the linear svm. in particular we can replace the ridge penalty k by the sparsityinducing lasso penalty k which will set some coefficients to zero and hence perform feature selection. publicly available software package liblinear in r is available for fitting such lasso-regularized supportvector classifiers. computations and the kernel trick the form of the solution o o i xi for the optimal- and soft-margin classifier has some important consequences. for starters we can write the fitted function evaluated at a point x as c x s o ihx xii dp f d o o d o o cx s where we have deliberately replaced the transpose notation with the more suggestive inner product. furthermore we show in in section that the lagrange dual involves the data only through the pairwise inner products hxi xji elements of the n gram matrix xx this means that the computations for computing the svm solution scale linearly with p although potentially in n. with very large p the tens of thousands and even millions as we will see this can be convenient. it turns out that all ridge-regularized linear models with wide data can be reparametrized in this way. take ridge regression for example minimize d ky x x c c this has solution o y and with p large requires inversion of a p p matrix. however it can be shown that o o d in practice and with modern approximate solutions much faster than that. d x c svms and kernel methods has the same form as for the svm. pn o i xi with o d which means the solution can be obtained in rather than computations. again the gram matrix has played a role and o we now imagine expanding the p-dimensional feature vector x into a potentially much larger set h.x d hm.x for an example to latch onto think polynomial basis of total degree d. as long as we have an efficient way to compute the inner products hh.x h.xj for any x we can compute the svm solution in this enlarged space just as easily as in the original. it turns out that convenient kernel functions exist that do just that. for example kd z d c hx zid creates a basis expansion hd of polynomials of total degree d and kd z d hhd hd the polynomial kernels are mainly useful as existence proofs in practice other more useful kernels are used. probably the most popular is the radial kernel k.x z d e this is a positive definite function and can be thought of as computing an inner product in some feature space. here the feature space is in principle infinite-dimensional but of course effectively now one can think of the representation in a different light o i k.x xi f d o cx o s an expansion of radial basis functions each centered on one of the training examples. figure illustrates such an expansion in using such nonlinear kernels expands the scope of svms considerably allowing one to fit classifiers with nonlinear decision boundaries. one may ask what objective is being optimized when we move to this kernel representation. this is covered in the next section but as a sneak preview we present the criterion nx yj c nx c c k i k.xj xi minimize where the n n matrix k has entries k.xj xi as an illustrative example in we can visualize the nonlinear boundaries we generated the data in figure we show two svm a bivariate function k.x z rp is positive-definite if for every q every q q matrix k d fk.xi xj formed using distinct entries xq is positive definite. the feature space is defined in terms of the eigen-functions of the kernel. computations and the kernel trick figure radial basis functions in the left panel shows a collection of radial basis functions each centered on one of the seven observations. the right panel shows a function obtained from a particular linear expansion of these basis functions. solutions both using a radial kernel. in the left panel some margin errors are committed but the solution looks reasonable. however with the flexibility of the enlarged feature space by decreasing the budget b we can typically overfit the training data as is the case in the right panel. a separate little blue island was created to accommodate the one blue point in a sea of brown. figure simulated data in two classes in with svm classifiers computed using the radial kernel the left panel uses a larger value of b than the right. the solid lines are the decision boundaries in the original space boundaries in the expanded feature space. the dashed lines are the projected margins in both cases. jkxxjxx llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll svms and kernel methods function fitting using kernels the analysis in the previous section is heuristic replacing inner products by kernels that compute inner products in some feature space. indeed this is how kernels were first introduced in the svm world. there is however a rich literature behind such approaches which goes by the name function fitting in reproducing-kernel hilbert spaces we give a very brief overview here. one starts with a bivariate positive-definite kernel k w rp rp and we consider a space hk of functions f w rp generated by the kernel f z z the kernel also induces a norm on the space kf k hk which can be thought of as a roughness measure. we can now state a very general optimization problem for fitting a func tion to data when restricted to this class l.yi c f c hk nx minimize f hk a search over a possibly infinite-dimensional function space. here l is an arbitrary loss function. the magic of these spaces in the context of this problem is that one can show that the solution is finite-dimensional f d nx o o i k.x xi a linear basis expansion with basis functions ki d k.x xi anchored at each of the observed vectors xi in the training data. moreover using the reproducing property of the kernel in this space one can show that the penalty reduces to k o f hk here k is the n gram matrix of evaluations of the kernel equivalent to the xx matrix for the linear case. o i o j k.xi xj d o d nx nx k o hence the abstract problem reduces to the generalized ridge problem minimize nx c nx l c k i k.xi xj here kz d z is considered a function of the first argument and the second argument is a parameter. example string kernels for protein classification indeed if l is the hinge loss as in this is the equivalent loss plus penalty criterion being fit by the kernel svm. alternatively if l is the binomial deviance loss as in this would fit a kernel version of logistic regression. hence most fitting methods can be generalized to accommodate kernels. this formalization opens the door to a wide variety of applications depending on the kernel function used. alternatively as long as we can compute suitable similarities between objects we can build sophisticated classifiers and other models for making predictions about other attributes of the in the next section we consider a particular example. example string kernels for protein classification one of the important problems in computational biology is to classify proteins into functional and structural classes based on their sequence similarities. protein molecules can be thought of as strings of amino acids and differ in terms of length and composition. in the example we consider the lengths vary between and amino-acid molecules each of which can be one of different types labeled using the letters of the alphabet. here follow two protein examples and of length and respectively iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv erlfknlslikkyidgqkkkcgeerrrvnqfldylqeflgvmntewi phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaerlqenlqayrtfhvlla rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk lwglkvlqelsqwtvrsihdlrfisshqtgip we treat the proteins x as documents consisting of letters with a dictionary of size our feature vector hm.x will consist of the counts for all m-grams in the protein that is distinct sequences of consecutive letters of length m. as an illustration we use m d which results in possible sub-sequences hence will be a vector of length with each element the number of times that particular sub-sequence occurs in the protein x. in our example the sub-sequence lqe occurs once in the d first and twice in the second protein so the number of possible sequences of length m is which can be very d and as long as the similarities behave like inner products i.e. they form positive semi-definite matrices. svms and kernel methods large for moderate m. also the vast majority of the sub-sequences do not match the strings in our training set which means hm.x will be sparse. it turns out that we can compute the inner product matrix or string kernel d efficiently using tree structures without actually computing the individual vectors. armed with the kernel we figure roc curves for two classifiers fit to the protein data. the roc curves were computed using cross-validation and trace the trade-off between false-positive and true-positive error rates as the classifier threshold is varied. the area under the curve summarizes the overall performance of each classifier. here the svm is slightly superior to kernel logistic regression. can now use it to fit a regularized svm or logistic regression model as outlined in the previous section. the data consist of proteins in two classes negative and positive we fit both the kernel svm and kernel logistic regression models. for both methods cross-validation suggested a very small value for figure shows the roc trade-off curve for each using cross-validation. here the svm outperforms logistic regression. protein classificationfalse-positive ratetrue-positive svms concluding remarks svms concluding remarks svms have been wildly successful and are one of the must have tools in any machine-learning toolbox. they have been extended to cover many different scenarios other than two-class classification with some awkwardness in cases. the extension to nonlinear function-fitting via kernels the machine in the name generated a mini industry. kernels are parametrized learned from data with special problem-specific structure and so on. on the other hand we know that fitting high-dimensional nonlinear functions is intrinsically difficult curse of dimensionality and svms are not immune. the quadratic penalty implicit in kernel methodology means all features are included in the model and hence sparsity is generally not an option. why then this unbridled enthusiasm? classifiers are far less sensitive to bias variance tradeoffs and svms are mostly popular for their classification performance. the ability to define a kernel for measuring similarities between abstract objects and then train a classifier is a novelty added by these approaches that was missed in the past. kernel smoothing and local regression the phrase kernel methodology might mean something a little different to statisticians trained in the period. kernel smoothing represents a broad range of tools for performing non- and semi-parametric regression. figure shows a gaussian kernel smooth fit to some artificial data fxi yign it computes at each point a weighted average of the y-values of neighboring points with weights given by the height of the kernel. in its simplest form this estimate can be written as f d nx o yi xi where xi represents the radial kernel with width parameter notice the similarity to here the o i d yi and the complexity of the model is controlled by despite this similarity and the use of the same kernel these methodologies are rather different. the focus here is on local estimation and the kernel does the localizing. expression is almost a weighted average almost because here is the normalized gaussian density with mean and variance svms and kernel methods figure a gaussian kernel smooth of simulated data. the points come from the blue curve with added random errors. the kernel smoother fits a weighted mean of the observations with the weighting kernel centered at the target point in this case. the points shaded orange contribute to the fit at as moves across the domain the smoother traces out the green curve. the width of the kernel is a tuning parameter. we have depicted the gaussian weighting kernel in this figure for illustration in fact its vertical coordinates are all positive and integrate to one. pn xi in fact the nadaraya watson estimator is more explicit fn w dpn pn o yi xi xi although figure is one-dimensional the same formulation applies to x in higher dimensions. weighting kernels other than the gaussian are typically favored in par ticular near-neighbor kernels with compact support. for example the tricube kernel used by the lowess smoother in r is defined as follows define di d i d n and let d.m be the mth smallest distance of the mth nearest neighbor to let ui d di i d n. kernel smoothing and local regression the tricube kernel is given by xi d i if ui otherwise where s d mn the span of the kernel. near-neighbor kernels such as this adapt naturally to the local density of the xi wider in low-density regions narrower in high-density regions. a tricube kernel is illustrated in figure figure local regression fit to the simulated data. at each point we fit a locally weighted linear least-squares model and use the fitted value to estimate o here we use the tricube kernel with a span of the orange points are in the weighting neighborhood and we see the orange linear fit computed by kernel weighted least squares. the green dot is the fitted value at from this local linear fit. weighted means suffer from boundary bias we can see in figure that the estimate appears biased upwards at both boundaries. the reason is that for example on the left the estimate for the function on the boundary averages points always to the right and since the function is locally increasing there is an upward bias. local linear regression is a natural generalization that fixes such problems. at each point we solve the following weighted regression svms and kernel methods least-squares problem o nx o d arg min o c one can show that to first order xi xi d o then o o removes the boundary bias exactly. figure illustrates the procedure on our simulated data using the tricube kernel with a span of of the data. in practice the width of the kernel span here has to be selected by some means typically we use cross-validation. local regression works in any dimension that is we can fit two- or higher-dimensional surfaces using exactly the same technique. here the ability to remove boundary bias really pays off since the boundaries can be complex. these are referred to as memory-based methods since there is no fitted model. we have to save all the training data and recompute the local fit every time we make a prediction. like kernel svms and their relatives kernel smoothing and local regression break down in high dimensions. here the near neighborhoods become so wide that they are no longer local. notes and details in the late and early machine-learning research was largely driven by prediction problems and the neural-network community at att bell laboratories was amongst the leaders. the problem of the day was the us post-office handwritten zip-code ocr challenge a image classification problem. vladimir vapnik was part of this team and along with colleagues invented a more direct approach to classification the support-vector machine. this started with the seminal paper by boser et al. which introduced the optimal margin classifier separating hyperplane see also vapnik the ideas took off quite rapidly attracting a large cohort of researchers and evolved into the more general class of kernel methods that is models framed in reproducing-kernel hilbert spaces. a good general reference is sch olkopf and smola x c define a linear decision boundary fx j f d in rp affine set of codimension one. the unit vector normal to the boundary is where k denotes the or euclidean norm. how should one compute the distance from a point x to this boundary? if is any point on the boundary geometry of separating hyperplanes. let f d notes and details f d we can project x onto the normal giving us k d f ing to can be written as c nx as claimed in note that this is the signed distance since f will be positive or negative depending on what side of the boundary it lies on. the support in svm. the lagrange primal problem correspond minimize yi xi andpn nx yi c x i dpn where are the lagrange multipliers. on differentiating we find that yi d with i d yi we get and note that the positivity constraint on will lead to some of the i being nx zero. plugging into we obtain the lagrange dual problem nx maximize yi yj x subject to yi d i xj nx the svm loss function. the constraint in can be succinctly captured via the expression nx yi c x i c b we only require a if our margin is less than and we get charged for the sum of these we now use a lagrange multiplier to enforce the constraint leading to c nx yi c x minimize k i c multiplying by d gives us the svm estimates a classifier. the following derivation is due to wahba et al. consider ey jxdx f yf cg minimize f svms and kernel methods dropping the dependence on x the objective can be written as pc f cc f c where pc d pr.y d d x and d pr.y d d x d pc. from this we see that f if pc if o svm and ridged logistic regression. rosset et al. show that the limiting solution as to for separable data coincides with that of the svm in the sense that o converges to the same quantity for the svm. however because of the required normalization for logistic regression the svm solution is preferable. on the other hand for overlapped situations the logistic-regression solution has some advantages since its target is the logit of the class probabilities. the kernel trick. the trick here is to observe that from the score x c d which means we can write equations we have o d x for some we now plug this into the score equations and some simple manipulation gives the result. a similar result holds for ridged logistic regression and in fact any linear model with a ridge penalty on the coefficients and tibshirani polynomial kernels. consider z d c hx for x z in expanding we get z d c c c c c this corresponds to with p p p d the same is true for p and for degree d reproducing kernel hilbert spaces. suppose k has eigen then sion k.x z with hk if f we say f ci with i kf hk often kf k hk behaves like a roughness penalty in that it penalizes unlikely members in the span of z that these correspond to rough functions. if f has some high loadings cj on functions with small eigenvalues not prominent members of the span the norm becomes large. smoothing splines and their generalizations correspond to function fitting in a rkhs notes and details this methodology and the data we use in our example come from leslie et al. local regression and bias reduction. by expanding the unknown true f in a first-order taylor expansion about the target point one can show that e o f and loader inference after model selection the classical theory of model selection focused on f tests performed within gaussian regression models. inference after model selection instance assessing the accuracy of a fitted regression curve was typically done ignoring the model selection process. this was a matter of necessity the combination of discrete model selection and continuous regression analysis was too awkward for simple mathematical description. electronic computation has opened the door to a more honest analysis of estimation accuracy one that takes account of the variability induced by data-based model selection. figure displays the cholesterol data an example we will use for illustration in what follows cholestyramine a proposed cholesterollowering drug was administered to n d men for an average of seven years each. the response variable di was the ith man s decrease in cholesterol level over the course of the experiment. also measured was ci his compliance or the proportion of the intended dose actually taken ranging from for perfect compliers to zero for the four men who took none at all. here the ci values have been transformed to approximately follow a standard normal distribution ci n we wish to predict cholesterol decrease from compliance. polynomial regression models with di a j th-order polynomial in ci were considered for degrees j d or the cp criterion was applied and selected a cubic model j d as best. the curve in figure is the ols least squares cubic regression curve fit to the cholesterol data set f.ci di i d we are interested in answering the following question how accurate is the simultaneous confidence intervals figure cholesterol data cholesterol decrease plotted versus adjusted compliance for men taking cholestyramine. the green curve is ols cubic regression with cubic selected by the cp criterion. how accurate is the fitted curve? fitted curve taking account of cp selection as well as ols estimation? section for an answer. currently there is no overarching theory for inference after model selection. this chapter more modestly presents a short series of vignettes that illustrate promising analyses of individual situations. see also section for a brief report on progress in post-selection inference for the lasso. simultaneous confidence intervals in the early just before the beginnings of the computer revolution substantial progress was made on the problem of setting simultaneous confidence intervals. simultaneous here means that there exists a catalog of parameters of possible interest c d and we wish to set a confidence interval for each of them with some fixed probability typically that all of the intervals will contain their respective parameters. compliancecholesterol decrease inference after model selection as a first example we return to the diabetes data of section n d diabetes patients each have had p d medical variables measured at baseline with the goal of predicting prog disease progression one year later. let x be the matrix with ith row x i the measurements for patient i x has been standardized so that each of its columns has mean and sum of squares also let y be the of centered prog measurements is subtracting off the mean of the prog values. ordinary least squares applied to the normal linear model y nn.x o d y x o np. v d x yields mle satisfying as at nent of is the student-t confidence interval for j the j th compo o j v jj t q where d is the usual unbiased estimate of d ky x o q d n p d q d is the quantile of a student-t distribution with q and t degrees of freedom. the catalog c in is now f the individual intervals shown in table each have coverage but they are not simultaneous there is a greater than chance that at least one of the j values lies outside its claimed interval. valid simultaneous intervals for the parameters appear on the right side of table these are the scheff e intervals o j v jj k. pq pq equals for p d q d discussed next. the crucial constant k. and d that makes the scheff e intervals wider than the t intervals by a factor of one expects to pay an extra price for simultaneous coverage but a factor greater than two induces sticker shock. scheff e s method depends on the pivotal quantity q o o v simultaneous confidence intervals table maximum likelihood estimates o variables separate student-t confidence limits also simultaneous scheff e intervals. the scheff e intervals are wider by a factor of for diabetes predictor student-t scheff e o lower upper lower upper age sex bmi map tc ldl hdl tch ltg glu which under model has a scaled f distribution pq is the th quantile of a pfpq distribution then prfq k. if k. yields q pfpq o v o pr d k. pq pq g d for any choice of and in model having observed o and defines an elliptical confidence region e for the parameter vector suppose we are interested in a particular linear combination of the coor dinates of say c d c fpq is distributed as qq the two chi-squared variates being independent. calculating the percentiles of fpq was a major project of the pre-war period. inference after model selection figure ellipsoid of possible vectors defined by determines confidence intervals for c d c according to the bounding hyperplane construction illustrated. the red line shows the confidence interval for c if c is a unit vector v c d c where c is a fixed p-dimensional vector. if exists in e then we must have which turns out to be the interval centered at o max e min e o c c o c c d c v pq agrees with where c is the j th coordinate vector the construction is illustrated in figure np. independently of theorem e then with probability the confidence statement for c d c be simultaneously true for all choices of the vector c. qq will if o here we can think of model selection as the choice of the linear combination of interest d c scheff e s theorem allows data snooping the statistician can examine the data and then choose which many s to estimate without invalidating the resulting confidence intervals. an important application has the o j s as independent estimates of efficacy for competing treatments perhaps different experimental drugs for the same target disease o j n j for j d j lcb simultaneous confidence intervals the nj being known sample sizes. in this case the catalog c might comprise all pairwise differences i j as the statistician tries to determine which treatments are better or worse than the others. the fact that scheff e s limits apply to all possible linear combinations is a blessing and a curse the curse being their very large width as seen c in table narrower simultaneous limits are possible if we restrict the catalog c for instance to just the pairwise differences i j a serious objection along fisherian lines is that the scheff e confidence limits are accurate without being correct. that is the intervals have the claimed overall frequentist coverage probability but may be misleading when applied to individual cases. suppose for instance that d for j d j in and that we observe o d with j o jj for all the others. even if we looked at the data before singling out o for attention the usual student-t interval seems more appropriate than its much longer scheff e version this point is made more convincingly in our next vignette. a familiar but pernicious abuse of model selection concerns multiple hypothesis testing. suppose we observe n independent normal variates zi each with its own effect size for i d n and as in section we wish to test the null hypotheses n zi w d being alert to the pitfalls of simultaneous testing we employ a false-discovery rate control algorithm which rejects r of the n null hypotheses say for cases ir. equaled in the example of figure so far so good. the familiar abuse comes in then setting the usual confidence intervals coverage for the r selected cases. this ignores the model selection process the data-based selection of the r cases must be taken into account in making legitimate inferences even if r is only so multiplicity is not a concern. this problem is addressed by the theory of false-coverage control. suppose algorithm a sets confidence intervals for r of the n cases of which inference after model selection r are actually false coverages i.e. ones not containing the true effect size the false-coverage rate of a is the expected proportion of noncoverages fcr.a d efrrg the expectation being with respect to model the goal as with the fdr theory of section is to construct algorithm a to control fcr below some fixed value q. the byq controls fcr below level q in three easy steps beginning with model let pi be the p-value corresponding to zi pi d for left-sided significance testing and order the p.i values in ascending order p.n calculate r d maxfi w p.i i qng and in the bhq algorithm declare the r corresponding null hypotheses false. zi z. r for each of the r cases construct the confidence interval where r d rqn d theorem under model byq has fcr q moreover none of the intervals contain d a simulated example of byq was run according to these specifications n d d n q d zi for i d for i d n in this situation we have null cases and non-null cases but of which had because this is a simulation we can plot the pairs to assess the byq algorithm s performance. this is done in figure for the non-null cases green points. byq declared r d cases non-null those having zi circled points of the declarations short for benjamini yekutieli see the chapter endnotes. simultaneous confidence intervals figure simulation experiment with n cases of which are non-null the green points are these non-null cases. the fdr control algorithm bhq d declared the circled cases having zi to be non-null of which the red points were actually null. the heavy black lines show byq confidence intervals for the cases only of which failed to contain actual bayes posterior intervals for non-null cases dotted lines have half the width and slope of byq limits. were actually null cases red circled points giving false-discovery proportion d the heavy black lines trace the byq confidence limits as a function of z the first thing to notice is that fcr control has indeed been achieved only of the declared cases lie outside their limits nulls and non-nulls for a false-coverage rate of d safely less than q d the second thing however is that the byq limits provide a misleading idea of the location of given zi they are much too wide and slope too low especially for more negative zi values. in this situation we can describe precisely the posterior distribution of given zi for the non-null cases n zi zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllby.loby.up inference after model selection this following from the bayes credible limits n and bayes rule n zi are indicated by the dotted lines in figure they are half as wide as the byq limits and have slope rather than in practice of course we would only see the zi not the making unavailable to us. we return to this example in chapter where empirical bayes methods will be seen to provide a good approximation to the bayes limits. figure as with scheff e s method the byq intervals can be accused of being accurate but not correct. correct here has a bayesianfisherian flavor that is hard to pin down except perhaps in large-scale applications where empirical bayes analyses can suggest appropriate inferences. accuracy after model selection the cubic regression curve for the cholesterol data seen in figure was selected according to the cp criterion of section polynomial regression models predicting cholesterol decrease di in terms of powers degrees of adjusted compliance ci were fit by ordinary least squares for degrees table shows cp estimates being minimized at degree table cp table for cholesterol data of figure comparing ols polynomial models of degrees through the cubic model degree d is the minimizer subtracted from the cp values for easier comparison assumes d degree cp we wish to assess the accuracy of the fitted curve taking account of both the cp model selection method and the ols fitting process. the bootstrap accuracy after model selection is a natural candidate for the job. here we will employ the nonparametric bootstrap of section than the parametric bootstrap of section though this would be no more difficult to carry out. the cholesterol data set comprises n d pairs xi d consists of pairs di a nonparametric bootstrap sample x chosen at random and with replacement from the original let t be the curve obtained by applying the cpols algorithm to the original data and for a given set x and likewise t point c on the compliance scale let for the algorithm applied to x o c d t x be the value of t evaluated at compliance d c. figure a histogram of nonparametric bootstrap replications for polynomial regression estimates of cholesterol decreases d at adjusted compliance c d blue histogram adaptive estimator o c using full cpols algorithm for each bootstrap data set line histogram using ols only with degree for each bootstrap data set. bootstrap standard errors are and cholesterol decrease q degree inference after model selection b d nonparametric bootstrap replications t were figure shows the histogram of the o c replications for c d it is labeled adaptive to indicate that cp model selection as well as ols this is as opposed to the fixed fitting was carred out anew for each x histogram where there was no cp selection cubic ols regression always being used. figure bootstrap standard-error estimates of o c solid black curve adaptive estimator using full cpols model selection estimate red dashed curve using ols only with polynomial degree fixed at blue dotted curve bagged estimator using bootstrap smoothing average standard-error ratios adaptivefixed d adaptivesmoothed d for the bootstrap estimate of standard error obtained from the adaptive values o c was compared with for the fixed in this case accounting for model selection adaptation adds more than to the standard error estimates. the same comparison was made at all values ten times more than needed for assessing standard errors but helpful for the comparisons that follow. the latter is not the usual ols assessment following that would be appropriate for a parametric bootstrap comparison. rather it s the nonparametric one-sample bootstrap assessment resampling pairs yi as individual sample points. compliance cstandard errors of qcfixedadaptivesmoothed accuracy after model selection of the adjusted compliance c. figure graphs the results the adaptive standard errors averaged greater than the fixed values. the standard we ignored model selection in assessingbse. confidence intervals o bse would be roughly too short if figure adaptive histogram of figure now split into of bootstrap replications where cp selected linear d d as best versus having m m regression cases are shifted about units downward. m cases resemble the fixed histogram in figure histograms are scaled to have equal areas. having an honest assessment of standard error doesn t mean that t x is a good estimator. model selection can induce an unpleasant jumpiness in an estimator as the original data vector x crosses definitional boundaries. this happened in our example for of the d the cp algorithm selected linear regression m bootstrap samples x as best and in these cases o tended toward smaller values. figure d histogram shifted about units down from the m shows the m histogram now resembles the fixed histogram in figure discontinuous estimators such as t x can t be bayesian bayes posterior expectations being continuous. they can also suffer frequentist difficulties including excess variability and overly long confidence intervals. cholesterol decrease q m inference after model selection bagging or bootstrap smoothing is a tactic for improving a discontinuous estimation rule by averaging in and chapter replications ft average suppose t is any estimator for which we have obtained bootstrap b d bg. the bagged version of t is the bx s.x d b t the letter s here stands for smooth. small changes in x even ones that move across a model selection definitional boundary produce only small changes in the bootstrap average s.x. averaging over the bootstrap replications of t x gave a bagged estimate sc.x for each value of c. bagging reduced the standard errors of the cpols estimates t x by about as indicated by the green dotted curve in figure t x where did the green dotted curve come from? all bootstrap values were needed to produce the single value sc.x. it seems as if we would need to bootstrap the bootstrap in order to computebse sc.x fortunately a more economical calculation is possible one that requires only the original b bootstrap computations for t x. define nbj d xj occurs in x for b d b and j d n. for instance d says that data point occurred three times in nonparametric bootstrap sample the b by n matrix fnbjg completely describes the b bootstrap samples. also denote where dots denote averaging over b d theorem the infinitesimal jackknife estimate of standard error b nbj and t d b t b b and let covj indicate the covariance in the bootstrap sample between nbj and t t d t bx t p covj d b p for the bagged estimate is bseij sc.x d nx j accuracy after model selection effort. keeping track of nbj as we generate the bootstrap replications t us to compute covj and bse sc.x without any additional computational figure the ratio ofbseij sc.x t x averaging in fact corollary the ratiobseij sc.x t x is always we expect averaging to reduce variability and this is seen to hold true in we have the following general result. allows the savings due to bagging increase with the nonlinearity of t as a function of the counts nbj in the language of section in the nonlinearity of s.p as a function of p. model-selection estimators such as the cpols rule tend toward greater nonlinearity and bigger savings. table proportion of nonparametric bootstrap replications of cpols algorithm that selected degrees m d also infinitesimal jackknife standard deviations for proportions which mostly exceed the estimates themselves. proportion bsdij m d the first line of table shows the proportions in which the various degrees were selected in the cholesterol bootstrap replications for linear for quadratic for cubic etc. with b d the proportions seem very accurate the binomial standard error for being just d for instance. indicate whether theorem suggests otherwise. now let t the bth bootstrap sample x made the cp choice m d d if m if m d t b d bg is the observed proportion the bagged value of ft applying the bagging theorem yieldedbseij d as seen in the inference after model selection second line of the table with similarly huge standard errors for the other proportions. the binomial standard errors are internal saying how quickly the bootstrap resampling process is converging to its ultimate value as b the infinitesimal jackknife estimates are external if we collected a new set of data pairs di the new proportion table might look completely different than the top line of table frequentist statistics has the advantage of being applicable to any algorithmic procedure for instance to our cpols estimator. this has great appeal in an era of enormous data sets and fast computation. the drawback compared with bayesian statistics is that we have no guarantee that our chosen algorithm is best in any way. classical statistics developed a theory of best for a catalog of comparatively simple estimation and testing problems. in this sense modern inferential theory has not yet caught up with modern problems such as data-based model selection though techniques such as model averaging bagging suggest promising steps forward. selection bias many a sports fan has been victimized by selection bias. your team does wonderfully well and tops the league standings. but the next year with the same players and the same opponents you re back in the pack. this is the winner s curse a more picturesque name for selection bias the tendency of unusually good bad comparative performances not to repeat themselves. modern scientific technology allows the simultaneous investigation of hundreds or thousands of candidate situations with the goal of choosing the top performers for subsequent study. this is a setup for the heartbreak of selection bias. an apt example is offered by the prostate study data of section where we observe statistics zi measuring patient control differences for n d genes zi n i d n here is the effect size for gene i the true difference between the patient and control populations. genes with large positive or negative values of would be promising targets for further investigation. gene number with d at selection bias tained the biggest z-value says that is unbiased for can we believe the obvious estimate d no is the correct selection bias answer. gene has won a contest for bigness among contenders. in addition to being good a large value of it has almost certainly been lucky with the noise in pushing in the positive direction or else it would not have won the contest. this is the essence of selection bias. false-discovery rate theory chapter provided a way to correct for selection bias in simultaneous hypothesis testing. this was extended to false-coverage rates in section our next vignette concerns the realistic estimation of effect sizes in the face of selection bias. we begin by assuming that an effect size has been obtained from a prior density might include discrete atoms and then z n observed and n is assumed known for this discussion. the marginal density of z is f dz exp where d tweedie s formula is an intriguing expression for the bayes expectation of given z. theorem observed z is in model the posterior expectation of having d d d z c dz with l log f the especially convenient feature of tweedie s formula is that is expressed directly in terms of the marginal density f this is a setup for empirical bayes estimation. we don t know but in large-scale situations we can estimate the marginal density f from the observations z d zn perhaps by poisson regression as in table yielding d zi c the solid curve in figure shows for the prostate study data d d with o log o f dz l l inference after model selection figure the solid curve is tweedie s estimate local false-discovery ratecfdr.z from figure scale on right. at z d d andcfdr.z d for gene for the prostate study data. the dashed line shows the with d tweedie s estimate is with d and o jzij agreeing with the local false-discovery rate curvecfdr.z of figf obtained using fourth-degree log polynomial regression as in section the curve has hovering near zero for cfdr.z d so even though zi d has a one-sided p-value of ure that says these are mostly null genes. increases for z equaling for z d at that point with genes to consider at once it still is not a sure thing that gene i is non-null. about of the genes with zi near will be non-null and these will have effect sizes averaging about all of this nicely illustrates the combination of frequentist and bayesian inference possible in large-scale studies and also the combination of estimation and hypothesis-testing ideas in play. if the prior density in is assumed to be normal tweedie s formula gives the james stein estimator the corresponding curve in figure in that case would be a straight line passing through the origin at slope like the james stein estimator ridge regression and the lasso of chapter tweedie s formula is a shrinkage estimator. for d the most extreme observation it gave valuee m zlllz fdrtweediefdr d shrinking the maximum likelihood estimate more than one unit toward the origin. selection bias bayes estimators are immune to selection bias as discussed in sections and this offers some hope that tweedie s empirical bayes estimates might be a realistic cure for the winners curse. a small simulation experiment was run as a test. a hundred data sets z each of length n d were generated accord ing to a combination of exponential and normal sampling e and n l.z was computed as in section now using a natural for i d for each z o spline model with five degrees of freedom. this gave tweedie s estimates d zi c o l i d for that data set z. for each data set z the largest zi values and the corresponding and values were recorded yielding the uncorrected differences and corrected differences zi the hope being that empirical bayes shrinkage would correct the selection bias in the zi values. figure shows the data sets top cases each uncorrected and corrected differences. selection bias is quite obvious with the uncorrected differences shifted one unit to the right of zero. in this case at least the empirical bayes corrections have worked well the corrected differences being nicely centered at zero. bias correction often adds variance but in this case it hasn t. finally it is worth saying that the empirical part of empirical bayes is less the estimation of bayesian rules from the aggregate data than the application of such rules to individual cases. for the prostate data we began with no definite prior opinions but arrived at strong not uninformative bayesian conclusions for say in the prostate study. inference after model selection figure corrected and uncorrected differences for top cases in each of simulations tweedie corrections effectively counteracted selection bias. combined bayes frequentist estimation as mentioned previously bayes estimates are at least theoretically immune from selection bias. let z d zn represent the prostate study data of the previous section with parameter vector d bayes rule d yields the posterior density of given z. a data-based model selection rule such as estimate the corresponding to the largest observation zi has no effect on the likelihood function z fixed or on having chosen a prior our posterior estimate of is unaffected by the fact that d happens to be largest. this same argument applies just as well to any data-based model selection procedure for instance a preliminary screening of possible variables to include in a regression analysis the cp choice of a cubic regression in figure having no effect on its bayes posterior accuracy. there is a catch the chosen prior must apply to the entire parameter vector and not just the part we are interested in this is differencesfrequency combined bayes frequentist estimation feasible in one-parameter situations like the stopping rule example of figure it becomes difficult and possibly dangerous in higher dimensions. empirical bayes methods such as tweedie s rule can be thought of as allowing the data vector z to assist in the choice of a high-dimensional prior an effective collaboration between bayesian and frequentist methodology. our chapter s final vignette concerns another bayes frequentist estimad ff tion technique. dropping the boldface notation suppose that f is a multi-dimensional family of densities with playing the role of and that we are interested in estimating a particular parameter d t a prior g. has been chosen yielding posterior expectation o d e ft how accurate is o the usual answer would be calculated from the posterior distribution of given x. this is obviously the correct answer if g. is based on genuine prior experience. most often though and especially in high-dimensional problems the prior reflects mathematical convenience and a desire to be uninformative as in chapter there is a danger of circular reasoning in using a self-selected prior distribution to calculate the accuracy of its own estimator. an alternate approach discussed next is to calculate the frequentist accuracy of o that is even though is a bayes estimate we consider o simply as a function of x and compute its frequentist variability. the next theorem leads to a computationally efficient way of doing so. bayes and frequentist standard errors for o operate in conceptually orthogonal directions as pictured in figure here we are supposing that the prior is unavailable or uncertain forcing more attention on frequentist calculations. for convenience we will take the family f to be a p-parameter expo nential family f d e now with being the parameter vector called above. the p p covariance matrix of x is denoted v d cov let covx indicate the posterior covariance given x between d t the parameter of interest and covx d covf t n o inference after model selection o a p vector. covx leads directly to a frequentist estimate of accuracy for o theorem the delta method estimate of standard error for o d eft isbsedelta x vo covx where vo is v evaluated at the mle o bsedeltaf o the theorem allows us to calculate the frequentist accuracy estimate with hardly any additional computational effort beyond that required for o itself. suppose we have used an mcmc or gibbs sampling algorithm section to generate a sample from the bayes posterior distribution of given x t these yield the usual estimate for eft bx o d t t bx b t and dp they also give a similar expression for covf t t d t t dp can o covx d b b b from which we for an example of theorem in action we consider the diabetes i the ith row of x the matrix of data of section with x prediction so xi is the vector of predictors for patient i. the response vector y of progression scores has now been rescaled to have d in the normal regression y nn.x i the prior distribution g. was taken to be g. d ce obtained from parametric bootstrap resampling taking the empirical covariance matrix vo may be known theoretically calculated by numerical differentiation in or of bootstrap replications o i by dividing the original data vector y by its estimated standard error from the linear model efyg d x combined bayes frequentist estimation with d and c the constant that makes g. integrate to this is the bayesian lasso prior so called because of its connection to the lasso and lasso plays no part in what follows. posterior distribution g. jy. let an mcmc algorithm generated b samples from the b i x i o it has bayes posterior standard error the expectation of the ith patient s response yi. the bayes posterior expectation of is d x bx o d d bx which we can compare withbsedelta. o the frequentist standard error figure shows the mcmc replications o i for patient i d the point estimate o i equaled with bayes and frequenbsebayes d and bsedelta d tist standard error estimates the frequentist standard error is smaller in this casebsedelta was less thanbsebayes for all patients the difference averaging a modest bsebayes i o things can work out differently. suppose we are interested in the poste d x b x c d rior cdf of given y. for any given value of c let c c c cdf.c d bx if x if x so t is our mcmc assessment of cjyg. the solid curve in figure graphs cdf.c. b t if we believe prior then the curve exactly represents the posterior distribution of given y for the simulation error due to stopping at b replications. whether or not we believe the prior we can use inference after model selection figure a histogram of mcmc replications for posterior distribution of expected progression for patient in the diabetes study model and prior the bayes posterior expectation is frequentist standard error for o posterior standard error d was smaller than bayes theorem with t d t in to evaluate the frequentist the dashed vertical red lines show cdf.c plus or minus onebsedelta unit. accuracy of the curve. the standard errors are disturbingly large for instance at c d the central credible interval for c-values between cdf.c and has frequentist standard errors about for each endpoint of the interval s length. if we believe prior then is an exact credible interval for and moreover is immune to any selection bias involved in our focus on if not the large frequentist standard errors are a reminder that calculation might turn out much differently in a new version of the diabetes study even ignoring selection bias. to return to our main theme bayesian calculations encourage a disregard for model selection effects. this can be dangerous in objective bayes mcmc errorsbayes posterior notes and details figure the solid curve is the posterior cdf of vertical red bars indicate one frequentist standard error as obtained from theorem black triangles are endpoints of the central credible interval. settings where one can t rely on genuine prior experience. theorem serves as a frequentist checkpoint offering some reassurance as in figure or sounding a warning as in figure notes and details optimality theories statements of best possible results are marks of maturity in applied mathematics. classical statistics achieved two such theories for unbiased or asymptotically unbiased estimation and for hypothesis testing. most of this book and all of this chapter venture beyond these safe havens. how far from best are the cpols bootstrap smoothed estimates of section at this time we can t answer such questions though we can offer appealing methodologies in their pursuit a few of which have been highlighted here. the cholestyramine example comes from efron and feldman where it is discussed at length. data for a control group is also analyzed there. scheff e intervals. scheff e s paper came at the beginning c inference after model selection of a period of healthy development in simultaneous inference techniques mostly in classical normal theory frameworks. miller gives a clear and thorough summary. the followed with a more computer-intensive approach nicely developed in westfall and young s book leading up to benjamini and hochberg s false-discovery rate paper here and benjamini and yekutieli s false-coverage rate algorithm. scheff e s construction is derived by transforming to the case v d i using the inverse square root of matrix v which makes the ellipsoid of figure into a circle. then q d in and for a linear combination d d it is straightforward to see that prfq k. pq g d amounts to and d v d v d v o kdk k. pq for all choices of d the geometry of figure now being transparent. changing coordinates back to o d v d v and c d v yields restricting the catalog c suppose that all the sample sizes nj in take the same value n and that we wish to set simultaneous confidence intervals for all pairwise differences i j tukey s studentized range pivotal quantity unpublished o i o j i j t d max i j has a distribution not depending on or this implies that i j o i o j p is a set of simultaneous level- confidence intervals for all pairwise differences i j where t is the th quantile of t factor n comes from o j t n n j in p table half-width of tukey studentized range simultaneous confidence intervals for pairwise differences i j units of n for p d and n d compared with scheff e intervals p tukey scheff e notes and details reducing the catalog c from all linear combinations c to only pairwise differences shortens the simultaneous intervals. table shows the comparison between the tukey and scheff e intervals for p d and n d calculating t was a substantial project in the early berk et al. now carry out the analogous computations for general catalogs of linear constraints. they discuss at length the inferential basis of such procedures. discontinuous estimators. looking at figure suggests that a confidence interval for t x will move far left for data sets x where cp selects linear regression d as best. this kind of jumpy behavior lengthens the intervals needed to attain a desired coverage level. more seriously intervals for m d may give misleading inferences another example of accurate but incorrect behavior. bagging in addition to reducing interval length improves inferential correctness as discussed in efron theorem and its corollary. theorem is proved in section of efron with a parametric bootstrap version appearing in section the corollary is a projection result illustrated in figure of that paper let l.n be the n-dimensional subspace of b-dimensional euclidean space spanned by the columns of the b n matrix and t the b-vector with components t t bseij.s bseboot.t kt then is the projection of t where ot if o will be substantially less than into l.n in the language of section d s.p is very nonlinear as a function of p then the ratio in tweedie s formula. for convenience take d in bayes rule can then be arranged to give d p with d z c log f this is a one-parameter exponential family having natural parameter equal to z. differentiating as in gives d d dz d z c d dz log f inference after model selection which is tweedie s formula when d the formula first appears in robbins who credits it to a personal communication from m. k. tweedie. efron discusses general exponential family versions of tweedie s formula and its application to selection bias situations. theorem the delta method standard error approximation for a statistic t is bsedelta dh ov where rt is the gradient vector and ov is an estimate of the covariance matrix of x. other names include the taylor series method as in and propagation of errors in the physical sciences literature. the proof of theorem in section of efron consists of showing that covx d rt when t d eft standard deviations are only a first step in assessing the frequentist accuracy of t the paper goes on to show how theorem can be improved to give confidence intervals correcting the impression in figure that cdf.c can range outside and prior gives bayesian lasso. applying bayes rule with density log g. jy d x c as discussed in tibshirani comparison with shows that the maximizing value of map estimate agrees with the lasso estimate. park and casella named the bayesian lasso and suggested an appropriate mcmc algorithm. their choice d was based on marginal maximum likelihood calculations giving their analysis an empirical bayes aspect ignored in their and our analyses. empirical bayes estimation strategies classic statistical inference was focused on the analysis of individual cases a single estimate a single hypothesis test. the interpretation of direct evidence bearing on the case of interest the number of successes and failures of a new drug in a clinical trial as a familiar example dominated statistical practice. the story of modern statistics very much involves indirect evidence learning from the experience of others in the language of sections and carried out in both frequentist and bayesian settings. the computerintensive prediction algorithms described in chapters use regression theory the frequentist s favored technique to mine indirect evidence on a massive scale. false-discovery rate theory chapter collects indirect evidence for hypothesis testing by means of bayes theorem as implemented through empirical bayes estimation. empirical bayes methodology has been less studied than bayesian or frequentist theory. as with the james stein estimator it can seem to be little more than plugging obvious frequentist estimates into bayes estimation rules. this conceals a subtle and difficult task learning the equivalent of a bayesian prior distribution from ongoing statistical observations. our final chapter concerns the empirical bayes learning process both as an exercise in applied deconvolution and as a relatively new form of statistical inference. this puts us back where we began in chapter examining the two faces of statistical analysis the algorithmic and the inferential. bayes deconvolution a familiar formulation of empirical bayes inference begins by assuming that an unknown prior density g. our object of interest has produced a random sample of real-valued variates n i d n g. i empirical bayes estimation strategies density may include discrete atoms of probability. the i are unobservable but each yields an observable random variable xi according to a known family of density functions pi i xi from the observed sample xn we wish to estimate the prior density g. a famous example has pi i the poisson family xi poi. i xi as in robbins formula section still more familiar is the normal model often with d a binomial model was used in the medical example of section n i xi bi.ni i there the ni differ from case to case accounting for the need for the first subscript i in pi i let fi denote the marginal density of xi obtained from fi dz t pi di the integral being over the space t of possible values. the statistician has only the marginal observations available fi xi i d n from which he or she wishes to estimate the density in in the normal model fi is the convolution of the unknown g. with a known normal density denoted f d g fi not depending on i. estimating g using a sample xn from f is a problem in deconvolution. in general we might call the estimation of g in model the bayes deconvolution problem. n an artificial example appears in figure where g. is a mixture distribution seven-eighths n and one-eighth uniform over the interval a normal sampling model xi n i is assumed yielding f by convolution as in the convolution process makes f wider bayes deconvolution figure an artificial example of the bayes deconvolution problem. the solid curve is g. the prior density of the dashed curve is the density of an observation x from marginal distribution f d g n we wish to estimate g. on the basis of a random sample xn from f and smoother than g as illustrated in the figure. having observed a random sample from f we wish to estimate the deconvolute g which begins to look difficult in the figure s example. deconvolution has a well-deserved reputation for difficulty. it is the classic ill-posed problem because of the convolution process large changes in g. are smoothed out often yielding only small changes in f deconvolution operates in the other direction with small changes in the estimation of f disturbingly magnified on the g scale. nevertheless modern computation modern theory and most of all modern sample sizes together can make empirical deconvolution a practical reality. why would we want to estimate g. in the prostate data example is called we might wish to know prf d the probability of a null gene ones whose effect size is zero or perhaps prfj j the proportion of genes that are substantially non-null. or we might want to estimate bayesian posterior expectations like ef jx d xg in figure or posterior densities as in figure two main strategies have developed for carrying out empirical bayes estimation modeling on the scale called g-modeling here and modeling and xgq and fxfxgq empirical bayes estimation strategies on the x scale called f we begin in the next section with gmodeling. g-modeling and estimation there has been a substantial amount of work on the asymptotic accuracy of estimates og. in the empirical bayes model most often in the normal sampling framework the results are discouraging with the rate of convergence of og. to g. as slow as n in our terminology much of this work has been carried out in a nonparametric gmodeling framework allowing the unknown prior density g. to be virtually anything at all. more optimistic results are possible if the g-modeling is pursued parametrically that is by restricting g. to lie within some parametric family of possibilities. we assume for the sake of simpler exposition that the space t of pos sible values is finite and discrete say the prior density g. is now represented by a vector g d gm with components t gj d pr d d for j d m a p-parameter exponential family for g can be written as g d g. d eq where the p-vector is the natural parameter and q is a known m p structure matrix. notation means that the j th component of g. is j the j th row of q the function is the normalizer that makes with q g. sum to gj d eq j mx d log j eq in the nodes example of figure the set of possible values was d and q was a fifth-degree polynomial matrix t q d polyt g-modeling and estimation in r notation indicating a five-parameter exponential family for g in the development that follows we will assume that the kernel pi in does not depend on i i.e. that xi has the same family of conditional distributions p.xij i for all i as in the poisson and normal situations and but not the binomial case and moreover we assume that the sample space x for the xi observations is finite and discrete say x d x.n pkj d pr xi d x.kj i d none of this is necessary but it simplifies the exposition. define for k d n and j d m and the corresponding n m matrix p d having kth row pk d pkm the convolution-type formula for the marginal density f now reduces to an inner product xi d x.k dpm pkj gj in fact we can write the entire marginal density f d in terms of matrix multiplication fn. fk. d pr d p kg. the vector of counts y d yn with f d pg. yk d xi d x.k is a sufficient statistic in the iid situation. it has a multinomial distribution y multn.n f indicating n independent draws for a density f on n categories. all of this provides a concise description of the g-modeling probability model g. d eq f d pg. y multn.n f empirical bayes estimation strategies the inferential task goes in the reverse direction y o f g.o d eqo figure a schematic diagram of empirical bayes estimation as explained in the text. sn is the n-dimensional simplex containing the p-parameter family f of allowable probability distributions f the vector of observed proportions yn yields mle f which is then deconvolved to obtain estimate g.o a schematic diagram of the estimation process appears in figure the vector of observed proportions yn is a point in sn the simplex the parametric family of allowable f vectors of all possible probability vectors f on n categories yn is the usual nonparametric estimate of f d ff ag f indicated by the red curve is a curved p-dimensional surface in sn. here a is the space of allowable vectors in family the nonparametric estimate yn is projected down to the parametric estimate f if we are using mle estimation f will be the closest point in f to yn measured according to a deviance metric as in finally f is mapped back to the estimate g.o by inverting mapping is not actually necessary with g-modeling since having found o g.o is obtained directly from the inversion step is more difficult for f section likelihood regularization and accuracy the maximum likelihood estimation process for g-modeling is discussed in more detail in the next section where formulas for its accuracy will be developed. likelihood regularization and parametric g-modeling as in allows us to work in low-dimensional parametric families just five parameters for the nodes example where classic maximum likelihood methods can be more confidently applied. even here though some regularization will be necessary for stable estimation as discussed in what follows. the g-model probability mechanism yields a log likelihood for the multinomial vector y of counts as a function of say ly fk. yk log fk. ly d log its score function p h d p determines the mle o according to p p p matrix of second derivatives r fisher information matrix ly the vector of partial derivatives h for ly d the ly d h l gives the ny d nx the exponential family model yields simple expressions for p and i. define ly ly i. d pkj wkj d gj fk. and the corresponding m-vector wk. d wkm. lemma the score function p ly under model is where wc. d nx p ly d qwc. wk. and q is the m p structure matrix in the technical lemmas in this section are not essential to following the subsequent discussion. empirical bayes estimation strategies lemma the fisher information matrix i. evaluated at d o is i.o d q wk.o q nx yk is the sample size in the empirical bayes model where n dpn see the chapter endnotes for a brief discussion of lemmas and i.o is the usual maximum likelihood estimate of the covariance matrix of o but we will use a regularized version of the mle that is less variable. in the examples that follow o was found by numerical even though g. is an exponential family the marginal density f in is not. as a result some care is needed in avoiding local maxima of ly these tend to occur at corner values of where one of its components goes to infinity. a small amount of regularization pulls o away from the corners decreasing its variance at the possible expense of increased bias. instead of maximizing ly we maximize a penalized likelihood m. d ly s. where s. is a positive penalty function. our examples use s. d k d equal which prevents the maximizer o of m. from venturing too far into corners. h the following lemma is discussed in the chapter endnotes. lemma the maximizer o of m. has approximate bias vector and covariance matrix bias.o d c rs.o and var.o d c rs.o ps.o i.o c rs.o px where i.o is given in with s. regularization the bias is zero and var.o d i.o using the nonlinear maximizer nlm in r. likelihood regularization and accuracy the usual mle approximations including s. reduces variance while introducing bias. for s. d k we calculate rs. d k i k ps. d k and with i the p p identity matrix. adding the penalty s. in pulls the mle of toward zero and the mle of g. toward a flat distribution over t looking at var.o in a measure of the regularization effect is tr.rs.o tr.i.o most often we will be more interested in the accuracy of og d g.o than which was never more than a few percent in our examples. in that of o itself. letting d.o d diag.g.o g.o the m p derivative matrix h is d d. with q the structure matrix in the usual first-order delta-method calculations then give the following theorem. theorem the penalized maximum likelihood estimate og d g.o has estimated bias vector and covariance matrix bias. og d d.o and var. og d d.o d.o with bias.o and var.o as in the many approximations going into theorem can be short-circuited by means of the parametric bootstrap section starting from o and f d pg.o we resample the count vector multn.n f based on y and the penalized mle o yielding og d g.o y note that the bias treats model as the true prior and arises as a result of the penalization. convergence of the nlm search process is speeded up by starting from o gives bias and covariance estimates b replications og empirical bayes estimation strategies og dbias d og and cvar d bx dpb og og og og og og og and og table comparison of delta method and bootstrap standard errors and biases for the nodes study estimate of g in figure all columns except the first multiplied by standard error bias g. delta delta boot boot table compares the delta method of theorem with the parametric bootstrap d replications for the surgical nodes example of section both the standard errors square roots of the diagonal elements of var. og and biases are well approximated by the delta method formulas the delta method also performed reasonably well on the two examples of the next section. it did less well on the artificial example of figure where g. d i c e d uniform on and n the vertical bars in figure indicate one standard error obtained from the parametric bootd for the sample space of and asstrap taking t suming a natural spline model in with five degrees of freedom g. d eq q d nst likelihood regularization and accuracy figure the red curve is g. for the artificial example of figure vertical bars are one standard error for g-model estimate g.o specifications sample size n d observations xi n i using parametric bootstrap b d the light dashed line follows bootstrap means og j some definitional bias is apparent. the sampling model was xi n i for i d n d in this case the delta method standard errors were about too small. the light dashed curve in figure traces ng. the average of the b d bootstrap replications g there is noticeable bias compared with g. the reason is simple the exponential family for g. does not include g. in fact ng. is the closest member of the exponential family to g. this kind of definitional bias is a disadvantage of parametric g-modeling. our g-modeling examples and those of the next section bring together a variety of themes from modern statistical practice classical maximum likelihood theory exponential family modeling regularization bootstrap methods large data sets of parallel structure indirect evidence and a combination of bayesian and frequentist thinking all of this enabled by massive computer power. taken together they paint an attractive picture of the range of inferential methodology in the twenty-first century. empirical bayes estimation strategies two examples we now reconsider two previous data sets from a g-modeling point of view. the first is the artificial microarray-type example comprising n independent observations zi with n i d n d for i d for i d n p figure displays the points for i d illustrating the bayes posterior conditional intervals these required knowing the bayes prior distribution n we would like to recover intervals using just the observed data zi i d without knowledge of the prior. figure histogram of observed sample of n d values zi from simulations a histogram of the z-values is shown in figure g-modeling was applied to them with playing the role of z-valuesfrequency two examples d q was composed of a and z being x taking t delta function at d and a fifth-degree polynomial basis for the nonzero again a family of spike-and-slab priors. the penalized mle og d estimated the probability of d as d which also provided bias estimate figure purple curves show g-modeling estimates of conditional credible intervals for given z in artificial microarray example they are a close match to the actual bayes intervals dotted lines cf. figure the estimated posterior density of given z is d cz the standard normal density and cz the constant required for to integrate to let q. denote the th quantile of the purple curves in figure trace the estimated credible intervals they are a close match to the actual credible intervals the solid black curve in figure shows for slab portion of the estimated prior. as an estimate of the actual slab density zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllby.loby.up empirical bayes estimation strategies figure the heavy black curve is the g-modeling estimate of for in the artificial microarray example suppressing the atom at zero d it is only a rough estimate of the actual nonzero density n n it is only roughly accurate but apparently still accurate enough to yield the reasonably good posterior intervals seen in figure the fundamental impediment to deconvolution that large changes in g. produce only small changes in f can sometimes operate in the statistician s favor when only a rough knowledge of g suffices for applied purposes. our second example concerns the prostate study data last seen in figure n d men cancer patients and normal controls each have had their genetic activities measured on a microarray of n d genes genei yields a test statistic zi comparing patients with controls zi n with the gene s effect size. we will take the variance parameter to be estimated rather than assuming density for the effects? as a d what is the prior the local false-discovery rate program locfdr section was applied to the zi values as shown in figure locfdr is an f modeling method where probability models are proposed directly for two examples figure the green curve is a six-parameter poisson regression estimate fit to counts of the observed zi values for the prostate data. the dashed curve is the empirical null zi n the f program locfdr estimated null probability d d genes with z-values lying beyond the red triangles have estimated fdr values less than the marginal density f rather than for the prior density see section here we can compare locfdr s results with those from gmodeling. the former d in the notation of that is it estimated the null distribution as n with probability d of a gene being null d only genes were estimated to have local fdr values less than the with zi and the with zi are more pessimistic results than in figure where we used the theoretical null n rather than the empirical null n the g-modeling approach was applied to the prostate study data assuming zi d as suggested by the n using a six-parameter poisson regression fit to the zi values of the type employed in section z-valuescounts empirical bayes estimation strategies structure matrix q in had a delta function at d and a fiveparameter natural spline basis for t d for the discretized space this gave a penalized mle og having null probability d figure the g-modeling estimate for the non-null density for the prostate study data also indicating the null atom d about of the genes are estimated to have effect sizes the red bars show one standard error as computed from theorem the non-null distribution for appears in figure where it is seen to be modestly unimodal around d dashed red bars indicate one standard error for the og..j estimates obtained from theorem the accuracy is not very good. it is better for larger regions of the space for examplebprfjj d here g-modeling estimated less prior null probability compared with from f but then attributed much of the non-null probability to small values of taking literally suggests genes with true generalized linear mixed models false-discovery d from g-modeling. for large figure the black curve is the empirical bayes estimated values of jzj it nearly matches the locfdr f estimate fdr.z red curve. effect sizes that doesn t mean we can say with certainty which figure compares the g-modeling empirical bayes false-discovery rate as in with the f estimatecfdr.z produced by locfdr. where it counts in the tails they are nearly the same. d d cz z generalized linear mixed models the g-modeling theory can be extended to the situation where each observation xi is accompanied by an observed vector of covariates ci say of dimension d. we return to the generalized linear model setup of section where each xi has a one-parameter exponential family density indexed by its own natural parameter d xi in notation empirical bayes estimation strategies our key assumption is that each is the sum of a deterministic compo nent depending on the covariates ci and a random term i d i c c i here i is an unobserved realization from g. d expfq and is an unknown d-dimensional parameter. if d then is a g-model as while if all the i d then it is a standard glm taken together represents a generalized linear mixed model the likelihood and accuracy calculations of section extend to glmms as referenced in the endnotes but here we will only discuss a glmm analysis of the nodes study of section in addition to ni the number of nodes removed and xi the number found positive a vector of four covariates ci d sexi smokei progi was observed for each patient a standardized version of age in years sex being for female or for male smoke being for no or for yes to longterm smoking and prog being a post-operative prognosis score with large values more favorable. glmm model was applied to the nodes data. now was the logit log where xi bi.ni as in table i.e. is the probability that any one node from patient i is positive. to make the correspondence with the analysis in section exact we used a variant of d logit. i c c i now with d i is exactly the binomial probability for the ith case. maximum likelihood estimates were calculated for in d and q d polyt and with t in the mle prior g.o was almost the same as that estimated without covariates in figure kbsek. sex table shows the mle values a parametric bootstrap simulation and the z-values o looks like it has a significant effect with males tending toward larger values of that is a greater number of positive nodes. the big effect though is prog larger values of prog indicating smaller values of here the setup is more specific f is exponential family and i is on the o their standard errors o o o natural-parameter scale. generalized linear mixed models o for glmm table maximum likelihood estimates analysis of the nodes data and standard errors from a parametric bootstrap simulation large values of progi predict low values of o o o age sex smoke prog mle boot st err z-value figure distribution of individual probabilities of a positive node for best and worst levels of factor prog from glmm analysis of nodes data. figure displays the distribution of d implied by the glmm model for the best and worst values of prog age sex and smoke to their average values and letting have distribution g.o the implied distribution is concentrated near d for the bestlevel prog while it is roughly uniform over for the worst level. the random effects we have called i are sometimes called frailties a composite of unmeasured individual factors lumped together as an index of disease susceptibility. taken together figures and show substantial frailty and covariate effects both at work in the nodes data. in positive nodedensityworst prognosisbest prognosis empirical bayes estimation strategies the language of section we have amassed indirect evidence for each patient using both bayesian and frequentist methods. deconvolution and f empirical bayes applications have traditionally been dominated by f modeling not the g-modeling approach of the previous sections where probability models for the marginal density f usually exponential families are fit directly to the observed sample xn we have seen several examples robbins estimator in table the bottom line locfdr s poisson regression estimates in figures and and tweedie s estimate in figure both the advantages and the disadvantages of f can be seen in the inferential diagram of figure for f the red curve now can represent an exponential family ff whose concave log likelihood function greatly simplifies the calculation of f from yn this comes at a price the deconvolution step from f to a prior distribution g.o is problematical as discussed below. this is only a problem if we want to know g. the traditional applications of f apply to problems where the desired answer can be phrased directly in terms of f this was the case for robbins formula the local false-discovery rate and tweedie s formula nevertheless f methodology for the estimation of the prior g. does exist an elegant example being the fourier method described next. a function f and its fourier transform are related by dz f tx dx and f d z for the normal case where xi d i c zi with zi transform of f is a multiple of that for g. d tx dt n the fourier so on the transform scale estimating g from f amounts to removing the factor exp.t the fourier method begins with the empirical density n f that puts probability on each observed value xi and then proceeds in three steps. n f is smoothed using the sinc kernel expressed directly as a kernel estimate nx where the kernel is og. d n dz z n f dx q f d n deconvolution and f nx sinc xi x q f say is calculated. sinc.x d sin.x x the fourier transform of finally og. is taken to be the inverse fourier transform of this last step eliminating the unwanted factor e a pleasantly surprising aspect of the fourier method is that og. can be in et cos.tx dt d large values of smooth n og. at the expense of increased bias. despite its compelling rationale there are two drawbacks to the fourier method. first of all it applies only to situations xi d i c zi where xi is i plus iid noise. more seriously the biasvariance trade-off in the choice of can be quite unfavorable. f more in reducing the variance of this is illustrated in figure for the artificial example of figure the black curve is the standard deviation of the g-modeling estimate of g. for in under specifications the red curve graphs the standard deviation of the f estimate with d a value that produced roughly the same amount of bias as the gmodeling estimate in figure the ratio of red to black standard deviations averages more than over the range of this comparison is at least partly unfair g-modeling is parametric while the fourier method is almost nonparametric in its assumptions about f or g. it can be greatly improved by beginning the three-step algorithm with a parametric estimate o f rather than n f the blue dotted curve in figure does this with o f a poisson regression on the data xn as in figure but here using a natural spline basis giving the estimate o f dx og. dz empirical bayes estimation strategies figure standard deviations of estimated prior density og. for the artificial example of figure based on n d observations xi n i black curve using g-modeling under specifications red curve nonparametric f d blue curve parametric f with o structure matrix having five degrees of freedom. f estimated from poisson regression with a we see a substantial decrease in standard deviation though still not attaining g-modeling rates. as commented before the great majority of empirical bayes applications have been of the robbinsfdrtweedie variety where f is the natural choice. g-modeling comes into its own for situations like the nodes data analysis of figures and where we really want an estimate of the prior g. twenty-first-century science is producing more such data sets an impetus for the further development of g-modeling strategies. table concerns the g-modeling estimation of ex d ef jx d xg ex dz t t d g. d for the artificial example under the same specifications as in figure samples of size n d of xi n i were drawn from model yielding mle og. and estimates oex for x between gqg modelparametricf modelnon parametricf model deconvolution and f table standard deviation of oef jxg computed from parametric bootstrap simulations of og. the g-modeling is as in figure with n d observations xi each simulation. the column info is the implied empirical bayes information for estimating ef jxg obtained from one other observation xi. n i from the artificial example for x ef jxg sd. oe info and one thousand such estimates oex were generated averaging almost exactly ex with standard deviations as shown. accuracy is reasonably good the coefficient of variation sd. oexex being about for large values of jxj. is a favorable case results are worse for other conditional estimates such as ef d xg. theorem implies that for large values of the sample size n the variance of oex decreases as say var n oex n var ix d o cxn n oex o by analogy with the fisher information bound we can define the empirical bayes information for estimating ex in one observation to be x so that varf oexg i empirical bayes inference leads us directly into the world of indirect evidence learning from the experience of others as in sections and so if xi d each other observation xj provides units of information for learning ef jxi d with the usual fisher d for the direct estimation of i from xi. this information value i is a favorable case as mentioned and ix is often much smaller. the main point perhaps is that assuming a bayes prior is not a casual matter and empirical bayes estimation strategies can amount to the assumption of an enormous amount of relevant other information. notes and details empirical bayes and james stein estimation chapters and exploded onto the statistics scene almost simultaneously in the they represented a genuinely new branch of statistical inference unlike the computerbased extensions of classical methodology reviewed in previous chapters. their development as practical tools has been comparatively slow. the pace has quickened in the twenty-first century with false-discovery rates chapter as a major step forward. a practical empirical bayes methodology for use beyond traditional f venues such as fdr is the goal of the g-modeling approach. lemmas and the derivations of lemmas and are straightforward but somewhat involved exercises in differential calculus carried out in remark b of efron here we will present just p fk. d a sample of the calculations. from the gradient vector l with respect to is p fk. d pg. where pg. is the m p derivative matrix pk pg. d l d dq with d as in the last equality following after some work by differentiation of log g. d q let lk d log fk suppressing from the notation. the gradient with respect to of lk is then p lk d p fkfk d q dpkfk the vector dpkfk has components pkj gj fkfk d wkj pk d fk. this gives p using g the independent score functions p score p ly d q o d pm.o lemma the penalized mle o satisfies ykwk. which is lemma pm. c rm. lk d q wk. adding up lk over the full sample yields the overall notes and details pm. rm. where is the true value of or o standard mle theory shows that the random variable p and covariance fisher information matrix i. while ically approximates i. substituting in c rs. ly c rs. o ly ps. ly has mean ly asymptot where z has mean and covariance i. this gives bias.o and var.o as in lemma note that the bias is with respect to a true parametric model and is a consequence of the penalization. the sinc kernel. the fourier transform of the scaled sinc function s.x d is the indicator of the interval while that of exp.i txj formula is the convolution n f s so q f is nx f has the product transform qf d i ei txj n n the effect of the sinc convolution is to censor the high-frequency t n f or nf larger yields more censoring. formula components of has upper limits because of all of this is due to stefanski and carroll smoothers other than the sinc kernel have been suggested in the literature but without substantial improvements on deconvolution performance. conditional expectation efron considers estimating ef d xg and other such conditional expectations both for f modeling and for g-modeling. ef jx d xg is by far the easiest case as might be expected from the simple form of tweedie s estimate epilogue something important changed in the world of statistics in the new millennium. twentieth-century statistics even after the heated expansion of its late period could still be contained within the classic bayesian frequentist fisherian inferential triangle this is not so in the twenty-first century. some of the topics discussed in part iii false-discovery rates post-selection inference empirical bayes modeling the lasso fit within the triangle but others seem to have escaped heading south from the frequentist corner perhaps in the direction of computer science. the escapees were the large-scale prediction algorithms of chapters neural nets deep learning boosting random forests and support-vector machines. notably missing from their development were parametric probability models the building blocks of classical inference. prediction algorithms are the media stars of the big-data era. it is worth asking why they have taken center stage and what it means for the future of the statistics discipline. the why is easy enough prediction is commercially valuable. modern equipment has enabled the collection of mountainous data troves which the data miners can then burrow into extracting valuable information. moreover prediction is the simplest use of regression theory it can be carried out successfully without probability models perhaps with the assistance of nonparametric analysis tools such as cross-validation permutations and the bootstrap. a great amount of ingenuity and experimentation has gone into the development of modern prediction algorithms with statisticians playing an important but not dominant there is no shortage of impressive success stories. in the absence of optimality criteria either frequentist or bayesian the prediction community grades algorithmic excellence on per all papers mentioned in this section have their complete references in the bibliography. footnotes will identify papers not fully specified in the text. epilogue formance within a catalog of often-visited examples such as the spam and digits data sets of chapters and meanwhile traditional statistics probability models optimality criteria bayes priors asymptotics has continued successfully along on a parallel track. pessimistically or optimistically one can consider this as a bipolar disorder of the field or as a healthy duality that is bound to improve both branches. there are historical and intellectual arguments favoring the optimists side of the story. the first thing to say is that the current situation is not entirely unprecedented. by the end of the nineteenth century there was available an impressive inventory of statistical methods bayes theorem least squares correlation regression the multivariate normal distribution but these existed more as individual algorithms than as a unified discipline. statistics as a distinct intellectual enterprise was not yet well-formed. a small but crucial step forward was taken in when the astrophysicist arthur claimed that mean absolute deviation was superior to the familiar root mean square estimate for the standard deviation from a normal sample. fisher in showed that this was wrong and moreover in a clear mathematical sense the root mean square was the best possible estimate. eddington conceded the point while fisher went on to develop the theory of sufficiency and optimal optimal is the key word here. before fisher statisticians didn t really understand estimation. the same can be said now about prediction. despite their impressive performance on a raft of test problems it might still be possible to do much better than neural nets deep learning random forests and boosting or perhaps they are coming close to some as-yet unknown theoretical minimum. it is the job of statistical inference to connect dangling algorithms to the central core of well-understood methodology. the connection process is already underway. section showed how adaboost the original machine learning algorithm could be restated as a close cousin of logistic regression. purely empirical approaches like the common task framework are ultimately unsatisfying without some form of principled justification. our optimistic scenario has the big-datadata-science prediction world rejoining the mainstream of statistical inference to the benefit of both branches. this empirical approach to optimality is sometimes codified as the common task framework and donoho eddington became world-famous for his empirical verification of einstein s relativity theory. see stigler for the full story. epilogue development of the statistics discipline since the end of the nineteenth century as discussed in the text. whether or not we can predict the future of statistics we can at least examine the past to see how we ve gotten where we are. the next figure does so in terms of a new triangle diagram this time with the poles labeled applications mathematics and computation. mathematics here is shorthand for the mathematicallogical justification of statistical methods. computation stands for the empiricalnumerical approach. statistics is a branch of applied mathematics and is ultimately judged by how well it serves the world of applications. mathematical logic la fisher has been the traditional vehicle for the development and understanding of statistical methods. computation slow and difficult before the was only a bottleneck but now has emerged as a competitor to perhaps a seven-league boots enabler of mathematical analysis. at any one time the discipline s energy and excitement is directed unequally toward the three poles. the figure attempts in admittedly crude fashion to track the changes in direction over the past years. epilogue the tour begins at the end of the nineteenth century. mathematicians of the caliber of gauss and laplace had contributed to the available methodology but the subsequent development was almost entirely applicationsdriven. was especially influential applying the gauss laplace formulation to census data and his average man. a modern reader will search almost in vain for any mathematical symbology in nineteenth-century statistics journals. karl pearson s chi-square paper was a bold step into the new century applying a new mathematical tool matrix theory in the service of statistical methodology. he and weldon went on to found biometrika in the first recognizably modern statistics journal. pearson s paper and biometrika launched the statistics discipline on a fifty-year march toward the mathematics pole of the triangle. student s t statistic was a crucial first result in small-sample exact inference and a major influence on fisher s thinking. fisher s great estimation paper a more coherent version of its predecessor. it introduced a host of fundamental ideas including sufficiency efficiency fisher information maximum likelihood theory and the notion of optimal estimation. optimality is a mark of maturity in mathematics making the year statistical inference went from a collection of ingenious techniques to a coherent discipline. this represents neyman and pearson s paper on optimal hypothesis testing. a logical completion of fisher s program it nevertheless aroused his strong antipathy. this was partly personal but also reflected fisher s concern that mathematization was squeezing intuitive correctness out of statistical thinking neyman s seminal paper on confidence intervals. his sophisticated mathematical treatment of statistical inference was a harbinger of decision theory. adolphe quetelet was a tireless organizer helping found the royal statistical society in with the american statistical association following in epilogue the publication of wald s statistical decision functions. decision theory completed the full mathematization of statistical inference. this date can also stand for savage s and de finetti s decision-theoretic formulation of bayesian inference. we are as far as possible from the applications corner of the triangle now and it is fair to describe the as a nadir of the influence of the statistics discipline on scientific applications. the arrival of electronic computation in the mid began the process of stirring statistics out of its inward-gazing preoccupation with mathematical structure. tukey s paper the future of data analysis argued for a more application- and computation-oriented discipline. mosteller and tukey later suggested changing the field s name to data analysis a prescient hint of today s data science. cox s proportional hazards paper. immensely useful in its own right it signaled a growing interest in biostatistical applications and particularly survival analysis which was to assert its scientific importance in the analysis of aids epidemic data. the bootstrap and later the widespread use of mcmc electronic computation used for the extension of classic statistical inference. this stands for false-discovery rates and a year later the both are computer-intensive algorithms firmly rooted in the ethos of statistical inference. they lead however in different directions as indicated by the split in the diagram. microarray technology inspires enormous interest in large-scale inference both in theory and as applied to the analysis of microbiological data. benjamini and hochberg and tibshirani epilogue random forests it joins and the resurgence of neural nets in the ranks of machine learning prediction algorithms. data science a more popular successor to tukey and mosteller s data analysis at one extreme it seems to represent a statistics discipline without parametric probability models or formal inference. the data science association defines a practitioner as one who uses scientific methods to liberate and create meaning from raw data. in practice the emphasis is on the algorithmic processing of large data sets for the extraction of useful information with the prediction algorithms as exemplars. this represents the traditional line of statistical thinking of the kind that could be located within figure but now energized with a renewed focus on applications. of particular applied interest are biology and genetics. genome-wide association studies show a different face of big data. prediction is important but not sufficient for the scientific understanding of disease. a cohesive inferential theory was forged in the first half of the twentieth century but unity came at the price of an inwardly focused discipline of reduced practical utility. in the century s second half electronic computation unleashed a vast expansion of useful and much used statistical methodology. expansion accelerated at the turn of the millennium further increasing the reach of statistical thinking but now at the price of intellectual cohesion. it is tempting but risky to speculate on the future of statistics. what will the mathematics applications computation diagram look like say years from now? the appetite for statistical analysis seems to be always increasing both from science and from society in general. data science has blossomed in response but so has the traditional wing of the field. the data-analytic initiatives represented in the diagram by and are in actuality not isolated points but the centers of overlapping distributions. breiman for random forests freund and schapire for boosting. personalized medicine in which an individual s genome predicts his or her optimal treatment has attracted grail-like attention. epilogue a hopeful scenario for the future is one of an increasing overlap that puts data science on a solid footing while leading to a broader general formulation of statistical inference. references abu-mostafa y. hints. neural computation achanta r. and hastie t. telugu ocr framework using deep learning. tech. rept. statistics department stanford university. akaike h. information theory and an extension of the maximum likelihood principle. pages of second international symposium on information theory akad emiai kiad o budapest. anderson t. w. an introduction to multivariate statistical analysis. third edn. wiley series in probability and statistics. wiley-interscience. bastien f. lamblin p. pascanu r. bergstra j. goodfellow i. j. bergeron a. bouchard n. and bengio y. theano new features and speed improvements. deep learning and unsupervised feature learning nips workshop. becker r. chambers j. and wilks a. the new s language a programming environment for data analysis and graphics. pacific grove ca wadsworth and brookscole. bellhouse d. r. the reverend thomas bayes frs a biography to celebrate the tercentenary of his birth. statist. sci. with comments and a rejoinder by the author. bengio y. courville a. and vincent p. representation learning a review and new perspectives. ieee transactions on pattern analysis and machine intelligence benjamini y. and hochberg y. controlling the false discovery rate a practical and powerful approach to multiple testing. j. roy. statist. soc. ser. b benjamini y. and yekutieli d. false discovery rate-adjusted multiple confi dence intervals for selected parameters. j. amer. statist. assoc. berger j. o. the case for objective bayesian analysis. bayesian anal. berger j. o. and pericchi l. r. the intrinsic bayes factor for model selection and prediction. j. amer. statist. assoc. bergstra j. breuleux o. bastien f. lamblin p. pascanu r. desjardins g. turian j. warde-farley d. and bengio y. theano a cpu and gpu math expression compiler. in proceedings of the python for scientific computing conference berk r. brown l. buja a. zhang k. and zhao l. valid post-selection inference. ann. statist. references berkson j. application of the logistic function to bio-assay. j. amer. statist. assoc. bernardo j. m. reference posterior distributions for bayesian inference. j. roy. birch m. w. the detection of partial association. i. the case. j. roy. statist. statist. soc. ser. b with discussion. soc. ser. b bishop c. neural networks for pattern recognition. clarendon press oxford. boos d. d. and serfling r. j. a note on differentials and the clt and lil for statistical functions with application to m ann. statist. boser b. guyon i. and vapnik v. a training algorithm for optimal margin classifiers. in proceedings of colt ii. breiman l. bagging predictors. mach. learn. breiman l. arcing classifiers discussion. annals of statistics breiman l. random forests. machine learning breiman l. friedman j. olshen r. a. and stone c. j. classification and regression trees. wadsworth statisticsprobability series. wadsworth advanced books and software. carlin b. p. and louis t. a. bayes and empirical bayes methods for data analysis. monographs on statistics and applied probability vol. chapman hall. carlin b. p. and louis t. a. bayes and empirical bayes methods for data analysis. edn. texts in statistical science. chapman hallcrc. chambers j. m. and hastie t. j. statistical models in s. chapman hall computer science series. chapman hall. cleveland w. s. lowess a program for smoothing scatterplots by robust locally weighted regression. amer. statist. cox d. r. the regression analysis of binary sequences. j. roy. statist. soc. ser. b cox d. r. the analysis of binary data. methuen s monographs on applied probability and statistics. methuen co. cox d. r. regression models and life-tables. j. roy. statist. soc. ser. b cox d. r. partial likelihood. biometrika cox d. r. and hinkley d. v. theoretical statistics. chapman hall. cox d. r. and reid n. parameter orthogonality and approximate conditional inference. j. roy. statist. soc. ser. b with a discussion. crowley j. asymptotic normality of a new nonparametric statistic for use in organ transplant studies. j. amer. statist. assoc. de finetti b. probability induction and statistics. the art of guessing. john wiley sons london-new york-sydney. dembo a. cover t. m. and thomas j. a. information-theoretic inequalities. ieee trans. inform. theory dempster a. p. laird n. m. and rubin d. b. maximum likelihood from incomplete data via the em algorithm. j. roy. statist. soc. ser. b diaconis p. and ylvisaker d. conjugate priors for exponential families. ann. statist. references diciccio t. and efron b. more accurate confidence intervals in exponential families. biometrika donoho d. l. years of data science. r-bloggers. www.r-bloggers. edwards a. w. f. likelihood. expanded edn. johns hopkins university press. revised reprint of the original. efron b. the two sample problem with censored data. pages of proc. berkeley symp. math. statist. and prob. vol. university of california press. efron b. defining the curvature of a statistical problem applications to second order efficiency. ann. statist. with discussion and a reply by the author. efron b. the efficiency of cox s likelihood function for censored data. j. amer. statist. assoc. efron b. bootstrap methods another look at the jackknife. ann. statist. efron b. the jackknife the bootstrap and other resampling plans. cbms-nsf regional conference series in applied mathematics vol. society for industrial and applied mathematics efron b. estimating the error rate of a prediction rule improvement on cross validation. j. amer. statist. assoc. efron b. bootstrap confidence intervals for a class of parametric problems. biometrika efron b. how biased is the apparent error rate of a prediction rule? j. amer. statist. assoc. efron b. better bootstrap confidence intervals. j. amer. statist. assoc. with comments and a rejoinder by the author. efron b. logistic regression survival analysis and the kaplan meier curve. j. amer. statist. assoc. efron b. bayes and likelihood calculations from confidence intervals. biometrika efron b. r. a. fisher in the century paper presented at the r. a. fisher lecture. statist. sci. with comments and a rejoinder by the author. efron b. the estimation of prediction error covariance penalties and crossvalidation. j. amer. statist. assoc. with comments and a rejoinder by the author. efron b. large-scale inference empirical bayes methods for estimation testing and prediction. institute of mathematical statistics monographs vol. cambridge university press. efron b. tweedie s formula and selection bias. j. amer. statist. assoc. efron b. estimation and accuracy after model selection. j. amer. statist. assoc. efron b. two modeling strategies for empirical bayes estimation. statist. sci. efron b. frequentist accuracy of bayesian estimates. j. roy. statist. soc. ser. b references efron b. empirical bayes deconvolution estimates. biometrika efron b. and feldman d. compliance as an explanatory variable in clinical trials. j. amer. statist. assoc. efron b. and gous a. scales of evidence for model selection fisher versus jeffreys. pages of model selection. ims lecture notes monograph series vol. beachwood oh institute of mathematics and statististics. with discussion and a rejoinder by the authors. efron b. and hinkley d. v. assessing the accuracy of the maximum likelihood estimator observed versus expected fisher information. biometrika with comments and a reply by the authors. efron b. and morris c. limiting the risk of bayes and empirical bayes estima tors. ii. the empirical bayes case. j. amer. statist. assoc. efron b. and morris c. stein s paradox in statistics. scientific american efron b. and petrosian v. a simple test of independence for truncated data with applications to redshift surveys. astrophys. j. efron b. and stein c. the jackknife estimate of variance. ann. statist. efron b. and thisted r. estimating the number of unseen species how many words did shakespeare know? biometrika efron b. and tibshirani r. an introduction to the bootstrap. monographs on statistics and applied probability vol. chapman hall. efron b. and tibshirani r. improvements on cross-validation the boot strap method. j. amer. statist. assoc. efron b. hastie t. johnstone i. and tibshirani r. least angle regression. annals of statistics discussion and a rejoinder by the authors. finney d. j. the estimation from individual records of the relationship between dose and quantal response. biometrika fisher r. a. frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population. biometrika fisher r. a. theory of statistical estimation. math. proc. cambridge phil. soc. fisher r. a. inverse probability. math. proc. cambridge phil. soc. fisher r. a. corbet a. and williams c. the relation between the number of species and the number of individuals in a random sample of an animal population. j. anim. ecol. fithian w. sun d. and taylor j. optimal inference after model selection. arxiv e-prints oct. freund y. and schapire r. experiments with a new boosting algorithm. pages of machine learning proceedings of the thirteenth international conference. morgan kauffman san francisco. freund y. and schapire r. a decision-theoretic generalization of online learning and an application to boosting. journal of computer and system sciences friedman j. greedy function approximation a gradient boosting machine. an nals of statistics references friedman j. and popescu b. predictive learning via rule ensembles. tech. rept. stanford university. friedman j. hastie t. and tibshirani r. additive logistic regression a statis tical view of boosting discussion. annals of statistics friedman j. hastie t. and tibshirani r. glmnet lasso and elastic-net regu larized generalized linear models. r package version friedman j. hastie t. and tibshirani r. regularization paths for generalized linear models via coordinate descent. journal of statistical software geisser s. a predictive approach to the random effect model. biometrika gerber m. and chopin n. sequential quasi monte carlo. j. roy. statist. soc. b with discussion doi gholami s. janson l. worhunsky d. j. tran t. b. squires malcolm i. jin l. x. spolverato g. votanopoulos k. i. schmidt c. weber s. m. bloomston m. cho c. s. levine e. a. fields r. c. pawlik t. m. maithel s. k. efron b. norton j. a. and poultsides g. a. number of lymph nodes removed and survival after gastric cancer resection an analysis from the us gastric cancer collaborative. j. amer. coll. surg. good i. and toulmin g. the number of new species and the increase in popu lation coverage when a sample is increased. biometrika hall p. theoretical comparison of bootstrap confidence intervals. ann. statist. with discussion and a reply by the author. hampel f. r. the influence curve and its role in robust estimation. j. amer. statist. assoc. hampel f. r. ronchetti e. m. rousseeuw p. j. and stahel w. a. robust statistics the approach based on influence functions. wiley series in probability and mathematical statistics. john wiley sons. harford t. big data a big mistake? significance hastie t. and loader c. local regression automatic kernel carpentry discussion. statistical science hastie t. and tibshirani r. generalized additive models. chapman and hall. hastie t. and tibshirani r. efficient quadratic regularization for expression arrays. biostatistics hastie t. tibshirani r. and friedman j. the elements of statistical learning. data mining inference and prediction. second edn. springer series in statistics. springer. hastie t. tibshirani r. and wainwright m. statistical learning with sparsity the lasso and generalizations. chapman and hall crc press. hoeffding w. the large-sample power of tests based on permutations of obser vations. ann. math. statist. hoeffding w. asymptotically optimal tests for multinomial distributions. ann. math. statist. hoerl a. e. and kennard r. w. ridge regression biased estimation for nonor thogonal problems. technometrics huber p. j. robust estimation of a location parameter. ann. math. statist. references jaeckel l. a. estimating regression coefficients by minimizing the dispersion of the residuals. ann. math. statist. james w. and stein c. estimation with quadratic loss. pages of proc. berkeley symposium on mathematical statistics and probability vol. i. university of california press. jansen l. fithian w. and hastie t. effective degrees of freedom a flawed metaphor. biometrika javanmard a. and montanari a. confidence intervals and hypothesis testing for high-dimensional regression. j. of machine learning res. jaynes e. prior probabilities. ieee trans. syst. sci. cybernet. jeffreys h. theory of probability. third ed. clarendon press. johnson n. l. and kotz s. distributions in statistics discrete distributions. houghton mifflin co. johnson n. l. and kotz s. distributions in statistics. continuous univariate distributions. houghton mifflin co. johnson n. l. and kotz s. distributions in statistics. continuous univariate distributions. houghton mifflin co. johnson n. l. and kotz s. distributions in statistics continuous multivariate distributions. john wiley sons. kaplan e. l. and meier p. nonparametric estimation from incomplete obser vations. j. amer. statist. assoc. kass r. e. and raftery a. e. bayes factors. j. amer. statist. assoc. kass r. e. and wasserman l. the selection of prior distributions by formal rules. j. amer. statist. assoc. kuffner r. zach n. norel r. hawe j. schoenfeld d. wang l. li g. fang l. mackey l. hardiman o. cudkowicz m. sherman a. ertaylan g. grossewentrup m. hothorn t. van ligtenberg j. macke j. h. meyer t. scholkopf b. tran l. vaughan r. stolovitzky g. and leitner m. l. crowdsourced analysis of clinical trial data to predict amyotrophic lateral sclerosis progression. nat biotech lecun y. and cortes c. mnist handwritten digit database. httpyann.lecun.comexdbmnist. lecun y. bengio y. and hinton g. deep learning. nature lee j. sun d. sun y. and taylor j. exact post-selection inference with application to the lasso. annals of statistics lehmann e. l. theory of point estimation. wiley series in probability and mathematical statistics. john wiley sons. leslie c. eskin e. cohen a. weston j. and noble w. s. mismatch string kernels for discriminative pretein classification. bioinformatics liaw a. and wiener m. classification and regression by randomforest. r news liberman m. reproducible research and the common task method simons foundation frontiers of data science lecture april video available. references lockhart r. taylor j. tibshirani r. and tibshirani r. a significance test for the lasso. annals of statistics with discussion and a rejoinder by the authors. lynden-bell d. a method for allowing for known observational selection in small samples applied to quasars. mon. not. roy. astron. soc. mallows c. l. some comments on cp. technometrics mantel n. and haenszel w. statistical aspects of the analysis of data from retrospective studies of disease. j. natl. cancer inst. mardia k. v. kent j. t. and bibby j. m. multivariate analysis. academic press. mccullagh p. and nelder j. generalized linear models. monographs on statis tics and applied probability. chapman hall. mccullagh p. and nelder j. generalized linear models. second edn. mono graphs on statistics and applied probability. chapman hall. metropolis n. rosenbluth a. w. rosenbluth m. n. teller a. h. and teller e. equation of state calculations by fast computing machines. j. chem. phys. miller jr r. g. a trustworthy jackknife. ann. math. statist miller jr r. g. simultaneous statistical inference. second edn. springer series in statistics. new york springer-verlag. nesterov y. gradient methods for minimizing composite functions. mathemati cal programming neyman j. outline of a theory of statistical estimation based on the classical theory of probability. phil. trans. roy. soc. neyman j. frequentist probability and frequentist statistics. synthese neyman j. and pearson e. s. on the problem of the most efficient tests of statistical hypotheses. phil. trans. roy. soc. a ng a. neural networks. httpdeeplearning.stanford.edu wikiindex.phpneural_networks. lecture notes. ngiam j. chen z. chia d. koh p. w. le q. v. and ng a. tiled convolutional neural networks. pages of lafferty j. williams c. shawetaylor j. zemel r. and culotta a. advances in neural information processing systems curran associates inc. o hagan a. fractional bayes factors for model comparison. j. roy. statist. soc. ser. b with discussion and a reply by the author. park t. and casella g. the bayesian lasso. j. amer. statist. assoc. pearson k. on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. phil. mag. pritchard j. stephens m. and donnelly p. inference of population structure using multilocus genotype data. genetics quenouille m. h. notes on bias in estimation. biometrika r core team. r a language and environment for statistical computing. r foundation for statistical computing vienna austria. references ridgeway g. generalized boosted models a guide to the gbm package. avail able online. ridgeway g. and macdonald j. m. doubly robust internal benchmarking and false discovery rates for detecting racial bias in police stops. j. amer. statist. assoc. ripley b. d. pattern recognition and neural networks. cambridge university press. robbins h. an empirical bayes approach to statistics. pages of proc. berkeley symposium on mathematical statistics and probability vol. i. university of california press. rosset s. zhu j. and hastie t. margin maximizing loss functions. in thrun s. saul l. and sch olkopf b. advances in neural information processing systems mit press. rubin d. b. the bayesian bootstrap. ann. statist. savage l. j. the foundations of statistics. john wiley sons chapman hill. schapire r. the strength of weak learnability. machine learning schapire r. and freund y. boosting foundations and algorithms. mit press. scheff e h. a method for judging all contrasts in the analysis of variance. biometrika sch olkopf b. and smola a. learning with kernels support vector machines regularization optimization and beyond computation and machine learning. mit press. schwarz g. estimating the dimension of a model. ann. statist. senn s. a note concerning a selection paradox of dawid s. amer. statist. soric b. statistical discoveries and effect-size estimation. j. amer. statist. assoc. spevack m. a complete and systematic concordance to the works of shake speare. vol. georg olms verlag. srivastava n. hinton g. krizhevsky a. sutskever i. and salakhutdinov r. dropout a simple way to prevent neural networks from overfitting. j. of machine learning res. stefanski l. and carroll r. j. deconvoluting kernel density estimators. statis tics stein c. inadmissibility of the usual estimator for the mean of a multivariate normal distribution. pages of proc. berkeley symposium on mathematical statististics and probability vol. i. university of california press. stein c. estimation of the mean of a multivariate normal distribution. ann. statist. stein c. on the coverage probability of confidence sets based on a prior distribution. pages of sequential methods in statistics. banach center publication vol. pwn warsaw. stigler s. m. how ronald fisher became a mathematical statistician. math. sci. hum. math. soc. sci. references stone m. cross-validatory choice and assessment of statistical predictions. j. roy. statist. soc. b with discussion and a reply by the author. storey j. d. taylor j. and siegmund d. strong control conservative point estimation and simultaneous conservative consistency of false discovery rates a unified approach. j. roy. statist. soc. b tanner m. a. and wong w. h. the calculation of posterior distributions by data augmentation. j. amer. statist. assoc. with discussion and a reply by the authors. taylor j. loftus j. and tibshirani r. tests in adaptive regression via the kac rice formula. annals of statistics thisted r. and efron b. did shakespeare write a newly-discovered poem? biometrika tibshirani r. noninformative priors for one parameter of many. biometrika tibshirani r. regression shrinkage and selection via the lasso. j. roy. statist. soc. b tibshirani r. a simple method for assessing sample sizes in microarray experi ments. bmc bioinformatics tibshirani r. bien j. friedman j. hastie t. simon n. taylor j. and tibshirani r. strong rules for discarding predictors in lasso-type problems. j. roy. statist. soc. b tibshirani r. tibshirani r. taylor j. loftus j. and reid s. selectiveinfer ence tools for post-selection inference. r package version tukey j. w. bias and confidence in not-quite large samples in abstracts of papers. ann. math. statist. tukey j. w. a survey of sampling from contaminated distributions. pages of contributions to probability and statistics essays in honor of harold hotelling olkin et. al ed.. stanford university press. tukey j. w. the future of data analysis. ann. math. statist. tukey j. w. exploratory data analysis. behavioral science series. addison wesley. van de geer s. b uhlmann p. ritov y. and dezeure r. on asymptotically optimal confidence regions and tests for high-dimensional models. annals of statistics vapnik v. the nature of statistical learning theory. springer. wager s. wang s. and liang p. s. dropout training as adaptive regularization. pages of burges c. bottou l. welling m. ghahramani z. and weinberger k. advances in neural information processing systems curran associates inc. wager s. hastie t. and efron b. confidence intervals for random forests the jacknife and the infintesimal jacknife. j. of machine learning res. wahba g. spline models for observational data. siam. wahba g. lin y. and zhang h. gacv for support vector machines. pages of smola a. bartlett p. sch olkopf b. and schuurmans d. advances in large margin classifiers. mit press. wald a. statistical decision functions. john wiley sons chapman hall. references wedderburn r. w. m. quasi-likelihood functions generalized linear models and the gauss newton method. biometrika welch b. l. and peers h. w. on formulae for confidence points based on integrals of weighted likelihoods. j. roy. statist. soc. b westfall p. and young s. resampling-based multiple testing examples and methods for p-value adjustment. wiley series in probability and statistics. wileyinterscience. xie m. and singh k. confidence distribution the frequentist distribution esti mator of a parameter a review. int. statist. rev. with discussion. ye j. on measuring and correcting the effects of data mining and model selec tion. j. amer. statist. assoc. zhang c.-h. and zhang s. confidence intervals for low-dimensional parame ters with high-dimensional data. j. roy. statist. soc. b zou h. hastie t. and tibshirani r. on the degrees of freedom of the lasso. ann. statist. author index abu-mostafa y. achanta r. akaike h. anderson t. w. bastien f. becker r. bellhouse d. r. bengio y. benjamini y. berger j. o. bergeron a. bergstra j. berk r. berkson j. bernardo j. m. bibby j. m. bien j. birch m. w. bishop c. bloomston m. boos d. d. boser b. bouchard n. breiman l. breuleux o. brown l. b uhlmann p. buja a. carlin b. p. carroll r. j. casella g. chambers j. chen z. chia d. cho c. s. chopin n. cleveland w. s. cohen a. corbet a. cortes c. courville a. cover t. m. cox d. r. crowley j. cudkowicz m. de finetti b. dembo a. dempster a. p. desjardins g. dezeure r. diaconis p. diciccio t. donnelly p. donoho d. l. edwards a. w. f. efron b. ertaylan g. eskin e. fang l. feldman d. fields r. c. finney d. j. fisher r. a. fithian w. freund y. friedman j. geisser s. gerber m. gholami s. good i. author index goodfellow i. j. gous a. grosse-wentrup m. guyon i. haenszel w. hall p. hampel f. r. hardiman o. harford t. hastie t. hawe j. hinkley d. v. hinton g. hochberg y. hoeffding w. hoerl a. e. hothorn t. huber p. j. jaeckel l. a. james w. jansen l. janson l. javanmard a. jaynes e. jeffreys h. jin l. x. johnson n. l. johnstone i. kaplan e. l. kass r. e. kennard r. w. kent j. t. koh p. w. kotz s. krizhevsky a. kuffner r. laird n. m. lamblin p. le q. v. lecun y. lee j. lehmann e. l. leitner m. l. leslie c. levine e. a. li g. liang p. s. liaw a. liberman m. lin y. loader c. lockhart r. loftus j. louis t. a. lynden-bell d. macdonald j. m. macke j. h. mackey l. maithel s. k. mallows c. l. mantel n. mardia k. v. mccullagh p. meier p. metropolis n. meyer t. miller r. g. jr montanari a. morris c. nelder j. nesterov y. neyman j. ng a. ngiam j. noble w. s. norel r. norton j. a. o hagan a. olshen r. a. park t. pascanu r. pawlik t. m. pearson e. s. pearson k. peers h. w. pericchi l. r. petrosian v. popescu b. poultsides g. a. pritchard j. quenouille m. h. r core team raftery a. e. reid n. reid s. ridgeway g. ripley b. d. ritov y. author index robbins h. ronchetti e. m. rosenbluth a. w. rosenbluth m. n. rosset s. rousseeuw p. j. rubin d. b. salakhutdinov r. savage l. j. schapire r. scheff e h. schmidt c. schoenfeld d. sch olkopf b. schwarz g. senn s. serfling r. j. sherman a. siegmund d. simon n. singh k. smola a. soric b. spevack m. spolverato g. squires i. malcolm srivastava n. stahel w. a. stefanski l. stein c. stephens m. stigler s. m. stolovitzky g. stone c. j. stone m. storey j. d. sun d. sun y. sutskever i. tanner m. a. taylor j. teller a. h. teller e. thisted r. thomas j. a. tibshirani r. toulmin g. tran l. tran t. b. tukey j. w. turian j. van de geer s. van ligtenberg j. vapnik v. vaughan r. vincent p. votanopoulos k. i. wager s. wahba g. wainwright m. wald a. wang l. wang s. warde-farley d. wasserman l. weber s. m. wedderburn r. w. m. welch b. l. westfall p. weston j. wiener m. wilks a. williams c. wong w. h. worhunsky d. j. xie m. ye j. yekutieli d. ylvisaker d. young s. zach n. zhang c.-h. zhang h. zhang k. zhang s. zhao l. zhu j. zou h. subject index abc method accelerated gradient descent acceleration accuracy after model selection accurate but not correct activation function leaky rectified linear rectified linear relu tanh active set adaboost algorithm adaptation adaptive estimator adaptive rate control additive model adaptive adjusted compliance admixture modeling aic see akaike information criterion akaike information criterion allele frequency american statistical association ancillary apparent error arcsin transformation arthur eddington asymptotics xvi autoencoder backfitting backpropagation bagged estimate bagging balance equations barycentric plot basis expansion bayes deconvolution factor false-discovery rate posterior distribution posterior probability shrinkage t-statistic theorem inference information criterion lasso lasso prior model selection trees bayes frequentist estimation bayesian bayesian information criterion bayesianism bca accuracy and correctness confidence density interval method benjamini and hochberg benjamini yekutieli bernoulli best-approximating linear subspace best-subset selection beta distribution bhq bias bias-corrected and accelerated see bca method confidence intervals percentile method subject index bias-correction value biased estimation bic see bayesian information criterion big-data era xv binomial coherent behavior common task framework compliance computational bottleneck computer age xv computer-intensive conditional inference inference statistics conditional conditional distribution full lasso conditionality confidence density distribution interval region conjugate prior priors filters layer convolution distribution log-likelihood standard deviation bioassay biometrika bivariate normal bonferroni bound boole s inequality boosting bootstrap baron munchausen bayesian cdf confidence intervals ideal estimate jackknife after moving blocks multisample nonparametric out of bootstrap packages parametric probabilities replication sample sample size smoothing t t intervals bound form bounding hyperplane burn-in byq algorithm causal inference xvi censored data not truncated centering central limit theorem chain rule for differentiation classic statistical inference classification classification accuracy classification error classification tree cochran mantel haenszel test convex optimization corrected differences correlation effects covariance formula penalty coverage coverage level coverage matching prior cox model see proportional hazards model cp cram er rao lower bound credible interval cross-validation estimate k-fold leave one out cumulant generating function curse of dimensionality dark energy data analysis data science xvii subject index data sets binomial gamma gaussian normal poisson divide-and-conquer algorithm document retrieval dose response dropout learning dti see diffusion tensor imaging early computer-age xvi early stopping effect size efficiency eigenratio elastic net ellipsoid em algorithm missing data empirical bayes estimation strategies information large-scale testing empirical null estimation maximum-likelihood estimation empirical probability distribution ensemble ephemeral predictors epoch equilibrium distribution equivariant exact inferences expectation parameter experimental design xvi exponential family p-parameter curved one-parameter f distribution f tests f fake-data principle false coverage control false discovery control control theorem proportion rate false-discovery als aml see leukemia baseball butterfly cell infusion cholesterol diabetes dose-response galaxy handwritten digits headneck cancer human ancestry insurance kidney function leukemia ncog nodes pediatric cancer police prostate protein classification shakespear spam student score supernova vasoconstriction data snooping de finetti b. de finetti savage school debias decision rule decision theory xvi deconvolution deep learning definitional bias degrees of freedom delta method deviance deviance residual diffusion tensor imaging direct evidence directional derivatives distribution beta subject index rate family of probability densities family-wise error rate fdr see false-discovery rate feed-forward fiducial constructions density inference fisher fisher information bound matrix fisherian correctness fisherian inference fixed-knot regression splines flat prior forward pass forward-stagewise fitting forward-stepwise computations logistic regression regression fully connected layer functional gradient descent fwer see family-wise error rate g-modeling gamma distribution general estimating equations xvi general information criterion generalized linear mixed model linear model ridge problem genome genome-wide association studies gibbs sampling glm see generalized linear model glmm see generalized linear mixed model frailties frequentism fourier method transform frequentist inference strongly google flu trends gradient boosting gradient descent gram matrix gram-schmidt orthogonalization graphical lasso graphical models xvi greenwood s formula group lasso hadamard product handwritten digits haplotype estimation hazard rate parametric estimate hidden layer high-order interaction hinge loss hints learning with hoeffding s lemma holm s procedure homotopy path hypergeometric distribution imputation inadmissible indirect evidence inductive inference inference inference after model selection inferential triangle infinitesimal forward stagewise infinitesimal jackknife estimate standard deviations influence function empirical influenza outbreaks input distortion input layer insample error inverse chi-squared inverse gamma irls see iteratively reweighted least iteratively reweighted least squares squares jackknife estimate of standard error standard error subject index estimation ridge regression james stein jeffreys prior jeffreys prior prior multiparameter scale jumpiness of estimator kaplan meier estimate karush kuhn tucker optimality conditions kernel function logistic regression method svm trick kernel smoothing knots kullback leibler distance regularization lagrange dual form multiplier large-scale hypothesis testing testing large-scale prediction algorithms lasso modification path penalty learning from the experience of others learning rate least squares least-angle regression least-favorable family left-truncated lehmann alternative life table likelihood function concavity limited-translation rule lindsey s method linearly separable link function local false-discovery rate local regression local translation invariance log polynomial regression log-rank statistic log-rank test logic of inductive inference logistic regression multiclass logit loss plus penalty machine learning mallows cp see cp mantel haenzel test map map estimate margin marginal density markov chain monte carlo see mcmc markov chain theory martingale theory matching prior matlab matrix completion max pool layer maximized a-posteriori probability see map maximum likelihood maximum likelihood estimation mcmc mcnemar test mean absolute deviation median unbiased memory-based methods meter reader meter-reader microarrays minitab misclassification error missing data em algorithm missing-species problem mixed features mixture density model averaging model selection criteria monotone lasso monotonic increasing function multinomial subject index distribution from poisson multiple testing multivariate analysis normal n-gram n-p complete nadaraya watson estimator natural parameter natural spline model ncog see northern california oncology group nested models neural information processing systems neural network adaptive tuning number of hidden layers neurons neyman s construction predictor one-sample nonparametric bootstrap one-sample problems oob see out-of-bag error optical character recognition optimal separating hyperplane optimal-margin classifier optimality oracle orthogonal parameters out-of-bag error out-the-box learning algorithm output layer outsample error over parametrized overfitting overshrinks p-value packageprogram gbm glmnet lars liblinear locfdr lowess nlm randomforest selectiveinference pairwise inner products parameter space parametric bootstrap parametric family parametric models partial likelihood partial logistic regression partial residual path-wise coordinate descent penalized least squares likelihood logistic regression maximum likelihood percentile method central interval permutation null permutation test phylogenetic tree piecewise neyman pearson non-null noncentral chi-square variable nonlinear transformations nonlinearity nonparameteric regression nonparametric mle percentile interval normal correlation coefficient distribution multivariate regression model theory nuclear norm nuisance parameters objective bayes inference intervals prior distribution northern california oncology group ocr see optical character recognition offset ols algorithm estimation subject index linear nonlinear pivotal argument quantity statistic rule poisson distribution regression poisson regression polynomial kernel positive-definite function post-selection inference posterior density posterior distribution postwar era prediction errors rule predictors principal components prior distribution beta conjugate coverage matching gamma normal objective bayes proper probit analysis propagation of errors proper prior proportional hazards model proximal-newton q-value qq plot qr decomposition quadratic program quasilikelihood quetelet adolphe r random forest adaptive nearest-neighbor estimator leave-one-out cross-validated error monte carlo variance interval theorem score function score tests second-order accuracy sampling variance standard error randomization rao blackwell rate annealing rectified linear regression regression rule regression to the mean regression tree regularization path relevance relevance function relevance theory reproducing kernel hilbert space resampling plans simplex vector residual deviance response ridge regression james stein ridge regularization logistic regression right-censored risk set rkhs see reproducing-kernel hilbert space robbins formula robust estimation royal statistical society s language sample correlation coefficient sample size coherency sampling distribution sas savage l. j. scale of evidence fisher jeffreys scheff e subject index selection bias self-consistent separating hyperplane geometry seven-league boots shrinkage estimator sigmoid function significance level simulation simultaneous confidence intervals simultaneous inference sinc kernel single-nucleotide polymorphism see snp smoothing operator snp soft margin classifier soft-threshold softmax spam filter sparse models principal components sparse matrix sparsity split-variable randomization spss squared error standard candles standard error external internal standard interval stein s paradox unbiased risk estimate stepwise selection stochastic gradient descent stopping rule stopping rules string kernel strong rules structure structure matrix student t confidence interval distribution statistic two-sample studentized range subgradient condition equation subjective prior distribution subjective probability subjectivism sufficiency sufficient statistic vector supervised learning support set vector vector classifiers vector machine sure see stein s unbiased risk estimate survival analysis survival curve svm lagrange dual lagrange primal loss function taylor series theoretical null tied weights time series xvi training set transformation invariance transient episodes trees averaging best-first depth terminal node tricube kernel trimmed mean triple-point xv true error rate true-discovery rates tukey j. w. tukey j. w. tweedie s formula twenty-first-century methods xvi two-groups model uncorrected differences uninformative prior universal approximator unlabeled images subject index unobserved covariates validation set vapnik v. variable-importance plot variance variance reduction velocity vector voting warm starts weak learner weight decay regularization sharing weighted exponential loss weighted least squares weighted majority vote weights wide data wilks likelihood ratio statistic winner s curse winsorized mean working response z. zero set