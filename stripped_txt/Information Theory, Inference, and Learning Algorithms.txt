copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. information theory inference and learning algorithms david j.c. mackay copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. information theory inference and learning algorithms david j.c. mackay mackaymrao.cam.ac.uk university press version printing march please send feedback on this book via httpwww.inference.phy.cam.ac.ukmackayitila version of this book was published by c.u.p. in september it will remain viewable on-screen on the above website in postscript djvu and pdf formats. in the second printing minor typos were corrected and the book design was slightly altered to modify the placement of section numbers. in the third printing minor typos were corrected and chapter was renamed dependent random variables of correlated in the fourth printing minor typos were corrected. replace this page with their own page ii. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. contents preface introduction to information theory probability entropy and inference more about inference i data compression the source coding theorem symbol codes stream codes codes for integers v ii noisy-channel coding dependent random variables communication over a noisy channel the noisy-channel coding theorem error-correcting codes and real channels iii further topics in information theory hash codes codes for information retrieval binary codes very good linear codes exist further exercises on information theory message passing communication over constrained noiseless channels crosswords and codebreaking why have sex? information acquisition and evolution iv probabilities and inference an example inference task clustering exact inference by complete enumeration maximum likelihood and clustering useful probability distributions exact marginalization exact marginalization in trellises exact marginalization in graphs laplace s method copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models model comparison and occam s razor monte carlo methods monte carlo methods exact monte carlo sampling variational methods independent component analysis and latent variable modelling random inference topics decision theory bayesian inference and sampling theory v neural networks introduction to neural networks the single neuron as a capacity of a single neuron learning as inference networks boltzmann machines supervised learning in multilayer networks gaussian processes deconvolution vi sparse graph codes low-density parity-check codes convolutional codes and turbo codes repeataccumulate codes digital fountain codes vii appendices a notation b some physics c some mathematics bibliography index copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. preface this book is aimed at senior undergraduates and graduate students in engineering science mathematics and computing. it expects familiarity with calculus probability theory and linear algebra as taught in a or secondyear undergraduate course on mathematics for scientists and engineers. conventional courses on information theory cover not only the beautiful theoretical ideas of shannon but also practical solutions to communication problems. this book goes further bringing in bayesian data modelling monte carlo methods variational methods clustering algorithms and neural networks. why unify information theory and machine learning? because they are two sides of the same coin. in the a single cybernetics was populated by information theorists computer scientists and neuroscientists all studying common problems. information theory and machine learning still belong together. brains are the ultimate compression and communication systems. and the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning. how to use this book the essential dependencies between chapters are indicated in the on the next page. an arrow from one chapter to another indicates that the second chapter requires some of the within parts i ii iv and v of this book chapters on advanced or optional topics are towards the end. all chapters of part iii are optional on a reading except perhaps for chapter passing. the same system sometimes applies within a chapter the sections often deal with advanced topics that can be skipped on a reading. for example in two key chapters chapter source coding theorem and chapter noisy-channel coding theorem the reader should detour at section and section respectively. pages viix show a few ways to use this book. first i give the roadmap for a course that i teach in cambridge information theory pattern recognition and neural networks the book is also intended as a textbook for traditional courses in information theory. the second roadmap shows the chapters for an introductory information theory course and the third for a course aimed at an understanding of state-of-the-art error-correcting codes. the fourth roadmap shows how to use the text in a conventional course on machine learning. v vi probability entropy and inference more about inference i data compression the source coding theorem symbol codes stream codes codes for integers dependent random variables communication over a noisy channel the noisy-channel coding theorem error-correcting codes and real channels iii further topics in information theory hash codes binary codes an example inference task clustering exact inference by complete enumeration maximum likelihood and clustering useful probability distributions exact marginalization exact marginalization in trellises exact marginalization in graphs laplace s method model comparison and occam s razor monte carlo methods ising models exact monte carlo sampling variational methods independent component analysis random inference topics decision theory bayesian inference and sampling theory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory iv probabilities and inference preface ii noisy-channel coding monte carlo methods very good linear codes exist further exercises on information theory v neural networks message passing constrained noiseless channels crosswords and codebreaking why have sex? dependencies introduction to neural networks the single neuron as a capacity of a single neuron learning as inference networks boltzmann machines supervised learning in multilayer networks gaussian processes deconvolution vi sparse graph codes low-density parity-check codes convolutional codes and turbo codes repeataccumulate codes digital fountain codes preface probability entropy and inference probability entropy and inference more about inference more about inference i data compression the source coding theorem the source coding theorem symbol codes symbol codes stream codes stream codes codes for integers dependent random variables dependent random variables communication over a noisy channel communication over a noisy channel the noisy-channel coding theorem the noisy-channel coding theorem error-correcting codes and real channels error-correcting codes and real channels iii further topics in information theory hash codes binary codes an example inference task clustering an example inference task clustering exact inference by complete enumeration exact inference by complete enumeration maximum likelihood and clustering maximum likelihood and clustering useful probability distributions exact marginalization exact marginalization exact marginalization in trellises exact marginalization in graphs laplace s method laplace s method model comparison and occam s razor monte carlo methods monte carlo methods ising models ising models exact monte carlo sampling exact monte carlo sampling variational methods variational methods independent component analysis random inference topics decision theory bayesian inference and sampling theory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory introduction to information theory iv probabilities and inference vii ii noisy-channel coding monte carlo methods monte carlo methods very good linear codes exist further exercises on information theory v neural networks message passing constrained noiseless channels crosswords and codebreaking why have sex? my cambridge course on information theory pattern recognition and neural networks introduction to neural networks introduction to neural networks the single neuron as a the single neuron as a capacity of a single neuron capacity of a single neuron learning as inference learning as inference networks networks boltzmann machines supervised learning in multilayer networks gaussian processes deconvolution vi sparse graph codes low-density parity-check codes low-density parity-check codes convolutional codes and turbo codes repeataccumulate codes digital fountain codes viii probability entropy and inference probability entropy and inference more about inference i data compression the source coding theorem the source coding theorem symbol codes symbol codes stream codes stream codes codes for integers dependent random variables dependent random variables communication over a noisy channel communication over a noisy channel the noisy-channel coding theorem the noisy-channel coding theorem error-correcting codes and real channels iii further topics in information theory hash codes binary codes an example inference task clustering exact inference by complete enumeration maximum likelihood and clustering useful probability distributions exact marginalization exact marginalization in trellises exact marginalization in graphs laplace s method model comparison and occam s razor monte carlo methods ising models exact monte carlo sampling variational methods independent component analysis random inference topics decision theory bayesian inference and sampling theory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory introduction to information theory iv probabilities and inference preface ii noisy-channel coding monte carlo methods very good linear codes exist further exercises on information theory v neural networks message passing constrained noiseless channels crosswords and codebreaking why have sex? short course on information theory introduction to neural networks the single neuron as a capacity of a single neuron learning as inference networks boltzmann machines supervised learning in multilayer networks gaussian processes deconvolution vi sparse graph codes low-density parity-check codes convolutional codes and turbo codes repeataccumulate codes digital fountain codes preface probability entropy and inference more about inference i data compression the source coding theorem symbol codes stream codes codes for integers dependent random variables communication over a noisy channel the noisy-channel coding theorem error-correcting codes and real channels error-correcting codes and real channels iii further topics in information theory hash codes hash codes binary codes binary codes an example inference task clustering exact inference by complete enumeration maximum likelihood and clustering useful probability distributions exact marginalization exact marginalization exact marginalization in trellises exact marginalization in trellises exact marginalization in graphs exact marginalization in graphs laplace s method model comparison and occam s razor monte carlo methods ising models exact monte carlo sampling variational methods independent component analysis random inference topics decision theory bayesian inference and sampling theory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory iv probabilities and inference ix ii noisy-channel coding monte carlo methods very good linear codes exist very good linear codes exist further exercises on information theory further exercises on information theory v neural networks message passing message passing constrained noiseless channels constrained noiseless channels crosswords and codebreaking why have sex? advanced course on information theory and coding introduction to neural networks the single neuron as a capacity of a single neuron learning as inference networks boltzmann machines supervised learning in multilayer networks gaussian processes deconvolution vi sparse graph codes low-density parity-check codes low-density parity-check codes convolutional codes and turbo codes convolutional codes and turbo codes repeataccumulate codes repeataccumulate codes digital fountain codes digital fountain codes x probability entropy and inference probability entropy and inference more about inference more about inference i data compression the source coding theorem symbol codes stream codes codes for integers dependent random variables communication over a noisy channel the noisy-channel coding theorem error-correcting codes and real channels iii further topics in information theory hash codes binary codes an example inference task clustering an example inference task clustering exact inference by complete enumeration exact inference by complete enumeration maximum likelihood and clustering maximum likelihood and clustering useful probability distributions exact marginalization exact marginalization exact marginalization in trellises exact marginalization in graphs laplace s method laplace s method model comparison and occam s razor model comparison and occam s razor monte carlo methods monte carlo methods ising models ising models exact monte carlo sampling exact monte carlo sampling variational methods variational methods independent component analysis independent component analysis random inference topics decision theory bayesian inference and sampling theory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory iv probabilities and inference preface ii noisy-channel coding monte carlo methods monte carlo methods very good linear codes exist further exercises on information theory v neural networks message passing constrained noiseless channels crosswords and codebreaking why have sex? a course on bayesian inference and machine learning introduction to neural networks introduction to neural networks the single neuron as a the single neuron as a capacity of a single neuron capacity of a single neuron learning as inference learning as inference networks networks boltzmann machines boltzmann machines supervised learning in multilayer networks supervised learning in multilayer networks gaussian processes gaussian processes deconvolution vi sparse graph codes low-density parity-check codes convolutional codes and turbo codes repeataccumulate codes digital fountain codes copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. xi preface about the exercises you can understand a subject only by creating it for yourself. the exercises play an essential role in this book. for guidance each has a rating to that used by knuth from to to indicate its in addition exercises that are especially recommended are marked by a marginal encouraging rat. some exercises that require the use of a computer are marked with a c. answers to many exercises are provided. use them wisely. where a solution is provided this is indicated by including its page number alongside the rating. solutions to many of the other exercises will be supplied to instructors using this book in their teaching please email solutionscambridge.org. summary of codes for exercises especially recommended c recommended parts require a computer solution provided on page simple minute medium hour moderately hard hard research project internet resources the website httpwww.inference.phy.cam.ac.ukmackayitila contains several resources software. teaching software that i use in lectures interactive software and research software written in perl octave tcl c and gnuplot. also some animations. corrections to the book. thank you in advance for emailing these! this book. the book is provided in postscript pdf and djvu formats for on-screen viewing. the same copyright restrictions apply as to a normal book. about this edition this is the fourth printing of the edition. in the second printing the design of the book was altered slightly. page-numbering generally remained unchanged except in chapters and where a few paragraphs and equations moved around. all equation section and exercise numbers were unchanged. in the third printing chapter was renamed dependent random variables instead of correlated which was sloppy. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. preface xii acknowledgments i am most grateful to the organizations who have supported me while this book gestated the royal society and darwin college who gave me a fantastic research fellowship in the early years the university of cambridge the keck centre at the university of california in san francisco where i spent a productive sabbatical and the gatsby charitable foundation whose support gave me the freedom to break out of the escher staircase that book-writing had become. my work has depended on the generosity of free software authors. i wrote the book in latex three cheers for donald knuth and leslie lamport! our computers run the gnulinux operating system. i use emacs perl and gnuplot every day. thank you richard stallman thank you linus torvalds thank you everyone. many readers too numerous to name here have given feedback on the book and to them all i extend my sincere acknowledgments. i especially wish to thank all the students and colleagues at cambridge university who have attended my lectures on information theory and machine learning over the last nine years. the members of the inference research group have given immense support and i thank them all for their generosity and patience over the last ten years mark gibbs michelle povinelli simon wilson coryn bailer-jones matthew davey katriona macphee james miskin david ward edward ratzer seb wills john barry john winn phil cowans hanna wallach matthew garrett and especially sanjoy mahajan. thank you too to graeme mitchison mike cates and davin yap. finally i would like to express my debt to my personal heroes the mentors from whom i have learned so much yaser abu-mostafa andrew blake john bridle peter cheeseman steve gull hinton john steve luttrell robert mackay bob mceliece radford neal roger sewell and john skilling. dedication this book is dedicated to the campaign against the arms trade. www.caat.org.uk peace cannot be kept by force. it can only be achieved through understanding. albert einstein copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter in the chapter you will need to be familiar with the binomial distribution. and to solve the exercises in the text which i urge you to do you will need to know stirling s approximation for the factorial function x! xx and be able to apply it to r! these topics are reviewed below. n unfamiliar notation? see appendix a the binomial distribution example a bent coin has probability f of coming up heads. the coin is tossed n times. what is the probability distribution of the number of heads r? what are the mean and variance of r? solution. the number of heads has a binomial distribution. p j f n f the mean er and variance varr of this distribution are by r n p j f n r figure the binomial distribution p j f n er varr ehr n p j f n rather than evaluating the sums over r in and directly it is easiest to obtain the mean and variance by noting that r is the sum of n independent random variables namely the number of heads in the toss is either zero or one the number of heads in the second toss and so forth. in general ex y ex ey varx y varx vary for any random variables x and y if x and y are independent so the mean of r is the sum of the means of those random variables and the variance of r is the sum of their variances. the mean number of heads in a single toss is f f f and the variance of the number of heads in a single toss is f f f f f f so the mean and variance of r are er n f and varr n f f copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter approximating x! and let s derive stirling s approximation by an unconventional route. we start from the poisson distribution with mean p j r! r for large this distribution is well approximated at least in the vicinity of r by a gaussian distribution with mean and variance r! let s plug r into this formula then rearrange it. this is stirling s approximation for the factorial function. x! xx ln x! x ln x x ln r figure the poisson distribution p j we have derived not only the leading order behaviour x! xx but also at no cost the next-order correction term we now apply stirling s approximation to ln r! r! r ln n n r r ln n n r since all the terms in this equation are logarithms this result can be rewritten in any base. we will denote natural logarithms e by ln and logarithms to base by log if we introduce the binary entropy function recall that x loge x loge loge note that x x x log x log then we can rewrite the approximation as or equivalently n x if we need a more accurate approximation we can include terms of the next order from stirling s approximation figure the binary entropy function. n n r copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory the fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. shannon in the half of this book we study how to measure information content we learn how to compress data and we learn how to communicate perfectly over imperfect communication channels. we start by getting a feeling for this last problem. how can we achieve perfect communication over an imperfect noisy communication channel? some examples of noisy communication channels are an analogue telephone line over which two modems communicate digital information the radio communication link from galileo the jupiter-orbiting space craft to earth reproducing cells in which the daughter cells dna contains information from the parent cells a disk drive. the last example shows that communication doesn t have to involve information going from one place to another. when we write a on a disk drive we ll read it in the same location but at a later time. these channels are noisy. a telephone line from cross-talk with other lines the hardware in the line distorts and adds noise to the transmitted signal. the deep space network that listens to galileo s puny transmitter receives background radiation from terrestrial and cosmic sources. dna is subject to mutations and damage. a disk drive which writes a binary digit one or zero also known as a bit by aligning a patch of magnetic material in one of two orientations may later fail to read out the stored binary digit the patch of material might spontaneously magnetization or a glitch of background noise might cause the reading circuit to report the wrong value for the binary digit or the writing head might not induce the magnetization in the place because of interference from neighbouring bits. in all these cases if we transmit data e.g. a string of bits over the channel there is some probability that the received message will not be identical to the modem phone line modem galileo radio waves earth parent cell daughter cell daughter cell computer memory disk drive computer memory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory transmitted message. we would prefer to have a communication channel for which this probability was zero or so close to zero that for practical purposes it is indistinguishable from zero. let s consider a noisy disk drive that transmits each bit correctly with probability and incorrectly with probability f this model communication channel is known as the binary symmetric channel x y p x f p x f p x f p x f f f f figure the binary symmetric channel. the transmitted symbol is x and the received symbol y. the noise level the probability that a bit is is f figure a binary data sequence of length transmitted over a binary symmetric channel with noise level f image copyright united feature syndicate inc. used with permission. as an example let s imagine that f that is ten per cent of the bits are a useful disk drive would no bits at all in its entire lifetime. if we expect to read and write a gigabyte per day for ten years we require a bit error probability of the order of or smaller. there are two approaches to this goal. the physical solution the physical solution is to improve the physical characteristics of the communication channel to reduce its error probability. we could improve our disk drive by using more reliable components in its circuitry evacuating the air from the disk enclosure so as to eliminate the turbu lence that perturbs the reading head from the track using a larger magnetic patch to represent each bit or using higher-power signals or cooling the circuitry in order to reduce thermal noise. these physical typically increase the cost of the communication channel. the system solution information theory and coding theory an alternative much more exciting approach we accept the given noisy channel as it is and add communication systems to it so that we can detect and correct the errors introduced by the channel. as shown in we add an encoder before the channel and a decoder after it. the encoder encodes the source message s into a transmitted message t adding redundancy to the original message in some way. the channel adds noise to the transmitted message yielding a received message r. the decoder uses the known redundancy introduced by the encoding system to infer both the original signal s and the added noise. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes for the binary symmetric channel source s encoder t decoder r noisy channel figure the system solution for achieving reliable communication over a noisy channel. the encoding system introduces systematic redundancy into the transmitted vector t. the decoding system uses this known redundancy to deduce from the received vector r both the original source vector and the noise introduced by the channel. whereas physical solutions give incremental channel improvements only at an ever-increasing cost system solutions can turn noisy channels into reliable communication channels with the only cost being a computational requirement at the encoder and decoder. information theory is concerned with the theoretical limitations and po what is the best error-correcting performance we tentials of such systems. could achieve? coding theory is concerned with the creation of practical encoding and decoding systems. error-correcting codes for the binary symmetric channel we now consider examples of encoding and decoding systems. what is the simplest way to add useful redundancy to a transmission? make the rules of the game clear we want to be able to detect and correct errors and retransmission is not an option. we get only one chance to encode transmit and decode. repetition codes a straightforward idea is to repeat every bit of the message a prearranged number of times for example three times as shown in table we call this repetition code imagine that we transmit the source message s over a binary symmetric channel with noise level f using this repetition code. we can describe the channel as adding a sparse noise vector n to the transmitted vector adding in modulo arithmetic i.e. the binary algebra in which a possible noise vector n and received vector r t n are shown in source sequence transmitted sequence s t table the repetition code s t n r figure an example transmission using how should we decode this received vector? the optimal algorithm looks at the received bits three at a time and takes a majority vote copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory received sequence r likelihood ratio p s p s decoded sequence algorithm majority-vote decoding algorithm for also shown are the likelihood ratios assuming the channel is a binary symmetric channel f at the risk of explaining the obvious let s prove this result. the optimal decoding decision in the sense of having the smallest probability of being wrong is to which value of s is most probable given r. consider the decoding of a single bit s which was encoded as ts and gave rise to three received bits r by bayes theorem the posterior probability of s is p p j sp p we can spell out the posterior probability of the two alternatives thus p p j s p p p j s p this posterior probability is determined by two factors the prior probability p and the data-dependent term p j s which is called the likelihood of s. the normalizing constant p needn t be computed when the optimal decoding decision which is to guess if p r p r and otherwise. to p r and p r we must make an assumption about the prior probabilities of the two hypotheses s and s and we must make an assumption about the probability of r given s. we assume that the prior probabilities are equal p p then maximizing the posterior probability p r is equivalent to maximizing the likelihood p s. and we assume that the channel is a binary symmetric channel with noise level f so that the likelihood is p s p ts n p j tns where n is the number of transmitted bits in the block we are considering and p j tn f if if rn tn rn tn thus the likelihood ratio for the two hypotheses is n p s p s p equals each factor p if rn the ratio is greater than since f so the winning hypothesis is the one with the most votes each vote counting for a factor of in the likelihood ratio. p j p j if rn and f f f copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes for the binary symmetric channel thus the majority-vote decoder shown in algorithm is the optimal decoder if we assume that the channel is a binary symmetric channel and that the two possible source messages and have equal prior probability. we now apply the majority vote decoder to the received vector of the three received bits are all so we decode this triplet as a in the second triplet of there are two and one so we decode this triplet as a which in this case corrects the error. not all errors are corrected however. if we are unlucky and two errors fall in a single block as in the triplet of then the decoding rule gets the wrong answer as shown in s figure decoding the received vector from t n r corrected errors undetected errors exercise show that the error probability is reduced by the use of by computing the error probability of this code for a binary symmetric channel with noise level f the error probability is dominated by the probability that two bits in a block of three are which scales as f in the case of the binary symmetric channel with f the code has a probability of error after decoding of pb per bit. figure shows the result of transmitting a binary image over a binary symmetric channel using the repetition code. the exercise s rating e.g. indicates its exercises are the easiest. exercises that are accompanied by a marginal rat are especially recommended. if a solution or partial solution is provided the page is indicated after the rating for example this exercise s solution is on page s encoder t channel f r decoder figure transmitting source bits over a binary symmetric channel with f using a repetition code and the majority vote decoding algorithm. the probability of decoded bit error has fallen to about the rate has fallen to copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory more useful codes pb figure error probability pb versus rate for repetition codes over a binary symmetric channel with f the right-hand shows pb on a logarithmic scale. we would like the rate to be large and pb to be small. more useful codes rate rate the repetition code has therefore reduced the probability of error as desired. yet we have lost something our rate of information transfer has fallen by a factor of three. so if we use a repetition code to communicate data over a telephone line it will reduce the error frequency but it will also reduce our communication rate. we will have to pay three times as much for each phone call. similarly we would need three of the original noisy gigabyte disk drives in order to create a one-gigabyte disk drive with pb can we push the error probability lower to the values required for a sellable disk drive we could achieve lower error probabilities by using repetition codes with more repetitions. exercise show that the probability of error of rn the repe tition code with n repetitions is pb for odd n n xnn f assuming f which of the terms in this sum is the biggest? how much bigger is it than the second-biggest term? use stirling s approximation to approximate the largest term and approximately the probability of error of the repetition code with n repetitions. in the assuming f how many repetitions are required to get the probability of error down to about so to build a single gigabyte disk drive with the required reliability from noisy gigabyte drives with f we would need sixty of the noisy disk drives. the between error probability and rate for repetition codes is shown in block codes the hamming code we would like to communicate with tiny probability of error and at a substantial rate. can we improve on repetition codes? what if we add redundancy to blocks of data instead of encoding one bit at a time? we now study a simple block code. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes for the binary symmetric channel a block code is a rule for converting a sequence of source bits s of length k say into a transmitted sequence t of length n bits. to add redundancy we make n greater than k. in a linear block code the extra n k bits are linear functions of the original k bits these extra bits are called parity-check bits. an example of a linear block code is the hamming code which transmits n bits for every k source bits. t s s t s t figure pictorial representation of encoding for the hamming code. the encoding operation for the code is shown pictorially in we arrange the seven transmitted bits in three intersecting circles. the four transmitted bits are set equal to the four source bits the parity-check bits are set so that the parity within each circle is even the parity-check bit is the parity of the three source bits is it is if the sum of those bits is even and if the sum is odd the second is the parity of the last three and the third parity bit is the parity of source bits one three and four. as an example shows the transmitted codeword for the case s table shows the codewords generated by each of the sixteen settings of the four source bits. these codewords have the special property that any pair from each other in at least three bits. s t s t s t s t table the sixteen codewords ftg of the hamming code. any pair of codewords from each other in at least three bits. because the hamming code is a linear code it can be written compactly in terms of matrices as follows. the transmitted codeword t is obtained from the source sequence s by a linear operation t gts where g is the generator matrix of the code gt and the encoding operation uses arithmetic etc.. in the encoding operation i have assumed that s and t are column vectors. if instead they are row vectors then this equation is replaced by t sg copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. where introduction to information theory g i it easier to relate to the right-multiplication than the left-multiplication many coding theory texts use the left-multiplying conventions however. the rows of the generator matrix can be viewed as four basis vectors lying in a seven-dimensional binary space. the sixteen codewords are obtained by making all possible linear combinations of these vectors. decoding the hamming code when we invent a more complex encoder s t the task of decoding the received vector r becomes less straightforward. remember that any of the bits may have been including the parity bits. if we assume that the channel is a binary symmetric channel and that all source vectors are equiprobable then the optimal decoder the source vector s whose encoding ts from the received vector r in the fewest bits. to the likelihood function to see why this is so. we could solve the decoding problem by measuring how far r is from each of the sixteen codewords in table then picking the closest. is there a more way of the most probable source vector? syndrome decoding for the hamming code for the hamming code there is a pictorial solution to the decoding problem based on the encoding picture as a example let s assume the transmission was t and the noise the second bit so the received vector is r we write the received vector into the three circles as shown in and look at each of the three circles to see whether its parity is even. the circles whose parity is not even are shown by dashed lines in the decoding task is to the smallest set of bits that can account for these violations of the parity rules. pattern of violations of the parity checks is called the syndrome and can be written as a binary vector for example in the syndrome is z because the two circles are unhappy and the third circle is happy to solve the decoding task we ask the question can we a unique bit that lies inside all the unhappy circles and outside all the happy circles? if so the of that bit would account for the observed syndrome. in the case shown in the bit lies inside the two unhappy circles and outside the happy circle no other single bit has this property so is the only single bit capable of explaining the syndrome. let s work through a couple more examples. figure shows what happens if one of the parity bits is by the noise. just one of the checks is violated. only lies inside this unhappy circle and outside the other two happy circles so is as the only single bit capable of explaining the syndrome. if the central bit is received shows that all three checks are violated only lies inside all three circles so is as the suspect bit. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes for the binary symmetric channel figure pictorial representation of decoding of the hamming code. the received vector is written into the diagram as shown in in the received vector is shown assuming that the transmitted vector was as in and the bits labelled by were the violated parity checks are highlighted by dashed circles. one of the seven bits is the most probable suspect to account for each syndrome i.e. each pattern of violated and parity checks. in examples and the most probable suspect is the one bit that was in example two bits have been and the most probable suspect is marked by a circle in which shows the output of the decoding algorithm. algorithm actions taken by the optimal decoder for the hamming code assuming a binary symmetric channel with small noise level f the syndrome vector z lists whether each parity check is violated or going through the checks in the order of the bits and r r r r r r syndrome z this bit none if you try any one of the seven bits you ll that a syndrome is obtained in each case seven non-zero syndromes one for each bit. there is only one other syndrome the all-zero syndrome. so if the channel is a binary symmetric channel with a small noise level f the optimal decoder at most one bit depending on the syndrome as shown in algorithm each syndrome could have been caused by other noise patterns too but any other noise pattern that has the same syndrome must be less probable because it involves a larger number of noise events. what happens if the noise actually more than one bit? figure shows the situation when two bits and are received the syndrome makes us suspect the single bit so our optimal decoding algorithm this bit giving a decoded pattern with three errors as shown in if we use the optimal decoding algorithm any two-bit error pattern will lead to a decoded seven-bit vector that contains three errors. general view of decoding for linear codes syndrome decoding we can also describe the decoding problem for a linear code in terms of matrices. the four received bits purport to be the four source bits and the received bits purport to be the parities of the source bits as by the generator matrix g. we evaluate the three parity-check bits for the received bits and see whether they match the three received bits the between these two triplets are called the syndrome of the received vector. if the syndrome is zero if all three parity checks are happy then the received vector is a codeword and the most probable decoding is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory s encoder t channel f r decoder figure transmitting source bits over a binary symmetric channel with f using a hamming code. the probability of decoded bit error is about parity bits given by reading out its four bits. if the syndrome is non-zero then the noise sequence for this block was non-zero and the syndrome is our pointer to the most probable error pattern. the computation of the syndrome vector is a linear operation. if we the matrix p such that the matrix of equation is p gt where is the identity matrix then the syndrome vector is z hr where the parity-check matrix h is given by h in modulo arithmetic so h p all the codewords t gts of the code satisfy ht exercise prove that this is so by evaluating the matrix hgt. since the received vector r is given by r gts n the syndrome-decoding problem is to the most probable noise vector n satisfying the equation hn z a decoding algorithm that solves this problem is called a maximum-likelihood decoder. we will discuss decoding problems like this in later chapters. summary of the hamming code s properties every possible received vector of length bits is either a codeword or it s one away from a codeword. since there are three parity constraints each of which might or might not be violated there are distinct syndromes. they can be divided into seven non-zero syndromes one for each of the one-bit error patterns and the all-zero syndrome corresponding to the zero-noise case. the optimal decoder takes no action if the syndrome is zero otherwise it uses this mapping of non-zero syndromes onto one-bit error patterns to the suspect bit. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes for the binary symmetric channel there is a decoding error if the four decoded bits do not all match the source bits the probability of block error pb is the probability that one or more of the decoded bits in one block fail to match the corresponding source bits the probability of bit error pb is the average probability that a decoded bit fails to match the corresponding source bit pb p s pb k k p sk in the case of the hamming code a decoding error will occur whenever the noise has more than one bit in a block of seven. the probability of block error is thus the probability that two or more bits are in a block. this probability scales as of as did the probability of error for the repetition code but notice that the hamming code communicates at a greater rate r figure shows a binary image transmitted over a binary symmetric channel using the hamming code. about of the decoded bits are in error. notice that the errors are correlated often two or three successive decoded bits are exercise this exercise and the next three refer to the hamming code. decode the received strings r r r r exercise calculate the probability of block error pb of the hamming code as a function of the noise level f and show that to leading order it goes as show that to leading order the probability of bit error pb goes as exercise find some noise vectors that give the all-zero syndrome is noise vectors that leave all the parity checks unviolated. how many such noise vectors are there? exercise i asserted above that a block decoding error will result whenever two or more bits are in a single block. show that this is indeed so. principle there might be error patterns that after decoding led only to the corruption of the parity bits with no source bits incorrectly decoded. summary of codes performances figure shows the performance of repetition codes and the hamming code. it also shows the performance of a family of linear block codes that are generalizations of hamming codes called bch codes. this shows that we can using linear block codes achieve better performance than repetition codes but the asymptotic situation still looks grim. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory pb more useful codes figure error probability pb versus rate r for repetition codes the hamming code and bch codes with blocklengths up to over a binary symmetric channel with f the righthand shows pb on a logarithmic scale. more useful codes rate rate exercise design an error-correcting code and a decoding algorithm for it estimate its probability of error and add it to t worry if you it to make a code better than the hamming code or if you it to a good decoder for your code that s the point of this exercise. exercise a hamming code can correct any one error might there be a code that can correct any two errors? optional extra does the answer to this question depend on whether the code is linear or nonlinear? exercise design an error-correcting code other than a repetition code that can correct any two errors in a block of size n what performance can the best codes achieve? there seems to be a between the decoded bit-error probability pb we would like to reduce and the rate r we would like to keep large. how can this be characterized? what points in the pb plane are achievable? this question was addressed by claude shannon in his pioneering paper of in which he both created the of information theory and solved most of its fundamental problems. at that time there was a widespread belief that the boundary between achievable and nonachievable points in the pb plane was a curve passing through the origin pb if this were so then in order to achieve a vanishingly small error probability pb one would have to reduce the rate correspondingly close to zero. no pain no gain. however shannon proved the remarkable result that the boundary be- tween achievable and nonachievable points meets the r axis at a non-zero value r c as shown in for any channel there exist codes that make it possible to communicate with arbitrarily small probability of error pb at non-zero rates. the half of this book iiii will be devoted to understanding this remarkable result which is called the noisy-channel coding theorem. example f the maximum rate at which communication is possible with arbitrarily small pb is called the capacity of the channel. the formula for the capacity of a copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. summary pb achievable not achievable c rate c rate achievable not achievable figure shannon s noisy-channel coding theorem. the solid curve shows the shannon limit on achievable values of pb for the binary symmetric channel with f rates up to r c are achievable with arbitrarily small pb. the points show the performance of some textbook codes as in the equation the shannon limit solid curve is r where c and are in equation binary symmetric channel with noise level f is cf f f the channel we were discussing earlier with noise level f has capacity c let us consider what this means in terms of noisy disk drives. the repetition code could communicate over this channel with pb at a rate r thus we know how to build a single gigabyte disk drive with pb from three noisy gigabyte disk drives. we also know how to make a single gigabyte disk drive with pb from sixty noisy one-gigabyte drives and now shannon passes by notices us juggling with disk drives and codes and says what performance are you trying to achieve? you don t need sixty disk drives you can get that performance with just two disk drives is less than and if you want pb or or anything you can get there with two disk drives too! the above statements might not be quite right since as we shall see shannon proved his noisy-channel coding theorem by studying sequences of block codes with ever-increasing blocklengths and the required blocklength might be bigger than a gigabyte size of our disk drive in which case shannon might say well you can t do it with those tiny disk drives but if you had two noisy terabyte drives you could make a single high-quality terabyte drive from them summary the hamming code by including three parity-check bits in a block of bits it is possible to detect and correct any single bit error in each block. shannon s noisy-channel coding theorem information can be communicated over a noisy channel at a non-zero rate with arbitrarily small error probability. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory information theory addresses both the limitations and the possibilities of communication. the noisy-channel coding theorem which we will prove in chapter asserts both that reliable communication at any rate beyond the capacity is impossible and that reliable communication at all rates up to capacity is possible. the next few chapters lay the foundations for this result by discussing how to measure information content and the intimately related topic of data compression. further exercises exercise consider the repetition code one way of viewing this code is as a concatenation of with we encode the source stream with then encode the resulting output with we could call this code this idea motivates an alternative decoding algorithm in which we decode the bits three at a time using the decoder for then decode the decoded bits from that decoder using the decoder for evaluate the probability of error for this decoder and compare it with the probability of error for the optimal decoder for do the concatenated encoder and decoder for those for have advantages over solutions solution to exercise an error is made by if two or more bits are in a block of three. so the error probability of is a sum of two terms the probability that all three bits are f and the probability that exactly two bits are f these expressions are not obvious see example the expressions are p f n and p f n pb pb f f this probability is dominated for small f by the term see exercise for further discussion of this problem. solution to exercise the probability of error for the repetition code rn is dominated by the probability that bits are which goes odd n as f n can be approximated using the binary entropy function the term where this approximation introduces an error of order pn as shown in equation so notation denotes the smallest integer greater than or equal to n pb pb f f log this answer is a little out because the approximation we used overestimated setting this equal to the required value of we n log and we did not distinguish between and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions a slightly more careful answer of explicit computation goes as follows. to the next order we taking the approximation for n this approximation can be proved from an accurate version of stirling s approximation or by considering the binomial distribution with p and noting n where from which equation follows. the distinction between and is not important in this term since has a maximum at n k then the probability of error odd n is to leading order pb n f f f the equation pb can be written f f log log f log f in equation the logarithms can be taken to any base as long as it s the same base throughout. in equation i use base which may be solved for n iteratively the iteration starting from this answer is found to be stable so n is the blocklength at which pb solution to exercise the probability of block error of the hamming code is a sum of six terms the probabilities that or errors occur in one block. pb f to leading order this goes as pb the probability of bit error of the hamming code is smaller than the probability of block error because a block error rarely corrupts all bits in the decoded block. the leading-order behaviour is found by considering the outcome in the most probable case where the noise vector has weight two. the decoder will erroneously a third bit so that the received vector length in three bits from the transmitted vector. that means if we average over all seven bits the probability that a randomly chosen bit is is times the block error probability to leading order. now what we really care about is the probability that copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory a source bit is are parity bits or source bits more likely to be among these three bits or are all seven bits equally likely to be corrupted when the noise vector has weight two? the hamming code is in fact completely symmetric in the protection it to the seven bits a binary symmetric channel. symmetry can be proved by showing that the role of a parity bit can be exchanged with a source bit and the resulting code is still a hamming code see below. the probability that any one bit ends up corrupted is the same for all seven bits. so the probability of bit error the source bits is simply three sevenths of the probability of block error. pb pb symmetry of the hamming code to prove that the code protects all bits equally we start from the paritycheck matrix h h the symmetry among the seven transmitted bits will be easiest to see if we reorder the seven bits using the permutation then we can rewrite h thus now if we take any two parity constraints that t and add them together we get another parity constraint. for example row asserts even and row asserts even and the sum of these two constraints is even we can drop the terms and since they are even whatever and are thus we have derived the parity constraint even which we can if we wish add into the parity-check matrix as a fourth row. set of vectors satisfying ht will not be changed. we thus the fourth row is the sum two of the top two rows. notice that the second third and fourth rows are all cyclic shifts of the top row. if having added the fourth redundant constraint we drop the constraint we obtain a new parity-check matrix which still for all codewords and which looks just like the starting h in except that all the columns have shifted along one copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions to the right and the rightmost column has reappeared at the left cyclic permutation of the columns. this establishes the symmetry among the seven bits. iterating the above procedure more times we can make a total of seven h matrices for the same original code each of which assigns each bit to a role. we may also construct the super-redundant seven-row parity-check matrix for the code this matrix is redundant in the sense that the space spanned by its rows is only three-dimensional not seven. this matrix is also a cyclic matrix. every row is a cyclic permutation of the top row. cyclic codes if there is an ordering of the bits tn such that a linear code has a cyclic parity-check matrix then the code is called a cyclic code. the codewords of such a code also have cyclic properties any cyclic permutation of a codeword is a codeword. for example the hamming code with its bits ordered as above consists of all seven cyclic shifts of the codewords and and the codewords and cyclic codes are a cornerstone of the algebraic approach to error-correcting codes. we won t use them again in this book however as they have been superceded by sparse-graph codes vi. solution to exercise there are non-zero noise vectors which give the all-zero syndrome these are precisely the non-zero codewords of the hamming code. notice that because the hamming code is linear the sum of any two codewords is a codeword. graphs corresponding to codes solution to exercise when answering this question you will probably that it is easier to invent new codes than to optimal decoders for them. there are many ways to design codes and what follows is just one possible train of thought. we make a linear block code that is similar to the hamming code but bigger. many codes can be conveniently expressed in terms of graphs. in we introduced a pictorial representation of the hamming code. if we replace that s big circles each of which shows that the parity of four particular bits is even by a parity-check node that is connected to the four bits then we obtain the representation of the hamming code by a bipartite graph as shown in the circles are the transmitted bits. the squares are the parity-check nodes to be confused with the parity-check bits which are the three most peripheral circles. the graph is a bipartite graph because its nodes fall into two classes bits and checks figure the graph of the hamming code. the circles are the bit nodes and the squares are the parity-check nodes. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to information theory and there are edges only between nodes in classes. the graph and the code s parity-check matrix are simply related to each other each parity-check node corresponds to a row of h and each bit node corresponds to a column of h for every in h there is an edge between the corresponding pair of nodes. having noticed this connection between linear codes and graphs one way to invent linear codes is simply to think of a bipartite graph. for example a pretty bipartite graph can be obtained from a dodecahedron by calling the vertices of the dodecahedron the parity-check nodes and putting a transmitted bit on each edge in the dodecahedron. this construction a paritycheck matrix in which every column has weight and every row has weight weight of a binary vector is the number of it contains. this code has n bits and it appears to have mapparent paritycheck constraints. actually there are only m independent constraints the constraint is redundant is if constraints are then the is automatically so the number of source bits is k n m the code is a code. it is hard to a decoding algorithm for this code but we can estimate its probability of error by its lowest-weight codewords. if we all the bits surrounding one face of the original dodecahedron then all the parity checks will be so the code has codewords of weight one for each face. since the lowest-weight codewords have weight we say that the code has distance d the hamming code had distance and could correct all single errors. a code with distance can correct all double errors but there are some triple errors that it cannot correct. so the error probability of this code assuming a binary symmetric channel will be dominated at least for low noise levels f by a term of order f perhaps something like f of course there is no obligation to make codes whose graphs can be represented on a plane as this one can the best linear codes which have simple graphical descriptions have graphs that are more tangled as illustrated by the tiny code of furthermore there is no reason for sticking to linear codes indeed some nonlinear codes codes whose codewords cannot be by a linear equation like ht have very good properties. but the encoding and decoding of a nonlinear code are even trickier tasks. solution to exercise code and decoding it with syndrome decoding. bits then the number of possible error patterns of weight up to two is first let s assume we are making a linear if there are n transmitted for n that s patterns. now every distinguishable error pattern must give rise to a distinct syndrome and the syndrome is a list of m bits so the maximum possible number of syndromes is for a code m so there are at most syndromes. the number of possible error patterns of weight up to two is bigger than the number of syndromes so we can immediately rule out the possibility that there is a code that is figure the graph the dodecahedron code. the circles are the transmitted bits and the triangles are the parity checks. one parity check is redundant. figure graph of a low-density parity-check code code with blocklength n and m parity-check constraints. each white circle represents a transmitted bit. each bit participates in j constraints represented by squares. the edges between nodes were placed at random. chapter for more. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions the same counting argument works for nonlinear codes too. when the decoder receives r t n his aim is to deduce both t and n from r. if it is the case that the sender can select any transmission t from a code of size st and the channel can select any noise vector from a set of size sn and those two selections can be recovered from the received bit string r which is one of at most possible strings then it must be the case that so for a k two-error-correcting code whether linear or nonlinear stsn solution to exercise there are various strategies for making codes that can correct multiple errors and i strongly recommend you think out one or two of them for yourself. if your approach uses a linear code e.g. one with a collection of m parity checks it is helpful to bear in mind the counting argument given in the previous exercise in order to anticipate how many parity checks m you might need. examples of codes that can correct any two errors are the dodecahedron code on page and the pentagonful code to be introduced on further simple ideas for making codes that can correct multiple errors from codes that can correct only one error are discussed in section solution to exercise the probability of error of order is to leading whereas the probability of error of is dominated by the probability of f the tors of weight four that cause it to make a decoding error. decoding procedure is therefore suboptimal since there are noise vec it has the advantage however of requiring smaller computational resources only memorization of three bits and counting up to three rather than counting up to nine. this simple code illustrates an important concept. concatenated codes are widely used in practice because concatenation allows large codes to be implemented using simple encoding and decoding hardware. some of the best known practical codes are concatenated codes. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference this chapter and its sibling chapter devote some time to notation. just as the white knight distinguished between the song the name of the song and what the name of the song was called we will sometimes need to be careful to distinguish between a random variable the value of the random variable and the proposition that asserts that the random variable has a particular value. in any particular chapter however i will use the most simple and friendly notation possible at the risk of upsetting pure-minded readers. for example if something is true with probability i will usually simply say that it is true probabilities and ensembles an ensemble x is a triple where the outcome x is the value of a random variable which takes on one of a set of possible values ax ai aig having probabilities px pig with p ai pi pi and the name a is mnemonic for alphabet one example of an ensemble is a letter that is randomly selected from an english document. this ensemble is shown in there are twenty-seven possible letters az and a space character p ai abbreviations. briefer notation will sometimes be used. for example p ai may be written as p or p probability of a subset. if t is a subset of ax then p p t p ai for example fa e i o ug then if we v to be vowels from v p a joint ensemble xy is an ensemble in which each outcome is an ordered pair x y with x ax aig and y ay bjg. we call p y the joint probability of x and y. commas are optional when writing ordered pairs so xy x y. n.b. in a joint ensemble xy the two variables are not necessarily independent. i ai pi a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z figure probability distribution over the outcomes for a randomly selected letter in an english language document from the frequently asked questions manual for linux the picture shows the probabilities by the areas of white squares. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probabilities and ensembles figure the probability distribution over the possible bigrams xy in an english language document the frequently asked questions manual for linux. x a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z y marginal probability. we can obtain the marginal probability p from the joint probability p y by summation p ai p ai y similarly using briefer notation the marginal probability of y is p p y conditional probability p ai j y bj p ai y bj p bj if p bj p bj then p ai j y bj is we pronounce p ai j y bj the probability that x equals ai given y equals bj example an example of a joint ensemble is the ordered pair xy consisting of two successive letters in an english document. the possible outcomes are ordered pairs such as aa ab ac and zz of these we might expect ab and ac to be more probable than aa and zz. an estimate of the joint probability distribution for two neighbouring characters is shown graphically in this joint ensemble has the special property that its two marginal distributions p and p are identical. they are both equal to the monogram distribution shown in from this joint ensemble p y we can obtain conditional distributions p j x and p y by normalizing the rows and columns respectively the probability p j x q is the probability distribution of the second letter given that the letter is a q. as you can see in the two most probable values for the second letter y given copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. x a b c d e f g h i j k l m n o p q r s t u v w x y z probability entropy and inference figure conditional probability distributions. p j x each row shows the conditional distribution of the second letter y given the letter x in a bigram xy. p y each column shows the conditional distribution of the letter x given the second letter y. x a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z y a b c d e f g h i j k l m n o p q r s t u v w x y z y p j x p y that the letter x is q are u and space is common after q because the source document makes heavy use of the word faq. the probability p y u is the probability distribution of the letter x given that the second letter y is a u. as you can see in the two most probable values for x given y u are n and o. rather than writing down the joint probability directly we often an ensemble in terms of a collection of conditional probabilities. the following rules of probability theory will be useful. denotes assumptions on which the probabilities are based. product rule obtained from the of conditional probability p y jh p yhp jh p j xhp this rule is also known as the chain rule. sum rule a rewriting of the marginal probability p yhp jh p p yhp jh p jh p xy xy p y jh p yhp jh bayes theorem obtained from the product rule p j xh independence. two random variables x and y are independent written x?y if and only if p y p exercise are the random variables x and y in the joint ensemble of independent? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the meaning of probability i said that we often an ensemble in terms of a collection of condi tional probabilities. the following example illustrates this idea. example jo has a test for a nasty disease. we denote jo s state of health by the variable a and the test result by b. a a jo has the disease jo does not have the disease. the result of the test is either positive or negative the test is reliable in of cases of people who really have the disease a positive result is returned and in of cases of people who do not have the disease a negative result is obtained. the piece of background information is that of people of jo s age and background have the disease. ok jo has the test and the result is positive. what is the probability that jo has the disease? solution. we write down all the provided probabilities. the test reliability the conditional probability of b given a p a p a p a p a and the disease prevalence tells us about the marginal probability of a p p from the marginal p and the conditional probability p a we can deduce the joint probability p b p a and any other probabilities we are interested in. for example by the sum rule the marginal probability of b the probability of getting a positive result is p p a p a jo has received a positive result b and is interested in how plausible it is that she has the disease that a the man in the street might be duped by the statement the test is reliable so jo s positive result implies that there is a chance that jo has the disease but this is incorrect. the correct solution to an inference problem is found using bayes theorem. p b p a p a p a so in spite of the positive result the probability that jo has the disease is only the meaning of probability probabilities can be used in two ways. probabilities can describe frequencies of outcomes in random experiments but giving noncircular of the terms frequency and random is a challenge what does it mean to say that the frequency of a tossed coin s copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference box the cox axioms. if a set of beliefs satisfy these axioms then they can be mapped onto probabilities satisfying p p p and the rules of probability and p p p y p yp notation. let the degree of belief in proposition x be denoted by bx. the negation of x is written x. the degree of belief in a conditional proposition x assuming proposition y to be true is represented by bxj y. axiom degrees of belief can be ordered if bx is greater than by and by is greater than bz then bx is greater than bz. beliefs can be mapped onto real numbers. axiom the degree of belief in a proposition x and its negation x are related. there is a function f such that bx f axiom the degree of belief in a conjunction of propositions x y and y is related to the degree of belief in the conditional proposition xj y and the degree of belief in the proposition y. there is a function g such that bx y g y by coming up heads is if we say that this frequency is the average fraction of heads in long sequences we have to average and it is hard to average without using a word synonymous to probability! i will not attempt to cut this philosophical knot. probabilities can also be used more generally to describe degrees of belief in propositions that do not involve random variables for example the probability that mr. s. was the murderer of mrs. s. given the evidence either was or wasn t and it s the jury s job to assess how probable it is that he was the probability that thomas had a child by one of his slaves the probability that shakespeare s plays were written by francis bacon or to pick a modern-day example the probability that a particular signature on a particular cheque is genuine the man in the street is happy to use probabilities in both these ways but some books on probability restrict probabilities to refer only to frequencies of outcomes in repeatable random experiments. nevertheless degrees of belief can be mapped onto probabilities if they satisfy simple consistency rules known as the cox axioms thus probabilities can be used to describe assumptions and to describe inferences given those assumptions. the rules of probability ensure that if two people make the same assumptions and receive the same data then they will draw identical conclusions. this more general use of probability to quantify beliefs is known as the bayesian viewpoint. it is also known as the subjective interpretation of probability since the probabilities depend on assumptions. advocates of a bayesian approach to data modelling and pattern recognition do not view this subjectivity as a defect since in their view you cannot do inference without making assumptions. in this book it will from time to time be taken for granted that a bayesian approach makes sense but the reader is warned that this is not yet a globally held view the of statistics was dominated for most of the century by non-bayesian methods in which probabilities are allowed to describe only random variables. the big between the two approaches is that copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. forward probabilities and inverse probabilities bayesians also use probabilities to describe inferences. forward probabilities and inverse probabilities probability calculations often fall into one of two categories forward probability and inverse probability. here is an example of a forward probability problem exercise an urn contains k balls of which b are black and w k b are white. fred draws a ball at random from the urn and replaces it n times. what is the probability distribution of the number of times a black ball is drawn nb? what is the expectation of nb? what is the variance of nb? what is the standard deviation of nb? give numerical answers for the cases n and n when b and k forward probability problems involve a generative model that describes a process that is assumed to give rise to some data the task is to compute the probability distribution or expectation of some quantity that depends on the data. here is another example of a forward probability problem exercise an urn contains k balls of which b are black and w k b are white. we the fraction fb bk. fred draws n times from the urn exactly as in exercise obtaining nb blacks and computes the quantity z fbn n fb what is the expectation of z? in the case n and fb what is the probability distribution of z? what is the probability that z compare z with the quantities computed in the previous exercise. like forward probability problems inverse probability problems involve a generative model of a process but instead of computing the probability distribution of some quantity produced by the process we compute the conditional probability of one or more of the unobserved variables in the process given the observed variables. this invariably requires the use of bayes theorem. example there are eleven urns labelled by u each containing ten balls. urn u contains u black balls and u white balls. fred selects an urn u at random and draws n times with replacement from that urn obtaining nb blacks and n nb whites. fred s friend bill looks on. if after n draws nb blacks have been drawn what is the probability that the urn fred is using is urn u from bill s point of view? doesn t know the value of u. solution. the joint probability distribution of the random variables u and nb can be written p nb j n p j u n from the joint probability of u and nb we can obtain the conditional distribution of u given nb p nb n p nb j n p j n p j u n p j n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference figure joint probability of u and nb for bill and fred s urn problem after n draws. u u p nb n figure conditional probability of u given nb and n u nb the marginal probability of u is p for all u. you wrote down the probability of nb given u and n p j u n when you solved exercise are doing the highly recommended exercises aren t you? if we fu then p j u n n nb u what about the denominator p j n this is the marginal probability of nb which we can obtain using the sum rule p j n p nb j n p j u n so the conditional probability of u given nb is p nb n p j u n p j n n p j n nb u this conditional distribution can be found by normalizing column of and is shown in the normalizing constant the marginal probability of nb is p n the posterior probability is correct for all u including the end-points u and u where fu and fu respectively. the posterior probability that u given nb is equal to zero because if fred were drawing from urn it would be impossible for any black balls to be drawn. the posterior probability that u is also zero because there are no white balls in that urn. the other hypotheses u u u all have non-zero posterior probability. terminology of inverse probability in inverse probability problems it is convenient to give names to the probabilities appearing in bayes theorem. in equation we call the marginal probability p the prior probability of u and p j u n is called the likelihood of u. it is important to note that the terms likelihood and probability are not synonyms. the quantity p j u n is a function of both nb and u. for u p j u n a probability over nb. for nb p j u n the likelihood of u. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. forward probabilities and inverse probabilities never say the likelihood of the data always say the likelihood of the parameters the likelihood function is not a probability distribution. you want to mention the data that a likelihood function is associated with you may say the likelihood of the parameters given the data the conditional probability p nb n is called the posterior probability of u given nb. the normalizing constant p j n has no u-dependence so its value is not important if we simply wish to evaluate the relative probabilities of the alternative hypotheses u. however in most data-modelling problems of any complexity this quantity becomes important and it is given various names p j n is known as the evidence or the marginal likelihood. if denotes the unknown parameters d denotes the data and h denotes the overall hypothesis space the general equation is written p j dh p j jh p jh posterior likelihood prior evidence inverse probability and prediction example assuming again that bill has observed nb blacks in n draws let fred draw another ball from the same urn. what is the probability that the next drawn ball is a black? should make use of the posterior probabilities in solution. by the sum rule p is black j nb n p is black j u nb n nb n since the balls are drawn with replacement from the chosen urn the probability p is black j u nb n is just fu whatever nb and n are. so p is black j nb n fup nb n using the values of p nb n given in we obtain p is black j nb n comment. notice the between this prediction obtained using probability theory and the widespread practice in statistics of making predictions by selecting the most plausible hypothesis here would be that the urn is urn u and then making the predictions assuming that hypothesis to be true would give a probability of that the next ball is black. the correct prediction is the one that takes into account the uncertainty by marginalizing over the possible values of the hypothesis u. marginalization here leads to slightly more moderate less extreme predictions. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference inference as inverse probability now consider the following exercise which has the character of a simple investigation. example bill tosses a bent coin n times obtaining a sequence of heads and tails. we assume that the coin has a probability fh of coming up heads we do not know fh. if nh heads have occurred in n tosses what is the probability distribution of fh? example n might be and nh might be or after a lot more tossing we might have n and nh what is the probability that the n outcome will be a head given nh heads in n tosses? unlike example this problem has a subjective element. given a restricted of probability that says probabilities are the frequencies of random variables this example is from the eleven-urns example. whereas the urn u was a random variable the bias fh of the coin would not normally be called a random variable. it is just a but unknown parameter that we are interested in. yet don t the two examples and seem to have an essential similarity? when n and nh to solve example we have to make an assumption about what the bias of the coin fh might be. this prior probability distribution over fh p corresponds to the prior over u in the eleven-urns problem. in that example the helpful problem p in real life we have to make assumptions in order to assign priors these assumptions will be subjective and our answers will depend on them. exactly the same can be said for the other probabilities in our generative model too. we are assuming for example that the balls are drawn from an urn independently but could there not be correlations in the sequence because fred s ball-drawing action is not perfectly random? indeed there could be so the likelihood function that we use depends on assumptions too. in real data modelling problems priors are subjective and so are likelihoods. here p denotes a probability density rather than a probability distribution. we are now using p to denote probability densities over continuous variables as well as probabilities over discrete variables and probabilities of logical propositions. the probability that a continuous variable v lies between values a dv p p is dimensionless. the density p is a dimensional quantity having dimensions inverse to the dimensions of v in contrast to discrete probabilities which are dimensionless. don t be surprised to see probability densities greater than this is normal a and b b a is to be r b and nothing is wrong as long as r b conditional and joint probability densities are in just the same way as conditional and joint probabilities. a dv p for any interval b. exercise assuming a uniform prior on fh p solve the problem posed in example sketch the posterior distribution of fh and compute the probability that the n outcome will be a head for n and nh n and nh n and nh n and nh you will the beta integral useful z dpa pfa a pafb fb fa!fb! fb copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. forward probabilities and inverse probabilities you may also it instructive to look back at example and equation people sometimes confuse assigning a prior distribution to an unknown parameter such as fh with making an initial guess of the value of the parameter. but the prior over fh p is not a simple statement like initially i would guess fh the prior is a probability density over fh which the prior degree of belief that fh lies in any interval f it may well be the case that our prior for fh is symmetric about so that the mean of fh under the prior is in this case the predictive distribution for the toss would indeed be p head dfh p headj fh dfh p but the prediction for subsequent tosses will depend on the whole prior distribution not just its mean. data compression and inverse probability consider the following task. example write a computer program capable of compressing binary like this one the string shown contains and intuitively compression works by taking advantage of the predictability of a in this case the source of the appears more likely to emit than a data compression program that compresses this must implicitly or explicitly be addressing the question what is the probability that the next character in this is a do you think this problem is similar in character to example i do. one of the themes of this book is that data compression and data modelling are one and the same and that they should both be addressed like the urn of example using inverse probability. example is solved in chapter the likelihood principle please solve the following two exercises. a b example urn a contains three balls one black and two white urn b contains three balls two black and one white. one of the urns is selected at random and one ball is drawn. the ball is black. what is the probability that the selected urn is urn a? figure urns for example example urn a contains balls one black two white one green and one pink urn b contains hundred balls two hundred black one hundred white yellow cyan sienna green silver gold and purple. of a s balls are black of b s are black. one of the urns is selected at random and one ball is drawn. the ball is black. what is the probability that the urn is urn a? p g s c g p y figure urns for example copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference ai pi hpi i a b c d e f g h i j k l m n o p q r s t u v w x y z what do you notice about your solutions? does each answer depend on the detailed contents of each urn? the details of the other possible outcomes and their probabilities are irrelevant. all that matters is the probability of the outcome that actually happened that the ball drawn was black given the hypotheses. we need only to know the likelihood i.e. how the probability of the data that happened varies with the hypothesis. this simple rule about inference is known as the likelihood principle. the likelihood principle given a generative model for data d given parameters p and having observed a particular outcome all inferences and predictions should depend only on the function p j in spite of the simplicity of this principle many classical statistical methods violate it. of entropy and related functions the shannon information content of an outcome x is to be hx p it is measured in bits. word bit is also used to denote a variable whose value is or i hope context will always make clear which of the two meanings is intended. in the next few chapters we will establish that the shannon information content hai is indeed a natural measure of the information content of the event x ai. at that point we will shorten the name of this quantity to the information content the fourth column in table shows the shannon information content of the possible outcomes when a random character is picked from an english document. the outcome x z has a shannon information content of bits and x e has an information content of bits. the entropy of an ensemble x is to be the average shannon in formation content of an outcome hx p log p convention for p with the log like the information content entropy is measured in bits. that log since when it is convenient we may also write hx as hp where p is the vector pi. another name for the entropy of x is the uncertainty of x. example the entropy of a randomly selected letter in an english document is about bits assuming its probability is as given in table we obtain this number by averaging log in the fourth column under the probability distribution pi in the third column. pi pi xi table shannon information contents of the outcomes az. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decomposability of the entropy we now note some properties of the entropy function. hx with equality pi for one i. means if and only if entropy is maximized if p is uniform hx logjaxj with equality pi for all i. notation the vertical bars j j have two meanings. if ax is a set jaxj denotes the number of elements in ax if x is a number then jxj is the absolute value of x. the redundancy measures the fractional between hx and its maximum possible value logjaxj. the redundancy of x is hx log jaxj we won t make use of redundancy in this book so i have not assigned a symbol to it. the joint entropy of x y is hx y p y log p y entropy is additive for independent random variables hx y hx hy p y p our for information content so far apply only to discrete probability distributions over sets ax. the can be extended to sets though the entropy may then be the case of a probability density over a continuous set is addressed in section further important and exercises to do with entropy will come along in section decomposability of the entropy the entropy function a recursive property that can be very useful when computing entropies. for convenience we ll stretch our notation so that we can write hx as hp where p is the probability vector associated with the ensemble x. let s illustrate the property by an example imagine that a random variable x is created by a fair coin to determine whether x then if x is not a fair coin a second time to determine whether x is or the probability distribution of x is p p p what is the entropy of x? we can either compute it by brute force hx log log log or we can use the following decomposition in which the value of x is revealed gradually. imagine learning whether x and then if x is not learning which non-zero value is the case. the revelation of whether x or not entails copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference revealing a binary variable whose probability distribution is this revelation has an entropy log bit. if x is not we learn the value of the second coin this too is a binary variable whose probability distribution is and whose entropy is bit. we only get to experience the second revelation half the time however so the entropy can be written log hx generalizing the observation we are making about the entropy of any probability distribution p pig is that hp pi when it s written as a formula this property looks regrettably ugly nev ertheless it is a simple property and one that you should make use of. generalizing further the entropy has the property for any m that hp h pm pi pm pi pm pi example a source produces a character x from the alphabet a a b zg with probability x is a numeral with probability x is a vowel e i o u and with probability it s one of the consonants. all numerals are equiprobable and the same goes for vowels and consonants. estimate the entropy of x. solution. log log log log log log bits. the ei in leibler is pronounced the same as in heist. gibbs inequality the relative entropy or kullbackleibler divergence between two probability distributions p and qx that are over the same alphabet ax is dklpjjq p log p qx the relative entropy gibbs inequality dklpjjq with equality only if p q. note that in general the relative entropy is not symmetric under interchange of the distributions p and q in general dklpjjq dklqjjp so dkl although it is sometimes called the kl distance is not strictly a distance. the relative entropy is important in pattern recognition and neural networks as well as in information theory. gibbs inequality is probably the most important inequality in this book. it and many other inequalities can be proved using the concept of convexity. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. jensen s inequality for convex functions jensen s inequality for convex functions the words convex and concave may be pronounced convex-smile and concave-frown this terminology has useful redundancy while one may forget which way up convex and concave are it is harder to confuse a smile with a frown. convex functions. a function f is convex over b if every chord of the function lies above the function as shown in that is for all b and f a function f is strictly convex if for all b the equality holds only for and similar apply to concave and strictly concave functions. f figure of convexity. some strictly convex functions are ex and for all x and x log x for x log x x log x figure convex functions. centre of gravity jensen s inequality. if f is a convex function and x is a random variable then e f where e denotes expectation. if f is strictly convex and e f then the random variable x is a constant. jensen s inequality can also be rewritten for a concave function with the direction of the inequality reversed. a physical version of jensen s inequality runs as follows. if a collection of masses pi are placed on a convex curve f at locations f then the centre of gravity of those masses which is at lies above the curve. if this fails to convince you then feel free to do the following exercise. exercise prove jensen s inequality. example three squares have average area the average of the lengths of their sides is m. what can be said about the size of the largest of the three squares? jensen s inequality. solution. let x be the length of the side of a square and let the probability of x be over the three lengths then the information that we have is that e and e where f is the function mapping lengths to areas. this is a strictly convex function. we notice that the equality e f holds therefore x is a constant and the three lengths must all be equal. the area of the largest square is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference convexity and concavity also relate to maximization if f is concave and there exists a point at which for all k then f has its maximum value at that point. the converse does not hold if a concave f is maximized at some x it is not necessarily true that the gradient rf is equal to zero there. for example f is maximized at x where its derivative is and f logp for a probability p is maximized on the boundary of the range at p where the gradient df exercises sums of random variables exercise two ordinary dice with faces labelled are thrown. what is the probability distribution of the sum of the values? what is the probability distribution of the absolute between the values? one hundred ordinary dice are thrown. what roughly is the probability distribution of the sum of the values? sketch the probability distribution and estimate its mean and standard deviation. how can two cubical dice be labelled using the numbers so that when the two dice are thrown the sum has a uniform probability distribution over the integers is there any way that one hundred dice could be labelled with integers such that the probability distribution of the sum is uniform? inference problems exercise if q p and a ln pq show that p sketch this function and its relationship to the hyperbolic tangent function tanhu it will be useful to be in logarithms also. if b log pq what is p as a function of b? exercise let x and y be dependent random variables with x a binary variable taking values in ax use bayes theorem to show that the log posterior probability ratio for x given y is log p y p y log p j x p j x log p p exercise let x and be random variables such that and are conditionally independent given a binary variable x. use bayes theorem to show that the posterior probability ratio for x given fdig is p p p j x p j x p j x p j x p p this exercise is intended to help you think about the central-limit theorem which says that if independent random variables xn have means and variances n then in the has a distribution that tends to a normal distribution limit of large n the sum pn xn with meanpn and variance pn n. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises life in high-dimensional spaces probability distributions and volumes have some unexpected properties in high-dimensional spaces. exercise consider a sphere of radius r in an n real space. show that the fraction of the volume of the sphere that is in the surface shell lying at values of the radius between r and r where r is f evaluate f for the cases n n and n with implication points that are uniformly distributed in a sphere in n dimensions where n is large are very likely to be in a thin shell near the surface. expectations and entropies you are probably familiar with the idea of computing the expectation of a function of x e hf p maybe you are not so comfortable with computing this expectation in cases where the function f depends on the probability p the next few examples address this concern. exercise let pa pb and pc let f f and f what is e what is e exercise for an arbitrary ensemble what is e exercise let pa pb and pc let ga gb and gc what is e exercise let pa pb and pc what is the proba bility that p what is p log exercise prove the assertion that hx logjaxj with equality pi for all i. denotes the number of elements in the set ax. use jensen s inequality if your attempt to use jensen does not succeed remember that jensen involves both a random variable and a function and you have quite a lot of freedom in choosing these think about whether your chosen function f should be convex or concave. exercise prove that the relative entropy dklpjjq inequality with equality only if p q. exercise prove that the entropy is indeed decomposable as described in equations copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference g h f exercise a random variable x is selected by a bent coin with bias f to determine whether the outcome is in or then either a second bent coin with bias g or a third bent coin with bias h respectively. write down the probability distribution of x. use the decomposability of the entropy to the entropy of x. how compact an expression is obtained if you make use of the binary entropy function compared with writing out the four-term entropy explicitly. find the derivative of hx with respect to f xx. exercise an unbiased coin is until one head is thrown. what is the entropy of the random variable x the number of repeat the calculation for the case of a biased coin with probability f of coming up heads. solve the problem both directly and by using the decomposability of the entropy further exercises forward probability exercise an urn contains w white balls and b black balls. two balls are drawn one after the other without replacement. prove that the probability that the ball is white is equal to the probability that the second is white. exercise a circular coin of diameter a is thrown onto a square grid whose squares are b b. b what is the probability that the coin will lie entirely within one square? exercise s needle. a needle of length a is thrown onto a plane covered with equally spaced parallel lines with separation b. what is the probability that the needle will cross a line? if a b s noodle on average a random curve of length a is expected to intersect the lines times. exercise two points are selected at random on a straight line segment of length what is the probability that a triangle can be constructed out of the three resulting segments? exercise an unbiased coin is until one head is thrown. what is the expected number of tails and the expected number of heads? fred who doesn t know that the coin is unbiased estimates the bias using hh t where h and t are the numbers of heads and tails tossed. compute and sketch the probability distribution of n.b. this is a forward probability problem a sampling theory problem not an inference problem. don t use bayes theorem. exercise fred rolls an unbiased six-sided die once per second not ing the occasions when the outcome is a six. what is the mean number of rolls from one six to the next six? between two rolls the clock strikes one. what is the mean number of rolls until the next six? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises now think back before the clock struck. what is the mean number of rolls going back in time until the most recent six? what is the mean number of rolls from the six before the clock struck to the next six? is your answer to from your answer to explain. another version of this exercise refers to fred waiting for a bus at a bus-stop in poissonville where buses arrive independently at random poisson process with on average one bus every six minutes. what is the average wait for a bus after fred arrives at the stop? minutes. so what is the time between the two buses the one that fred just missed and the one that he catches? minutes. explain the apparent paradox. note the contrast with the situation in clockville where the buses are spaced exactly minutes apart. there as you can the mean wait at a bus-stop is minutes and the time between the missed bus and the next one is minutes. conditional probability exercise you meet fred. fred tells you he has two brothers alf and bob. what is the probability that fred is older than bob? fred tells you that he is older than alf. now what is the probability that fred is older than bob? is what is the conditional probability that f b given that f a? exercise the inhabitants of an island tell the truth one third of the time. they lie with probability on an occasion after one of them made a statement you ask another was that statement true? and he says yes what is the probability that the statement was indeed true? exercise compare two ways of computing the probability of error of the repetition code assuming a binary symmetric channel did this once for exercise and that they give the same answer. binomial distribution method. add the probability that all three bits are to the probability that exactly two bits are sum rule method. using the sum rule compute the marginal probability that r takes on each of the eight possible values p ps p s. then compute the posterior probabil ity of s for each of the eight values of r. fact by symmetry only two example cases r and r need be considered. notice that some of the inferred bits are better determined than others. from the posterior probability p r you can read out the case-by-case error probability the probability that the more probable hypothesis is not correct p r. find the average error probability using the sum rule p p j r equation gives the posterior probability of the input s given the received vector r. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference exercise the frequency pn of the nth most frequent word in english is roughly approximated by pn n for n n remarkable law is known as zipf s law and applies to the word frequencies of many languages if we assume that english is generated by picking words at random according to this distribution what is the entropy of english word? calculation can be found in prediction and entropy of printed english c.e. shannon bell syst. tech. j. but inexplicably the great man made numerical errors in it. solutions solution to exercise no they are not independent. if they were then all the conditional distributions p j x would be identical functions of y regardless of x solution to exercise we the fraction fb bk. the number of black balls has a binomial distribution. p j fb n n nb b the mean and variance of this distribution are enb n fb varnb n fb these results were derived in example the standard deviation of nb is pvarnb fb. when bk and n the expectation and variance of nb are and the standard deviation is when bk and n the expectation and variance of nb are and the standard deviation is solution to exercise the numerator of the quantity z fbn n fb can be recognized as the denominator is equal to the variance of nb which is by the expectation of the numerator. so the expectation of z is random variable like z which measures the deviation of data from the expected value is sometimes called in the case n and fb n fb is and varnb is the numerator has possible values only one of which is smaller than fbn has probability p so the probability that z is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solution to exercise we wish to prove given the property f that if p pi and pi i pif f i pixi! we proceed by recursion working from the right-hand side. proof does not handle cases where some pi such details are left to the pedantic reader. at the line we use the of convexity with i pi at the second line i pi i pixi! f f i i i pixi! pi! pixi i pif i pi f pi pi pi pi pi and so forth. solution to exercise f i pixi i pi! for the outcomes the probabilities are p f the value of one die has mean and variance so the sum of one hundred has mean and variance and by the central-limit theorem the probability distribution is roughly gaussian to the integers with this mean and variance. in order to obtain a sum that has a uniform distribution we have to start from random variables some of which have a spiky distribution with the probability mass concentrated at the extremes. the unique solution is to have one ordinary die and one with faces yes a uniform distribution can be created in several ways for example by labelling the rth die with the numbers to think about does this uniform distribution contradict the central-limit theorem? solution to exercise a ln p q p q ea and q p gives p p p the hyperbolic tangent is ea ea ea tanha ea ea copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. so f probability entropy and inference in the case b pq we can repeat steps replacing e by to obtain p solution to exercise p y p j xp p log p y p y p y p y p j x p j x p p log p j x p j x log p p solution to exercise the conditional independence of and given x means p p j xp j x this gives a separation of the posterior probability ratio into a series of factors one for each data point times the prior probability ratio. p p p x p x p j x p j x p p p j x p j x p p life in high-dimensional spaces solution to exercise the volume of a hypersphere of radius r in n dimensions is in fact v n rn but you don t need to know this. for this question all that we need is the r-dependence v n rn so the fractional volume in r is rn rn the fractional volumes in the shells for the required cases are n notice that no matter how small is for large enough n essentially all the probability mass is in the surface shell of thickness copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solution to exercise f and f pa pb pc f e for each x f so solution to exercise for general x e e p e jaxj solution to exercise pa pb pc ga gb and gc solution to exercise e pb p pb log pa pc p solution to exercise this type of question can be approached in two ways either by the function to be maximized the maximum and proving it is a global maximum this strategy is somewhat risky since it is possible for the maximum of a function to be at the boundary of the space at a place where the derivative is not zero. alternatively a carefully chosen inequality can establish the answer. the second method is much neater. proof by the recommended method. since it is slightly easier to ln than we temporarily hx to be measured using natural logarithms thus scaling it down by a factor of log e. hx xi pi ln pi ln pi we maximize subject to the constraint pi pi which can be enforced with a lagrange multiplier at a maximum gp hx xi pi ln pi ln pi ln pi so all the pi are equal. that this extremum is indeed a maximum is established by the curvature which is negative pi copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference proof using jensen s inequality method. the inequality. first a reminder of if f is a convex function and x is a random variable then e f if f is strictly convex and e f then the random variable x is a constant probability the secret of a proof using jensen s inequality is to choose the right func tion and the right random variable. we could f log u log u is a convex function and think of hx pi log as the mean of f where u p but this would not get us there it would give us an inequality in the wrong direction. if instead we pi then we u hx now we know from exercise that jaxj so hx log jaxj equality holds only if the random variable u is a constant which means p is a constant for all x. solution to exercise dklpjjq p log p qx we prove gibbs inequality using jensen s inequality. let f log and u qx p then dklpjjq ef qx f xx p p with equality only if u qx p is a constant that is if qx p px in the above proof the expectations were with respect to second solution. the probability distribution p a second solution method uses jensen s inequality with qx instead. we f u log u and let u p qx then qx dklpjjq xx f xx p qx log p qx qx! f p qx p with equality only if u p qx is a constant that is if qx p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solution to exercise hx f f solution to exercise the probability that there are tails and then one head we get the head on the xth toss is p f if the toss is a tail the probability distribution for the future looks just like it did before we made the toss. thus we have a recursive expression for the entropy rearranging hx f hx solution to exercise the probability of the number of tails t is p for t the expected number of heads is by of the problem. the expected number of tails is which may be shown to be in a variety of ways. for example since the situation after one tail is thrown is equivalent to the opening situation we can write down the recurrence relation et et et the probability distribution of the estimator t given that f is plotted in the probability of is simply the probability of the corresponding value of t. figure the probability distribution of the estimator t given that f solution to exercise the mean number of rolls from one six to the next six is six we start counting rolls after the of the two sixes. the probability that the next six occurs on the rth roll is the probability of not getting a six for r rolls multiplied by the probability of then getting a six p r for r this probability distribution of the number of rolls r may be called an exponential distribution since p r where and z is a normalizing constant. the mean number of rolls from the clock until the next six is six. the mean number of rolls going back in time until the most recent six is six. et p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. probability entropy and inference the mean number of rolls from the six before the clock struck to the six after the clock struck is the sum of the answers to and less one that is eleven. rather than explaining the between and let me give another hint. imagine that the buses in poissonville arrive independently at random poisson process with on average one bus every six minutes. imagine that passengers turn up at bus-stops at a uniform rate and are scooped up by the bus without delay so the interval between two buses remains constant. buses that follow gaps bigger than six minutes become overcrowded. the passengers representative complains that two-thirds of all passengers found themselves on overcrowded buses. the bus operator claims no no only one third of our buses are overcrowded can both these claims be true? solution to exercise binomial distribution method. from the solution to exercise pb f f sum rule method. the marginal probabilities of the eight values of r are illustrated by p f p f f f the posterior probabilities are represented by p r f f f figure the probability distribution of the number of rolls from one to the next solid line p r and the probability distribution line of the number of rolls from the before to the next rtot p r r the probability p is about the probability p is about the mean of is and the mean of rtot is and p r f the probabilities of error in these representative cases are thus f f f f f p r f f f and notice that while the average probability of error of is about the probability r that any particular bit is wrong is either about f or f p r f the average error probability using the sum rule is p xr p j r f so f f f f p f f solution to exercise the entropy is bits per word. the two terms are for the cases r and the remaining are for the other outcomes which share the same probability of occurring and identical error probability f copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter if you are eager to get on to information theory data compression and noisy channels you can skip to chapter data compression and data modelling are intimately connected however so you ll probably want to come back to this chapter by the time you get to chapter before reading chapter it might be good to look at the following exercises. exercise a die is selected at random from two twenty-faced dice on which the symbols are written with nonuniform frequency as follows. symbol number of faces of die a number of faces of die b the randomly chosen die is rolled times with the following outcomes what is the probability that the die is die a? exercise assume that there is a third twenty-faced die die c on which the symbols are written once each. as above one of the three dice is selected at random and rolled times giving the outcomes what is the probability that the die is die a die b die c? exercise inferring a decay constant unstable particles are emitted from a source and decay at a distance x a real number that has an exponential probability distribution with characteristic length decay events can be observed only if they occur in a window extending from x cm to x cm. n decays are observed at locations xng. what is x exercise forensic evidence two people have left traces of their own blood at the scene of a crime. a suspect oliver is tested and found to have type o blood. the blood groups of the two traces are found to be of type o common type in the local population having frequency and of type ab rare type with frequency do these data o and ab blood were found at scene give evidence in favour of the proposition that oliver was one of the two people present at the crime? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference it is not a controversial statement that bayes theorem provides the correct language for describing the inference of a message communicated over a noisy channel as we used it in chapter but strangely when it comes to other inference problems the use of bayes theorem is not so widespread. a inference problem when i was an undergraduate in cambridge i was privileged to receive supervisions from steve gull. sitting at his desk in a dishevelled in st. john s college i asked him how one ought to answer an old tripos question unstable particles are emitted from a source and decay at a distance x a real number that has an exponential probability distribution with characteristic length decay events can be observed only if they occur in a window extending from x cm to x cm. n decays are observed at locations xng. what is x i had scratched my head over this for some time. my education had provided me with a couple of approaches to solving such inference problems constructing estimators of the unknown parameters or the model to the data or to a processed version of the data. since the mean of an unconstrained exponential distribution is it seemed reasonable to examine the sample mean xnn and see if an estimator could be obtained from it. it was evident that the estimator would be appropriate for cm but not for cases where the truncation of the distribution at the right-hand side is with a little ingenuity and the introduction of ad hoc bins promising estimators for cm could be constructed. but there was no obvious estimator that would work under all conditions. to a histogram derived from the data. i was stuck. nor could i a satisfactory approach based on the density p what is the general solution to this problem and others like it? is it always necessary when confronted by a new inference problem to grope in the dark for appropriate estimators and worry about the best estimator that means? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a inference problem figure the probability density p as a function of x. x figure the probability density p as a function of for three values of x. when plotted this way round the function is known as the likelihood of the marks indicate the three values of that were used in the preceding steve wrote down the probability of one data point given where p dx x otherwise this seemed obvious enough. then he wrote bayes theorem p xng p p p suddenly the straightforward distribution p xngj the probability of the data given the hypothesis was being turned on its head so as to the probability of a hypothesis given the data. a simple showed the probability of a single data point p as a familiar function of x for values of each curve was an innocent exponential normalized to have area plotting the same function as a function of for a value of x something remarkable happens a peak emerges to help understand these two points of view of the one function shows a surface plot of p as a function of x and the likelihood function p is the product of the n functions of p j for a dataset consisting of several points e.g. the six points fxgn x figure the probability density p as a function of x and figures and are vertical sections through this surface. figure the likelihood function in the case of a six-point dataset p as a function of copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference if you have any understanding this chapter i recommend ensuring you are happy with exercises and then noting their similarity to exercise steve summarized bayes theorem as embodying the fact that what you know about after the data arrive is what you knew before and what the data told you probabilities are used here to quantify degrees of belief. to nip possible confusion in the bud it must be emphasized that the hypothesis that correctly describes the situation is not a stochastic variable and the fact that the bayesian uses a probability distribution p does not mean that he thinks of the world as stochastically changing its nature between the states described by the hypotheses. he uses the notation of probabilities to represent his beliefs about the mutually exclusive micro-hypotheses values of of which only one is actually true. that probabilities can denote degrees of belief given assumptions seemed reasonable to me. the posterior probability distribution represents the unique and complete solution to the problem. there is no need to invent estimators nor do we need to invent criteria for comparing alternative estimators with each other. whereas orthodox statisticians twenty ways of solving a problem and another twenty criteria for deciding which of these solutions is the best bayesian statistics only one answer to a well-posed problem. assumptions in inference our inference is conditional on our assumptions example the prior p critics view such priors as a because they are subjective but i don t see how it could be otherwise. how can one perform inference without making assumptions? i believe that it is of great value that bayesian methods force one to make these tacit assumptions explicit. first once assumptions are made the inferences are objective and unique reproducible with complete agreement by anyone who has the same information and makes the same assumptions. for example given the assumptions listed above h and the data d everyone will agree about the posterior probability of the decay length p dh p j p jh second when the assumptions are explicit they are easier to criticize and easier to modify indeed we can quantify the sensitivity of our inferences to the details of the assumptions. for example we can note from the likelihood curves in that in the case of a single data point at x the likelihood function is less strongly peaked than in the case x the details of the prior p become increasingly important as the sample mean gets closer to the middle of the window in the case x the likelihood function doesn t have a peak at all such data merely rule out small values of and don t give any information about the relative probabilities of large values of so in this case the details of the prior at the end of things are not important but at the end the prior is important. third when we are not sure which of various alternative assumptions is the most appropriate for a problem we can treat this question as another inference task. thus given data d we can compare alternative assumptions h using bayes theorem p j d i p jh ip j i p j i copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the bent coin where i denotes the highest assumptions which we are not questioning. fourth we can take into account our uncertainty regarding such assumptions when we make subsequent predictions. rather than choosing one particular assumption and working out our predictions about some quantity t p i we obtain predictions that take into account our uncertainty about h by using the sum rule p d i p dh ip j d i this is another contrast with orthodox statistics in which it is conventional to test a default model and then if the test accepts the model at some level to use exclusively that model to make predictions. steve thus persuaded me that probability theory reaches parts that ad hoc methods cannot reach. let s look at a few more examples of simple inference problems. the bent coin a bent coin is tossed f times we observe a sequence s of heads and tails we ll denote by the symbols a and b. we wish to know the bias of the coin and predict the probability that the next toss will result in a head. we encountered this task in example and we will encounter it again in chapter when we discuss adaptive data compression. it is also the original inference problem studied by thomas bayes in his essay published in as in exercise we will assume a uniform prior distribution and obtain a posterior distribution by multiplying by the likelihood. a critic might object where did this prior come from? i will not claim that the uniform prior is in any way fundamental indeed we ll give examples of nonuniform priors later. the prior is a subjective assumption. one of the themes of this book is you can t do inference or data compression without making assumptions. we give the name to our assumptions. ll be introducing an alternative set of assumptions in a moment. the probability given p a that f tosses result in a sequence s that contains ffa fbg counts of the two outcomes is example p aabaj pa f papa our model assumes a uniform prior distribution for pa p pa pfa a pafb p pa and pb pa. inferring unknown parameters given a string of length f of which fa are as and fb are bs we are interested in inferring what pa might be predicting whether the next character is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference an a or a b. are always expressed as probabilities. so predicting whether the next character is an a is the same as computing the probability that the next character is an a. assuming to be true the posterior probability of pa given a string s of length f that has counts ffa fbg is by bayes theorem p pa p j s p the factor p pa which as a function of pa is known as the likelihood function was given in equation the prior p a was given in equation our inference of pa is thus pfa a pafb p the normalizing constant is given by the beta integral p j s dpa pfa p exercise sketch the posterior probability p j s aba f a pafb what is the most probable value of pa the value that maximizes the posterior probability density? what is the mean value of p a under this distribution? fb fb fa!fb! the answer same p j s bbb f from inferences to predictions questions for the posterior probability our prediction about the next toss the probability that the next toss is an a is obtained by integrating over pa. this has the of taking into account our uncertainty about pa when making predictions. by the sum rule p s f z dpa p pap j s f the probability of an a given pa is simply pa so a pafb pfa p f p s f dpa pa z dpa pafb p f fb fb! a which is known as laplace s rule. fa! fb! fb fa fa fb the bent coin and model comparison imagine that a scientist introduces another theory for our data. he asserts that the source is not really a bent coin but is really a perfectly formed die with one face painted heads a and the other painted tails b thus the parameter pa which in the original model could take any value between and is according to the new hypothesis not a free parameter at all rather it is equal to hypothesis is termed so that the of each model indicates its number of free parameters. how can we compare these two models in the light of data? we wish to infer how probable is relative to copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the bent coin and model comparison model comparison as inference in order to perform model comparison we write down bayes theorem again but this time with a argument on the left-hand side. we wish to know how probable is given the data. by bayes theorem p j s f p p f similarly the posterior probability of is p j s f p p f the normalizing constant in both cases is p f which is the total probability of getting the observed data. if and are the only models under consideration this probability is given by the sum rule p f p p to evaluate the posterior probabilities of the hypotheses we need to assign values to the prior probabilities p and p in this case we might set these to each. and we need to evaluate the data-dependent terms p and p we can give names to these quantities. the quantity p is a measure of how much the data favour and we call it the evidence for model we already encountered this quantity in equation where it appeared as the normalizing constant of the inference we made the inference of pa given the data. how model comparison works the evidence for a model is usually the normalizing constant of an earlier bayesian inference. we evaluated the normalizing constant for model in the evidence for model is very simple because this model has no parameters to infer. to be we have p pfa thus the posterior probability ratio of model to model is p j s f p j s f p p fb pfa fa!fb! some values of this posterior probability ratio are illustrated in table the lines illustrate that some outcomes favour one model and some favour the other. no outcome is completely incompatible with either model. with small amounts of data tosses say it is typically not the case that one of the two models is overwhelmingly more probable than the other. but with more data the evidence against given by any data set with the ratio fa fb from mounts up. you can t predict in advance how much data are needed to be pretty sure which theory is true. it depends what p a is. the simpler model since it has no adjustable parameters is able to lose out by the biggest margin. the odds may be hundreds to one against it. the more complex model can never lose out by a large margin there s no data set that is actually unlikely given model copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference table outcome of model comparison between models and for the bent coin model states that pa pb figure typical behaviour of the evidence in favour of as bent coin tosses accumulate under three conditions horizontal axis is the number of tosses f the vertical axis on the left is ln p j f p j f vertical axis shows the values of p j f p j f the three rows show independent simulated experiments. also the right-hand f data fb p j s f p j s f is true pa is true pa pa exercise show that after f tosses have taken place the biggest value that the log evidence ratio log p p can have scales linearly with f if is more probable but the log evidence in favour of can grow at most as log f exercise putting your sampling theory hat on assuming fa has not yet been measured compute a plausible range that the log evidence ratio might lie in as a function of f and the true value of p a and sketch it as a function of f for pa pa and pa sketch the log evidence as a function of the random variable f a and work out the mean and standard deviation of fa. typical behaviour of the evidence figure shows the log evidence ratio as a function of the number of tosses f in a number of simulated experiments. in the left-hand experiments was true. in the right-hand ones was true and the value of pa was either or we will discuss model comparison more in a later chapter. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. an example of legal evidence an example of legal evidence the following example illustrates that there is more to bayesian inference than the priors. two people have left traces of their own blood at the scene of a crime. a suspect oliver is tested and found to have type o blood. the blood groups of the two traces are found to be of type o common type in the local population having frequency and of type ab rare type with frequency do these data o and ab blood were found at scene give evidence in favour of the proposition that oliver was one of the two people present at the crime? a careless lawyer might claim that the fact that the suspect s blood type was found at the scene is positive evidence for the theory that he was present. but this is not so. denote the proposition the suspect and one unknown person were present by s. the alternative states two unknown people from the population were present the prior in this problem is the prior probability ratio between the propositions s and this quantity is important to the verdict and would be based on all other available information in the case. our task here is just to evaluate the contribution made by the data d that is the likelihood ratio p j shp j in my view a jury s task should generally be to multiply together carefully evaluated likelihood ratios from each independent piece of admissible evidence with an equally carefully reasoned prior probability. view is shared by many statisticians but learned british appeal judges recently disagreed and actually overturned the verdict of a trial because the jurors had been taught to use bayes theorem to handle complicated dna evidence. the probability of the data given s is the probability that one unknown person drawn from the population has blood type ab p j sh pab given s we already know that one trace will be of type o. the probability of the data given is the probability that two unknown people drawn from the population have types o and ab p j po pab in these equations h denotes the assumptions that two people were present and left blood there and that the probability distribution of the blood groups of unknown people in an explanation is the same as the population frequencies. dividing we obtain the likelihood ratio p j sh p j thus the data in fact provide weak evidence against the supposition that oliver was present. this result may be found surprising so let us examine it from various points of view. first consider the case of another suspect alberto who has type ab. intuitively the data do provide evidence in favour of the theory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference that this suspect was present relative to the null hypothesis and indeed the likelihood ratio in this case is p j p j pab now let us change the situation slightly imagine that of people are of blood type o and the rest are of type ab. only these two blood types exist in the population. the data at the scene are the same as before. consider again how these data our beliefs about oliver a suspect of type o and alberto a suspect of type ab. intuitively we still believe that the presence of the rare ab blood provides positive evidence that alberto was there. but does the fact that type o blood was detected at the scene favour the hypothesis that oliver was present? if this were the case that would mean that regardless of who the suspect is the data make it more probable they were present everyone in the population would be under greater suspicion which would be absurd. the data may be compatible with any suspect of either blood type being present but if they provide evidence for some theories they must also provide evidence against other theories. here is another way of thinking about this imagine that instead of two people s blood stains there are ten and that in the entire local population of one hundred there are ninety type o suspects and ten type ab suspects. consider a particular type o suspect oliver without any other information and before the blood test results come in there is a one in chance that he was at the scene since we know that out of the suspects were present. we now get the results of blood tests and that nine of the ten stains are of type ab and one of the stains is of type o. does this make it more likely that oliver was there? no there is now only a one in ninety chance that he was there since we know that only one person present was of type o. maybe the intuition is aided by writing down the formulae for the general case where no blood stains of individuals of type o are found and nab of type ab a total of n individuals in all and unknown people come from a large population with fractions po pab. may be other blood types too. the task is to evaluate the likelihood ratio for the two hypotheses s the type o suspect and unknown others left n stains and n unknowns left n stains the probability of the data under hypothesis is just the probability of getting no nab individuals of the two types when n individuals are drawn at random from the population p nab j n no! nab! o pnab pno ab in the case of hypothesis s we need the distribution of the n other individuals o pnab ab p nab j s the likelihood ratio is nab! p nab j s p nab j non po this is an instructive result. the likelihood ratio i.e. the contribution of these data to the question of whether oliver was present depends simply on a comparison of the frequency of his blood type in the observed data with the background frequency in the population. there is no dependence on the counts of the other types found at the scene or their frequencies in the population. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises if there are more type o stains than the average number expected under hypothesis then the data give evidence in favour of the presence of oliver. conversely if there are fewer type o stains than the expected number under then the data reduce the probability of the hypothesis that he was there. in the special case non po the data contribute no evidence either way regardless of the fact that the data are compatible with the hypothesis s. exercises exercise the three doors normal rules. on a game show a contestant is told the rules as follows there are three doors labelled a single prize has been hidden behind one of them. you get to select one door. initially your chosen door will not be opened. instead the gameshow host will open one of the other two doors and he will do so in such a way as not to reveal the prize. for example if you choose door he will then open one of doors and and it is guaranteed that he will choose which one to open so that the prize will not be revealed. at this point you will be given a fresh choice of door you can either stick with your choice or you can switch to the other closed door. all the doors will then be opened and you will receive whatever is behind your choice of door. imagine that the contestant chooses door then the gameshow host opens door revealing nothing behind the door as promised. should the contestant stick with door or switch to door or does it make no exercise the three doors earthquake scenario. imagine that the game happens again and just as the gameshow host is about to open one of the doors a violent earthquake rattles the building and one of the three doors open. it happens to be door and it happens not to have the prize behind it. the contestant had initially chosen door repositioning his the host suggests ok since you chose door initially door is a valid door for me to open according to the rules of the game i ll let door stay open. let s carry on as if nothing happened. should the contestant stick with door or switch to door or does it make no assume that the prize was placed randomly that the gameshow host does not know where it is and that the door open because its latch was broken by the earthquake. similar alternative scenario is a gameshow whose confused host forgets the rules and where the prize is and opens one of the unchosen doors at random. he opens door and the prize is not revealed. should the contestant choose what s behind door or door does the optimal decision for the contestant depend on the contestant s beliefs about whether the gameshow host is confused or not? exercise another example in which the emphasis is not on priors. you visit a family whose three children are all at the local school. you don t copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference know anything about the sexes of the children. while walking clumsily round the home you stumble through one of the three unlabelled bedroom doors that you know belong one each to the three children and that the bedroom contains girlie in quantities to convince you that the child who lives in that bedroom is a girl. later you sneak a look at a letter addressed to the parents which reads from the headmaster we are sending this letter to all parents who have male children at the school to inform them about the following boyish matters. these two sources of evidence establish that at least one of the three children is a girl and that at least one of the children is a boy. what are the probabilities that there are two girls and one boy two boys and one girl? exercise mrs s is found stabbed in her family garden. mr s behaves strangely after her death and is considered as a suspect. on investigation of police and social records it is found that mr s had beaten up his wife on at least nine previous occasions. the prosecution advances this data as evidence in favour of the hypothesis that mr s is guilty of the murder. ah no says mr s s highly paid lawyer statistically only one in a thousand wife-beaters actually goes on to murder his so the wife-beating is not strong evidence at all. in fact given the wife-beating evidence alone it s extremely unlikely that he would be the murderer of his wife only a chance. you should therefore him innocent. is the lawyer right to imply that the history of wife-beating does not point to mr s s being the murderer? or is the lawyer a slimy trickster? if the latter what is wrong with his argument? received an indignant letter from a lawyer about the preceding paragraph i d like to add an extra inference exercise at this point does my suggestion that mr. s. s lawyer may have been a slimy trickster imply that i believe all lawyers are slimy tricksters? no. exercise a bag contains one counter known to be either white or black. a white counter is put in the bag is shaken and a counter is drawn out which proves to be white. what is now the chance of drawing a white counter? that the state of the bag after the operations is exactly identical to its state before. exercise you move into a new house the phone is connected and you re pretty sure that the phone number is but not as sure as you would like to be. as an experiment you pick up the phone and dial you obtain a busy signal. are you now more sure of your phone number? if so how much? exercise in a game two coins are tossed. if either of the coins comes up heads you have won a prize. to claim the prize you must point to one of your coins that is a head and say look that coin s a head i ve won you watch fred play the game. he tosses the two coins and he the u.s.a. it is estimated that million women are abused each year by their partners. in women were victims of homicide of those women were slain by husbands and boyfriends. httpwww.umn.edumincavapapersfactoid.htm httpwww.gunfree.inter.netvpcwomenfs.htm copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions points to a coin and says look that coin s a head i ve won what is the probability that the other coin is a head? exercise a statistical statement appeared in the guardian on friday january when spun on edge times a belgian one-euro coin came up heads times and tails it looks very suspicious to me said barry blight a statistics lecturer at the london school of economics. if the coin were unbiased the chance of getting a result as extreme as that would be less than but do these data give evidence that the coin is biased rather than fair? see equation solutions solution to exercise probabilities let the data be d. assuming equal prior p d p j d and p d solution to exercise the probability of the data given each hypothesis is p j a p j b p j c figure posterior probability for the bias pa of a bent coin given two data sets. so p d p j d p j d p j s aba f a pa p j s bbb f solution to exercise p j s aba f pa. the most probable value of pa the value that maximizes the posterior probability density is the mean value of pa is see copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference the right-hand figure range of plausible values of the log evidence in favour of as a function of f the vertical axis on the left is log p j p j vertical axis shows the values of p j p j the solid line shows the log evidence if the random variable fa takes on its mean value fa paf the dotted lines show the log evidence if fa is at its or percentile. also p j s bbb f the most probable value of pa the value that maximizes the posterior probability density is the mean value of pa is see is true pa is true pa pa solution to exercise the curves in were found by the mean and standard deviation of fa then setting fa to the mean two standard deviations to get a plausible range for fa and computing the three corresponding values of the log evidence ratio. solution to exercise let hi denote the hypothesis that the prize is behind door i. we make the following assumptions the three hypotheses and are equiprobable a priori i.e. p p p the datum we receive after choosing door is one of d and d door or is opened respectively. we assume that these two possible outcomes have the following probabilities. if the prize is behind door then the host has a free choice in this case we assume that the host selects at random between d and d otherwise the choice of the host is forced and the probabilities are and p p p p p p now using bayes theorem we evaluate the posterior probabilities of the hypotheses p j d p p p j d p the denominator p is because it is the normalizing constant for this posterior distribution. so p j d p p j d p p j d p j d p j d so the contestant should switch to door in order to have the biggest chance of getting the prize. many people this outcome surprising. there are two ways to make it more intuitive. one is to play the game thirty times with a friend and keep track of the frequency with which switching gets the prize. alternatively you can perform a thought experiment in which the game is played with a million doors. the rules are now that the contestant chooses one door then the game copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions show host opens doors in such a way as not to reveal the prize leaving the contestant s selected door and one other door closed. the contestant may now stick or switch. imagine the contestant confronted by a million doors of which doors and have not been opened door having been the contestant s initial guess. where do you think the prize is? solution to exercise if door is opened by an earthquake the inference comes out even though visually the scene looks the same. the nature of the data and the probability of the data are both now the possible data outcomes are that any number of the doors might have opened. we could label the eight possible outcomes d secondly it might be that the prize is visible after the earthquake has opened one or more doors. so the data d consists of the value of d and a statement of whether the prize was revealed. it is hard to say what the probabilities of these outcomes are since they depend on our beliefs about the reliability of the door latches and the properties of earthquakes but it is possible to extract the desired posterior probability without naming the values of p for each d. all that matters are the relative values of the quantities p p p for the value of d that actually occurred. is the likelihood principle which we met in section the value of d that actually occurred is d and no prize visible first it is clear that p since the datum that no prize is visible is incompatible with now assuming that the contestant selected door how does the probability p compare with p assuming that earthquakes are not sensitive to decisions of game show contestants these two quantities have to be equal by symmetry. we don t know how likely it is that door falls its hinges but however likely it is it s just as likely to do so whether the prize is behind door or door so if p and p are equal we obtain p p p p p p p p p the two possible hypotheses are now equally likely. if we assume that the host knows where the prize is and might be acting deceptively then the answer might be further because we have to view the host s words as part of the data. confused? it s well worth making sure you understand these two gameshow problems. don t worry i slipped up on the second problem the time i met it. there is a general rule which helps immensely when you have a confusing probability problem always write down the probability of everything. gull from this joint probability any desired inference can be mechanically ob tained solution to exercise the statistic quoted by the lawyer indicates the probability that a randomly selected wife-beater will also murder his wife. the probability that the husband was the murderer given that the wife has been murdered is a completely quantity. where the prize is door door door none pnone pnone pnone e k a u q h t r a e y b d e n e p o s r o o d h c i h w figure the probability of everything for the second three-door problem assuming an earthquake has just occurred. here is the probability that door alone is opened by an earthquake. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference to deduce the latter we need to make further assumptions about the probability that the wife is murdered by someone else. if she lives in a neighbourhood with frequent random murders then this probability is large and the posterior probability that the husband did it the absence of other evidence may not be very large. but in more peaceful regions it may well be that the most likely person to have murdered you if you are found murdered is one of your closest relatives. let s work out some illustrative numbers with the help of the statistics on page let m denote the proposition that a woman has been murdered h the proposition that the husband did it and b the proposition that he beat her in the year preceding the murder. the statement someone else did it is denoted by h we need to p m p h m and p h m in order to compute the posterior probability p b m from the statistics we can read out p m and if two million women out of million are beaten then p h m finally we need a value for p h m if a man murders his wife how likely is it that this is the time he laid a on her? i expect it s pretty unlikely so maybe p h m is or larger. by bayes theorem then p b m one way to make obvious the sliminess of the lawyer on is to construct arguments with the same logical structure as his that are clearly wrong. for example the lawyer could say not only was mrs. s murdered she was murdered between and statistically only one in a million wife-beaters actually goes on to murder his wife between and so the wife-beating is not strong evidence at all. in fact given the wife-beating evidence alone it s extremely unlikely that he would murder his wife in this way only a chance. solution to exercise there are two hypotheses. your number is it is another number. the data d are when i dialed i got a busy signal what is the probability of d given each hypothesis? if your number is then we expect a busy signal with certainty p on the other hand if is true then the probability that the number dialled returns a busy signal is smaller than since various other outcomes were also possible ringing tone or a number-unobtainable signal for example. the value of this probability p will depend on the probability that a random phone number similar to your own phone number would be a valid phone number and on the probability that you get a busy signal when you dial a valid phone number. i estimate from the size of my phone book that cambridge has about valid phone numbers all of length six digits. the probability that a random six-digit number is valid is therefore about if we exclude numbers beginning with and from the random choice the probability is about if we assume that telephone numbers are clustered then a misremembered number might be more likely to be valid than a randomly chosen number so the probability that our guessed number would be valid assuming is true might be bigger than copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions anyway must be somewhere between and we can carry forward this uncertainty in the probability and see how much it matters at the end. the probability that you get a busy signal when you dial a valid phone number is equal to the fraction of phones you think are in use or when you make your tentative call. this fraction varies from town to town and with the time of day. in cambridge during the day i would guess that about of phones are in use. at maybe or fewer. the probability p is the product of and that is about according to our estimates there s about a one-in-a-thousand chance of getting a busy signal when you dial a random number or one-in-ahundred if valid numbers are strongly clustered or if you dial in the wee hours. how do the data your beliefs about your phone number? the posterior probability ratio is the likelihood ratio times the prior probability ratio p j d p j d p p p p the likelihood ratio is about or so the posterior probability ratio is swung by a factor of or in favour of if the prior probability of was then the posterior probability is p j d p j d p j d or solution to exercise we compare the models the coin is fair and the coin is biased with the prior on its bias set to the uniform distribution p use of a uniform prior seems reasonable to me since i know that some coins such as american pennies have severe biases when spun on edge so the situations p or p or p would not surprise me. when i mention the coin is fair a pedant would say how absurd to even consider that the coin is fair any coin is surely biased to some extent and of course i would agree. so will pedants kindly understand as meaning the coin is fair to within one part in a thousand i.e. p the likelihood ratio is p p thus the data give scarcely any evidence either way in fact they give weak evidence to one in favour of no no objects the believer in bias your silly uniform prior doesn t represent my prior beliefs about the bias of biased coins i was expecting only a small bias to be as generous as possible to the let s see how well it could fare if the prior were presciently set. let us allow a prior of the form p where figure the probability distribution of the number of heads given the two hypotheses that the coin is fair and that it is biased with the prior distribution of the bias being uniform. the outcome heads gives weak evidence in favour of the hypothesis that the coin is fair. beta distribution with the original uniform prior reproduced by setting by tweaking the likelihood ratio for over p p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about inference can be increased a little. it is shown for several values of in even the most favourable choice of can yield a likelihood ratio of only two to one in favour of in conclusion the data are not very suspicious they can be construed as giving at most two-to-one evidence in favour of one or other of the two hypotheses. are these wimpy likelihood ratios the fault of over-restrictive priors? is there any way of producing a very suspicious conclusion? the prior that is bestmatched to the data in terms of likelihood is the prior that sets p to f with probability one. let s call this model the likelihood ratio is p f so the strongest evidence that these data can possibly muster against the hypothesis that there is no bias is six-to-one. while we are noticing the absurdly misleading answers that sampling theory statistics produces such as the p-value of in the exercise we just solved let s stick the boot in. if we make a tiny change to the data set increasing the number of heads in tosses from to we that the p-value goes below the mystical value of p-value is the sampling theory statistician would happily squeak the probability of getting a result as extreme as heads is smaller than we thus reject the null hypothesis at a level of the correct answer is shown for several values of in the values worth highlighting from this table are the likelihood ratio when uses the standard uniform prior which is in favour of the null hypothesis second the most favourable choice of from the point of view of can only yield a likelihood ratio of about in favour of be warned! a p-value of is often interpreted as implying that the odds are stacked about twenty-to-one against the null hypothesis. but the truth in this case is that the evidence either slightly favours the null hypothesis or disfavours it by at most to one depending on the choice of prior. the p-values and levels of classical statistics should be treated with extreme caution. shun them! here ends the sermon. p p figure likelihood ratio for various choices of the prior distribution s hyperparameter p p figure likelihood ratio for various choices of the prior distribution s hyperparameter when the data are heads in trials. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. part i data compression copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. notation x is a member of the x a set a s is a subset of the s a set a s a s is a subset of or equal to the set a v b a v is the union of the sets b and a v b a v is the intersection of the sets b and a number of elements jaj in set a about chapter in this chapter we discuss how to measure the information content of the outcome of a random experiment. this chapter has some tough bits. if you the mathematical details hard skim through them and keep going you ll be able to enjoy chapters and without this chapter s tools. before reading chapter you should have read chapter and worked on exercises and and exercise below. the following exercise is intended to help you think about how to measure information content. exercise please work on this problem before reading chapter you are given balls all equal in weight except for one that is either heavier or lighter. you are also given a two-pan balance to use. in each use of the balance you may put any number of the balls on the left pan and the same number on the right pan and push a button to initiate the weighing there are three possible outcomes either the weights are equal or the balls on the left are heavier or the balls on the left are lighter. your task is to design a strategy to determine which is the odd ball and whether it is heavier or lighter than the others in as few uses of the balance as possible. while thinking about this problem you may it helpful to consider the following questions how can one measure information? when you have the odd ball and whether it is heavy or light how much information have you gained? once you have designed a strategy draw a tree showing for each of the possible outcomes of a weighing what weighing you perform next. at each node in the tree how much information have the outcomes so far given you and how much information remains to be gained? how much information is gained when you learn the state of a coin the states of two coins the outcome when a four-sided die is rolled? how much information is gained on the step of the weighing problem if balls are weighed against the other how much is gained if are weighed against on the step leaving out balls? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem how to measure the information content of a random variable? in the next few chapters we ll be talking about probability distributions and random variables. most of the time we can get by with sloppy notation but occasionally we will need precise notation. here is the notation that we established in chapter an ensemble x is a triple where the outcome x is the value of a random variable which takes on one of a set of possible values ax ai aig having probabilities px pig with p ai pi pi and how can we measure the information content of an outcome x ai from such an ensemble? in this chapter we examine the assertions p ai that the shannon information content hx ai pi is a sensible measure of the information content of the outcome x ai and that the entropy of the ensemble hx pi pi is a sensible measure of the ensemble s average information content. hp p p p hp p figure the shannon information content hp and the binary entropy function hp p p p function of p. as a p figure shows the shannon information content of an outcome with probability p as a function of p. the less probable an outcome is the greater its shannon information content. figure also shows the binary entropy function hp p p p p which is the entropy of the ensemble x whose alphabet and probability distribution are ax fa bgpx fp pg. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem information content of independent random variables why should log have anything to do with the information content? why not some other function of pi? we ll explore this question in detail shortly but notice a nice property of this particular function hx log imagine learning the value of two independent random variables x and y. the of independence is that the probability distribution is separable into a product p y p intuitively we might want any measure of the amount of information gained to have the property of additivity that is for independent random variables x and y the information gained when we learn x and y should equal the sum of the information gained if x alone were learned and the information gained if y alone were learned. the shannon information content of the outcome x y is hx y log p y log p log p log p so it does indeed satisfy hx y hx hy if x and y are independent. exercise show that if x and y are independent the entropy of the outcome x y hx y hx hy in words entropy is additive for independent variables. we now explore these ideas with some examples then in section and in chapters and we prove that the shannon information content and the entropy are related to the number of bits needed to describe the outcome of an experiment. the weighing problem designing informative experiments have you solved the weighing problem yet? are you sure? notice that in three uses of the balance which reads either left heavier right heavier or balanced the number of conceivable outcomes is whereas the number of possible states of the world is the odd ball could be any of twelve balls and it could be heavy or light. so in principle the problem might be solvable in three weighings but not in two since if you know how you can determine the odd weight and whether it is heavy or light in three weighings then you may read on. if you haven t found a strategy that always gets there in three weighings i encourage you to think about exercise some more. why is your strategy optimal? what is it about your series of weighings that allows useful information to be gained as quickly as possible? the answer is that at each step of an optimal procedure the three outcomes left heavier right heavier and balance are as close as possible to equiprobable. an optimal solution is shown in suboptimal strategies such as weighing balls against on the step do not achieve all outcomes with equal probability these two sets of balls can never balance so the only possible outcomes are left heavy and right heavy such a binary outcome rules out only half of the possible hypotheses copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. how to measure the information content of a random variable? weigh b b b b b b b b b b b b b bbn weigh weigh weigh a a a a au a a a a au a a a a au figure an optimal solution to the weighing problem. at each step there are two boxes the left box shows which hypotheses are still possible the right box shows the balls involved in the next weighing. the hypotheses are written with e.g. denoting that is the odd ball and it is heavy. weighings are written by listing the names of the balls on the two pans separated by a line for example in the weighing balls and are put on the left-hand side and and on the right. in each triplet of arrows the upper arrow leads to the situation when the left side is heavier the middle arrow to the situation when the right side is heavier and the lower arrow to the situation when the outcome is balanced. the three points labelled correspond to impossible outcomes. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem so a strategy that uses such outcomes must sometimes take longer to the right answer. the insight that the outcomes should be as near as possible to equiprobable makes it easier to search for an optimal strategy. the weighing must divide the possible hypotheses into three groups of eight. then the second weighing must be chosen so that there is a split of the hypotheses. thus we might conclude the outcome of a random experiment is guaranteed to be most informative if the probability distribution over outcomes is uniform. this conclusion agrees with the property of the entropy that you proved when you solved exercise the entropy of an ensemble x is biggest if all the outcomes have equal probability pi guessing games in the game of twenty questions one player thinks of an object and the other player attempts to guess what the object is by asking questions that have yesno answers for example is it alive? or is it human? the aim is to identify the object with as few questions as possible. what is the best strategy for playing this game? for simplicity imagine that we are playing the rather dull version of twenty questions called sixty-three example the game sixty-three what s the smallest number of yesno questions needed to identify an integer x between and intuitively the best questions successively divide the possibilities into equal sized sets. six questions one reasonable strategy asks the following questions is x is x mod is x mod is x mod is x mod is x mod notation x mod pronounced x modulo denotes the remainder when x is divided by for example mod and mod the answers to these questions if translated from fyes nog to give the binary expansion of x for example what are the shannon information contents of the outcomes in this example? if we assume that all values of x are equally likely then the answers to the questions are independent and each has shannon information content bit the total shannon information gained is always six bits. furthermore the number x that we learn from these questions is a six-bit binary number. our questioning strategy a way of encoding the random variable x as a binary so far the shannon information content makes sense it measures the length of a binary that encodes x. however we have not yet studied ensembles where the outcomes have unequal probabilities. does the shannon information content make sense there too? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. how to measure the information content of a random variable? x n x n x y figure a game of submarine. the submarine is hit on the attempt. a b c d e f g h x n x n move question outcome p hx total info. j the game of submarine how many bits can one bit convey? in the game of battleships each player hides a of ships in a sea represented by a square grid. on each turn one player attempts to hit the other s ships by at one square in the opponent s sea. the response to a selected square such as is either miss hit or hit and destroyed in a boring version of battleships called submarine each player hides just one submarine in one square of an eight-by-eight grid. figure shows a few pictures of this game in progress the circle represents the square that is being at and the show squares in which the outcome was a miss x n the submarine is hit x y shown by the symbol s on the attempt. each shot made by a player an ensemble. the two possible outcomes are fy ng corresponding to a hit and a miss and their probabilities depend on the state of the board. at the beginning p and p at the second shot if the shot missed p and p at the third shot if the two shots missed p and p the shannon information gained from an outcome x is hx if we are lucky and hit the submarine on the shot then hx bits now it might seem a little strange that one binary outcome can convey six bits. but we have learnt the hiding place which could have been any of squares so we have by one lucky binary question indeed learnt six bits. what if the shot misses? the shannon information that we gain from this outcome is hx bits does this make sense? it is not so obvious. let s keep going. if our second shot also misses the shannon information content of the second outcome is bits if we miss thirty-two times at a new square each time the total shannon information gained is bits copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem why this round number? well what have we learnt? we now know that the submarine is not in any of the squares we at learning that fact is just like playing a game of sixty-three asking as our question is x one of the thirty-two numbers corresponding to these squares i at? and receiving the answer no this answer rules out half of the hypotheses so it gives us one bit. after unsuccessful shots the information gained is bits the unknown location has been narrowed down to one quarter of the original hypothesis space. what if we hit the submarine on the shot when there were squares left? the shannon information content of this outcome is bits the total shannon information content of all the outcomes is bits so once we know where the submarine is the total shannon information content gained is bits. this result holds regardless of when we hit the submarine. if we hit it when there are n squares left to choose from n was in equation then the total information gained is n n n n n n bits what have we learned from the examples so far? i think the submarine example makes quite a convincing case for the claim that the shannon information content is a sensible measure of information content. and the game of sixty-three shows that the shannon information content can be intimately connected to the size of a that encodes the outcomes of a random experiment thus suggesting a possible connection to data compression. in case you re not convinced let s look at one more example. the wenglish language wenglish is a language similar to english. wenglish sentences consist of words drawn at random from the wenglish dictionary which contains words all of length characters. each word in the wenglish dictionary was constructed at random by picking letters from the probability distribution over a depicted in some entries from the dictionary are shown in alphabetical order in notice that the number of words in the dictionary is much smaller than the total number of possible words of length letters because the probability of the letter z is about only of the words in the dictionary begin with the letter z. in contrast the probability of the letter a is about and of the words begin with the letter a. of those words two start az and start aa. let s imagine that we are reading a wenglish document and let s discuss the shannon information content of the characters as we acquire them. if we aaail aaaiu aaald abati azpan aztdn odrcr zatnt zxast figure the wenglish dictionary. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. data compression are given the text one word at a time the shannon information content of each word is log bits since wenglish uses all its words with equal probability. the average information content per character is therefore bits. now let s look at the information content if we read the document one character at a time. if say the letter of a word is a the shannon information content is log bits. if the letter is z the shannon information content is log bits. the information content is thus highly variable at the character. the total information content of the characters in a word however is exactly bits so the letters that follow an initial z have lower average information content per character than the letters that follow an initial a. a rare initial letter such as z indeed conveys more information about what the word is than a common initial letter. similarly in english if rare characters occur at the start of the word xyl... then often we can identify the whole word immediately whereas words that start with common characters pro... require more characters before we can identify them. data compression the preceding examples justify the idea that the shannon information content of an outcome is a natural measure of its information content. improbable outcomes do convey more information than probable outcomes. we now discuss the information content of a source by considering how many bits are needed to describe the outcome of an experiment. if we can show that we can compress data from a particular source into a of l bits per source symbol and recover the data reliably then we will say that the average information content of that source is at most l bits per symbol. example compression of text a is composed of a sequence of bytes. a byte is composed of bits and can have a decimal value between and a typical text is composed of the ascii character set values to this character set uses only seven of the eight bits in a byte. here we use the word bit with its meaning a symbol with two values not to be confused with the unit of information content. exercise by how much could the size of a be reduced given that it is an ascii how would you achieve this reduction? intuitively it seems reasonable to assert that an ascii contains as much information as an arbitrary of the same size since we already know one out of every eight bits before we even look at the this is a simple example of redundancy. most sources of data have further redundancy english text use the ascii characters with non-equal frequency certain pairs of letters are more probable than others and entire words can be predicted given the context and a semantic understanding of the text. some simple data compression methods that measures of information content one way of measuring the information content of a random variable is simply to count the number of possible outcomes jaxj. number of elements in a set a is denoted by jaj. if we gave a binary name to each outcome the copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem length of each name would be jaxj bits if jaxj happened to be a power of we thus make the following the raw bit content of x is jaxj is a lower bound for the number of binary questions that are always guaranteed to identify an outcome from the ensemble x. it is an additive quantity the raw bit content of an ordered pair x y having jaxjjay j possible outcomes y this measure of information content does not include any probabilistic element and the encoding rule it corresponds to does not compress the source data it simply maps each outcome to a constant-length binary string. exercise could there be a compressor that maps an outcome x to a binary code cx and a decompressor that maps c back to x such that every possible outcome is compressed into a binary code of length shorter than bits? even though a simple counting argument shows that it is impossible to make a reversible compression program that reduces the size of all amateur compression enthusiasts frequently announce that they have invented a program that can do this indeed that they can further compress compressed by putting them through their compressor several times. stranger yet patents have been granted to these modern-day alchemists. see the comp.compression frequently asked questions for further there are only two ways in which a compressor can actually compress a lossy compressor compresses some but maps some to the same encoding. we ll assume that the user requires perfect recovery of the source so the occurrence of one of these confusable leads to a failure in applications such as image compression lossy compression is viewed as satisfactory. we ll denote by the probability that the source string is one of the confusable so a lossy compressor has a probability of failure. if can be made very small then a lossy compressor may be practically useful. a lossless compressor maps all to encodings if it shortens some it necessarily makes others longer. we try to design the compressor so that the probability that a is lengthened is very small and the probability that it is shortened is large. in this chapter we discuss a simple lossy compressor. in subsequent chapters we discuss lossless compression methods. information content in terms of lossy compression whichever type of compressor we construct we need somehow to take into account the probabilities of the outcomes. imagine comparing the information contents of two text one in which all ascii characters copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. information content in terms of lossy compression are used with equal probability and one in which the characters are used with their frequencies in english text. can we a measure of information content that distinguishes between these two intuitively the latter contains less information per character because it is more predictable. one simple way to use our knowledge that some symbols have a smaller probability is to imagine recoding the observations into a smaller alphabet thus losing the ability to encode some of the more improbable symbols and then measuring the raw bit content of the new alphabet. for example we might take a risk when compressing english text guessing that the most infrequent characters won t occur and make a reduced ascii code that omits the characters f g thereby reducing the size of the alphabet by seventeen. the larger the risk we are willing to take the smaller our alphabet becomes. we introduce a parameter that describes the risk we are taking when using this compression method is the probability that there will be no name for an outcome x. example let ax a b c d e f g h g g and px the raw bit content of this ensemble is bits corresponding to binary names. but notice that p fa b c dg so if we are willing to run a risk of of not having a name for x then we can get by with four names half as many names as are needed if every x ax has a name. x cx a b c d e f g h x a b c d e f g h cx table shows binary names that could be given to the outcomes in the cases and when we need bits to encode the outcome when we need only bits. table binary names for the outcomes for two failure probabilities let us now formalize this idea. to make a compression strategy with risk we make the smallest possible subset such that the probability that x is not in is less than or equal to i.e. p for each value of we can then a new measure of information content the log of the size of this smallest subset ensembles in which several elements have the same probability there may be several smallest subsets that contain elements but all that matters is their sizes are equal so we will not dwell on this ambiguity. the smallest subset is the smallest subset of ax satisfying p the subset can be constructed by ranking the elements of ax in order of decreasing probability and adding successive elements starting from the most probable elements until the total probability is we can make a data compression code by assigning a binary name to each element of the smallest subset. this compression scheme motivates the following measure of information content the essential bit content of x is note that is the special case of with p for all x ax. do not confuse and with the function displayed in figure shows for the ensemble of example as a function of copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. efgh p s d abc the source coding theorem figure the outcomes of x example ranked by their probability. the essential bit content the labels on the graph show the smallest set as a function of note bits and bits. extended ensembles is this compression method any more useful if we compress blocks of symbols from a source? we now turn to examples where the outcome x xn is a string of n independent identically distributed random variables from a single ensemble x. we will denote by x n the ensemble xn remember that entropy is additive for independent variables so hx n n hx. example consider a string of n of a bent coin x xn where xn with probabilities the most probable strings x are those with most if rx is the number of in x then prx p to evaluate n we must the smallest subset this subset will contain all x with rx up to some and some of the x with rx figures and show graphs of n against for the cases n and n the steps are the values of at which changes by and the cusps where the slope of the staircase changes are the points where rmax changes by exercise what are the mathematical shapes of the curves between the cusps? for the examples shown in n depends strongly on the value of so it might not seem a fundamental or useful of information content. but we will consider what happens as n the number of independent variables in x n increases. we will the remarkable result that n becomes almost independent of and for all it is very close to n hx where hx is the entropy of one of the random variables. figure illustrates this asymptotic tendency for the binary ensemble of n n becomes an increasingly function example as n increases copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. information content in terms of lossy compression p figure the sixteen outcomes of the ensemble x with ranked by probability. the essential bit content the upper schematic diagram indicates the strings probabilities by the vertical lines lengths to scale. figure n for n binary variables with n n n n for figure n binary variables with copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem figure the top strings are samples from x where and the bottom two are the most and least probable strings in this ensemble. the column shows the log-probabilities of the random strings which may be compared with the entropy hx bits. x except for tails close to and as long as we are allowed a tiny probability of error compression down to n h bits is possible. even if we are allowed a large probability of error we still can compress only down to n h bits. this is the source coding theorem. theorem shannon s source coding theorem. let x be an ensemble with entropy hx h bits. given and there exists a positive integer such that for n n n typicality why does increasing n help? let s examine long strings from x n table shows samples from x n for n and the probability of a string x that contains r and is p pr the number of strings that contain r is nr so the number of r has a binomial distribution p these functions are shown in the mean of r is n and its standard deviation is pn if n is then r n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. typicality nr p pr p nrp n n t t r r figure anatomy of the typical set t for and n and n these graphs show nr the number of strings containing r the probability p of a single string that contains r the same probability on a log scale and the total probability nrp of all strings that contain r the number r is on the horizontal axis. the plot of p also shows by a dotted line the mean value of p which equals when n and when n the typical set includes only the strings that have p close to this value. the range marked t shows the set tn in section for n and and n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. if n then the source coding theorem r notice that as n gets bigger the probability distribution of r becomes more concentrated in the sense that while the range of possible values of r grows as n the standard deviation of r grows only as pn that r is most likely to fall in a small range of values implies that the outcome x is also most likely to fall in a corresponding small subset of outcomes that we will call the typical set. of the typical set let us typicality for an arbitrary ensemble x with alphabet ax. our of a typical string will involve the string s probability. a long string of n symbols will usually contain about occurrences of the symbol occurrences of the second etc. hence the probability of this string is roughly p p p ppi n i so that the information content of a typical string is p nxi pi pi n h which is the information content of x is so the random variable very likely to be close in value to n h. we build our of typicality on this observation. we the typical elements of an x to be those elements that have probability close to h that the typical set unlike the smallest subset does not include the most probable elements of an x but we will show that these most probable elements contribute negligible probability. we introduce a parameter that how close the probability has to be to h for an element to be typical we call the set of typical elements the typical set tn x tn an n p we will show that whatever value of we choose the typical set contains almost all the probability as n increases. this important result is sometimes called the asymptotic equipartition principle. asymptotic equipartition principle. for an ensemble of n independent identically distributed random variables x n xn with n large the outcome x xn is almost certain to belong to a subset of an x having only hx members each having probability close to hx. notice that if hx then hx is a tiny fraction of the number of possible outcomes jan xj jaxjn the term equipartition is chosen to describe the idea that the members of the typical set have roughly equal probability. should not be taken too literally hence my use of quotes around asymptotic equipartition see page a second meaning for equipartition in thermal physics is the idea that each degree of freedom of a classical system has equal average energy kt this second meaning is not intended here. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. proofs p hx tn figure schematic diagram showing all strings in the ensemble x n ranked by their probability and the typical set tn the asymptotic equipartition principle is equivalent to shannon s source coding theorem statement. n i.i.d. random variables each with entropy hx can be compressed into more than n hx bits with negligible risk of information loss as n conversely if they are compressed into fewer than n hx bits it is virtually certain that information will be lost. these two theorems are equivalent because we can a compression algorithm that gives a distinct name of length n hx bits to each x in the typical set. proofs this section may be skipped if found tough going. the law of large numbers our proof of the source coding theorem uses the law of large numbers. mean and variance of a real random variable are eu pu p u eu p and varu technical note strictly i am assuming here that u is a function ux of a sample x from a discrete ensemble x. then the summations pu p should be written px p this means that p is a sum of delta functions. this restriction guarantees that the mean and variance of u do exist which is not necessarily the case for general p chebyshev s inequality let t be a non-negative real random variable and let be a positive real number. then p proof p p we multiply each term by and obtain p p we add the missing terms and obtain p p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem chebyshev s inequality let x be a random variable and let be a positive real number. then proof take t and apply the previous proposition. weak law of large numbers. take x to be the average of n independent random variables hn having common mean and common variance hn. then h x n pn p proof obtained by showing that and that x hn we are interested in x being very close to the mean very small. no matter how large h is and no matter how small the required is and no matter how small the desired probability that we can always achieve it by taking n large enough. proof of theorem we apply the law of large numbers to the random variable p for x drawn from the ensemble x n this random variable can be written as the average of n information contents hn each of which is a random variable with mean h hx and variance term hn is the shannon information content of the nth outcome. n we again the typical set with parameters n and thus tn an x n p for all x tn the probability of x p and by the law of large numbers p tn we have thus proved the asymptotic equipartition principle. as n increases the probability that x falls in tn approaches for any how does this result relate to source coding? we must relate tn to n we will show that for any given there is a big n such that n n h. part n n h the set tn is not the best subset for compression. so the size of tn gives an upper bound on we show how small n must be by calculating how big tn could possibly be. we are free to set to any convenient value. the smallest possible probability that a member of tn can have is and the total probability contained by tn can t be any bigger than so jtn that is the size of the typical set is bounded by jtn if we set and such that tn becomes a witness to the fact that n jtn n then p and the set n n h h h figure schematic illustration of the two parts of the theorem. given any and we show that for large enough n n n lies below the line h and above the line h copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. cco c tn tn comments part n n h imagine that someone claims this second part is not so that for any n the smallest subset is smaller than the above inequality would allow. we can make use of our typical set to show that they must be mistaken. remember that we are free to set to any value we choose. we will set so that our task is to prove that a subset having and achieving p cannot exist n greater than an that we will specify. so let us consider the probability of falling in this rival smaller subset the probability of the subset is p p p where tn denotes the complement fx tn the maximum value of the term is found if tn contains outcomes all with the maximum probability the maximum value the second term can have is p tn so p tn we can now set and such that p which shows that cannot satisfy the of a subset thus any subset with size has probability less than so by the of n n n n is essentially a constant function of for as illustrated in and thus for large enough n the function comments the source coding theorem has two parts n n h both results are interesting. the part tells us that even if the probability of error is extremely small the number of bits per symbol n n needed to specify a long n string x with vanishingly small error probability does not have to exceed h bits. we need to have only a tiny tolerance for error and the number of bits required drops from to n n h and what happens if we are yet more tolerant to compression errors? part tells us that even if is very close to so that errors are made most of the time the average number of bits per symbol needed to specify x must still be at least h bits. these two extremes tell us that regardless of our allowance for error the number of bits per symbol needed to specify x is h bits no more and no less. caveat regarding asymptotic equipartition i put the words asymptotic equipartition in quotes because it is important not to think that the elements of the typical set tn really do have roughly the same probability as each other. they are similar in probability only in p are within of each other. now as the sense that their values of is decreased how does n have to increase if we are to keep our bound on the mass of the typical set p tn constant? n must grow as so if we write in terms of n as for some constant then copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem the most probable string in the typical set will be of order times greater than the least probable string in the typical set. as decreases n increases and this ratio grows exponentially. thus we have equipartition only in a weak sense! why did we introduce the typical set? the best choice of subset for block compression is not a typical set. so why did we bother introducing the typical set? the answer is we can count the typical set. we know that all its elements have almost identical probability h and we know the whole set has probability almost so the typical set must have roughly h elements. without the help of the typical set is very similar to it would have been hard to count how many elements there are in exercises weighing problems exercise while some people when they encounter the weighing problem with balls and the three-outcome balance think that weighing six balls against six balls is a good weighing others say no weighing six against six conveys no information at all explain to the second group why they are both right and wrong. compute the information gained about which is the odd ball and the information gained about which is the odd ball and whether it is heavy or light. exercise solve the weighing problem for the case where there are balls of which one is known to be odd. exercise you are given balls all of which are equal in weight except for one that is either heavier or lighter. you are also given a bizarre twopan balance that can report only two outcomes the two sides balance or the two sides do not balance design a strategy to determine which is the odd ball in as few uses of the balance as possible. exercise you have a two-pan balance your job is to weigh out bags of with integer weights to pounds inclusive. how many weights do you need? are allowed to put weights on either pan. you re only allowed to put one bag on the balance at a time. exercise is it possible to solve exercise weighing problem with balls and the three-outcome balance using a sequence of three weighings such that the balls chosen for the second weighing do not depend on the outcome of the and the third weighing does not depend on the or second? find a solution to the general n weighing problem in which exactly one of n balls is odd. show that in w weighings an odd ball can be from among n balls. exercise you are given balls and the three-outcome balance of exercise this time two of the balls are odd each odd ball may be heavy or light and we don t know which. we want to identify the odd balls and in which direction they are odd. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises estimate how many weighings are required by the optimal strategy. and what if there are three odd balls? how do your answers change if it is known that all the regular balls weigh g that light balls weigh g and heavy ones weigh g? source coding with a lossy compressor with loss exercise let px sketch for n and n n as a function of exercise let py sketch n and n n as a function of for exercise physics students. discuss the relationship between the proof of the asymptotic equipartition principle and the equivalence large systems of the boltzmann entropy and the gibbs entropy. distributions that don t obey the law of large numbers the law of large numbers which we used in this chapter shows that the mean of a set of n i.i.d. random variables has a probability distribution that becomes narrower with width as n increases. however we have proved this property only for discrete random variables that is for real numbers taking on a set of possible values. while many random variables with continuous probability distributions also satisfy the law of large numbers there are important distributions that do not. some continuous distributions do not have a mean or variance. exercise sketch the cauchy distribution p z x what is its normalizing constant z? can you evaluate its mean or variance? consider the sum z where and are independent random variables from a cauchy distribution. what is p what is the probability distribution of the mean of and what is the probability distribution of the mean of n samples from this cauchy distribution? other asymptotic properties exercise bound. we derived the weak law of large numbers from chebyshev s inequality by letting the random variable t in the inequality p be a function t of the random variable x we were interested in. other useful inequalities can be obtained by using other functions. the bound which is useful for bounding the tails of a distribution is obtained by letting t expsx. show that and p a for any s p a for any s copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem where gs is the moment-generating function of x gs p esx curious functions related to p log exercise this exercise has no purpose at all it s included for the enjoyment of those who like mathematical curiosities. sketch the function f for x hint work out the inverse function to f that is the function gy such that if x gy then y f it s closely related to p log solutions solution to exercise let p y p then hx y xxy xxy xx hx hy p log p p log p p log p p log p p log p solution to exercise an ascii can be reduced in size by a factor of this reduction could be achieved by a block code that maps blocks into blocks by copying the information-carrying bits into bytes and ignoring the last bit of every character. solution to exercise the pigeon-hole principle states you can t put pigeons into holes without using one of the holes twice. similarly you can t give ax outcomes unique binary names of some length l shorter than jaxj bits because there are only such binary names and l jaxj implies jaxj so at least two inputs to the compressor would compress to the same output solution to exercise between the cusps all the changes in probability are equal and the number of elements in t changes by one at each step. so varies logarithmically with solution to exercise this solution was found by dyson and lyness in and presented in the following elegant form by john conway in be warned the symbols a b and c are used to name the balls to name the pans of the balance to name the outcomes and to name the possible states of the odd ball! label the balls by the sequences aab aba abb abc bbc bca bcb bcc caa cab cac cca and in the copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions weighings put aab caa cab cac in pan a aba abb abc bbc in pan b. aab aba abb abc bbc bca bcb bcc aba bca caa cca aab abb bcb cab now in a given weighing a pan will either end up in the canonical position that it assumes when the pans are balanced or above that position or below it so the three weighings determine for each pan a sequence of three of these letters. if both sequences are ccc then there s no odd ball. otherwise for just one of the two pans the sequence is among the above and names the odd ball whose weight is above or below the proper one according as the pan is a or b. in w weighings the odd ball can be from among n balls in the same way by labelling them with all the non-constant sequences of w letters from a b c whose change is a-to-b or b-to-c or c-to-a and at the wth weighing putting those whose wth letter is a in pan a and those whose wth letter is b in pan b. solution to exercise the curves n and are shown in note that bits. n n as a function of for n n n n figure n axis against for n binary variables with solution to exercise the gibbs entropy is kbpi pi ln where i runs over all states of the system. this entropy is equivalent from the factor of kb to the shannon entropy of the ensemble. pi whereas the gibbs entropy can be for any ensemble the boltzmann entropy is only for microcanonical ensembles which have a probability distribution that is uniform over a set of accessible states. the boltzmann entropy is to be sb kb ln where is the number of accessible states of the microcanonical ensemble. this is equivalent from the factor of kb to the perfect information content of that constrained ensemble. the gibbs entropy of a microcanonical ensemble is trivially equal to the boltzmann entropy. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the source coding theorem we now consider a thermal distribution canonical ensemble where the probability of a state x is p z ex kbt with this canonical ensemble we can associate a corresponding microcanonical ensemble an ensemble with total energy to the mean energy of the canonical ensemble to within some precision now the total energy to a precision is equivalent to the value of ln to within our of the typical set tn was precisely that it consisted of all elements that have a value of log p very close to the mean value of log p under the canonical ensemble hx. thus the microcanonical ensemble is equivalent to a uniform distribution over the typical set of the canonical ensemble. our proof of the asymptotic equipartition principle thus proves for the case of a system whose energy is separable into a sum of independent terms that the boltzmann entropy of the microcanonical ensemble is very close large n to the gibbs entropy of the canonical ensemble if the energy of the microcanonical ensemble is constrained to equal the mean energy of the canonical ensemble. solution to exercise the normalizing constant of the cauchy distribution p z is z dx the mean and variance of this distribution are both distribution is symmetrical about zero but this does not imply that its mean is zero. the mean is the value of a divergent integral. the sum z where and both have cauchy distributions has probability density given by the convolution p z which after a considerable labour using standard methods gives p which we recognize as a cauchy distribution with width parameter the original distribution has width parameter this implies that the mean of the two points has a cauchy distribution with width parameter generalizing the mean of n samples from a cauchy distribution is cauchy-distributed with the same parameters as the individual samples. the probability distribution of the mean does not become narrower as the central-limit theorem does not apply to the cauchy distribution be cause it does not have a variance. an alternative neat method for getting to equation makes use of the fourier transform of the cauchy distribution which is a biexponential convolution in real space corresponds to multiplication in fourier space so the fourier transform of z is simply reversing the transform we obtain equation copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solution to exercise the function f has inverse function note gy log gy log y i obtained a tentative graph of f by plotting gy with y along the vertical axis and gy along the horizontal axis. the resulting graph suggests that f is single valued for x and looks surprisingly well-behaved and ordinary for x f is two-valued. f is equal both to and for x is about f is however it might be argued that this approach to sketching f is only partly valid if we f as the limit of the sequence of functions x xx xxx this sequence does not have a limit for x on account of a pitchfork bifurcation at x and for x the sequence s limit is single-valued the lower of the two values sketched in the figure f xxx at three scales. x x shown copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter in the last chapter we saw a proof of the fundamental status of the entropy as a measure of average information content. we a data compression scheme using length block codes and proved that as n increases it is possible to encode n i.i.d. variables x xn into a block of n bits with vanishing probability of error whereas if we attempt to encode x n into n bits the probability of error is virtually we thus the possibility of data compression but the block coding in the proof did not give a practical algorithm. in this chapter and the next we study practical data compression algorithms. whereas the last chapter s compression scheme used large blocks of size and was lossy in the next chapter we discuss variable-length compression schemes that are practical for small block sizes and that are not lossy. imagine a rubber glove with water. if we compress two of the glove some other part of the glove has to expand because the total volume of water is constant. is essentially incompressible. similarly when we shorten the codewords for some outcomes there must be other codewords that get longer if the scheme is not lossy. in this chapter we will discover the information-theoretic equivalent of water volume. before reading chapter you should have worked on exercise we will use the following notation for intervals x means that x and x x means that x and x copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes in this chapter we discuss variable-length symbol codes which encode one source symbol at a time instead of encoding huge strings of n source symbols. these codes are lossless unlike the last chapter s block codes they are guaranteed to compress and decompress without any errors but there is a chance that the codes may sometimes produce encoded strings longer than the original source string. the idea is that we can achieve compression on average by assigning shorter encodings to the more probable outcomes and longer encodings to the less probable. the key issues are what are the implications if a symbol code is lossless? if some codewords are shortened by how much do other codewords have to be lengthened? making compression practical. how can we ensure that a symbol code is easy to decode? optimal symbol codes. how should we assign codelengths to achieve the best compression and what is the best achievable compression? we again verify the fundamental status of the shannon information content and the entropy proving source coding theorem codes. there exists a variable-length encoding c of an ensemble x such that the average length of an encoded symbol lc x lc x hx the average length is equal to the entropy hx only if the codelength for each outcome is equal to its shannon information content. we will also a constructive procedure the coding algorithm that produces optimal symbol codes. notation for alphabets. an denotes the set of ordered n of elements from the set a i.e. all strings of length n the symbol a will denote the set of all strings of length composed of elements from the set a. example example copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes symbol codes a symbol code c for an ensemble x is a mapping from the range of x ax aig to cx will denote the codeword corresponding to x and lx will denote its length with li lai. the extended code c is a mapping from a concatenation without punctuation of the corresponding codewords x to obtained by xn cxn term mapping here is a synonym for function example a symbol code for the ensemble x by ax f a b c d g px f g is shown in the margin. using the extended code we may encode acdbac as cacdbac ai cai a b c d li there are basic requirements for a useful symbol code. first any encoded string must have a unique decoding. second the symbol code must be easy to decode. and third the code should achieve as much compression as possible. any encoded string must have a unique decoding a code cx is uniquely decodeable if under the extended code c no two distinct strings have the same encoding i.e. x y a x x y cx cy the code above is an example of a uniquely decodeable code. the symbol code must be easy to decode a symbol code is easiest to decode if it is possible to identify the end of a codeword as soon as it arrives which means that no codeword can be a of another codeword. word c is a of another word d if there exists a tail string t such that the concatenation ct is identical to d. for example is a of and so is we will show later that we don t lose any performance if we constrain our symbol code to be a code. a symbol code is called a code if no codeword is a of any other codeword. a code is also known as an instantaneous or self-punctuating code because an encoded string can be decoded from left to right without looking ahead to subsequent codewords. the end of a codeword is immediately recognizable. a code is uniquely decodeable. codes are also known as codes or condition codes codes correspond to trees as illustrated in the margin of the next page. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes example the code is a code because is not a of nor is a of example let this code is not a code because is a of example the code is a code. example the code is a code. exercise is uniquely decodeable? example consider exercise and any weighing strategy that the odd ball and whether it is heavy or light can be viewed as assigning a ternary code to each of the possible states. this code is a code. the code should achieve as much compression as possible the expected length lc x of a symbol code c for ensemble x is lc x p lx we may also write this quantity as where i jaxj. example let lc x pili i and ax f a b c d g px f and consider the code the entropy of x is bits and the expected length x of this code is also bits. the sequence of symbols x is encoded as cx is a code and is therefore uniquely decodeable. notice that the codeword lengths satisfy li or equivalently pi example consider the length code for the same ensemble x the expected length x is bits. example consider the expected length x is bits which is less than hx. but the code is not uniquely decodeable. the sequence x encodes as which can also be decoded as example consider the code the expected length x of this code is bits. the sequence of symbols x is encoded as cx is a code? it is not because ca is a of both cb and cc. codes can be represented on binary trees. complete codes correspond to binary trees with no unused branches. is an incomplete code. pi ai cai a b c d hpi li a b c d pi ai cai a b c d hpi li copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes is uniquely decodeable? this is not so obvious. if you think that it might not be uniquely decodeable try to prove it so by a pair of strings x and y that have the same encoding. of unique decodeability is given in equation certainly isn t easy to decode. when we receive it is possible that x could start aa ab or ac once we have received the second symbol is still ambiguous as x could be abd. or acd. but eventually a unique decoding crystallizes once the next appears in the encoded stream. is in fact uniquely decodeable. comparing with the code we see that the codewords of are the reverse of s. that is uniquely decodeable proves that is too since any string from is identical to a string from read backwards. what limit is imposed by unique decodeability? we now ask given a list of positive integers flig does there exist a uniquely decodeable code with those integers as its codeword lengths? at this stage we ignore the probabilities of the symbols once we understand unique decodeability better we ll reintroduce the probabilities and discuss how to make an optimal uniquely decodeable symbol code. in the examples above we have observed that if we take a code such as and shorten one of its codewords for example then we can retain unique decodeability only if we lengthen other codewords. thus there seems to be a constrained budget that we can spend on codewords with shorter codewords being more expensive. let us explore the nature of this budget. if we build a code purely from codewords of length l equal to three how many codewords can we have and retain unique decodeability? the answer is once we have chosen all eight of these codewords is there any way we could add to the code another codeword of some other length and retain unique decodeability? it would seem not. what if we make a code that includes a length-one codeword with the other codewords being of length three? how many length-three codewords can we have? if we restrict attention to codes then we can have only four codewords of length three namely what about other codes? is there any other way of choosing codewords of length that can give more codewords? intuitively we think this unlikely. a codeword of length appears to have a cost that is times smaller than a codeword of length let s a total budget of size which we can spend on codewords. if we set the cost of a codeword whose length is l to then we have a pricing system that the examples discussed above. codewords of length cost each codewords of length cost each. we can spend our budget on any codewords. if we go over our budget then the code will certainly not be uniquely decodeable. if on the other hand xi then the code may be uniquely decodeable. this inequality is the kraft inequality. kraft inequality. for any uniquely decodeable code cx over the binary copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. what limit is imposed by unique decodeability? alphabet the codeword lengths must satisfy i where i jaxj. completeness. if a uniquely decodeable code the kraft inequality with equality then it is called a complete code. we want codes that are uniquely decodeable codes are uniquely decodeable and are easy to decode. so life would be simpler for us if we could restrict attention to codes. fortunately for any source there is an optimal symbol code that is also a code. kraft inequality and codes. given a set of codeword lengths that satisfy the kraft inequality there exists a uniquely decodeable code with these codeword lengths. the kraft inequality might be more accurately referred to as the kraft mcmillan inequality kraft proved that if the inequality is then a code exists with the given lengths. mcmillan proved the converse that unique decodeability implies that the inequality holds. proof of the kraft inequality. s consider the quantity lin sn i i i xin the quantity in the exponent lin is the length of the encoding of the string x ain for every string x of length n there is one term in the above sum. introduce an array al that counts how many strings x have encoded length l. then lmin mini li and lmax maxi li sn n lmax xln lmin now assume c is uniquely decodeable so that for all x y cx cy. concentrate on the x that have encoded length l. there are a total of distinct bit strings of length l so it must be the case that al so sn n lmax xln lmin n lmax xln lmin n lmax thus sn lmaxn for all n now if s were greater than then as n increases sn would be an exponentially growing function and for large enough n an exponential always exceeds a polynomial such as lmaxn but our result lmaxn is true for any n therefore s exercise prove the result stated above that for any set of codeword lengths flig satisfying the kraft inequality there is a code having those lengths. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes t e g d u b e d o c l o b m y s l a t o t e h t figure the symbol coding budget. the cost of each codeword length l is indicated by the size of the box it is written in. the total budget available when making a uniquely decodeable code is you can think of this diagram as showing a codeword supermarket with the codewords arranged in aisles by their length and the cost of each codeword indicated by the size of its box on the shelf. if the cost of the codewords that you take exceeds the budget then your code will not be uniquely decodeable. figure selections of codewords made by codes and from section copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. what s the most compression that we can hope for? a pictorial view of the kraft inequality may help you solve this exercise. imagine that we are choosing the codewords to make a symbol code. we can draw the set of all candidate codewords in a supermarket that displays the cost of the codeword by the area of a box the total budget available the on the right-hand side of the kraft inequality is shown at one side. some of the codes discussed in section are illustrated in notice that the codes that are codes and have the property that to the right of any selected codeword there are no other selected codewords because codes correspond to trees. notice that a complete code corresponds to a complete tree having no unused branches. we are now ready to put back the symbols probabilities fpig. given a set of symbol probabilities english language probabilities of for example how do we make the best symbol code one with the smallest possible expected length lc x? and what is that smallest possible expected length? it s not obvious how to assign the codeword lengths. if we give short codewords to the more probable symbols then the expected length might be reduced on the other hand shortening some codewords necessarily causes others to lengthen by the kraft inequality. what s the most compression that we can hope for? we wish to minimize the expected length of a code lc x xi xi hx pi log log z pili pi log log z lc x xi pili as you might have guessed the entropy appears as the lower bound on the expected length of a code. lower bound on expected length. the expected length lc x of a uniquely decodeable code is bounded below by hx. proof. we the implicit probabilities qi where z so that li log log z. we then use gibbs inequality pi pi log pi pi log with equality if qi pi and the kraft inequality z the equality lc x hx is achieved only if the kraft equality z is and if the codelengths satisfy li this is an important result so let s say it again optimal source codelengths. the expected length is minimized and is equal to hx only if the codelengths are equal to the shannon information contents li implicit probabilities by codelengths. conversely any choice of codelengths flig implicitly a probability distribution fqig qi for which those codelengths would be the optimal codelengths. if the code is complete then z and the implicit probabilities are given by qi copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes how much can we compress? so we can t compress below the entropy. how close can we expect to get to the entropy? theorem source coding theorem for symbol codes. for an ensemble x there exists a code c with expected length satisfying hx lc x hx proof. we set the codelengths to integers slightly larger than the optimum lengths li where denotes the smallest integer greater than or equal to are not asserting that the optimal code necessarily uses these lengths we are simply choosing these lengths because we can use them to prove the theorem. we check that there is a code with these lengths by that the kraft inequality is xi pi then we lc x hx the cost of using the wrong codelengths if we use a code whose lengths are not equal to the optimal codelengths the average message length will be larger than the entropy. if the true probabilities are fpig and we use a complete code with lengths li we can view those lengths as implicit probabilities qi continuing from equation the average length is lc x hx pi log piqi i.e. it exceeds the entropy by the relative entropy dklpjjq on optimal source coding with symbol codes coding given a set of probabilities p how can we design an optimal code? for example what is the best symbol code for the english language ensemble shown in when we say optimal let s assume our aim is to minimize the expected length lc x. how not to do it one might try to roughly split the set ax in two and continue bisecting the subsets so as to a binary tree from the root. this construction has the right spirit as in the weighing problem but it is not necessarily optimal it achieves lc x hx x a b c d e f g h i j k l m n o p q r s t u v w x y z p figure an ensemble in need of a symbol code. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. optimal source coding with symbol codes coding algorithm coding algorithm. ai pi hpi a b c d e li cai table code created by the algorithm. the coding algorithm we now present a beautifully simple algorithm for an optimal code. the trick is to construct the code backwards starting from the tails of the codewords we build the binary tree from its leaves. take the two least probable symbols in the alphabet. these two symbols will be given the longest codewords which will have equal length and only in the last digit. combine these two symbols into a single symbol and repeat. since each step reduces the size of the alphabet by one this algorithm will have assigned strings to all the symbols after jaxj steps. example let ax a g and px g. c d b e x a b c d e step step step step the codewords are then obtained by concatenating the binary digits in reverse order c the codelengths selected by the algorithm of table are in some cases longer and in some cases shorter than the ideal codelengths the shannon the expected length of the information contents code is l bits whereas the entropy is h bits. if at any point there is more than one way of selecting the two least probable symbols then the choice may be made in any manner the expected length of the code will not depend on the choice. exercise prove that there is no better symbol code for a source than the code. example we can make a code for the probability distribution over the alphabet introduced in the result is shown in this code has an expected length of bits the entropy of the ensemble is bits. observe the disparities between the assigned codelengths and the ideal codelengths constructing a binary tree top-down is suboptimal in previous chapters we studied weighing problems in which we built ternary or binary trees. we noticed that balanced trees ones in which at every step the two possible outcomes were as close as possible to equiprobable appeared to describe the most experiments. this gave an intuitive motivation for entropy as a measure of information content. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes figure code for the english language ensemble statistics. ai pi greedy a b c d e f g table a greedily-constructed code compared with the code. ai pi pi a b c d e f g h i j k l m n o p q r s t u v w x y z li cai a n s i o e t b g c d h k x y u j z q v w m f p r l it is not the case however that optimal codes can always be constructed by a greedy top-down method in which the alphabet is successively divided into subsets that are as near as possible to equiprobable. example find the optimal binary symbol code for the ensemble g g ax f a px f g d b c e f notice that a greedy top-down method can split this set into two subsets fa b c dg and fe f gg which both have probability and that fa b c dg can be divided into subsets fa bg and fc dg which have probability so a greedy top-down method gives the code shown in the third column of table which has expected length the coding algorithm yields the code shown in the fourth column which has expected length disadvantages of the code the algorithm produces an optimal symbol code for an ensemble but this is not the end of the story. both the word ensemble and the phrase symbol code need careful attention. changing ensemble if we wish to communicate a sequence of outcomes from one unchanging ensemble then a code may be convenient. but often the appropriate copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. disadvantages of the code ensemble changes. if for example we are compressing text then the symbol frequencies will vary with context in english the letter u is much more probable after a q than after an e and furthermore our knowledge of these context-dependent symbol frequencies will also change as we learn the statistical properties of the text source. codes do not handle changing ensemble probabilities with any elegance. one brute-force approach would be to recompute the code every time the probability over symbols changes. another attitude is to deny the option of adaptation and instead run through the entire in advance and compute a good probability distribution which will then remain throughout transmission. the code itself must also be communicated in this scenario. such a technique is not only cumbersome and restrictive it is also suboptimal since the initial message specifying the code and the document itself are partially redundant. this technique therefore wastes bits. the extra bit an equally serious problem with codes is the innocuous-looking extra bit relative to the ideal average length of hx a code achieves a length that hx lc x as proved in theorem a code thus incurs an overhead of between and bits per symbol. if hx were large then this overhead would be an unimportant fractional increase. but for many applications the entropy may be as low as one bit per symbol or even smaller so the overhead lc x hx may domiin some contexts long nate the encoded length. consider english text strings of characters may be highly predictable. for example in the context strings_of_ch one might predict the next nine symbols to be aracters_ with a probability of each. a traditional code would be obliged to use at least one bit per character making a total cost of nine bits where virtually no information is being conveyed bits in total to be precise. the entropy of english given a good model is about one bit per character so a code is likely to be highly a traditional patch-up of codes uses them to compress blocks of symbols for example the extended sources x n we discussed in chapter the overhead per block is at most bit so the overhead per symbol is at most bits. for large blocks the problem of the extra bit may be removed but only at the expenses of losing the elegant instantaneous decodeability of simple coding and having to compute the probabilities of all relevant strings and build the associated tree. one will end up explicitly computing the probabilities and codes for a huge number of strings most of which will never actually occur. exercise beyond symbol codes codes therefore although widely trumpeted as optimal have many defects for practical purposes. they are optimal symbol codes but for practical purposes we don t want a symbol code. the defects of codes are by arithmetic coding which dispenses with the restriction that each symbol must translate into an integer number of bits. arithmetic coding is the main topic of the next chapter. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes summary kraft inequality. if a code is uniquely decodeable its lengths must satisfy xi for any lengths satisfying the kraft inequality there exists a code with those lengths. optimal source codelengths for an ensemble are equal to the shannon information contents li pi and conversely any choice of codelengths implicit probabilities qi z the relative entropy dklpjjq measures how many bits per symbol are wasted by using a code whose implicit probabilities are q when the ensemble s true probability distribution is p. source coding theorem for symbol codes. for an ensemble x there ex ists a code whose expected length hx lc x hx the coding algorithm generates an optimal symbol code iteratively. at each iteration the two least probable symbols are combined. exercises exercise is the code uniquely decodeable? exercise is the ternary code uniquely decodeable? exercise make codes for x x and x where ax and px compute their expected lengths and compare them with the entropies hx hx and hx repeat this exercise for x and x where px exercise find a probability distribution such that there are two optimal codes that assign lengths flig to the four symbols. exercise of exercise assume that the four probabilities are ordered such that let q be the set of all probability vectors p such that there are two optimal codes with lengths. give a complete description of q. find three probability vectors which are the convex hull of q i.e. such that any p q can be written as p where are positive. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises exercise write a short essay discussing how to play the game of twenty questions optimally. twenty questions one player thinks of an object and the other player has to guess the object using as few binary questions as possible preferably fewer than twenty. exercise show that if each probability pi is equal to an integer power of then there exists a source code whose expected length equals the entropy. exercise make ensembles for which the between the entropy and the expected length of the code is as big as possible. exercise a source x has an alphabet of eleven characters fa b c d e f g h i j kg all of which have equal probability find an optimal uniquely decodeable symbol code for this source. how much greater is the expected length of this optimal code than the entropy of x? exercise consider the optimal symbol code for an ensemble x with alphabet size i from which all symbols have identical probability p i is not a power of show that the fraction f of the i symbols that are assigned codelengths equal to l ie i f and that the expected length of the optimal symbol code is l l f by the excess length l hx with respect to i show that the excess length is bounded by lnln ln ln exercise consider a sparse binary source with px dis cuss how codes could be used to compress this source estimate how many codewords your proposed solutions require. exercise american carried the following puzzle in the poisoned glass. mathematicians are curious birds the police commissioner said to his wife. you see we had all those partly glasses lined up in rows on a table in the hotel kitchen. only one contained poison and we wanted to know which one before searching that glass for our lab could test the liquid in each glass but the tests take time and money so we wanted to make as few of them as possible by simultaneously testing mixtures of small samples from groups of glasses. the university sent over a copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes mathematics professor to help us. he counted the glasses smiled and said any glass you want commissioner. we ll test it won t that waste a test? i asked. he said s part of the best procedure. we can test one glass it doesn t matter which one. how many glasses were there to start with? the commissioner s wife asked. i don t remember. somewhere between and what was the exact number of glasses? solve this puzzle and then explain why the professor was in fact wrong and the commissioner was right. what is in fact the optimal procedure for identifying the one poisoned glass? what is the expected waste relative to this optimum if one followed the professor s strategy? explain the relationship to symbol coding. exercise assume that a sequence of symbols from the ensemble x introduced at the beginning of this chapter is compressed using the code imagine picking one bit at random from the binary encoded sequence c what is the probability that this bit is a exercise how should the binary encoding scheme be to make optimal symbol codes in an encoding alphabet with q symbols? known as radix q mixture codes it is a tempting idea to construct a metacode from several symbol codes that assign codewords to the alternative symbols then switch from one code to another choosing whichever assigns the shortest codeword to the current symbol. clearly we cannot do this for free. if one wishes to choose between two codes then it is necessary to lengthen the message in a way that indicates which of the two codes is being used. if we indicate this choice by a single leading bit it will be found that the resulting code is suboptimal because it is incomplete is it fails the kraft equality. exercise prove that this metacode is incomplete and explain why this combined code is suboptimal. solutions solution to exercise yes is uniquely decodeable even though it is not a code because no two strings can map onto the same string only the codeword contains the symbol solution to exercise we wish to prove that for any set of codeword lengths flig satisfying the kraft inequality there is a code having those lengths. this is readily proved by thinking of the codewords illustrated in as being in a codeword supermarket with size indicating cost. we imagine purchasing codewords one at a time starting from the shortest codewords the biggest purchases using the budget shown at the right of we start at one side of the codeword supermarket say the pi ai cai a b c d hpi li copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. figure the codeword supermarket and the symbol coding budget. the cost of each codeword length l is indicated by the size of the box it is written in. the total budget available when making a uniquely decodeable code is figure proof that coding makes an optimal symbol code. we assume that the rival code which is said to be optimal assigns unequal length codewords to the two symbols with smallest probability a and b. by interchanging codewords a and c of the rival code where c is a symbol with rival codelength as long as b s we can make a code better than the rival code. this shows that the rival code was not optimal. solutions t e g d u b e d o c l o b m y s l a t o t e h t symbol probability a b c pa pb pc codewords cha chb chc rival code s rival codewords code cra crb crc crc crb cra top and purchase the codeword of the required length. we advance down the supermarket a distance and purchase the next codeword of the next required length and so forth. because the codeword lengths are getting longer and the corresponding intervals are getting shorter we can always buy an adjacent codeword to the latest purchase so there is no wasting of the budget. thus at the ith codeword we have advanced a distance pi down the supermarket ifp we will have purchased all the codewords without running out of budget. solution to exercise the proof that coding is optimal depends on proving that the key step in the algorithm the decision to give the two symbols with smallest probability equal encoded lengths cannot lead to a larger expected length than any other code. we can prove this by contradiction. assume that the two symbols with smallest probability called a and b to which the algorithm would assign equal length codewords do not have equal lengths in any optimal symbol code. the optimal symbol code is some other rival code in which these two codewords have unequal lengths la and lb with la lb. without loss of generality we can assume that this other code is a complete code because any codelengths of a uniquely decodeable code can be realized by a code. in this rival code there must be some other symbol c whose probability pc is greater than pa and whose length in the rival code is greater than or equal to lb because the code for b must have an adjacent codeword of equal or greater length a complete code never has a solo codeword of the maximum length. consider exchanging the codewords of a and c so that a is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes encoded with the longer codeword that was c s and c which is more probable than a gets the shorter codeword. clearly this reduces the expected length of the code. the change in expected length is pclc la. thus we have contradicted the assumption that the rival code is optimal. therefore it is valid to give the two symbols with smallest probability equal encoded lengths. coding produces optimal symbol codes. solution to exercise a code for x where ax and px is this code has lc x whereas the entropy hx is a code for x is this has expected length lc x whereas the entropy hx is a code for x maps the sixteen source strings to the following codelengths this has expected length lc x whereas the entropy hx is when px the code for x has lengths the expected length is bits and the entropy is bits. a code for x is shown in table the expected length is bits and the entropy is bits. solution to exercise the set of probabilities gives rise to two optimal sets of codelengths because at the second step of the coding algorithm we can choose any of the three possible pairings. we may either put them in a constant length code or the code both codes have expected length another solution is and a third is solution to exercise let pmax be the largest probability in pi. the between the expected length l and the entropy h can be no bigger than maxpmax see exercises to understand where the curious comes from. solution to exercise length entropy solution to exercise there are two ways to answer this problem correctly and one popular way to answer it incorrectly. let s give the incorrect answer erroneous answer. can pick a random bit by picking a random source symbol xi with probability pi then picking a random bit from cxi. if we fi to be the fraction of the bits of cxi that are we p is xi pifi ai pi li cai table code for x when column shows the assigned codelengths and column the codewords. some strings whose probabilities are identical e.g. the fourth and receive codelengths. ai cai a b c d pi li copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions this answer is wrong because it falls for the bus-stop fallacy which was introduced in exercise if buses arrive at random and we are interested in the average time from one bus until the next we must distinguish two possible averages the average time from a randomly chosen bus until the next the average time between the bus you just missed and the next bus. the second average is twice as big as the because by waiting for a bus at a random time you bias your selection of a bus in favour of buses that follow a large gap. you re unlikely to catch a bus that comes seconds after a preceding bus! similarly the symbols c and d get encoded into longer-length binary strings than a so when we pick a bit from the compressed string at random we are more likely to land in a bit belonging to a c or a d than would be given by the probabilities pi in the expectation all the probabilities need to be scaled up by li and renormalized. correct answer in the same style. every time symbol xi is encoded li bits are added to the binary string of which fili are the expected number of added per symbol is pifili xi and the expected total number of bits added per symbol is pili xi so the fraction of in the transmitted string is p is pi pifili pi pili for a general symbol code and a general ensemble the expectation is the correct answer. but in this case we can use a more powerful argument. information-theoretic answer. the encoded string c is the output of an optimal compressor that compresses samples from x down to an expected length of hx bits. we can t expect to compress this data any further. but if the probability p is were not equal to then it would be possible to compress the binary string further a block compression code say. therefore p is must be equal to indeed the probability of any sequence of l bits in the compressed stream taking on any particular value must be the output of a perfect compressor is always perfectly random bits. to put it another way if the probability p is were not equal to then the information content per bit of the compressed string would be at most which would be less than but this contradicts the fact that we can recover the original data from c so the information content per bit of the compressed string must be hxlc x solution to exercise the general coding algorithm for an encoding alphabet with q symbols has one from the binary case. the process of combining q symbols into symbol reduces the number of symbols by so if we start with a symbols we ll only end up with a copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. symbol codes complete q-ary tree if a mod is equal to otherwise we know that whatever code we make it must be an incomplete tree with a number of missing leaves equal modulo to a mod for example if a ternary tree is built for eight symbols then there will unavoidably be one missing leaf in the tree. the optimal q-ary code is made by putting these extra leaves in the longest branch of the tree. this can be achieved by adding the appropriate number of symbols to the original source symbol set all of these extra symbols having probability zero. the total number of leaves is then equal to for some integer r. the symbols are then repeatedly combined by taking the q symbols with smallest probability and replacing them by a single symbol as in the binary coding algorithm. solution to exercise we wish to show that a greedy metacode which picks the code which gives the shortest encoding is actually suboptimal because it violates the kraft inequality. we ll assume that each symbol x is assigned lengths lkx by each of the candidate codes ck. let us assume there are k alternative codes and that we can encode which code is being used with a header of length log k bits. then the metacode assigns lengths that are given by k min k lkx we compute the kraft sum mink lkx s k xx let s divide the set ax into non-overlapping subsets fakgk ak contains all the symbols x that the metacode sends via code k. then such that subset now if one sub-code k the kraft equality must be the case that then it with equality only if all the symbols x are in ak which would mean that we are only using one of the k codes. so s k xk s k k with equality only if equation is an equality for all codes k. but it s impossible for all the symbols to be in all the non-overlapping subsets fakgk so we can t have equality holding for all k. so s another way of seeing that a mixture code is suboptimal is to consider the binary tree that it think of the special case of two codes. the bit we send which code we are using. now in a complete code any subsequent binary string is a valid string. but once we know that we are using say code a we know that what follows can only be a codeword corresponding to a symbol x whose encoding is shorter under code a than code b. so some strings are invalid continuations and the mixture code is incomplete and suboptimal. for further discussion of this issue and its relationship to probabilistic modelling read about bits back coding in section and in frey copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter before reading chapter you should have read the previous chapter and worked on most of the exercises in it. we ll also make use of some bayesian modelling ideas that arrived in the vicinity of exercise copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes in this chapter we discuss two data compression schemes. arithmetic coding is a beautiful method that goes hand in hand with the philosophy that compression of data from a source entails probabilistic modelling of that source. as of the best compression methods for text use arithmetic coding and several state-of-the-art image compression systems use it too. lempelziv coding is a universal method designed under the philosophy that we would like a single compression algorithm that will do a reasonable job for any source. in fact for many real life sources this algorithm s universal properties hold only in the limit of unfeasibly large amounts of data but all the same lempelziv compression is widely used and often the guessing game as a motivation for these two compression methods consider the redundancy in a typical english text such have redundancy at several levels for example they contain the ascii characters with non-equal frequency certain consecutive pairs of letters are more probable than others and entire words can be predicted given the context and a semantic understanding of the text. to illustrate the redundancy of english and a curious way in which it could be compressed we can imagine a guessing game in which an english speaker repeatedly attempts to predict the next character in a text for simplicity let us assume that the allowed alphabet consists of the upper case letters abc... z and a space the game involves asking the subject to guess the next character repeatedly the only feedback being whether the guess is correct or not until the character is correctly guessed. after a correct guess we note the number of guesses that were made when the character was and ask the subject to guess the next character in the same way. one sentence gave the following result when a human was asked to guess a sentence. the numbers of guesses are listed below each character. t h e r e i s n o r e v e r s e o n a m o t o r c y c l e notice that in many cases the next letter is guessed immediately in one guess. in other cases particularly at the start of syllables more guesses are needed. what do this game and these results us? first they demonstrate the redundancy of english from the point of view of an english speaker. second this game might be used in a data compression scheme as follows. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. arithmetic codes the string of numbers listed above was obtained by presenting the text to the subject. the maximum number of guesses that the subject will make for a given letter is twenty-seven so what the subject is doing for us is performing a time-varying mapping of the twenty-seven letters fa b c onto the twenty-seven numbers which we can view as symbols in a new alphabet. the total number of symbols has not been reduced but since he uses some of these symbols much more frequently than others for example and it should be easy to compress this new string of symbols. how would the uncompression of the sequence of numbers work? at uncompression time we do not have the original string there. we have only the encoded sequence. imagine that our subject has an absolutely identical twin who also plays the guessing game with us as if we knew the source text. if we stop him whenever he has made a number of guesses equal to the given number then he will have just guessed the correct letter and we can then say yes that s right and move to the next character. alternatively if the identical twin is not available we could design a compression system with the help of just one human as follows. we choose a window length l that is a number of characters of context to show the human. for every one of the possible strings of length l we ask them what would you predict is the next character? and if that prediction were wrong what would your next guesses be? after tabulating their answers to these questions we could use two copies of these enormous tables at the encoder and the decoder in place of the two human twins. such a language model is called an lth order markov model. these systems are clearly unrealistic for practical compression but they illustrate several principles that we will make use of now. arithmetic codes when we discussed variable-length symbol codes and the optimal algorithm for constructing them we concluded by pointing out two practical and theoretical problems with codes these defects are by arithmetic codes which were invented by elias by rissanen and by pasco and subsequently made practical by witten et al. in an arithmetic code the probabilistic modelling is clearly separated from the encoding operation. the system is rather similar to the guessing game. the human predictor is replaced by a probabilistic model of the source. as each symbol is produced by the source the probabilistic model supplies a predictive distribution over all possible values of the next symbol that is a list of positive numbers fpig that sum to one. if we choose to model the source as producing i.i.d. symbols with some known distribution then the predictive distribution is the same every time but arithmetic coding can with equal ease handle complex adaptive models that produce context-dependent predictive distributions. the predictive model is usually implemented in a computer program. the encoder makes use of the model s predictions to create a binary string. the decoder makes use of an identical twin of the model as in the guessing game to interpret the binary string. let the source alphabet be ax aig and let the ith symbol ai have the special meaning end of transmission the source spits out a sequence xn the source does not necessarily produce i.i.d. symbols. we will assume that a computer program is provided to the encoder that assigns a copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes predictive probability distribution over ai given the sequence that has occurred thus far p ai j the receiver has an identical program that produces the same predictive probability distribution p ai j figure binary strings real intervals within the real line we encountered a picture like this when we discussed the symbol-code supermarket in chapter concepts for understanding arithmetic coding notation for intervals. the interval is all numbers between and including but not a binary transmission an interval within the real line from to for example the string is interpreted as a binary real number which corresponds to the interval in binary i.e. the interval in base ten. the longer string corresponds to a smaller interval because has the string as a the new interval is a sub-interval of the interval a one-megabyte binary bits is thus viewed as specifying a number between and to a precision of about two million decimal places two million decimal digits because each byte translates into a little more than two decimal digits. now we can also divide the real line into i intervals of lengths equal to the probabilities p ai as shown in p p p p p figure a probabilistic model real intervals within the real line we may then take each interval ai and subdivide it into intervals deis proportional to indeed the length of the interval aiaj will be precisely noted aiai such that the length of aiaj p aj j ai. the joint probability p ai aj p aip aj j ai iterating this procedure the interval can be divided into a sequence of intervals corresponding to all possible length strings xn such that the length of an interval is equal to the probability of the string given our model. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. algorithm arithmetic coding. iterative procedure to the interval v for the string xn arithmetic codes u v p v u for n to n compute the cumulative probabilities qn and rn v u prnxn j u u pqnxn j p v u formulae describing arithmetic coding the process depicted in can be written explicitly as follows. the intervals are in terms of the lower and upper cumulative probabilities qnai j rnai j i p j p j as the nth symbol arrives we subdivide the interval at the points by qn and rn. for example starting with the symbol the intervals and ai are p p p and ai p algorithm describes the general procedure. to encode a string xn we locate the interval corresponding to xn and send a binary string whose interval lies within that interval. this encoding can be performed on the as we now illustrate. example compressing the tosses of a bent coin imagine that we watch as a bent coin is tossed some number of times example and section the two outcomes when the coin is tossed are denoted a and b. a third possibility is that the experiment is halted an event denoted by the end of symbol because the coin is bent we expect that the probabilities of the outcomes a and b are not equal though beforehand we don t know which is the more probable outcome. encoding let the source string be we pass along the string one symbol at a time and use our model to compute the probability distribution of the next copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes symbol given the string thus far. let these probabilities be context thus far probability of next symbol b bb bbb bbba p p b p bb p bbb p bbba p p b p bb p bbb p bbba p p b p bb p bbb p bbba figure shows the corresponding intervals. the interval b is the middle of the interval bb is the middle of b and so forth. b b b b a b ba bba bbba bb bbb bbbb figure illustration of the arithmetic coding process as the sequence is transmitted. bbbaa bbba bbbab c c cco when the symbol b is observed the encoder knows that the encoded string will start or but does not know which. the encoder writes nothing for the time being and examines the next symbol which is b the interval bb lies wholly within interval so the encoder can write the bit the third symbol b narrows down the interval a little but not quite enough for it to lie wholly within interval only when the next a is read from the source can we transmit some more bits. interval bbba lies wholly within the interval so the encoder adds to the it has written. finally when the arrives we need a procedure for terminating the encoding. magnifying the interval right we note that the marked interval is wholly contained by so the encoding can be completed by appending copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. arithmetic codes exercise show that the overhead required to terminate a message is never more than bits relative to the ideal message length given the probabilistic model h hxjh this is an important result. arithmetic coding is very nearly optimal. the message length is always within two bits of the shannon information content of the entire source string so the expected message length is within two bits of the entropy of the entire message. decoding the decoder receives the string and passes along it one symbol at a time. first the probabilities p p p are computed using the identical program that the encoder used and the intervals a b and are deduced. once the two bits have been examined it is certain that the original string must have been started with a b since the interval lies wholly within interval b the decoder can then use the model to compute p b p b p b and deduce the boundaries of the intervals ba bb and continuing we decode the second b once we reach the third b once we reach and so forth with the unambiguous of once the whole binary string has been read. with the convention that denotes the end of the message the decoder knows to stop decoding. transmission of multiple how might one use arithmetic coding to communicate several distinct over the binary channel? once the character has been transmitted we imagine that the decoder is reset into its initial state. there is no transfer of the learnt statistics of the to the second if however we did believe that there is a relationship among the that we are going to compress we could our alphabet introducing a second character that marks the end of the but instructs the encoder and decoder to continue using the same probabilistic model. the big picture notice that to communicate a string of n letters both the encoder and the decoder needed to compute only njaj conditional probabilities the probabilities of each possible letter in each context actually encountered just as in the guessing game. this cost can be contrasted with the alternative of using a code with a large block size order to reduce the possible onebit-per-symbol overhead discussed in section where all block sequences that could occur must be considered and their probabilities evaluated. notice how arithmetic coding is it can be used with any source alphabet and any encoded alphabet. the size of the source alphabet and the encoded alphabet can change with time. arithmetic coding can be used with any probability distribution which can change utterly from context to context. furthermore if we would like the symbols of the encoding alphabet and to be used with unequal frequency that can easily be arranged by subdividing the right-hand interval in proportion to the required frequencies. how the probabilistic model might make its predictions the technique of arithmetic coding does not force one to produce the predictive probability in any particular way but the predictive distributions might copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes figure illustration of the intervals by a simple bayesian probabilistic model. the size of an intervals is proportional to the probability of the string. this model anticipates that the source is likely to be biased towards one of a and b so sequences having lots of as or lots of bs have larger intervals than sequences of the same length that are as and bs. aa ab ba aaa aab aba abb baa bab bba aaaa aaab aaba aabb abaa abab abba abbb baaa baab baba babb bbaa bbab bbba bb bbb bbbb a b naturally be produced by a bayesian model. figure was generated using a simple model that always assigns a probability of to and assigns the remaining to a and b divided in proportion to probabilities given by laplace s rule plaj fa fa fb where is the number of times that a has occurred so far and fb is the count of bs. these predictions correspond to a simple bayesian model that expects and adapts to a non-equal frequency of use of the source symbols a and b within a figure displays the intervals corresponding to a number of strings of length up to note that if the string so far has contained a large number of bs then the probability of b relative to a is increased and conversely if many as occur then as are made more probable. larger intervals remember require fewer bits to encode. details of the bayesian model having emphasized that any model could be used arithmetic coding is not wedded to any particular set of probabilities let me explain the simple adaptive copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. arithmetic codes probabilistic model used in the preceding example we encountered this model in exercise assumptions the model will be described using parameters pa and pb below which should not be confused with the predictive probabilities in a particular context for example p s baa. a bent coin labelled a and b is tossed some number of times l which we don t know beforehand. the coin s probability of coming up a when tossed is pa and pb pa the parameters pa pb are not known beforehand. the source string s indicates that l was and the sequence of outcomes was baaba. it is assumed that the length of the string l has an exponential probability distribution p this distribution corresponds to assuming a constant probability for the termination symbol at each character. it is assumed that the non-terminal characters in the string are selected independently at random from an ensemble with probabilities p fpa pbg the probability pa is throughout the string to some unknown value that could be anywhere between and the probability of an a occurring as the next symbol given pa only we knew it is the probability given pa that an unterminated string of length f is a given string s that contains ffa fbg counts of the two outcomes is the bernoulli distribution p pa f pfa a pafb we assume a uniform prior distribution for pa p pa and pb pa. it would be easy to assume other priors on pa with beta distributions being the most convenient to handle. this model was studied in section the key result we require is the predictive distribution for the next symbol given the string so far s. this probability that the next character is a or b that it is not was derived in equation and is precisely laplace s rule exercise compare the expected message length when an ascii is compressed by the following three methods. read the whole the empirical frequency of each symbol construct a code for those frequencies transmit the code by transmitting the lengths of the codewords then transmit the using the code. actual codewords don t need to be transmitted since we can use a deterministic method for building the tree given the codelengths. arithmetic code using the laplace model. plaj fa arithmetic code using a dirichlet model. this model s predic tions are pdaj fa copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes where is to a number such as a small value of corresponds to a more responsive version of the laplace model the probability over characters is expected to be more nonuniform reproduces the laplace model. take care that the header of your message is self-delimiting. special cases worth considering are short with just a few hundred characters large in which some characters are never used. further applications of arithmetic coding generation of random samples arithmetic coding not only a way to compress strings believed to come from a given model it also a way to generate random strings from a model. imagine sticking a pin into the unit interval at random that line having been divided into subintervals in proportion to probabilities pi the probability that your pin will lie in interval i is pi. so to generate a sample from a model all we need to do is feed ordinary random bits into an arithmetic decoder for that model. an random bit sequence corresponds to the selection of a point at random from the line so the decoder will then select a string at random from the assumed distribution. this arithmetic method is guaranteed to use very nearly the smallest number of random bits possible to make the selection an important point in communities where random numbers are expensive! is not a joke. large amounts of money are spent on generating random bits in software and hardware. random numbers are valuable. a simple example of the use of this technique is in the generation of random bits with a nonuniform distribution exercise compare the following two techniques for generating random symbols from a nonuniform distribution the standard method use a standard random number generator to generate an integer between and rescale the integer to test whether this uniformly distributed random variable is less than and emit a or accordingly. arithmetic coding using the correct model fed with standard ran dom bits. roughly how many random bits will each method use to generate a thousand samples from this sparse distribution? data-entry devices when we enter text into a computer we make gestures of some sort maybe we tap a keyboard or scribble with a pointer or click with a mouse an text entry system is one where the number of gestures required to enter a given text string is small. writing can be viewed as an inverse process to data compression. in data compression the aim is to map a given text string into a small number of bits. in text entry we want a small sequence of gestures to produce our intended text. by inverting an arithmetic coder we can obtain an text entry device that is driven by continuous pointing gestures et al. compression text bits writing text gestures copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. lempelziv coding in this system called dasher the user zooms in on the unit interval to locate the interval corresponding to their intended string in the same style as a language model as used in text compression controls the sizes of the intervals such that probable strings are quick and easy to identify. after an hour s practice a novice user can write with one driving dasher at about words per minute that s about half their normal typing speed on a regular keyboard. it s even possible to write at words per minute hands-free using gaze direction to drive dasher and mackay dasher is available as free software for various lempelziv coding the lempelziv algorithms which are widely used for data compression the compress and gzip commands are in philosophy to arithmetic coding. there is no separation between modelling and coding and no opportunity for explicit modelling. basic lempelziv algorithm the method of compression is to replace a substring with a pointer to an earlier occurrence of the same substring. for example if the string is we parse it into an ordered dictionary of substrings that have not appeared before as follows we include the empty substring as the substring in the dictionary and order the substrings in the dictionary by the order in which they emerged from the source. after every comma we look along the next part of the input sequence until we have read a substring that has not been marked before. a moment s will that this substring is longer by one bit than a substring that has occurred earlier in the dictionary. this means that we can encode each substring by giving a pointer to the earlier occurrence of that and then sending the extra bit by which the new substring in the dictionary from the earlier substring. if at the nth bit we have enumerated sn substrings then we can give the value of the pointer in dlog sne bits. the code for the above sequence is then as shown in the fourth line of the following table punctuation included for clarity the upper lines indicating the source string and the value of sn source substrings sn snbinary bit notice that the pointer we send is empty because given that there is only one substring in the dictionary the string no bits are needed to convey the choice of that substring as the the encoded string is the encoding in this simple case is actually a longer string than the source string because there was no obvious redundancy in the source string. exercise prove that any uniquely decodeable code from to necessarily makes some strings longer if it makes some strings shorter. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes one reason why the algorithm described above lengthens a lot of strings is because it is it transmits unnecessary bits to put it another way its code is not complete. once a substring in the dictionary has been joined there by both of its children then we can be sure that it will not be needed possibly as part of our protocol for terminating a message so at that point we could drop it from our dictionary of substrings and them all along one thereby reducing the length of subsequent pointer messages. equivalently we could write the second into the dictionary at the point previously occupied by the parent. a second unnecessary overhead is the transmission of the new bit in these cases the second time a is used we can be sure of the identity of the next bit. decoding the decoder again involves an identical twin at the decoding end who constructs the dictionary of substrings as the data are decoded. exercise encode the string using the basic lempelziv algorithm described above. exercise decode the string that was encoded using the basic lempelziv algorithm. practicalities in this description i have not discussed the method for terminating a string. there are many variations on the lempelziv algorithm all exploiting the same idea but using procedures for dictionary management etc. the resulting programs are fast but their performance on compression of english text although useful does not match the standards set in the arithmetic coding literature. theoretical properties in contrast to the block code code and arithmetic coding methods we discussed in the last three chapters the lempelziv algorithm is without making any mention of a probabilistic model for the source. yet given any ergodic source one that is memoryless on long timescales the lempelziv algorithm can be proven asymptotically to compress down to the entropy of the source. this is why it is called a universal compression algorithm. for a proof of this property see cover and thomas it achieves its compression however only by memorizing substrings that have happened so that it has a short name for them the next time they occur. the asymptotic timescale on which this universal performance is achieved may for many sources be unfeasibly long because the number of typical substrings that need memorizing may be enormous. the useful performance of the algorithm in practice is a of the fact that many contain multiple repetitions of particular short sequences of characters a form of redundancy to which the algorithm is well suited. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. demonstration common ground i have emphasized the in philosophy behind arithmetic coding and lempelziv coding. there is common ground between them though in principle one can design adaptive probabilistic models and thence arithmetic codes that are universal that is models that will asymptotically compress any source in some class to within some factor of its entropy. however for practical purposes i think such universal models can only be constructed if the class of sources is severely restricted. a general purpose compressor that can discover the probability distribution of any source would be a general purpose intelligence! a general purpose intelligence does not yet exist. demonstration an interactive aid for exploring arithmetic coding dasher.tcl is a demonstration arithmetic-coding software package written by radford consists of encoding and decoding modules to which the user adds a module the probabilistic model. it should be emphasized that there is no single general-purpose arithmetic-coding compressor a new model has to be written for each type of source. radford neal s package includes a simple adaptive model similar to the bayesian model demonstrated in section the results using this laplace model should be viewed as a basic benchmark since it is the simplest possible probabilistic model it simply assumes the characters in the come independently from a ensemble. the counts ffig of the symbols faig are rescaled and rounded as the is read such that all the counts lie between and a state-of-the-art compressor for documents containing text and images djvu uses arithmetic it uses a carefully designed approximate arithmetic coder for binary alphabets called the z-coder et al. which is much faster than the arithmetic coding software described above. one of the neat tricks the z-coder uses is this the adaptive model adapts only occasionally save on computer time with the decision about when to adapt being pseudo-randomly controlled by whether the arithmetic encoder emitted a bit. the jbig image compression standard for binary images uses arithmetic coding with a context-dependent model which adapts using a rule similar to laplace s rule. ppm is a leading method for text compression and it uses arithmetic coding. there are many lempelziv-based programs. gzip is based on a version of lempelziv called and lempel compress is based on lzw in my experience the best is gzip with compress being inferior on most bzip is a block-sorting compressor which makes use of a neat hack called the burrowswheeler transform and wheeler this method is not based on an explicit probabilistic model and it only works well for larger than several thousand characters but in practice it is a very compressor for in which the context of a character is a good predictor for that is a lot of information about the burrowswheeler transform on the net. httpdogma.netdatacompressionbwt.shtml copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. compression of a text table gives the computer time in seconds taken and the compression achieved when these programs are applied to the latex containing the text of this chapter of size bytes. stream codes method compression compressed size uncompression time sec of time sec table comparison of compression algorithms applied to a text laplace model gzip compress bzip ppmz compression of a sparse interestingly gzip does not always do so well. table gives the compression achieved when these programs are applied to a text containing characters each of which is either and with probabilities and the laplace model is quite well matched to this source and the benchmark arithmetic coder gives good performance followed closely by compress gzip is worst. an ideal model for this source would compress the into about bytes. the laplace-model compressor falls short of this performance because it is implemented using only eight-bit precision. the ppmz compressor compresses the best of all but takes much more computer time. method compression compressed size uncompression time sec bytes time sec laplace model gzip gzip compress bzip ppmz summary in the last three chapters we have studied three classes of data compression codes. fixed-length block codes these are mappings from a number of source symbols to a binary message. only a tiny fraction of the source strings are given an encoding. these codes were fun for identifying the entropy as the measure of compressibility but they are of little practical use. table comparison of compression algorithms applied to a random of characters and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises on stream codes symbol codes symbol codes employ a variable-length code for each symbol in the source alphabet the codelengths being integer lengths determined by the probabilities of the symbols. s algorithm constructs an optimal symbol code for a given set of symbol probabilities. every source string has a uniquely decodeable encoding and if the source symbols come from the assumed distribution then the symbol code will compress to an expected length per character l lying in the interval h statistical in the source may make the actual length longer or shorter than this mean length. if the source is not well matched to the assumed distribution then the mean length is increased by the relative entropy dkl between the source distribution and the code s implicit distribution. for sources with small entropy the symbol has to emit at least one bit per source symbol compression below one bit per source symbol can be achieved only by the cumbersome procedure of putting the source data into blocks. stream codes. the distinctive property of stream codes compared with symbol codes is that they are not constrained to emit at least one bit for every symbol read from the source stream. so large numbers of source symbols may be coded into a smaller number of bits. this property could be obtained using a symbol code only if the source stream were somehow chopped into blocks. arithmetic codes combine a probabilistic model with an encoding algorithm that each string with a sub-interval of of size equal to the probability of that string under the model. this code is almost optimal in the sense that the compressed length of a string x closely matches the shannon information content of x given the probabilistic model. arithmetic codes with the philosophy that good compression requires data modelling in the form of an adaptive bayesian model. lempelziv codes are adaptive in the sense that they memorize strings that have already occurred. they are built on the philosophy that we don t know anything at all about what the probability distribution of the source will be and we want a compression algorithm that will perform reasonably well whatever that distribution is. both arithmetic codes and lempelziv codes will fail to decode correctly if any of the bits of the compressed are altered. so if compressed are to be stored or transmitted over noisy media error-correcting codes will be essential. reliable communication over unreliable channels is the topic of part ii. exercises on stream codes exercise describe an arithmetic coding algorithm to encode random bit strings of length n and weight k k ones and n k zeroes where n and k are given. for the case n k show in detail the intervals corresponding to all source substrings of lengths exercise how many bits are needed to specify a selection of k objects from n objects? and k are assumed to be known and the copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes selection of k objects is unordered. how might such a selection be made at random without being wasteful of random bits? exercise a binary source x emits independent identically distributed symbols with probability distribution where find an optimal uniquely-decodeable symbol code for a string x of three successive samples from this source. estimate one decimal place the factor by which the expected length of this optimal code is greater than the entropy of the three-bit string x. where x x x. an arithmetic code is used to compress a string of samples from the source x. estimate the mean and standard deviation of the length of the compressed exercise describe an arithmetic coding algorithm to generate random bit strings of length n with density f each bit has probability f of being a one where n is given. exercise use a lempelziv algorithm in which as discussed on the dictionary of is pruned by writing new into the space occupied by that will not be needed again. such can be when both their children have been added to the dictionary of may neglect the issue of termination of encoding. use this algorithm to encode the string highlight the bits that follow a on the second occasion that that is used. discussed earlier these bits could be omitted. exercise show that this lempelziv code is still not complete that is there are binary strings that are not encodings of any string. exercise give examples of simple sources that have low entropy but would not be compressed well by the lempelziv algorithm. further exercises on data compression the following exercises may be skipped by the reader who is eager to learn about noisy channels. exercise consider a gaussian distribution in n dimensions n p estimate the mean the radius of a point x to be r and variance of the square of the radius you may helpful the integral z dx though you should be able to estimate the required quantities without it. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on data compression assuming that n is large show that nearly all the probability of a gaussian is contained in a thin shell of radius pn find the thickness of the shell. evaluate the probability density at a point in that thin shell and at the origin x and compare. use the case n as an example. notice that nearly all the probability mass is located in a part of the space from the region of highest probability density. exercise explain what is meant by an optimal binary symbol code. find an optimal binary symbol code for the ensemble probability density is maximized here almost all probability mass is here figure schematic representation of the typical set of an n gaussian distribution. a fa b c d e f g h i jg p and compute the expected length of the code. exercise a string y consists of two independent samples from an ensemble x ax fa b cgpx what is the entropy of y? construct an optimal binary symbol code for the string y and its expected length. exercise strings of n independent samples from an ensemble with p are compressed using an arithmetic code that is matched to that ensemble. estimate the mean and standard deviation of the compressed strings lengths for the case n exercise source coding with variable-length symbols. in the chapters on source coding we assumed that we were encoding into a binary alphabet in which both symbols should be used with equal frequency. in this question we explore how the encoding alphabet should be used if the symbols take times to transmit. a poverty-stricken student communicates for free with a friend using a telephone by selecting an integer n making the friend s phone ring n times then hanging up in the middle of the nth ring. this process is repeated so that a string of symbols is received. what is the optimal way to communicate? if large integers n are selected then the message takes longer to communicate. if only small integers n are used then the information content per symbol is small. we aim to maximize the rate of information transfer per unit time. assume that the time taken to transmit a number of rings n and to redial is ln seconds. consider a probability distribution over n fpng. the average duration per symbol to be lp pnln copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes and the entropy per symbol to be hp pn pn show that for the average information rate per second to be maximized the symbols must be used with probabilities of the form pn z where z and the implicit equation hp lp that is is the rate of communication. show that these two equations imply that must be set such that log z assuming that the channel has the property ln n seconds the optimal distribution p and show that the maximal information rate is bit per second. how does this compare with the information rate per second achieved if p is set to that is only the symbols n and n are selected and they have equal probability? discuss the relationship between the results derived above and the kraft inequality from source coding theory. how might a random binary source be encoded into a sequence of symbols for transmission over the channel in equation exercise how many bits does it take to a pack of cards? exercise in the card game bridge the four players receive cards each from the deck of and start each game by looking at their own hand and bidding. the legal bids are in ascending order t t and successive bids must follow this order a bid of say may only be followed by higher bids such as or or t us neglect the double bid. the players have several aims when bidding. one of the aims is for two partners to communicate to each other as much as possible about what cards are in their hands. let us concentrate on this task. after the cards have been dealt how many bits are needed for north to convey to south what her hand is? assuming that e and w do not bid at all what is the maximum total information that n and s can convey to each other while bidding? assume that n starts the bidding and that once either n or s stops bidding the bidding stops. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions exercise my old arabic microwave oven had buttons for entering cooking times and my new roman microwave has just the buttons of the roman microwave are labelled minutes minute seconds second and start i ll abbreviate these strings to the symbols m c x i to enter one minute and twenty-three seconds the arabic sequence is arabic roman m c x i figure alternative keypads for microwave ovens. and the roman sequence is each of these keypads a code mapping the cooking times from to into a string of symbols. which times can be produced with two or three symbols? example can be produced by three symbols in either code and are the two codes complete? give a detailed answer. for each code name a cooking time that it can produce in four symbols that the other code cannot. discuss the implicit probability distributions over times to which each of these codes is best matched. concoct a plausible probability distribution over times that a real user might use and evaluate roughly the expected number of symbols and maximum number of symbols that each code requires. discuss the ways in which each code is or invent a more cooking-time-encoding system for a mi crowave oven. exercise is the standard binary representation for positive inte gers a uniquely decodeable code? design a binary code for the positive integers i.e. a mapping from n to cn that is uniquely decodeable. try to design codes that are codes and that satisfy the kraft equality pn motivations any data terminated by a special end of character can be mapped onto an integer so a code for integers can be used as a self-delimiting encoding of too. large correspond to large integers. also one of the building blocks of a universal coding scheme that is a coding scheme that will work ok for a large variety of sources is the ability to encode integers. finally in microwave ovens cooking times are positive integers! discuss criteria by which one might compare alternative codes for integers equivalently alternative self-delimiting codes for solutions solution to exercise the worst-case situation is when the interval to be represented lies just inside a binary interval. in this case we may choose either of two binary intervals as shown in these binary intervals copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes source string s interval binary intervals p figure termination of arithmetic coding in the worst case where there is a two bit overhead. either of the two binary intervals marked on the right-hand side may be chosen. these binary intervals are no smaller than p are no smaller than p so the binary encoding has a length no greater than which is two bits more than the ideal message length. solution to exercise the standard method uses random bits per generated symbol and so requires bits to generate one thousand samples. arithmetic coding uses on average about bits per generated symbol and so requires about bits to generate one thousand samples an overhead of roughly two bits associated with termination. fluctuations in the number of would produce variations around this mean with standard deviation solution to exercise the encoding is which comes from the parsing which is encoded thus solution to exercise the decoding is solution to exercise this problem is equivalent to exercise bits the selection of k objects from n objects requires dlog n bits. this selection could be made using arithmetic coding. the selection corresponds to a binary string of length n in which the bits represent which objects are selected. initially the probability of a is kn and the probability of a is thereafter given that the emitted string thus far of length n contains k the probability of a is and the probability of a is solution to exercise this lempelziv code is still not complete because for example after have been collected the pointer could be any of the strings but it cannot be or thus there are some binary strings that cannot be produced as encodings. solution to exercise sources with low entropy that are not well compressed by lempelziv include copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions sources with some symbols that have long range correlations and intervening random junk. an ideal model should capture what s correlated and compress it. lempelziv can compress the correlated features only by memorizing all cases of the intervening junk. as a simple example consider a telephone book in which every line contains an number new number pair the number of characters per line is drawn from the alphabet the characters and occur in a predictable sequence so the true information content per line assuming all the phone numbers are seven digits long and assuming that they are random sequences is about bans. ban is the information content of a random integer between and a state language model could easily capture the regularities in these data. a lempelziv algorithm will take a long time before it compresses such a down to bans per line however because in order for it to learn that the string is always followed by for any three digits ddd it will have to see all those strings. so near-optimal compression will only be achieved after thousands of lines of the have been read. figure a source with low entropy that is not well compressed by lempelziv. the bit sequence is read from left to right. each line from the line above in f of its bits. the image width is pixels. sources with long range correlations for example two-dimensional images that are represented by a sequence of pixels row by row so that vertically adjacent pixels are a distance w apart in the source stream where w is the image width. consider for example a fax transmission in which each line is very similar to the previous line the true entropy is only per pixel where f is the probability that a pixel from its parent. lempelziv algorithms will only compress down to the entropy once all strings of length have occurred and their successors have been memorized. there are only about particles in the universe so we can say that lempelziv codes will never capture the redundancy of such an image. another highly redundant texture is shown in the image was made by dropping horizontal and vertical pins randomly on the plane. it contains both long-range vertical correlations and long-range horizontal correlations. there is no practical way that lempelziv fed with a pixel-by-pixel scan of this image could capture both these correlations. biological computational systems can readily identify the redundancy in these images and in images that are much more complex thus we might anticipate that the best data compression algorithms will result from the development of intelligence methods. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. stream codes figure a texture consisting of horizontal and vertical pins dropped at random on the plane. sources with intricate redundancy such as generated by computers. for example a latex followed by its encoding into a postscript the information content of this pair of is roughly equal to the information content of the latex alone. a picture of the mandelbrot set. the picture has an information content equal to the number of bits required to specify the range of the complex plane studied the pixel sizes and the colouring rule used. a picture of a ground state of a frustrated antiferromagnetic ising model which we will discuss in chapter like this binary image has interesting correlations in two directions. figure frustrated triangular ising model in one of its ground states. cellular automata shows the state history of steps of a cellular automaton with cells. the update rule in which each cell s new state depends on the state of preceding cells was selected at random. the information content is equal to the information in the boundary bits and the propagation rule which here can be described in bits. an optimal compressor will thus give a compressed length which is essentially constant independent of the vertical height of the image. lempelziv would only give this zero-cost compression once the cellular automaton has entered a periodic limit cycle which could easily take about iterations. in contrast the jbig compression method which models the probability of a pixel given its local context and uses arithmetic coding would do a good job on these images. solution to exercise for a one-dimensional gaussian the variance of x is so the mean value of in n dimensions since the components of x are independent random variables is n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions figure the time-history of a cellular automaton with cells. the variance of similarly is n times the variance of where x is a one-dimensional gaussian variable. dx the integral is found to be so thus the variance of is for large n the central-limit theorem indicates that r has a gaussian distribution with mean n and standard deviation so the probability density of r must similarly be concentrated about r pn the thickness of this shell is given by turning the standard deviation of into a standard deviation on r for small log r log so setting r has standard deviation the probability density of the gaussian at a point xshell where r pn is p n n whereas the probability density at the origin is p thus p exp the probability density at the typical radius is times smaller than the density at the origin. if n then the probability density at the origin is times greater. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. codes for integers this chapter is an aside which may safely be skipped. solution to exercise to discuss the coding of integers we need some the standard binary representation of a positive integer n will be denoted by cbn e.g. the standard binary length of a positive integer n lbn length of the string cbn. for example is the the standard binary representation cbn is not a uniquely decodeable code for integers since there is no way of knowing when an integer has ended. for example is identical to it would be uniquely decodeable if we knew the standard binary length of each integer before it was received. noticing that all positive integers have a standard binary representation that starts with a we might another representation the headless binary representation of a positive integer n will be denoted by cbn e.g. and denotes the null string. this representation would be uniquely decodeable if we knew the length lbn of the integer. so how can we make a uniquely decodeable code for integers? two strate gies can be distinguished. self-delimiting codes. we communicate somehow the length of the integer lbn which is also a positive integer then communicate the original integer n itself using cbn. codes with end of characters. we code the integer into blocks of length b bits and reserve one of the symbols to have the special meaning end of the coding of integers into blocks is arranged so that this reserved symbol is not needed for any other purpose. the simplest uniquely decodeable code for integers is the unary code which can be viewed as a code with an end of character. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. codes for integers unary code. an integer n is encoded by sending a string of followed by a n cun the unary code has length lun n. the unary code is the optimal code for integers if the probability distribution over n is pun self-delimiting codes we can use the unary code to encode the length of the binary encoding of n and make a self-delimiting code code we send the unary code for lbn followed by the headless binary representation of n. culbncbn table shows the codes for some integers. the overlining indicates the division of each string into the parts culbn and cbn. we might equivalently view as consisting of a string of zeroes followed by the standard binary representation of n cbn. the codeword has length the implicit probability distribution over n for the code is separable into the product of a probability distribution over the length l and a uniform distribution over integers having that length p p l lbn l otherwise now for the above code the header that communicates the length always occupies the same number of bits as the standard binary representation of the integer or take one. if we are expecting to encounter large integers then this representation seems suboptimal since it leads to all occupying a size that is double their original uncoded size. instead of using the unary code to encode the length lbn we could use code we send the length lbn using followed by the headless binary representation of n. iterating this procedure we can a sequence of codes. code code n cbn lbn table n table and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. codes for integers n table two codes with symbols and spaces have been included to show the byte boundaries. codes with symbols we can also make byte-based representations. s use the term byte here to denote any string of bits not just a string of length bits. if we encode the number in some base for example decimal then we can represent each digit in a byte. in order to represent a digit from to in a byte we need four bits. because this leaves extra four-bit symbols that correspond to no decimal digit. we can use these as symbols to indicate the end of our positive integer. clearly it is redundant to have more than one symbol so a more code would encode the integer into base and use just the sixteenth symbol as the punctuation character. generalizing this idea we can make similar byte-based codes for integers in bases and and in any base of the form these codes are almost complete. that a code is complete if it the kraft inequality with equality. the codes remaining is that they provide the ability to encode the integer zero and the empty string neither of which was required. exercise consider the implicit probability distribution over inte gers corresponding to the code with an character. if the code has eight-bit blocks the integer is coded in base what is the mean length in bits of the integer under the implicit distribution? if one wishes to encode binary of expected size about one hundred kilobytes using a code with an character what is the optimal block size? encoding a tiny to illustrate the codes we have discussed we now use each code to encode a small consisting of just characters claude shannon if we map the ascii characters onto seven-bit symbols in decimal c l etc. this character corresponds to the integer n the unary code for n consists of this many one zeroes followed by a one. if all the oceans were turned into ink and if we wrote a hundred bits with every cubic millimeter there might be enough ink to write cun. the standard binary representation of n is this sequence of bits cbn exercise write down or describe the following self-delimiting representations of the above number n and which of these encodings is the shortest? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. algorithm elias s encoder for an integer n. codes for integers comparing the codes one could answer the question which of two codes is superior? by a sentence of the form for n k code is superior for n k code is superior but i contend that such an answer misses the point any complete code corresponds to a prior for which it is optimal you should not say that any other code is superior to it. other codes are optimal for other priors. these implicit priors should be thought about so as to achieve the best code for one s application. notice that one cannot for free switch from one code to another choosing if one were to do this then it would be necessary to whichever is shorter. lengthen the message in some way that indicates which of the two codes is being used. if this is done by a single leading bit it will be found that the resulting code is suboptimal because it fails the kraft equality as was discussed in exercise another way to compare codes for integers is to consider a sequence of probability distributions such as monotonic probability distributions over n and rank the codes as to how well they encode any of these distributions. a code is called a universal code if for any distribution in a given class it encodes into an average length that is within some factor of the ideal average length. let me say this again. we are meeting an alternative world view rather than out a good prior over integers as advocated above many theorists have studied the problem of creating codes that are reasonably good codes for any priors in a broad class. here the class of priors conventionally considered is the set of priors that assign a monotonically decreasing probability over integers and have entropy. several of the codes we have discussed above are universal. another code which elegantly transcends the sequence of self-delimiting codes is elias s universal code for integers which chooses from all the codes it works by sending a sequence of messages each of which encodes the length of the next message and indicates by a single bit whether or not that message is the integer its standard binary representation. because a length is a positive integer and all positive integers begin with all the leading can be omitted. write loop f g if blog nc halt prepend cbn to the written string nblog nc the encoder of c! is shown in algorithm the encoding is generated from right to left. table shows the resulting codewords. exercise show that the elias code is not actually the best code for a prior distribution that expects very large integers. this by constructing another code and specifying how large n must be for your code to give a shorter length than elias s. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. n c!n solutions codes for integers n c!n n c!n n c!n table elias s universal code for integers. examples from to solution to exercise the use of the symbol in a code that represents the integer in some base q corresponds to a belief that there is a probability of that the current character is the last character of the number. thus the prior to which this code is matched puts an exponential prior distribution over the length of the integer. the expected number of characters is q so the expected length of the integer is bits. we wish to q such that q log q bits. a value of q between and this constraint so blocks are roughly the optimal size assuming there is one character. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. part ii noisy-channel coding copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. dependent random variables in the last three chapters on data compression we concentrated on random vectors x coming from an extremely simple probability distribution namely the separable distribution in which each component xn is independent of the others. in this chapter we consider joint ensembles in which the random variables are dependent. this material has two motivations. first data from the real world have interesting correlations so to do data compression well we need to know how to work with models that include dependences. second a noisy channel with input x and output y a joint ensemble in which x and y are dependent if they were independent it would be impossible to communicate over the channel so communication over noisy channels topic of chapters is described in terms of the entropy of joint ensembles. more about entropy this section gives and exercises to do with entropy carrying on from section the joint entropy of x y is hx y p y log p y entropy is additive for independent random variables hx y hx hy p y p the conditional entropy of x given y bk is the entropy of the proba bility distribution p y bk. hx j y bk p y bk log p y bk the conditional entropy of x given y is the average over y of the con ditional entropy of x given y. hx j y p p y log p p y log p y this measures the average uncertainty that remains about x when y is known. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more about entropy the marginal entropy of x is another name for the entropy of x hx used to contrast it with the conditional entropies listed above. chain rule for information content. from the product rule for probabil ities equation we obtain so log p y log p log p j x hx y hx hy j x in words this says that the information content of x and y is the information content of x plus the information content of y given x. chain rule for entropy. the joint marginal entropy are related by entropy conditional entropy and hx y hx hy j x hy hx j y in words this says that the uncertainty of x and y is the uncertainty of x plus the uncertainty of y given x. the mutual information between x and y is ix y hx hx j y and ix y iy x and ix y it measures the average reduction in uncertainty about x that results from learning the value of y or vice versa the average amount of information that x conveys about y. the conditional mutual information between x and y given z ck is the mutual information between the random variables x and y in the joint ensemble p y j z ck ix y j z ck hx j z ck hx j y z ck the conditional mutual information between x and y given z is the average over z of the above conditional mutual information. ix y j z hx j z hx j y z no other three-term entropies will be for example expressions such as ix y z and ix j y z are illegal. but you may put conjunctions of arbitrary numbers of variables in each of the three spots in the expression ix y j z for example ia b c d j e f is it measures how much information on average c and d convey about a and b assuming e and f are known. figure shows how the total entropy hx y of a joint ensemble can be broken down. this is important. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. dependent random variables hx y hx hy hx j y ix y hy jx figure the relationship between joint information marginal entropy conditional entropy and mutual entropy. exercises exercise consider three independent random variables u v w with entropies hu hv hw. let x v and y w what is hx y what is hx j y what is ix y exercise referring to the of conditional entropy an example that it is possible for hx j y bk to exceed hx but that the average hx j y is less than hx. so data are helpful they do not increase uncertainty on average. exercise prove the chain rule for entropy equation y hx hy j x. exercise prove that the mutual information ix y hx hx j y ix y iy x and ix y see exercise and note that ix y dklp yjjp exercise the entropy distance between two random variables can be to be the between their joint entropy and their mutual information dh y hx y ix y prove that the entropy distance the axioms for a distance dh y dh x dh y dh x and dh z dh y dh z. we are unlikely to see dh y again but it is a good function on which to practise inequality-proving. exercise a joint ensemble xy has the following joint distribution. p y x y what is the joint entropy hx y what are the marginal entropies hx and hy for each value of y what is the conditional entropy hx j y? what is the conditional entropy hx j y what is the conditional entropy of y given x? what is the mutual information between x and y copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises exercise consider the ensemble xy z in which ax ay az x and y are independent with px fp pg and py fq and z y mod if q what is pz? what is iz x? for general p and q what is pz? what is iz x? notice that this ensemble is related to the binary symmetric channel with x input y noise and z output. hy figure a misleading representation of entropies with hxy ixy hyx hxy hx three term entropies exercise many texts draw in the form of a venn diagram discuss why this diagram is a misleading representation of entropies. hint consider the three-variable ensemble xy z in which x and y are independent binary variables and z is to be z x y mod further exercises the data-processing theorem the data processing theorem states that data processing can only destroy information. exercise prove this theorem by considering an ensemble w dr in which w is the state of the world d is data gathered and r is the processed data so that these three variables form a markov chain that is the probability p d r can be written as w d r p d r p wp j d show that the average information that r conveys about w iw r is less than or equal to the average information that d conveys about w iw d. this theorem is as much a caution about our of information as it is a caution about data processing! copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. dependent random variables inference and information measures exercise the three cards. one card is white on both faces one is black on both faces and one is white on one side and black on the other. the three cards are and their orientations randomized. one card is drawn and placed on the table. the upper face is black. what is the colour of its lower face? the inference problem. does seeing the top face convey information about the colour of the bottom face? discuss the information contents and entropies in this situation. let the value of the upper face s colour be u and the value of the lower face s colour be l. imagine that we draw a random card and learn both u and l. what is the entropy of u hu what is the entropy of l hl? what is the mutual information between u and l iu l? entropies of markov processes exercise in the guessing game we imagined predicting the next letter in a document starting from the beginning and working towards the end. consider the task of predicting the reversed text that is predicting the letter that precedes those already known. most people this a harder task. assuming that we model the language using an n model says the probability of the next character depends only on the n preceding characters is there any between the average information contents of the reversed language and the forward language? solutions solution to exercise see exercise for an example where hx j y exceeds hx y we can prove the inequality hx j y hx by turning the expression into a relative entropy bayes theorem and invoking gibbs inequality p p y log p y log p y p hx j y p xxy xx p log p j x log p p j xp p p j x log p p j x the last expression is a sum of relative entropies between the distributions p j x and p so hx j y hx with equality only if p j x p for all x and y is only if x and y are independent. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solution to exercise the chain rule for entropy follows from the decomposition of a joint probability p y p y log hx y xxy xxy xx p hx hy j x p j p log p log p p j p j x log p j x solution to exercise symmetry of mutual information ix y hx hx j y p log p y log p p y p p y log p y p xx xxy xxy p y log p y this expression is symmetric in x and y so ix y hx hx j y hy hy j x we can prove that mutual information is positive in two ways. one is to continue from ix y p y log p y p which is a relative entropy and use gibbs inequality on which asserts that this relative entropy is with equality only if p y p that is if x and y are independent. the other is to use jensen s inequality on p y log p p y logxxy p y p y p log solution to exercise z x y mod if q pz and iz x hz hz j x for general q and p pz the mutual information is iz x j x three term entropies solution to exercise the depiction of entropies in terms of venn diagrams is misleading for at least two reasons. first one is used to thinking of venn diagrams as depicting sets but what are the sets hx and hy depicted in and what are the objects that are members of those sets? i think this diagram encourages the novice student to make inappropriate analogies. for example some students imagine copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. dependent random variables figure a misleading representation of entropies continued. ixy hxyz hx hzx hy hyxz hz ixyz a hzy hxyz hzxy that the random outcome y might correspond to a point in the diagram and thus confuse entropies with probabilities. secondly the depiction in terms of venn diagrams encourages one to believe that all the areas correspond to positive quantities. in the special case of two random variables it is indeed true that hx j y ix y and hy j x are positive quantities. but as soon as we progress to three-variable ensembles we obtain a diagram with positive-looking areas that may actually correspond to negative quantities. figure correctly shows relationships such as hx hz j x hy j x z hx y z but it gives the misleading impression that the conditional mutual information ix y j z is less than the mutual information ix y in fact the area labelled a can correspond to a negative quantity. consider the joint ensemble y z in which x and y are independent binary variables and z is to be z x y mod then clearly hx hy bit. also hz bit. and hy j x hy since the two variables are independent. so the mutual information between x and y is zero. ix y however if z is observed x and y become dependent knowing x given z tells you what y is y z x mod so ix y j z bit. thus the area labelled a must correspond to bits for the to give the correct answers. the above example is not at all a capricious or exceptional illustration. the binary symmetric channel with input x noise y and output z is a situation in which ix y and noise are independent but ix y j z you see the output the unknown input and the unknown noise are intimately related!. the venn diagram representation is therefore valid only if one is aware that positive areas may represent negative quantities. with this proviso kept in mind the interpretation of entropies in terms of sets can be helpful solution to exercise for any joint ensemble xy z the following chain rule for mutual information holds. in the case w d r w and r are independent given d so ix y z ix y ix z j y now iw r j d using the chain rule twice we have and so iw d r iw d iw d r iw r iw d j r iw r iw d copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter before reading chapter you should have read chapter and worked on exercise and exercises copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over a noisy channel the big picture source coding channel coding source compressor decompressor encoder noisy channel decoder in chapters we discussed source coding with block codes symbol codes and stream codes. we implicitly assumed that the channel from the compressor to the decompressor was noise-free. real channels are noisy. we will now spend two chapters on the subject of noisy-channel coding the fundamental possibilities and limitations of error-free communication through a noisy channel. the aim of channel coding is to make the noisy channel behave like a noiseless channel. we will assume that the data to be transmitted has been through a good compressor so the bit stream has no obvious redundancy. the channel code which makes the transmission will put back redundancy of a special sort designed to make the noisy received signal decodeable. suppose we transmit bits per second with over a noisy channel that bits with probability f what is the rate of transmission of information? we might guess that the rate is bits per second by subtracting the expected number of errors per second. but this is not correct because the recipient does not know where the errors occurred. consider the case where the noise is so great that the received symbols are independent of the transmitted symbols. this corresponds to a noise level of f since half of the received symbols are correct due to chance alone. but when f no information is transmitted at all. given what we have learnt about entropy it seems reasonable that a measure of the information transmitted is given by the mutual information between the source and the received signal that is the entropy of the source minus the conditional entropy of the source given the received signal. we will now review the of conditional entropy and mutual information. then we will examine whether it is possible to use such a noisy channel to communicate reliably. we will show that for any channel q there is a non-zero rate the capacity cq up to which information can be sent copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. review of probability and information with arbitrarily small probability of error. review of probability and information as an example we take the joint distribution xy from exercise the marginal distributions p and p are shown in the margins. p y x p y p the joint entropy is hx y bits. the marginal entropies are hx bits and hy bits. we can compute the conditional distribution of x for each value of y and the entropy of each of those conditional distributions p y y x hx j ybits hx j y note that whereas hx j y is less than hx hx j y is greater than hx. so in some cases learning y can increase our uncertainty about x. note also that although p y is a distribution from p the conditional entropy hx j y is equal to hx. so learning that y is changes our knowledge about x but does not reduce the uncertainty of x as measured by the entropy. on average though learning y does convey information about x since hx j y hx. ix y hx hx j y bits. one may also evaluate hy jx bits. the mutual information is noisy channels a discrete memoryless channel q is characterized by an input alphabet ax an output alphabet ay and a set of conditional probability distributions p j x one for each x ax. these transition probabilities may be written in a matrix qjji p bj j x ai i usually orient this matrix with the output variable j indexing the rows and the input variable i indexing the columns so that each column of q is a probability vector. with this convention we can obtain the probability of the output py from a probability distribution over the input px by right-multiplication py qpx copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over a noisy channel some useful model channels are binary symmetric channel. ax ay p x f p x f binary erasure channel. ax ay x y p x f p x f x y p x f p x f p x p x p x f p x f noisy typewriter. ax ay the letters fa b z the letters are arranged in a circle and when the typist attempts to type b what comes out is either a b or c with probability each when the input is c the output is b c or d and so forth with the letter adjacent to the letter a. c pppq c pppq c pppq c pppq c pppq pppq pppq c c c pppq c pppq cw p fj x g p gj x g p hj x g a b c d e f g h a b c d e f g h c c y z y z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z z channel. ax ay x y p x p x p x f p x f inferring the input given the output if we assume that the input x to a channel comes from an ensemble x then we obtain a joint ensemble xy in which the random variables x and y have the joint distribution p y p j xp now if we receive a particular symbol y what was the input symbol x? we typically won t know for certain. we can write down the posterior distribution of the input using bayes theorem p y p j xp p p j xp p j example consider a binary symmetric channel with probability of error f let the input ensemble be px assume we observe y p y p x p j copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. information conveyed by a channel thus x is still less probable than x although it is not as improbable as it was before. exercise now assume we observe y compute the probability of x given y example consider a z channel with probability of error f let the input ensemble be px assume we observe y p y so given the output y we become certain of the input. exercise alternatively assume we observe y compute p y information conveyed by a channel we now consider how much information can be communicated through a channel. in operational terms we are interested in ways of using the channel such that all the bits that are communicated are recovered with negligible probability of error. in mathematical terms assuming a particular input ensemble x we can measure how much information the output conveys about the input by the mutual information ix y hx hx j y hy hy jx our aim is to establish the connection between these two ideas. let us evaluate ix y for some of the channels above. hint for computing mutual information we will tend to think of ix y as hx hx j y i.e. how much the uncertainty of the input x is reduced when we look at the output y but for computational purposes it is often handy to evaluate hy jx instead. hx y hx hy hx j y ix y hy jx figure the relationship between joint information marginal entropy conditional entropy and mutual entropy. this is important so i m showing it twice. example consider the binary symmetric channel again with f and px we already evaluated the marginal probabilities p implicitly above p p the mutual information is ix y hy hy jx copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over a noisy channel what is hy jx? it is to be the weighted sum over x of hy j x but hy j x is the same for each value of x hy j x is and hy j x is so ix y hy hy jx bits throughout this book log means this may be contrasted with the entropy of the source hx bits. note here we have used the binary entropy function hp p p log p p log example and now the z channel with px as above. p ix y hy hy jx bits the entropy of the source as above is hx bits. notice that the mutual information ix y for the z channel is bigger than the mutual information for the binary symmetric channel with the same f the z channel is a more reliable channel. exercise compute the mutual information between x and y for the binary symmetric channel with f when the input distribution is px exercise compute the mutual information between x and y for the z channel with f when the input distribution is px maximizing the mutual information we have observed in the above examples that the mutual information between the input and the output depends on the chosen input ensemble. let us assume that we wish to maximize the mutual information conveyed by the channel by choosing the best possible input ensemble. we the capacity of the channel to be its maximum mutual information. the capacity of a channel q is cq max px ix y the distribution px that achieves the maximum is called the optimal input distribution denoted by may be multiple optimal input distributions achieving the same value of ix y in chapter we will show that the capacity does indeed measure the maximum amount of error-free information that can be transmitted over the channel per unit time. example consider the binary symmetric channel with f above we considered px and found ix y bits. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the noisy-channel coding theorem how much better can we do? by symmetry the optimal input distribution is and the capacity is ix y cqbsc bits we ll justify the symmetry argument later. if there s any doubt about the symmetry argument we can always resort to explicit maximization of the mutual information ix y ix y example the noisy typewriter. the optimal input distribution is a uni form distribution over x and gives c bits. example consider the z channel with f identifying the optimal input distribution is not so straightforward. we evaluate ix y explicitly for px first we need to compute p the probability of y is easiest to write down figure the mutual information ix y for a binary symmetric channel with f as a function of the input distribution. p f then the mutual information is ix y hy hy jx f f this is a non-trivial function of shown in it is maximized for f by we cqz notice the optimal input distribution is not we can communicate slightly more information by using input symbol more frequently than ix y figure the mutual information ix y for a z channel with f as a function of the input distribution. exercise what is the capacity of the binary symmetric channel for general f exercise show that the capacity of the binary erasure channel with f is cbec what is its capacity for general f comment. the noisy-channel coding theorem it seems plausible that the capacity we have may be a measure of information conveyed by a channel what is not obvious and what we will prove in the next chapter is that the capacity indeed measures the rate at which blocks of data can be communicated over the channel with arbitrarily small probability of error. we make the following an k block code for a channel q is a list of s codewords xs an x length n using this code we can encode a signal s each of as xs. number of codewords s is an integer but the number of bits by choosing a codeword k log s is not necessarily an integer. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over a noisy channel the rate of the code is r kn bits per channel use. will use this of the rate for any channel not only channels with binary inputs note however that it is sometimes conventional to the rate of a code for a channel with q input symbols to be kn log q. a decoder for an k block code is a mapping from the set of length-n y to a codeword label strings of channel outputs an the extra symbol can be used to indicate a failure the probability of block error of a code and decoder for a given channel and for a given probability distribution over the encoded signal p is pb p sin j sin the maximal probability of block error is pbm max sin p sin j sin the optimal decoder for a channel code is the one that minimizes the probability of block error. it decodes an output y as the input s that has maximum posterior probability p y. p y p j sp p j argmax p y a uniform prior distribution on s is usually assumed in which case the optimal decoder is also the maximum likelihood decoder i.e. the decoder that maps an output y to the input s that has maximum likelihood p j s. the probability of bit error pb is assuming that the codeword number s is represented by a binary vector s of length k bits it is the average probability that a bit of sout is not equal to the corresponding bit of sin over all k bits. shannon s noisy-channel coding theorem one. associated with each discrete memoryless channel there is a non-negative number c the channel capacity with the following property. for any and r c for large enough n there exists a block code of length n and rate r and a decoding algorithm such that the maximal probability of block error is of the theorem for the noisy typewriter channel in the case of the noisy typewriter we can easily the theorem because we can create a completely error-free communication strategy using a block code of length n we use only the letters b e h z i.e. every third letter. these letters form a non-confusable subset of the input alphabet any output can be uniquely decoded. the number of inputs in the non-confusable subset is so the error-free information rate of this system is bits which is equal to the capacity c which we evaluated in example pbm achievable r c figure portion of the r pbm plane asserted to be achievable by the part of shannon s noisy channel coding theorem. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. intuitive preview of proof pppq pppq pppq pppq b e h z a b c d e f g h i j k l m n o p q r s t u v w x y z figure a non-confusable subset of inputs for the noisy typewriter. a b c d e f g h i y z a b c d e f g h i j k l m n o p q r s t u v w x y z figure extended channels obtained from a binary symmetric channel with transition probability n n n how does this translate into the terms of the theorem? the following table explains. the theorem associated with each discrete memoryless channel there is a non-negative number c. for any and r c for large enough n there exists a block code of length n and rate r and a decoding algorithm how it applies to the noisy typewriter the capacity c is no matter what and r are we set the blocklength n to the block code is fb e zg. the value of k is given by so k and this code has rate which is greater than the requested value of r. the decoding algorithm maps the received letter to the nearest letter in the code such that the maximal probability of block error is the maximal probability of block error is zero which is less than the given intuitive preview of proof extended channels to prove the theorem for any given channel we consider the extended channel corresponding to n uses of the channel. the extended channel has jaxjn possible inputs x and jay jn possible outputs. extended channels obtained from a binary symmetric channel and from a z channel are shown in and with n and n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over a noisy channel figure extended channels obtained from a z channel with transition probability each column corresponds to an input and each row is a output. figure some typical outputs in an y corresponding to typical inputs x. a subset of the typical sets shown in that do not overlap each other. this picture can be compared with the solution to the noisy typewriter in n n n an typical y y typical y for a given typical x an typical y y exercise find the transition probability matrices q for the extended channel with n derived from the binary erasure channel having erasure probability by selecting two columns of this transition probability matrix we can a code for this channel with blocklength n what is the best choice of two columns? what is the decoding algorithm? to prove the noisy-channel coding theorem we make use of large blocklengths n the intuitive idea is that if n is large an extended channel looks a lot like the noisy typewriter. any particular input x is very likely to produce an output in a small subspace of the output alphabet the typical output set given that input. so we can a non-confusable subset of the inputs that produce essentially disjoint output sequences. for a given n let us consider a way of generating such a non-confusable subset of the inputs and count up how many distinct inputs it contains. imagine making an input sequence x for the extended channel by drawing it from an ensemble x n where x is an arbitrary ensemble over the input alphabet. recall the source coding theorem of chapter and consider the number of probable output sequences y. the total number of typical output sequences y is hy all having similar probability. for any particular typical input sequence x there are about hy jx probable sequences. some of these subsets of an we now imagine restricting ourselves to a subset of the typical inputs x such that the corresponding typical output sets do not overlap as shown in we can then bound the number of non-confusable inputs by dividing the size of the typical y set hy by the size of each typical-y y are depicted by circles in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises given-typical-x set hy jx. so the number of non-confusable inputs if they are selected from the set of typical inputs x x n is hy hy jx ixy the maximum value of this bound is achieved if x is the ensemble that maximizes ix y in which case the number of non-confusable inputs is c thus asymptotically up to c bits per cycle and no more can be communicated with vanishing error probability. this sketch has not rigorously proved that reliable communication really is possible that s our task for the next chapter. further exercises exercise refer back to the computation of the capacity of the z channel with f why is less than one could argue that it is good to favour the input since it is transmitted without error and also argue that it is good to favour the input since it often gives rise to the highly prized output which allows certain of the input! try to make a convincing argument. in the case of general f show that the optimal input distribution is f what happens to if the noise level f is very close to exercise sketch graphs of the capacity of the z channel the binary symmetric channel and the binary erasure channel as a function of f exercise what is the capacity of the ten-output channel whose transition probability matrix is exercise consider a gaussian channel with binary input x and real output alphabet ay with transition probability density qy j x where is the signal amplitude. compute the posterior probability of x given y assuming that the two inputs are equiprobable. put your answer in the form p y copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over a noisy channel sketch the value of p y as a function of y. assume that a single bit is to be transmitted. what is the optimal decoder and what is its probability of error? express your answer in terms of the signal-to-noise ratio and the error function cumulative probability function of the gaussian distribution z dz that this of the error function may not correspond to other people s. pattern recognition as a noisy channel we may think of many pattern recognition problems in terms of communication channels. consider the case of recognizing handwritten digits as postcodes on envelopes. the author of the digit wishes to communicate a message from the set ax this selected message is the input to the channel. what comes out of the channel is a pattern of ink on paper. if the ink pattern is represented using binary pixels the channel q has as its output a random variable y ay an example of an element from this alphabet is shown in the margin. exercise estimate how many patterns in ay are recognizable as the aim of this problem is to try to demonstrate the character existence of as many patterns as possible that are recognizable as discuss how one might model the channel p j x estimate the entropy of the probability distribution p j x one strategy for doing pattern recognition is to create a model for p j x for each value of the input x then use bayes theorem to infer x given y. p y p j xp p j this strategy is known as full probabilistic modelling or generative modelling. this is essentially how current speech recognition systems work. in addition to the channel model p j x one uses a prior probability distribution p which in the case of both character recognition and speech recognition is a language model that the probability of the next characterword given the context and the known grammar and statistics of the language. random coding exercise given twenty-four people in a room what is the probability that there are at least two people present who have the same birthday day and month of birth? what is the expected number of pairs of people with the same birthday? which of these two questions is easiest to solve? which answer gives most insight? you may it helpful to solve these problems and those that follow using notation such as a number of days in year and s number of people exercise the birthday problem may be related to a coding scheme. assume we wish to convey a message to an outsider identifying one of figure some more copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions the twenty-four people. we could simply communicate a number s from as having agreed a mapping of people onto numbers alternatively we could convey a number from ax identifying the day of the year that is the selected person s birthday apologies to leapyearians. receiver is assumed to know all the people s birthdays. what roughly is the probability of error of this communication scheme assuming it is used for a single transmission? what is the capacity of the communication channel and what is the rate of communication attempted by this scheme? exercise now imagine that there are k rooms in a building each containing q people. might think of k and q as an example. the aim is to communicate a selection of one person from each room by transmitting an ordered list of k days ax. compare the probability of error of the following two schemes. as before where each room transmits the birthday of the selected person. to each k-tuple of people one drawn from each room an ordered k-tuple of randomly selected days from ax is assigned ktuple has nothing to do with their birthdays. this enormous list of s qk strings is known to the receiver. when the building has selected a particular person from each room the ordered string of days corresponding to that k-tuple of people is transmitted. what is the probability of error when q and k what is the probability of error when q and k is large e.g. k solutions solution to exercise if we assume we observe y p y p x p j solution to exercise if we observe y p y solution to exercise the probability that y is so the mutual information is ix y hy hy j x bits solution to exercise we again compute the mutual information using ix y hy hy j x. the probability that y is and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over a noisy channel hy j x p j x p j x p j x so the mutual information is ix y hy hy j x bits solution to exercise by symmetry the optimal input distribution is then the capacity is c ix y hy hy j x would you like to the optimal input distribution without invoking symmetry? we can do this by computing the mutual information in the general case where the input ensemble is ix y hy hy j x f the only p-dependence is in the term f which is maximized by setting the argument to this value is given by setting solution to exercise answer by symmetry the optimal input distribution is the capacity is most easily evaluated by writing the mutual information as ix y hx hx j y the conditional entropy hx j y is py p j y when y is known x is uncertain only if y which occurs with probability f f so the conditional entropy hx j y is f c ix y hx hx j y f f the binary erasure channel fails a fraction f of the time. its capacity is precisely f which is the fraction of the time that the channel is reliable. this result seems very reasonable but it is far from obvious how to encode information so as to communicate reliably over this channel. answer alternatively without invoking the symmetry assumed above we can start from the input ensemble the probability that y is f and when we receive y the posterior probability of x is the same as the prior probability so ix y hx hx j y f f this mutual information achieves its maximum value of when copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions q n n the extended channel solution to exercise is shown in the best code for this channel with n is obtained by choosing two columns that have minimal overlap for example columns and the decoding algorithm returns if the extended channel output is among the top four and if it s among the bottom four and gives up if the output is solution to exercise mutual information between input and output of the z channel is in example we showed that the ix y hy hy j x f we this expression with respect to taking care not to confuse with loge d ix y f f f setting this derivative to zero and rearranging using skills developed in exercise we obtain f so the optimal input distribution is f as the noise level f tends to this expression tends to you can prove using l hopital s rule. for all values of f is smaller than a rough intuition for why input is used less than input is that when input is used the noisy channel injects entropy into the received string whereas when input is used the noise has zero entropy. solution to exercise the capacities of the three channels are shown in for any f the bec is the channel with highest capacity and the bsc the lowest. solution to exercise the logarithm of the posterior probability ratio given y is ay ln p y p y ln qy j x qy j x figure the extended channel obtained from a binary erasure channel with erasure probability a block code consisting of the two codewords and the optimal decoder for this code. z bsc bec figure capacities of the z channel binary symmetric channel and binary erasure channel. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over a noisy channel using our skills picked up from exercise we rewrite this in the form p y the optimal decoder selects the most probable hypothesis this can be done simply by looking at the sign of ay. if ay then decode as the probability of error is pb dy qy j x dy random coding solution to exercise the probability that s people whose birthdays are drawn at random from a days all have distinct birthdays is aa s as the probability that two more people share a birthday is one minus this quantity which for s and a is about this exact way of answering the question is not very informative since it is not clear for what value of s the probability changes from being close to to being close to the number of pairs is ss and the probability that a particular pair shares a birthday is so the expected number of collisions is ss a this answer is more instructive. the expected number of collisions is tiny if s pa and big if s pa. we can also approximate the probability that all birthdays are distinct for small s thus aa s as exp i! ss a a copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter before reading chapter you should have read chapters and exercise is especially recommended. cast of characters q c x n c n xs s the noisy channel the capacity of the channel an ensemble used to create a random code a random code the length of the codewords a codeword the sth in the code the number of a chosen codeword selects s the total number of codewords in the code the source s k s the number of bits conveyed by the choice of one codeword s r kn from s assuming it is chosen with uniform probability a binary representation of the number s the rate of the code in bits per channel use called instead the decoder s guess of s copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. rpb r c figure portion of the r pb plane to be proved achievable and not achievable the noisy-channel coding theorem the theorem the theorem has three parts two positive and one negative. the main positive result is the for every discrete memoryless channel the channel capacity pb c max px ix y has the following property. for any and r c for large enough n there exists a code of length n and rate r and a decoding algorithm such that the maximal probability of block error is if a probability of bit error pb is acceptable rates up to rpb are achiev able where rpb c for any pb rates greater than rpb are not achievable. jointly-typical sequences we formalize the intuitive preview of the last chapter. we will codewords xs as coming from an ensemble x n and consider the random selection of one codeword and a corresponding channel output y thus a joint ensemble we will use a typical-set decoder which decodes a received signal y as s if xs and y are jointly typical a term to be shortly. the proof will then centre on determining the probabilities that the true input codeword is not jointly typical with the output sequence and that a false input codeword is jointly typical with the output. we will show that for large n both probabilities go to zero as long as there are fewer than c codewords and the ensemble x is the optimal input distribution. joint typicality. a pair of sequences x y of length n are to be jointly typical tolerance with respect to the distribution p y if x is typical of p i.e. y is typical of p i.e. and x y is typical of p y i.e. log log log n n n p p hy p y hx y copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. jointly-typical sequences the jointly-typical set jn is the set of all jointly-typical sequence pairs of length n example. here is a jointly-typical pair of length n for the ensemble p y in which p has and p j x corresponds to a binary symmetric channel with noise level x y notice that x has and so is typical of the probability p any tolerance and y has so it is typical of p p and x and y in bits which is the typical number of for this channel. joint typicality theorem. let x y be drawn from the ensemble by then p y p yn n the probability that x y are jointly typical tolerance tends to as n to be precise the number of jointly-typical sequences jjn is close to hxy if x n and y n i.e. and are independent samples with the same marginal distribution as p y then the probability that lands in the jointly-typical set is about ixy to be precise jjn p jn proof. the proof of parts and by the law of large numbers follows that of the source coding theorem in chapter for part let the pair x y play the role of x in the source coding theorem replacing p there by the probability distribution p y. for the third part p jn p jjn a cartoon of the jointly-typical set is shown in two independent typical vectors are jointly typical with probability p jn because the total number of independent typical pairs is the area of the dashed rectangle hy and the number of jointly-typical pairs is roughly hxy so the probability of hitting a jointly-typical pair is roughly hxy hxn hy ixy copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the noisy-channel coding theorem y the set of x the set of all input figure the jointly-typical set. the horizontal direction represents an strings of length n the vertical direction represents an all output strings of length n the outer box contains all conceivable inputoutput pairs. each dot represents a jointly-typical pair of sequences y. the total number of jointly-typical sequences is about hxy an x hx hxy dots qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq hy jx hxjy hy an y proof of the noisy-channel coding theorem analogy imagine that we wish to prove that there is a baby in a class of one hundred babies who weighs less than kg. individual babies are to catch and weigh. shannon s method of solving the task is to scoop up all the babies and weigh them all at once on a big weighing machine. if we that their average weight is smaller than kg there must exist at least one baby who weighs less than kg indeed there must be many! shannon s method isn t guaranteed to reveal the existence of an underweight child since it relies on there being a tiny number of elephants in the class. but if we use his method and get a total weight smaller than kg then our task is solved. from skinny children to fantastic codes we wish to show that there exists a code and a decoder having small probability of error. evaluating the probability of error of any particular coding and decoding system is not easy. shannon s innovation was this instead of constructing a good coding and decoding system and evaluating its error probability shannon calculated the average probability of block error of all codes and proved that this average is small. there must then exist individual codes that have small probability of block error. random coding and typical-set decoding consider the following encodingdecoding system whose rate is we p and generate the s codewords of a n figure shannon s method for proving one baby weighs less than kg. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. proof of the noisy-channel coding theorem qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq ya yb yd yc qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqq k code c at random according to p n p figure a random code. example decodings by the typical set decoder. a sequence that is not jointly typical with any of the codewords such as ya is decoded as a sequence that is jointly typical with codeword alone yb is decoded as similarly yc is decoded as a sequence that is jointly typical with more than one codeword such as yd is decoded as a random code is shown schematically in the code is known to both sender and receiver. a message s is chosen from and xs is transmitted. the received signal is y with p j xs n p j xs n the signal is decoded by typical-set decoding. typical-set decoding. decode y as if y are jointly typical and there is no other such that y are jointly typical otherwise declare a failure this is not the optimal decoding algorithm but it will be good enough and easier to analyze. the typical-set decoder is illustrated in a decoding error occurs if s. there are three probabilities of error that we can distinguish. first there is the probability of block error for a particular code c that is pbc p sjc this is a quantity to evaluate for any given code. second there is the average over all codes of this block error probability hpbi p sjcp copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the noisy-channel coding theorem hpbi is just the probability that there is a decoding error at step of the process on the previous page. fortunately this quantity is much easier to evaluate than the quantity p sjc. third the maximal block error probability of a code c pbmc max s p sj sc is the quantity we are most interested in we wish to show that there exists a code c with the required rate whose maximal block error probability is small. we will get to this result by the average block error probability hpbi. once we have shown that this can be made smaller than a desired small number we immediately deduce that there must exist at least one code c whose block error probability is also less than this small number. finally we show that this code whose block error probability is satisfactorily small but whose maximal block error probability is unknown could conceivably be enormous can be to make a code of slightly smaller rate whose maximal block error probability is also guaranteed to be small. we modify the code by throwing away the worst of its codewords. we therefore now embark on the average probability of block error. probability of error of typical-set decoder there are two sources of error when we use typical-set decoding. either the output y is not jointly typical with the transmitted codeword xs or there is some other codeword in c that is jointly typical with y. by the symmetry of the code construction the average probability of error averaged over all codes does not depend on the selected value of s we can assume without loss of generality that s the probability that the input and the output y are not jointly typical vanishes by the joint typicality theorem s part we give a name to the upper bound on this probability satisfying as n for any desired we can a blocklength n such that the p y jn the probability that and y are jointly typical for a given is by part and there are rival values of to worry about. thus the average probability of error hpbi hpbi the inequality that bounds a total probability of error ptot by the sum of the probabilities of all sorts of events each of which is to cause error is called a union bound. it is only an equality if the events that cause error never occur at the same time as each other. ptot the average probability of error can be made by increasing n if ix y we are almost there. we make three we choose p in the proof to be the optimal input distribution of the channel. then the condition ix y becomes c copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication errors above capacity figure how expurgation works. in a typical random code a small fraction of the codewords are involved in collisions pairs of codewords are close to each other that the probability of error when either codeword is transmitted is not tiny. we obtain a new code from a random code by deleting all these confusable codewords. the resulting code has slightly fewer codewords so has a slightly lower rate and its maximal probability of error is greatly reduced. pb achievable r c figure portion of the r pb plane proved achievable in the part of the theorem. ve proved that the maximal probability of block error pbm can be made arbitrarily small so the same goes for the bit error probability pb which must be smaller than pbm. a random code expurgated since the average probability of error over all codes is there must exist a code with mean probability of block error pbc to show that not only the average but also the maximal probability of error pbm can be made small we modify this code by throwing away the worst half of the codewords the ones most likely to produce errors. those that remain must all have conditional probability of error less than we use these remaining codewords to a new code. this new code has codewords i.e. we have reduced the rate from to negligible reduction if n is large and achieved pbm this trick is called expurgation the resulting code may not be the best code of its rate and length but it is still good enough to prove the noisy-channel coding theorem which is what we are trying to do here. in conclusion we can construct a code of rate where c with maximal probability of error we obtain the theorem as stated by setting and n large for the remaining conditions to hold. the theorem s part is thus proved. communication errors above capacity we have proved for any discrete memoryless channel the achievability of a portion of the r pb plane shown in we have shown that we can turn any noisy channel into an essentially noiseless binary channel with rate up to c bits per cycle. we now extend the right-hand boundary of the region of achievability at non-zero error probabilities. is called rate-distortion theory. we do this with a new trick. since we know we can make the noisy channel into a perfect channel with a smaller rate it is to consider communication with errors over a noiseless channel. how fast can we communicate over a noiseless channel if we are allowed to make errors? consider a noiseless binary channel and assume that we force communication at a rate greater than its capacity of bit. for example if we require the sender to attempt to communicate at r bits per cycle then he must throw away half of the information. what is the best way to do this if the aim is to achieve the smallest possible probability of bit error? one simple strategy is to communicate a fraction of the source bits and ignore the rest. the receiver guesses the missing fraction at random and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the noisy-channel coding theorem optimum simple pb r figure a simple bound on achievable points pb and shannon s bound. the average probability of bit error is pb the curve corresponding to this strategy is shown by the dashed line in the risk of corruption evenly among all the bits. pb can this optimum be achieved? we can do better than this terms of minimizing pb by spreading out in fact we can achieve which is shown by the solid curve in so how we reuse a tool that we just developed namely the k code for a noisy channel and we turn it on its head using the decoder to a lossy compressor. we take an excellent k code for the binary symmetric channel. assume that such a code has a rate kn and that it is capable of correcting errors introduced by a binary symmetric channel whose transition probability is q. asymptotically codes exist that have recall that if we attach one of these capacity-achieving codes of length n to a binary symmetric channel then the probability distribution over the outputs is close to uniform since the entropy of the output is equal to the entropy of the source plus the entropy of the noise and the optimal decoder of the code in this situation typically maps a received vector of length n to a transmitted vector in qn bits from the received vector. we take the signal that we wish to send and chop it into blocks of length n n not k. we pass each block through the decoder and obtain a shorter signal of length k bits which we communicate over the noiseless channel. to decode the transmission we pass the k bit message to the encoder of the original code. the reconstituted message will now from the original message in some of its bits typically qn of them. so the probability of bit error will be pb q. the rate of this lossy compressor is r nk now attaching this lossy compressor to our capacity-c error-free communicator we have proved the achievability of communication up to the curve r by r c for further reading about rate-distortion theory see gallager p. or mceliece p. the non-achievable region of the theorem the source encoder noisy channel and decoder a markov chain s x y p x y p sp j xp j y the data processing inequality must apply to this chain is ix y furthermore by the of channel capacity ix y n c so is n c. assume that a system achieves a rate r and a bit error probability pb then the mutual information is is n but is n c is not achievable so r is not achievable. c exercise fill in the details in the preceding argument. if the bit errors between and s are independent then we have is n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. computing capacity sections contain advanced material. the reader is encouraged to skip to section what if we have complex correlations among those bit errors? why does the inequality is n hold? computing capacity we have proved that the capacity of a channel is the maximum rate at which reliable communication can be achieved. how can we compute the capacity of a given discrete memoryless channel? we need to its optimal input distribution. in general we can the optimal input distribution by a computer search making use of the derivative of the mutual information with respect to the input probabilities. exercise find the derivative of ix y with respect to the input probability pi y for a channel with conditional probabilities qjji. exercise show that ix y is a concave function of the input prob ability vector p. since ix y is concave in the input distribution p any probability distribution p at which ix y is stationary must be a global maximum of ix y so it is tempting to put the derivative of ix y into a routine that a local maximum of ix y that is an input distribution p such that y for all i where is a lagrange multiplier associated with the constraint pi pi however this approach may fail to the right answer because ix y might be maximized by a distribution that has pi for some inputs. a simple example is given by the ternary confusion channel. ternary confusion channel. ax ay p x p x p x p x p x p x whenever the input is used the output is random the other inputs are reliable inputs. the maximum information rate of bit is achieved by making no use of the input exercise sketch the mutual information for this channel as a function of the input distribution p. pick a convenient two-dimensional representation of p. the optimization routine must therefore take account of the possibility that as we go up hill on ix y we may run into the inequality constraints pi exercise describe the condition similar to equation that is at a point where ix y is maximized and describe a computer program for the capacity of a channel. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the noisy-channel coding theorem results that may help in the optimal input distribution all outputs must be used. ix y is a convex function of the channel parameters. there may be several optimal input distributions but they all look the same at the output. exercise prove that no output y is unused by an optimal input distri reminder the term convex means convex and the term concave means concave the little smile and frown symbols are included simply to remind you what convex and concave mean. bution unless it is unreachable that is has qy j x for all x. exercise prove that ix y is a convex function of qy j x. exercise prove that all optimal input distributions of a channel have the same output probability distribution p p j x. these results along with the fact that ix y is a concave function of the input probability vector p prove the validity of the symmetry argument that we have used when the capacity of symmetric channels. if a channel is invariant under a group of symmetry operations for example interchanging the input symbols and interchanging the output symbols then given any optimal input distribution that is not symmetric i.e. is not invariant under these operations we can create another input distribution by averaging together this optimal input distribution and all its permuted forms that we can make by applying the symmetry operations to the original optimal input distribution. the permuted distributions must have the same ix y as the original by symmetry so the new input distribution created by averaging must have ix y bigger than or equal to that of the original distribution because of the concavity of i. symmetric channels in order to use symmetry arguments it will help to have a of a symmetric channel. i like gallager s a discrete memoryless channel is a symmetric channel if the set of outputs can be partitioned into subsets in such a way that for each subset the matrix of transition probabilities has the property that each row more than is a permutation of each other row and each column is a permutation of each other column. example this channel p x p x p x p x p x p x is a symmetric channel because its outputs can be partitioned into and so that the matrix can be rewritten p x p x p x p x p x p x copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. other coding theorems symmetry is a useful property because as we will see in a later chapter communication at capacity can be achieved over symmetric channels by linear codes. exercise prove that for a symmetric channel with any number of inputs the uniform distribution over the inputs is an optimal input distribution. exercise are there channels that are not symmetric whose optimal input distributions are uniform? find one or prove there are none. other coding theorems the noisy-channel coding theorem that we proved in this chapter is quite general applying to any discrete memoryless channel but it is not very the theorem only says that reliable communication with error probability and rate r can be achieved by using codes with large blocklength n the theorem does not say how large n needs to be to achieve given values of r and presumably the smaller is and the closer r is to c the larger n has to be. err c r figure a typical random-coding exponent. noisy-channel coding theorem version with explicit n for a discrete memoryless channel a blocklength n and a rate r there exist block codes of length n whose average probability of error pb exp err where err is the random-coding exponent of the channel a convex decreasing positive function of r for r c. the random-coding exponent is also known as the reliability function. an expurgation argument it can also be shown that there exist block codes for which the maximal probability of error pbm is also exponentially small in n the of err is given in gallager p. err approaches zero as r c the typical behaviour of this function is illustrated in the computation of the random-coding exponent for interesting channels is a challenging task on which much has been expended. even for simple channels like the binary symmetric channel there is no simple expression for err. lower bounds on the error probability as a function of blocklength the theorem stated above asserts that there are codes with pb smaller than exp err. but how small can the error probability be? could it be much smaller? for any code with blocklength n on a discrete memoryless channel the probability of error assuming all source messages are used with equal probability pb espr copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the noisy-channel coding theorem where the function espr the sphere-packing exponent of the channel is a convex decreasing positive function of r for r c. for a precise statement of this result and further references see gallager p. noisy-channel coding theorems and coding practice imagine a customer who wants to buy an error-correcting code and decoder for a noisy channel. the results described above allow us to the following service if he tells us the properties of his channel the desired rate r and the desired error probability pb we can after working out the relevant functions c err and espr advise him that there exists a solution to his problem using a particular blocklength n indeed that almost any randomly chosen code with that blocklength should do the job. unfortunately we have not found out how to implement these encoders and decoders in practice the cost of implementing the encoder and decoder for a random code with large n would be exponentially large in n furthermore for practical purposes the customer is unlikely to know exactly what channel he is dealing with. so berlekamp suggests that the sensible way to approach error-correction is to design encoding-decoding systems and plot their performance on a variety of idealized channels as a function of the channel s noise level. these charts of which is illustrated on page can then be shown to the customer who can choose among the systems on without having to specify what he really thinks his channel is like. with this attitude to the practical problem the importance of the functions err and espr is diminished. further exercises exercise a binary erasure channel with input x and output y has transition probability matrix q q q q q find the mutual information ix y between the input and output for general input distribution and show that the capacity of this channel is c q bits. a z channel has transition probability matrix q q q show that using a code two uses of a z channel can be made to emulate one use of an erasure channel and state the erasure probability of that erasure channel. hence show that the capacity of the z channel cz cz q bits. explain why the result cz equality. q is an inequality rather than an copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions exercise a transatlantic cable contains n indistinguishable electrical wires. you have the job of out which wire is which that is to create a consistent labelling of the wires at each end. your only tools are the ability to connect wires to each other in groups of two or more and to test for connectedness with a continuity tester. what is the smallest number of transatlantic trips you need to make and how do you do it? how would you solve the problem for larger n such as n as an illustration if n were then the task can be solved in two steps by labelling one wire at one end a connecting the other two together crossing the atlantic measuring which two wires are connected labelling them b and c and the unconnected one a then connecting b to a and returning across the atlantic whereupon on disconnecting b from c the identities of b and c can be deduced. this problem can be solved by persistent search but the reason it is posed in this chapter is that it can also be solved by a greedy approach based on maximizing the acquired information. let the unknown permutation of wires be x. having chosen a set of connections of wires c at one end you can then make measurements at the other end and these measurements y convey information about x. how much? and for what set of connections is the information that y conveys about x maximized? solutions solution to exercise the mutual information is if the input distribution is p p? ix y hy hy jx p? we can build a good sketch of this function in two ways by careful inspection of the function or by looking at special cases. for the plots the two-dimensional representation of p i will use has and as the independent variables so that p p? if we use the quantities and p? as our two by inspection. degrees of freedom the mutual information becomes very simple ix y p?. converting back to and we obtain the sketch shown at the left below. this function is like a tunnel rising up the direction of increasing and to obtain the required plot of ix y we have to strip away the parts of this tunnel that live outside the feasible simplex of probabilities we do this by redrawing the surface showing only the parts where and a full plot of the function is shown at the right. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the noisy-channel coding theorem figure skeleton of the mutual information for the ternary confusion channel. special cases. channel and ix y in the special case p? the channel is a noiseless binary in the special case the term is equal to so ix y p?. we know how to sketch that from the previous chapter in the special case the channel is a z channel with error probability these special cases allow us to construct the skeleton shown in solution to exercise necessary and conditions for p to maximize ix y are and pi and pi for all i where is a constant related to the capacity by c e. this result can be used in a computer program that evaluates the derivatives and increments and decrements the probabilities pi in proportion to the between those derivatives. this result is also useful for lazy human who are good guessers. having guessed the optimal input distribution one can simply that equation holds. solution to exercise we certainly expect nonsymmetric channels with uniform optimal input distributions to exist since when inventing a channel we have ij degrees of freedom whereas the optimal input distribution is just so in the ij space of perturbations around a symmetric channel we expect there to be a subspace of perturbations of dimension ij ij that leave the optimal input distribution unchanged. here is an explicit example a bit like a z channel. q solution to exercise the labelling problem can be solved for any n with just two trips one each way across the atlantic. the key step in the information-theoretic approach to this problem is to write down the information content of one partition the combinatorial object that is the connecting together of subsets of wires. if n wires are grouped together into subsets of size subsets of size then the number of such partitions is n gr! yr and the information content of one such partition is the log of this quantity. in a greedy strategy we choose the partition to maximize this information content. one game we can play is to maximize this information content with respect to the quantities gr treated as real numbers subject to the constraint introducing a lagrange multiplier for the constraint the pr grr n derivative is log grr! log r! log gr copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions which when set to zero leads to the rather nice expression gr r! the optimal gr is proportional to a poisson distribution! we can solve for the n gives the implicit equation lagrange multiplier by plugging gr into the constraint pr grr n which where is a convenient reparameterization of the lagrange multiplier. figure shows a graph of shows the deduced noninteger assignments gr when and nearby integers gr that motivate setting the partition to this partition produces a random partition at the other end which has an information content of log bits which is a lot more than half the total information content we need to acquire to infer the transatlantic permutation log bits. contrast if all the wires are joined together in pairs the information content generated is only about bits. how to choose the second partition is left to the reader. a shannonesque approach is appropriate picking a random partition at the other end using the same fgrg you need to ensure the two partitions are as unlike each other as possible. if n or then the labelling problem has solutions that are particularly simple to implement called knowltongraham partitions partition ng into disjoint sets in two ways a and b subject to the condition that at most one element appears both in an a set of cardinality j and in a b set of cardinality k for each j and k graham and knowlton figure approximate solution of the cable-labelling problem using lagrange multipliers. the parameter as a function of n the value is highlighted. non-integer values of the function gr are shown by lines and integer values of gr motivated by those non-integer values are shown by crosses. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter before reading chapter you should have read chapters and you will also need to be familiar with the gaussian distribution. one-dimensional gaussian distribution. if a random variable y is gaus sian and has mean and variance which we write y or p normaly then the distribution of y is p j use the symbol p for both probability densities and probabilities. the inverse-variance is sometimes called the precision of the gaussian distribution. multi-dimensional gaussian distribution. if y yn has a multivariate gaussian distribution then p j x a za xtay where x is the mean of the distribution a is the inverse of the variancecovariance matrix and the normalizing constant is za this distribution has the property that the variance of yi and the covariance of yi and yj are given by e ij where is the inverse of the matrix a. the marginal distribution p of one component yi is gaussian the joint marginal distribution of any subset of the components is multivariate-gaussian and the conditional density of any subset given the values of another subset for example p j yj is also gaussian. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes real channels the noisy-channel coding theorem that we have proved shows that there exist reliable error-correcting codes for any noisy channel. in this chapter we address two questions. first many practical channels have real rather than discrete inputs and outputs. what can shannon tell us about these continuous channels? and how should digital signals be mapped into analogue waveforms and vice versa? second how are practical error-correcting codes made and what is achieved in practice relative to the possibilities proved by shannon? the gaussian channel the most popular model of a real-input real-output channel is the gaussian channel. the gaussian channel has a real input x and a real output y. the condi tional distribution of y given x is a gaussian distribution p j x this channel has a continuous input and output but is discrete in time. we will show below that certain continuous-time channels are equivalent to the discrete-time gaussian channel. this channel channel. is sometimes called the additive white gaussian noise as with discrete channels we will discuss what rate of error-free information communication can be achieved over this channel. motivation in terms of a continuous-time channel consider a physical say channel with inputs and outputs that are continuous in time. we put in xt and out comes yt xt nt. our transmission has a power cost. the average power of a transmission of length t may be constrained thus z t dt p the received signal is assumed to from xt by additive noise nt example johnson noise which we will model as white gaussian noise. the magnitude of this noise is by the noise spectral density copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes and real channels how could such a channel be used to communicate information? consider in a signal of duration t made transmitting a set of n real numbers fxngn up of a weighted combination of orthonormal basis functions xt n where r t dt the receiver can then compute the scalars yn z t dt xn t xn nn dt for n n if there were no noise then yn would equal xn. the white gaussian noise nt adds scalar noise nn to the estimate yn. this noise is gaussian nn where is the spectral density introduced above. thus a continuous channel used in this way is equivalent to the gaussian channel at dt p t a constraint on tion the power constraint r t the signal amplitudes xn xt figure three basis functions and a weighted combination of them xt with and n p t xn n p t n before returning to the gaussian channel we the bandwidth sured in hertz of the continuous channel to be w n max where n max is the maximum number of orthonormal functions that can be produced in an interval of length t this can be motivated by imagining creating a band-limited signal of duration t from orthonormal cosine and sine curves of maximum frequency w the number of orthonormal functions is n max t this relates to the nyquist sampling theorem if the highest frequency present in a signal is w then the signal can be fully determined from its values at a series of discrete sample points separated by the nyquist interval seconds. so the use of a real continuous channel with bandwidth w noise spectral density and power p is equivalent to nt uses per second of a gaussian channel with noise level and subject to the signal power constraint n of imagine that the gaussian channel yn xn nn is used with an encoding system to transmit binary source bits at a rate of r bits per channel use. how can we compare two encoding systems that have rates of communication r and that use powers n? transmitting at a large rate r is good using small power is good too. it is conventional to measure the rate-compensated signal-to-noise ratio by nr to the noise spectral density the ratio of the power per source bit eb n is one of the measures used to compare coding schemes for gaussian channels. is dimensionless but it is usually reported in the units of decibels the value given is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. inferring the input to a real channel inferring the input to a real channel the best detection of pulses in shannon wrote a memorandum on the problem of best between two types of pulses of known shape represented by vectors and given that one of them has been transmitted over a noisy channel. this is a pattern recognition problem. it is assumed that the noise is gaussian with probability density p a where a is the inverse of the variancecovariance matrix of the noise a symmetric and matrix. a is a multiple of the identity matrix then the noise is white for more general a the noise is coloured the probability of the received vector y given that the source signal was s zero or one is then p j s a xstay the optimal detector is based on the posterior probability ratio p p p y p y exp p j s p j s ln p p where is a constant independent of the received vector y xt xt ln p p if the detector is forced to make a decision guess either s or s then the decision that minimizes the probability of error is to guess the most probable hypothesis. we can write the optimal decision in terms of a discriminant function with the decisions ay ay guess s ay guess s ay guess either. notice that ay is a linear function of the received vector ay wty where w capacity of gaussian channel until now we have measured the joint marginal and conditional entropy of discrete variables only. in order to the information conveyed by continuous variables there are two issues we must address the length of the real line and the precision of real numbers. y figure two pulses and represented as vectors and a noisy version of one of them y. w figure the weight vector w that is used to discriminate between and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. inputs error-correcting codes and real channels figure a probability density p question can we the entropy of this density? we could evaluate the entropies of a sequence of probability distributions with decreasing grain-size g but these entropies tend to z p log p independent of g the entropy goes up by one bit for every halving of g. dx which is not z p log integral. p dx is an illegal how much information can we convey in one use of a gaussian channel? if we are allowed to put any real number x into the gaussian channel we could communicate an enormous string of n digits dn by setting x dn the amount of error-free information conveyed in just a single transmission could be made arbitrarily large by increasing n and the communication could be made arbitrarily reliable by increasing the number of zeroes at the end of x. there is usually some power cost associated with large inputs however not to mention practical limits in the dynamic range acceptable to a receiver. it is therefore conventional to introduce a cost function vx for every input x and constrain codes to have an average cost less than or equal to some maximum value. a generalized channel coding theorem including a cost function for the inputs can be proved see mceliece the result is a channel capacity that is a function of the permitted cost. for the gaussian channel we will assume a cost vx such that the average power of the input is constrained. we motivated this cost function above in the case of real electrical channels in which the physical power consumption is indeed quadratic in x. the constraint makes it impossible to communicate information in one use of the gaussian channel. precision it is tempting to joint marginal and conditional entropies for real variables simply by replacing summations by integrals but this is not a well operation. as we discretize an interval into smaller and smaller divisions the entropy of the discrete distribution diverges the logarithm of the granularity also it is not permissible to take the logarithm of a dimensional quantity such as a probability density p dimensions are there is one information measure however that has a well-behaved limit namely the mutual information and this is the one that really matters since it measures how much information one variable conveys about another. in the discrete case ix y p y log p y p now because the argument of the log is a ratio of two probabilities over the same space it is ok to have p y p and p be probability densities and replace the sum by an integral ix y z dx dy p y log p y p z dx dy p j x log p j x p we can now ask these questions for the gaussian channel what probability distribution p maximizes the mutual information to the constraint v? and does the maximal mutual information still measure the maximum error-free communication rate of this real channel as it did for the discrete channel? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. capacity of gaussian channel exercise prove that the probability distribution p that maximizes the mutual information to the constraint v is a gaussian distribution of mean zero and variance v. exercise show that the mutual information ix y in the case of this optimized distribution is c v this is an important result. we see that the capacity of the gaussian channel is a function of the signal-to-noise ratio inferences given a gaussian input distribution if p normalx v and p j x normaly x then the marginal distribution of y is p normaly and the posterior distribution of the input given that the output is y is p y p j xp normal x v y v v step from to is made by completing the square in the exponent. this formula deserves careful study. the mean of the posterior distribution y can be viewed as a weighted combination of the value that best the output x y and the value that best the prior x v v v y y the weights and are the precisions of the two gaussians that we multiplied together in equation the prior and the likelihood. the precision of the posterior distribution is the sum of these two precisions. this is a general property whenever two independent sources contribute information via gaussian distributions about an unknown variable the precisions add. is the dual to the better-known relationship when independent variables are added their variances add noisy-channel coding theorem for the gaussian channel we have evaluated a maximal mutual information. does it correspond to a maximum possible rate of error-free information transmission? one way of proving that this is so is to a sequence of discrete channels all derived from the gaussian channel with increasing numbers of inputs and outputs and prove that the maximum mutual information of these channels tends to the asserted c. the noisy-channel coding theorem for discrete channels applies to each of these derived channels thus we obtain a coding theorem for the continuous channel. alternatively we can make an intuitive argument for the coding theorem for the gaussian channel. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes and real channels geometrical view of the noisy-channel coding theorem sphere packing consider a sequence x xn of inputs and the corresponding output y as two points in an n dimensional space. for large n the noise power is very likely to be close to n the output y is therefore very likely to be close to the surface of a sphere of radius pn centred on x. similarly if the original signal x is generated at random subject to an average power constraint v then x is likely to lie close to a sphere centred on the origin of radius pn v and because the total average power of y is v the received signal y is likely to lie on the surface of a sphere of radius pn centred on the origin. the volume of an n sphere of radius r is v n rn now consider making a communication system based on non-confusable inputs x that is inputs whose spheres do not overlap the maximum number s of non-confusable inputs is given by dividing the volume of the sphere of probable ys by the volume of the sphere for y given x s pn pn thus the capacity is bounded by c n log m v a more detailed argument like the one used in the previous chapter can establish equality. back to the continuous channel recall that the use of a real continuous channel with bandwidth w noise spectral density and power p is equivalent to nt uses per second of a gaussian channel with and subject to the constraint n substituting the result for the capacity of the gaussian channel we the capacity of the continuous channel to be c w p bits per second. this formula gives insight into the of practical communication. imagine that we have a power constraint. what is the best bandwidth to make use of that power? introducing i.e. the bandwidth for which the signal-to-noise ratio is shows as a function of the capacity increases to an asymptote of log e. it is dramatically better terms of capacity for power to transmit at a low signal-to-noise ratio over a large bandwidth than with high signal-to-noise in a narrow bandwidth this is one motivation for wideband communication methods such as the direct sequence spread-spectrum approach used in mobile phones. of course you are not alone and your electromagnetic neighbours may not be pleased if you use a large bandwidth so for social reasons engineers often have to make do with higher-power narrow-bandwidth transmitters. y t i c a p a c bandwidth figure capacity versus bandwidth for a real channel log as a function of copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. what are the capabilities of practical error-correcting codes? what are the capabilities of practical error-correcting codes? nearly all codes are good but nearly all codes require exponential look-up tables for practical implementation of the encoder and decoder exponential in the blocklength n and the coding theorem required n to be large. by a practical error-correcting code we mean one that can be encoded and decoded in a reasonable amount of time for example a time that scales as a polynomial function of the blocklength n preferably linearly. the shannon limit is not achieved in practice the non-constructive proof of the noisy-channel coding theorem showed that good block codes exist for any noisy channel and indeed that nearly all block codes are good. but writing down an explicit and practical encoder and decoder that are as good as promised by shannon is still an unsolved problem. very good codes. given a channel a family of block codes that achieve arbitrarily small probability of error at any communication rate up to the capacity of the channel are called very good codes for that channel. good codes are code families that achieve arbitrarily small probability of error at non-zero communication rates up to some maximum rate that may be less than the capacity of the given channel. bad codes are code families that cannot achieve arbitrarily small probability of error or that can achieve arbitrarily small probability of error only by decreasing the information rate to zero. repetition codes are an example of a bad code family. codes are not necessarily useless for practical purposes. practical codes are code families that can be encoded and decoded in time and space polynomial in the blocklength. most established codes are linear codes let us review the of a block code and then add the of a linear block code. an k block code for a channel q is a list of s codewords each of length n xs an x. the signal to be encoded s which comes from an alphabet of size is encoded as xs. a linear k block code is a block code in which the codewords fxsg make up a k-dimensional subspace of an x. the encoding operation can be represented by an n k binary matrix gt such that if the signal to be encoded in binary notation is s vector of length k bits then the encoded signal is t gts modulo the codewords ftg can be as the set of vectors satisfying ht mod where h is the parity-check matrix of the code. for example the hamming code of section takes k signal bits s and transmits them followed by three parity-check bits. the n transmitted symbols are given by gts mod coding theory was born with the work of hamming who invented a family of practical error-correcting codes each able to correct one error in a block of length n of which the repetition code and the code are gt copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes and real channels the simplest. since then most established codes have been generalizations of hamming s codes bosechaudhuryhocquenhem codes codes reedsolomon codes and goppa codes to name a few. convolutional codes another family of linear codes are convolutional codes which do not divide the source stream into blocks but instead read and transmit bits continuously. the transmitted bits are a linear function of the past source bits. usually the rule for generating the transmitted bits involves feeding the present source bit into a linear-feedback shift-register of length k and transmitting one or more linear functions of the state of the shift register at each iteration. the resulting transmitted bit stream is the convolution of the source stream with a linear the impulse-response function of this may have or duration depending on the choice of feedback shift-register. we will discuss convolutional codes in chapter are linear codes good one might ask is the reason that the shannon limit is not achieved in practice because linear codes are inherently not as good as random codes? the answer is no the noisy-channel coding theorem can still be proved for linear codes at least for some channels chapter though the proofs like shannon s proof for random codes are non-constructive. linear codes are easy to implement at the encoding end. is decoding a linear code also easy? not necessarily. the general decoding problem the maximum likelihood s in the equation gts n r is in fact np-complete et al. problems are computational problems that are all equally and which are widely believed to require exponential computer time to solve in general. so attention focuses on families of codes for which there is a fast decoding algorithm. concatenation one trick for building codes with practical decoders is the idea of concatenation. an encoderchanneldecoder system c q d can be viewed as a super-channel with a smaller probability of error and with complex correlations among its errors. we can create an encoder and decoder for this super-channel the code consisting of the outer code followed by the inner code c is known as a concatenated code. some concatenated codes make use of the idea of interleaving. we read the data in blocks the size of each block being larger than the blocklengths of the constituent codes c and after encoding the data of one block using code the bits are reordered within the block in such a way that nearby bits are separated from each other once the block is fed to the second code c. a simple example of an interleaver is a rectangular code or product code in which the data are arranged in a block and encoded horizontally using an linear code then vertically using a linear code. exercise show that either of the two codes can be viewed as the inner code or the outer code. as an example shows a product code in which we encode with the repetition code known as the hamming code c q d copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. what are the capabilities of practical error-correcting codes? figure a product code. a string encoded using a concatenated code consisting of two hamming codes and a noise pattern that bits. the received vector. after decoding using the horizontal decoder and after subsequently using the vertical decoder. the decoded vector matches the original. after decoding in the other order three errors still remain. horizontally then with vertically. the blocklength of the concatenated code is the number of source bits per codeword is four shown by the small rectangle. we can decode conveniently not optimally by using the individual decoders for each of the subcodes in some sequence. it makes most sense to decode the code which has the lowest rate and hence the greatest errorcorrecting ability. figure shows what happens if we receive the codeword of with some errors bits as shown and apply the decoder for and then the decoder for the decoder corrects three of the errors but erroneously the third bit in the second row where there are two bit errors. the decoder can then correct all three of these errors. figure shows what happens if we decode the two codes in the other order. in columns one and two there are two errors so the decoder introduces two extra errors. it corrects the one error in column the decoder then cleans up four of the errors but erroneously infers the second bit. interleaving the motivation for interleaving is that by spreading out bits that are nearby in one code we make it possible to ignore the complex correlations among the errors that are produced by the inner code. maybe the inner code will mess up an entire codeword but that codeword is spread out one bit at a time over several codewords of the outer code. so we can treat the errors introduced by the inner code as if they are independent. other channel models in addition to the binary symmetric channel and the gaussian channel coding theorists keep more complex channels in mind also. burst-error channels are important models in practice. reedsolomon codes use galois appendix with large numbers of elements as their input alphabets and thereby automatically achieve a degree of burst-error tolerance in that even if successive bits are corrupted only successive symbols in the galois representation are corrupted. concatenation and interleaving can give further protection against burst errors. the concatenated reedsolomon codes used on digital compact discs are able to correct bursts of errors of length bits. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes and real channels exercise the technique of interleaving which allows bursts of errors to be treated as independent is widely used but is theoretically a poor way to protect data against burst errors in terms of the amount of redundancy required. explain why interleaving is a poor method using the following burst-error channel as an example. time is divided into chunks of length n clock cycles during each chunk there is a burst with probability b during a burst the channel is a binary symmetric channel with f if there is no burst the channel is an error-free binary channel. compute the capacity of this channel and compare it with the maximum communication rate that could conceivably be achieved if one used interleaving and treated the errors as independent. fading channels are real channels like gaussian channels except that the received power is assumed to vary with time. a moving mobile phone is an important example. the incoming radio signal is nearby objects so that there are interference patterns and the intensity of the signal received by the phone varies with its location. the received power can easily vary by decibels factor of ten as the phone s antenna moves through a distance similar to the wavelength of the radio signal few centimetres. the state of the art what are the best known codes for communicating over gaussian channels? all the practical codes are linear codes and are either based on convolutional codes or block codes. convolutional codes and codes based on them textbook convolutional codes. the de facto standard error-correcting code for satellite communications is a convolutional code with constraint length convolutional codes are discussed in chapter concatenated convolutional codes. the above convolutional code can be used as the inner code of a concatenated code whose outer code is a reed solomon code with eight-bit symbols. this code was used in deep space communication systems such as the voyager spacecraft. for further reading about reedsolomon codes see lin and costello the code for galileo. a code using the same format but using a longer constraint length for its convolutional code and a larger reed solomon code was developed by the jet propulsion laboratory the details of this code are unpublished outside jpl and the decoding is only possible using a room full of special-purpose hardware. in this was the best code known of rate turbo codes. in berrou glavieux and thitimajshima reported work on turbo codes. the encoder of a turbo code is based on the encoders of two convolutional codes. the source bits are fed into each encoder the order of the source bits being permuted in a random way and the resulting parity bits from each constituent code are transmitted. the decoding algorithm involves iteratively decoding each constituent code using its standard decoding algorithm then using the output of the decoder as the input to the other decoder. this decoding algorithm figure the encoder of a turbo code. each box contains a convolutional code. the source bits are reordered using a permutation before they are fed to the transmitted codeword is obtained by concatenating or interleaving the outputs of the two convolutional codes. the random permutation is chosen when the code is designed and thereafter. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. summary is an instance of a message-passing algorithm called the sumproduct algorithm. turbo codes are discussed in chapter and message passing in chapters and h block codes gallager s low-density parity-check codes. the best block codes known for gaussian channels were invented by gallager in but were promptly forgotten by most of the coding theory community. they were rediscovered in and shown to have outstanding theoretical and practical properties. like turbo codes they are decoded by message-passing algorithms. we will discuss these beautifully simple codes in chapter the performances of the above codes are compared for gaussian channels in summary random codes are good but they require exponential resources to encode and decode them. non-random codes tend for the most part not to be as good as random codes. for a non-random code encoding may be easy but even for linear codes the decoding problem remains very the best practical codes employ very large block sizes are based on semi-random code constructions and make use of probabilitybased decoding algorithms. nonlinear codes figure a low-density parity-check matrix and the corresponding graph of a low-density parity-check code with blocklength n and m constraints. each white circle represents a transmitted bit. each bit participates in j constraints represented by squares. each constraint forces the sum of the k bits to which it is connected to be even. this code is a code. outstanding performance is obtained when the blocklength is increased to n most practically used codes are linear but not all. digital soundtracks are encoded onto cinema as a binary pattern. the likely errors the involve dirt and scratches which produce large numbers of and respectively. we want none of the codewords to look like or so that it will be easy to detect errors caused by dirt and scratches. one of the codes used in digital cinema sound systems is a nonlinear code consisting of of the binary patterns of weight errors other than noise another source of uncertainty for the receiver is uncertainty about the timing of the transmitted signal xt. in ordinary coding theory and information theory the transmitter s time t and the receiver s time u are assumed to be perfectly synchronized. but if the receiver receives a signal yu where the receiver s time u is an imperfectly known function ut of the transmitter s time t then the capacity of this channel for communication is reduced. the theory of such channels is incomplete compared with the synchronized channels we have discussed thus far. not even the capacity of channels with synchronization errors is known ferreira et al. codes for reliable communication over channels with synchronization errors remain an active research area and mackay copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further reading error-correcting codes and real channels for a review of the history of spread-spectrum methods see scholtz exercises the gaussian channel exercise consider a gaussian channel with a real input x and signal to noise ratio what is its capacity c? if the input is constrained to be binary x what is the capacity of this constrained channel? if in addition the output of the channel is thresholded using the mapping y y y what is the capacity of the resulting channel? plot the three capacities above as a function of from to ll need to do a numerical integral to evaluate exercise for large integers k and n what fraction of all binary errorcorrecting codes of length n and rate r kn are linear codes? answer will depend on whether you choose to the code to be an ordered list of codewords that is a mapping from s to xs or to the code to be an unordered list so that two codes consisting of the same codewords are identical. use the latter a code is a set of codewords how the encoder operates is not part of the of the code. erasure channels exercise design a code for the binary erasure channel and a decoding algorithm and evaluate their probability of error. design of good codes for erasure channels is an active research area byers et al. see also chapter exercise design a code for the q-ary erasure channel whose input x is drawn from and whose output y is equal to x with probability f and equal to otherwise. erasure channel is a good model for packets transmitted over the internet which are either received reliably or are lost. exercise how do redundant arrays of independent disks work? these are information storage systems consisting of about ten disk drives of which any two or three can be disabled and the others are able to still able to reconstruct any requested what codes are used and how far are these systems from the shannon limit for the problem they are solving? how would you design a better raid system? some information is provided in the solution section. see httpwww.acnc. see also chapter people say raid stands for redundant array of inexpensive disks but i think that s silly raid would still be a good idea even if the disks were expensive! copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solutions solution to exercise introduce a lagrange multiplier for the power constraint and another for the constraint of normalization of p f ix y dx p dx p z dx p dy p j x ln p p j x make the functional derivative with respect to p z dy p j ln p j p dx p dy p j x p the factor is found using p dx p j x to be p j and the whole of the last term collapses in a of smoke to which can be absorbed into the term. substitute p j x and set the derivative to zero p j x p ln this condition must be by lnp for all x. dy writing a taylor expansion of lnp abycy only a quadratic function lnp a would satisfy the constraint higher order terms yp p would produce terms in xp that are not present on the right-hand side. therefore p is gaussian. we can obtain this optimal output distribution by using a gaussian input distribution p z dy p j x ln solution to exercise given a gaussian input distribution of variance v the output distribution is v since x and the noise are independent random variables and variances add for independent random variables. the mutual information is ix y z dx dy p j x log p j x dy p log p log v v log solution to exercise the capacity of the channel is one minus the information content of the noise that it adds. that information content is per chunk the entropy of the selection of whether the chunk is bursty plus with probability b the entropy of the bits n which adds up to n b per chunk accurate if n is large. so per bit the capacity is for n c n in contrast interleaving which treats bursts of errors as independent causes the channel to be treated as a binary symmetric channel with f whose capacity is about copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. error-correcting codes and real channels interleaving throws away the useful information about the correlatedness of the errors. theoretically we should be able to communicate about times faster using a code and decoder that explicitly treat bursts as bursts. solution to exercise figure capacities top to bottom in each graph c c and c versus the signal-to-noise ratio the lower graph is a loglog plot. c v putting together the results of exercises and we deduce that a gaussian channel with real input x and signal to noise ratio has capacity if the input is constrained to be binary x the capacity is achieved by using these two inputs with equal probability. the capacity is reduced to a somewhat messy integral dy n log n dy p log p where n x expy x and p x n this capacity is smaller than the unconstrained capacity but for small signal-to-noise ratio the two capacities are close in value. if the output is thresholded then the gaussian channel is turned into a binary symmetric channel whose transition probability is given by the error function on page the capacity is where f solution to exercise there are several raid systems. one of the easiest to understand consists of disk drives which store data at rate using a hamming code each successive four bits are encoded with the code and the seven codeword bits are written one to each disk. two or perhaps three disk drives can go down and the others can recover the data. the channel model here is a binary erasure channel because it is assumed that we can tell when a disk is dead. it is not possible to recover the data for some choices of the three dead disk drives can you see why? exercise give an example of three disk drives that if lost lead to failure of the above raid system and three that can be lost without failure. solution to exercise the hamming code has codewords of weight if any set of three disk drives corresponding to one of those codewords is lost then the other four disks can recover only bits of information about the four source bits a fourth bit is lost. exercise with q there are no binary mds codes. this is discussed further in section any other set of three disk drives can be lost without problems because the corresponding four by four submatrix of the generator matrix is invertible. a better code would be a digital fountain see chapter copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. part iii further topics in information theory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter in chapters we concentrated on two aspects of information theory and coding theory source coding the compression of information so as to make use of data transmission and storage channels and channel coding the redundant encoding of information so as to be able to detect and correct communication errors. in both these areas we started by ignoring practical considerations concentrating on the question of the theoretical limitations and possibilities of coding. we then discussed practical source-coding and channel-coding schemes shifting the emphasis towards computational feasibility. but the prime criterion for comparing encoding schemes remained the of the code in terms of the channel resources it required the best source codes were those that achieved the greatest compression the best channel codes were those that communicated at the highest rate with a given probability of error. in this chapter we now shift our viewpoint a little thinking of ease of information retrieval as a primary goal. it turns out that the random codes which were theoretically useful in our study of channel coding are also useful for rapid information retrieval. information retrieval is one of the problems that brains seem to solve and content-addressable memory is one of the topics we will study when we look at neural networks. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. string length number of strings number of possible strings n s figure cast of characters. hash codes codes for information retrieval the information-retrieval problem a simple example of an information-retrieval problem is the task of implementing a phone directory service which in response to a person s name returns a that that person is listed in the directory and the person s phone number and other details. we could formalize this problem as follows with s being the number of names that must be stored in the directory. you are given a list of s binary strings of length n bits xsg where s is considerably smaller than the total number of possible strings we will call the superscript s in xs the record number of the string. the idea is that s runs over customers in the order in which they are added to the directory and xs is the name of customer s. we assume for simplicity that all people have names of the same length. the name length might be say n bits and we might want to store the details of ten million customers so s we will ignore the possibility that two customers have identical names. the task is to construct the inverse of the mapping from s to xs i.e. to make a system that given a string x returns the value of s such that x xs if one exists and otherwise reports that no such s exists. we have the record number we can go and look in memory location s in a separate memory full of phone numbers to the required number. the aim when solving this task is to use minimal computational resources in terms of the amount of memory used to store the inverse mapping from x to s and the amount of time to compute the inverse mapping. and preferably the inverse mapping should be implemented in such a way that further new strings can be added to the directory in a small amount of computer time too. some standard solutions the simplest and dumbest solutions to the information-retrieval problem are a look-up table and a raw list. the look-up table is a piece of memory of size s s being the amount of memory required to store an integer between and s. in each of the locations we put a zero except for the locations x that correspond to strings xs into which we write the value of s. the look-up table is a simple and quick solution but only if there is memory for the table and if the cost of looking up entries in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. hash codes codes for information retrieval memory is independent of the memory size. but in our of the task we assumed that n is about bits or more so the amount of memory required would be of size this solution is completely out of the question. bear in mind that the number of particles in the solar system is only about the raw list is a simple list of ordered pairs xs ordered by the value of s. the mapping from x to s is achieved by searching through the list of strings starting from the top and comparing the incoming string x with each record xs until a match is found. this system is very easy to maintain and uses a small amount of memory about sn bits but is rather slow to use since on average million pairwise comparisons will be made. exercise show that the average time taken to the required string in a raw list assuming that the original names were chosen at random is about s n binary comparisons. that you don t have to compare the whole string of length n since a comparison can be terminated as soon as a mismatch occurs show that you need on average two binary comparisons per incorrect string match. compare this with the worst-case search time assuming that the devil chooses the set of strings and the search key. the standard way in which phone directories are made improves on the look-up table and the raw list by using an alphabetically-ordered list. alphabetical list. the strings fxsg are sorted into alphabetical order. searching for an entry now usually takes less time than was needed for the raw list because we can take advantage of the sortedness for example we can open the phonebook at its middle page and compare the name we there with the target string if the target is greater than the middle string then we know that the required string if it exists will be found in the second half of the alphabetical directory. otherwise we look in the half. by iterating this splitting-in-the-middle procedure we can identify the target string or establish that the string is not listed in se string comparisons. the expected number of binary comparisons per string comparison will tend to increase as the search progresses but the total number of binary comparisons required will be no greater than sen the amount of memory required is the same as that required for the raw list. adding new strings to the database requires that we insert them in the correct location in the list. to that location takes about dlog se binary comparisons. can we improve on the well-established alphabetized list? let us consider our task from some new viewpoints. the task is to construct a mapping x s from n bits to log s bits. this is a pseudo-invertible mapping since for any x that maps to a non-zero s the customer database contains the pair xs that takes us back. where have we come across the idea of mapping from n bits to m bits before? we encountered this idea twice in source coding we studied block codes which were mappings from strings of n symbols to a selection of one label in a list. the task of information retrieval is similar to the task copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. hash codes string length number of strings size of hash function m bits size of hash table n s t figure revised cast of characters. we never actually solved of making an encoder for a typical-set compression code. the second time that we mapped bit strings to bit strings of another dimensionality was when we studied channel codes. there we considered codes that mapped from k bits to n bits with n greater than k and we made theoretical progress using random codes. in hash codes we put together these two notions. we will study random codes that map from n bits to m bits where m is smaller than n the idea is that we will map the original high-dimensional space down into a lower-dimensional space one in which it is feasible to implement the dumb look-up table method which we rejected a moment ago. hash codes first we will describe how a hash code works then we will study the properties of idealized hash codes. a hash code implements a solution to the informationretrieval problem that is a mapping from x to s with the help of a pseudorandom function called a hash function which maps the n string x to an m string hx where m is smaller than n m is typically chosen such that the table size t is a little bigger than s say ten times bigger. for example if we were expecting s to be about a million we might map x into a hash h of the size n of each item x. the hash function is some deterministic function which should ideally be indistinguishable from a random code. for practical purposes the hash function must be quick to compute. two simple examples of hash functions are division method. the table size t is a prime number preferably one that is not close to a power of the hash value is the remainder when the integer x is divided by t variable string addition method. this method assumes that x is a string of bytes and that the table size t is the characters of x are added modulo this hash function has the defect that it maps strings that are anagrams of each other onto the same hash. it may be improved by putting the running total through a pseudorandom permutation after each character is added. in the variable string exclusive-or method with table size the string is hashed twice in this way with the initial running total being set to and respectively the result is a hash. having picked a hash function hx we implement an information retriever as follows. encoding. a piece of memory called the hash table is created of size b memory units where b is the amount of memory needed to represent an integer between and s. this table is initially set to zero throughout. each memory xs is put through the hash function and at the location in the hash table corresponding to the resulting vector hs hxs the integer s is written unless that entry in the hash table is already occupied in which case we have a collision between xs and some earlier which both happen to have the same hash code. collisions can be handled in various ways we will discuss some in a moment but let us complete the basic picture. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. hash codes codes for information retrieval algorithm c code implementing the variable string exclusive-or method to create a hash h in the range from a string x. author thomas niemann. unsigned char this array contains a random int hashchar int h unsigned char permutation from to x is a pointer to the first char is the first character if return x while special handling of empty string initialize two hashes proceed to the next character exclusive-or with the two hashes x and put through the randomizer h end of string is reached when shift left bits and add return h hash is concatenation of and figure use of hash functions for information retrieval. for each string xs the hash h hxs is computed and the value of s is written into the hth row of the hash table. blank rows in the hash table contain the value zero. the table size is t strings hash function hashes hash table bits n bits xs s a a a a a au hxs s copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. collision resolution decoding. to retrieve a piece of information corresponding to a target vector x we compute the hash h of x and look at the corresponding location in the hash table. if there is a zero then we know immediately that the string x is not in the database. the cost of this answer is the cost of one hash-function evaluation and one look-up in the table of size if on the other hand there is a non-zero entry s in the table there are two possibilities either the vector x is indeed equal to xs or the vector xs is another vector that happens to have the same hash code as the target x. third possibility is that this non-zero entry might have something to do with our yet-to-be-discussed collision-resolution system. to check whether x is indeed equal to xs we take the tentative answer s look up xs in the original forward database and compare it bit by bit with x if it matches then we report s as the desired answer. this successful retrieval has an overall cost of one hash-function evaluation one look-up in the table of size another look-up in a table of size s and n binary comparisons which may be much cheaper than the simple solutions presented in section exercise if we have checked the few bits of xs with x and found them to be equal what is the probability that the correct entry has been retrieved if the alternative hypothesis is that x is actually not in the database? assume that the original source strings are random and the hash function is a random hash function. how many binary evaluations are needed to be sure with odds of a billion to one that the correct entry has been retrieved? the hashing method of information retrieval can be used for strings x of arbitrary length if the hash function hx can be applied to strings of any length. collision resolution we will study two ways of resolving collisions appending in the table and storing elsewhere. appending in table when encoding if a collision occurs we continue down the hash table and write the value of s into the next available location in memory that currently contains a zero. if we reach the bottom of the table before encountering a zero we continue from the top. when decoding if we compute the hash code for x and that the s contained in the table doesn t point to an xs that matches the cue x we continue down the hash table until we either an s whose xs does match the cue x in which case we are done or else encounter a zero in which case we know that the cue x is not in the database. for this method it is essential that the table be substantially bigger in size than s. if s then the encoding rule will become stuck with nowhere to put the last strings. storing elsewhere a more robust and method is to use pointers to additional pieces of memory in which collided strings are stored. there are many ways of doing copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. hash codes codes for information retrieval this. as an example we could store in location h in the hash table a pointer must be distinguishable from a valid record number s to a bucket where all the strings that have hash code h are stored in a sorted list. the encoder sorts the strings in each bucket alphabetically as the hash table and buckets are created. the decoder simply has to go and look in the relevant bucket and then check the short list of strings that are there by a brief alphabetical search. this method of storing the strings in buckets allows the option of making the hash table quite small which may have practical we may make it so small that almost all strings are involved in collisions so all buckets contain a small number of strings. it only takes a small number of binary comparisons to identify which of the strings in the bucket matches the cue x. planning for collisions a birthday problem exercise if we wish to store s entries using a hash function whose output has m bits how many collisions should we expect to happen assuming that our hash function is an ideal random function? what size m of hash table is needed if we would like the expected number of collisions to be smaller than what size m of hash table is needed if we would like the expected number of collisions to be a small fraction say of s? the similarity of this problem to exercise other roles for hash codes checking arithmetic if you wish to check an addition that was done by hand you may useful the method of casting out nines. in casting out nines one the sum modulo nine of all the digits of the numbers to be summed and compares it with the sum modulo nine of the digits of the putative answer. a little practice these sums can be computed much more rapidly than the full original addition. example in the calculation shown in the margin the sum modulo nine of the digits in is and the sum modulo nine of is the calculation thus passes the casting-out-nines test. casting out nines gives a simple example of a hash function. for any addition expression of the form a b c where a b c are decimal numbers we h by ha b c sum modulo nine of all digits in a b c then it is nice property of decimal arithmetic that if a b c m n o then the hashes ha b c and hm n o are equal. exercise what evidence does a correct casting-out-nines match give in favour of the hypothesis that the addition has been done correctly? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. other roles for hash codes error detection among friends are two the same? if the are on the same computer we could just compare them bit by bit. but if the two are on separate machines it would be nice to have a way of that two are identical without having to transfer one of the from a to b. even if we did transfer one of the we would still like a way to whether it has been received without this problem can be solved using hash codes. let alice and bob be the holders of the two alice sent the to bob and they wish to it has been received without error. if alice computes the hash of her and sends it to bob and bob computes the hash of his using the same m hash function and the two hashes match then bob can deduce that the two are almost surely the same. example what is the probability of a false negative i.e. the probability given that the two do that the two hashes are nevertheless identical? if we assume that the hash function is random and that the process that causes the to knows nothing about the hash function then the probability of a false negative is a hash gives a probability of false negative of about it is common practice to use a linear hash function called a cyclic redundancy check to detect errors in cyclic redundancy check is a set of paritycheck bits similar to the parity-check bits of the hamming code. to have a false-negative rate smaller than one in a billion m bits is plenty if the errors are produced by noise. exercise such a simple parity-check code only detects errors it doesn t help correct them. since error-correcting codes exist why not use one of them to get some error-correcting capability too? tamper detection what if the between the two are not simply noise but are introduced by an adversary a clever forger called fiona who the original to make a forgery that purports to be alice s how can alice make a digital signature for the so that bob can that no-one has tampered with the and how can we prevent fiona from listening in on alice s signature and attaching it to other let s assume that alice computes a hash function for the and sends it securely to bob. if alice computes a simple hash function for the like the linear cyclic redundancy check and fiona knows that this is the method of verifying the s integrity fiona can make her chosen to the and then easily identify linear algebra a further single bits that when restore the hash function of the to its original value. linear hash functions give no security against forgers. we must therefore require that the hash function be hard to invert so that no-one can construct a tampering that leaves the hash function we would still like the hash function to be easy to compute however so that bob doesn t have to do hours of work to verify every he received. such a hash function easy to compute but hard to invert is called a one-way copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. hash codes codes for information retrieval hash function. finding such functions is one of the active research areas of cryptography. a hash function that is widely used in the free software community to that two do not is which produces a hash. the details of how it works are quite complicated involving convoluted exclusiveor-ing and if-ing and even with a good one-way hash function the digital signatures described above are still vulnerable to attack if fiona has access to the hash function. fiona could take the tampered and hunt for a further tiny to it such that its hash matches the original hash of alice s this would take some time on average about attempts if the hash function has bits but eventually fiona would a tampered that matches the given hash. to be secure against forgery digital signatures must either have enough bits for such a random search to take too long or the hash function itself must be kept secret. fiona has to hash to cheat. is not very many so a hash function is not large enough for forgery prevention. another person who might have a motivation for forgery is alice herself. for example she might be making a bet on the outcome of a race without wishing to broadcast her prediction publicly a method for placing bets would be for her to send to bob the bookie the hash of her bet. later on she could send bob the details of her bet. everyone can that her bet is consistent with the previously publicized hash. method of secret publication was used by isaac newton and robert hooke when they wished to establish priority for ideas without revealing them. hooke s hash function was alphabetization as illustrated by the conversion of ut tensio sic vis into the anagram ceiiinosssttuv. such a protocol relies on the assumption that alice cannot change her bet after the event without the hash coming out wrong. how big a hash function do we need to use to ensure that alice cannot cheat? the answer is from the size of the hash we needed in order to defeat fiona above because alice is the author of both alice could cheat by searching for two that have identical hashes to each other. for example if she d like to cheat by placing two bets for the price of one she could make a large number of versions of bet one from each other in minor details only and a large number of versions of bet two and hash them all. if there s a collision between the hashes of two bets of types then she can submit the common hash and thus buy herself the option of placing either bet. example if the hash has m bits how big do and need to be for alice to have a good chance of two bets with the same hash? this is a birthday problem like exercise if there are montagues and capulets at a party and each is assigned a birthday of m bits the expected number of collisions between a montague and a capulet is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises so to minimize the number of hashed alice should make and equal and will need to hash about until she two that match. alice has to hash to cheat. is the square root of the number of hashes fiona had to make. if alice has the use of c computers for t years each computer taking t ns to evaluate a hash the bet-communication system is secure against alice s dishonesty only if m ct bits. further reading the bible for hash codes is volume of knuth i highly recommend the story of doug mcilroy s spell program as told in section of programming pearls this astonishing piece of software makes use of a data structure to store the spellings of all the words of dictionary. further exercises exercise what is the shortest the address on a typical international letter could be if it is to get to a unique human recipient? the permitted characters are how long are typical email addresses? exercise how long does a piece of text need to be for you to be pretty sure that no human has written that string of characters before? how many notes are there in a new melody that has not been composed before? exercise pattern recognition by molecules. some proteins produced in a cell have a regulatory role. a regulatory protein controls the transcription of genes in the genome. this control often involves the protein s binding to a particular dna sequence in the vicinity of the regulated gene. the presence of the bound protein either promotes or inhibits transcription of the gene. use information-theoretic arguments to obtain a lower bound on the size of a typical protein that acts as a regulator to one gene in the whole human genome. assume that the genome is a sequence of nucleotides drawn from a four letter alphabet fa c g tg a protein is a sequence of amino acids drawn from a twenty letter alphabet. establish how long the recognized dna sequence has to be in order for that sequence to be unique to the vicinity of one gene treating the rest of the genome as a random sequence. then discuss how big the protein must be to recognize a sequence of that length uniquely. some of the sequences recognized by dna-binding regulatory proteins consist of a subsequence that is repeated twice or more for example the sequence gccccccacccctgccccc copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. hash codes codes for information retrieval is a binding site found upstream of the alpha-actin gene in humans. does the fact that some binding sites consist of a repeated subsequence your answer to part solutions solution to exercise first imagine comparing the string x with another random string xs. the probability that the bits of the two strings match is the probability that the second bits match is assuming we stop comparing once we hit the mismatch the expected number of matches is so the expected number of comparisons is assuming the correct string is located at random in the raw list we will have to compare with an average of strings before we it which costs binary comparisons and comparing the correct strings takes n binary comparisons giving a total expectation of s n binary comparisons if the strings are chosen at random. in the worst case may indeed happen in practice the other strings are very similar to the search key so that a lengthy sequence of comparisons is needed to each mismatch. the worst case is when the correct string is last in the list and all the other strings in the last bit only giving a requirement of sn binary comparisons. solution to exercise the likelihood ratio for the two hypotheses xs x and xs x contributed by the datum the bits of xs and x are equal is p p if the r bits all match the likelihood ratio is to one. on that bits match the odds are a billion to one in favour of assuming we start from even odds. a complete answer we should compute the evidence given by the prior information that the hash entry s has been found in the table at hx. this fact gives further evidence in favour of solution to exercise let the hash function have an output alphabet of size t if m were equal to s then we would have exactly enough bits for each entry to have its own unique hash. the probability that one particular pair of entries collide under a random hash function is the number of pairs is ss so the expected number of collisions between pairs is exactly ss if we would like this to be smaller than then we need t ss so m s we need twice as many bits as the number of bits log s that would be to give each entry a unique name. if we are happy to have occasional collisions involving a fraction f of the names s then we need t sf the probability that one particular name is collided-with is f st so m s copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions which means for f that we need an extra bits above s. the important point to note is the scaling of t with s in the two cases if we want the hash function to be collision-free then we must have t greater than if we are happy to have a small frequency of collisions then t needs to be of order s only. solution to exercise the posterior probability ratio for the two hypotheses h calculation correct and calculation incorrect is the product of the prior probability ratio p and the likelihood ratio p jhp this second factor is the answer to the question. the numerator p jh is equal to the denominator s value depends on our model of errors. if we know that the human calculator is prone to errors involving multiplication of the answer by or to transposition of adjacent digits neither of which the hash value then p could be equal to also so that the correct match gives no evidence in favour of h. but if we assume that errors are random from the point of view of the hash function then the probability of a false positive is p and the correct match gives evidence in favour of h. solution to exercise if you add a tiny m extra bits of hash to a huge n you get pretty good error detection the probability that an error is undetected is less than one in a billion. to do error correction requires far more check bits the number depending on the expected types of corruption and on the size. for example if just eight random bits in a megabyte are corrupted it would take about bits to specify which are the corrupted bits and the number of parity-check bits used by a successful error-correcting code would have to be at least this number by the counting argument of exercise solution to exercise we want to know the length l of a string such that it is very improbable that that string matches any part of the entire writings of humanity. let s estimate that these writings total about one book for each person living and that each book contains two million characters pages with characters per page that s characters drawn from an alphabet of say characters. the probability that a randomly chosen string of length l matches at one point in the collected works of humanity is so the expected number of matches is which is vanishingly small if l log because of the redundancy and repetition of humanity s writings it is possible that l is an overestimate. so if you want to write something unique sit down and compose a string of ten characters. but don t write gidnebinzz because i already thought of that string. as for a new melody if we focus on the sequence of notes ignoring duration and stress and allow leaps of up to an octave at each note then the number of choices per note is the pitch of the note is arbitrary. the number of melodies of length r notes in this rather ugly ensemble of tunes is for example there are of length r restricting the permitted intervals will reduce this including duration and stress will increase it again. we restrict the permitted intervals to repetitions and tones or semitones the reduction is particularly severe is this why the melody of ode to joy sounds so boring? the number of recorded compositions is probably less than a million. if you learn new melodies per week for every week of your life then you will have learned melodies at age based copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. hash codes codes for information retrieval in guess that tune one player chooses a melody and sings a gradually-increasing number of its notes while the other participants try to guess the whole melody. the parsons code is a related hash function for melodies each pair of consecutive notes is coded as u up if the second note is higher than the r repeat if the pitches are equal and d down otherwise. you can out how well this hash function works at httpmusipedia.org. on empirical experience of playing the game guess that tune it seems to me that whereas many four-note sequences are shared in common between melodies the number of collisions between sequences is rather smaller most famous sequences are unique. solution to exercise let the dna-binding protein recognize a sequence of length l nucleotides. that is it binds preferentially to that dna sequence and not to any other pieces of dna in the whole genome. reality the recognized sequence may contain some wildcard characters e.g. the in tataaa which denotes any of a c g and t so to be precise we are assuming that the recognized sequence contains l non-wildcard characters. assuming the rest of the genome is random i.e. that the sequence consists of random nucleotides a c g and t with equal probability which is obviously untrue but it shouldn t make too much to our calculation the chance that there is no other occurrence of the target sequence in the whole genome of length n nucleotides is roughly which is close to one only if that is n using n we require the recognized sequence to be longer than lmin nucleotides. l log n log what size of protein does this imply? a weak lower bound can be obtained by assuming that the information content of the protein sequence itself is greater than the information content of the nucleotide sequence the protein prefers to bind to we have argued above must be at least bits. this gives a minimum protein length of amino acids. thinking realistically the recognition of the dna sequence by the protein presumably involves the protein coming into contact with all sixteen nucleotides in the target sequence. if the protein is a monomer it must be big enough that it can simultaneously make contact with sixteen nucleotides of dna. one helical turn of dna containing ten nucleotides has a length of nm so a contiguous sequence of sixteen nucleotides has a length of nm. the diameter of the protein must therefore be about nm or greater. egg-white lysozyme is a small globular protein with a length of amino acids and a diameter of about nm. assuming that volume is proportional to sequence length and that volume scales as the cube of the diameter a protein of diameter nm must have a sequence of length amino acids. if however a target sequence consists of a twice-repeated sub-sequence we can get by with a much smaller protein that recognizes only the sub-sequence and that binds to the dna strongly only if it can form a dimer both halves of which are bound to the recognized sequence. halving the diameter of the protein we now only need a protein whose length is greater than amino acids. a protein of length smaller than this cannot by itself serve as a regulatory protein to one gene because it s simply too small to be able to make a match its available surface does not have enough information content. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter in chapters we established shannon s noisy-channel coding theorem for a general channel with any input and output alphabets. a great deal of attention in coding theory focuses on the special case of channels with binary inputs. constraining ourselves to these channels matters and leads us into an exceptionally rich world which we will only taste in this book. one of the aims of this chapter is to point out a contrast between shannon s aim of achieving reliable communication over a noisy channel and the apparent aim of many in the world of coding theory. many coding theorists take as their fundamental problem the task of packing as many spheres as possible with radius as large as possible into an n space with no spheres overlapping. prizes are awarded to people who packings that squeeze in an extra few spheres. while this is a fascinating mathematical topic we shall see that the aim of maximizing the distance between codewords in a code has only a tenuous relationship to shannon s aim of reliable communication. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes we ve established shannon s noisy-channel coding theorem for a general channel with any input and output alphabets. a great deal of attention in coding theory focuses on the special case of channels with binary inputs the implicit choice being the binary symmetric channel. the optimal decoder for a code given a binary symmetric channel the codeword that is closest to the received vector closest in hamming distance. the hamming distance between two binary vectors is the number of coordinates in which the two vectors decoding errors will occur if the noise takes us from the transmitted codeword t to a received vector r that is closer to some other codeword. the distances between codewords are thus relevant to the probability of a decoding error. distance properties of a code the distance of a code is the smallest separation between two of its codewords. example the hamming code has distance d all pairs of its codewords in at least bits. the maximum number of errors it can correct is t in general a code with distance d is a more precise term for distance is the minimum distance of the code. the distance of a code is often denoted by d or dmin. we ll now constrain our attention to linear codes. in a linear code all codewords have identical distance properties so we can summarize all the distances between the code s codewords by counting the distances from the all-zero codeword. the weight enumerator function of a code aw is to be the number of codewords in the code that have weight w. the weight enumerator function is also known as the distance distribution of the code. example the weight enumerator functions of the hamming code and the dodecahedron code are shown in and obsession with distance since the maximum number of errors that a code can guarantee to correct t is related to its distance d by t many coding theorists focus on the distance of a code searching for codes of a given size that have the biggest possible distance. much of practical coding theory has focused on decoders that give the optimal decoding for all error patterns of weight up to the half-distance t of their codes. example the hamming distance between and is w aw total figure the graph of the hamming code and its weight enumerator function. d if d is odd and d if d is even. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. obsession with distance w aw total figure the graph the dodecahedron code circles are the transmitted bits and the triangles are the parity checks one of which is redundant and the weight enumerator function lines. the dotted lines show the average weight enumerator function of all random linear codes with the same size of generator matrix which will be computed shortly. the lower shows the same functions on a log scale. a bounded-distance decoder is a decoder that returns the closest codeword to a received binary vector r if the distance from r to that codeword is less than or equal to t otherwise it returns a failure message. the rationale for not trying to decode when more than t errors have occurred might be we can t guarantee that we can correct more than t errors so we won t bother trying who would be interested in a decoder that corrects some error patterns of weight greater than t but not others? this defeatist attitude is an example of worst-case-ism a widespread mental ailment which this book is intended to cure. the fact is that bounded-distance decoders cannot reach the shannon limit of the binary symmetric channel only a decoder that often corrects more than t errors can do this. the state of the art in error-correcting codes have decoders that work way beyond the minimum distance of the code. of good and bad distance properties given a family of codes of increasing blocklength n and with rates approaching a limit r we may be able to put that family in one of the following categories which have some similarities to the categories of good and bad codes earlier a sequence of codes has good distance if dn tends to a constant greater than zero. a sequence of codes has bad distance if dn tends to zero. a sequence of codes has very bad distance if d tends to a constant. example a low-density generator-matrix code is a linear code whose n generator matrix g has a small number of per row regardless of how big n is. the minimum distance of such a code is at most so low-density generator-matrix codes have very bad distance. while having large distance is no bad thing we ll see later on why an emphasis on distance can be unhealthy. figure the graph of a low-density generator-matrix code. the rightmost m of the transmitted bits are each connected to a single distinct parity constraint. the leftmost k transmitted bits are each connected to a small number of parity constraints. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes figure schematic picture of part of hamming space perfectly by t-spheres centred on the codewords of a perfect code. t t t perfect codes a t-sphere a sphere of radius t in hamming space centred on a point x is the set of points whose hamming distance from x is less than or equal to t. the hamming code has the beautiful property that if we place about each of its codewords those spheres perfectly hamming space without overlapping. as we saw in chapter every binary vector of length is within a distance of t of exactly one codeword of the hamming code. a code is a perfect t-error-correcting code if the set of t-spheres centred on the codewords of the code the hamming space without overlapping. let s recap our cast of characters. the number of codewords is s the number of points in the entire hamming space is the number of points in a hamming sphere of radius t is t for a code to be perfect with these parameters we require s times the number of points in the t-sphere to equal for a perfect code or equivalently t t for a perfect code the number of noise vectors in one sphere must equal the number of possible syndromes. the hamming code this numerological condition because copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. perfect codes t t t t figure schematic picture of hamming space not perfectly by t-spheres centred on the codewords of a code. the grey regions show points that are at a hamming distance of more than t from any codeword. this is a misleading picture as for any code with large t in high dimensions the grey space between the spheres takes up almost all of hamming space. how happy we would be to use perfect codes if there were large numbers of perfect codes to choose from with a wide range of blocklengths and rates then these would be the perfect solution to shannon s problem. we could communicate over a binary symmetric channel with noise level f for example by picking a perfect t-error-correcting code with blocklength n and t f where f and n and are chosen such that the probability that the noise more than t bits is satisfactorily small. however there are almost no perfect codes. the only nontrivial perfect binary codes are the hamming codes which are perfect codes with t and blocklength n below the rate of a hamming code approaches as its blocklength n increases the repetition codes of odd blocklength n which are perfect codes with t the rate of repetition codes goes to zero as and one remarkable code with codewords of blocklength n known as the binary golay code. second golay code of length n over a ternary alphabet was discovered by a finnish football-pool enthusiast called juhani virtakallio in there are no other binary perfect codes. why this shortage of perfect codes? is it because precise numerological coincidences like those by the parameters of the hamming code and the golay code are rare? are there plenty of almost-perfect codes for which the t-spheres almost the whole space? no. in fact the picture of hamming spheres centred on the codewords almost hamming space is a misleading one for most codes whether they are good codes or bad codes almost all the hamming space is taken up by the space between t-spheres is shown in grey in having established this gloomy picture we spend a moment in the properties of the perfect codes mentioned above. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes figure three codewords. wn xn un vn n the hamming codes the hamming code can be as the linear code whose paritycheck matrix contains as its columns all the non-zero vectors of length since these vectors are all any single produces a distinct syndrome so all single-bit errors can be detected and corrected. we can generalize this code with m parity constraints as follows. the hamming codes are single-error-correcting codes by picking a number of parity-check constraints m the blocklength n is n the paritycheck matrix contains as its columns all the n non-zero vectors of length m bits. the few hamming codes have the following rates checks m k r kn repetition code hamming code exercise what is the probability of block error of the k hamming code to leading order when the code is used for a binary symmetric channel with noise density f perfectness is unattainable proof we will show in several ways that useful perfect codes do not exist useful means having large blocklength n and rate close neither to nor shannon proved that given a binary symmetric channel with any noise level f there exist codes with large blocklength n and rate as close as you like to cf that enable communication with arbitrarily small error probability. for large n the number of errors per block will typically be about fn so these codes of shannon are almost-certainly-fn codes. let s pick the special case of a noisy channel with f can we a large perfect code that is fn well let s suppose that such a code has been found and examine just three of its codewords. that the code ought to have rate r so it should have an enormous number r of codewords. without loss of generality we choose one of the codewords to be the all-zero codeword and the other two to have overlaps with it as shown in the second codeword from the in a fraction u v of its coordinates. the third codeword from the in a fraction v w and from the second in a fraction u w. a fraction x of the coordinates have value zero in all three codewords. now if the code is fn its minimum distance must be greater copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. weight enumerator function of random linear codes than so u v v w and u w summing these three inequalities and dividing by two we have u v w so if f we can deduce u v w so that x which is impossible. such a code cannot exist. so the code cannot have three codewords let alone r. we conclude that whereas shannon proved there are plenty of codes for communicating over a binary symmetric channel with f there are no perfect codes that can do this. we now study a more general argument that indicates that there are no large perfect linear codes for general rates than and we do this by the typical distance of a random linear code. weight enumerator function of random linear codes imagine making a code by picking the binary entries in the parity-check matrix h at random. what weight enumerator function should we expect? the weight enumerator of one particular code with parity-check matrix h awh is the number of codewords of weight w which can be written awh xxjxjw where the sum is over all vectors x whose weight is w and the truth function equals one if hx and zero otherwise. we can the expected value of aw n m figure a random binary parity-check matrix. hawi xh p xxjxjwxh p by evaluating the probability that a particular word of weight w is a codeword of the code over all binary linear codes in our ensemble. by symmetry this probability depends only on the weight w of the word not on the details of the word. the probability that the entire syndrome hx is zero can be found by multiplying together the probabilities that each of the m bits in the syndrome is zero. each bit zm of the syndrome is a sum of w random bits so the probability that zm is the probability that hx is thus p xh independent of w. the expected number of words of weight w is given by summing over all words of weight w the probability that each word is a codeword. the number of words of weight w is hawi so for any w copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes figure the expected weight enumerator function hawi of a random linear code with n and m lower shows hawi on a logarithmic scale. capacity r_gv f figure contrast between shannon s channel capacity c and the gilbert rate rgv the maximum communication rate achievable using a bounded-distance decoder as a function of noise level f for any given rate r the maximum tolerable noise level for shannon is twice as big as the maximum tolerable noise level for a worst-case-ist who uses a bounded-distance decoder. for large n we can use n and r mn to write n m n r for any w as a concrete example shows the expected weight enumerator function of a random linear code with n and m gilbertvarshamov distance for weights w such that r the expectation of aw is smaller than for weights such that r the expectation is greater than we thus expect for large n that the minimum distance of a random linear code will be close to the distance dgv by r this distance dgv n distance for rate r and blocklength n r is the gilbertvarshamov the gilbertvarshamov conjecture widely believed asserts that large n it is not possible to create binary codes with minimum distance greater than dgv. the gilbertvarshamov rate rgv is the maximum rate at which you can reliably communicate with a bounded-distance decoder on assuming that the gilbertvarshamov conjecture is true. why sphere-packing is a bad perspective and an obsession with distance is inappropriate if one uses a bounded-distance decoder the maximum tolerable noise level will a fraction fbd dminn of the bits. so assuming dmin is equal to the gilbert distance dgv we have rgv rgv now here s the crunch what did shannon say is achievable? he said the maximum possible rate of communication is the capacity c so for a given rate r the maximum tolerable noise level according to shannon is given by r our conclusion imagine a good code of rate r has been chosen equations and respectively the maximum noise levels tolerable by a bounded-distance decoder fbd and by shannon s decoder f fbd f bounded-distance decoders can only ever cope with half the noise-level that shannon proved is tolerable! how does this relate to perfect codes? a code is perfect if there are tspheres around its codewords that hamming space without overlapping. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. berlekamp s bats figure two overlapping spheres whose radius is almost as big as the distance between their centres. but when a typical random linear code is used to communicate over a binary symmetric channel near to the shannon limit the typical number of bits is fn and the minimum distance between codewords is also fn or a little bigger if we are a little below the shannon limit. so the fn around the codewords overlap with each other that each sphere almost contains the centre of its nearest neighbour! the reason why this overlap is not disastrous is because in high dimensions the volume associated with the overlap shown shaded in is a tiny fraction of either sphere so the probability of landing in it is extremely small. the moral of the story is that worst-case-ism can be bad for you halving your ability to tolerate noise. you have to be able to decode way beyond the minimum distance of a code to get to the shannon limit! nevertheless the minimum distance of a code is of interest in practice because under some conditions the minimum distance dominates the errors made by a code. berlekamp s bats a blind bat lives in a cave. it about the centre of the cave which corresponds to one codeword with its typical distance from the centre controlled by a friskiness parameter f displacement of the bat from the centre corresponds to the noise vector. the boundaries of the cave are made up of stalactites that point in towards the centre of the cave each stalactite is analogous to the boundary between the home codeword and another codeword. the stalactite is like the shaded region in but reshaped to convey the idea that it is a region of very small volume. decoding errors correspond to the bat s intended trajectory passing inside a stalactite. collisions with stalactites at various distances from the centre are possible. if the friskiness is very small the bat is usually very close to the centre of the cave collisions will be rare and when they do occur they will usually involve the stalactites whose tips are closest to the centre point. similarly under low-noise conditions decoding errors will be rare and they will typically involve low-weight codewords. under low-noise conditions the minimum distance of a code is relevant to the small probability of error. figure berlekamp s schematic picture of hamming space in the vicinity of a codeword. the jagged solid line encloses all points to which this codeword is the closest. the t-sphere around the codeword takes up a small fraction of this space. t copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes if the friskiness is higher the bat may often make excursions beyond the safe distance t where the longest stalactites start but it will collide most frequently with more distant stalactites owing to their greater number. there s only a tiny number of stalactites at the minimum distance so they are relatively unlikely to cause the errors. similarly errors in a real error-correcting code depend on the properties of the weight enumerator function. at very high friskiness the bat is always a long way from the centre of the cave and almost all its collisions involve contact with distant stalactites. under these conditions the bat s collision frequency has nothing to do with the distance from the centre to the closest stalactite. concatenation of hamming codes it is instructive to play some more with the concatenation of hamming codes a concept we visited in because we will get insights into the notion of good codes and the relevance or otherwise of the minimum distance of a code. we can create a concatenated code for a binary symmetric channel with noise density f by encoding with several hamming codes in succession. the table recaps the key properties of the hamming codes indexed by number of constraints m all the hamming codes have minimum distance d and can correct one error in n blocklength n k n m number of source bits pb probability of block error to leading order if we make a product code by concatenating a sequence of c hamming in such a codes with increasing m we can choose those parameters fmcgc way that the rate of the product code rc nc mc nc c tends to a non-zero limit as c increases. for example if we set etc. then the asymptotic rate is the blocklength n is a rapidly-growing function of c so these codes are somewhat impractical. a further weakness of these codes is that their minimum distance is not very good every one of the constituent hamming codes has minimum distance so the minimum distance of the cth product is the blocklength n grows faster than so the ratio dn tends to zero as c increases. in contrast for typical random codes the ratio dn tends to a constant such that r. concatenated hamming codes thus have bad distance. nevertheless it turns out that this simple sequence of codes yields good codes for some channels but not very good codes section to recall the of the terms good and very good rather than prove this result we will simply explore it numerically. figure shows the bit error probability pb of the concatenated codes assuming that the constituent codes are decoded in sequence as described in section one-code-at-a-time decoding is suboptimal as we saw there. the horizontal axis shows the rates of the codes. as the number of concatenations increases the rate drops to and the error probability drops towards zero. the channel assumed in the is the binary symmetric r c figure the rate r of the concatenated hamming code as a function of the number of concatenations c. c figure the blocklength nc curve and minimum distance dc curve of the concatenated hamming code as a function of the number of concatenations c. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. distance isn t everything channel with f this is the highest noise level that can be tolerated using this concatenated code. the take-home message from this story is distance isn t everything. the minimum distance of a code although widely worshipped by coding theorists is not of fundamental importance to shannon s mission of achieving reliable communication over noisy channels. pb r figure the bit error probabilities versus the rates r of the concatenated hamming codes for the binary symmetric channel with f labels alongside the points show the blocklengths n the solid line shows the shannon limit for this channel. the bit error probability drops to zero while the rate tends to so the concatenated hamming codes are a good code family. figure the error probability associated with a single codeword of weight d d f as a function of f exercise prove that there exist families of codes with bad distance that are very good codes. distance isn t everything let s get a quantitative feeling for the of the minimum distance of a code for the special case of a binary symmetric channel. the error probability associated with one low-weight codeword let a binary code have blocklength n and just two codewords which in d places. for simplicity let s assume d is even. what is the error probability if this code is used on a binary symmetric channel with noise level f bit matter only in places where the two codewords the error probability is dominated by the probability that of these bits are what happens to the other bits is irrelevant since the optimal decoder ignores them. p error d f this error probability associated with a single codeword of weight d is plotted in using the approximation for the binomial we can further approximate p error f where f is called the bhattacharyya parameter of the channel. now consider a general linear code with distance d. its block error probability must be at least d f independent of the blocklength n of the code. for this reason a sequence of codes of increasing blocklength n and constant distance d very bad distance cannot have a block error probability that tends to zero on any binary symmetric channel. if we are interested in making superb error-correcting codes with tiny tiny error probability we might therefore shun codes with bad distance. however being pragmatic we should look more carefully at in chapter we argued that codes for disk drives need an error probability smaller than about if the raw error probability in the disk drive is about the error probability associated with one codeword at distance d is smaller than if the raw error probability in the disk drive is about the error probability associated with one codeword at distance d is smaller than for practical purposes therefore it is not essential for a code to have good distance. for example codes of blocklength known to have many codewords of weight can nevertheless correct errors of weight with tiny error probability. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes i wouldn t want you to think i am recommending the use of codes with bad distance in chapter we will discuss low-density parity-check codes my favourite codes which have both excellent performance and good distance. the union bound the error probability of a code on the binary symmetric channel can be bounded in terms of its weight enumerator function by adding up appropriate multiples of the error probability associated with a single codeword p error this inequality which is an example of a union bound is accurate for low noise levels f but inaccurate for high noise levels because it overcounts the contribution of errors that cause confusion with more than one codeword at a time. exercise poor man s noisy-channel coding theorem. pretending that the union bound is accurate and using the average weight enumerator function of a random linear code as aw estimate the maximum rate rubf at which one can communicate over a binary symmetric channel. or to look at it more positively using the union bound as an inequality show that communication at rates up to rubf is possible over the binary symmetric channel. in the following chapter by analysing the probability of error of syndrome decoding for a binary linear code and using a union bound we will prove shannon s noisy-channel coding theorem symmetric binary channels and thus show that very good linear codes exist. dual codes a concept that has some importance in coding theory though we will have no immediate use for it in this book is the idea of the dual of a linear errorcorrecting code. an k linear error-correcting code can be thought of as a set of codewords generated by adding together all combinations of k independent basis codewords. the generator matrix of the code consists of those k basis codewords conventionally written as row vectors. for example the hamming code s generator matrix is g and its sixteen codewords were displayed in table the codewords of this code are linear combinations of the four vectors and an k code may also be described in terms of an m n parity-check matrix m n k as the set of vectors ftg that satisfy ht copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. dual codes one way of thinking of this equation is that each row of h a vector to which t must be orthogonal if it is a codeword. the generator matrix k vectors from which all codewords can be built and the parity-check matrix a set of m vectors to which all codewords are orthogonal. the dual of a code is obtained by exchanging the generator matrix and the parity-check matrix. the set of all vectors of length n that are orthogonal to all codewords in a code c is called the dual of the code c?. if t is orthogonal to and then it is also orthogonal to so all codewords are orthogonal to any linear combination of the m rows of h. so the set of all linear combinations of the rows of the parity-check matrix is the dual code. for our hamming code the parity-check matrix is h p the dual of the hamming code is the code shown in table a possibly unexpected property of this pair of codes is that the dual is contained within the code itself every word in the dual code is a codeword of the original hamming code. this relationship can be written using set notation the possibility that the set of dual vectors can overlap the set of codeword vectors is counterintuitive if we think of the vectors as real vectors how can a vector be orthogonal to itself? but when we work in modulo-two arithmetic many non-zero vectors are indeed orthogonal to themselves! exercise give a simple rule that distinguishes whether a binary vector is orthogonal to itself as is each of the three vectors and some more duals in general if a code has a systematic generator matrix g where p is a k m matrix then its parity-check matrix is h table the eight codewords of the dual of the hamming code. with table copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes example the repetition code has generator matrix its parity-check matrix is g h the two codewords are and the dual code has generator matrix g? h or equivalently modifying g? into systematic form by row additions g? we call this dual code the simple parity code it is the code with one parity-check bit which is equal to the sum of the two source bits. the dual code s four codewords are and in this case the only vector common to the code and the dual is the all-zero codeword. goodness of duals if a sequence of codes is good are their duals good too? examples can be constructed of all cases good codes with good duals linear codes bad codes with bad duals and good codes with bad duals. the last category is especially important many state-of-the-art codes have the property that their duals are bad. the classic example is the low-density parity-check code whose dual is a low-density generator-matrix code. exercise show that low-density generator-matrix codes are bad. a family of low-density generator-matrix codes is by two parameters j k which are the column weight and row weight of all rows and columns respectively of g. these weights are independent of n for example k show that the code has low-weight codewords then use the argument from exercise show that low-density parity-check codes are good and have good distance. solutions see gallager and mackay self-dual codes the hamming code had the property that the dual was contained in the code itself. a code is self-orthogonal if it is contained in its dual. for example the dual of the hamming code is a self-orthogonal code. one way of seeing this is that the overlap between any pair of rows of h is even. codes that contain their duals are important in quantum error-correction and shor it is intriguing though not necessarily useful to look at codes that are self-dual. a code c is self-dual if the dual of the code is identical to the code. c? c some properties of self-dual codes can be deduced copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. generalizing perfectness to other channels if a code is self-dual then its generator matrix is also a parity-check matrix for the code. self-dual codes have rate i.e. m k all codewords have even weight. exercise what property must the matrix p satisfy if the code with generator matrix g is self-dual? examples of self-dual codes the repetition code is a simple example of a self-dual code. the smallest non-trivial self-dual code is the following code. g h g pt exercise find the relationship of the above code to the hamming code. duals and graphs let a code be represented by a graph in which there are nodes of two types parity-check constraints and equality constraints joined by edges which represent the bits of the code all of which need be transmitted. the dual code s graph is obtained by replacing all parity-check nodes by equality nodes and vice versa. this type of graph is called a normal graph by forney further reading duals are important in coding theory because functions involving a code as the posterior distribution over codewords can be transformed by a fourier transform into functions over the dual code. for an accessible introduction to fourier analysis on groups see terras see also macwilliams and sloane generalizing perfectness to other channels having given up on the search for perfect codes for the binary symmetric channel we could console ourselves by changing channel. we could call a code a perfect u-error-correcting code for the binary erasure channel if it can restore any u erased bits and never more than u. rather than using the word perfect however the conventional term for such a code is a maximum distance separable code or mds code. as we already noted in exercise the hamming code is not an mds code. it can recover some sets of erased bits but not all. if any bits corresponding to a codeword of weight are erased then one bit of information is unrecoverable. this is why the code is a poor choice for a raid system. in a perfect u-error-correcting code for the binary erasure channel the number of redundant bits must be n k u. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes a tiny example of a maximum distance separable code is the simple paritycheck code whose parity-check matrix is h this code has codewords all of which have even parity. all codewords are separated by a distance of any single erased bit can be restored by setting it to the parity of the other two bits. the repetition codes are also maximum distance separable codes. exercise can you make an k code with m n k parity symbols for a q-ary erasure channel such that the decoder can recover the codeword when any m symbols are erased in a block of n for the channel with q symbols there is an k code which can correct any m erasures. for the q-ary erasure channel with q there are large numbers of mds codes of which the reedsolomon codes are the most famous and most widely used. as long as the size q is bigger than the blocklength n mds block codes of any rate can be found. further reading see lin and costello summary shannon s codes for the binary symmetric channel can almost always correct fn errors but they are not fn codes. reasons why the distance of a code has little relevance the shannon limit shows that the best codes must be able to cope with a noise level twice as big as the maximum noise level for a boundeddistance decoder. when the binary symmetric channel has f no code with a bounded-distance decoder can communicate at all but shannon says good codes exist for such channels. concatenation shows that we can get good performance even if the dis tance is bad. the whole weight enumerator function is relevant to the question of whether a code is a good code. the relationship between good codes and distance properties is discussed further in exercise further exercises exercise a codeword t is selected from a linear k code c and it is transmitted over a noisy channel the received signal is y. we assume that the channel is a memoryless channel such as a gaussian channel. given an assumed channel model p j t there are two decoding problems. the codeword decoding problem is the task of inferring which codeword t was transmitted given the received signal. the bitwise decoding problem is the task of inferring for each transmitted bit tn how likely it is that that bit was a one rather than a zero. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises consider optimal decoders for these two decoding problems. prove that the probability of error of the optimal bitwise-decoder is closely related to the probability of error of the optimal codeword-decoder by proving the following theorem. theorem if a binary linear code has minimum distance dmin then for any given channel the codeword bit error probability of the optimal bitwise decoder pb and the block error probability of the maximum likelihood decoder pb are related by pb pb dmin n pb exercise what are the minimum distances of the hamming code and the hamming code? exercise let aw be the average weight enumerator function of a random linear code with n and m estimate from principles the value of aw at w exercise a code with minimum distance greater than dgv. a rather nice code is generated by this generator matrix which is based on measuring the parities of all the g triplets of source bits find the minimum distance and weight enumerator function of this code. exercise find the minimum distance of the pentagonful low density parity-check code whose parity-check matrix is h show that nine of the ten rows are independent so the code has parameters n k using a computer its weight enumerator function. exercise replicate the calculations used to produce check the assertion that the highest noise level that s correctable is explore alternative concatenated sequences of codes. can you a better sequence of concatenated codes better in the sense that it has either higher asymptotic rate r or can tolerate a higher noise level f figure the graph of the pentagonful low-density parity-check code with bit nodes and parity-check nodes graph is known as the petersen graph. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes exercise investigate the possibility of achieving the shannon limit with linear block codes using the following counting argument. assume a linear code of large blocklength n and rate r kn the code s parity-check matrix h has m n k rows. assume that the code s optimal decoder which solves the syndrome decoding problem hn z allows reliable communication over a binary symmetric channel with probability f how many typical noise vectors n are there? roughly how many distinct syndromes z are there? since n is reliably deduced from z by the optimal decoder the number of syndromes must be greater than or equal to the number of typical noise vectors. what does this tell you about the largest possible value of rate r for a given f exercise linear binary codes use the input symbols and with equal probability implicitly treating the channel as a symmetric channel. investigate how much loss in communication rate is caused by this assumption if in fact the channel is a highly asymmetric channel. take as an example a z-channel. how much smaller is the maximum possible rate of communication using symmetric inputs than the capacity of the channel? about exercise show that codes with very bad distance are bad codes as in section exercise one linear code can be obtained from another by puncturing. puncturing means taking each codeword and deleting a set of bits. puncturing turns an k code into an k code where n another way to make new linear codes from old is shortening. shortening means constraining a set of bits to be zero and then deleting them from the codewords. typically if we shorten by one bit half of the code s codewords are lost. shortening typically turns an k code into an code where n k another way to make a new linear code from two old ones is to make the intersection of the two codes a codeword is only retained in the new code if it is present in both of the two old codes. discuss the on a code s distance-properties of puncturing shortening and intersection. is it possible to turn a code family with bad distance into a code family with good distance or vice versa by each of these three manipulations? exercise todd ebert s hat puzzle three players enter a room and a red or blue hat is placed on each person s head. the colour of each hat is determined by a coin toss with the outcome of one coin toss having no on the others. each person can see the other players hats but not his own. no communication of any sort is allowed except for an initial strategy session before the group enters the room. once they have had a chance to look at the other hats the players must simultaneously guess their copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions if you already know the hat puzzle you could try the scottish version of the rules in which the prize is only awarded to the group if they all guess correctly. in the reformed scottish version all the players must guess correctly and there are two rounds of guessing. those players who guess during round one leave the room. the remaining players must guess in round two. what strategy should the team adopt to maximize their chance of winning? own hat s colour or pass. the group shares a million prize if at least one player guesses correctly and no players guess incorrectly. the same game can be played with any number of players. the general problem is to a strategy for the group that maximizes its chances of winning the prize. find the best strategies for groups of size three and seven. when you ve done three and seven you might be able to solve exercise estimate how many binary low-density parity-check codes have self-orthogonal duals. that we don t expect a huge number since almost all low-density parity-check codes are good but a lowdensity parity-check code that contains its dual must be bad exercise in we plotted the error probability associated with a single codeword of weight d as a function of the noise level f of a binary symmetric channel. make an equivalent plot for the case of the gaussian channel showing the error probability associated with a single codeword of weight d as a function of the rate-compensated signal-tonoise ratio because depends on the rate you have to choose a code rate. choose r or solutions solution to exercise the probability of block error to leading order is pb solution to exercise a binary vector is perpendicular to itself if it has even weight i.e. an even number of solution to exercise the self-dual code has two equivalent parity-check matrices g and these must be equivalent to each other through row additions that is there is a matrix u such that so from the right-hand sides of this equation we have u pt so the left-hand sides become thus if a code with generator matrix g is self-dual then p is an orthogonal matrix modulo and vice versa. ptp ik solution to exercise the and codes are intimately related. the code whose parity-check matrix is h p is obtained by appending an extra parity-check bit which can be thought of as the parity of all seven bits of the hamming code and reordering the four bits. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes if an k code with m n k parity solution to exercise symbols has the property that the decoder can recover the codeword when any m symbols are erased in a block of n then the code is said to be maximum distance separable no mds binary codes exist apart from the repetition codes and simple parity codes. for q some mds codes can be found. as a simple example here is a code for the erasure channel. the code is in terms of the multiplication and addition rules of gf which are given in appendix the elements of the input alphabet are a b c d e fg and the generator matrix of the code is g a b c d e f the resulting codewords are solution to exercise quick rough proof of the theorem. let x denote the between the reconstructed codeword and the transmitted codeword. for any given channel output r there is a posterior distribution over x. this posterior distribution is positive only on vectors x belonging to the code the sums that follow are over codewords x. the block error probability is pb p r the average bit error probability averaging over all bits in the codeword is pb p r wx n where wx is the weight of codeword x. now the weights of the non-zero codewords satisfy wx n dmin n substituting the inequalities into the we obtain pb pb dmin n pb which is a factor of two stronger on the right than the stated result in making the proof watertight i have weakened the result a little. careful proof. the theorem relates the performance of the optimal block decoding algorithm and the optimal bitwise decoding algorithm. we introduce another pair of decoding algorithms called the blockguessing decoder and the bit-guessing decoder. the idea is that these two algorithms are similar to the optimal block decoder and the optimal bitwise decoder but lend themselves more easily to analysis. we now these decoders. let x denote the inferred codeword. for any given code copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions the optimal block decoder returns the codeword x that maximizes the posterior probability p r which is proportional to the likelihood p x. the probability of error of this decoder is called pb. the optimal bit decoder returns the value of a that maximizes the posterior probability p aj r the n bits xn for each of px p r a. the probability of error of this decoder is called pb. the block-guessing decoder returns a random codeword x with probabil ity distribution given by the posterior probability p r. the probability of error of this decoder is called pg b. the bit-guessing decoder returns for each of the n bits xn a random bit from the probability distribution p aj r. the probability of error of this decoder is called pg b the theorem states that the optimal bit error probability pb is bounded above by pb and below by a given multiple of pb the left-hand inequality in is trivially true if a block is correct all its constituent bits are correct so if the optimal block decoder outperformed the optimal bit decoder we could make a better bit decoder from the block decoder. we prove the right-hand inequality by establishing that the bit-guessing decoder is nearly as good as the optimal bit decoder pg b the bit-guessing decoder s error probability is related to the block guessing decoder s by then since pg b pb we have pb pg b pg b dmin n pg b dmin n pg b dmin n pb we now prove the two lemmas. near-optimality of guessing consider the case of a single bit with posterior probability the optimal bit decoder has probability of error p optimal the guessing decoder picks from and the truth is also distributed with the same probability. the probability that the guesser and the truth match is the probability that they mismatch is the guessing error probability p guess optimal since pg b is the average of many such error probabilities p guess and pb is the average of the corresponding optimal error probabilities p optimal we obtain the desired relationship between pg b and pb. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. binary codes relationship between bit error probability and block error probability the bitguessing and block-guessing decoders can be combined in a single system we can draw a sample xn from the marginal distribution p j r by drawing a sample x from the joint distribution p xj r then discarding the value of x. we can distinguish between two cases the discarded value of x is the correct codeword or not. the probability of bit error for the bit-guessing decoder can then be written as a sum of two terms pg b p correctp errorj x correct p incorrectp errorj x incorrect pg bp errorj x incorrect now whenever the guessed x is incorrect the true x must from it in at least d bits so the probability of bit error in these cases is at least dn so pg b d n pg b qed. solution to exercise the number of typical noise vectors n is roughly the number of distinct syndromes z is so reliable communication implies or in terms of the rate r mn m n r a bound which agrees precisely with the capacity of the channel. this argument is turned into a proof in the following chapter. solution to exercise the group to win three-quarters of the time. in the three-player case it is possible for three-quarters of the time two of the players will have hats of the same colour and the third player s hat will be the opposite colour. the group can win every time this happens by using the following strategy. each player looks at the other two players hats. if the two hats are colours he passes. if they are the same colour the player guesses his own hat is the opposite colour. this way every time the hat colours are distributed two and one one player will guess correctly and the others will pass and the group will win the game. when all the hats are the same colour however all three players will guess incorrectly and the group will lose. when any particular player guesses a colour it is true that there is only a chance that their guess is right. the reason that the group wins of the time is that their strategy ensures that when players are guessing wrong a great many are guessing wrong. for larger numbers of players the aim is to ensure that most of the time no one is wrong and occasionally everyone is wrong at once. in the game with players there is a strategy for which the group wins out of every times they play. in the game with players the group can win out of times. if you have not out these winning strategies for teams of and i recommend thinking about the solution to the three-player game in terms copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions of the locations of the winning and losing states on the three-dimensional hypercube then thinking laterally. if the number of players n is the optimal strategy can be using a hamming code of length n and the probability of winning the prize is nn each player is with a number n n the two colours are mapped onto and any state of their hats can be viewed as a received vector out of a binary channel. a random binary vector of length n is either a codeword of the hamming code with probability or it in exactly one bit from a codeword. each player looks at all the other bits and considers whether his bit can be set to a colour such that the state is a codeword can be deduced using the decoder of the hamming code. if it can then the player guesses that his hat is the other colour. if the state is actually a codeword all players will guess and will guess wrong. if the state is a non-codeword only one player will guess and his guess will be correct. it s quite easy to train seven players to follow the optimal strategy if the cyclic representation of the hamming code is used copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter in this chapter we will draw together several ideas that we ve encountered so far in one nice short proof. we will simultaneously prove both shannon s noisy-channel coding theorem symmetric binary channels and his source coding theorem binary sources. while this proof has connections to many preceding chapters in the book it s not essential to have read them all. on the noisy-channel coding side our proof will be more constructive than the proof given in chapter there we proved that almost any random code is very good here we will show that almost any linear code is very good. we will make use of the idea of typical sets and and we ll borrow from the previous chapter s calculation of the weight enumerator function of random linear codes on the source coding side our proof will show that random linear hash functions can be used for compression of compressible binary sources thus giving a link to chapter copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. very good linear codes exist in this chapter we ll use a single calculation to prove simultaneously the source coding theorem and the noisy-channel coding theorem for the binary symmetric channel. incidentally this proof works for much more general channel models not only the binary symmetric channel. for example the proof can be reworked for channels with non-binary outputs for time-varying channels and for channels with memory as long as they have binary inputs satisfying a symmetry property cf. section a simultaneous proof of the source coding and noisy-channel coding theorems we consider a linear error-correcting code with binary parity-check matrix h. the matrix has m rows and n columns. later in the proof we will increase n and m keeping m n the rate of the code r m n if all the rows of h are independent then this is an equality r mn in what follows we ll assume the equality holds. eager readers may work out the expected rank of a random binary matrix h s very close to m and pursue the that the rank has on the rest of this proof s negligible. a codeword t is selected satisfying ht mod and a binary symmetric channel adds noise x giving the received signal r t x mod the receiver aims to infer both t and x from r using a syndrome-decoding approach. syndrome decoding was introduced in section and the receiver computes the syndrome z hr mod ht hx mod hx mod the syndrome only depends on the noise x and the decoding problem is to the most probable x that hx z mod in this chapter x denotes the noise added by the channel not the input to the channel. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. very good linear codes exist this best estimate for the noise vector is then subtracted from r to give the best guess for t. our aim is to show that as long as r where f is the probability of the binary symmetric channel the optimal decoder for this syndrome-decoding problem has vanishing probability of error as n increases for random h. we prove this result by studying a sub-optimal strategy for solving the decoding problem. neither the optimal decoder nor this typical-set decoder would be easy to implement but the typical-set decoder is easier to analyze. the typical-set decoder examines the typical set t of noise vectors the set of noise vectors that satisfy log n hx checking to see if any of we ll leave out the and that those typical vectors the observed syndrome make a typical-set rigorous. enthusiasts are encouraged to revisit section and put these details into this proof. z if exactly one typical vector does so the typical set decoder reports that vector as the hypothesized noise vector. if no typical vector matches the observed syndrome or more than one does then the typical set decoder reports an error. the probability of error of the typical-set decoder for a given matrix h can be written as a sum of two terms ptsjh p p tsjh where p is the probability that the true noise vector x is itself not typical and p tsjh is the probability that the true x is typical and at least one other typical vector clashes with it. the probability vanishes as n increases as we proved when we studied typical sets we concentrate on the second probability. to recap we re imagining a true noise vector x and if any of the typical noise vectors from x x then we have an error. we use the truth function x whose value is one if the statement x is true and zero otherwise. we can bound the number of type ii errors made when the noise is x thus of errors given x and h t x x the number of errors is either zero or one the sum on the right-hand side may exceed one in cases where several typical noise vectors have the same syndrome. we can now write down the probability of a type-ii error by averaging over equation is a union bound. x p tsjh p t x x now we will the average of this probability of type-ii error over all linear codes by averaging over h. by showing that the average probability of type-ii error vanishes we will thus show that there exist linear codes with vanishing error probability indeed that almost all linear codes are very good. we denote averaging over all binary matrices h by h the average probability of type-ii error is ts xh p tsjh dp tsjheh copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. data compression by linear hash codes p t p t x x x h now the quantity h x already cropped up when we were calculating the expected weight enumerator function of random linear codes for any non-zero binary vector v the probability that hv averaging over all matrices h is so ts p where jtj denotes the size of the typical set. as you will recall from chapter there are roughly hx noise vectors in the typical set. so jtj ts this bound on the probability of error either vanishes or grows exponentially as n increases that we are keeping m proportional to n as n increases. it vanishes if substituting r mn we have thus established the noisy-channel coding theorem for the binary symmetric channel very good linear codes exist for any rate r satisfying hx mn where hx is the entropy of the channel noise per bit. r hx exercise redo the proof for a more general channel. data compression by linear hash codes the decoding game we have just played can also be viewed as an uncompression game. the world produces a binary noise vector x from a source p the noise has redundancy the probability is not we compress it with a linear compressor that maps the n input x noise to the m output z syndrome. our uncompression task is to recover the input x from the output z. the rate of the compressor is rcompressor mn don t care about the possibility of linear redundancies in our of the rate here. the result that we just found that the decoding problem can be solved for almost any h with vanishing error probability as long as hx mn thus instantly proves a source coding theorem given a binary source x of entropy hx and a required compressed rate r hx there exists a linear compressor x z hx mod having rate mn equal to that required rate r and an associated uncompressor that is virtually lossless. this theorem is true not only for a source of independent identically distributed symbols but also for any source for which a typical set can be sources with memory and time-varying sources for example all that s required is that the source be ergodic. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. notes very good linear codes exist this method for proving that codes are good can be applied to other linear codes such as low-density parity-check codes aji et al. for each code we need an approximation of its expected weight enumerator function. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on information theory the most exciting exercises which will introduce you to further ideas in information theory are towards the end of this chapter. refresher exercises on source coding and noisy channels exercise let x be an ensemble with ax and px consider source coding using the block coding of x where every x x containing or fewer is assigned a distinct codeword while the other xs are ignored. if the assigned codewords are all of the same length the minimum length required to provide the above set with distinct codewords. calculate the probability of getting an x that will be ignored. exercise let x be an ensemble with px the ensemble is encoded using the symbol code c consider the codeword corresponding to x x n where n is large. compute the entropy of the fourth bit of transmission. compute the conditional entropy of the fourth bit given the third bit. estimate the entropy of the hundredth bit. estimate the conditional entropy of the hundredth bit given the ninety-ninth bit. exercise two fair dice are rolled by alice and the sum is recorded. bob s task is to ask a sequence of questions with yesno answers to out this number. devise in detail a strategy that achieves the minimum possible average number of questions. exercise how can you use a coin to draw straws among people? exercise in a magic trick there are three participants the magician an assistant and a volunteer. the assistant who claims to have paranormal abilities is in a soundproof room. the magician gives the volunteer six blank cards white and one blue. the volunteer writes a different integer from to on each card as the magician is watching. the volunteer keeps the blue card. the magician arranges the white cards in some order and passes them to the assistant. the assistant then announces the number on the blue card. how does the trick work? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on information theory exercise how does this trick work? here s an ordinary pack of cards into random order. please choose cards from the pack any that you wish. don t let me see their faces. no don t give them to me pass them to my assistant esmerelda. she can look at them. now esmerelda show me four of the cards. hmm nine of spades six of clubs four of hearts ten of diamonds. the hidden card then must be the queen of spades! the trick can be performed as described above for a pack of cards. use information theory to give an upper bound on the number of cards for which the trick can be performed. exercise find a probability sequence p such that hp exercise consider a discrete memoryless source with ax fa b c dg and px there are eight-letter words that can be formed from the four letters. find the total number of such words that are in the typical set tn where n and exercise consider and the channel whose transition probability matrix is the source as q fa b c d eg ps note that the source alphabet has symbols but the channel alphabet ax ay has only four. assume that the source produces symbols at exactly the rate that the channel accepts channel symbols. for a given explain how you would design a system for communicating the source s output over the channel with an average error probability per source symbol less than be as explicit as possible. in particular do not invoke shannon s noisy-channel coding theorem. exercise consider a binary symmetric channel and a code c assume that the four codewords are used with probabilities what is the decoding rule that minimizes the probability of decoding error? optimal decoding rule depends on the noise level f of the binary symmetric channel. give the decoding rule for each range of values of f for f between and exercise find the capacity and optimal input distribution for the three-input three-output channel whose transition probabilities are q copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on information theory exercise the input to a channel q is a word of bits. the output is also a word of bits. each time it is used the channel exactly one of the transmitted bits but the receiver does not know which one. the other seven bits are received without error. all bits are equally likely to be the one that is derive the capacity of this channel. show by describing an explicit encoder and decoder that it is possible reliably is with zero error probability to communicate bits per cycle over this channel. exercise a channel with input x fa b cg and output y fr s t ug has conditional probability matrix q what is its capacity? hhj hhj hhj a b c r s t u table some valid isbns. hyphens are included for legibility. exercise the ten-digit number on the cover of a book known as the isbn incorporates an error-detecting code. the number consists of nine source digits satisfying xn and a tenth check digit whose value is given by nxn! mod here if then the tenth digit is shown using the roman numeral x. show that a valid isbn nxn! mod imagine that an isbn is communicated over an unreliable human channel which sometimes digits and sometimes reorders digits. show that this code can be used to detect not correct all errors in which any one of the ten digits is example show that this code can be used to detect all errors in which any two adjacent digits are transposed example what other transpositions of pairs of non-adjacent digits can be detected? if the tenth digit were to be nxn! mod why would the code not work so well? the detection of both of single digits and transpositions of digits. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on information theory a b c d a b c d remember d dp p exercise a channel with input x and output y has transition proba bility matrix f f g g g g f f q px p assuming an input distribution of the form p p p write down the entropy of the output hy and the conditional entropy of the output given the input hy jx. show that the optimal input distribution is given by p where f write down the optimal input distribution and the capacity of the channel in the case f g and comment on your answer. f f exercise what are the in the redundancies needed in an error-detecting code can reliably detect that a block of data has been corrupted and an error-correcting code can detect and correct errors? further tales from information theory the following exercises give you the chance to discover for yourself the answers to some more surprising results of information theory. exercise communication of information from correlated sources. imagine that we want to communicate data from two data sources x and x to a central location c via noise-free one-way communication channels the signals xa and xb are strongly dependent so their joint information content is only a little greater than the marginal information content of either of them. for example c is a weather collator who wishes to receive a string of reports saying whether it is raining in allerton and whether it is raining in bognor the joint probability of xa and xb might be p xb xa xb the weather collator would like to know n successive values of xa and xb exactly but since he has to pay for every bit of information he receives he is interested in the possibility of avoiding buying n bits from source a and n bits from source b. assuming that variables xa and xb are generated repeatedly from this distribution can they be encoded at rates ra and rb in such a way that c can reconstruct all the variables with the sum of information transmission rates on the two lines being less than two bits per cycle? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on information theory xa encode ra ta xb encode rb tb hhhj c rb hx x hx hx j x achievable hx j x hx ra the answer which you should demonstrate is indicated in in the general case of two dependent sources x and x there exist codes for the two transmitters that can achieve reliable communication of both x and x to c as long as the information rate from x ra exceeds hx j x the information rate from x rb exceeds hx j x and the total information rate ra rb exceeds the joint entropy hx x and wolf so in the case of xa and xb above each transmitter must transmit at a rate greater than bits and the total rate ra rb must be greater than bits for example ra rb there exist codes that can achieve these rates. your task is to out why this is so. try to an explicit solution in which one of the sources is sent as plain text tb xb and the other is encoded. exercise multiple access channels. consider a channel with two sets of inputs and one output for example a shared telephone line a simple model system has two binary inputs xa and xb and a ternary output y equal to the arithmetic sum of the two inputs that s or there is no noise. users a and b cannot communicate with each other and they cannot hear the output of the channel. if the output is a the receiver can be certain that both inputs were set to and if the output is a the receiver can be certain that both inputs were set to but if the output is then it could be that the input state was or how should users a and b use this channel so that their messages can be deduced from the received signals? how fast can a and b communicate? clearly the total information rate from a and b to the receiver cannot be two bits. on the other hand it is easy to achieve a total information rate rarb of one bit. can reliable communication be achieved at rates rb such that ra rb the answer is indicated in some practical codes for multi-user channels are presented in ratzer and mackay exercise broadcast channels. a broadcast channel consists of a single transmitter and two or more receivers. the properties of the channel are by a conditional distribution qya yb j x. ll assume the channel is memoryless. the task is to add an encoder and two decoders to enable reliable communication of a common message at rate to both receivers an individual message at rate ra to receiver a and an individual message at rate rb to receiver b. the capacity region of the broadcast channel is the convex hull of the set of achievable rate triplets ra rb. a simple benchmark for such a channel is given by time-sharing signaling. if the capacities of the two channels considered separately figure communication of information from dependent sources. xa and xb are dependent sources dependence is represented by the dotted arrow. strings of values of each variable are encoded using codes of rate ra and rb into transmissions ta and tb which are communicated over noise-free channels to a receiver c. the achievable rate region. both strings can be conveyed without error even though ra hx and rb hx x hhj ya yb figure the broadcast channel. x is the channel input ya and yb are the outputs. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on information theory xa xb p xb y rb y xb xa achievable ra are c and c then by devoting a fraction of the transmission time to channel a and to channel b we can achieve ra rb we can do better than this however. as an analogy imagine speaking simultaneously to an american and a belarusian you are in american and in belarusian but neither of your two receivers understands the other s language. if each receiver can distinguish whether a word is in their own language or not then an extra binary can be conveyed to both recipients by using its bits to decide whether the next transmitted word should be from the american source text or from the belarusian source text. each recipient can concatenate the words that they understand in order to receive their personal message and can also recover the binary string. an example of a broadcast channel consists of two binary symmetric channels with a common input. the two halves of the channel have probabilities fa and fb. we ll assume that a has the better half-channel i.e. fa fb closely related channel is a degraded broadcast channel in which the conditional probabilities are such that the random variables have the structure of a markov chain x ya yb i.e. yb is a further degraded version of ya. in this special case it turns out that whatever information is getting through to receiver b can also be recovered by receiver a. so there is no point distinguishing between and rb the task is to the capacity region for the rate pair ra where is the rate of information reaching both a and b and ra is the rate of the extra information reaching a. the following exercise is equivalent to this one and a solution to it is illustrated in exercise variable-rate error-correcting codes for channels with unknown noise level. in real life channels may sometimes not be well characterized before the encoder is installed. as a model of this situation imagine that a channel is known to be a binary symmetric channel with noise level either fa or fb. let fb fa and let the two capacities be ca and cb. those who like to live dangerously might install a system designed for noise level fa with rate ra ca in the event that the noise level turns out to be fb our experience of shannon s theories would lead us to expect that there figure multiple access channels. a general multiple access channel with two transmitters and one receiver. a binary multiple access channel with output equal to the sum of two inputs. the achievable region. rb c c ra figure rates achievable by simple timesharing. r c a bc f a f b f figure rate of reliable communication r as a function of noise level f for shannonesque codes designed to operate at noise levels fa line and fb line. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on information theory r c a bc f a f b f figure rate of reliable communication r as a function of noise level f for a desired variable-rate code. figure an achievable region for the channel with unknown noise level. assuming the two possible noise levels are fa and fb the dashed lines show the rates ra rb that are achievable using a simple time-sharing approach and the solid line shows rates achievable using a more cunning approach. would be a catastrophic failure to communicate information reliably line in a conservative approach would design the encoding system for the worstcase scenario installing a code with rate rb cb line in in the event that the lower noise level fa holds true the managers would have a feeling of regret because of the wasted capacity ca rb. is it possible to create a system that not only transmits reliably at some rate whatever the noise level but also communicates some extra lowerpriority bits if the noise level is low as shown in this code communicates the high-priority bits reliably at all noise levels between fa and fb and communicates the low-priority bits also if the noise level is fa or below. this problem is mathematically equivalent to the previous problem the degraded broadcast channel. the lower rate of communication was there called and the rate at which the low-priority bits are communicated if the noise level is low was called ra. an illustrative answer is shown in for the case fa and fb also shows the achievable region for a broadcast channel whose two half-channels have noise levels fa and fb i admit i the gap between the simple time-sharing solution and the cunning solution disappointingly small. in chapter we will discuss codes for a special class of broadcast channels namely erasure channels where every symbol is either received without error or erased. these codes have the nice property that they are rateless the number of symbols transmitted is determined on the such that reliable comunication is achieved whatever the erasure statistics of the channel. exercise multiterminal information networks are both important practically and intriguing theoretically. consider the following example of a two-way binary channel two people both wish to talk over the channel and they both want to hear what the other person is saying but you can hear the signal transmitted by the other person only if you are transmitting a zero. what simultaneous information rates from a to b and from b to a can be achieved and how? everyday examples of such networks include the vhf channels used by ships and computer ethernet networks which all the devices are unable to hear anything if two or more devices are broadcasting simultaneously. obviously we can achieve rates of in both directions by simple timesharing. but can the two information rates be made larger? finding the capacity of a general two-way channel is still an open problem. however we can obtain interesting results concerning achievable points for the simple binary channel discussed above as indicated in there exist codes that can achieve rates up to the boundary shown. there may exist better codes too. solutions solution to exercise cq bits. hint for the last part a solution exists that involves a simple code. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises on information theory xb xa figure a general two-way channel. the rules for a binary two-way channel. the two tables show the outputs ya and yb that result for each state of the inputs. achievable region for the two-way binary channel. rates below the solid line are achievable. the dotted line shows the obviously achievable region which can be attained by simple time-sharing. xa ya p ybjxa xb yb ya xb xa yb xb b r achievable ra copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. message passing one of the themes of this book is the idea of doing complicated calculations using simple distributed hardware. it turns out that quite a few interesting problems can be solved by message-passing algorithms in which simple messages are passed locally among simple processors whose operations lead after some time to the solution of a global problem. counting as an example consider a line of soldiers walking in the mist. the commander wishes to perform the complex calculation of counting the number of soldiers in the line. this problem could be solved in two ways. first there is a solution that uses expensive hardware the loud booming voices of the commander and his men. the commander could shout all soldiers report back to me within one minute! then he could listen carefully as the men respond molesworth here sir! fotheringtonthomas here sir! and so on. this solution relies on several expensive pieces of hardware there must be a reliable communication channel to and from every soldier the commander must be able to listen to all the incoming messages even when there are hundreds of soldiers and must be able to count and all the soldiers must be well-fed if they are to be able to shout back across the possibly-large distance separating them from the commander. the second way of this global function the number of soldiers does not require global communication hardware high iq or good food we simply require that each soldier can communicate single integers with the two adjacent soldiers in the line and that the soldiers are capable of adding one to a number. each soldier follows these rules if you are the front soldier in the line say the number one to the soldier behind you. algorithm message-passing rule-set a. if you are the rearmost soldier in the line say the number one to the soldier in front of you. if a soldier ahead of or behind you says a number to you add one to it and say the new number to the soldier on the other side. if the clever commander can not only add one to a number but also add two numbers together then he can the global number of soldiers by simply adding together copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. message passing figure a line of soldiers counting themselves using message-passing rule-set a. the commander can add from the soldier in front from the soldier behind and for himself and deduce that there are soldiers in total. figure a swarm of guerillas. the number said to him by the soldier in front of him the number said to the commander by the soldier behind him one equals the total number of soldiers in front is the number behind count the commander himself. this solution requires only local communication hardware and simple computations and addition of integers. commander separation this clever trick makes use of a profound property of the total number of soldiers that it can be written as the sum of the number of soldiers in front of a point and the number behind that point two quantities which can be computed separately because the two groups are separated by the commander. if the soldiers were not arranged in a line but were travelling in a swarm then it would not be easy to separate them into two groups in this way. the commander jim guerillas in could not be counted using the above message-passing rule-set a because while the guerillas do have neighbours by lines it is not clear who is in front and who is behind furthermore since the graph of connections between the guerillas contains cycles it is not possible for a guerilla in a cycle as jim to separate the group into two groups those in front and those behind a swarm of guerillas can be counted by a message-passing algo rithm if they are arranged in a graph that contains no cycles. rule-set b is a message-passing algorithm for counting a swarm of guerillas whose connections form a cycle-free graph also known as a tree as illustrated in any guerilla can deduce the total in the tree from the messages that they receive. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. counting figure a swarm of guerillas whose connections form a tree. commander jim algorithm message-passing rule-set b. count your number of neighbours n keep count of the number of messages you have received from your neighbours m and of the values vn of each of those messages. let v be the running total of the messages you have received. if the number of messages you have received m is equal to n then identify the neighbour who has not sent you a message and tell them the number v if the number of messages you have received is equal to n then the number v is the required total. for each neighbour n f say to neighbour n the number v vn. g a figure a triangular grid. how many paths are there from a to b? one path is shown. b copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. path-counting a more profound task than counting squaddies is the task of counting the number of paths through a grid and how many paths pass through any given point in the grid. figure shows a rectangular grid and a path through the grid connecting points a and b. a valid path is one that starts from a and proceeds to b by rightward and downward moves. our questions are how many such paths are there from a to b? if a random path from a to b is selected what is the probability that it passes through a particular node in the grid? we say random we mean that all paths have exactly the same probability of being selected. how can a random path from a to b be selected? counting all the paths from a to b doesn t seem straightforward. the number of paths is expected to be pretty big even if the permitted grid were a diagonal strip only three nodes wide there would still be about possible paths. the computational breakthrough is to realize that to the number of paths we do not have to enumerate all the paths explicitly. pick a point p in the grid and consider the number of paths from a to p. every path from a to p must come in to p through one of its upstream neighbours upstream meaning above or to the left. so the number of paths from a to p can be found by adding up the number of paths from a to each of those neighbours. this message-passing algorithm is illustrated in for a simple grid with ten vertices connected by twelve directed edges. we start by sending the message from a. when any node has received messages from all its upstream neighbours it sends the sum of them on to its downstream neighbours. at b the number emerges we have counted the number of paths from a to b without enumerating them all. as a sanity-check shows the distinct paths from a to b. having counted all paths we can now move on to more challenging problems computing the probability that a random path goes through a given vertex and creating a random path. probability of passing through a node by making a backward pass as well as the forward pass we can deduce how many of the paths go through each node and if we divide that by the total number of paths we obtain the probability that a randomly selected path passes through that node. figure shows the backward-passing messages in the lower-right corners of the tables and the original forward-passing messages in the upper-left corners. by multiplying these two numbers at a given vertex we the total number of paths passing through that vertex. for example four paths pass through the central vertex. figure shows the result of this computation for the triangular grid. the area of each blob is proportional to the probability of passing through the corresponding node. random path sampling exercise if one creates a random path from a to b by a fair coin at every junction where there is a choice of two directions is message passing a n p m b figure every path from a to p enters p through an upstream neighbour of p either m or n so we can the number of paths from a to p by adding the number of paths from a to m to the number from a to n. a b figure messages sent in the forward pass. a b figure the paths. a b figure messages sent in the forward and backward passes. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. finding the lowest-cost path the resulting path a uniform random sample from the set of all paths? imagine trying it for the grid of there is a neat insight to be had here and i d like you to have the satisfaction of it out. exercise having run the forward and backward algorithms between points a and b on a grid how can one draw one path from a to b uniformly at random? a figure the probability of passing through each node and a randomly chosen path. b the message-passing algorithm we used to count the paths to b is an example of the sumproduct algorithm. the sum takes place at each node when it adds together the messages coming from its predecessors the product was not mentioned but you can think of the sum as a weighted sum in which all the summed terms happened to have weight finding the lowest-cost path imagine you wish to travel as quickly as possible from ambridge to bognor the various possible routes are shown in along with the cost in hours of traversing each edge in the graph. for example the route ail nb has a cost of hours. we would like to the lowest-cost path without explicitly evaluating the cost of all paths. we can do this by for each node what the cost of the lowest-cost path to that node from a is. these quantities can be computed by message-passing starting from node a. the message-passing algorithm is called the minsum algorithm or viterbi algorithm. for brevity we ll call the cost of the lowest-cost path from node a to node x the cost of x each node can broadcast its cost to its descendants once it knows the costs of all its possible predecessors. let s step through the algorithm by hand. the cost of a is zero. we pass this news on to h and i. as the message passes along each edge in the graph the cost of that edge is added. we the costs of h and i are and respectively similarly then the costs of j and l are found to be and respectively but what about k? out of the edge hk comes the message that a path of cost exists from a to k via h and from edge ik we learn of an alternative path of cost the minsum algorithm sets the cost of k equal to the minimum of these min and records which was the smallest-cost route into k by retaining only the edge ik and pruning away the other edges leading to k figures and e show the remaining two iterations of the algorithm which reveal that there is a path from a to b with cost the minsum algorithm encounters a tie where the minimum-cost a hhhj h i hhhj hhhj j k l hhhj hhhj m n hhhj b figure route diagram from ambridge to bognor showing the costs associated with the edges. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. message passing path to a node is achieved by more than one route to it then the algorithm can pick any of those routes at random. we can recover this lowest-cost path by backtracking from b following the trail of surviving edges back to a. we deduce that the lowest-cost path is aikmb. other applications of the minsum algorithm imagine that you manage the production of a product from raw materials via a large set of operations. you wish to identify the critical path in your process that is the subset of operations that are holding up production. if any operations on the critical path were carried out a little faster then the time to get from raw materials to product would be reduced. the critical path of a set of operations can be found using the minsum algorithm. in chapter the minsum algorithm will be used in the decoding of error-correcting codes. summary and related ideas some global functions have a separability property. for example the number of paths from a to p separates into the sum of the number of paths from a to m point to p s left and the number of paths from a to n point above p. such functions can be computed by message-passing. other functions do not have such separability properties for example the number of pairs of soldiers in a troop who share the same birthday the size of the largest group of soldiers who share a common height to the nearest centimetre the length of the shortest tour that a travelling salesman could take that visits every soldier in a troop. one of the challenges of machine learning is to low-cost solutions to problems like these. the problem of a large subset of variables that are approximately equal can be solved with a neural network approach and brody and brody a neural approach to the travelling salesman problem will be discussed in section a a a a a hhhj h i hhhj hhhj hhhj hhhj hhhj hhhj h i h i h i h i hhj hhhj hhhj hhhj hhhj j k l j k l j k l j k l j k l m n hhhj b m n hhhj b m n hhhj b hhhj hhhj hhhj hhhj hhhj hhhj m hhhj hhhj n b hhhj m hhhj n b further exercises exercise describe the asymptotic properties of the probabilities de picted in for a grid in a triangle of width and height n exercise in image processing the integral image ix y obtained from an image f y x and y are pixel coordinates is by figure minsum message-passing algorithm to the cost of getting to each node and thence the lowest cost route from a to b. ix y x y f v show that the integral image ix y can be computed by message passing. show that from the integral image some simple functions of the image can be obtained. for example give an expression for the sum of the image intensities f y for all y in a rectangular region extending from to copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solutions solution to exercise since there are paths through the grid of they must all have probability but a strategy based on fair will produce paths whose probabilities are powers of solution to exercise to make a uniform random walk each forward step of the walk should be chosen using a biased coin at each junction with the biases chosen in proportion to the backward messages emanating from the two options. for example at the choice after leaving a there is a message coming from the east and a coming from south so one should go east with probability and south with probability this is how the path in was generated. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over constrained noiseless channels in this chapter we study the task of communicating over a constrained noiseless channel a constrained channel over which not all strings from the input alphabet may be transmitted. we make use of the idea introduced in chapter that global properties of graphs can be computed by a local message-passing algorithm. three examples of constrained binary channels a constrained channel can be by rules that which strings are permitted. example in channel a every must be followed by at least one a valid string for this channel is channel a the substring is forbidden. as a motivation for this model consider a channel in which are represented by pulses of electromagnetic energy and the device that produces those pulses requires a recovery time of one clock cycle after generating a pulse before it can generate another. example channel b has the rule that all must come in groups of two or more and all must come in groups of two or more. a valid string for this channel is channel b and are forbidden. as a motivation for this model consider a disk drive in which successive bits are written onto neighbouring points in a track along the disk surface the values and are represented by two opposite magnetic orientations. the strings and are forbidden because a single isolated magnetic domain surrounded by domains having the opposite orientation is unstable so that might turn into for example. example channel c has the rule that the largest permitted runlength is two that is each symbol can be repeated at most once. a valid string for this channel is channel c and are forbidden. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. three examples of constrained binary channels a physical motivation for this model is a disk drive in which the rate of rotation of the disk is not known accurately so it is to distinguish between a string of two and a string of three which are represented by oriented magnetizations of duration and respectively where is the known time taken for one bit to pass by to avoid the possibility of confusion and the resulting loss of synchronization of sender and receiver we forbid the string of three and the string of three all three of these channels are examples of runlength-limited channels. the rules constrain the minimum and maximum numbers of successive and channel runlength of runlength of minimum maximum minimum maximum unconstrained a b c in channel a runs of may be of any length but runs of are restricted to length one. in channel b all runs must be of length two or more. in channel c all runs must be of length one or two. the capacity of the unconstrained binary channel is one bit per channel use. what are the capacities of the three constrained channels? be fair we haven t the capacity of such channels yet please understand capacity as meaning how many bits can be conveyed reliably per channel-use. some codes for a constrained channel let us concentrate for a moment on channel a in which runs of may be of any length but runs of are restricted to length one. we would like to communicate a random binary over this channel as as possible. a simple starting point is a code that maps each source bit into two transmitted bits this is a code and it respects the constraints of channel a so the capacity of channel a is at least can we do better? is redundant because if the of two received bits is a zero we know that the second bit will also be a zero. we can achieve a smaller average transmitted length using a code that omits the redundant zeroes in is such a variable-length code. if the source symbols are used with equal frequency then the average transmitted length per source bit is l so the average communication rate is r and the capacity of channel a must be at least can we do better than there are two ways to argue that the infor mation rate could be increased above r the argument assumes we are comfortable with the entropy as a measure of information content. the idea is that starting from code we can reduce the average message length without greatly reducing the entropy code s t code s t copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over constrained noiseless channels rf figure top the information content per source symbol and mean transmitted length per source symbol as a function of the source density. bottom the information content per transmitted symbol in bits as a function of f of the message we send by decreasing the fraction of that we transmit. imagine feeding into a stream of bits in which the frequency of is f a stream could be obtained from an arbitrary binary by passing the source into the decoder of an arithmetic code that is optimal for compressing binary strings of density f the information rate r achieved is the entropy of the source divided by the mean transmitted length thus lf f f rf lf f the original code without preprocessor corresponds to f what happens if we perturb f a little towards smaller f setting f for small negative in the vicinity of f the denominator lf varies linearly with in contrast the numerator only has a second-order dependence on exercise find to order the taylor expansion of as a function of to order rf increases linearly with decreasing it must be possible to increase r by decreasing f figure shows these functions rf does indeed increase as f decreases and has a maximum of about bits per channel use at f maxf rf by this argument we have shown that the capacity of channel a is at least exercise if a containing a fraction f is transmitted by what fraction of the transmitted stream is what fraction of the transmitted bits is if we drive code with a sparse source of density f a second more fundamental approach counts how many valid sequences of length n there are sn we can communicate log sn bits in n channel cycles by giving one name to each of these valid sequences. the capacity of a constrained noiseless channel we the capacity of a noisy channel in terms of the mutual information between its input and its output then we proved that this number the capacity was related to the number of distinguishable messages sn that could be reliably conveyed over the channel in n uses of the channel by c lim n log sn in the case of the constrained noiseless channel we can adopt this identity as our of the channel s capacity. however the name s which when we were making codes for noisy channels ran over messages s s is about to take on a new role labelling the states of our channel copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. counting the number of possible messages f f f f f f f f f f f f f f f f sn j j b sn j j m m m m a a a a a a a aau m m m m figure state diagram for channel a. trellis section. trellis. connection matrix. figure state diagrams trellis sections and connection matrices for channels b and c. a c a a a a a aau sn a n n n n a n n n n so in this chapter we will denote the number of distinguishable messages of length n by mn and the capacity to be c lim n log mn once we have out the capacity of a channel we will return to the task of making a practical code for that channel. counting the number of possible messages first let us introduce some representations of constrained channels. in a state diagram states of the transmitter are represented by circles labelled with the name of the state. directed edges from one state to another indicate that the transmitter is permitted to move from the state to the second and a label on that edge indicates the symbol emitted when that transition is made. figure shows the state diagram for channel a. it has two states and when transitions to state are made a is transmitted when transitions to state are made a is transmitted transitions from state to state are not possible. we can also represent the state diagram by a trellis section which shows two successive states in time at two successive horizontal locations the state of the transmitter at time n is called sn. the set of possible state sequences can be represented by a trellis as shown in a valid sequence corresponds to a path through the trellis and the number of copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over constrained noiseless channels h h h h h h h h h h h h figure counting the number of paths in the trellis of channel a. the counts next to the nodes are accumulated by passing from left to right across the trellises. h h h h h h h h h h h h channel a channel b channel c h h h h h h h h h h h h a a a a a aau a a a a a aau a aau a a h h h h a a h h h h a a a a a aau h h h h a a a aau a a a aau a a h h h h a a h h h h a a a a a aau h h h h h h h h a a a a a aau h h h h a a a a a aau h h h h a a a a a aau h h h h a a a a a aau h h h h a a a a a aau h h h h a a a a a aau h h h h a a a a a aau h h h h figure counting the number of paths in the trellises of channels a b and c. we assume that at the start the bit is preceded by so that for channels a and b any initial character is permitted but for channel c the character must be a copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. counting the number of possible messages n mn mn n mn figure counting the number of paths in the trellis of channel a. valid sequences is the number of paths. for the purpose of counting how many paths there are through the trellis we can ignore the labels on the edges and summarize the trellis section by the connection matrix a in which if there is an edge from state s to and otherwise figure shows the state diagrams trellis sections and connection matrices for channels b and c. let s count the number of paths for channel a by message-passing in its trellis. figure shows the few steps of this counting process and shows the number of paths ending in each state after n steps for n the total number of paths of length n mn is shown along the top. we recognize mn as the fibonacci series. exercise show that the ratio of successive terms in the fibonacci series tends to the golden ratio thus to within a constant factor mn scales as mn as n so the capacity of channel a is c lim n how can we describe what we just did? the count of the number of paths is a vector cn we can obtain from cn using acn so cn an where is the state count before any symbols are transmitted. in we assumed i.e. that either of the two symbols is permitted at s cn n. in the limit the outset. the total number of paths is mn cn cn becomes dominated by the principal right-eigenvector of a. cn constant r copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over constrained noiseless channels here is the principal eigenvalue of a. so to the capacity of any constrained channel all we need to do is the principal eigenvalue of its connection matrix. then c back to our model channels comparing and and c it looks as if channels b and c have the same capacity as channel a. the principal eigenvalues of the three trellises are the same eigenvectors for channels a and b are given at the bottom of table and indeed the channels are intimately related. t hd s hd t s figure an accumulator and a equivalence of channels a and b if we take any valid string s for channel a and pass it through an accumulator obtaining t by tn sn mod for n then the resulting string is a valid string for channel b because there are no in s so there are no isolated digits in t. the accumulator is an invertible operator so similarly any valid string t for channel b can be mapped onto a valid string s for channel a through the binary sn tn mod for n because and are equivalent in modulo arithmetic the is also a blurrer convolving the source stream with the channel c is also intimately related to channels a and b. exercise what is the relationship of channel c to channels a and b? practical communication over constrained channels ok how to do it in practice? since all three channels are equivalent we can concentrate on channel a. fixed-length solutions we start with explicitly-enumerated codes. the code in the table achieves a rate of exercise similarly enumerate all strings of length that end in the zero state. are of them. hence show that we can map bits source strings to transmitted bits and achieve rate what rate can be achieved by mapping an integer number of source bits to n transmitted bits? s cs table a runlength-limited code for channel a. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. practical communication over constrained channels optimal variable-length solution the optimal way to convey information over the constrained channel is to the optimal transition probabilities for all points in the trellis and make transitions with these probabilities. when discussing channel a we showed that a sparse source with density f driving code would achieve capacity. and we know how to make we design an arithmetic code that is optimal for compressing a sparse source then its associated decoder gives an optimal mapping from dense random binary strings to sparse strings. the task of the optimal probabilities is given as an exercise. exercise show that the optimal transition probabilities q can be found as follows. find the principal right- and left-eigenvectors of a that is the solutions of aer and elt with largest eigenvalue then construct a matrix q whose invariant distribution is proportional to er i a namely el i el s exercise might give helpful cross-fertilization here. exercise show that when sequences are generated using the optimal transition probability matrix the entropy of the resulting sequence is asymptotically per symbol. consider the conditional entropy of just one symbol given the previous one assuming the previous one s distribution is the invariant distribution. in practice we would probably use approximations to the optimal variable-length solution. one might dislike variable-length solutions because of the resulting unpredictability of the actual encoded length in any particular case. perhaps in some applications we would like a guarantee that the encoded length of a source of size n bits will be less than a given length such as nc for example a disk drive is easier to control if all blocks of bytes are known to take exactly the same amount of disk real-estate. for some constrained channels we can make a simple to our variable-length encoding and such a guarantee as follows. we two codes two mappings of binary strings to variable-length encodings having the property that for any source string x if the encoding of x under the code is shorter than average then the encoding of x under the second code is longer than average and vice versa. then to transmit a string x we encode the whole string with both codes and send whichever encoding has the shortest length prepended by a suitably encoded single bit to convey which of the two codes is being used. exercise how many valid sequences of length starting with a are there for the run-length-limited channels shown in what are the capacities of these channels? using a computer the matrices q for generating a random path through the trellises of the channel a and the two run-length-limited channels shown in figure state diagrams and connection matrices for channels with maximum runlengths for equal to and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over constrained noiseless channels exercise consider the run-length-limited channel in which any length of run of is permitted and the maximum run length of is a large number l such as nine or ninety. estimate the capacity of this channel. the two terms in a series expansion involving l. what roughly is the form of the optimal matrix q for generating a random path through the trellis of this channel? focus on the values of the elements the probability of generating a given a preceding and the probability of generating a given a preceding run of check your answer by explicit computation for the channel in which the maximum runlength of is nine. variable symbol durations we can add a further frill to the task of communicating over constrained channels by assuming that the symbols we send have durations and that our aim is to communicate at the maximum possible rate per unit time. such channels can come in two unconstrained and constrained. unconstrained channels with variable symbol durations we encountered an unconstrained noiseless channel with variable symbol durations in exercise solve that problem and you ve done this topic. the task is to determine the optimal frequencies with which the symbols should be used given their durations. there is a nice analogy between this task and the task of designing an optimal symbol code when we make an binary symbol code for a source with unequal probabilities pi the optimal message lengths are so pi i similarly when we have a channel whose symbols have durations li some units of time the optimal probability with which those symbols should be used is where is the capacity of the channel in bits per unit time. constrained channels with variable symbol durations once you have grasped the preceding topics in this chapter you should be able to out how to and the capacity of these the trickiest constrained channels. exercise a classic example of a constrained channel with variable symbol durations is the morse channel whose symbols are the dot the dash the short space between letters in morse code the long space between words d d s and s the constraints are that spaces may only be followed by dots and dashes. find the capacity of this channel in bits per unit time assuming that all four symbols have equal durations or that the symbol durations are and time units respectively. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions exercise how well-designed is morse code for english say the probability distribution of exercise how is it to get dna into a narrow tube? to an information theorist the entropy associated with a constrained channel reveals how much information can be conveyed over it. in statistical physics the same calculations are done for a reason to predict the thermodynamics of polymers for example. as a toy example consider a polymer of length n that can either sit in a constraining tube of width l or in the open where there are no constraints. in the open the polymer adopts a state drawn at random from the set of one dimensional random walks with say possible directions per step. the entropy of this walk is log per step i.e. a free energy of the polymer is to be total of n log times this where t is the temperature. in the tube the polymer s onedimensional walk can go in directions unless the wall is in the way so the connection matrix is for example l now what is the entropy of the polymer? what is the change in entropy associated with the polymer entering the tube? if possible obtain an expression as a function of l. use a computer to the entropy of the walk for a particular value of l e.g. and plot the probability density of the polymer s transverse location in the tube. notice the in capacity between two channels one constrained and one unconstrained is directly proportional to the force required to pull the dna into the tube. solutions solution to exercise a transmitted by contains on average one-third and two-thirds if f the fraction of is f f solution to exercise a valid string for channel c can be obtained from a valid string for channel a by inverting it then passing it through an accumulator. these operations are invertible so any valid string for c can also be mapped onto a valid string for a. the only proviso here comes from the edge if we assume that the character transmitted over channel c is preceded by a string of zeroes so that the character is forced to be a then the two channels are exactly equivalent only if we assume that channel a s character must be a zero. solution to exercise with n transmitted bits the largest integer number of source bits that can be encoded is so the maximum rate of a length code with n is figure model of dna squashed in a narrow tube. the dna will have a tendency to pop out of the tube because outside the tube its random walk has greater entropy. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. communication over constrained noiseless channels solution to exercise let the invariant distribution be p s er s where is a normalization constant. the entropy of st given assuming comes from the invariant distribution is hlog el el er s p log p s er s el s log el s s i log log log el now is either or so the contributions from the terms proportional to log are all zero. so here as in chapter st denotes the ensemble whose random variable is the state st. log log el s el xs er el s log el s el log log log el s er s log el s solution to exercise the principal eigenvalues of the connection matrices of the two channels are and the capacities are and bits. solution to exercise the channel is similar to the unconstrained binary channel runs of length greater than l are rare if l is large so we only expect weak from this channel these will show up in contexts where the run length is close to l. the capacity of the channel is very close to one bit. a lower bound on the capacity is obtained by considering the simple variable-length code for this channel which replaces occurrences of the maximum runlength string by and otherwise leaves the source unchanged. the average rate of this code is because the invariant distribution will hit the add an extra zero state a fraction of the time. we can reuse the solution for the variable-length channel in exercise the capacity is the value of such that the equation is the terms in the sum correspond to the possible strings that can be emitted the sum is exactly given by copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions l true capacity n arn we used we anticipate that should be a little less than in order for to equal rearranging and solving approximately for using x x arn r ln we evaluated the true capacities for l and l in an earlier exercise. the table compares the approximate capacity with the true capacity for a selection of values of l. the element will be close to a tiny bit larger since in the unconstrained binary channel when a run of length l has occurred we have a choice of printing or let the probability of selecting be f let us estimate the entropy of the remaining n characters in the stream as a function of f assuming the rest of the matrix q to have been set to its optimal value. the entropy of the next n characters in the stream is the entropy of the bit plus the entropy of the remaining characters which is roughly bits if we select as the bit and bits if is selected. more precisely if c is the capacity of the channel is roughly hthe next n chars f c n c f c n f and setting to zero to the optimal f we obtain f f f f f the probability of emitting a thus decreases from about to about as the number of emitted increases. here is the optimal matrix our rough theory works. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. crosswords and codebreaking in this chapter we make a random walk through a few topics related to language modelling. crosswords the rules of crossword-making may be thought of as a constrained channel. the fact that many valid crosswords can be made demonstrates that this constrained channel has a capacity greater than zero. there are two archetypal crossword formats. in a type a american crossword every row and column consists of a succession of words of length or more separated by one or more spaces. in a type b british crossword each row and column consists of a mixture of words and single characters separated by one or more spaces and every character lies in at least one word or vertical. whereas in a type a crossword every letter lies in a horizontal word and a vertical word in a typical type b crossword only about half of the letters do so the other half lie in one word only. type a crosswords are harder to create than type b because of the constraint that no single characters are permitted. type b crosswords are generally harder to solve because there are fewer constraints per character. why are crosswords possible? if a language has no redundancy then any letters written on a grid form a valid crossword. in a language with high redundancy on the other hand it is hard to make crosswords perhaps a small number of trivial ones. the possibility of making crosswords in a language thus demonstrates a bound on the redundancy of that language. crosswords are not normally written in genuine english. they are written in word-english the language consisting of strings of words from a dictionary separated by spaces. d u f f s t u d g i l d s b p v j d p b a f a r t i t o a d i e u a v a l a n c h e u s h e r t o t o o l a v r i d e r n r l a n e i i a s h m o t h e r g o o s e g a l l e o n n e t t l e s e v i l s c u l t e i n o i w t s t r e s s s o l e b a s r o a s t b e e f n o b e l c i t e s u t t e r r o t m i e u a e h e i r s n e e r c o r e b r e m n e r r o t a t e s m u m a t l a s m a t t e a n e h c t o p e p a u l m i s h a p k i t e s a u s t r a l i a e p i c c a r t e e l p t a e u s i s t e r k e n n y r a h r o c k e t s e x c u s e s a l o h a i r o n t r e e i a t o p k t t s i r e s l a t e e a r l e l t o n d e s p e r a t e s a b r e y s e r a t o m s s a y r r n exercise estimate the capacity of word-english in bits per character. think of word-english as a constrained channel and see exercise figure crosswords of types a and b the fact that many crosswords can be made leads to a lower bound on the entropy of word-english. for simplicity we now model word-english by wenglish the language introduced in section which consists of w words all of length l. the entropy of such a language per character including inter-word spaces is hw w l copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. crosswords we ll that the conclusions we come to depend on the value of hw and are not terribly sensitive to the value of l. consider a large crossword of size s squares in area. let the number of words be fws and let the number of letter-occupied squares be for typical crosswords of types a and b made of words of length l the two fractions fw and have roughly the values in table we now estimate how many crosswords there are of size s using our simple model of wenglish. we assume that wenglish is created at random by generating w strings from a monogram memoryless source with entropy if for example the source used all a characters with equal probability then a bits. if instead we use chapter s distribution then the entropy is the redundancy of wenglish stems from two sources it tends to use some letters more than others and there are only w words in the dictionary. let s now count how many crosswords there are by imagining in the squares of a crossword at random using the same distribution that produced the wenglish dictionary and evaluating the probability that this random scribbling produces valid words in all rows and columns. the total number of typical of the squares in the crossword that can be made is the probability that one word of length l is validly is jtj w and the probability that the whole crossword made of fws words is validly by a single typical is approximately so the log of the number of valid crosswords of size s is estimated to be log s fw log w s fwl which is an increasing function of s only if fwl so arbitrarily many crosswords can be made only if there s enough words in the wenglish dictionary that hw fwl plugging in the values of and fw from table we the following. a b l l l l l l fw table factors fw and by which the number of words and number of letter-squares respectively are smaller than the total number of squares. this calculation underestimates the number of valid wenglish crosswords by counting only crosswords with typical strings. if the monogram distribution is non-uniform then the true count is dominated by atypical in which crossword-friendly words appear more often. crossword type a condition for crosswords hw b hw l l if we set bits and assume there are w words in a normal english-speaker s dictionary all with length l then we that the condition for crosswords of type b is but the condition for crosswords of type a is only just this with my experience that crosswords of type a usually contain more obscure words. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further reading crosswords and codebreaking figure a binary pattern in which every pixel is adjacent to four black and four white pixels. these observations about crosswords were made by shannon i learned about them from wolf and siegel the topic is closely related to the capacity of two-dimensional constrained channels. an example of a two-dimensional constrained channel is a two-dimensional bar-code as seen on parcels. exercise a two-dimensional channel is by the constraint that of the eight neighbours of every interior pixel in an n n rectangular grid four must be black and four white. counts of black and white pixels around boundary pixels are not constrained. a binary pattern satisfying this constraint is shown in what is the capacity of this channel in bits per pixel for large n simple language models the zipfmandelbrot distribution the crudest model for a language is the monogram model which asserts that each successive word is drawn independently from a distribution over words. what is the nature of this distribution over words? zipf s law asserts that the probability of the rth most probable word in a language is approximately p where the exponent has a value close to and is a constant. according to zipf a loglog plot of frequency versus word-rank should show a straight line with slope eter v asserting that the probabilities are given by mandelbrot s of zipf s law introduces a third param p for some documents such as jane austen s emma the zipfmandelbrot distribution well other documents give distributions that are not so well by a zipf mandelbrot distribution. figure shows a plot of frequency versus rank for the latex source of this book. qualitatively the graph is similar to a straight line but a curve is noticeable. to be fair this source is not written in pure english it is a mix of english maths symbols such as x and latex commands. to theand of i is harriet information probability figure fit of the zipfmandelbrot distribution to the empirical frequencies of words in jane austen s emma the parameters are v copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. simple language models the of a is x probability information shannon bayes book figure loglog plot of frequency versus rank for the words in the latex of this book. figure zipf plots for four languages randomly generated from dirichlet processes with parameter ranging from to also shown is the zipf plot for this book. the dirichlet process assuming we are interested in monogram models for languages what model should we use? one in modelling a language is the unboundedness of vocabulary. the greater the sample of language the greater the number of words encountered. a generative model for a language should emulate this property. if asked what is the next word in a newly-discovered work of shakespeare? our probability distribution over words must surely include some non-zero probability for words that shakespeare never used before. our generative monogram model for language should also satisfy a consistency rule called exchangeability. if we imagine generating a new language from our generative model producing an ever-growing corpus of text all statistical properties of the text should be homogeneous the probability of a particular word at a given location in the stream of text should be the same everywhere in the stream. the dirichlet process model is a model for a stream of symbols we think of as words that the exchangeability rule and that allows the vocabulary of symbols to grow without limit. the model has one parameter as the stream of symbols is produced we identify each new symbol by a unique integer w. when we have seen a stream of length f symbols we the probability of the next symbol in terms of the counts ffwg of the symbols seen so far thus the probability that the next symbol is a new symbol never seen before is f the probability that the next symbol is symbol w is fw f figure shows zipf plots plots of symbol frequency versus rank for million-symbol documents generated by dirichlet process priors with values of ranging from to it is evident that a dirichlet process is not an adequate model for observed distributions that roughly obey zipf s law. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. crosswords and codebreaking figure zipf plots for the words of two languages generated by creating successive characters from a dirichlet process with and declaring one character to be the space character. the two curves result from two choices of the space character. with a small tweak however dirichlet processes can produce rather nice zipf plots. imagine generating a language composed of elementary symbols using a dirichlet process with a rather small value of the parameter so that the number of reasonably frequent symbols is about if we then declare one of those symbols called characters rather than words to be a space character then we can identify the strings between the space characters as words if we generate a language in this way then the frequencies of words often come out as very nice zipf plots as shown in which character is selected as the space character determines the slope of the zipf plot a less probable space character gives rise to a richer language with a shallower slope. units of information content the information content of an outcome x whose probability is p is to be hx log p the entropy of an ensemble is an average information content hx p log p when we compare hypotheses with each other in the light of data it is often convenient to compare the log of the probability of the data under the alternative hypotheses log evidence for hi log p jhi or in the case where just two hypotheses are being compared we evaluate the log odds log p p which has also been called the weight of evidence in favour of the log evidence for a hypothesis log p jhi is the negative of the information content of the data d if the data have large information content given a hypothesis then they are surprising to that hypothesis if some other hypothesis is not so surprised by the data then that hypothesis becomes more probable. information content surprise value and log likelihood or log evidence are the same thing. all these quantities are logarithms of probabilities or weighted sums of logarithms of probabilities so they can all be measured in the same units. the units depend on the choice of the base of the logarithm. the names that have been given to these units are shown in table copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a taste of banburismus unit bit nat ban deciban expression that has those units table units of measurement of information content. p loge p p p the bit is the unit that we use most in this book. because the word bit has other meanings a backup name for this unit is the shannon. a byte is bits. a megabyte is bytes. if one works in natural logarithms information contents and weights of evidence are measured in nats. the most interesting units are the ban and the deciban. the history of the ban let me tell you why a factor of ten in probability is called a ban. when alan turing and the other codebreakers at bletchley park were breaking each new day s enigma code their task was a huge inference problem to infer given the day s cyphertext which three wheels were in the enigma machines that day what their starting positions were what further letter substitutions were in use on the steckerboard and not least what the original german messages were. these inferences were conducted using bayesian methods course! and the chosen units were decibans or half-decibans the deciban being judged the smallest weight of evidence discernible to a human. the evidence in favour of particular hypotheses was tallied using sheets of paper that were specially printed in banbury a town about miles from bletchley. the inference task was known as banburismus and the units in which banburismus was played were called bans after that town. a taste of banburismus the details of the code-breaking methods of bletchley park were kept secret for a long time but some aspects of banburismus can be pieced together. i hope the following description of a small part of banburismus is not too how much information was needed? the number of possible settings of the enigma machine was about to deduce the state of the machine it was therefore necessary to about decibans from somewhere as good puts it. banburismus was aimed not at deducing the entire state of the machine but only at out which wheels were in use the logic-based bombes fed with guesses of the plaintext were then used to crack what the settings of the wheels were. the enigma machine once its wheels and plugs were put in place implemented a continually-changing permutation cypher that wandered deterministically through a state space of permutations. because an enormous number of messages were sent each day there was a good chance that whatever state one machine was in when sending one character of a message there would be another machine in the same state while sending a particular character in another message. because the evolution of the machine s state was deterministic the two machines would remain in the same state as each other ve been most helped by descriptions codesandciphers.org.uklectures and by jack good who worked with turing at bletchley. given by tony sale copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. crosswords and codebreaking for the rest of the transmission. the resulting correlations between the outputs of such pairs of machines provided a dribble of information-content from which turing and his co-workers extracted their daily decibans. how to detect that two messages came from machines with a common state sequence the hypotheses are the null hypothesis which states that the machines are in states and that the two plain messages are unrelated and the match hypothesis which says that the machines are in the same state and that the two plain messages are unrelated. no attempt is being made here to infer what the state of either machine is. the data provided are the two cyphertexts x and y let s assume they both have length t and that the alphabet size is a in enigma. what is the probability of the data given the two hypotheses? first the null hypothesis. this hypothesis asserts that the two cyphertexts are given by and x y where the codes ct and are two unrelated time-varying permutations of the alphabet and and are the plaintext messages. an exact computation of the probability of the data y would depend on a language model of the plain text and a model of the enigma machine s guts but if we assume that each enigma machine is an ideal random time-varying permutation then the probability distribution of the two cyphertexts is uniform. all cyphertexts are equally likely. p y for all x y of length t what about this hypothesis asserts that a single time-varying permutation ct underlies both x and y what is the probability of the data y? we have to make some assumptions about the plaintext language. if it were the case that the plaintext language was completely random then the probability of and would be uniform and so would that of x and y so the probability p y would be equal to p y and the two hypotheses and would be indistinguishable. we make progress by assuming that the plaintext is not completely random. both plaintexts are written in a language and that language has redundancies. assume for example that particular plaintext letters are used more often than others. so even though the two plaintext messages are unrelated they are slightly more likely to use the same letters as each other if is true two synchronized letters from the two cyphertexts are slightly more likely to be identical. similarly if a language uses particular bigrams and trigrams frequently then the two plaintext messages will occasionally contain the same bigrams and trigrams at the same time as each other giving rise if is true copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a taste of banburismus u little-jack-horner-sat-in-the-corner-eating-a-christmas-pie--he-put-in-h v ride-a-cock-horse-to-banbury-cross-to-see-a-fine-lady-upon-a-white-horse matches table two aligned pieces of english plaintext u and v with matches marked by notice that there are twelve matches including a run of six whereas the expected number of matches in two completely random strings of length t would be about the two corresponding cyphertexts from two machines in identical states would also have twelve matches. to a little burst of or identical letters. table shows such a coincidence in two plaintext messages that are unrelated except that they are both written in english. the codebreakers hunted among pairs of messages for pairs that were suspiciously similar to each other counting up the numbers of matching monograms bigrams trigrams etc. this method was used by the polish codebreaker rejewski. let s look at the simple case of a monogram language model and estimate how long a message is needed to be able to decide whether two machines are in the same state. i ll assume the source language is monogram-english the language in which successive letters are drawn i.i.d. from the probability distribution fpig of the probability of x and y is nonuniform consider two single characters xt ctut and yt ctvt the probability that they are identical is xutvt p vt xi i m we give this quantity the name m for match probability for both english and german m is about rather than value that would hold for a completely random language. assuming that ct is an ideal random permutation the probability of xt and yt is by symmetry p yt m a if xt yt for xt yt. given a pair of cyphertexts x and y of length t that match in m places and do not match in n places the log evidence in favour of is then log p y p y m log ma n log m log ma n log ma a every match contributes log ma in favour of every non-match contributes log db ma db in favour of match probability for monogram-english coincidental match probability log-evidence for per match log-evidence for per non-match if there were m matches and n non-matches in a pair of length t for example the weight of evidence in favour of would be decibans or a likelihood ratio of to in favour. the expected weight of evidence from a line of text of length t characters is the expectation of which depends on whether or is true. if is true then matches are expected to turn up at rate m and the expected weight of evidence is decibans per characters. if is true m copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. crosswords and codebreaking then spurious matches are expected to turn up at rate and the expected weight of evidence is decibans per characters. typically roughly characters need to be inspected in order to have a weight of evidence greater than a hundred to one decibans in favour of one hypothesis or the other. so two english plaintexts have more matches than two random strings. furthermore because consecutive characters in english are not independent the bigram and trigram statistics of english are nonuniform and the matches tend to occur in bursts of consecutive matches. same observations also apply to german. using better language models the evidence contributed by runs of matches was more accurately computed. such a scoring system was worked out by turing and by good. positive results were passed on to automated and human-powered codebreakers. according to good the longest false-positive that arose in this work was a string of consecutive matches between two machines that were actually in unrelated states. further reading for further reading about turing and bletchley park see hodges and good for an in-depth read about cryptography schneier s book is highly recommended. it is readable clear and entertaining. exercises exercise another weakness in the design of the enigma machine which was intended to emulate a perfectly random time-varying permutation is that it never mapped a letter to itself. when you press q what comes out is always a letter from q. how much information per character is leaked by this design how long a crib would be needed to be that the crib is correctly aligned with the cyphertext? and how long a crib would be needed to be able to identify the correct key? crib is a guess for what the plaintext was. imagine that the brits know that a very important german is travelling from berlin to aachen and they intercept enigma-encoded messages sent to aachen. it is a good bet that one or more of the original plaintext messages contains the string obersturmbannfuehrerxgrafxheinrichxvonxweizsaecker the name of the important chap. a crib could be used in a brute-force approach to the correct enigma key the received messages through all possible engima machines and see if any of the putative decoded texts match the above plaintext. this question centres on the idea that the crib can also be used in a much less expensive manner slide the plaintext crib along all the encoded messages until a perfect mismatch of the crib and the encoded message is found if correct this alignment then tells you a lot about the key. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. why have sex? information acquisition and evolution evolution has been happening on earth for about the last years. undeniably information has been acquired during this process. thanks to the tireless work of the blind watchmaker some cells now carry within them all the information required to be outstanding spiders other cells carry all the information required to make excellent octopuses. where did this information come from? the entire blueprint of all organisms on the planet has emerged in a teaching process in which the teacher is natural selection individuals have more progeny the being by the local environment the other organisms. the teaching signal is only a few bits per individual an individual simply has a smaller or larger number of grandchildren depending on the individual s fitness is a broad term that could cover the ability of an antelope to run faster than other antelopes and hence avoid being eaten by a lion the ability of a lion to be well-enough and run fast enough to catch one antelope per day the ability of a peacock to attract a peahen to mate with it the ability of a peahen to rear many young simultaneously. the of an organism is largely determined by its dna both the coding regions or genes and the non-coding regions play an important role in regulating the transcription of genes. we ll think of as a function of the dna sequence and the environment. how does the dna determine and how does information get from natural selection into the genome? well if the gene that codes for one of an antelope s proteins is defective that antelope might get eaten by a lion early in life and have only two grandchildren rather than forty. the information content of natural selection is fully contained in a of which survived to have children an information content of at most one bit per the teaching signal does not communicate to the ecosystem any description of the imperfections in the organism that caused it to have fewer children. the bits of the teaching signal are highly redundant because throughout a species individuals who are similar to each other will be failing to have for similar reasons. so how many bits per generation are acquired by the species as a whole by natural selection? how many bits has natural selection succeeded in conveying to the human branch of the tree of life since the divergence between copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. why have sex? information acquisition and evolution australopithecines and apes years ago? assuming a generation time of years for reproduction there have been about generations of human precursors since the divergence from apes. assuming a population of individuals each receiving a couple of bits of information from natural selection the total number of bits of information responsible for modifying the genomes of million b.c. into today s human genome is about bits. however as we noted natural selection is not smart at collating the information that it dishes out to the population and there is a great deal of redundancy in that information. if the population size were twice as great would it evolve twice as fast? no because natural selection will simply be correcting the same defects twice as often. john maynard smith has suggested that the rate of information acquisition by a species is independent of the population size and is of order bit per generation. this would allow for only bits of between apes and humans a number that is much smaller than the total size of the human genome bits. human genome contains about nucleotides. it is certainly the case that the genomic overlap between apes and humans is huge but is the that small? in this chapter we ll develop a crude model of the process of information acquisition through evolution based on the assumption that a gene with two defects is typically likely to be more defective than a gene with one defect and an organism with two defective genes is likely to be less than an organism with one defective gene. undeniably this is a crude model since real biological systems are baroque constructions with complex interactions. nevertheless we persist with a simple model because it readily yields striking results. what we from this simple model is that john maynard smith s of bit per generation is correct for an asexually-reproducing population in contrast if the species reproduces sexually the rate of information acquisition can be as large as pg bits per generation where g is the size of the genome. we ll also interesting results concerning the maximum mutation rate that a species can withstand. the model we study a simple model of a reproducing population of n individuals with a genome of size g bits variation is produced by mutation or by recombination sex and truncation selection selects the n children at each generation to be the parents of the next. we striking between populations that have recombination and populations that do not. the genotype of each individual is a vector x of g bits each having a good state xg and a bad state xg the f of an individual is simply the sum of her bits the bits in the genome could be considered to correspond either to genes that have good alleles and bad alleles or to the nucleotides of a genome. we will concentrate on the latter interpretation. the essential property of that we are assuming is that it is locally a roughly linear function of the genome that is that there are many possible changes one f xg g copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. rate of increase of could make to the genome each of which has a small on and that these combine approximately linearly. we the normalized f f we consider evolution by natural selection under two models of variation. variation by mutation. the model assumes discrete generations. at each generation t every individual produces two children. the children s genotypes from the parent s by random mutations. natural selection selects the n progeny in the child population to reproduce and a new generation starts. selection of the n individuals at each generation is known as truncation selection. the simplest model of mutations is that the child s bits fxgg are independent. each bit has a small probability of being which thinking of the bits as corresponding roughly to nucleotides is taken to be a constant m independent of xg. alternatively we thought of the bits as corresponding to genes then we would model the probability of the discovery of a good gene p xg as being a smaller number than the probability of a deleterious mutation in a good gene p xg variation by recombination crossover or sex. our organisms are haploid not diploid. they enjoy sex by recombination. the n individuals in the population are married into m couples at random and each couple has c children with c children being our standard assumption so as to have the population double and halve every generation as before. the c children s genotypes are independent given the parents each child obtains its genotype z by random crossover of its parents genotypes x and y. the simplest model of recombination has no linkage so that zg xg with probability yg with probability once the m c progeny have been born the parents pass away the n progeny are selected by natural selection and a new generation starts. we now study these two models of variation in detail. rate of increase of theory of mutations we assume that the genotype of an individual with normalized f fg is subjected to mutations that bits with probability m. we show that if the average normalized f of the population is greater than then the optimal mutation rate is small and the rate of acquisition of information is at most of order one bit per generation. since it is easy to achieve a normalized of f by simple mutation we ll assume f and work in terms of the excess normalized f if an individual with excess normalized has a child and the mutation rate m is small the probability distribution of the excess normalized of the child has mean child copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. and variance why have sex? information acquisition and evolution m g m g if the population of parents has mean and variance then the child population before selection will have mean and variance natural selection chooses the upper half of this distribution so the mean and variance of at the next generation are given by m g m g where is the mean deviation from the mean measured in standard deviations and is the factor by which the child distribution s variance is reduced by selection. the numbers and are of order for the case of a gaussian distribution and if we assume that the variance is in dynamic equilibrium i.e. then so and the factor in equation is equal to if we take the results for the gaussian distribution an approximation that becomes poorest when the discreteness of becomes important i.e. for small m. the rate of increase of normalized is thus dt m which assuming is maximized for df g at which point mopt df so the rate of increase of f f g is at most df dt per generation for a population with low the rate of increase of the rate of increase if may exceed unit per generation. indeed if m is of order pg this initial spurt can last only of order pg generations. for the rate of increase of is smaller than one per generation. as the approaches g the optimal mutation rate tends to m so that an average of bits are per genotype and the rate of increase of is also equal to information is gained at a rate of about bits per generation. it takes about generations for the genotypes of all individuals in the population to attain perfection. for m the is given by c copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. rate of increase of no sex sex histogram of parents histogram of children s selected children s figure why sex is better than sex-free reproduction. if mutations are used to create variation among children then it is unavoidable that the average of the children is lower than the parents the greater the variation the greater the average selection bumps up the mean again. in contrast recombination produces variation without a decrease in average the typical amount of variation scales as pg where g is the genome size so after selection the average rises by opg. subject to the constraint where c is a constant of integration equal to if f if the mean number of bits per genotype mg exceeds then the f approaches an equilibrium value feqm this theory is somewhat inaccurate in that the true probability distribution of is non-gaussian asymmetrical and quantized to integer values. all the same the predictions of the theory are not grossly at variance with the results of simulations described below. theory of sex the analysis of the sexual population becomes tractable with two approximations we assume that the gene-pool mixes rapidly that correlations between genes can be neglected second we assume homogeneity i.e. that the fraction fg of bits g that are in the good state is the same f for all g. given these assumptions if two parents of f f g mate the probability distribution of their children s has mean equal to the parents f the variation produced by sex does not reduce the average the standard deviation of the of the children scales as pgf f since after selection the increase in is proportional to this standard deviation the increase per generation scales as the square root of the size of the genome pg. as shown in box the mean f g evolves in accordance with the equation d dt f where the solution of this equation is for t c where c is a constant of integration c so this idealized system reaches a state of eugenic perfection within a time generations. pg f simulations figure shows the of a sexual population of n individuals with a genome size of g starting from a random initial state with normalized it also shows the theoretical curve f from equation which remarkably well. in contrast and show the evolving when variation is produced by mutation at rates m and m respectively. note the in the horizontal scales from panel copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. why have sex? information acquisition and evolution how does f depend on f let s assume the two parents of a child both have exactly f good bits and by our homogeneity assumption that those bits are independent random subsets of the g bits. the number of bits that are good in both parents is roughly f and the number that are good in one parent only is roughly so the of the child will be f plus the sum of fair coin which has a binomial distribution of mean f f and variance f f the of a child is thus roughly distributed as fchild normal mean f variance f f box details of the theory of sex. the important property of this distribution contrasted with the distribution under mutation is that the mean is equal to the parents the variation produced by sex does not reduce the average if we include the parental population s variance which we will write as f f the children s are distributed as fchild normal mean f variance f f natural selection selects the children on the upper side of this distribution. the mean increase in will be and the variance of the surviving children will be f f f f where then the factor in is and if there is dynamic equilibrium we conclude that under sex and natural selection the mean of the population increases at a rate proportional to the square root of the size of the genome this constant to be d dt f f bits per generation copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the maximal tolerable mutation rate figure fitness as a function of time. the genome size is g the dots show the of six randomly selected individuals from the birth population at each generation. the initial population of n had randomly generated genomes with f variation produced by sex alone. line shows theoretical curve for homogeneous population. variation produced by mutation with and without sex when the mutation rate is mg or bits per genome. the dashed line shows the curve figure maximal tolerable mutation rate shown as number of errors per genome versus normalized f fg. left panel genome size g right g independent of genome size a parthenogenetic species sex can tolerate only of order error per genome per generation a species that uses recombination can tolerate far greater mutation rates. sex no sex sex no sex g g mg with sex without sex with sex without sex f f exercise dependence on population size. how do the results for a sexual population depend on the population size? we anticipate that there is a minimum population size above which the theory of sex is accurate. how is that minimum population size related to g? exercise dependence on crossover mechanism. in the simple model of sex each bit is taken at random from one of the two parents that is we allow crossovers to occur with probability between any two adjacent nucleotides. how is the model if the crossover probability is smaller? if crossovers occur exclusively at hot-spots located every d bits along the genome? the maximal tolerable mutation rate what if we combine the two models of variation? what is the maximum mutation rate that can be tolerated by a species that has sex? the rate of increase of is given by df dt m f f g copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. why have sex? information acquisition and evolution which is positive if the mutation rate m f f g let us compare this rate with the result in the absence of sex which from equation is that the maximum tolerable mutation rate is the tolerable mutation rate with sex is of order pg times greater than that without sex! m g a parthenogenetic species could try to wriggle out of this bound on its mutation rate by increasing its litter sizes. but if mutation on average mg bits the probability that no bits are in one genome is roughly so a mother needs to have roughly emg in order to have a good chance of having one child with the same as her. the litter size of a non-sexual species thus has to be exponential in mg mg is bigger than if the species is to persist. so the maximum tolerable mutation rate is pinned close to for a nonsexual species whereas it is a larger number of order for a species with recombination. turning these results around we can predict the largest possible genome size for a given mutation rate m. for a parthenogenetic species the largest genome size is of order and for a sexual species taking the m as the mutation rate per nucleotide per generation and keightley and allowing for a maximum brood size of is mg we predict that all species with more than g coding nucleotides make at least occasional use of recombination. if the brood size is then this number falls to g fitness increase and information acquisition for this simple model it is possible to relate increasing to information acquisition. if the bits are set at random the is roughly f if evolution leads to a population in which all individuals have the maximum f g then g bits of information have been acquired by the species namely for each bit xg the species has out which of the two states is the better. we the information acquired at an intermediate to be the amount of selection in bits required to select the perfect state from the gene pool. let a fraction fg of the population have xg because is the information required to a black ball in an urn containing black and white balls in the ratio f we the information acquired to be bits fg i if all the fractions fg are equal to fg then g i g which is well approximated by the rate of information acquisition is thus roughly two times the rate of increase of in the population. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. discussion discussion these results quantify the well known argument for why species reproduce by sex with recombination namely that recombination allows useful mutations to spread more rapidly through the species and allows deleterious mutations to be more rapidly cleared from the population smith felsenstein maynard smith maynard smith and a population that reproduces by recombination can acquire information from natural selection at a rate of order pg times faster than a parthenogenetic population and it can tolerate a mutation rate that is of order pg times greater. for genomes of size g coding nucleotides this factor of pg is substantial. this enormous advantage conferred by sex has been noted before by kondrashov but this meme which kondrashov calls the deterministic mutation hypothesis does not seem to have throughout the evolutionary research community as there are still numerous papers in which the prevalence of sex is viewed as a mystery to be explained by elaborate mechanisms. the cost of males stability of a gene for sex or parthenogenesis why do people declare sex to be a mystery? the main motivation for being is an idea called the cost of males sexual reproduction is disadvantageous compared with asexual reproduction it s argued because of every two produced by sex one average is a useless male incapable of child-bearing and only one is a productive female. in the same time a parthenogenetic mother could give birth to two female clones. to put it another way the big advantage of parthenogenesis from the point of view of the individual is that one is able to pass on of one s genome to one s children instead of only thus if there were two versions of a species one reproducing with and one without sex the single mothers would be expected to outstrip their sexual cousins. the simple model presented thus far did not include either genders or the ability to convert from sexual reproduction to asexual but we can easily modify the model. we modify the model so that one of the g bits in the genome determines whether an individual prefers to reproduce parthenogenetically or sexually the results depend on the number of children had by a single parthenogenetic mother kp and the number of children born by a sexual couple ks. both ks and ks are reasonable models. the former ks would seem most appropriate in the case of unicellular organisms where the cytoplasm of both parents goes into the children. the latter ks is appropriate if the children are solely nurtured by one of the parents so single mothers have just as many as a sexual pair. i concentrate on the latter model since it gives the greatest advantage to the parthenogens who are supposedly expected to outbreed the sexual community. because parthenogens have four children per generation the maximum tolerable mutation rate for them is twice the expression derived before for kp if the is large the maximum tolerable rate is mg initially the genomes are set randomly with f with half of the population having the gene for parthenogenesis. figure shows the outcome. during the learning phase of evolution in which the is increasing rapidly pockets of parthenogens appear but then disappear within a couple of generations as their sexual cousins overtake them in and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. why have sex? information acquisition and evolution figure results when there is a gene for parthenogenesis and no interbreeding and single mothers produce as many children as sexual couples. g n mg mg vertical axes show the of the two sub-populations and the percentage of the population that is parthenogenetic. mg mg s e s s e n t i f e g a t n e c r e p sexual fitness parthen fitness sexual fitness parthen fitness in the presence of a higher mutation rate however leave them behind. once the population reaches its top however the parthenogens can take over if the mutation rate is low the parthenogens never take over. the breadth of the sexual population s is of order pg so a mutant parthenogenetic colony arising with slightly above-average will last for about pgmg generations before its falls below that of its sexual cousins. as long as the population size is large for some sexual individuals to survive for this time sex will not die out. in a unstable environment where the function is continually changing the parthenogens will always lag behind the sexual community. these results are consistent with the argument of haldane and hamilton that sex is helpful in an arms race with parasites. the parasites an function which changes with time and a sexual population will always ascend the current function more rapidly. additive function of course our results depend on the function that we assume and on our model of selection. is it reasonable to model to order as a sum of independent terms? maynard smith argues that it is the more good genes you have the higher you come in the pecking order for example. the directional selection model has been used extensively in theoretical population genetic studies we might expect real functions to involve interactions in which case crossover might reduce the average however since recombination gives the biggest advantage to species whose functions are additive we might predict that evolution will have favoured species that used a representation of the genome that corresponds to a function that has only weak interactions. and even if there are interactions it seems plausible that the would still involve a sum of such interacting terms with the number of terms being some fraction of the genome size g. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises exercise investigate how fast sexual and asexual species evolve if they have a function with interactions. for example let the be a sum of exclusive-ors of pairs of bits compare the evolving with those of the sexual and asexual species with a simple additive function. furthermore if the function were a highly nonlinear function of the genotype it could be made more smooth and locally linear by the baldwin the baldwin hinton and nowlan has been widely studied as a mechanism whereby learning guides evolution and it could also act at the level of transcription and translation. consider the evolution of a peptide sequence for a new purpose. assume the of the peptide is a highly nonlinear function of the sequence perhaps having a small island of good sequences surrounded by an ocean of equally bad sequences. in an organism whose transcription and translation machinery is the will be an equally nonlinear function of the dna sequence and evolution will wander around the ocean making progress towards the island only by a random walk. in contrast an organism having the same dna sequence but whose dna-to-rna transcription or rna-to-protein translation is faulty will occasionally by mistranslation or mistranscription accidentally produce a working enzyme and it will do so with greater probability if its dna sequence is close to a good sequence. one cell might produce proteins from the one mrna sequence of which have no enzymatic and one does. the one working catalyst will be enough for that cell to have an increased relative to rivals whose dna sequence is further from the island of good sequences. for this reason i conjecture that at least early in evolution and perhaps still now the genetic code was not implemented perfectly but was implemented noisily with some codons coding for a distribution of possible amino acids. this noisy code could even be switched on and from cell to cell in an organism by having multiple aminoacyl-trna synthetases some more reliable than others. whilst our model assumed that the bits of the genome do not interact ignored the fact that the information is represented redundantly assumed that there is a direct relationship between phenotypic and the genotype and assumed that the crossover probability in recombination is high i believe these qualitative results would still hold if more complex models of and crossover were used the relative of sex will still scale as pg. only in small in-bred populations are the of sex expected to be diminished. in summary why have sex? because sex is good for your bits! further reading how did a high-information-content self-replicating system ever emerge in the place? in the general area of the origins of life and other tricky questions about evolution i highly recommend maynard smith and maynard smith and kondrashov maynard smith ridley dyson cairns-smith and further exercises exercise how good must the error-correcting machinery in dna replication be given that mammals have not all died out long ago? estimate the probability of nucleotide substitution per cell division. appendix copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. why have sex? information acquisition and evolution exercise given that dna replication is achieved by bumbling brownian motion and ordinary thermodynamics in a biochemical porridge at a temperature of c it s astonishing that the error-rate of dna replication is about per replicated nucleotide. how can this reliability be achieved given that the energetic between a correct base-pairing and an incorrect one is only one or two hydrogen bonds and the thermal energy kt is only about a factor of four smaller than the free energy associated with a hydrogen bond? if ordinary thermodynamics is what favours correct base-pairing surely the frequency of incorrect base-pairing should be about f where is the free energy i.e. an error frequency of f how has dna replication cheated thermodynamics? the situation is equally perplexing in the case of protein synthesis which translates an mrna sequence into a polypeptide in accordance with the genetic code. two chemical reactions are protected against errors the binding of trna molecules to amino acids and the production of the polypeptide in the ribosome which involves base-pairing. again the is high error rate of about and this can t be caused by the energy of the correct state being especially low the correct polypeptide sequence is not expected to be lower in energy than any other sequence. how do cells perform error correction? like dna replication exercise while the genome acquires information through natural selection at a rate of a few bits per generation your brain acquires information at a greater rate. estimate at what rate new information can be stored in long term memory by your brain. think of learning the words of a new language for example. solutions solution to exercise for small enough n whilst the average of the population increases some unlucky bits become frozen into the bad state. bad genes are sometimes known as hitchhikers. the homogeneity assumption breaks down. eventually all individuals have identical genotypes that are mainly but contain some too. the smaller the population the greater the number of frozen is expected to be. how small can the population size n be if the theory of sex is accurate? we experimentally that the theory based on assuming homogeneity poorly only if the population size n is smaller than if n is smaller than pg information cannot possibly be acquired at a rate as big as pg since the information content of the blind watchmaker s decisions cannot be any greater than bits per generation this being the number of bits required to specify which of the children get to reproduce. baum et al. analyzing a similar model show that the population size n should be about pglog to make hitchhikers unlikely to arise. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. part iv probabilities and inference copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about part iv the number of inference problems that can perhaps should be tackled by bayesian inference methods is enormous. in this book for example we discuss the decoding problem for error-correcting codes the task of inferring clusters from data the task of interpolation through noisy data and the task of classifying patterns given labelled examples. most techniques for solving these problems can be categorized as follows. exact methods compute the required quantities directly. only a few interesting problems have a direct solution but exact methods are important as tools for solving subtasks within larger problems. methods for the exact solution of inference problems are the subject of chapters and approximate methods can be subdivided into deterministic approximations which include maximum likelihood laplace s method and and variational methods and monte carlo methods techniques in which random numbers play an integral part which will be discussed in chapters and this part of the book does not form a one-dimensional story. rather the ideas make up a web of interrelated threads which will recombine in subsequent chapters. chapter which is an honorary member of this part discussed a range of simple examples of inference problems and their bayesian solutions. to give further motivation for the toolbox of inference methods discussed in this part chapter discusses the problem of clustering subsequent chapters discuss the probabilistic interpretation of clustering as mixture modelling. chapter discusses the option of dealing with probability distributions by completely enumerating all hypotheses. chapter introduces the idea of maximization methods as a way of avoiding the large cost associated with complete enumeration and points out reasons why maximum likelihood is not good enough. chapter reviews the probability distributions that arise most often in bayesian inference. chapters and discuss another way of avoiding the cost of complete enumeration marginalization. chapter discusses message-passing methods appropriate for graphical models using the decoding of error-correcting codes as an example. chapter combines these ideas with message-passing concepts from chapters and these chapters are a prerequisite for the understanding of advanced error-correcting codes. chapter discusses deterministic approximations including laplace s method. this chapter is a prerequisite for understanding the topic of complexity control in learning algorithms an idea that is discussed in general terms in chapter copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about part iv chapter discusses monte carlo methods. chapter gives details of state-of-the-art monte carlo techniques. chapter introduces the ising model as a test-bed for probabilistic methods. an exact message-passing method and a monte carlo method are demonstrated. a motivation for studying the ising model is that it is intimately related to several neural network models. chapter describes exact monte carlo methods and demonstrates their application to the ising model. chapter discusses variational methods and their application to ising models and to simple statistical inference problems including clustering. this chapter will help the reader understand the network and the em algorithm which is an important method in latent-variable modelling. chapter discusses a particularly simple latent variable model called independent component analysis. chapter discusses a ragbag of assorted inference topics. chapter discusses a simple example of decision theory. chapter discusses between sampling theory and bayesian methods. a theme what inference is about a widespread misconception is that the aim of inference is to the most probable explanation for some data. while this most probable hypothesis may be of interest and some inference methods do locate it this hypothesis is just the peak of a probability distribution and it is the whole distribution that is of interest. as we saw in chapter the most probable outcome from a source is often not a typical outcome from that source. similarly the most probable hypothesis given some data may be atypical of the whole set of reasonablyplausible hypotheses. about chapter before reading the next chapter exercise and section the input to a gaussian channel are recommended reading. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. an example inference task clustering human brains are good at regularities in data. one way of expressing regularity is to put a set of objects into groups that are similar to each other. for example biologists have found that most objects in the natural world fall into one of two categories things that are brown and run away and things that are green and don t run away. the group they call animals and the second plants. we ll call this operation of grouping things together clustering. if the biologist further sub-divides the cluster of plants into subclusters we would call this hierarchical clustering but we won t be talking about hierarchical clustering yet. in this chapter we ll just discuss ways to take a set of n objects and group them into k clusters. there are several motivations for clustering. first a good clustering has predictive power. when an early biologist encounters a new green thing he has not seen before his internal model of plants and animals in predictions for attributes of the green thing it s unlikely to jump on him and eat him if he touches it he might get grazed or stung if he eats it he might feel sick. all of these predictions while uncertain are useful because they help the biologist invest his resources example the time spent watching for predators well. thus we perform clustering because we believe the underlying cluster labels are meaningful will lead to a more description of our data and will help us choose better actions. this type of clustering is sometimes called mixture density modelling and the objective function that measures how well the predictive model is working is the information content of the data log second clusters can be a useful aid to communication because they allow lossy compression. the biologist can give directions to a friend such as go to the third tree on the right then take a right turn than go past the large green thing with red berries then past the large green thing with thorns then the brief category name tree is helpful because it is to identify an object. similarly in lossy image compression the aim is to convey in as few bits as possible a reasonable reproduction of a picture one way to do this is to divide the image into n small patches and a close match to each patch in an alphabet of k image-templates then we send a close to the image by sending the list of labels kn of the matching templates. the task of creating a good library of image-templates is equivalent to a set of cluster centres. this type of clustering is sometimes called vector quantization we can formalize a vector quantizer in terms of an assignment rule x kx for assigning datapoints x to one of k codenames and a reconstruction rule k mk the aim being to choose the functions kx and mk so as to copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. k-means clustering minimize the expected distortion which might be to be d p ideal objective function would be to minimize the psychologically perceived distortion of the image. since it is hard to quantify the distortion perceived by a human vector quantization and lossy compression are not so crisply problems as data modelling and lossless compression. in vector quantization we don t necessarily believe that the templates fmkg have any natural meaning they are simply tools to do a job. we note in passing the similarity of the assignment rule the encoder of vector quantization to the decoding problem when decoding an error-correcting code. a third reason for making a cluster model is that failures of the cluster model may highlight interesting objects that deserve special attention. if we have trained a vector quantizer to do a good job of compressing satellite pictures of ocean surfaces then maybe patches of image that are not well compressed by the vector quantizer are the patches that contain ships! if the biologist encounters a green thing and sees it run slither away this with his cluster model says green things don t run away cues him to pay special attention. one can t spend all one s time being fascinated by things the cluster model can help sift out from the multitude of objects in one s world the ones that really deserve attention. a fourth reason for liking clustering algorithms is that they may serve as models of learning processes in neural systems. the clustering algorithm that we now discuss the k-means algorithm is an example of a competitive learning algorithm. the algorithm works by having the k clusters compete with each other for the right to own the data points. k-means clustering figure n data points. the k-means algorithm is an algorithm for putting n data points in an idimensional space into k clusters. each cluster is parameterized by a vector mk called its mean. the data points will be denoted by fxng where the superscript n runs from to the number of data points n each vector x has i components xi. we will assume that the space that x lives in is a real space and that we have a metric that distances between points for example about the name... as far as i know the k in k-means clustering simply refers to the chosen number of clusters. if newton had followed the same naming policy maybe we would learn at school about calculus for the variable x it s a silly name but we are stuck with it. dx y to start the k-means algorithm the k means fmkg are initialized in some way for example to random values. k-means is then an iterative two-step algorithm. in the assignment step each data point n is assigned to the nearest mean. in the update step the means are adjusted to match the sample means of the data points that they are responsible for. the k-means algorithm is demonstrated for a toy two-dimensional data set in where means are used. the assignments of the points to the two clusters are indicated by two point styles and the two means are shown by the circles. the algorithm converges after three iterations at which point the assignments are unchanged so the means remain unmoved when updated. the k-means algorithm always converges to a point. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. an example inference task clustering algorithm the k-means clustering algorithm. initialization. set k means fmkg to random values. assignment step. each data point n is assigned to the nearest mean. we denote our guess for the cluster kn that the point xn belongs to by fdmk xng argmin k an alternative equivalent representation of this assignment of points to clusters is given by responsibilities which are indicator variables rn to one if mean k is the closest mean to datapoint xn otherwise rn k in the assignment step we set rn is zero. k k rn k if if k k what about ties? we don t expect two means to be exactly the same distance from a data point but if a tie does happen is set to the smallest of the winning fkg. update step. the model parameters the means are adjusted to match the sample means of the data points that they are responsible for. mk xn rn k xn rk where rk is the total responsibility of mean k rk rn k what about means with no responsibilities? if rk then we leave the mean mk where it is. repeat the assignment step and update step until the assign ments do not change. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. k-means clustering data figure k-means algorithm applied to a data set of points. k means evolve to stable locations after three iterations. assignment update assignment update assignment update run run figure k-means algorithm applied to a data set of points. two separate runs both with k means reach solutions. each frame shows a successive assignment step. exercise see if you can prove that k-means always converges. a physical analogy and an associated lyapunov function. lyapunov function is a function of the state of the algorithm that decreases whenever the state changes and that is bounded below. if a system has a lyapunov function then its dynamics converge. the k-means algorithm with a larger number of means is demonstrated in the outcome of the algorithm depends on the initial condition. in the case after iterations a steady state is found in which the data points are fairly evenly split between the four clusters. in the second case after six iterations half the data points are in one cluster and the others are shared among the other three clusters. questions about this algorithm the k-means algorithm has several ad hoc features. why does the update step set the mean to the mean of the assigned points? where did the distance d come from? what if we used a measure of distance between x and m? how can we choose the best distance? vector quantization the distance copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. an example inference task clustering figure k-means algorithm for a case with two dissimilar clusters. the n large data. a stable set of assignments and means. note that four points belonging to the broad cluster have been incorrectly assigned to the narrower cluster. assigned to the right-hand cluster are shown by plus signs. figure two elongated clusters and the stable solution found by the k-means algorithm. function is provided as part of the problem but i m assuming we are interested in data-modelling rather than vector quantization. how do we choose k? having found multiple alternative clusterings for a given k how can we choose among them? cases where k-means might be viewed as failing. further questions arise when we look for cases where the algorithm behaves badly with what the man in the street would call clustering figure shows a set of data points generated from a mixture of two gaussians. the right-hand gaussian has less weight one of the data points and it is a less broad cluster. figure shows the outcome of using k-means clustering with k means. four of the big cluster s data points have been assigned to the small cluster and both means end up displaced to the left of the true centres of the clusters. the k-means algorithm takes account only of the distance between the means and the data points it has no representation of the weight or breadth of each cluster. consequently data points that actually belong to the broad cluster are incorrectly assigned to the narrow cluster. figure shows another case of k-means behaving badly. the data evidently fall into two elongated clusters. but the only stable state of the k-means algorithm is that shown in the two clusters have been sliced in half! these two examples show that there is something wrong with the distance d in the k-means algorithm. the k-means algorithm has no way of representing the size or shape of a cluster. a criticism of k-means is that it is a hard rather than a soft algorithm points are assigned to exactly one cluster and all points assigned to a cluster are equals in that cluster. points located near the border between two or more clusters should arguably play a partial role in determining the locations of all the clusters that they could plausibly be assigned to. but in the k-means algorithm each borderline point is dumped in one cluster and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. soft k-means clustering has an equal vote with all the other points in that cluster and no vote in any other clusters. soft k-means clustering these criticisms of k-means motivate the soft k-means algorithm algorithm the algorithm has one parameter which we could term the assignment step. each data point xn is given a soft degree of assignment to each of the means. we call the degree to which xn is assigned to cluster k the responsibility rn responsibility of cluster k for point n. k algorithm soft k-means algorithm version rn k dmk the sum of the k responsibilities for the nth point is update step. the model parameters the means are adjusted to match the sample means of the data points that they are responsible for. mk xn rn k xn rk where rk is the total responsibility of mean k rk rn k notice the similarity of this soft k-means algorithm to the hard k-means algorithm the update step is identical the only is that the responsibilities rn can take on values between and whereas the assignment in the k-means algorithm involved a min over the distances the rule for assigning the responsibilities is a soft-min k exercise show that as the goes to the soft k-means algo rithm becomes identical to the original hard k-means algorithm except for the way in which means with no assigned points behave. describe what those means do instead of sitting still. dimensionally the is an inverse-length-squared so we can as sociate a lengthscale with it. the soft k-means algorithm is demonstrated in the lengthscale is shown by the radius of the circles surrounding the four means. each panel shows the point reached for a value of the lengthscale conclusion at this point we may have some of the problems with the original kmeans algorithm by introducing an extra complexity-control parameter but how should we set and what about the problem of the elongated clusters copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. large an example inference task clustering figure soft k-means algorithm version applied to a data set of points. k implicit lengthscale parameter varied from a large to a small value. each picture shows the state of all four means with the implicit lengthscale shown by the radius of the four circles after running the algorithm for several tens of iterations. at the largest lengthscale all four means converge exactly to the data mean. then the four means separate into two groups of two. at shorter lengthscales each of these pairs itself bifurcates into subgroups. small and the clusters of unequal weight and width? adding one parameter is not going to make all these problems go away. we ll come back to these questions in a later chapter as we develop the mixture-density-modelling view of clustering. further reading for a vector-quantization approach to clustering see luttrell exercises exercise explore the properties of the soft k-means algorithm version assuming that the datapoints fxg come from a single separable two-dimensional gaussian distribution with mean zero and variances set k assume n is large and investigate the points of the algorithm as is varied. assume that and with exercise consider the soft k-means algorithm applied to a large amount of one-dimensional data that comes from a mixture of two equalweight gaussians with true means and standard deviation for example show that the hard k-means algorithm with k leads to a solution in which the two means are further apart than the two true means. discuss what happens for other values of and the value of such that the soft algorithm puts the two means in the correct places. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solutions solution to exercise we can associate an energy with the state of the k-means algorithm by connecting a spring between each point xn and the mean that is responsible for it. the energy of one spring is proportional to its squared length namely mk where is the of the spring. the total energy of all the springs is a lyapunov function for the algorithm because the assignment step can only decrease the energy a point only changes its allegiance if the length of its spring would be reduced the update step can only decrease the energy moving mk to the mean is the way to minimize the energy of its springs and the energy is bounded below which is the second condition for a lyapunov function. since the algorithm has a lyapunov function it converges. if the means are initialized to solution to exercise and the assignment step for a point at location gives and the updated m is r p r p p figure schematic diagram of the bifurcation as the largest data variance increases from below to above the data variance is indicated by the ellipse. now m is a point but the question is is it stable or unstable? for tiny m is we can taylor-expand so z p for small m m either grows or decays exponentially under this mapping depending on whether is greater than or less than the point m is stable if if and unstable otherwise. this derivation shows that this result is general holding for any true probability distribution p having variance not just the gaussian. then there is a bifurcation and there are two stable points surrounding the unstable point at m to illustrate this bifurcation shows the outcome of running the soft k-means algorithm with on one-dimensional data with standard deviation for various values of figure shows this pitchfork bifurcation from the other point of view where the data s standard deviation is and the algorithm s lengthscale is varied on the horizontal axis. data density mean locations figure the stable mean locations as a function of for constant found numerically lines and the approximation lines. data density mean locns. figure the stable mean locations as a function of for constant copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. an example inference task clustering here is a cheap theory to model how the parameters behave beyond the bifurcation based on continuing the series expansion. this continuation of the series is rather suspect since the series isn t necessarily expected to converge beyond the bifurcation point but the theory well anyway. we take our analytic approach one term further in the expansion then we can solve for the shape of the bifurcation to leading order which depends on the fourth moment of the distribution z p we use the fact that p is gaussian to the fourth moment. this map has a point at m such that i.e. m the thin line in shows this theoretical approximation. figure shows the bifurcation as a function of for shows the bifurcation as a function of for exercise why does the pitchfork in tend to the values as give an analytic expression for this asymptote. solution to exercise the asymptote is the mean of the gaussian r normalx dx copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact inference by complete enumeration we open our toolbox of methods for handling probabilities by discussing a brute-force inference method complete enumeration of all hypotheses and evaluation of their probabilities. this approach is an exact method and the of carrying it out will motivate the smarter exact and approximate methods introduced in the following chapters. the burglar alarm bayesian probability theory is sometimes called common sense when thinking about the following questions please ask your common sense what it thinks the answers are we will then see how bayesian methods your everyday intuition. example fred lives in los angeles and commutes miles to work. whilst at work he receives a phone-call from his neighbour saying that fred s burglar alarm is ringing. what is the probability that there was a burglar in his house today? while driving home to investigate fred hears on the radio that there was a small earthquake that day near his home. oh he says feeling relieved it was probably the earthquake that set the alarm what is the probability that there was a burglar in his house? pearl let s introduce variables b burglar was present in fred s house today a alarm is ringing p receives a phonecall from the neighbour reporting the alarm e small earthquake took place today near fred s house and r radio report of earthquake is heard by fred. the probability of all these variables might factorize as follows p e a p r p b ep ap j e and plausible values for the probabilities are burglar probability p p e.g. gives a mean burglary rate of once every three years. earthquake probability p p jearthquake jburglar jphonecall figure belief network for the burglar alarm problem. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact inference by complete enumeration with e.g. our assertion that the earthquakes are independent of burglars i.e. the prior probability of b and e is p e p seems reasonable unless we take into account opportunistic burglars who strike immediately after earthquakes. alarm ringing probability we assume the alarm will ring if any of the following three events happens a burglar enters the house and triggers the alarm s assume the alarm has a reliability of i.e. of burglars trigger the alarm an earthquake takes place and triggers the alarm of alarms are triggered by earthquakes? or some other event causes a false alarm let s assume the false alarm rate f is so fred has false alarms from non-earthquake causes once every three years. type of dependence of a on b and e is known as a noisy-or the probabilities of a given b and e are then p b e f p b e f p b e f p b e f p b e f p b e f p b e f p b e f or in numbers p b e p b e p b e p b e p b e p b e p b e p b e we assume the neighbour would never phone if the alarm is not ringing a and that the radio is a trustworthy reporter too e we won t need to specify the probabilities p a or p e in order to answer the questions above since the outcomes p and r give us certainty respectively that a and e we can answer the two questions about the burglar by computing the posterior probabilities of all hypotheses given the available information. let s start by reminding ourselves that the probability that there is a burglar before either p or r is observed is p and the probability that an earthquake took place is p and these two propositions are independent. first when p we know that the alarm is ringing a the posterior probability of b and e becomes p ej a p b ep p the numerator s four possible values are p b e p p p b e p p p b e p p p b e p p the normalizing constant is the sum of these four numbers p and the posterior probabilities are p e a p e a p e a p e a copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact inference for continuous hypothesis spaces to answer the question what s the probability a burglar was there? we marginalize over the earthquake variable e p a p e a p e a p a p e a p e a so there is nearly a chance that there was a burglar present. it is important to note that the variables b and e which were independent a priori are now dependent. the posterior distribution is not a separable function of b and e. this fact is illustrated most simply by studying the of learning that e when we learn e the posterior probability of b is given by p e a p e a a i.e. by dividing the bottom two rows of by their sum p a the posterior probability of b is p e a p e a there is thus now an chance that a burglar was in fred s house. it is in accordance with everyday intuition that the probability that b possible cause of the alarm reduces when fred learns that an earthquake an alternative explanation of the alarm has happened. explaining away this phenomenon that one of the possible causes of some data data in this case being a becomes less probable when another of the causes becomes more probable even though those two causes were independent variables a priori is known as explaining away. explaining away is an important feature of correct inferences and one that any intelligence should replicate. if we believe that the neighbour and the radio service are unreliable or capricious so that we are not certain that the alarm really is ringing or that an earthquake really has happened the calculations become more complex but the explaining-away persists the arrival of the earthquake report r simultaneously makes it more probable that the alarm truly is ringing and less probable that the burglar was present. in summary we solved the inference questions about the burglar by enumerating all four hypotheses about the variables e their posterior probabilities and marginalizing to obtain the required inferences about b. exercise after fred receives the phone-call about the burglar alarm but before he hears the radio report what from his point of view is the probability that there was a small earthquake today? exact inference for continuous hypothesis spaces many of the hypothesis spaces we will consider are naturally thought of as continuous. for example the unknown decay length of section lives in a continuous one-dimensional space and the unknown mean and standard deviation of a gaussian live in a continuous two-dimensional space. in any practical computer implementation such continuous spaces will necessarily be discretized however and so can in principle be enumerated at a grid of parameter values for example. in we plotted the likelihood copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact inference by complete enumeration figure enumeration of an entire hypothesis space for one gaussian with parameters axis and function for the decay length as a function of by evaluating the likelihood at a series of points. a two-parameter model let s look at the gaussian distribution as an example of a model with a twodimensional hypothesis space. the one-dimensional gaussian distribution is parameterized by a mean and a standard deviation p normalx figure shows an enumeration of one hundred hypotheses about the mean and standard deviation of a one-dimensional gaussian distribution. these hypotheses are evenly spaced in a ten by ten square grid covering ten values of and ten values of each hypothesis is represented by a picture showing the probability density that it puts on x. we now examine the inference of and given data points xn n n assumed to be drawn independently from this density. imagine that we acquire data for example the points shown in we can now evaluate the posterior probability of each of the one hundred subhypotheses by evaluating the likelihood of each that is the value of p j the likelihood values are shown diagrammatically in using the line thickness to encode the value of the likelihood. subhypotheses with likelihood smaller than times the maximum likelihood have been deleted. using a grid we can represent the same information by plotting the likelihood as a surface plot or contour plot as a function of and a mixture model eyeballing the data you might agree that it seems more plausible that they come not from a single gaussian but from a mixture of two gaussians by two means two standard deviations and two mixing the horizontal figure five datapoints coordinate is the value of the datum xn the vertical coordinate has no meaning. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact inference for continuous hypothesis spaces figure likelihood function given the data of represented by line thickness. subhypotheses having likelihood smaller than times the maximum likelihood are not shown. sigma mean mean sigma figure the likelihood function for the parameters of a gaussian distribution. surface plot and contour plot of the log likelihood as a function of and the data set of n points had mean and s copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact inference by complete enumeration figure enumeration of the entire hypothesis space for a mixture of two gaussians. weight of the mixture components is in the top half and in the bottom half. means and vary horizontally and standard deviations and vary vertically. and satisfying p let s enumerate the subhypotheses for this alternative model. the parameter space is so it becomes challenging to represent it on a single page. figure enumerates subhypotheses with values of the parameters the means are varied between values each in the horizontal directions. the standard deviations take on four values each vertically. and takes on two values vertically. we can represent the inference about these parameters in the light of the datapoints as shown in if we wish to compare the one-gaussian model with the mixture-of-two model we can the models posterior probabilities by evaluating the marginal likelihood or evidence for each model h p the evidence is given by integrating over the parameters the integration can be implemented numerically by summing over the alternative enumerated values of p p where p is the prior distribution over the grid of parameter values which i take to be uniform. for the mixture of two gaussians this integral is a integral if it is to be performed at all accurately the grid of points will need to be much than the grids shown in the if the uncertainty about each of k parameters has been reduced by say a factor of ten by observing the data then brute-force integration requires a grid of at least points. this copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact inference for continuous hypothesis spaces figure inferring a mixture of two gaussians. likelihood function given the data of represented by line thickness. the hypothesis space is identical to that shown in subhypotheses having likelihood smaller than times the maximum likelihood are not shown hence the blank regions which correspond to hypotheses that the data have ruled out. exponential growth of computation with model size is the reason why complete enumeration is rarely a feasible computational strategy. exercise imagine a mixture of ten gaussians to data in a twenty-dimensional space. estimate the computational cost of implementing inferences for this model by enumeration of a grid of parameter values. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. maximum likelihood and clustering rather than enumerate all hypotheses which may be exponential in number we can save a lot of time by homing in on one good hypothesis that the data well. this is the philosophy behind the maximum likelihood method which the setting of the parameter vector that maximizes the likelihood p j for some models the maximum likelihood parameters can be instantly from the data for more complex models the maximum likelihood parameters may require an iterative algorithm. for any model it is usually easiest to work with the logarithm of the likelihood rather than the likelihood since likelihoods being products of the probabilities of many data points tend to be very small. likelihoods multiply log likelihoods add. maximum likelihood for one gaussian we return to the gaussian for our examples. assume we have data fxngn the log likelihood is ln p j the likelihood can be expressed in terms of two functions of the data the sample mean and the sum of square deviations n xnn s ln p j because the likelihood depends on the data only through and s these two quantities are known as statistics. example the log likelihood with respect to and show that if the standard deviation is known to be the maximum likelihood mean of a gaussian is equal to the sample mean for any value of solution. ln p n when copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. maximum likelihood for one gaussian figure the likelihood function for the parameters of a gaussian distribution. surface plot and contour plot of the log likelihood as a function of and the data set of n points had mean and s the posterior probability of for various values of the posterior probability of for various values of as a density over ln sigma sigma mean mean r o i r e t s o p mean if we taylor-expand the log likelihood about the maximum we can approximate error bars on the maximum likelihood parameter we use a quadratic approximation to estimate how far from the maximum-likelihood parameter setting we can go before the likelihood falls by some standard factor for example or in the special case of a likelihood that is a gaussian function of the parameters the quadratic approximation is exact. example find the second derivative of the log likelihood with respect to and the error bars on given the data and solution. ln p n comparing this curvature with the curvature of the log of a gaussian distribution over of standard deviation we can deduce that the error bars on from the likelihood function are which is pn the error bars have this property at the two points the likelihood is smaller than its maximum value by a factor of example find the maximum likelihood standard deviation of a gaussian whose mean is known to be in the light of data fxngn find the second derivative of the log likelihood with respect to ln and error bars on ln solution. the likelihood s dependence on is ln p j stot where stot pnxn to the maximum of the likelihood we can with respect to ln s often most hygienic to with copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. maximum likelihood and clustering respect to ln u rather than u when u is a scale variable we use dundln u nun. ln p ln j stot this derivative is zero when stot n i.e. the second derivative is n ln p j stot and at the maximum-likelihood value of this equals so error bars on ln are exercise show that the values of and ln that jointly maximize the likelihood are where n maximum likelihood for a mixture of gaussians we now derive an algorithm for a mixture of gaussians to onedimensional data. in fact this algorithm is so important to understand that you gentle reader get to derive the algorithm. please work through the following exercise. exercise a random variable x is assumed to have a probability distribution that is a mixture of two gaussians p pk where the two gaussians are given the labels k and k the prior probability of the class label k is are the means of the two gaussians and both have standard deviation for brevity we denote these parameters by a data set consists of n points fxngn which are assumed to be independent samples from this distribution. let kn denote the unknown class label of the nth point. assuming that and are known show that the posterior probability of the class label kn of the nth point can be written as p xn p xn copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. enhancements to soft k-means and give expressions for and assume now that the means are not known and that we wish to infer them from the data fxngn standard deviation is known. in the remainder of this question we will derive an iterative algorithm for values for that maximize the likelihood p p let l denote the natural log of the likelihood. show that the derivative of the log likelihood with respect to is given by l pkjn where pkjn p k j xn appeared above at equation show neglecting terms in is approximately given by p k j xn that the second derivative k l pkjn hence show that from an initial state an approximate newtonraphson step updates these parameters to where pn pkjnxn pn pkjn newtonraphson method for maximizing updates to h assuming that sketch a contour plot of the likelihood function as a function of and for the data set shown above. the data set consists of points. describe the peaks in your sketch and indicate their widths. notice that the algorithm you have derived for maximizing the likelihood is identical to the soft k-means algorithm of section now that it is clear that clustering can be viewed as mixture-density-modelling we are able to derive enhancements to the k-means algorithm which rectify the problems we noted earlier. enhancements to soft k-means algorithm shows a version of the soft-k-means algorithm corresponding to a modelling assumption that each cluster is a spherical gaussian having its own width cluster has its own k. the algorithm updates the lengthscales for itself. the algorithm also includes cluster weight parameters which also update themselves allowing accurate modelling of data from clusters of unequal weights. this algorithm is demonstrated in for two data sets that we ve seen before. the second example shows copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. maximum likelihood and clustering assignment step. the responsibilities are algorithm the soft k-means algorithm version rn k k dmk where i is the dimensionality of x. update step. each cluster s parameters mk and k are adjusted to match the data points that it is responsible for. rn k xn rk mk xn k xn rn k where rk is the total responsibility of mean k irk rk pk rk rk rn k t t t t t figure soft k-means algorithm with k applied to the data set of to the little n large data set of t t t t t t rn k i exp i xn qi with in place of k i i i xn rn k i i mk rk i algorithm the soft k-means algorithm version which corresponds to a model of axis-aligned gaussians. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a fatal of maximum likelihood t t t t figure soft k-means algorithm version applied to the data consisting of two cigar-shaped clusters. k t t t t t figure soft k-means algorithm version applied to the little n large data set. k that convergence can take a long time but eventually the algorithm the small cluster and the large cluster. soft k-means version is a maximum-likelihood algorithm for a mixture of spherical gaussians to data spherical meaning that the variance of the gaussian is the same in all directions. this algorithm is still no good at modelling the cigar-shaped clusters of if we wish to model the clusters by axis-aligned gaussians with possibly-unequal variances we replace the assignment rule and the variance update rule by the rules and displayed in algorithm this third version of soft k-means is demonstrated in on the two cigars data set of after iterations the algorithm correctly locates the two clusters. figure shows the same algorithm applied to the little n large data set again the correct cluster locations are found. a fatal of maximum likelihood finally sounds a cautionary note when we k means to our toy data set we sometimes that very small clusters form covering just one or two data points. this is a pathological property of soft k-means clustering versions and exercise investigate what happens if one mean mk sits exactly on k is small top of one data point show that if the variance then no return is possible k becomes ever smaller. t t t t a proof that the algorithm does indeed maximize the likelihood is deferred to section figure soft k-means algorithm applied to a data set of points. k notice that at convergence one very small cluster has formed between two data points. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. kaboom! maximum likelihood and clustering soft k-means can blow up. put one cluster exactly on one data point and let its variance go to zero you can obtain an arbitrarily large likelihood! maximum likelihood methods can break down by highly tuned models that part of the data perfectly. this phenomenon is known as the reason we are not interested in these solutions with enormous likelihood is this sure these parameter-settings may have enormous posterior probability density but the density is large over only a very small volume of parameter space. so the probability mass associated with these likelihood spikes is usually tiny. we conclude that maximum likelihood methods are not a satisfactory general solution to data-modelling problems the likelihood may be large at certain parameter settings. even if the likelihood does not have spikes the maximum of the likelihood is often unrepresentative in highdimensional problems. even in low-dimensional problems maximum likelihood solutions can be unrepresentative. as you may know from basic statistics the maximum likelihood estimator for a gaussian s standard deviation n is a biased estimator a topic that we ll take up in chapter the maximum a posteriori method a popular replacement for maximizing the likelihood is maximizing the bayesian posterior probability density of the parameters instead. however multiplying the likelihood by a prior and maximizing the posterior does not make the above problems go away the posterior density often also has spikes and the maximum of the posterior probability density is often unrepresentative of the whole posterior distribution. think back to the concept of typicality which we encountered in chapter in high dimensions most of the probability mass is in a typical set whose properties are quite from the points that have the maximum probability density. maxima are atypical. a further reason for disliking the maximum a posteriori is that it is basisdependent. if we make a nonlinear change of basis from the parameter to the parameter u f then the probability density of is transformed to p p the maximum of the density p will usually not coincide with the maximum of the density p illustrating such nonlinear changes of basis see the next chapter. it seems undesirable to use a method whose answers change when we change representation. further reading the soft k-means algorithm is at the heart of the automatic package autoclass et al. hanson et al. further exercises exercises where maximum likelihood may be useful exercise make a version of the k-means algorithm that models the data as a mixture of k arbitrary gaussians i.e. gaussians that are not constrained to be axis-aligned. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises exercise a photon counter is pointed at a remote star for one minute in order to infer the brightness i.e. the rate of photons arriving at the counter per minute assuming the number of photons collected r has a poisson distribution with mean p j r! what is the maximum likelihood estimate for given r find error bars on ln same situation but now we assume that the counter detects not only photons from the star but also background photons. the background rate of photons is known to be b photons per minute. we assume the number of photons collected r has a poisson distribution with mean now given r detected photons what is the maximum likelihood estimate for comment on this answer discussing also the bayesian posterior distribution and the unbiased estimator of sampling theory r b. exercise a bent coin is tossed n times giving na heads and nb tails. assume a beta distribution prior for the probability of heads p for example the uniform distribution. find the maximum likelihood and maximum a posteriori values of p then the maximum likelihood and maximum a posteriori values of the logit a compare with the predictive distribution i.e. the probability that the next toss will come up heads. exercise two men looked through prison bars one saw stars the other tried to infer where the window frame was. from the other side of a room you look through a window and see stars at locations fxn yng. you can t see the window edges because it is totally dark apart from the stars. assuming the window is rectangular and that the visible stars locations are independently randomly distributed what are the inferred values of ymin xmax ymax according to maximum likelihood? sketch the likelihood as a function of xmax for xmin ymin and ymax. exercise a sailor infers his location y by measuring the bearings of three buoys whose locations yn are given on his chart. let the true bearings of the buoys be assuming that his measurement of each bearing is subject to gaussian noise of small standard deviation what is his inferred location by maximum likelihood? the sailor s rule of thumb says that the boat s position can be taken to be the centre of the cocked hat the triangle produced by the intersection of the three measured bearings can you persuade him that the maximum likelihood answer is better? exercise maximum likelihood of an exponential-family model. assume that a variable x comes from a probability distribution of the form p w zw exp xk wkfkx! ymax ymin b q q b a q a q a qq a a a a a a b figure the standard way of drawing three slightly inconsistent bearings on a chart produces a triangle called a cocked hat. where is the sailor? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. maximum likelihood and clustering where the functions fkx are given and the parameters w fwkg are not known. a data set fxng of n points is supplied. show by the log likelihood that the maximum-likelihood parameters wml satisfy p wmlfkx xx n xn fkxn where the left-hand sum is over all x and the right-hand sum is over the data points. a shorthand for this result is that each function-average under the model must equal the function-average found in the data hfkip j wml hfkidata exercise maximum entropy of models to constraints. when confronted by a probability distribution p about which only a few facts are known the maximum entropy principle a rule for choosing a distribution that those constraints. according to maxent you should select the p that maximizes the entropy h p log subject to the constraints. assuming the constraints assert that the averages of certain functions fkx are known i.e. hfkip fk show by introducing lagrange multipliers for each constraint including normalization that the maximum-entropy distribution has the form exp xk wkfkx! p z where the parameters z and fwkg are set such that the constraints are and hence the maximum entropy method gives identical results to maximum likelihood of an exponential-family model exercise. the maximum entropy method has sometimes been recommended as a method for assigning prior distributions in bayesian modelling. while the outcomes of the maximum entropy method are sometimes interesting and thought-provoking i do not advocate maxent as the approach to assigning priors. maximum entropy is also sometimes proposed as a method for solving inference problems for example given that the mean score of this unfair six-sided die is what is its probability distribution i think it is a bad idea to use maximum entropy in this way it can give silly answers. the correct way to solve inference problems is to use bayes theorem. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises exercises where maximum likelihood and map have a b c d-g scientist xn a b c d e f g figure seven measurements fxng of a parameter by seven scientists each having his own noise-level exercise this exercise explores the idea that maximizing a probability density is a poor way to a point that is representative of the density. consider a gaussian distribution in a k-dimensional space p w show that nearly all of the probability mass of a gaussian is in a thin shell of radius r and of thickness proportional to rpk. for example in dimensions of the mass of a gaussian with is in a shell of radius and thickness however the probability density at the origin is times bigger than the density at this shell where most of the probability mass is. i now consider two gaussian densities in dimensions that in radius by just and that contain equal total probability mass. show that the maximum probability density is greater at the centre of the gaussian with smaller by a factor of in ill-posed problems a typical posterior distribution is often a weighted superposition of gaussians with varying means and standard deviations so the true posterior has a skew peak with the maximum of the probability density located near the mean of the gaussian distribution that has the smallest standard deviation not the gaussian with the greatest weight. exercise the seven scientists. n datapoints fxng are drawn from n distributions all of which are gaussian with a common mean but with unknown standard deviations what are the maximum likelihood parameters given the data? for example seven scientists b c d e f g with experimental skills measure you expect some of them to do accurate work to have small and some of them to turn in wildly inaccurate answers to have enormous figure shows their seven results. what is and how reliable is each scientist? i hope you agree that intuitively it looks pretty certain that a and b are both inept measurers that dg are better and that the true value of is somewhere close to but what does maximizing the likelihood tell you? exercise problems with map method. a collection of widgets i k have a property called wodge wi which we measure widget by widget in noisy experiments with a known noise level our model for these quantities is that they come from a gaussian prior p j where w is not known. our prior for this variance is over log from to scenario suppose four widgets have been measured and give the following data we are interested in inferring the wodges of these four widgets. find the values of w and that maximize the posterior probability p log d. marginalize over and the posterior probability density of w given the data. skills required. see mackay for solution. find maxima of p j d. two maxima one at wmp with error bars on all four parameters copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. maximum likelihood and clustering from gaussian approximation to the posterior and one at with error bars scenario suppose in addition to the four measurements above we are now informed that there are four more widgets that have been measured with a much less accurate instrument having thus we now have both well-determined and ill-determined parameters as in a typical ill-posed problem. the data from these measurements were a string of uninformative values we are again asked to infer the wodges of the widgets. intuitively our inferences about the well-measured widgets should be negligibly by this vacuous information about the poorly-measured widgets. but what happens to the map method? find the values of w and that maximize the posterior probability p log d. find maxima of p j d. only one maximum wmp with error bars on all eight parameters solutions solution to exercise figure shows a contour plot of the likelihood function for the data points. the peaks are pretty-near centred on the points and and are pretty-near circular in their contours. the width of each of the peaks is a standard deviation of the peaks are roughly gaussian in shape. solution to exercise the log likelihood is ln p w ln zw xk wkfkxn figure the likelihood as a function of and ln p w ln zw fkx now the fun part is what happens when we the log of the normalizing constant exp zwxx fkx xx p wfkx ln zw so exp zwxx ln p w p wfkx fkx and at the maximum of the likelihood xx p wmlfkx n xn fkxn copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. useful probability distributions in bayesian data modelling there s a small collection of probability distributions that come up again and again. the purpose of this chapter is to introduce these distributions so that they won t be intimidating when encountered in combat situations. there is no need to memorize any of them except perhaps the gaussian if a distribution is important enough it will memorize itself and otherwise it can easily be looked up. distributions over integers binomial poisson exponential we already encountered the binomial distribution and the poisson distribution on page the binomial distribution for an integer r with parameters f bias f and n number of trials is f p j f n r ng the binomial distribution arises for example when we a bent coin with bias f n times and observe the number of heads r. the poisson distribution with parameter is p j r! r the poisson distribution arises for example when we count the number of photons r that arrive in a pixel during a interval given that the mean intensity on the pixel corresponds to an average number of photons the exponential distribution on integers p j f f f r arises in waiting problems. how long will you have to wait until a six is rolled if a fair six-sided dice is rolled? answer the probability distribution of the number of rolls r is exponential over integers with parameter f the distribution may also be written p j f f r where r figure the binomial distribution p j f n on a linear scale and a logarithmic scale r figure the poisson distribution p j on a linear scale and a logarithmic scale copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. useful probability distributions distributions over unbounded real numbers gaussian student cauchy biexponential inverse-cosh. the gaussian distribution or normal distribution with mean and standard deviation is p z where x z it is sometimes useful to work with the quantity which is called the precision parameter of the gaussian. a sample z from a standard univariate gaussian can be generated by computing z where and are uniformly distributed in a second sample independent of the can then be obtained for free. the gaussian distribution is widely used and often asserted to be a very common distribution in the real world but i am sceptical about this assertion. yes unimodal distributions may be common but a gaussian is a special rather extreme unimodal distribution. it has very light tails the logprobability-density decreases quadratically. the typical deviation of x from is but the respective probabilities that x deviates from by more than and are and in my experience deviations from a mean four or times greater than the typical deviation may be rare but not as rare as i therefore urge caution in the use of gaussian distributions if a variable that is modelled with a gaussian actually has a heavier-tailed distribution the rest of the model will contort itself to reduce the deviations of the outliers like a sheet of paper being crushed by a rubber band. exercise pick a variable that is supposedly bell-shaped in probability distribution gather data and make a plot of the variable s empirical distribution. show the distribution as a histogram on a log scale and investigate whether the tails are well-modelled by a gaussian distribution. example of a variable to study is the amplitude of an audio signal. one distribution with heavier tails than a gaussian is a mixture of gaussians. a mixture of two gaussians for example is by two means two standard deviations and two mixing and satisfying p if we take an appropriately weighted mixture of an number of gaussians all having mean we obtain a student-t distribution where p s n z z figure three unimodal distributions. two student distributions with parameters s line cauchy distribution and line and a gaussian distribution with mean and standard deviation line shown on linear vertical scales and logarithmic vertical scales notice that the heavy tails of the cauchy distribution are scarcely evident in the upper bell-shaped curve copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. distributions over positive real numbers and n is called the number of degrees of freedom and is the gamma function. if n then the student distribution has a mean and that mean is if n the distribution also has a variance as n the student distribution approaches the normal distribution with mean and standard deviation s. the student distribution arises both in classical statistics the sampling-theoretic distribution of certain statistics and in bayesian inference the probability distribution of a variable coming from a gaussian distribution whose standard deviation we aren t sure of. in the special case n the student distribution is called the cauchy distribution. a distribution whose tails are intermediate in heaviness between student and gaussian is the biexponential distribution p s z s x where z the inverse-cosh distribution p is a popular model in independent component analysis. in the limit of large the probability distribution p becomes a biexponential distribution. in the limit p approaches a gaussian with mean zero and variance distributions over positive real numbers exponential gamma inverse-gamma and log-normal. the exponential distribution where p s z x x z s arises in waiting problems. how long will you have to wait for a bus in poissonville given that buses arrive independently at random with one every s minutes on average? answer the probability distribution of your wait x is exponential with mean s. the gamma distribution is like a gaussian distribution except whereas the gaussian goes from to gamma distributions go from to just as the gaussian distribution has two parameters and which control the mean and width of the distribution the gamma distribution has two parameters. it is the product of the one-parameter exponential distribution with a polynomial the exponent c in the polynomial is the second parameter. p s c s c where z x x x z copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. useful probability distributions x l ln x figure two gamma distributions with parameters c lines and lines shown on linear vertical scales and logarithmic vertical scales and shown as a function of x on the left and l ln x on the right this is a simple peaked distribution with mean sc and variance it is often natural to represent a positive real variable x in terms of its logarithm l ln x. the probability density of l is p p zl xl s xl s p where zl gamma distribution is named after its normalizing constant an odd convention it seems to me! figure shows a couple of gamma distributions as a function of x and of l. notice that where the original gamma distribution may have a spike at x the distribution over l never has such a spike. the spike is an artefact of a bad choice of basis. in the limit sc c we obtain the noninformative prior for a scale parameter the prior. this improper prior is called noninformative because it has no associated length scale no characteristic value of x so it prefers all values of x equally. it is invariant under the reparameterization x mx. if we transform the probability density into a density over l ln x we the latter density is uniform. exercise imagine that we reparameterize a positive variable x in terms of its cube root u if the probability density of x is the improper distribution what is the probability density of u? the gamma distribution is always a unimodal density over l ln x and as can be seen in the it is asymmetric. if x has a gamma distribution and we decide to work in terms of the inverse of x v we obtain a new distribution in which the density over l is left-for-right the probability density of v is called an inverse-gamma distribution p j s c where zv v zv copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. distributions over periodic variables v ln v figure two inverse gamma distributions with parameters c lines and lines shown on linear vertical scales and logarithmic vertical scales and shown as a function of x on the left and l ln x on the right. gamma and inverse gamma distributions crop up in many inference problems in which a positive quantity is inferred from data. examples include inferring the variance of gaussian noise from some noise samples and inferring the rate parameter of a poisson distribution from the count. gamma distributions also arise naturally in the distributions of waiting times between poisson-distributed events. given a poisson process with rate the probability density of the arrival time x of the mth event is log-normal distribution another distribution over a positive real number x is the log-normal distribution which is the distribution that results when l ln x has a normal distribution. we m to be the median value of x and s to be the standard deviation of ln x. p j m s z ln l z where implies p m s x z x ln x figure two log-normal distributions with parameters s line and line shown on linear vertical scales and logarithmic vertical scales they really do have the same value of the median m distributions over periodic variables a periodic variable is a real number having the property that and are equivalent. a distribution that plays for periodic variables the role played by the gaus sian distribution for real variables is the von mises distribution p j z exp the normalizing constant is z where is a bessel function. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. useful probability distributions a distribution that arises from brownian around the circle is the wrapped gaussian distribution p j distributions over probabilities beta distribution dirichlet distribution entropic distribution the beta distribution is a probability density over a variable p that is a probability p p the parameters may take any positive value. the normalizing constant is the beta function special cases include the uniform distribution the prior and the improper laplace prior if we transform the beta distribution to the corresponding density over the logit l ln p p we it is always a pleasant bell-shaped density over l while the density over p may have singularities at p and p more dimensions figure three beta distributions with and the upper shows p as a function of p the lower shows the corresponding density over the logit ln p p notice how well-behaved the densities are as a function of the logit. the dirichlet distribution is a density over an i-dimensional vector p whose i components are positive and sum to the beta distribution is a special case of a dirichlet distribution with i the dirichlet distribution is parameterized by a measure u vector with all ui which i will write here as u where m is a normalized measure over the i components mi and is positive p i pi dirichletipj to the simplex such that p is normalized i.e. pi pi the normalizing the function is the dirac delta function which restricts the distribution constant of the dirichlet distribution is i the vector m is the mean of the probability distribution z dirichletipj p dip m when working with a probability vector p it is often helpful to work in the softmax basis in which for example a three-dimensional probability p is represented by three numbers satisfying and pi z eai where z eai. this nonlinear transformation is analogous to the ln transformation for a scale variable and the logit transformation for a single probability p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. distributions over probabilities u u u ln p dirichlet distribution disappear and the density is given by in the softmax basis the ugly minus-ones in the exponents in the p i i ai the role of the parameter can be characterized in two ways. first measures the sharpness of the distribution it measures how we expect typical samples p from the distribution to be from the mean m just as the precision of a gaussian measures how far samples stray from its mean. a large value of produces a distribution over p that is sharply peaked around m. the of in higher-dimensional situations can be visualized by drawing a typical sample from the distribution dirichletipj with m set to the uniform vector mi and making a zipf plot that is a ranked plot of the values of the components pi. it is traditional to plot both pi axis and the rank axis on logarithmic scales so that power law relationships appear as straight lines. figure shows these plots for a single sample from ensembles with i and i and with from to for large the plot is shallow with many components having similar values. for small typically one component pi receives an overwhelming share of the probability and of the small probability that remains to be shared among the other components another component receives a similarly large share. in the limit as goes to zero the plot tends to an increasingly steep power law. second we can characterize the role of in terms of the predictive distribution that results when we observe samples from p and obtain counts f fi of the possible outcomes. the value of the number of samples from p that are required in order that the data dominate over the prior in predictions. exercise the dirichlet distribution a nice additivity property. imagine that a biased six-sided die has two red faces and four blue faces. the die is rolled n times and two bayesians examine the outcomes in order to infer the bias of the die and make predictions. one bayesian has access to the redblue colour outcomes only and he infers a twocomponent probability vector pb. the other bayesian has access to each full outcome he can see which of the six faces came up and he infers a six-component probability vector where figure three dirichlet distributions over a three-dimensional probability vector the upper show random draws from each distribution showing the values of and on the two axes. the triangle in the is the simplex of legal probability distributions. the lower show the same points in the softmax basis the two axes show and i i figure zipf plots for random samples from dirichlet distributions with various values of for each value of i or and each one sample p from the dirichlet distribution was generated. the zipf plot shows the probabilities pi ranked by magnitude versus their rank. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. useful probability distributions pr and pb assuming that the second bayesian assigns a dirichlet distribution to with hyperparameters show that in order for the bayesian s inferences to be consistent with those of the second bayesian the bayesian s prior should be a dirichlet distribution with hyperparameters hint a brute-force approach is to compute the integral p pb r p u a cheaper approach is to compute the predictive distributions given arbitrary data and the condition for the two predictive distributions to match for all data. the entropic distribution for a probability vector p is sometimes used in the maximum entropy image reconstruction community. p m m pi where m the measure is a positive vector and dklpjjm pi log pimi. further reading see and peto for fun with dirichlets. further exercises exercise n datapoints fxng are drawn from a gamma distribution p s c s c with unknown parameters s and c. what are the maximum likelihood parameters s and c? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization how can we avoid the exponentially large cost of complete enumeration of all hypotheses? before we stoop to approximate methods we explore two approaches to exact marginalization marginalization over continuous variables known as nuisance parameters by doing integrals and second summation over discrete variables by message-passing. exact marginalization over continuous parameters is a macho activity enjoyed by those who are in integration. this chapter uses gamma distributions as was explained in the previous chapter gamma distributions are a lot like gaussian distributions except that whereas the gaussian goes from to gamma distributions go from to inferring the mean and variance of a gaussian distribution we discuss again the one-dimensional gaussian distribution parameterized by a mean and a standard deviation p normalx when inferring these parameters we must specify their prior distribution. the prior gives us the opportunity to include knowledge that we have about and independent experiments or on theoretical grounds for example. if we have no such knowledge then we can construct an appropriate prior that embodies our supposed ignorance. in section we assumed a uniform prior over the range of parameters plotted. if we wish to be able to perform exact marginalizations it may be useful to consider conjugate priors these are priors whose functional form combines naturally with the likelihood such that the inferences have a convenient form. conjugate priors for and the conjugate prior for a mean is a gaussian we introduce two hyperparameters and which parameterize the prior on and write p in the limit we obtain the noninformative prior for a location parameter the prior. this is noninformative because it is invariant under the natural reparameterization c. the prior p const is also an improper prior that is it is not normalizable. the conjugate prior for a standard deviation is a gamma distribution which has two parameters and it is most convenient to the prior copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization density of the inverse variance precision parameter p b this is a simple peaked distribution with mean and variance in the limit we obtain the noninformative prior for a scale parameter the prior. this is noninformative because it is invariant under the reparameterization the prior is less strange-looking if we examine the resulting density over ln or ln which is this is the prior that expresses ignorance about by saying well it could be or it could be or it could be scale variables such as are usually best represented in terms of their logarithm. again this noninformative prior is improper. in the following examples i will use the improper noninformative priors for and using improper priors is viewed as distasteful in some circles so let me excuse myself by saying it s for the sake of readability if i included proper priors the calculations could still be done but the key points would be obscured by the of extra parameters. maximum likelihood and marginalization and the task of inferring the mean and standard deviation of a gaussian distribution from n samples is a familiar one though maybe not everyone understands the between the and buttons on their calculator. let us recap the formulae then derive them. given data d fxngn an estimator of is reminder when we change variables from to a one-to-one function of the probability density transforms from to here the jacobian is pll ln and two estimators of are n xnn and n there are two principal paradigms for statistics sampling theory and bayesian inference. in sampling theory known as frequentist or orthodox statistics one invents estimators of quantities of interest and then chooses between those estimators using some criterion measuring their sampling properties there is no clear principle for deciding which criterion to use to measure the performance of an estimator nor for most criteria is there any systematic procedure for the construction of optimal estimators. in bayesian inference in contrast once we have made explicit all our assumptions about the model and the data our inferences are mechanical. whatever question we wish to pose the rules of probability theory give a unique answer which consistently takes into account all the given information. human-designed estimators and intervals have no role in bayesian inference human input only enters into the important tasks of designing the hypothesis space is the of the model and all its probability distributions and out how to do the computations that implement inference in that space. the answers to our questions are probability distributions over the quantities of interest. we often that the estimators of sampling theory emerge automatically as modes or means of these posterior distributions when we choose a simple hypothesis space and turn the handle of bayesian inference. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. inferring the mean and variance of a gaussian distribution figure the likelihood function for the parameters of a gaussian distribution repeated from surface plot and contour plot of the log likelihood as a function of and the data set of n points had mean and s notice that the maximum is skew in the two estimators of standard deviation have values and the posterior probability of for various values of as a density over ln the posterior probability of p j d assuming a prior on obtained by projecting the probability mass in onto the axis. the maximum of p j d is at by contrast the maximum of p j d is at probabilities are shows as densities over ln sigma sigma mean mean psigmad in sampling theory the estimators above can be motivated as follows. is an unbiased estimator of which out of all the possible unbiased estimators of has smallest variance this variance is computed by averaging over an ensemble of imaginary experiments in which the data samples are assumed to come from an unknown gaussian distribution. the estimator n is the maximum likelihood estimator for the estimator is biased however the expectation of given averaging over many imagined experiments is not exercise give an intuitive explanation why the estimator is biased. this bias motivates the invention in sampling theory of which can be shown to be an unbiased estimator. or to be precise it is that is an unbiased estimator of we now look at some bayesian inferences for this problem assuming noninformative priors for and the emphasis is thus not on the priors but rather on the likelihood function and the concept of marginalization. the joint posterior probability of and is proportional to the likelihood function illustrated by a contour plot in the log likelihood is ln p j where s pnxn given the gaussian model the likelihood can be expressed in terms of the two functions of the data and s so these two quantities are known as statistics the posterior probability of and is using the improper priors p jfxngn p j p n p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization this function describes the answer to the question given the data and the noninformative priors what might and be? it may be of interest to the parameter values that maximize the posterior probability though it should be emphasized that posterior probability maxima have no fundamental status in bayesian inference since their location depends on the choice of basis. here we choose the basis ln in which our prior is so that the posterior probability maximum coincides with the maximum of the likelihood. as we saw in exercise the maximum likelihood solution for and ln is there is more to the posterior distribution than just its mode. as can be seen in the likelihood has a skew peak. as we increase the width of the conditional distribution of increases and if we to a sequence of values moving away from the sample mean we obtain a sequence of conditional distributions over whose maxima move to increasing values of the posterior probability of given is p p j p j we note the familiar scaling of the error bars on let us now ask the question given the data and the noninformative priors what might be? this question from the one we asked in that we are now not interested in this parameter must therefore be marginalized over. the posterior probability of is p jfxngn p j p the data-dependent term p j appeared earlier as the normalizing constant in equation one name for this quantity is the evidence or marginal likelihood for we obtain the evidence for by integrating out a noninformative prior p constant is assumed we call this constant so that we can think of the prior as a top-hat prior of width the gaussian integral p j yields j p ln p j s ln the two terms are the log likelihood the log likelihood with the last term is the log of the occam factor which penalizes smaller values of will discuss occam factors more in chapter when we the log evidence with respect to ln to the most probable the additional volume factor shifts the maximum from to intuitively the denominator counts the number of noise measurements contained in the quantity s the sum contains n residuals squared but there are only noise measurements because the determination of one parameter from the data causes one dimension of noise to be gobbled up in unavoidable in the terminology of classical copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises statistics the bayesian s best guess for sets measure of deviance by equal to the number of degrees of freedom n figure shows the posterior probability of which is proportional to the marginal likelihood. this may be contrasted with the posterior probability of with to its most probable value which is shown in and d. the inference we might wish to make is given the data what is exercise marginalize over and obtain the posterior marginal distri bution of which is a student-t distribution p d further reading a bible of exact marginalization is bretthorst s book on bayesian spectrum analysis and parameter estimation. exercises exercise exercise requires macho integration capabilities. give a bayesian solution to exercise where seven scientists of varying capabilities have measured with personal noise levels and we are interested in inferring let the prior on each be a broad prior for example a gamma distribution with parameters c find the posterior distribution of plot it and explore its properties for a variety of data sets such as the one given and the data set fxng the posterior distribution of given and xn p j xn note that the normalizing constant for this inference is p j marginalize over to this normalizing constant then use bayes theorem a second time to p solutions solution to exercise the data points are distributed with mean squared deviation about the true mean. the sample mean is unlikely to exactly equal the true mean. the sample mean is the value of that minimizes the sum squared deviation of the data points from any other value of particular the true value of will have a larger value of the sum-squared deviation that so the expected mean squared deviation from the sample mean is neces sarily smaller than the mean squared deviation about the true mean. a b c d-g copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in trellises in this chapter we will discuss a few exact methods that are used in probabilistic modelling. as an example we will discuss the task of decoding a linear error-correcting code. we will see that inferences can be conducted most by message-passing algorithms which take advantage of the graphical structure of the problem to avoid unnecessary duplication of computations chapter decoding problems a codeword t is selected from a linear k code c and it is transmitted over a noisy channel the received signal is y. in this chapter we will assume that the channel is a memoryless channel such as a gaussian channel. given an assumed channel model p j t there are two decoding problems. the codeword decoding problem is the task of inferring which codeword t was transmitted given the received signal. the bitwise decoding problem is the task of inferring for each transmit ted bit tn how likely it is that that bit was a one rather than a zero. as a concrete example take the hamming code. in chapter we discussed the codeword decoding problem for that code assuming a binary symmetric channel. we didn t discuss the bitwise decoding problem and we didn t discuss how to handle more general channel models such as a gaussian channel. solving the codeword decoding problem by bayes theorem the posterior probability of the codeword t is p y p j tp p likelihood function. the factor in the numerator p j t is the likelihood of the codeword which for any memoryless channel is a separable function p j t n p j tn for example if the channel is a gaussian channel with transmissions and additive noise of standard deviation then the probability density copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decoding problems of the received signal yn in the two cases tn is p j tn p j tn from the point of view of decoding all that matters is the likelihood ratio which for the case of the gaussian channel is p j tn p j tn exercise show that from the point of view of decoding a gaussian channel is equivalent to a time-varying binary symmetric channel with a known noise level fn which depends on n. prior. the second factor in the numerator is the prior probability of the codeword p which is usually assumed to be uniform over all valid codewords. the denominator in is the normalizing constant p p j tp the complete solution to the codeword decoding problem is a list of all codewords and their probabilities as given by equation since the number of codewords in a linear code is often very large and since we are not interested in knowing the detailed probabilities of all the codewords we often restrict attention to a version of the codeword decoding problem. the map codeword decoding problem is the task of identifying the most probable codeword t given the received signal. if the prior probability over codewords is uniform then this task is identical to the problem of maximum likelihood decoding that is identifying the codeword that maximizes p j t. example in chapter for the hamming code and a binary symmetric channel we discussed a method for deducing the most probable codeword from the syndrome of the received signal thus solving the map codeword decoding problem for that case. we would like a more general solution. the map codeword decoding problem can be solved in exponential time order by searching through all codewords for the one that maximizes p j tp but we are interested in methods that are more than this. in section we will discuss an exact method known as the minsum algorithm which may be able to solve the codeword decoding problem more how much more depends on the properties of the code. it is worth emphasizing that map codeword decoding for a general linear code is known to be np-complete means in layman s terms that map codeword decoding has a complexity that scales exponentially with the blocklength unless there is a revolution in computer science. so restricting attention to the map decoding problem hasn t necessarily made the task much less challenging it simply makes the answer briefer to report. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in trellises solving the bitwise decoding problem formally the exact solution of the bitwise decoding problem is obtained from equation by marginalizing over the other bits. p j y p y we can also write this marginal with the aid of a truth function that is one if the proposition s is true and zero otherwise. p y xt p y xt p y p y computing these marginal probabilities by an explicit sum over all codewords t takes exponential time. but for certain codes the bitwise decoding problem can be solved much more using the forwardbackward algorithm. we will describe this algorithm which is an example of the sumproduct algorithm in a moment. both the minsum algorithm and the sumproduct algorithm have widespread importance and have been invented many times in many codes and trellises in chapters and we represented linear k codes in terms of their generator matrices and their parity-check matrices. in the case of a systematic block code the k transmitted bits in each block of size n are the source bits and the remaining m n bits are the parity-check bits. this means that the generator matrix of the code can be written repetition code simple parity code p gt ik and the parity-check matrix can be written h p im where p is an m k matrix. in this section we will study another representation of a linear code called a trellis. the codes that these trellises represent will not in general be systematic codes but they can be mapped onto systematic codes if desired by a reordering of the bits in a block. hamming code figure examples of trellises. each edge in a trellis is labelled by a zero by a square or a one by a cross. of a trellis our will be quite narrow. for a more comprehensive view of trellises the reader should consult kschischang and sorokine a trellis is a graph consisting of nodes known as states or vertices and edges. the nodes are grouped into vertical slices called times and the times are ordered such that each edge connects a node in one time to a node in a neighbouring time. every edge is labelled with a symbol. the leftmost and rightmost states contain only one node. apart from these two extreme nodes all nodes in the trellis have at least one edge connecting leftwards and at least one connecting rightwards. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solving the decoding problems on a trellis a trellis with n times a code of blocklength n as follows a codeword is obtained by taking a path that crosses the trellis from left to right and reading out the symbols on the edges that are traversed. each valid path through the trellis a codeword. we will number the leftmost time time and the rightmost time n we will number the leftmost state state and the rightmost state i where i is the total number of states in the trellis. the nth bit of the codeword is emitted as we move from time to time n. the width of the trellis at a given time is the number of nodes in that time. the maximal width of a trellis is what it sounds like. a trellis is called a linear trellis if the code it is a linear code. we will solely be concerned with linear trellises from now on as nonlinear trellises are much more complex beasts. for brevity we will only discuss binary trellises that is trellises whose edges are labelled with zeroes and ones. it is not hard to generalize the methods that follow to q-ary trellises. figures show the trellises corresponding to the repetition code which has k the parity code with k and the hamming code. exercise that the sixteen codewords listed in table are generated by the trellis shown in observations about linear trellises for any linear code the minimal trellis is the one that has the smallest number of nodes. in a minimal trellis each node has at most two edges entering it and at most two edges leaving it. all nodes in a time have the same left degree as each other and they have the same right degree as each other. the width is always a power of two. a minimal trellis for a linear k code cannot have a width greater than since every node has at least one valid codeword through it and there are only codewords. furthermore if we m n k the minimal trellis s width is everywhere less than this will be proved in section notice that for the linear trellises in all of which are minimal trellises k is the number of times a binary branch point is encountered as the trellis is traversed from left to right or from right to left. we will discuss the construction of trellises more in section but we now know enough to discuss the decoding problem. solving the decoding problems on a trellis we can view the trellis of a linear code as giving a causal description of the probabilistic process that gives rise to a codeword with time from left to right. each time a divergence is encountered a random source source of information bits for communication determines which way we go. at the receiving end we receive a noisy version of the sequence of edgelabels and wish to infer which path was taken or to be precise we want to identify the most probable path in order to solve the codeword decoding problem and we want to the probability that the transmitted symbol at time n was a zero or a one to solve the bitwise decoding problem. example consider the case of a single transmission from the hamming trellis shown in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in trellises figure posterior probabilities over the sixteen codewords when the received vector y has normalized likelihoods t likelihood posterior probability let the normalized likelihoods be that is the ratios of the likelihoods are p j p j p j p j etc. how should this received signal be decoded? if we threshold the likelihoods at to turn the signal into a binary received vector we have r which decodes using the decoder for the binary symmetric channel into this is not the optimal decoding procedure. optimal inferences are always obtained by using bayes theorem. we can the posterior probability over codewords by explicit enumeration of all sixteen codewords. this posterior distribution is shown in of course we aren t really interested in such brute-force solutions and the aim of this chapter is to understand algorithms for getting the same information out in less than computer time. examining the posterior probabilities we notice that the most probable codeword is actually the string t this is more than twice as probable as the answer found by thresholding using the posterior probabilities shown in we can also compute the posterior marginal distributions of each of the bits. the result is shown in notice that bits and are all quite inferred to be zero. the strengths of the posterior probabilities for bits and are not so great. in the above example the map codeword is in agreement with the bitwise decoding that is obtained by selecting the most probable state for each bit using the posterior marginal distributions. but this is not always the case as the following exercise shows. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solving the decoding problems on a trellis likelihood posterior marginals p j tn p j tn p y p y figure marginal posterior probabilities for the bits under the posterior distribution of n exercise find the most probable codeword in the case where the normalized likelihood is also or estimate the marginal posterior probability for each of the seven bits and give the bit-by-bit decoding. concentrate on the few codewords that have the largest probability. we now discuss how to use message passing on a code s trellis to solve the decoding problems. the minsum algorithm the map codeword decoding problem can be solved using the minsum algorithm that was introduced in section each codeword of the code corresponds to a path across the trellis. just as the cost of a journey is the sum of the costs of its constituent steps the log likelihood of a codeword is the sum of the bitwise log likelihoods. by convention we the sign of the log likelihood we would like to maximize and talk in terms of a cost which we would like to minimize. we associate with each edge a cost p j tn where tn is the transmitted bit associated with that edge and yn is the received symbol. the minsum algorithm presented in section can then identify the most probable codeword in a number of computer operations equal to the number of edges in the trellis. this algorithm is also known as the viterbi algorithm the sumproduct algorithm to solve the bitwise decoding problem we can make a small to the minsum algorithm so that the messages passed through the trellis the probability of the data up to the current point instead of the cost of the best route to this point we replace the costs on the edges p j tn by the likelihoods themselves p j tn. we replace the min and sum operations of the minsum algorithm by a sum and product respectively. let i run over nodesstates i be the label for the start state pi denote the set of states that are parents of state i and wij be the likelihood associated with the edge from node j to node i. we the forward-pass messages by copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in trellises these messages can be computed sequentially from left to right. exercise show that for a node i whose time-coordinate is n is proportional to the joint probability that the codeword s path passed through node i and that the n received symbols were yn. the message computed at the end node of the trellis is proportional to the marginal probability of the data. exercise what is the constant of proportionality? we a second set of backward-pass messages in a similar manner. let node i be the end node. these messages can be computed sequentially in a backward pass from right to left. exercise show that for a node i whose time-coordinate is n is proportional to the conditional probability given that the codeword s path passed through node i that the subsequent received symbols were yn finally to the probability that the nth bit was a or we do two summations of products of the forward and backward messages. let i run over nodes at time n and j run over nodes at time n and let tij be the value of tn associated with the trellis edge from node j to node i. for each value of t we compute rt n xij tij then the posterior probability that tn was t is p tj y z n where the normalizing constant z forward message that was computed earlier. exercise that the above sumproduct algorithm does com n should be identical to the rt n pute p tj y. other names for the sumproduct algorithm presented here are the forward backward algorithm the bcjr algorithm and belief propagation exercise a codeword of the simple parity code is transmitted and the received signal y has associated likelihoods shown in table use the minsum algorithm and the sumproduct algorithm in the trellis to solve the map codeword decoding problem and the bitwise decoding problem. your answers by enumeration of all codewords use logs to base and do the minsum computations by hand. when working the sumproduct algorithm by hand you may it helpful to use three colours of pen one for the one for the ws and one for the n p j tn tn tn table bitwise likelihoods for a codeword of copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. more on trellises more on trellises we now discuss various ways of making the trellis of a code. you may safely jump over this section. the span of a codeword is the set of bits contained between the bit in the codeword that is non-zero and the last bit that is non-zero inclusive. we can indicate the span of a codeword by a binary vector as shown in table codeword span table some codewords and their spans. a generator matrix is in trellis-oriented form if the spans of the rows of the generator matrix all start in columns and the spans all end in columns. how to make a trellis from a generator matrix first put the generator matrix into trellis-oriented form by row-manipulations similar to gaussian elimination. for example our hamming code can be generated by g but this matrix is not in trellis-oriented form for example rows and all have spans that end in the same column. by subtracting lower rows from upper rows we can obtain an equivalent generator matrix is one that generates the same set of codewords as follows g now each row of the generator matrix can be thought of as an subcode of the k code that is in this case a code with two codewords of length n for the row the code consists of the two codewords and the subcode by the second row consists of and it is easy to construct the minimal trellises of these subcodes they are shown in the left column of we build the trellis incrementally as shown in we start with the trellis corresponding to the subcode given by the row of the generator matrix. then we add in one subcode at a time. the vertices within the span of the new subcode are all duplicated. the edge symbols in the original trellis are left unchanged and the edge symbols in the second part of the trellis are wherever the new subcode has a and otherwise left alone. another hamming code can be generated by g copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in trellises figure trellises for four subcodes of the hamming code column and the sequence of trellises that are made when constructing the trellis for the hamming code column. each edge in a trellis is labelled by a zero by a square or a one by a cross. the hamming code generated by this matrix by a permutation of its bits from the code generated by the systematic matrix used in chapter and above. the parity-check matrix corresponding to this permutation is h the trellis obtained from the permuted matrix g given in equation is shown in notice that the number of nodes in this trellis is smaller than the number of nodes in the previous trellis for the hamming code in we thus observe that rearranging the order of the codeword bits can sometimes lead to smaller simpler trellises. trellises from parity-check matrices another way of viewing the trellis is in terms of the syndrome. the syndrome of a vector r is to be hr where h is the parity-check matrix. a vector is only a codeword if its syndrome is zero. as we generate a codeword we can describe the current state by the partial syndrome that is the product of h with the codeword bits thus far generated. each state in the trellis is a partial syndrome at one time coordinate. the starting and ending states are both constrained to be the zero syndrome. each node in a state represents a possible value for the partial syndrome. since h is an m n matrix where m n k the syndrome is at most an m vector. so we need at most nodes in each state. we can construct the trellis of a code from its parity-check matrix by walking from each end generating two trees of possible syndrome sequences. the intersection of these two trees the trellis of the code. in the pictures we obtain from this construction we can let the vertical coordinate represent the syndrome. then any horizontal edge is necessarily associated with a zero bit only a non-zero bit changes the syndrome figure trellises for the permuted hamming code generated from the generator matrix by the method of the parity-check matrix by the method on page each edge in a trellis is labelled by a zero by a square or a one by a cross. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions and any non-horizontal edge is associated with a one bit. in this representation we no longer need to label the edges in the trellis. figure shows the trellis corresponding to the parity-check matrix of equation solutions t likelihood posterior probability table the posterior probability over codewords for exercise solution to exercise the posterior probability over codewords is shown in table the most probable codeword is the marginal posterior probabilities of all seven bits are n likelihood posterior marginals p j tn p j tn p y p y so the bitwise decoding is which is not actually a codeword. solution to exercise the map codeword is and its likelihood is the normalizing constant of the sumproduct algorithm is z the intermediate are left to right the intermediate are right to left the bitwise decoding is p y p y p y the codewords probabilities are for copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in graphs we now take a more general view of the tasks of inference and marginalization. before reading this chapter you should read about message passing in chapter the general problem assume that a function p of a set of n variables x fxngn a product of m factors as follows is as p m fmxm each of the factors fmxm is a function of a subset xm of the variables that make up x. if p is a positive function then we may be interested in a second normalized function p z p z m fmxm where the normalizing constant z is by z m fmxm as an example of the notation we ve just introduced here s a function of three binary variables by the factors or or or or p p z the subsets of denoted by xm in the general function are here and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the general problem g g g figure the factor graph associated with the function p the function p by the way may be recognized as the posterior probability distribution of the three transmitted bits in a repetition code when the received signal is r and the channel is a binary symmetric channel with probability the factors and respectively enforce the constraints that and must be identical and that and must be identical. the factors are the likelihood functions contributed by each component of r. a function of the factored form can be depicted by a factor graph in which the variables are depicted by circular nodes and the factors are depicted by square nodes. an edge is put between variable node n and factor node m if the function fmxm has any dependence on variable xn. the factor graph for the example function is shown in the normalization problem the task to be solved is to compute the normalizing constant z. the marginalization problems the second task to be solved is to compute the marginal function of any variable xn by znxn p for example if f is a function of three variables then the marginal for n is by f this type of summation over all the except for xn is so important that it can be useful to have a special notation for it the not-sum or summary the third task to be solved is to compute the normalized marginal of any variable xn by pnxn p include the n in pnxn departing from our normal practice in the rest of the book where we would omit it. exercise show that the normalized marginal is related to the marginal znxn by pnxn znxn z we might also be interested in marginals over a subset of the variables such as p all these tasks are intractable in general. even if every factor is a function of only three variables the cost of computing exact solutions for z and for the marginals is believed in general to grow exponentially with the number of variables n for certain functions p however the marginals can be computed by exploiting the factorization of p the idea of how this copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in graphs arises is well illustrated by the message-passing examples of chapter the sumproduct algorithm that we now review is a generalization of messagepassing rule-set b as was the case there the sumproduct algorithm is only valid if the graph is tree-like. the sumproduct algorithm notation we identify the set of variables that the mth factor depends on xm by the set of their indices n for our example function the sets are n is a function of alone n n n and n similarly we the set of factors in which variable n participates by mn. we denote a set n with variable n excluded by n we introduce the shorthand xmnn or xmnn to denote the set of variables in xm with xn excluded i.e. xmnn n the sumproduct algorithm will involve messages of two types passing along the edges in the factor graph messages qn!m from variable nodes to factor nodes and messages rm!n from factor nodes to variable nodes. a message either type q or r that is sent along an edge connecting factor fm to variable xn is always a function of the variable xn. here are the two rules for the updating of the two sets of messages. from variable to factor qn!mxn from factor to variable rm!nxn xxmnn a xn rm!nxn fmxn fm how these rules apply to leaves in the factor graph a node that has only one edge connecting it to another node is called a leaf node. some factor nodes in the graph may be connected to only one variable node in which case the set n of variables appearing in the factor message update is an empty set and the product of functions is the empty product whose value is such a fac tor node therefore always broadcasts to its one neighbour xn the message rm!nxn fmxn. similarly there may be variable nodes that are connected to only one factor node so the set mnnm in is empty. these nodes perpetually broadcast the message qn!mxn starting and method the algorithm can be initialized in two ways. if the graph is tree-like then it must have nodes that are leaves. these leaf nodes can broadcast their figure a factor node that is a leaf node perpetually sends the message rm!nxn fmxn to its one neighbour xn. xn qn!mxn fm figure a variable node that is a leaf node perpetually sends the message qn!mxn copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the sumproduct algorithm messages to their respective neighbours from the start. for all leaf variable nodes n for all leaf factor nodes m qn!mxn rm!nxn fmxn we can then adopt the procedure used in chapter s message-passing ruleset b a message is created in accordance with the rules only if all the messages on which it depends are present. for example in the message from to will be sent only when the message from to has been received and the message from to can be sent only when the messages and have both been received. messages will thus through the tree one in each direction along every edge and after a number of steps equal to the diameter of the graph every message will have been created. the answers we require can then be read out. the marginal function of xn is obtained by multiplying all the incoming messages at that node. znxn rm!nxn g g g figure our model factor graph for the function p the normalizing constant z can be obtained by summing any marginal function z znxn and the normalized marginals obtained from znxn z pnxn exercise apply the sumproduct algorithm to the function in equation and check that the normalized marginals are consistent with what you know about the repetition code exercise prove that the sumproduct algorithm correctly computes the marginal functions znxn if the graph is tree-like. exercise describe how to use the messages computed by the sum product algorithm to obtain more complicated marginal functions in a tree-like graph for example for two variables and that are connected to one common factor node. starting and method alternatively the algorithm can be initialized by setting all the initial messages from variables to for all n m qn!mxn then proceeding with the factor message update rule alternating with the variable message update rule compared with method this lazy initialization method leads to a load of wasted computations whose results are gradually out by the correct answers computed by method after a number of iterations equal to the diameter of the factor graph the algorithm will converge to a set of messages satisfying the sumproduct relationships exercise apply this second version of the sumproduct algorithm to the function in equation and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in graphs the reason for introducing this lazy method is that method it can be applied to graphs that are not tree-like. when the sumproduct algorithm is run on a graph with cycles the algorithm does not necessarily converge and certainly does not in general compute the correct marginal functions but it is nevertheless an algorithm of great practical importance especially in the decoding of sparse-graph codes. sumproduct algorithm with normalization if we are interested in only the normalized marginals then another version of the sumproduct algorithm may be useful. the factor-to-variable messages rm!n are computed in just the same way but the variable-to-factor messages are normalized thus qn!mxn where is a scalar chosen such that qn!mxn xxn exercise apply this normalized version of the sumproduct algorithm to the function in equation and a factorization view of the sumproduct algorithm one way to view the sumproduct algorithm is that it reexpresses the original fmxm as another factored function the product of m factors p factored function which is the product of m n factors p m n nxn each factor is associated with a factor node m and each factor nxn is associated with a variable node. initially fmxm and nxn each time a factor-to-variable message rm!nxn is sent the factorization is updated thus nxn rm!nxn and each message can be computed in terms of and using f rm!nxn a rm!nxn xxmnn which from the assignment in that the product is over all n exercise that the update rules are equivalent to the sumproduct rules so nxn eventually becomes the marginal znxn. this factorization viewpoint applies whether or not the graph is tree-like. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the minsum algorithm computational tricks normalization is a good idea from a computational point of view because if p is a product of many factors its values are likely to be very large or very small. another useful computational trick involves passing the logarithms of the messages q and r instead of q and r themselves the computations of the products in the algorithm are then replaced by simpler additions. the summations in of course become more to carry them out and return the logarithm we need to compute softmax functions like l but this computation can be done using look-up tables along with the observation that the value of the answer l is typically just a little larger than maxi li. if we store in look-up tables values of the function negative then l can be computed exactly in a number of look-ups and additions scaling as the number of terms in the sum. if look-ups and sorting operations are cheaper than exp then this approach costs less than the direct evaluation the number of operations can be further reduced by omitting negligible contributions from the smallest of the flig. a third computational trick applicable to certain error-correcting codes is to pass not the messages but the fourier transforms of the messages. this again makes the computations of the factor-to-variable messages quicker. a simple example of this fourier transform trick is given in chapter at equation the minsum algorithm the sumproduct algorithm solves the problem of the marginal function of a given product p this is analogous to solving the bitwise decoding problem of section and just as there were other decoding problems example the codeword decoding problem we can other tasks involving p that can be solved by of the sumproduct algorithm. for example consider this task analogous to the codeword decoding problem the maximization problem. find the setting of x that maximizes the product p this problem can be solved by replacing the two operations add and multiply everywhere they appear in the sumproduct algorithm by another pair of operations that satisfy the distributive law namely max and multiply. if we replace summation p by maximization we notice that the quantity formerly known as the normalizing constant p z becomes maxx p thus the sumproduct algorithm can be turned into a maxproduct algorithm that computes maxx p and from which the solution of the maximization problem can be deduced. each marginal znxn then lists the maximum value that p can attain for each value of xn. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact marginalization in graphs in practice the maxproduct algorithm is most often carried out in the negative log likelihood domain where max and product become min and sum. the minsum algorithm is also known as the viterbi algorithm. the junction tree algorithm what should one do when the factor graph one is interested in is not a tree? there are several options and they divide into exact methods and approximate methods. the most widely used exact method for handling marginalization on graphs with cycles is called the junction tree algorithm. this algorithm works by agglomerating variables together until the agglomerated graph has no cycles. you can probably out the details for yourself the complexity of the marginalization grows exponentially with the number of agglomerated variables. read more about the junction tree algorithm in jordan there are many approximate methods and we ll visit some of them over the next few chapters monte carlo methods and variational methods to name a couple. however the most amusing way of handling factor graphs to which the sumproduct algorithm may not be applied is as we already mentioned to apply the sumproduct algorithm! we simply compute the messages for each node in the graph as if the graph were a tree iterate and cross our this so-called loopy message passing has great importance in the decoding of error-correcting codes and we ll come back to it in section and part vi. further reading for further reading about factor graphs and the sumproduct algorithm see kschischang et al. yedidia et al. yedidia et al. yedidia et al. wainwright et al. and forney see also pearl a good reference for the fundamental theory of graphical models is lauritzen a readable introduction to bayesian networks is given by jensen interesting message-passing algorithms that have capabilities from the sumproduct algorithm include expectation propagation and survey propagation et al. see also section exercises exercise express the joint probability distribution from the burglar alarm and earthquake problem as a factor graph and the marginal probabilities of all the variables as each piece of information comes to fred s attention using the sumproduct algorithm with normalization. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. laplace s method the idea behind the laplace approximation is simple. we assume that an unnormalized probability density p whose normalizing constant zp p dx is of interest has a peak at a point we taylor-expand the logarithm of p around this peak ln p ln p c where c ln p we then approximate p by an unnormalized gaussian p c p ln p ln p ln p and we approximate the normalizing constant zp by the normalizing constant of this gaussian zq p c we can generalize this integral to approximate zp for a density p over a k-dimensional space x. if the matrix of second derivatives of ln p at the maximum is a by aij ln p so that the expansion is generalized to ln p ln p then the normalizing constant can be approximated by zp zq p p det a predictions can be made using the approximation q. physicists also call this widely-used approximation the saddle-point approximation. qdet a copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. laplace s method the fact that the normalizing constant of a gaussian is given by z dkx det a can be proved by making an orthogonal transformation into the basis u in which a is transformed into a diagonal matrix. the integral then separates into a product of one-dimensional integrals each of the form z dui the product of the eigenvalues is the determinant of a. the laplace approximation is basis-dependent if x is transformed to a nonlinear function ux and the density is transformed to p p then in general the approximate normalizing constants zq will be this can be viewed as a defect since the true value zp is basis-independent or an opportunity because we can hunt for a choice of basis in which the laplace approximation is most accurate. exercises exercise also exercise a photon counter is pointed at a remote star for one minute in order to infer the rate of photons arriving at the counter per minute assuming the number of photons collected r has a poisson distribution with mean p j r! and assuming the improper prior p make laplace approximations to the posterior distribution over over log constant. the improper prior transforms to p exercise use laplace s method to approximate the integral da f f where f and are positive. check the accuracy of the approximation against the exact answer for and measure the error zp log zq in bits. exercise linear regression. n datapoints fxn tng are generated by the experimenter choosing each xn then the world delivering a noisy version of the linear function yx tn normalyxn assuming gaussian priors on and make the laplace approximation to the posterior distribution of and is exact in fact and obtain the predictive distribution for the next datapoint given mackay for further reading. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. model comparison and occam s razor occam s razor how many boxes are in the picture in particular how many boxes are in the vicinity of the tree? if we looked with x-ray spectacles would we see one or two boxes behind the trunk even more? occam s razor is the principle that states a preference for simple theories. accept the simplest explanation that the data thus according to occam s razor we should deduce that there is only one box behind the tree. is this an ad hoc rule of thumb? or is there a convincing reason for believing there is most likely one box? perhaps your intuition likes the argument well it would be a remarkable coincidence for the two boxes to be just the same height and colour as each other if we wish to make intelligences that interpret data correctly we must translate this intuitive feeling into a concrete theory. motivations for occam s razor if several explanations are compatible with a set of observations occam s razor advises us to buy the simplest. this principle is often advocated for one of two reasons the is aesthetic a theory with mathematical beauty is more likely to be correct than an ugly one that some experimental data figure a picture to be interpreted. it contains a tree and some boxes. or figure how many boxes are behind the tree? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. model comparison and occam s razor figure why bayesian inference embodies occam s razor. this gives the basic intuition for why complex models can turn out to be less probable. the horizontal axis represents the space of possible data sets d. bayes theorem rewards models in proportion to how much they predicted the data that occurred. these predictions are by a normalized probability distribution on d. this probability of the data given model hi p jhi is called the evidence for hi. a simple model makes only a limited range of predictions shown by p a more powerful model that has for example more free parameters than is able to predict a greater variety of data sets. this means however that does not predict the data sets in region as strongly as suppose that equal prior probabilities have been assigned to the two models. then if the data set falls in region the less powerful model will be the more probable model. evidence pdh pdh c d dirac the second reason is the past empirical success of occam s razor. however there is a for occam s razor namely coherent inference embodied by bayesian probability automatically embodies occam s razor quantitatively. it is indeed more probable that there s one box behind the tree and we can compute how much more probable one is than two. model comparison and occam s razor we evaluate the plausibility of two alternative theories and in the light of data d as follows using bayes theorem we relate the plausibility of model given the data p j d to the predictions made by the model about the data p and the prior plausibility of p this gives the following probability ratio between theory and theory p j d p j d p p p p the ratio on the right-hand side measures how much our initial beliefs favoured over the second ratio expresses how well the observed data were predicted by compared to how does this relate to occam s razor when is a simpler model than the ratio gives us the opportunity if we wish to insert a prior bias in favour of on aesthetic grounds or on the basis of experience. this would correspond to the aesthetic and empirical motivations for occam s razor mentioned earlier. but such a prior bias is not necessary the second ratio the data-dependent factor embodies occam s razor automatically. simple models tend to make precise predictions. complex models by their nature are capable of making a greater variety of predictions so if is a more complex model it must spread its predictive probability p more thinly over the data space than thus in the case where the data are compatible with both theories the simpler will turn out more probable than without our having to express any subjective dislike for complex models. our subjective prior just needs to assign equal prior probabilities to the possibilities of simplicity and complexity. probability theory then allows the observed data to express their opinion. let us turn to a simple example. here is a sequence of numbers the task is to predict the next two numbers and infer the underlying process that gave rise to this sequence. a popular answer to this question is the prediction with the explanation add to the previous number what about the alternative answer with the underlying rule being get the next number from the previous number x by evaluating copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. occam s razor i assume that this prediction seems rather less plausible. but the second rule the data just as well as the rule add so why should we it less plausible? let us give labels to the two general theories ha the sequence is an arithmetic progression add n where n is an integer. hc the sequence is generated by a cubic function of the form x e where c d and e are fractions. one reason for the second explanation hc less plausible might be that arithmetic progressions are more frequently encountered than cubic functions. this would put a bias in the prior probability ratio p in equation but let us give the two theories equal prior probabilities and concentrate on what the data have to say. how well did each theory predict the data? to obtain p jha we must specify the probability distribution that each model assigns to its parameters. first ha depends on the added integer n and the number in the sequence. let us say that these numbers could each have been anywhere between and then since only the pair of values fn number give rise to the observed data d the probability of the data given ha is p jha to evaluate p jhc we must similarly say what values the fractions c d and e might take on. choose to represent these numbers as fractions rather than real numbers because if we used real numbers the model would assign relative to ha an probability to d. real parameters are the norm however and are assumed in the rest of this chapter. a reasonable prior might state that for each fraction the numerator could be any number between and and the denominator is any number between and as for the initial value in the sequence let us leave its probability distribution the same as in ha. there are four ways of expressing the fraction c under this prior and similarly there are four and two possible solutions for d and e respectively. so the probability of the observed data given hc is found to be p jhc thus comparing p jhc with p jha even if our prior probabilities for ha and hc are equal the odds p jha p jhc in favour of ha over hc given the sequence d are about forty million to one. this answer depends on several subjective assumptions in particular the probability assigned to the free parameters n c d e of the theories. bayesians make no apologies for this there is no such thing as inference or prediction without assumptions. however the quantitative details of the prior probabilities have no on the qualitative occam s razor the complex theory hc always an occam factor because it has more parameters and so can predict a greater variety of data sets this was only a small example and there were only four data points as we move to larger copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. model comparison and occam s razor figure where bayesian inference into the data modelling process. this illustrates an abstraction of the part of the process in which data are collected and modelled. in particular this applies to pattern learning interpolation etc. the two double-framed boxes denote the two steps which involve inference. it is only in those two steps that bayes theorem can be used. bayes does not tell you how to invent models for example. the box each model to the data is the task of inferring what the model parameters might be given the model and the data. bayesian methods may be used to the most probable parameter values and error bars on those parameters. the result of applying bayesian methods to this problem is often little from the answers given by orthodox statistics. the second inference task model comparison in the light of the data is where bayesian methods are in a class of their own. this second inference problem requires a quantitative occam s razor to penalize over-complex models. bayesian methods can assign objective preferences to the alternative models in a way that automatically embodies occam s razor. gather data create alternative models fit each model to the data gather more data choose what data to gather next assign preferences to the alternative models choose future actions create new models decide whether to create new models and more sophisticated problems the magnitude of the occam factors typically increases and the degree to which our inferences are by the quantitative details of our subjective assumptions becomes smaller. bayesian methods and data analysis let us now relate the discussion above to real problems in data analysis. there are countless problems in science statistics and technology which require that given a limited data set preferences be assigned to alternative models of complexities. for example two alternative hypotheses accounting for planetary motion are mr. inquisition s geocentric model based on epicycles and mr. copernicus s simpler model of the solar system with the sun at the centre. the epicyclic model data on planetary motion at least as well as the copernican model but does so using more parameters. coincidentally for mr. inquisition two of the extra epicyclic parameters for every planet are found to be identical to the period and radius of the sun s cycle around the earth intuitively we mr. copernicus s theory more probable. the mechanism of the bayesian razor the evidence and the occam factor two levels of inference can often be distinguished in the process of data modelling. at the level of inference we assume that a particular model is true and we that model to the data i.e. we infer what values its free parameters should plausibly take given the data. the results of this inference are often summarized by the most probable parameter values and error bars on those parameters. this analysis is repeated for each model. the second level of inference is the task of model comparison. here we wish to compare the models in the light of the data and assign some sort of preference or ranking to the alternatives. note that both levels of inference are distinct from decision theory. the goal of inference is given a hypothesis space and a particular data set to assign probabilities to hypotheses. decision theory typically chooses between alternative actions on the basis of these probabilities so as to minimize the copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. occam s razor expectation of a loss function this chapter concerns inference alone and no loss functions are involved. when we discuss model comparison this should not be construed as implying model choice. ideal bayesian predictions do not involve choice between models rather predictions are made by summing over all the alternative models weighted by their probabilities. bayesian methods are able consistently and quantitatively to solve both the inference tasks. there is a popular myth that states that bayesian methods from orthodox statistical methods only by the inclusion of subjective priors which are to assign and which usually don t make much difference to the conclusions. it is true that at the level of inference a bayesian s results will often little from the outcome of an orthodox attack. what is not widely appreciated is how a bayesian performs the second level of inference this chapter will therefore focus on bayesian model comparison. model comparison is a task because it is not possible simply to choose the model that the data best more complex models can always the data better so the maximum likelihood model choice would lead us inevitably to implausible over-parameterized models which generalize poorly. occam s razor is needed. let us write down bayes theorem for the two levels of inference described above so as to see explicitly how bayesian model comparison works. each model hi is assumed to have a vector of parameters w. a model is by a collection of probability distributions a prior distribution p jhi which states what values the model s parameters might be expected to take and a set of conditional distributions one for each value of w the predictions p j whi that the model makes about the data d. model at the level of inference we assume that one model the ith say is true and we infer what the model s parameters w might be given the data d. using bayes theorem the posterior probability of the parameters w is p j dhi p j whip jhi p jhi that is posterior likelihood prior evidence the normalizing constant p jhi is commonly ignored since it is irrelevant to the level of inference i.e. the inference of w but it becomes important in the second level of inference and we name it the evidence for hi. it is common practice to use gradient-based methods to the maximum of the posterior which the most probable value for the parameters wmp it is then usual to summarize the posterior distribution by the value of wmp and error bars or intervals on these parameters. error bars can be obtained from the curvature of the posterior evaluating the hessian at wmp a ln p j dhijwmp and taylor-expanding the log posterior probability with p j dhi p j dhi we see that the posterior can be locally approximated as a gaussian with covariance matrix to error bars this approximation is good or not will depend on the problem we are solving. indeed the maximum and mean of the posterior distribution have copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. model comparison and occam s razor figure the occam factor. this shows the quantities that determine the occam factor for a hypothesis hi having a single parameter w. the prior distribution line for the parameter has width the posterior distribution line has a single peak at wmp with characteristic width the occam factor is jhi p j dhi p jhi wmp w no fundamental status in bayesian inference they both change under nonlinear reparameterizations. maximization of a posterior probability is useful only if an approximation like equation gives a good summary of the distribution. model comparison. at the second level of inference we wish to infer which model is most plausible given the data. the posterior probability of each model is p j d p jhip notice that the data-dependent term p jhi is the evidence for hi which appeared as the normalizing constant in the second term p is the subjective prior over our hypothesis space which expresses how plausible we thought the alternative models were before the data arrived. assuming that we choose to assign equal priors p to the alternative models models hi are ranked by evaluating the evidence. the normalizing constant p p jhip has been omitted from equation because in the data-modelling process we may develop new models after the data have arrived when an inadequacy of the models is detected for example. inference is open ended we continually seek more probable models to account for the data we gather. to repeat the key idea to rank alternative models hi a bayesian evaluates the evidence p jhi. this concept is very general the evidence can be evaluated for parametric and non-parametric models alike whatever our data-modelling task a regression problem a problem or a density estimation problem the evidence is a transportable quantity for comparing alternative models. in all these cases the evidence naturally embodies occam s razor. evaluating the evidence let us now study the evidence more closely to gain insight into how the bayesian occam s razor works. the evidence is the normalizing constant for equation p jhi p j whip jhi dw for many problems the posterior p j dhi p j whip jhi has a strong peak at the most probable parameters wmp then taking for simplicity the one-dimensional case the evidence can be approximated using laplace s method by the height of the peak of the integrand p j whip jhi times its width copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. occam s razor p jhi evidence best likelihood occam factor p jhi p j wmphi thus the evidence is found by taking the likelihood that the model can achieve and multiplying it by an occam factor which is a term with magnitude less than one that penalizes hi for having the parameter w. interpretation of the occam factor the quantity is the posterior uncertainty in w. suppose for simplicity that the prior p jhi is uniform on some large interval representing the range of values of w that were possible a priori according to hi then p jhi and occam factor i.e. the occam factor is equal to the ratio of the posterior accessible volume of hi s parameter space to the prior accessible volume or the factor by which hi s hypothesis space collapses when the data arrive. the model hi can be viewed as consisting of a certain number of exclusive submodels of which only one survives when the data arrive. the occam factor is the inverse of that number. the logarithm of the occam factor is a measure of the amount of information we gain about the model s parameters when the data arrive. a complex model having many parameters each of which is free to vary over a large range will typically be penalized by a stronger occam factor than a simpler model. the occam factor also penalizes models that have to be tuned to the data favouring models for which the required precision of the parameters is coarse. the magnitude of the occam factor is thus a measure of complexity of the model it relates to the complexity of the predictions that the model makes in data space. this depends not only on the number of parameters in the model but also on the prior probability that the model assigns to them. which model achieves the greatest evidence is determined by a between minimizing this natural complexity measure and minimizing the data in contrast to alternative measures of model complexity the occam factor for a model is straightforward to evaluate it simply depends on the error bars on the parameters which we already evaluated when the model to the data. figure displays an entire hypothesis space so as to illustrate the various probabilities in the analysis. there are three models which have equal prior probabilities. each model has one parameter w shown on a horizontal axis but assigns a prior range to that parameter. is the most or complex model assigning the broadest prior range. a one-dimensional data space is shown by the vertical axis. each model assigns a joint probability distribution p w jhi to the data and the parameters illustrated by a cloud of dots. these dots represent random samples from the full probability distribution. the total number of dots in each of the three model subspaces is the same because we assigned equal prior probabilities to the models. when a particular data set d is received line we infer the posterior distribution of w for a model say by reading out the density along that horizontal line and normalizing. the posterior probability p j is shown by the dotted curve at the bottom. also shown is the prior distribution p the case of model which is very poorly copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. model comparison and occam s razor figure a hypothesis space consisting of three exclusive models each having one parameter w and a one-dimensional data set d. the data set is a single measured value which from the parameter w by a small amount of additive noise. typical samples from the joint distribution p wh are shown by dots. these are not data points. the observed data set is a single particular value for d shown by the dashed horizontal line. the dashed curves below show the posterior probability of w for each model given this data set the evidence for the models is obtained by marginalizing onto the d axis at the left-hand side d d p p p p j p j p p p j p w w w matched to the data the shape of the posterior distribution will depend on the details of the tails of the prior p and the likelihood p j the curve shown is for the case where the prior falls more strongly. we obtain by marginalizing the joint distributions p w jhi onto the d axis at the left-hand side. for the data set d shown by the dotted horizontal line the evidence p for the more model has a smaller value than the evidence for this is because placed less predictive probability dots on that line. in terms of the distributions over w model has smaller evidence because the occam factor is smaller for than for the simplest model has the smallest evidence of all because the best that it can achieve to the data d is very poor. given this data set the most probable model is occam factor for several parameters if the posterior is well approximated by a gaussian then the occam factor is obtained from the determinant of the corresponding covariance matrix equation and chapter p jhi p j wmp hi evidence best likelihood p jhi occam factor where a ln p j dhi the hessian which we evaluated when we calculated the error bars on wmp and chapter as the amount of data collected increases this gaussian approximation is expected to become increasingly accurate. in summary bayesian model comparison is a simple extension of maximum likelihood model selection the evidence is obtained by multiplying the likelihood by the occam factor. to evaluate the occam factor we need only the hessian a if the gaussian approximation is good. thus the bayesian method of model comparison by evaluating the evidence is no more computationally demanding than the task of for each model the parameters and their error bars. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. or figure how many boxes are behind the tree? example example let s return to the example that opened this chapter. are there one or two boxes behind the tree in why do coincidences make us suspicious? let s assume the image of the area round the trunk and box has a size of pixels that the trunk is pixels wide and that colours of boxes can be distinguished. the theory that says there is one box near the trunk has four free parameters three coordinates the top three edges of the box and one parameter giving the box s colour. boxes could levitate there would be free parameters. the theory that says there are two boxes near the trunk has eight free parameters four plus a ninth a binary variable that indicates which of the two boxes is the closest to the viewer. what is the evidence for each model? we ll do we need a prior on the parameters to evaluate the evidence. for convenience let s work in pixels. let s assign a separable prior to the horizontal location of the box its width its height and its colour. the height could have any of say distinguishable values so could the width and so could the location. the colour could have any of values. we ll put uniform priors over these variables. we ll ignore all the parameters associated with other objects in the image since they don t come into the model comparison between and the evidence is p since only one setting of the parameters the data and it predicts the data perfectly. as for model six of its nine parameters are well-determined and three of them are partly-constrained by the data. if the left-hand box is furthest away for example then its width is at least pixels and at most if it s the closer of the two boxes then its width is between and pixels. m assuming here that the visible portion of the left-hand box is about pixels wide. to get the evidence we need to sum up the prior probabilities of all viable hypotheses. to do an exact calculation we need to be more about the data and the priors but let s just get the ballpark answer assuming that the two unconstrained real variables have half their values available and that the binary variable is completely undetermined. an exercise you can make an explicit model and work out the exact answer. p thus the posterior probability ratio is equal prior probability p p so the data are roughly to in favour of the simpler hypothesis. the four factors in can be interpreted in terms of occam factors. the more complex model has four extra parameters for sizes and colours three for sizes and one for colour. it has to pay two big occam factors and for the highly suspicious coincidences that the two box heights match exactly and the two colours match exactly and it also pays two lesser occam factors for the two lesser coincidences that both boxes happened to have one of their edges conveniently hidden behind a tree or behind each other. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. model comparison and occam s razor figure a popular view of model comparison by minimum description length. each model hi communicates the data d by sending the identity of the model sending the parameters of the model then sending the data relative to those parameters. as we proceed to more complex models the length of the parameter message increases. on the other hand the length of the data message decreases because a complex model is able to the data better making the residuals smaller. in this example the intermediate model achieves the optimum between these two trends. ld j ld j ld j minimum description length a complementary view of bayesian model comparison is obtained by replacing probabilities of events by the lengths in bits of messages that communicate the events without loss to a receiver. message lengths lx correspond to a probabilistic model over events x via the relations p lx p the mdl principle and boulton states that one should prefer models that can communicate the data in the smallest number of bits. consider a two-part message that states which model h is to be used and then communicates the data d within that model to some pre-arranged precision this produces a message of length ldh lh ld jh. the lengths lh for h an implicit prior p over the alternative models. similarly ld jh corresponds to a density p jh. thus a procedure for assigning message lengths can be mapped onto posterior probabilities ldh log p log log p j d const in principle then mdl can always be interpreted as bayesian model comparison and vice versa. however this simple discussion has not addressed how one would actually evaluate the key data-dependent term ld jh which corresponds to the evidence for h. often this message is imagined as being subdivided into a parameter block and a data block models with a small number of parameters have only a short parameter block but do not the data well and so the data message list of large residuals is long. as the number of parameters increases the parameter block lengthens and the data message becomes shorter. there is an optimum model complexity in the for which the sum is minimized. this picture glosses over some subtle issues. we have not the precision to which the parameters w should be sent. this precision has an important the precision to which real-valued data d are sent which assuming is small relative to the noise level just introduces an additive constant. as we decrease the precision to which w is sent the parameter message shortens but the data message typically lengthens because the truncated parameters do not match the data so well. there is a non-trivial optimal precision. in simple gaussian cases it is possible to solve for this optimal precision and freeman and it is closely related to the posterior error bars on the parameters where a ln p j dh. it turns out that the optimal parameter message length is virtually identical to the log of the occam factor in equation random element involved in parameter truncation means that the encoding is slightly sub-optimal. with care therefore one can replicate bayesian results in mdl terms. although some of the earliest work on complex model comparison involved the mdl framework and wallace mdl has no apparent advantages over the direct probabilistic approach. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. minimum description length mdl does have its uses as a pedagogical tool. the description length concept is useful for motivating prior probability distributions. also ways of breaking down the task of communicating data using a model can give helpful insights into the modelling process as will now be illustrated. on-line learning and cross-validation. in cases where the data consist of a sequence of points d tn the log evidence can be decomposed as a sum of on-line predictive performances log p jh log p jh log p j log p j log p j this decomposition can be used to explain the between the evidence and leave-one-out cross-validation as measures of predictive ability. cross-validation examines the average value of just the last term log p j under random re-orderings of the data. the evidence on the other hand sums up how well the model predicted all the data starting from scratch. the bits-back encoding method. another mdl thought experiment and van camp involves incorporating random bits into our message. the data are communicated using a parameter block and a data block. the parameter vector sent is a random sample from the posterior p j dh p j whp jhp jh. this sample w is sent to an arbitrary small granularity using a message length lw jh logp the data are encoded relative to w with a message of length ld j wh logp j once the data message has been received the random bits used to generate the sample w from the posterior can be deduced by the receiver. the number of bits so recovered is j these recovered bits need not count towards the message length since we might use some other optimally-encoded message as a random bit string thereby communicating that message at the same time. the net description cost is therefore lw jh ld j wh bits back log log p jh log thus this thought experiment has yielded the optimal description length. bitsback encoding has been turned into a practical compression method for data modelled with latent variable models by frey p jh p j wh p j dh further reading bayesian methods are introduced and contrasted with sampling-theory statistics in gull loredo the bayesian occam s razor is demonstrated on model problems in mackay useful textbooks are and tiao berger one debate worth understanding is the question of whether it s permissible to use improper priors in bayesian inference et al. if we want to do model comparison discussed in this chapter it is essential to use proper priors otherwise the evidences and the occam factors are copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. model comparison and occam s razor meaningless. only when one has no intention to do model comparison may it be safe to use improper priors and even in such cases there are pitfalls as dawid et al. explain. i would agree with their advice to always use proper priors tempered by an encouragement to be smart when making calculations recognizing opportunities for approximation. exercises p x p m x y x exercise random variables x come independently from a probability distribution p according to model p is a uniform distribution p x according to model p is a nonuniform distribution with an unknown parameter m p x mx given the data d what is the evidence for and exercise datapoints t are believed to come from a straight line. the experimenter chooses x and t is gaussian-distributed about y with variance according to model the straight line is horizontal so according to model is a parameter with prior distribution both models assign a prior distribution to given the data set d and assuming the noise level is what is the evidence for each model? exercise a six-sided die is rolled times and the numbers of times each face came up were f what is the probability that the die is a perfectly fair die assuming the alternative hypothesis says that the die has a biased distribution p and the prior density for p is uniform over the simplex pi pi pi solve this problem two ways exactly using the helpful dirichlet formulae and approximately using laplace s method. notice that your choice of basis for the laplace approximation is important. see mackay for discussion of this exercise. exercise the of race on the imposition of the death penalty for murder in america has been much studied. the following three-way table cases in which the defendant was convicted of murder. the three variables are the defendant s race the victim s race and whether the defendant was sentenced to death. from m. radelet racial characteristics and imposition of the death penalty american sociological review pp. white defendant black defendant death penalty yes no death penalty yes no white victim black victim white victim black victim copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises it seems that the death penalty was applied much more often when the victim was white then when the victim was black. when the victim was white of defendants got the death penalty but when the victim was black of defendants got the death penalty. these data provide an example of a phenomenon known as simpson s paradox a higher fraction of white defendants are sentenced to death overall but in cases involving black victims a higher fraction of black defendants are sentenced to death and in cases involving white victims a higher fraction of black defendants are sentenced to death. quantify the evidence for the four alternative hypotheses shown in i should mention that i don t believe any of these models is adequate several additional variables are important in murder cases such as whether the victim and murderer knew each other whether the murder was premeditated and whether the defendant had a prior criminal record none of these variables is included in the table. so this is an academic exercise in model comparison rather than a serious study of racial bias in the state of florida. the hypotheses are shown as graphical models with arrows showing dependencies between the variables v race m race and d death penalty given. model has only one free parameter the probability of receiving the death penalty model has four such parameters one for each state of the variables v and m. assign uniform priors to these variables. how sensitive are the conclusions to the choice of prior? v v m m v v m m d d d d v v m m v v m m d d d d figure four hypotheses concerning the dependence of the imposition of the death penalty d on the race of the victim v and the race of the convicted murderer m. for example asserts that the probability of receiving the death penalty does depend on the murderer s race but not on the victim s. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter the last couple of chapters have assumed that a gaussian approximation to the probability distribution we are interested in is adequate. what if it is not? we have already seen an example clustering where the likelihood function is multimodal and has nasty unboundedly-high spikes in certain locations in the parameter space so maximizing the posterior probability and a gaussian is not always going to work. this with laplace s method is one motivation for being interested in monte carlo methods. in fact monte carlo methods provide a general-purpose set of tools with applications in bayesian data modelling and many other this chapter describes a sequence of methods importance sampling rejection sampling the metropolis method gibbs sampling and slice sampling. for each method we discuss whether the method is expected to be useful for high-dimensional problems such as arise in inference with graphical models. graphical model is a probabilistic model in which dependencies and independencies of variables are represented by edges in a graph whose nodes are the variables. along the way the terminology of markov chain monte carlo methods is presented. the subsequent chapter discusses advanced methods for reducing random walk behaviour. for details of monte carlo methods theorems and proofs and a full list of references the reader is directed to neal gilks et al. and tanner in this chapter i will use the word sample in the following sense a sample from a distribution p is a single realization x whose probability distribution is p this contrasts with the alternative usage in statistics where sample refers to a collection of realizations fxg. cation convention i like my matrices to act to the right preferring when we discuss transition probability matrices i will use a right-multipli u mv to ut vtmt a transition probability matrix tij or tijj the probability given the current state is j of making the transition from j to i. the columns of t are probability vectors. if we write down a transition probability density we use the same convention for the order of its arguments t x is a transition probability density from x to this unfortunately means that you have to get used to reading from right to left the sequence xyz has probability t yt copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods the problems to be solved monte carlo methods are computational techniques that make use of random numbers. the aims of monte carlo methods are to solve one or both of the following problems. problem to generate samples fxrgr from a given probability distribu tion p problem to estimate expectations of functions under this distribution for example dn x p the probability distribution p which we call the target density might be a distribution from statistical physics or a conditional distribution arising in data modelling for example the posterior probability of a model s parameters given some observed data. we will generally assume that x is an n vector with real components xn but we will sometimes consider discrete spaces also. simple examples of functions whose expectations we might be interested in include the and second moments of quantities that we wish to predict from which we can compute means and variances for example if some quantity t depends on x we can the mean and variance of t under p by the expectations of the functions tx and then using and and vart it is assumed that p is complex that we cannot evaluate these expectations by exact methods so we are interested in monte carlo methods. we will concentrate on the problem because if we have solved it then we can solve the second problem by using the random samples fxrgr to give the estimator rxr are generated from p then the expectation of is if the vectors fxrgr also as the number of samples r increases the variance of will decrease as where is the variance of dn x p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods figure the function p how to draw samples from this density? the function p evaluated at a discrete set of uniformly spaced points fxig. how to draw samples from this discrete distribution? px px this is one of the important properties of monte carlo methods. the accuracy of the monte carlo estimate depends only on the variance of not on the dimensionality of the space sampled. to be precise the variance of goes as so regardless of the dimensionality of x it may be that as few as a dozen independent samples fxrg to estimate satisfactorily. we will later however that high dimensionality can cause other for monte carlo methods. obtaining independent samples from a given distribution p is often not easy. why is sampling from p hard? we will assume that the density from which we wish to draw samples p can be evaluated at least to within a multiplicative constant that is we can evaluate a function p such that p p if we can evaluate p why can we not easily solve problem why is it in general to obtain samples from p there are two the is that we typically do not know the normalizing constant z dn x p the second is that even if we did know z the problem of drawing samples from p is still a challenging one especially in high-dimensional spaces because there is no obvious way to sample from p without enumerating most or all of the possible states. correct samples from p will by tend to come from places in x-space where p is big how can we identify those places where p is big without evaluating p everywhere? there are only a few high-dimensional densities from which it is easy to draw samples for example the gaussian distribution. let us start with a simple one-dimensional example. imagine that we wish to draw samples from the density p p where p x we can plot this function but that does not mean we can draw samples from it. to start with we don t know the normalizing constant z. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the problems to be solved to give ourselves a simpler problem we could discretize the variable x and ask for samples from the discrete probability distribution over a set of uniformly spaced points fxig how could we solve this problem? if we evaluate p at each point xi we can compute z and pi and we can then sample from the probability distribution fpig using various methods based on a source of random bits section but what is the cost of this procedure and how does it scale with the dimensionality of the space n let us concentrate on the initial cost of evaluating z to compute z we have to visit every point in the space. in there are uniformly spaced points in one dimension. if our system had n dimensions n say then the corresponding number of points would be an unimaginable number of evaluations of p even if each component xn took only two discrete values the number of evaluations of p would be a number that is still horribly huge. if every electron in the universe are about of them were a gigahertz computer that could evaluate p for a trillion states every second and if we ran those computers for a time equal to the age of the universe seconds they would still only visit states. we d have to wait for more than universe ages to elapse before all states had been visited. systems with states are two a penny.? one example is a collection of spins such as a fragment of an ising model whose probability distribution is proportional to p where xn and ex jmnxmxn hnxn the energy function ex is readily evaluated for any x. but if we wish to evaluate this function at all states x the computer time required would be function evaluations. the ising model is a simple model which has been around for a long time but the task of generating samples from the distribution p p is still an active research area the exact samples from this distribution were created in the pioneering work of propp and wilson as we ll describe in chapter a useful analogy imagine the tasks of drawing random water samples from a lake and the average plankton concentration the depth of the lake at x y is p and we assert order to make the analogy work that the plankton concentration is a function of x the required average concentration is an integral like namely z z dn x p translation for american readers such systems are a dime a dozen incidentally this equivalence shows that the correct exchange rate between our currencies is p figure a lake whose depth at x y is p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods where z r dx dy p is the volume of the lake. you are provided with a boat a satellite navigation system and a plumbline. using the navigator you can take your boat to any desired location x on the map using the plumbline you can measure p at that point. you can also measure the plankton concentration there. problem is to draw water samples at random from the lake in such a way that each sample is equally likely to come from any point within the lake. problem is to the average plankton concentration. these are problems to solve because at the outset we know nothing about the depth p perhaps much of the volume of the lake is contained in narrow deep underwater canyons in which case to correctly sample from the lake and correctly estimate our method must implicitly discover the canyons and their volume relative to the rest of the lake. problems yes nevertheless we ll see that clever monte carlo methods can solve them. uniform sampling having accepted that we cannot exhaustively visit every location x in the state space we might consider trying to solve the second problem the expectation of a function by drawing random samples fxrgr uniformly from the state space and evaluating p at those points. then we could introduce a normalizing constant zr by figure a slice through a lake that includes some canyons. p r zr and estimate dn x by r p zr is anything wrong with this strategy? well it depends on the functions and p let us assume that is a benign smoothly varying function and concentrate on the nature of p as we learnt in chapter a highdimensional distribution is often concentrated in a small region of the state space known as its typical set t whose volume is given by jtj where hx is the entropy of the probability distribution p if almost all the probability mass is located in the typical set and is a benign function the value of dn x will be principally determined by the values that takes on in the typical set. so uniform sampling will only stand a chance of giving a good estimate of if we make the number of samples r large that we are likely to hit the typical set at least once or twice. so how many samples are required? let us take the case of the ising model again. the ising model may not be a good example since it doesn t necessarily have a typical set as in chapter the of a typical set was that all states had log probability close to the entropy which for an ising model would mean that the energy is very close to the mean energy but in the vicinity of phase transitions the variance of energy also known as the heat capacity may diverge which means that the energy of a random state is not necessarily expected to be very close to the mean energy. the total size of the state space is states and the typical set has size so each sample has a chance of of falling in the typical set. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. importance sampling y p o r t n e figure entropy of a ising model as a function of temperature. one state of a ising model. temperature the number of samples required to hit the typical set once is thus of order rmin so what is h? at high temperatures the probability distribution of an ising model tends to a uniform distribution and the entropy tends to hmax n bits which means rmin is of order under these conditions uniform sampling may well be a satisfactory technique for estimating but high temperatures are not of great interest. considerably more interesting are intermediate temperatures such as the critical temperature at which the ising model melts from an ordered phase to a disordered phase. the critical temperature of an ising model at which it melts is at this temperature the entropy of an ising model is roughly bits for this probability distribution the number of samples required simply to hit the typical set once is of order rmin which for n is about this is roughly the square of the number of particles in the universe. thus uniform sampling is utterly useless for the study of ising models of modest size. and in most high-dimensional problems if the distribution p is not actually uniform uniform sampling is unlikely to be useful. overview having established that drawing samples from a high-dimensional distribution p p is even if p is easy to evaluate we will now study a sequence of more sophisticated monte carlo methods importance sampling rejection sampling the metropolis method gibbs sampling and slice sampling. importance sampling importance sampling is not a method for generating samples from p it is just a method for estimating the expectation of a function it can be viewed as a generalization of the uniform sampling method. for illustrative purposes let us imagine that the target distribution is a one-dimensional density p let us assume that we are able to evaluate this density at any chosen point x at least to within a multiplicative constant thus we can evaluate a function p such that p p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods but p is too complicated a function for us to be able to sample from it directly. we now assume that we have a simpler density qx from which we can generate samples and which we can evaluate to within a multiplicative constant is we can evaluate where qx an example of the functions p and is shown in we call q the sampler density. in importance sampling we generate r samples fxrgr from qx. if these points were samples from p then we could estimate by equation but when we generate samples from q values of x where qx is greater than p will be over-represented in this estimator and points where qx is less than p will be under-represented. to take into account the fact that we have sampled from the wrong distribution we introduce weights wr p which we use to adjust the importance of each point in our estimator thus pr pr wr exercise prove that if qx is non-zero for all x where p is non-zero the estimator converges to the mean value of as r increases. what is the variance of this estimator asymptotically? hint consider the statistics of the numerator and the denominator separately. is the estimator an unbiased estimator for small r? a practical with importance sampling is that it is hard to estimate how reliable the estimator is. the variance of the estimator is unknown beforehand because it depends on an integral over x of a function involving p and the variance of is hard to estimate because the empirical variances of the quantities wr and are not necessarily a good guide to the true variances of the numerator and denominator in equation if the proposal density qx is small in a region where is large then it is quite possible even after many points xr have been generated that none of them will have fallen in that region. in this case the estimate of would be drastically wrong and there would be no indication in the empirical variance that the true variance of the estimator is large. cautionary illustration of importance sampling in a toy problem related to the modelling of amino acid probability distributions with a one-dimensional variable x i evaluated a quantity of interest using importance sampling. the results using a gaussian sampler and a cauchy sampler are shown in the horizontal axis shows the number of p x figure functions involved in importance sampling. we wish to estimate the expectation of under p p we can generate samples from the simpler distribution qx we can evaluate and p at any point. figure importance sampling in action using a gaussian sampler density using a cauchy sampler density. vertical axis shows the estimate the horizontal line indicates the true value of horizontal axis shows number of samples on a log scale. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. px qx phix figure a multimodal distribution p and a unimodal sampler qx. importance sampling samples on a log scale. in the case of the gaussian sampler after about samples had been evaluated one might be tempted to call a halt but evidently there are infrequent samples that make a huge contribution to and the value of the estimate at samples is wrong. even after a million samples have been taken the estimate has still not settled down close to the true value. in contrast the cauchy sampler does not from glitches it converges the scale shown here after about samples. this example illustrates the fact that an importance sampler should have heavy tails. exercise consider the situation where p is multimodal consisting of several widely-separated peaks. distributions like this arise frequently in statistical data modelling. discuss whether it is a wise strategy to do importance sampling using a sampler qx that is a unimodal distribution to one of these peaks. assume that the function whose mean is to be estimated is a smoothly varying function of x such as mx c. describe the typical evolution of the estimator as a function of the number of samples r. importance sampling in many dimensions we have already observed that care is needed in one-dimensional importance sampling problems. is importance sampling a useful technique in spaces of higher dimensionality say n consider a simple case-study where the target density p is a uniform distribution inside a sphere where the origin p rp rp i and the proposal density is a gaussian centred on qx normalxi an importance-sampling method will be in trouble if the estimator is dominated by a few large weights wr. what will be the typical range of values of the weights wr? we know from our discussions of typical sequences in part i see exercise for example that if is the distance from the origin of a sample from q the quantity has a roughly gaussian distribution with mean and standard deviation n thus almost all samples from q lie in a typical set with distance from the origin very close to pn let us assume that is chosen such that the typical set of q lies inside the sphere of radius rp it does not then the law of large numbers implies that almost all the samples generated from q will fall outside rp and will have weight zero. then we know that most samples from q will have a value of q that lies in the range exp n thus the weights wr p will typically have values in the range exp n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. p p u x x x so if we draw a hundred samples what will the typical range of weights be? we can roughly estimate the ratio of the largest weight to the median weight by doubling the standard deviation in equation the largest weight and the median weight will typically be in the ratio monte carlo methods figure rejection sampling. the functions involved in rejection sampling. we desire samples from p p we are able to draw samples from qx and we know a value c such that c p for all x. a point u is generated at random in the lightly shaded area under the curve c if this point also lies below p then it is accepted. r wmax wmed r in n dimensions therefore the largest weight after one hundred samples is likely to be roughly times greater than the median weight. thus an importance sampling estimate for a high-dimensional problem will very likely be utterly dominated by a few samples with huge weights. in conclusion importance sampling in high dimensions often from two first we need to obtain samples that lie in the typical set of p and this may take a long time unless q is a good approximation to p second even if we obtain samples in the typical set the weights associated with those samples are likely to vary by large factors because the probabilities of points in a typical set although similar to each other still by factors of order exppn so the weights will too unless q is a near-perfect approximation to p rejection sampling we assume again a one-dimensional density p p that is too complicated a function for us to be able to sample from it directly. we assume that we have a simpler proposal density qx which we can evaluate a multiplicative factor zq as before and from which we can generate samples. we further assume that we know the value of a constant c such that c p for all x a schematic picture of the two functions is shown in we generate two random numbers. the x is generated from the proposal density qx. we then evaluate c and generate a uniformly distributed random variable u from the interval c these two random numbers can be viewed as selecting a point in the two-dimensional plane as shown in we now evaluate p and accept or reject the sample x by comparing the value of u with the value of p if u p then x is rejected otherwise it is accepted which means that we add x to our set of samples fxrg. the value of u is discarded. why does this procedure generate samples from p the proposed point u comes with uniform probability from the lightly shaded area underneath the curve c as shown in the rejection rule rejects all the points that lie above the curve p so the points u that are accepted are uniformly distributed in the heavily shaded area under p this implies copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the metropolishastings method that the probability density of the x-coordinates of the accepted points must be proportional to p so the samples must be independent samples from p rejection sampling will work best if q is a good approximation to p if q is very from p then for c q to exceed p everywhere c will necessarily have to be large and the frequency of rejection will be large. rejection sampling in many dimensions in a high-dimensional problem it is very likely that the requirement that c be an upper bound for p will force c to be so huge that acceptances will be very rare indeed. finding such a value of c may be too since in many problems we know neither where the modes of p are located nor how high they are. as a case study consider a pair of n gaussian distributions with mean zero imagine generating samples from one with standard deviation and using rejection sampling to obtain samples from the other whose standard deviation is let us assume that these two standard deviations are close in value say is larger than must be larger than because if this is not the case there is no c such that c q exceeds p for all x. so what value of c is required if the dimensionality is n the density of qx at the origin is so for c q to exceed p we need to set c p ln with n and we c what will the acceptance rate be for this value of c? the answer is immediate since the acceptance rate is the ratio of the volume under the curve p to the volume under c qx the fact that p and q are both normalized here implies that the acceptance rate will be for example in general c grows exponentially with the dimensionality n so the acceptance rate is expected to be exponentially small in n rejection sampling therefore whilst a useful method for one-dimensional problems is not expected to be a practical technique for generating samples from high-dimensional distributions p the metropolishastings method importance sampling and rejection sampling work well only if the proposal density qx is similar to p in large and complex problems it is to create a single density qx that has this property. the metropolishastings algorithm instead makes use of a proposal density q which depends on the current state xt. the density xt might be a simple distribution such as a gaussian centred on the current xt. the proposal density x can be any density from which we can draw samples. in contrast to importance sampling and rejection sampling it is not necessary that xt look at all similar to p in order for the algorithm to be practically useful. an example of a proposal density is shown in this shows the density xt for two states and as before we assume that we can evaluate p for any x. a tentative new state is generated from the proposal density xt. to decide px cqx figure a gaussian p and a slightly broader gaussian qx scaled up by a factor c such that c qx p qx p p x qx x figure metropolishastings method in one dimension. the proposal distribution x is here shown as having a shape that changes as x changes though this is not typical of the proposal densities used in practice. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods whether to accept the new state we compute the quantity a p p qxt xt if a then the new state is accepted. otherwise the new state is accepted with probability a. if the step is accepted we set if the step is rejected then we set xt. note the from rejection sampling in rejection sampling rejected points are discarded and have no on the list of samples fxrg that we collected. here a rejection causes the current state to be written again onto the list. notation. i have used the superscript r r to label points that are independent samples from a distribution and the superscript t t to label the sequence of states in a markov chain. it is important to note that a metropolishastings simulation of t iterations does not produce t independent samples from the target distribution p the samples are dependent. to compute the acceptance probability we need to be able to compute the probability ratios p and qxt xt. if the proposal density is a simple symmetrical density such as a gaussian centred on the current point then the latter factor is unity and the metropolishastings method simply involves comparing the value of the target density at the two points. this special case is sometimes called the metropolis method. however with apologies to hastings i will call the general metropolishastings algorithm for asymmetric q the metropolis method since i believe important ideas deserve short names. convergence of the metropolis method to the target density it can be shown that for any positive q is any q such that x for all x as t the probability distribution of xt tends to p p statement should not be seen as implying that q has to assign positive probability to every point we will discuss examples later where x for some x notice also that we have said nothing about how rapidly the convergence to p takes place. the metropolis method is an example of a markov chain monte carlo method mcmc. in contrast to rejection sampling where the accepted points fxrg are independent samples from the desired distribution markov chain monte carlo methods involve a markov process in which a sequence of states fxtg is generated each sample xt having a probability distribution that depends on the previous value since successive samples are dependent the markov chain may have to be run for a considerable time in order to generate samples that are independent samples from p just as it was to estimate the variance of an importance sampling estimator so it is to assess whether a markov chain monte carlo method has converged and to quantify how long one has to wait to obtain samples that are independent samples from p demonstration of the metropolis method the metropolis method is widely used for high-dimensional problems. many implementations of the metropolis method employ a proposal distribution copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the metropolishastings method qx p l figure metropolis method in two dimensions showing a traditional proposal density that has a small step size that the acceptance frequency will be about with a length scale that is short relative to the longest length scale l of the probable region a reason for choosing a small length scale is that for most high-dimensional problems a large random step from a typical point is a sample from p is very likely to end in a state that has very low probability such steps are unlikely to be accepted. if is large movement around the state space will only occur when such a transition to a low-probability state is actually accepted or when a large random step chances to land in another probable state. so the rate of progress will be slow if large steps are used. the disadvantage of small steps on the other hand is that the metropolis method will explore the probability distribution by a random walk and a random walk takes a long time to get anywhere especially if the walk is made of small steps. exercise consider a one-dimensional random walk on each step of which the state moves randomly to the left or to the right with equal probability. show that after t steps of size the state is likely to have moved only a distance about pt the root mean square distance travelled. recall that the aim of monte carlo sampling is to generate a number of independent samples from the given distribution dozen say. if the largest length scale of the state space is l then we have to simulate a random-walk metropolis method for a time t before we can expect to get a sample that is roughly independent of the initial condition and that s assuming that every step is accepted if only a fraction f of the steps are accepted on average then this time is increased by a factor rule of thumb lower bound on number of iterations of a metropolis method. if the largest length scale of the space of probable states is l a metropolis method whose proposal distribution generates a random walk with step size must be run for at least t iterations to obtain an independent sample. this rule of thumb gives only a lower bound the situation may be much worse if for example the probability distribution consists of several islands of high probability separated by regions of low probability. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods figure metropolis method for a toy problem. the state sequence for t horizontal direction states from to vertical direction time from to the cross bars mark time intervals of duration histogram of occupancy of the states after and iterations. for comparison histograms resulting when successive points are drawn independently from the target distribution. metropolis independent sampling iterations iterations iterations iterations iterations iterations copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the metropolishastings method to illustrate how slowly a random walk explores a state space shows a simulation of a metropolis algorithm for generating samples from the distribution p x otherwise. the proposal distribution is otherwise. x x because the target distribution p is uniform rejections occur only when the proposal takes the state to or the simulation was started in the state and its evolution is shown in how long does it take to reach one of the end states x and x since the distance is steps the rule of thumb predicts that it will typically take a time t iterations to reach an end state. this is in the present example the step into an end state occurs on the iteration. how long does it take to visit both end states? the rule of thumb predicts about iterations are required to traverse the whole state space and indeed the encounter with the other end state takes place on the iteration. thus samples are generated only by simulating for about four hundred iterations per independent sample. this simple example shows that it is important to try to abolish random walk behaviour in monte carlo methods. a systematic exploration of the toy state space could get around it using the same step sizes in about twenty steps instead of four hundred. methods for reducing random walk behaviour are discussed in the next chapter. metropolis method in high dimensions the rule of thumb which gives a lower bound on the number of iterations of a random walk metropolis method also applies to higher-dimensional problems. consider the simple case of a target distribution that is an n dimensional gaussian and a proposal distribution that is a spherical gaussian of standard deviation in each direction. without loss of generality we can assume that the target distribution is a separable distribution aligned with the axes fxng and that it has standard deviation in direction n. let and be the largest and smallest of these standard deviations. let us assume that is adjusted such that the acceptance frequency is close to under this assumption each variable xn evolves independently of all the others executing a random walk with step size about the time taken to generate independent samples from the target distribution will be controlled by the largest lengthscale just as in the previous section where we needed at least t iterations to obtain an independent sample here we need t now how big can be? the bigger it is the smaller this number t becomes but if is too big bigger than then the acceptance rate will fall sharply. it seems plausible that the optimal must be similar to min. strictly this may not be true in special cases where the second smallest is greater than the optimal may be closer to that second smallest but our rough conclusion is this where simple spherical proposal distributions are used we will need at least t iterations to obtain an independent sample where and are the longest and shortest lengthscales of the target distribution. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods figure gibbs sampling. the joint density p from which samples are required. starting from a state xt is sampled from the conditional density p j xt a sample is then made from the conditional density p j a couple of iterations of gibbs sampling. p p j xt xt p j xt this is good news and bad news. it is good news because unlike the cases of rejection sampling and importance sampling there is no catastrophic dependence on the dimensionality n our computer will give useful answers in a time shorter than the age of the universe. but it is bad news all the same because this quadratic dependence on the lengthscale-ratio may still force us to make very lengthy simulations. fortunately there are methods for suppressing random walks in monte carlo simulations which we will discuss in the next chapter. gibbs sampling we introduced importance sampling rejection sampling and the metropolis method using one-dimensional examples. gibbs sampling also known as the heat bath method or glauber dynamics is a method for sampling from distributions over at least two dimensions. gibbs sampling can be viewed as a metropolis method in which a sequence of proposal distributions q are in terms of the conditional distributions of the joint distribution p it is assumed that whilst p is too complex to draw samples from directly its conditional distributions p are tractable to work with. for many graphical models not all these one-dimensional conditional distributions are straightforward to sample from. for example if a gaussian distribution for some variables d has an unknown mean m and the prior distribution of m is gaussian then the conditional distribution of m given d is also gaussian. conditional distributions that are not of standard form may still be sampled from by adaptive rejection sampling if the conditional distribution certain convexity properties and wild gibbs sampling is illustrated for a case with two variables x in on each iteration we start from the current state xt and is sampled from the conditional density p j with to xt a sample is then made from the conditional density p j using the copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gibbs sampling new value of this brings us to the new state and completes the iteration. in the general case of a system with k variables a single iteration involves sampling one parameter at a time p j xt xt p j p j xt k xt xt k xt k etc. convergence of gibbs sampling to the target density exercise show that a single variable-update of gibbs sampling can be viewed as a metropolis method with target density p and that this metropolis method has the property that every proposal is always accepted. because gibbs sampling is a metropolis method the probability distribution of xt tends to p as t as long as p does not have pathological properties. exercise discuss whether the syndrome decoding problem for a hamming code can be solved using gibbs sampling. the syndrome decoding problem if we are to solve it with a monte carlo approach is to draw samples from the posterior distribution of the noise vector n nn nn p f z z n f nn n z where fn is the normalized likelihood for the nth transmitted bit and z is the observed syndrome. the factor z is if n has the correct syndrome z and otherwise. what about the syndrome decoding problem for any linear error-correcting code? gibbs sampling in high dimensions gibbs sampling from the same defect as simple metropolis algorithms the state space is explored by a slow random walk unless a fortuitous parameterization has been chosen that makes the probability distribution p separable. if say two variables and are strongly correlated having marginal densities of width l and conditional densities of width then it will take at least about iterations to generate an independent sample from the target density. figure illustrates the slow progress made by gibbs sampling when l however gibbs sampling involves no adjustable parameters so it is an attractive strategy when one wants to get a model running quickly. an excellent software package bugs makes it easy to set up almost arbitrary probabilistic models and simulate them by gibbs sampling et al. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods terminology for markov chain monte carlo methods we now spend a few moments sketching the theory on which the metropolis method and gibbs sampling are based. we denote by ptx the probability distribution of the state of a markov chain simulator. visualize this distribution imagine running an collection of identical simulators in parallel. our aim is to a markov chain such that as t ptx tends to the desired distribution p a markov chain can be by an initial probability distribution and a transition probability t x. the probability distribution of the state at the iteration of the markov chain is given by dn x t xptx example an example of a markov chain is given by the metropolis demonstration of section for which the transition probability is t and the initial distribution was the probability distribution ptx of the state at the tth iteration is shown for t in an equivalent sequence of distributions is shown in for the chain that begins in initial state both chains converge to the target density the uniform density as t required properties when designing a markov chain monte carlo method we construct a chain with the following properties the desired distribution p is an invariant distribution of the chain. figure the probability distribution of the state of the markov chain of example a distribution is an invariant distribution of the transition probability t x if an invariant distribution is an eigenvector of the transition probability matrix that has eigenvalue dn x t copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. terminology for markov chain monte carlo methods the chain must also be ergodic that is ptx as t for any a couple of reasons why a chain might not be ergodic are figure the probability distribution of the state of the markov chain for initial condition its matrix might be reducible which means that the state space contains two or more subsets of states that can never be reached from each other. such a chain has many invariant distributions which one ptx would tend to as t would depend on the initial condition the transition probability matrix of such a chain has more than one eigenvalue equal to the chain might have a periodic set which means that for some initial conditions ptx doesn t tend to an invariant distribution but instead tends to a periodic limit-cycle. a simple markov chain with this property is the random walk on the n hypercube. the chain t takes the state from one corner to a randomly chosen adjacent corner. the unique invariant distribution of this chain is the uniform distribution over all states but the chain is not ergodic it is periodic with period two if we divide the states into states with odd parity and states with even parity we notice that every odd state is surrounded by even states and vice versa. so if the initial condition at time t is a state with even parity then at time t and at all odd times the state must have odd parity and at all even times the state will be of even parity. the transition probability matrix of such a chain has more than one eigenvalue with magnitude equal to the random walk on the hypercube for example has eigenvalues equal to and methods of construction of markov chains it is often convenient to construct t by mixing or concatenating simple base transitions b all of which satisfy p dn x xp for the desired density p i.e. they all have the desired density as an invariant distribution. these base transitions need not individually be ergodic. t is a mixture of several base transitions x if we make the transition by picking one of the base transitions at random and allowing it to determine the transition i.e. x t x where fpbg is a probability distribution over the base transitions. t is a concatenation of two base transitions x and x if we make a transition to an intermediate state using and then make a transition from state to using t x dn x copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods detailed balance many useful transition probabilities satisfy the detailed balance property t xbp t xap for all xb and xa this equation says that if we pick magic a state from the target density p and make a transition under t to another state it is just as likely that we will pick xb and go from xb to xa as it is that we will pick xa and go from xa to xb. markov chains that satisfy detailed balance are also called reversible markov chains. the reason why the detailed-balance property is of interest is that detailed balance implies invariance of the distribution p under the markov chain t which is a necessary condition for the key property that we want from our mcmc simulation that the probability distribution of the chain should converge to p exercise prove that detailed balance implies invariance of the distri bution p under the markov chain t proving that detailed balance holds is often a key step when proving that a markov chain monte carlo simulation will converge to the desired distribution. the metropolis method detailed balance for example. detailed balance is not an essential condition however and we will see later that irreversible markov chains can be useful in practice because they may have random walk properties. exercise show that if we concatenate two base transitions and that satisfy detailed balance it is not necessarily the case that the t thus detailed balance. exercise does gibbs sampling with several variables all updated in a deterministic sequence satisfy detailed balance? slice sampling slice sampling neal is a markov chain monte carlo method that has similarities to rejection sampling gibbs sampling and the metropolis method. it can be applied wherever the metropolis method can be applied that is to any system for which the target density p can be evaluated at any point x it has the advantage over simple metropolis methods that it is more robust to the choice of parameters like step sizes. the simplest version of slice sampling is similar to gibbs sampling in that it consists of one-dimensional transitions in the state space however there is no requirement that the one-dimensional conditional distributions be easy to sample from nor that they have any convexity properties such as are required for adaptive rejection sampling. and slice sampling is similar to rejection sampling in that it is a method that asymptotically draws samples from the volume under the curve described by p but there is no requirement for an upper-bounding function. i will describe slice sampling by giving a sketch of a one-dimensional sampling algorithm then giving a pictorial description that includes the details that make the method valid. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. slice sampling the skeleton of slice sampling let us assume that we want to draw samples from p p where x is a real number. a one-dimensional slice sampling algorithm is a method for making transitions from a two-dimensional point u lying under the curve p to another point lying under the same curve such that the probability distribution of u tends to a uniform distribution over the area under the curve p whatever initial point we start from like the uniform distribution under the curve p produced by rejection sampling a single transition u of a one-dimensional slice sampling algorithm has the following steps of which steps and will require further elaboration. evaluate p draw a vertical coordinate p create a horizontal interval xr enclosing x loop f g draw uniformxl xr evaluate p if p break out of loop else modify the interval xr there are several methods for creating the interval xr in step and several methods for modifying it at step the important point is that the overall method must satisfy detailed balance so that the uniform distribution for u under the curve p is invariant. the stepping out method for step in the stepping out method for creating an interval xr enclosing x we step out in steps of length w until we endpoints xl and xr at which p is smaller than u. the algorithm is shown in draw r xl x rw xr x rw while f xl xl w g while f xr xr w g the shrinking method for step whenever a point is drawn such that lies above the curve p we shrink the interval so that one of the end points is and such that the original point x is still enclosed in the interval. if x f xr g else f xl g properties of slice sampling like a standard metropolis method slice sampling gets around by a random walk but whereas in the metropolis method the choice of the step size is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods figure slice sampling. each panel is labelled by the steps of the algorithm that are executed in it. at step p is evaluated at the current point x. at step a vertical coordinate is selected giving the point shown by the box at steps an interval of size w containing is created at random. at step p is evaluated at the left end of the interval and is found to be larger than so a step to the left of size w is made. at step p is evaluated at the right end of the interval and is found to be smaller than so no stepping out to the right is needed. when step is repeated p is found to be smaller than so the stepping out halts. at step a point is drawn from the interval shown by a step establishes that this point is above p and step shrinks the interval to the rejected point in such a way that the original point x is still in the interval. when step is repeated the new coordinate is to the right-hand side of the interval gives a value of p greater than so this point is the outcome at step copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. slice sampling figure p critical to the rate of progress in slice sampling the step size is self-tuning. if the initial interval size w is too small by a factor f compared with the width of the probable region then the stepping-out procedure expands the interval size. the cost of this stepping-out is only linear in f whereas in the metropolis method the computer-time scales as the square of f if the step size is too small. if the chosen value of w is too large by a factor f then the algorithm spends a time proportional to the logarithm of f shrinking the interval down to the right size since the interval typically shrinks by a factor in the ballpark of each time a point is rejected. in contrast the metropolis algorithm responds to a too-large step size by rejecting almost all proposals so the rate of progress is exponentially bad in f there are no rejections in slice sampling. the probability of staying in exactly the same place is very small. exercise investigate the properties of slice sampling applied to the density shown in x is a real variable between and how long does it take typically for slice sampling to get from an x in the peak region x to an x in the tail region x and vice versa? that the probabilities of these transitions do yield an asymptotic probability density that is correct. how slice sampling is used in real problems an n density p p may be sampled with the help of the one-dimensional slice sampling method presented above by picking a sequence of directions and x xt xyt. the function p above is replaced by p p xyt. the directions may be chosen in various ways for example as in gibbs sampling the directions could be the coordinate axes alternatively the directions yt may be selected at random in any manner such that the overall procedure detailed balance. computer-friendly slice sampling the real variables of a probabilistic model will always be represented in a computer using a number of bits. in the following implementation of slice sampling due to skilling the stepping-out randomization and shrinking operations described above in terms of operations are replaced by binary and integer operations. we assume that the variable x that is being slice-sampled is represented by a b-bit integer x taking on one of b values many or all of which correspond to valid values of x. using an integer grid eliminates any errors in detailed balance that might ensue from variable-precision rounding of numbers. the mapping from x to x need not be linear if it is nonlinear we assume that the function p is replaced by an appropriately transformed function for example p p we assume the following operators on b-bit integers are available x n x n x n n randbitsl arithmetic sum modulo b of x and n modulo b of x and n bitwise exclusive-or of x and n sets n to a random l-bit integer. a slice-sampling procedure for integers is then as follows copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods given a current point x and a height y p p u randbitsb set l to a value l b do f n randbitsl u n u l l g until x or y a random translation u of the binary coordinate system. set initial l-bit sampling range. a random move within the current interval of width randomize the lowest l bits of x the translated coordinate system. if is not acceptable decrease l and try again with a smaller perturbation of x termination at or before l is assured. x b figure the sequence of intervals from which the new candidate points are drawn. the translation u is introduced to avoid permanent sharp edges where for example the adjacent binary integers and would otherwise be permanently in sectors making it for x to move from one to the other. the sequence of intervals from which the new candidate points are drawn is illustrated in first a point is drawn from the entire interval shown by the top horizontal line. at each subsequent draw the interval is halved in such a way as to contain the previous point x. if preliminary stepping-out from the initial range is required step above can be replaced by the following similar procedure l sets the initial width set l to a value l b do f n randbitsl u n u l l g until b or y these shrinking and stepping out methods shrink and expand by a factor of two per evaluation. a variant is to shrink or expand by more than one bit each time setting l l with taking at each step from any pre-assigned distribution may include allows extra exercise in the shrinking phase after an unacceptable has been produced the choice of is allowed to depend on the between the slice s height y and the value of p without spoiling the algorithm s validity. this. it might be a good idea to choose a larger value of when y p is large. investigate this idea theoretically or empirically. a feature of using the integer representation is that with a suitably extended number of bits the single integer x can represent two or more real parameters for example by mapping x to through a curve such as a peano curve. thus multi-dimensional slice sampling can be performed using the same software as for one dimension. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. practicalities practicalities can we predict how long a markov chain monte carlo simulation will take to equilibrate? by considering the random walks involved in a markov chain monte carlo simulation we can obtain simple lower bounds on the time required for convergence. but predicting this time more precisely is a problem and most of the theoretical results giving upper bounds on the convergence time are of little practical use. the exact sampling methods of chapter a solution to this problem for certain markov chains. can we diagnose or detect convergence in a running simulation? this is also a problem. there are a few practical tools available but none of them is perfect and carlin can we speed up the convergence time and time between independent samples of a markov chain monte carlo method? here there is good news as described in the next chapter which describes the hamiltonian monte carlo method overrelaxation and simulated annealing. further practical issues can the normalizing constant be evaluated? if the target density p is given in the form of an unnormalized density p with p z p the value of z may well be of interest. monte carlo methods do not readily yield an estimate of this quantity and it is an area of active research to ways of evaluating it. techniques for evaluating z include importance sampling by neal and annealed impor tance sampling thermodynamic integration during simulated annealing the acceptance ratio method and umbrella sampling by neal reversible jump markov chain monte carlo one way of dealing with z however may be to a solution to one s task that does not require that z be evaluated. in bayesian data modelling one might be able to avoid the need to evaluate z which would be important for model comparison by not having more than one model. instead of using several models in complexity for example and evaluating their relative posterior probabilities one can make a single hierarchical model having for example various continuous hyperparameters which play a role similar to that played by the distinct models in noting the possibility of not computing z i am not endorsing this approach. the normalizing constant z is often the single most important number in the problem and i think every should be devoted to calculating it. the metropolis method for big models our original description of the metropolis method involved a joint updating of all the variables using a proposal density x. for big problems it may be more to use several proposal distributions x each of which updates only some of the components of x. each proposal is individually accepted or rejected and the proposal distributions are repeatedly run through in sequence. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods exercise explain why the rate of movement through the state space will be greater when b proposals qb are considered individually in sequence compared with the case of a single proposal by the concatenation of qb. assume that each proposal distribution x has an acceptance rate f in the metropolis method the proposal density x typically has a number of parameters that control for example its width these parameters are usually set by trial and error with the rule of thumb being to aim for a rejection frequency of about it is not valid to have the width parameters be dynamically updated during the simulation in a way that depends on the history of the simulation. such a of the proposal density would violate the detailed-balance condition that guarantees that the markov chain has the correct invariant distribution. gibbs sampling in big models our description of gibbs sampling involved sampling one parameter at a time as described in equations for big problems it may be more to sample groups of variables jointly that is to use several proposal distributions a b p xa j xt p xb j xt k a xt xt k etc. how many samples are needed? at the start of this chapter we observed that the variance of an estimator depends only on the number of independent samples r and the value of dn x p we have now discussed a variety of methods for generating samples from p how many independent samples r should we aim for? in many problems we really only need about twelve independent samples from p imagine that x is an unknown vector such as the amount of corrosion present in each of underground pipelines around cambridge and is the total cost of repairing those pipelines. the distribution p describes the probability of a state x given the tests that have been carried out on some pipelines and the assumptions about the physics of corrosion. the quantity is the expected cost of the repairs. the quantity is the variance of the cost measures by how much we should expect the actual cost to from the expectation now how accurately would a manager like to know i would suggest there is little point in knowing to a precision than about after all the true cost is likely to by from if we obtain r independent samples from p we can estimate to a precision of which is smaller than so twelve samples allocation of resources assuming we have decided how many independent samples r are required an important question is how one should make use of one s limited computer resources to obtain these samples. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. figure three possible markov chain monte carlo strategies for obtaining twelve samples in a amount of computer time. time is represented by horizontal lines samples by white circles. a single run consisting of one long burn in period followed by a sampling period. four medium-length runs with initial conditions and a medium-length burn in period. twelve short runs. summary a typical markov chain monte carlo experiment involves an initial period in which control parameters of the simulation such as step sizes may be adjusted. this is followed by a burn in period during which we hope the simulation converges to the desired distribution. finally as the simulation continues we record the state vector occasionally so as to create a list of states fxrgr that we hope are roughly independent samples from p there are several possible strategies make one long run obtaining all r samples from it. make a few medium-length runs with initial conditions obtain ing some samples from each. make r short runs each starting from a random initial condition with the only state that is recorded being the state of each simulation. the strategy has the best chance of attaining convergence the last strategy may have the advantage that the correlations between the recorded samples are smaller. the middle path is popular with markov chain monte carlo experts et al. because it avoids the of discarding burn-in iterations in many runs while still allowing one to detect problems with lack of convergence that would not be apparent from a single run. finally i should emphasize that there is no need to make the points in the estimate nearly-independent. averaging over dependent points is it won t lead to any bias in the estimates. for example when you use strategy or you may if you wish include all the points between the and last sample in each run. of course estimating the accuracy of the estimate is harder when the points are dependent. summary monte carlo methods are a powerful tool that allow one to sample from any probability distribution that can be expressed in the form p z p monte carlo methods can answer virtually any query related to p by putting the query in the form z rxr copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods in high-dimensional problems the only satisfactory methods are those based on markov chains such as the metropolis method gibbs sampling and slice sampling. gibbs sampling is an attractive method because it has no adjustable parameters but its use is restricted to cases where samples can be generated from the conditional distributions. slice sampling is attractive because whilst it has step-length parameters its performance is not very sensitive to their values. simple metropolis algorithms and gibbs sampling algorithms although widely used perform poorly because they explore the space by a slow random walk. the next chapter will discuss methods for speeding up markov chain monte carlo simulations. slice sampling does not avoid random walk behaviour but it automatically chooses the largest appropriate step size thus reducing the bad of the random walk compared with say a metropolis method with a tiny step size. exercises exercise a study of importance sampling. we already established in section that importance sampling is likely to be useless in high-dimensional problems. this exercise explores a further cautionary tale showing that importance sampling can fail even in one dimension even with friendly gaussian distributions. imagine that we want to know the expectation of a function under a distribution p dx p and that this expectation is estimated by importance sampling with a distribution qx. alternatively perhaps we wish to estimate the normalizing constant z in p p using z dx p dx qx p qx p now let p and qx be gaussian distributions with mean zero and standard deviations and each point x drawn from q will have an associated weight p what is the variance of the weights? that p p so p is actually normalized and z though we can pretend that we didn t know that. what happens to the variance of the weights as check your theory by simulating this importance-sampling problem on a computer. q exercise consider the metropolis algorithm for the one-dimensional toy problem of section sampling from whenever the current state is one of the end states the proposal density given in equation will propose with probability a state that will be rejected. to reduce this waste fred the software responsible for generating samples from q so that when x the proposal density is on and similarly when x is always proposed. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises fred sets the software that implements the acceptance rule so that the software accepts all proposed moves. what probability p will fred s software generate samples from? what is the correct acceptance rule for fred s proposal density in order to obtain samples from p exercise implement gibbs sampling for the inference of a single one-dimensional gaussian which we studied using maximum likelihood in section assign a broad gaussian prior to and a broad gamma prior to the precision parameter each update of will involve a sample from a gaussian distribution and each update of requires a sample from a gamma distribution. exercise gibbs sampling for clustering. implement gibbs sampling for the inference of a mixture of k one-dimensional gaussians which we studied using maximum likelihood in section allow the clusters to have standard deviations assign priors to the means and standard deviations in the same way as the previous exercise. either the prior probabilities of the classes to be equal or put a uniform prior over the parameters and include them in the gibbs sampling. notice the similarity of gibbs sampling to the soft k-means clustering algorithm we can alternately assign the class labels fkng given the parameters then update the parameters given the class labels. the assignment step involves sampling from the probability distributions by the responsibilities and the update step updates the means and variances using probability distributions centred on the k-means algorithm s values do your experiments that monte carlo methods bypass the of maximum likelihood discussed in section a solution to this exercise and the previous one written in octave is exercise implement gibbs sampling for the seven scientists inference problem which we encountered in exercise and which you may have solved by exact marginalization s not essential to have done the latter. exercise a metropolis method is used to explore a distribution p that is actually a spherical gaussian distribution of standard deviation in all dimensions. the proposal density q is a spherical gaussian distribution of standard deviation roughly what is the step size if the acceptance rate is assuming this value of roughly how long would the method take to traverse the distribution and generate a sample independent of the initial condition? by how much does ln p change in a typical step? by how much should ln p vary when x is drawn from p what happens if rather than using a metropolis method that tries to change all components at once one instead uses a concatenation of metropolis updates changing one component at a time? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods exercise when discussing the time taken by the metropolis algorithm to generate independent samples we considered a distribution with longest spatial length scale l being explored using a proposal distribution with step size another dimension that a mcmc method must explore is the range of possible values of the log probability ln p assuming that the state x contains a number of independent random variables proportional to n when samples are drawn from p the asymptotic equipartition principle tell us that the value of ln p is likely to be close to the entropy of x varying either side with a standard deviation that scales as pn consider a metropolis method with a symmetrical proposal density that is one that qx x. assuming that accepted jumps either increase ln p by some amount or decrease it by a small amount e.g. ln e this a reasonable assumption? discuss how long it must take to generate roughly independent samples from p discuss whether gibbs sampling has similar properties. exercise markov chain monte carlo methods do not compute partition functions z yet they allow ratios of quantities like z to be estimated. for example consider a random-walk metropolis algorithm in a state space where the energy is zero in a connected accessible region and large everywhere else and imagine that the accessible space can be chopped into two regions connected by one or more corridor states. the fraction of times spent in each region at equilibrium is proportional to the volume of the region. how does the monte carlo method manage to do this without measuring the volumes? exercise philosophy. one curious defect of these monte carlo methods which are widely used by bayesian statisticians is that they are all non-bayesian hagan they involve computer experiments from which estimators of quantities of interest are derived. these estimators depend on the proposal distributions that were used to generate the samples and on the random numbers that happened to come out of our random number generator. in contrast an alternative bayesian approach to the problem would use the results of our computer experiments to infer the properties of the target function p and generate predictive distributions for quantities of interest such as this approach would give answers that would depend only on the computed values of p at the points fxrg the answers would not depend on how those points were chosen. can you make a bayesian monte carlo method? rasmussen and ghahramani for a practical attempt. solutions solution to exercise we wish to show that pr pr wr converges to the expectation of under p we consider the numerator and the denominator separately. first the denominator. consider a single importance weight wr p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions what is its expectation averaged under the distribution q of the point xr? hwri dx qx p dx zq p zp zq so the expectation of the denominator is wr r zp zq as long as the variance of wr is the denominator divided by r will converge to zp as r increases. fact the estimate converges to the right answer even if this variance is as long as the expectation is similarly the expectation of one term in the numerator is dx qx p dx zq p zp zq where is the expectation of under p so the numerator divided by r converges to zp zq with increasing r. thus converges to the numerator and the denominator are unbiased estimators of rzp and rzp respectively but their ratio is not necessarily an unbiased estimator for r. solution to exercise when the true density p is multimodal it is unwise to use importance sampling with a sampler density to one mode because on the rare occasions that a point is produced that lands in one of the other modes the weight associated with that point will be enormous. the estimates will have enormous variance but this enormous variance may not be evident to the user if no points in the other modes have been seen. solution to exercise the posterior distribution for the syndrome decoding problem is a pathological distribution from the point of view of gibbs sampling. the factor z is only on a small fraction of the space of possible vectors n namely the points that correspond to the valid codewords. no two codewords are adjacent so similarly any single bit from a viable state n will take us to a state with zero probability and so the state will never move in gibbs sampling. a general code has exactly the same problem. the points corresponding to valid codewords are relatively few in number and they are not adjacent least for any useful code. so gibbs sampling is no use for syndrome decoding for two reasons. first any reasonably good hypothesis is and as long as the state is not near a valid codeword gibbs sampling cannot help since none of the conditional distributions is and second once we are in a valid hypothesis gibbs sampling will never take us out of it. one could attempt to perform gibbs sampling using the bits of the original message s as the variables. this approach would not get locked up in the way just described but for a good code any single bit would substantially alter the reconstructed codeword so if one had found a state with reasonably large likelihood gibbs sampling would take an impractically large time to escape from it. solution to exercise each metropolis proposal will take the energy of the state up or down by some amount. the total change in energy copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods when b proposals are concatenated will be the end-point of a random walk with b steps in it. this walk might have mean zero or it might have a tendency to drift upwards most moves increase the energy and only a few decrease it. in general the latter will hold if the acceptance rate f is small the mean change in energy from any one move will be some and so the acceptance probability for the concatenation of b moves will be of order which scales roughly as f b. the mean-square-distance moved will be of order f where is the typical step size. in contrast the mean-square-distance moved when the moves are considered individually will be of order f theory theory figure importance sampling in one dimension. for r and the normalizing constant of a gaussian distribution in fact to be was estimated using importance sampling with a sampler density of standard deviation axis. the same random number seed was used for all runs. the three plots show the estimated normalizing constant the empirical standard deviation of the r weights of the weights. solution to exercise the weights are w p and x is drawn from q. the mean weight is z dx qx dx p assuming the integral converges. the variance is varw z dx p qx z dx dx p p qx qx zq p z p where zqz of in the exponent is positive i.e. if p. the integral in is only if the if this condition is the variance is q p varw p p q q as approaches the critical value about the variance becomes figure illustrates these phenomena for with varying from to the same random number seed was used for all runs so the weights and estimates follow smooth curves. notice that the empirical standard deviation of the r weights can look quite small and well-behaved at when the true standard deviation is nevertheless copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods this chapter discusses several methods for reducing random walk behaviour in metropolis methods. the aim is to reduce the time required to obtain independent samples. for brevity we will say independent samples when we mean independent samples hamiltonian monte carlo the hamiltonian monte carlo method is a metropolis method applicable to continuous state spaces that makes use of gradient information to reduce random walk behaviour. hamiltonian monte carlo method was originally called hybrid monte carlo for historical reasons. for many systems whose probability p can be written in the form p z not only ex but also its gradient with respect to x can be readily evaluated. it seems wasteful to use a simple random-walk metropolis method when this gradient is available the gradient indicates which direction one should go in to states that have higher probability! overview of hamiltonian monte carlo in the hamiltonian monte carlo method the state space x is augmented by momentum variables p and there is an alternation of two types of proposal. the proposal randomizes the momentum variable leaving the state x unchanged. the second proposal changes both x and p using simulated hamiltonian dynamics as by the hamiltonian hx p ex kp where kp is a kinetic energy such as kp these two proposals are used to create samples from the joint density ph p zh p zh this density is separable so the marginal distribution of x is the desired distribution so simply discarding the momentum variables we obtain a sequence of samples fxtg that asymptotically come from p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods algorithm octave source code for the hamiltonian monte carlo method. g grade x e finde x set gradient using initial x set objective function too for l p randn sizex h p p e loop l times initial momentum is evaluate hxp xnew x for tau gnew g make tau leapfrog steps p p epsilon gnew make half-step in p xnew xnew epsilon p gnew grade xnew p p epsilon gnew make half-step in p make step in x find new gradient endfor enew finde xnew hnew p p enew dh hnew h find new value of h decide whether to accept if dh accept elseif rand exp-dh accept else accept endif if accept g gnew endif endfor x xnew e enew hamiltonian monte carlo simple metropolis figure hamiltonian monte carlo used to generate samples from a bivariate gaussian with correlation for comparison a simple random-walk metropolis method given equal computer time. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. hamiltonian monte carlo details of hamiltonian monte carlo the proposal which can be viewed as a gibbs sampling update draws a new momentum from the gaussian density this proposal is always accepted. during the second dynamical proposal the momentum variable determines where the state x goes and the gradient of ex determines how the momentum p changes in accordance with the equations p because of the persistent motion of x in the direction of the momentum p during each dynamical proposal the state of the system tends to move a distance that goes linearly with the computer time rather than as the square root. the second proposal is accepted in accordance with the metropolis rule. if the simulation of the hamiltonian dynamics is numerically perfect then the proposals are accepted every time because the total energy hx p is a constant of the motion and so a in equation is equal to one. if the simulation is imperfect because of step sizes for example then some of the dynamical proposals will be rejected. the rejection rule makes use of the change in hx p which is zero if the simulation is perfect. the occasional rejections ensure that asymptotically we obtain samples pt from the required joint density ph p. the source code in describes a hamiltonian monte carlo method that uses the leapfrog algorithm to simulate the dynamics on the function findex whose gradient is found by the function gradex. figure shows this algorithm generating samples from a bivariate gaussian whose energy function is ex xtax with a corresponding to a variancecovariance matrix of in starting from the state marked by the arrow the solid line represents two successive trajectories generated by the hamiltonian dynamics. the squares show the endpoints of these two trajectories. each trajectory consists of tau leapfrog steps with epsilon these steps are indicated by the crosses on the trajectory in the inset. after each trajectory the momentum is randomized. here both trajectories are accepted the errors in the hamiltonian were only and respectively. figure shows how a sequence of four trajectories converges from an initial condition indicated by the arrow that is not close to the typical set of the target distribution. the trajectory parameters tau and epsilon were randomized for each trajectory using uniform distributions with means and respectively. the trajectory takes us to a new state similar in energy to the state. the second trajectory happens to end in a state nearer the bottom of the energy landscape. here since the potential energy e is smaller the kinetic energy k is necessarily larger than it was at the start of the trajectory. when the momentum is randomized before the third trajectory its kinetic energy becomes much smaller. after the fourth copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gibbs sampling overrelaxation monte carlo methods figure overrelaxation contrasted with gibbs sampling for a bivariate gaussian with correlation the state sequence for iterations each iteration involving one update of both variables. the overrelaxation method had excessively large value is chosen to make it easy to see how the overrelaxation method reduces random walk behaviour. the dotted line shows the contour detail of showing the two steps making up each iteration. time-course of the variable during iterations of the two methods. the overrelaxation method had neal gibbs sampling overrelaxation trajectory has been simulated the state appears to have become typical of the target density. figures and show a random-walk metropolis method using a gaussian proposal density to sample from the same gaussian distribution starting from the initial conditions of and respectively. in the step size was adjusted such that the acceptance rate was the number of proposals was so the total amount of computer time used was similar to that in the distance moved is small because of random walk behaviour. in the random-walk metropolis method was used and started from the same initial condition as and given a similar amount of computer time. overrelaxation the method of overrelaxation is a method for reducing random walk behaviour in gibbs sampling. overrelaxation was originally introduced for systems in which all the conditional distributions are gaussian. an example of a joint distribution that is not gaussian but whose conditional distributions are all gaussian is p y copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. overrelaxation overrelaxation for gaussian conditional distributions in ordinary gibbs sampling one draws the new value of the current variable xi from its conditional distribution ignoring the old value xt the i state makes lengthy random walks in cases where the variables are strongly correlated as illustrated in the left-hand panel of this uses a correlated gaussian distribution as the target density. i in adler s overrelaxation method one instead samples from a gaussian that is biased to the opposite side of the conditional distribution. if the conditional distribution of xi is and the current value of xi is xt i then adler s method sets xi to i i where and is a parameter between and usually set to a negative value. is positive then the method is called under-relaxation. exercise show that this individual transition leaves invariant the con i ditional distribution xi a single iteration of adler s overrelaxation like one of gibbs sampling updates each variable in turn as indicated in equation the transition matrix t x by a complete update of all variables in some order does not satisfy detailed balance. each individual transition for one coordinate just described does satisfy detailed balance so the overall chain gives a valid sampling strategy which converges to the target density p but when we form a chain by applying the individual transitions in a sequence the overall chain is not reversible. this temporal asymmetry is the key to why overrelaxation can be if say two variables are positively correlated then they will a short timescale evolve in a directed manner instead of by random walk as shown in this may reduce the time required to obtain independent samples. exercise the transition matrix t x by a complete update of all variables in some order does not satisfy detailed balance. if the updates were in a random order then t would be symmetric. investigate for the toy two-dimensional gaussian distribution the assertion that the advantages of overrelaxation are lost if the overrelaxed updates are made in a random order. ordered overrelaxation the overrelaxation method has been generalized by neal whose ordered overrelaxation method is applicable to any system where gibbs sampling is used. in ordered overrelaxation instead of taking one sample from the conditional distribution p we create k such samples xk where k might be set to twenty or so. often generating k extra samples adds a negligible computational cost to the initial computations required for making the sample. the points fxk i g are then sorted numerically and the current value of xi is inserted into the sorted list giving a list of k points. we give them ranks k. let be the rank of the current value of xi in the list. we set to the value that is an equal distance from the other end of the list that is the value with rank k the role played by adler s parameter is here played by the parameter k. when k we obtain ordinary gibbs sampling. for practical purposes neal estimates that ordered overrelaxation may speed up a simulation by a factor of ten or twenty. i i i copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods simulated annealing a third technique for speeding convergence is simulated annealing. in simulated annealing a temperature parameter is introduced which when large allows the system to make transitions that would be improbable at temperature the temperature is set to a large value and gradually reduced to this procedure is supposed to reduce the chance that the simulation gets stuck in an unrepresentative probability island. we asssume that we wish to sample from a distribution of the form p z where ex can be evaluated. in the simplest simulated annealing method we instead sample from the distribution pt zt ex t and decrease t gradually to often the energy function can be separated into two terms ex of which the term is nice example a separable function of x and the second is nasty in these cases a better simulated annealing method might make use of the distribution p z with t gradually decreasing to in this way the distribution at high temperatures reverts to a well-behaved distribution by simulated annealing is often used as an optimization method where the aim is to an x that minimizes ex in which case the temperature is decreased to zero rather than to as a monte carlo method simulated annealing as described above doesn t sample exactly from the right distribution because there is no guarantee that the probability of falling into one basin of the energy is equal to the total probability of all the states in that basin. the closely related simulated tempering method and parisi corrects the biases introduced by the annealing process by making the temperature itself a random variable that is updated in metropolis fashion during the simulation. neal s annealed importance sampling method removes the biases introduced by annealing by computing importance weights for each generated point. skilling s multi-state leapfrog method a fourth method for speeding up monte carlo simulations due to john skilling has a similar spirit to overrelaxation but works in more dimensions. this method is applicable to sampling from a distribution over a continuous state space and the sole requirement is that the energy ex should be easy to evaluate. the gradient is not used. this leapfrog method is not intended to be used on its own but rather in sequence with other monte carlo operators. instead of moving just one state vector x around the state space as was the case for all the monte carlo methods discussed thus far skilling s leapfrog method simultaneously maintains a set of s state vectors fxsg where s copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. skilling s multi-state leapfrog method might be six or twelve. the aim is that all s of these vectors will represent independent samples from the same distribution p skilling s leapfrog makes a proposal for the new state which is accepted or rejected in accordance with the metropolis method by leapfrogging the current state xs over another state vector xt xt xs xs all the other state vectors are left where they are so the acceptance probability depends only on the change in energy of xs. which vector t is the partner for the leapfrog event can be chosen in various ways. the simplest method is to select the partner at random from the other vectors. it might be better to choose t by selecting one of the nearest neighbours xs nearest by any chosen distance function as long as one then uses an acceptance rule that ensures detailed balance by checking whether point t is still among the nearest neighbours of the new point xs xt why the leapfrog is a good idea imagine that the target density p has strong correlations for example the density might be a needle-like gaussian with width and length where l as we have emphasized motion around such a density by standard methods proceeds by a slow random walk. imagine now that our set of s points is lurking initially in a location that is probable under the density but in an inappropriately small ball of size now under skilling s leapfrog method a typical move will take the point a little outside the current ball perhaps doubling its distance from the centre of the ball. after all the points have had a chance to move the ball will have increased in size if all the moves are accepted the ball will be bigger by a factor of two or so in all dimensions. the rejection of some moves will mean that the ball containing the points will probably have elongated in the needle s long direction by a factor of say two. after another cycle through the points the ball will have grown in the long direction by another factor of two. so the typical distance travelled in the long dimension grows exponentially with the number of iterations. now maybe a factor of two growth per iteration is on the optimistic side but even if the ball only grows by a factor of let s say per iteration the growth is nevertheless exponential. it will only take a number of iterations proportional to log l for the long dimension to be explored. exercise discuss how the of skilling s method scales with dimensionality using a correlated n gaussian distribution as an example. find an expression for the rejection probability assuming the markov chain is at equilibrium. also discuss how it scales with the strength of correlation among the gaussian variables. skilling s method is invariant under transformations so the rejection probability at equilibrium can be found by looking at the case of a separable gaussian. this method has some similarity to the adaptive direction sampling method of gilks et al. but the leapfrog method is simpler and can be applied to a greater variety of distributions. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods monte carlo algorithms as communication channels it may be a helpful perspective when thinking about speeding up monte carlo methods to think about the information that is being communicated. two communications take place when a sample from p is being generated. first the selection of a particular x from p necessarily requires that at least log random bits be consumed. the use of inverse arithmetic coding as a method for generating samples from given distributions second the generation of a sample conveys information about p from the subroutine that is able to evaluate p from any other subroutines that have access to properties of p consider a dumb metropolis method for example. in a dumb metropolis method the proposals x have nothing to do with p properties of p are only involved in the algorithm at the acceptance step when the ratio p is computed. the channel from the true distribution p to the user who is interested in computing properties of p thus passes through a bottleneck all the information about p is conveyed by the string of acceptances and rejections. if p were replaced by a distribution the only way in which this change would have an is that the string of acceptances and rejections would be changed. i am not aware of much use being made of this information-theoretic view of monte carlo algorithms but i think it is an instructive viewpoint if the aim is to obtain information about properties of p then presumably it is helpful to identify the channel through which this information and maximize the rate of information transfer. example the information-theoretic viewpoint a simple for the widely-adopted rule of thumb which states that the parameters of a dumb metropolis method should be adjusted such that the acceptance rate is about one half. let s call the acceptance history that is the binary string of accept or reject decisions a. the information learned about p after the algorithm has run for t steps is less than or equal to the information content of a since all information about p is mediated by a. and the information content of a is upper-bounded by t where f is the acceptance rate. this bound on information acquired about p is maximized by setting f another helpful analogy for a dumb metropolis method is an evolutionary one. each proposal generates a progeny from the current state x. these two individuals then compete with each other and the metropolis method uses a noisy rule. if the progeny is than the parent p p assuming the qq factor is unity then the progeny replaces the parent. the survival rule also allows progeny to replace the parent sometimes. insights about the rate of evolution can thus be applied to monte carlo methods. exercise let x and let p be a separable distribution p pxg with and for example let the proposal density of a dumb metropolis algorithm q involve a fraction m of the g bits in the state x. analyze how long it takes for the chain to copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. multi-state methods converge to the target density as a function of m. find the optimal m and deduce how long the metropolis method must run for. compare the result with the results for an evolving population under natural selection found in chapter the insight that the fastest progress that a standard metropolis method can make in information terms is about one bit per iteration gives a strong motivation for speeding up the algorithm. this chapter has already reviewed several methods for reducing random-walk behaviour. do these methods also speed up the rate at which information is acquired? exercise does gibbs sampling which is a smart metropolis method whose proposal distributions do depend on p allow information about p to leak out at a rate faster than one bit per iteration? find toy examples in which this question can be precisely investigated. exercise hamiltonian monte carlo is another smart metropolis method in which the proposal distributions depend on p can hamiltonian monte carlo extract information about p at a rate faster than one bit per iteration? exercise in importance sampling the weight wr p a number is computed and retained until the end of the computation. in contrast in the dumb metropolis method the ratio a p is reduced to a single bit is a bigger than or smaller than the random number u? thus in principle importance sampling preserves more information about p than does dumb metropolis. can you a toy example in which this extra information does indeed lead to faster convergence of importance sampling than metropolis? can you design a markov chain monte carlo algorithm that moves around adaptively like a metropolis method and that retains more useful information about the value of p like importance sampling? in chapter we noticed that an evolving population of n individuals can make faster evolutionary progress if the individuals engage in sexual reproduction. this observation motivates looking at monte carlo algorithms in which multiple parameter vectors x are evolved and interact. multi-state methods in a multi-state method multiple parameter vectors x are maintained they evolve individually under moves such as metropolis and gibbs there are also interactions among the vectors. the intention is either that eventually all the vectors x should be samples from p illustrated by skilling s leapfrog method or that information associated with the vectors x should allow us to approximate expectations under p as in importance sampling. genetic methods genetic algorithms are not often described by their proponents as monte carlo algorithms but i think this is the correct categorization and an ideal genetic algorithm would be one that can be proved to be a valid monte carlo algorithm that converges to a density. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods i ll use r to denote the number of vectors in the population. we aim to have p three types. p a genetic algorithm involves moves of two or first individual moves in which one state vector is perturbed xr which could be performed using any of the monte carlo methods we have mentioned so far. second we allow crossover moves of the form x y in a typical crossover move the progeny receives half his state vector from one parent x and half from the other y the secret of success in a genetic algorithm is that the parameter x must be encoded in such a way that the crossover of two independent states x and y both of which have good p should have a reasonably good chance of producing progeny who are equally this constraint is a hard one to satisfy in many problems which is why genetic algorithms are mainly talked about and hyped up and rarely used by serious experts. having introduced a crossover move x y we need to choose an acceptance rule. one easy way to obtain a valid algorithm is to accept or reject the crossover proposal using the metropolis rule with p as the target density this involves comparing the before and after the crossover using the ratio p p if the crossover operator is reversible then we have an easy proof that this procedure detailed balance and so is a valid component in a chain converging to p exercise discuss whether the above two operators individual variation and crossover with the metropolis acceptance rule will give a more monte carlo method than a standard method with only one state vector and no crossover. the reason why the sexual community could acquire information faster than the asexual community in chapter was because the crossover operation produced diversity with standard deviation pg then the blind watchmaker was able to convey lots of information about the function by killing the less the above two operators do not a speed-up of pg compared with standard monte carlo methods because there is no killing. what s required in order to obtain a speed-up is two things multiplication and death and at least one of these must operate selectively. either we must kill the state vectors or we must allow the state vectors to give rise to more while it s easy to sketch these ideas it is hard to a valid method for doing it. exercise design a birth rule and a death rule such that the chain converges to p i believe this is still an open research problem. particle particle which are particularly popular in inference problems involving temporal tracking are multistate methods that mix the ideas of importance sampling and markov chain monte carlo. see isard and blake isard and blake berzuini et al. berzuini and gilks doucet et al. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. methods that do not necessarily help methods that do not necessarily help it is common practice to use many initial conditions for a particular markov chain if you are worried about sampling well from a complicated density p can you ensure the states produced by the simulations are well distributed about the typical set of p by ensuring that the initial points are well distributed about the whole state space the answer is unfortunately no. in hierarchical bayesian models for example a large number of parameters fxng may be coupled together via another parameter as a hyperparameter. for example the quantities fxng might be independent noise signals and might be the inverse-variance of the noise source. the joint distribution of and fxng might be p p p p j n n where and p is a broad distribution describing our ignorance about the noise level. for simplicity let s leave out all the other variables data and such that might be involved in a realistic problem. let s imagine that we want to sample from p by gibbs sampling alternately sampling from the conditional distribution p j xn then sampling all the xn from their conditional distributions p j resulting marginal distribution of should asymptotically be the broad distribution p if n is large then the conditional distribution of given any particular setting of fxng will be tightly concentrated on a particular most-probable value of with width proportional to progress up and down the will therefore take place by a slow random walk with steps of size so to the initialization strategy. can we our slow convergence problem by using initial conditions located all over the state space sadly if we distribute the points fxng widely what we are actually doing is no. favouring an initial value of the noise level that is large. the random walk of the parameter will thus tend after the drawing of from p j xn always to start from one end of the further reading the hamiltonian monte carlo method et al. is reviewed in neal this excellent tome also reviews a huge range of other monte carlo methods including the related topics of simulated annealing and free energy estimation. further exercises exercise an important detail of the hamiltonian monte carlo method is that the simulation of the hamiltonian dynamics while it may be inaccurate must be perfectly reversible in the sense that if the initial condition p goes to then the same simulator must take to and the inaccurate dynamics must conserve state-space volume. leapfrog method in algorithm these rules. explain why these rules must be and create an example illustrating the problems that arise if they are not. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo methods exercise a multi-state idea for slice sampling. investigate the following multi-state method for slice sampling. as in skilling s multi-state leapfrog method maintain a set of s state vectors fxsg. update one state vector xs by one-dimensional slice sampling in a direction y determined by picking two other state vectors xv and xw at random and setting y xv xw. investigate this method on toy problems such as a highly-correlated multivariate gaussian distribution. bear in mind that if s is smaller than the number of dimensions n then this method will not be ergodic by itself so it may need to be mixed with other methods. are there classes of problems that are better solved by this slice-sampling method than by the standard methods for picking y such as cycling through the coordinate axes or picking u at random from a gaussian distribution? solutions xv xs xw solution to exercise consider the spherical gaussian distribution where all components have mean zero and variance in one dimension the nth if n we obtain the proposed coordinate n leapfrogs over n n n n and assuming that in energy contributed by this one dimension will be n are gaussian random variables from n is gaussian from where the change n n n n n n so the typical change in energy is n this positive change is bad news. in n dimensions the typical change in energy when a leapfrog move is made at equilibrium is thus the probability of acceptance of the move scales as this implies that skilling s method as described is not in very highdimensional problems at least not once convergence has occurred. nevertheless it has the impressive advantage that its convergence properties are independent of the strength of correlations between the variables a property that not even the hamiltonian monte carlo and overrelaxation methods copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter some of the neural network models that we will encounter are related to ising models which are idealized magnetic systems. it is not essential to understand the statistical physics of ising models to understand these neural networks but i hope you ll them helpful. ising models are also related to several other topics in this book. we will use exact tree-based computation methods like those introduced in chapter to evaluate properties of interest in ising models. ising models crude models for binary images. and ising models relate to two-dimensional constrained channels chapter a two-dimensional bar-code in which a black dot may not be completely surrounded by black dots and a white dot may not be completely surrounded by white dots is similar to an antiferromagnetic ising model at low temperature. evaluating the entropy of this ising model is equivalent to evaluating the capacity of the constrained channel for conveying bits. if you would like to jog your memory on statistical physics and thermodynamics you might appendix b helpful. i also recommend the book by reif copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models an ising model is an array of spins atoms that can take states that are magnetically coupled to each other. if one spin is say in the state then it is energetically favourable for its immediate neighbours to be in the same state in the case of a ferromagnetic model and in the opposite state in the case of an antiferromagnet. in this chapter we discuss two computational techniques for studying ising models. let the state x of an ising model with n spins be a vector in which each component xn takes values or if two spins m and n are neighbours we write n n the coupling between neighbouring spins is j. we jmn j if m and n are neighbours and jmn otherwise. the energy of a state x is jmnxmxn hxn ex j h where h is the applied if j then the model is ferromagnetic and if j it is antiferromagnetic. we ve included the factor of because each pair is counted twice in the sum once as n and once as m. at equilibrium at temperature t the probability that the state is x is p j h j h j h where kb is boltzmann s constant and j h j h relevance of ising models ising models are relevant for three reasons. ising models are important as models of magnetic systems that have a phase transition. the theory of universality in statistical physics shows that all systems with the same dimension two and the same symmetries have equivalent critical properties i.e. the scaling laws shown by their phase transitions are identical. so by studying ising models we can out not only about magnetic phase transitions but also about phase transitions in many other systems. second if we generalize the energy function to ex j h jmnxmxn hnxn where the couplings jmn and applied hn are not constant we obtain a family of models known as spin glasses to physicists and as copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models networks or boltzmann machines to the neural network community. in some of these models all spins are declared to be neighbours of each other in which case physicists call the system an spin glass and networkers call it a fully connected network. third the ising model is also useful as a statistical model in its own right. in this chapter we will study ising models using two computational techniques. some remarkable relationships in statistical physics we would like to get as much information as possible out of our computations. consider for example the heat capacity of a system which is to be c z xx where ex to work out the heat capacity of a system we might naively guess that we have to increase the temperature and measure the energy change. heat capacity however is intimately related to energy at constant temperature. let s start from the partition function z the mean energy is obtained by with respect to ln z z xx a further spits out the variance of the energy ln z z xx vare but the heat capacity is also the derivative of with respect to temperature ln z ln z so for any system at temperature t c vare kbt vare thus if we can observe the variance of the energy of a system at equilibrium we can estimate its heat capacity. i this an almost paradoxical relationship. consider a system with a set of states and imagine heating it up. at high temperature all states will be equiprobable so the mean energy will be essentially constant and the heat capacity will be essentially zero. but on the other hand with all states being equiprobable there will certainly be in energy. so how can the heat capacity be related to the the answer is in the words essentially zero above. the heat capacity is not quite zero at high temperature it just tends to zero. and it tends to zero as vare kbt with copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models the quantity vare tending to a constant at high temperatures. this behaviour of the heat capacity of systems at high temperatures is thus very general. the factor can be viewed as an accident of history. perature scales had been using capacity would be if only temkbt then the of heat c vare and heat capacity and would be identical quantities. exercise will call the entropy of a physical system s rather than h while we are in a statistical physics chapter we set kb the entropy of a system whose states are x at temperature t is where show that s pxln px s ln where is the mean energy of the system. show that s where the free energy f ln z and kt ising models monte carlo simulation in this section we study two-dimensional planar ising models using a simple gibbs-sampling method. starting from some initial state a spin n is selected at random and the probability that it should be given the state of the other spins and the temperature is computed p bn where and bn is the local bn jxm h factor of appears in equation because the two spin states are rather than spin n is set to with that probability and otherwise to then the next spin to update is selected at random. after many iterations this procedure converges to the equilibrium distribution an alternative to the gibbs sampling formula is the metropolis algorithm in which we consider the change in energy that results from the chosen spin from its current state xn and adopt this change in with probability p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bbbb bbbb bbbb bbbb bbbb bbbb figure rectangular ising model. t figure sample states of rectangular ising models with j at a sequence of temperatures t ising models monte carlo simulation this procedure has roughly double the probability of accepting energetically unfavourable moves so may be a more sampler but at very low temperatures the relative merits of gibbs sampling and the metropolis algorithm may be subtle. rectangular geometry i simulated an ising model with the rectangular geometry shown in and with periodic boundary conditions. a line between two spins indicates that they are neighbours. i set the external h and considered the two cases j which are a ferromagnet and antiferromagnet respectively. i started at a large temperature and changed the temperature every i iterations decreasing it gradually to t then increasing it gradually back to a large temperature again. this procedure gives a crude check on whether equilibrium has been reached at each temperature if not we d expect to see some hysteresis in the graphs we plot. it also gives an idea of the reproducibility of the results if we assume that the two runs with decreasing and increasing temperature are independent of each other. at each temperature i recorded the mean energy per spin and the standard deviation of the energy and the mean square value of the magnetization m m n xn xn one tricky decision that has to be made is how soon to start taking these measurements after a new temperature has been established it is to detect equilibrium or even to give a clear of a system s being at equilibrium in chapter we will see a solution to this problem. my crude strategy was to let the number of iterations at each temperature i be a few hundred times the number of spins n and to discard the of those iterations. with n i found i needed more than iterations to reach equilibrium at any given temperature. results for small n with j i simulated an l l grid for l let s have a quick think about what results we expect. at low temperatures the system is expected to be in a ground state. the rectangular ising model with j has two ground states the all state and the all state. the energy per spin of either ground state is at high temperatures the spins are independent all states are equally probable and the energy is expected to around a mean of with a standard deviation proportional to let s look at some results. in all temperature t is shown with kb the basic picture emerges with as few as spins top the energy rises monotonically. as we increase the number of spins to bottom some new details emerge. first as expected the at large temperature decrease as second the at intermediate temperature become relatively bigger. this is the signature of a collective phenomenon in this case a phase transition. only systems with n show true phase transitions but with n we are getting a hint of the critical figure shows details of the graphs for n and n figure shows a sequence of typical states from the simulation of n spins at a sequence of decreasing temperatures. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models figure monte carlo simulations of rectangular ising models with j mean energy and in energy as a function of temperature mean square magnetization as a function of temperature in the top row n and the bottom n for even larger n see later t figure schematic diagram to explain the meaning of a schottky anomaly. the curve shows the heat capacity of two gases as a function of temperature. the lower curve shows a normal gas whose heat capacity is an increasing function of temperature. the upper curve has a small peak in the heat capacity which is known as a schottky anomaly least in cambridge. the peak is produced by the gas having magnetic degrees of freedom with a number of accessible states. n mean energy and mean square magnetization y g r e n e y g r e n e temperature temperature n o i t a z i t e n g a m e r a u q s n a e m n o i t a z i t e n g a m e r a u q s n a e m temperature temperature contrast with schottky anomaly a peak in the heat capacity as a function of temperature occurs in any system that has a number of energy levels a peak is not in itself evidence of a phase transition. such peaks were viewed as anomalies in classical thermodynamics since normal systems with numbers of energy levels as a particle in a box have heat capacities that are either constant or increasing functions of temperature. in contrast systems with a number of levels produced small blips in the heat capacity graph let us refresh our memory of the simplest such system a two-level system with states x and x the mean energy is and the derivative with respect to is so the heat capacity is c dedt de kbt kbt and the in energy are given by vare ckbt which was evaluated in the heat capacity and are plotted in the take-home message at this point is that whilst schottky anomalies do have a peak in the heat capacity there is no peak in their the variance of the energy simply increases monotonically with temperature to a value proportional to the number of independent spins. thus it is a peak in the that is interesting rather than a peak in the heat capacity. the ising model has such a peak in its as can be seen in the second row of rectangular ising model with j what do we expect to happen in the case j the ground states of an system are the two checkerboard patterns and they have copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models monte carlo simulation y g r e n e y g r e n e f o d s n o i t a z i t e n g a m e r a u q s n a e m y t i c a p a c t a e h figure detail of monte carlo simulations of rectangular ising models with j mean energy and in energy as a function of temperature. fluctuations in energy deviation. mean square magnetization. heat capacity. n n heat capacity vare figure schottky anomaly heat capacity and in energy as a function of temperature for a two-level system with separation and kb temperature copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models energy per spin like the ground states of the j model. can this analogy be pressed further? a moment s will that the two systems are equivalent to each other under a checkerboard symmetry operation. if you take an j system in some state and all the spins that lie on the black squares of an checkerboard and set j then the energy is unchanged. magnetization changes of course. so all thermodynamic properties of the two systems are expected to be identical in the case of zero applied but there is a subtlety lurking here. have you spotted it? we are simulating grids with periodic boundary conditions. if the size of the grid in any direction is odd then the checkerboard operation is no longer a symmetry operation relating j to j because the checkerboard doesn t match up at the boundaries. this means that for systems of odd size the ground state of a system with j will have degeneracy greater than and the energy of those ground states will not be as low as per spin. so we expect qualitative between the cases j in odd-sized systems. these are expected to be most prominent for small systems. the frustrations are introduced by the boundaries and the length of the boundary grows as the square root of the system size so the fractional of this boundary-related frustration on the energy and entropy of the system will decrease as figure compares the energies of the ferromagnetic and antiferromagnetic models with n here the is striking. figure the two ground states of a rectangular ising model with j j j figure two states of rectangular ising models with j that have identical energy. j y g r e n e j figure monte carlo simulations of rectangular ising models with j and n mean energy and in energy as a function of temperature. temperature temperature triangular ising model we can repeat these computations for a triangular ising model. do we expect the triangular ising model with j to show physical properties from the rectangular ising model? presumably the j model will have broadly similar properties to its rectangular counterpart. but the case j is radically from what s gone before. think about it there is no unfrustrated ground state in any state there must be frustrations pairs of neighbours who have the same sign as each other. unlike the case of the rectangular model with odd size the frustrations are not introduced by the periodic boundary conditions. every set of three mutually neighbouring spins must be in a state of frustration as shown in lines show happy couplings which contribute to the energy dashed lines show unhappy couplings which contribute jjj. thus we certainly expect behaviour at low temperatures. in fact we might expect this system to have a non-zero entropy at absolute zero. triangular model violates third law of thermodynamics! let s look at some results. sample states are shown in and shows the energy and heat capacity for n hh hh hh hh bbbb hh hh hh hh bbbb hh hh hh hh bbbb hh hh hh hh bbbb hh hh hh hh bbbb hh hh hh hh bbbb figure in an antiferromagnetic triangular ising model any three neighbouring spins are frustrated. of the eight possible of three spins six have energy and two have energy copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. direct computation of partition function of ising models note how the results for j are. there is no peak at all in the standard deviation of the energy in the case j this indicates that the antiferromagnetic system does not have a phase transition to a state with long-range order. y g r e n e y g r e n e f o d s y t i c a p a c t a e h j temperature temperature temperature y g r e n e y g r e n e f o d s y t i c a p a c t a e h j temperature temperature temperature direct computation of partition function of ising models we now examine a completely approach to ising models. the transfer matrix method is an exact and abstract approach that obtains physical properties of the model from the partition function j b j b usual let kb the free energy is given by f where the summation is over all states x and the inverse temperature is ln z. the number of states is so direct computation of the partition function is not possible for large n to avoid enumerating all global states explicitly we can use a trick similar to the sumproduct algorithm discussed in chapter we concentrate on models that have the form of a long thin strip of width w with periodic boundary conditions in both directions and we iterate along the length of our model working out a set of partial partition functions at one location l in terms of partial partition functions at the previous location l each iteration involves a summation over all the states at the boundary. this operation is exponential in the width of the strip w the clever trick figure monte carlo simulations of triangular ising models with j and n j j d mean energy and in energy as a function of temperature. e fluctuations in energy deviation. f heat capacity. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models t j t j figure sample states of triangular ising models with j and j high temperatures at the top low at the bottom. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. direct computation of partition function of ising models is to note that if the system is translation-invariant along its length then we need to do only one iteration in order to the properties of a system of any length. the computational task becomes the evaluation of an s s matrix where s is the number of microstates that need to be considered at the boundary and the computation of its eigenvalues. the eigenvalue of largest magnitude gives the partition function for an thin strip. here is a more detailed explanation. label the states of the c columns of the thin strip sc with each s an integer from to the rth bit of sc indicates whether the spin in row r column c is up or down. the partition function is z xx exp c esc where esc is an appropriately energy and if we want periodic boundary conditions is to be one for e is j xmxn j xmxn j xmxn esc this of the energy has the nice property that the rectangular ising model it a matrix that is symmetric in its two indices sc the factors of are needed because vertical links are counted four times. let us figure illustration to help explain the counts all the contributions to the energy in the rectangle. the total energy is given by stepping the rectangle along. each horizontal bond inside the rectangle is counted once each vertical bond is half-inside the rectangle will be half-inside an adjacent rectangle so half its energy is included in the factor of appears in the second term because m and n both run over all nodes in column c so each bond is visited twice. for the state shown here the horizontal bonds contribute to and the vertical bonds contribute on the left and on the right assuming periodic boundary conditions between top and bottom. so then continuing from equation c z trace xa a where z becomes dominated by the largest eigenvalue are the eigenvalues of m. as the length of the strip c increases z max so the free energy per spin in the limit of an thin strip is given by f ln zw c c ln c ln it s really neat that all the thermodynamic properties of a long thin strip can be obtained from just the largest eigenvalue of this matrix m! computations i computed the partition functions of long-thin-strip ising models with the geometries shown in as in the last section i set the applied h to zero and considered the two cases j which are a ferromagnet and antiferromagnet respectively. i computed the free energy per spin f j h fn for widths from w to as a function of for h copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. w bbbb bbbb bbbb bbbb bbbb computational ideas triangular bbbb bbbb bbbb w hh hh hh hh bbbb bbbb hh hh hh hh bbbb hh hh hh hh bbbb hh hh hh hh hh hh hh hh bbbb bbbb hh hh hh hh hh hh hh hh bbbb ising models hh hh hh hh bbbb hh hh hh hh bbbb hh hh hh hh bbbb hh hh hh hh bbbb hh hh hh hh bbbb bbbb rectangular bbbb bbbb bbbb figure two long-thin-strip ising models. a line between two spins indicates that they are neighbours. the strips have width w and length. only the largest eigenvalue is needed. there are several ways of getting this quantity for example iterative multiplication of the matrix by an initial vector. because the matrix is all positive we know that the principal eigenvector is all positive too theorem so a reasonable initial vector is this iterative procedure may be faster than explicit computation of all eigenvalues. i computed them all anyway which has the advantage that we can the free energy of length strips using equation as well as ones. ferromagnets of width triangular rectangular y g r e n e e e r f temperature antiferromagnets of width triangular rectangular figure free energy per spin of long-thin-strip ising models. note the non-zero gradient at t in the case of the triangular antiferromagnet. temperature comments on graphs for large temperatures all ising models should show the same behaviour the free energy is entropy-dominated and the entropy per spin is the mean energy per spin goes to zero. the free energy per spin should tend to the free energies are shown in one of the interesting properties we can obtain from the free energy is the degeneracy of the ground state. as the temperature goes to zero the boltzmann distribution becomes concentrated in the ground state. if the ground state is degenerate there are multiple ground states with identical y p o r t n e triangular- rectangular triangular temperature figure entropies nats of width ising systems as a function of temperature obtained by the free energy curves in the rectangular ferromagnet and antiferromagnet have identical thermal properties. for the triangular systems the upper curve denotes the antiferromagnet and the lower curve the ferromagnet. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. direct computation of partition function of ising models rectangular ferromagnet width width triangular- rectangular- triangular triangular ising models width width width width temperature temperature y t i c a p a c t a e h energy then the entropy as t is non-zero. we can the entropy from the free energy using s the entropy of the triangular antiferromagnet at absolute zero appears to be about that is about half its high temperature value the mean energy as a function of temperature is plotted in it is evaluated using the identity hei ln figure shows the estimated heat capacity raw derivatives of the mean energy as a function of temperature for the triangular models with widths and figure shows the in energy as a function of temperature. all of these should show smooth graphs the roughness of the curves is due to inaccurate numerics. the nature of any phase transition is not obvious but the graphs seem compatible with the assertion that the ferromagnet shows and the antiferromagnet does not show a phase transition. the pictures of the free energy in give some insight into how we could predict the transition temperature. we can see how the two phases of the ferromagnetic systems each have simple free energies a straight sloping line through f t for the high temperature phase and a horizontal line for the low temperature phase. slope of each line shows what the entropy per spin of that phase is. the phase transition occurs roughly at the intersection of these lines. so we predict the transition temperature to be linearly related to the ground state energy. e r a v rectangular ferromagnet width width temperature triangular ising models width width width width temperature figure mean energy versus temperature of long thin strip ising models with width compare with figure heat capacities of rectangular model triangular models with widths and denoting ferromagnet and antiferromagnet. compare with figure energy variances per spin of rectangular model triangular models with widths and denoting ferromagnet and antiferromagnet. compare with copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. ising models comparison with the monte carlo results the agreement between the results of the two experiments seems very good. the two systems simulated long thin strip and the periodic square are not quite identical. one could a more accurate comparison by all eigenvalues for the strip of width w and computingp to get the partition function of a w w patch. exercises exercise what would be the best way to extract the entropy from the monte carlo simulations? what would be the best way to obtain the entropy and the heat capacity from the partition function computation? exercise an ising model may be generalized to have a coupling jmn between any spins m and n and the value of jmn could be for each m and n. in the special case where all the couplings are positive we know that the system has two ground states the all-up and all-down states. for a more general setting of jmn it is conceivable that there could be many ground states. imagine that it is required to make a spin system whose local minima are a given list of states xs. can you think of a way of setting j such that the chosen states are low energy states? you are allowed to adjust all the fjmng to whatever values you wish. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact monte carlo sampling the problem with monte carlo methods for high-dimensional problems the most widely used random sampling methods are markov chain monte carlo methods like the metropolis method gibbs sampling and slice sampling. the problem with all these methods is this yes a given algorithm can be guaranteed to produce samples from the target density p asymptotically once the chain has converged to the equilibrium distribution but if one runs the chain for too short a time t then the samples will come from some other distribution p for how long must the markov chain be run before it has converged as was mentioned in chapter this question is usually very hard to answer. however the pioneering work of propp and wilson allows one for certain chains to answer this very question furthermore propp and wilson show how to obtain exact samples from the target density. exact sampling concepts propp and wilson s exact sampling method known as perfect simulation or coupling from the past depends on three ideas. coalescence of coupled markov chains first if several markov chains starting from initial conditions share a single random-number generator then their trajectories in state space may coalesce and having coalesced will not separate again. if all initial conditions lead to trajectories that coalesce into a single trajectory then we can be sure that the markov chain has forgotten its initial condition. figure shows twenty-one markov chains identical to the one described in section which samples from using the metropolis algorithm each of the chains has a initial condition but they are all driven by a single random number generator the chains coalesce after about steps. figure shows the same markov chains with a random number seed in this case coalescence does not occur until steps have elapsed shown. figure shows similar markov chains each of which has identical proposal density to those in section and but in the proposed move at each step left or right is obtained in the same way by all the chains at any timestep independent of the current state. this coupling of the chains changes the statistics of coalescence. because two neighbouring paths merge only when a rejection occurs and rejections occur only at the walls this particular markov chain coalescence will occur only when the chains are all in the leftmost state or all in the rightmost state. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact monte carlo sampling figure coalescence the idea behind the exact sampling method. time runs from bottom to top. in the leftmost panel coalescence occurred within steps. coalescence properties are obtained depending on the way each state uses the random numbers it is supplied with. two runs of a metropolis simulator in which the random bits that determine the proposed step depend on the current state a random number seed was used in each case. in this simulator the random proposal left or right is the same for all states. in each panel one of the paths the one starting at location x has been highlighted. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact sampling concepts coupling from the past how can we use the coalescence property to an exact sample from the equilibrium distribution of the chain? the state of the system at the moment when complete coalescence occurs is not a valid sample from the equilibrium distribution for example in coalescence always occurs when the state is against one of the two walls because trajectories merge only at the walls. so sampling forward in time until coalescence occurs is not a valid method. the second key idea of exact sampling is that we can obtain exact samples by sampling from a time in the past up to the present. if coalescence has occurred the present sample is an unbiased sample from the equilibrium distribution if not we restart the simulation from a time further into the past reusing the same random numbers. the simulation is repeated at a sequence of ever more distant times with a doubling of from one run to the next being a convenient choice. when coalescence occurs at a time before the present we can record as an exact sample from the equilibrium distribution of the markov chain. figure shows two exact samples produced in this way. in the leftmost panel of we start twenty-one chains in all possible initial conditions at and run them forward in time. coalescence does not occur. we restart the simulation from all possible initial conditions at and reset the random number generator in such a way that the random numbers generated at each time t particular from t to t will be identical to what they were in the run. notice that the trajectories produced from t to t by these runs that started from are identical to a subset of the trajectories in the simulation with coalescence still does not occur so we double again to this time all the trajectories coalesce and we obtain an exact sample shown by the arrow. if we pick an earlier time such as all the trajectories must still end in the same point at t since every trajectory must pass through some state at t and all those states lead to the same point. so if we ran the markov chain for an time in the past from any initial condition it would end in the same state. figure shows an exact sample produced in the same way with the markov chains of this method called coupling from the past is important because it allows us to obtain exact samples from the equilibrium distribution but as described here it is of little practical use since we are obliged to simulate chains starting in all initial states. in the examples shown there are only twenty-one states but in any realistic sampling problem there will be an utterly enormous number of states think of the states of a system of binary spins for example. the whole point of introducing monte carlo methods was to try to avoid having to visit all the states of such a system! monotonicity having established that we can obtain valid samples by simulating forward from times in the past starting in all possible states at those times the third trick of propp and wilson which makes the exact sampling method useful in practice is the idea that for some markov chains it may be possible to detect coalescence of all trajectories without simulating all those trajectories. this property holds for example in the chain of which has the property that two trajectories never cross. so if we simply track the two trajectories starting from the leftmost and rightmost states we will know that copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact monte carlo sampling figure coupling from the past the second idea behind the exact sampling method. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact sampling concepts figure ordering of states the third idea behind the exact sampling method. the trajectories shown here are the left-most and right-most trajectories of in order to establish what the state at time zero is we only need to run simulations from and after which point coalescence occurs. two more exact samples from the target density generated by this method and random number seeds. the initial times required were and respectively. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact monte carlo sampling compute ai jijxj draw u from if u xi else xi algorithm gibbs sampling coupling method. the markov chains are coupled together by having all chains update the same spin i at each time step and having all chains share a common sequence of random numbers u. figure an exact sample from the ising model at its critical temperature produced by d.b. wilson. such samples can be produced within seconds on an ordinary computer by exact sampling. coalescence of all trajectories has occurred when those two trajectories coalesce. figure illustrates this idea by showing only the left-most and right-most trajectories of figure shows two more exact samples from the same equilibrium distribution generated by running the coupling from the past method starting from the two end-states alone. in two runs coalesced starting from in it was necessary to try times up to to achieve coalescence. exact sampling from interesting distributions in the toy problem we studied the states could be put in a one-dimensional order such that no two trajectories crossed. the states of many interesting state spaces can also be put into a partial order and coupled markov chains can be found that respect this partial order. example of a partial order on the four possible states of two spins is this and and the states and are not ordered. for such systems we can show that coalescence has occurred merely by verifying that coalescence has occurred for all the histories whose initial states were maximal and minimal states of the state space. as an example consider the gibbs sampling method applied to a ferromagnetic ising spin system with the partial ordering of states being thus state x is greater than or equal to state y if xi yi for all spins i. the maximal and minimal states are the the all-up and all-down states. the markov chains are coupled together as shown in algorithm propp and wilson show that exact samples can be generated for this system although the time to exact samples is large if the ising model is below its critical temperature since the gibbs sampling method itself is slowly-mixing under these conditions. propp and wilson have improved on this method for the ising model by using a markov chain called the single-bond heat bath algorithm to sample from a related model called the random cluster model they show that exact samples from the random cluster model can be obtained rapidly and can be converted into exact samples from the ising model. their ground-breaking paper includes an exact sample from a ising model at its critical temperature. a sample for a smaller ising model is shown in a generalization of the exact sampling method for non-attractive distributions the method of propp and wilson for the ising model sketched above can be applied only to probability distributions that are as they call them attractive rather than this term let s say what it means for practical purposes the method can be applied to spin systems in which all the couplings are positive the ferromagnet and to a few special spin systems with negative couplings as we already observed in chapter the rectangular ferromagnet and antiferromagnet are equivalent but it cannot be applied to general spin systems in which some couplings are negative because in such systems the trajectories followed by the all-up and all-down states are not guaranteed to be upper and lower bounds for the set of all trajectories. fortunately however we do not need to be so strict. it is possible to re-express the propp and wilson algorithm in a way that generalizes to the case of spin systems with negative couplings. the idea of the summary state version of exact sampling is still that we keep track of bounds on the set of copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact sampling from interesting distributions all trajectories and detect when these bounds are equal so as to exact samples. but the bounds will not themselves be actual trajectories and they will not necessarily be tight bounds. instead of simulating two trajectories each of which moves in a state space we simulate one trajectory envelope in an augmented state space where the symbol denotes either or we call the state of this augmented system the summary state an example summary state of a six-spin system is this summary state is shorthand for the set of states the update rule at each step of the markov chain takes a single spin enumerates all possible states of the neighbouring spins that are compatible with the current summary state and for each of these local scenarios computes the new value or of the spin using gibbs sampling to a random number u as in algorithm if all these new values agree then the new value of the updated spin in the summary state is set to the unanimous value or otherwise the new value of the spin in the summary state is the initial condition at time is given by setting all the spins in the summary state to which corresponds to considering all possible start in the case of a spin system with positive couplings this summary state simulation will be identical to the simulation of the uppermost state and lowermost states in the style of propp and wilson with coalescence occuring when all the symbols have disappeared. the summary state method can be applied to general spin systems with any couplings. the only shortcoming of this method is that the envelope may describe an unnecessarily large set of states so there is no guarantee that the summary state algorithm will converge the time for coalescence to be detected may be considerably larger than the actual time taken for the underlying markov chain to coalesce. the summary state scheme has been applied to exact sampling in belief networks by harvey and neal and to the triangular antiferromagnetic ising model by childs et al. summary state methods were introduced by huber they also go by the names sandwiching methods and bounding chains. further reading for further reading impressive pictures of exact samples from other distributions and generalizations of the exact sampling method browse the perfectlyrandom sampling for beautiful exact-sampling demonstrations running live in your web browser see jim propp s other uses for coupling the idea of coupling together markov chains by having them share a random number generator has other applications beyond exact sampling. pinto and neal have shown that the accuracy of estimates obtained from a markov chain monte carlo simulation second problem discussed in section using the estimator t xt copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exact monte carlo sampling figure a perfectly random tiling of a hexagon by lozenges provided by j.g. propp and d.b. wilson. can be improved by coupling the chain of interest which converges to p to a second chain which generates samples from a second simpler distribution q. the coupling must be set up in such a way that the states of the two chains are strongly correlated. the idea is that we estimate the expectations of a function of interest under p and under q in the normal way and compare the estimate under q with the true value of the expectation under q which we assume can be evaluated exactly. if is an overestimate then it is likely that will be an overestimate too. the can thus be used to correct exercises exercise is there any relationship between the probability distribution of the time taken for all trajectories to coalesce and the equilibration time of a markov chain? prove that there is a relationship or a single chain that can be realized in two ways that have coalescence times. exercise imagine that fred ignores the requirement that the random bits used at some time t in every run from increasingly distant times must be identical and makes a coupled-markov-chain simulator that uses fresh random numbers every time is changed. describe what happens if fred applies his method to the markov chain that is intended to sample from the uniform distribution over the states and using the metropolis method driven by a random bit source as in exercise investigate the application of perfect sampling to linear regression in holmes and mallick or holmes and denison and try to generalize it. exercise the concept of coalescence has many applications. some surnames are more frequent than others and some die out altogether. make copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions a model of this process how long will it take until everyone has the same surname? similarly variability in any particular portion of the human genome forms the basis of forensic dna is inherited like a surname. a dna is like a string of surnames. should the fact that these surnames are subject to coalescences so that some surnames are by chance more prevalent than others the way in which dna evidence is used in court? exercise how can you use a coin to create a random ranking of people? construct a solution that uses exact sampling. for example you could apply exact sampling to a markov chain in which the coin is repeatedly used alternately to decide whether to switch and second then whether to switch second and third. exercise finding the partition function z of a probability distribution is a problem. many markov chain monte carlo methods produce valid samples from a distribution without ever out what z is. is there any probability distribution and markov chain such that either the time taken to produce a perfect sample or the number of random bits used to create a perfect sample are related to the value of z? are there some situations in which the time to coalescence conveys information about z? solutions solution to exercise it is perhaps surprising that there is no direct relationship between the equilibration time and the time to coalescence. we can prove this using the example of the uniform distribution over the integers a a markov chain that converges to this distribution in exactly one iteration is the chain for which the probability of state given st is the uniform distribution for all st. such a chain can be coupled to a random number generator in two ways we could draw a random integer u a and set equal to u regardless of st or we could draw a random integer u a and set equal to u mod method would produce a cohort of trajectories locked together similar to the trajectories in except that no coalescence ever occurs. thus while the equilibration times of methods and are both one the coalescence times are respectively one and it seems plausible on the other hand that coalescence time provides some sort of upper bound on equilibration time. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gibbs inequality appeared in equation see also exercise variational methods variational methods are an important technique for the approximation of complicated probability distributions having applications in statistical physics data modelling and neural networks. variational free energy minimization one method for approximating a complex distribution in a physical system is mean theory. mean theory is a special case of a general variational free energy approach of feynman and bogoliubov which we will now study. the key piece of mathematics needed to understand this method is gibbs inequality which we repeat here. the relative entropy between two probability distributions qx and p that are over the same alphabet ax is dklqjjp qx log qx p the relative entropy dklqjjp inequality with equality only if q p in general dklqjjp dklpjjq. in this chapter we will replace the log by ln and measure the divergence in nats. probability distributions in statistical physics in statistical physics one often encounters probability distributions of the form p j j j where for example the state vector is x and ex j is some energy function such as ex j jmnxmxn hnxn the partition function constant is j j the probability distribution of equation is complex. not unbearably complex we can after all evaluate ex j for any particular x in a time copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational free energy minimization polynomial in the number of spins. but evaluating the normalizing constant j is as we saw in chapter and describing the properties of the probability distribution is also hard. knowing the value of ex j at a few arbitrary points x for example gives no useful information about what the average properties of the system are. an evaluation of j would be particularly desirable because from z we can derive all the thermodynamic properties of the system. variational free energy minimization is a method for approximating the complex distribution p by a simpler ensemble qx that is parameterized by adjustable parameters we adjust these parameters so as to get q to best approximate p in some sense. a by-product of this approximation is a lower bound on j. the variational free energy the objective function chosen to measure the quality of the approximation is the variational free energy qx ln qx j this expression can be manipulated into a couple of interesting forms qx j hex jiq sq qx ln qx where hex jiq is the average of the energy function under the distribution qx and sq is the entropy of the distribution qx set kb to one in the of s so that it is identical to the of the entropy h in part i. second we can use the of p j to write xx qx ln qx p j ln j dklqjjp where f is the true free energy by ln j and dklqjjp is the relative entropy between the approximating distribution qx and the true distribution p j. thus by gibbs inequality the variational free energy is bounded below by f and attains this value only for qx p j. our strategy is thus to vary in such a way that is minimized. the approximating distribution then gives a approximation to the true distribution that may be useful and the value of will be an upper bound for equivalently can the objective function be evaluated? is a lower bound for z. we have already agreed that the evaluation of various interesting sums over x is intractable. for example the partition function z j copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational methods the energy and the entropy heip ex j j z xx s p j ln p j are all presumed to be impossible to evaluate. so why should we suppose that this objective function which is also in terms of a sum over all x should be a convenient quantity to deal with? well for a range of interesting energy functions and for simple approximating distributions the variational free energy can be evaluated. variational free energy minimization for spin systems an example of a tractable variational free energy is given by the spin system whose energy function was given in equation which we can approximate with a separable approximating distribution qx a zq exp xn anxn! the variational parameters of the variational free energy are the components of the vector a. to evaluate the variational free energy we need the entropy of this distribution sq qx a ln qx a and the mean of the energy hex jiq qx aex j the entropy of the separable approximating distribution is simply the sum of the entropies of the individual spins sq h where qn is the probability that spin n is qn ean ean and h q ln q q ln q the mean energy under q is easy to obtain because pmn jmnxmxn is a sum of terms each involving the product of two independent random variables. are no self-couplings so jmn when m n. if we the mean value of xn to be which is given by ean ean tanhan copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational free energy minimization for spin systems we obtain hex jiq xx qx jmn jmnxmxn hnxn hn so the variational free energy is given by hex jmn hn h we now consider minimizing this function with respect to the variational parameters a. if q the derivative of the entropy is h e ln q q so we obtain jmn qm qm jmn hm! am xn this derivative is equal to zero when am xn jmn hm! so is extremized at any point that equation and tanhan the variational free energy may be a multimodal function in which case each stationary point minimum or saddle will satisfy equations and one way of using these equations in the case of a system with an arbitrary coupling matrix j is to update each parameter am and the corresponding value of using equation one at a time. this asynchronous updating of the parameters is guaranteed to decrease equations and may be recognized as the mean equations for a spin system. the variational parameter an may be thought of as the strength of a applied to an isolated spin n. equation describes the mean response of spin n and equation describes how the am is set in response to the mean state of all the other spins. the variational free energy derivation is a helpful viewpoint for mean theory for two reasons. this approach associates an objective function with the mean equations such an objective function is useful because it can help identify alternative dynamical systems that minimize the same function. figure the variational free energy of the two-spin system whose energy is ex as a function of the two variational parameters and the inverse-temperature is the function plotted is where notice that for the function is convex with respect to and for it is convex with respect to copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. h h h variational methods figure solutions of the variational free energy extremization problem for the ising model for three applied h. horizontal axis temperature t vertical axis magnetization the critical temperature found by mean theory is t mft c the theory is readily generalized to other approximating distributions. we can imagine introducing a more complex approximation qx that might for example capture correlations among the spins instead of modelling the spins as independent. one could then evaluate the variational free energy and optimize the parameters of this more complex approximation. the more degrees of freedom the approximating distribution has the tighter the bound on the free energy becomes. however if the complexity of an approximation is increased the evaluation of either the mean energy or the entropy typically becomes more challenging. example mean theory for the ferromagnetic ising model in the simple ising model studied in chapter every coupling jmn is equal to j if m and n are neighbours and zero otherwise. there is an applied hn h that is the same for all spins. a very simple approximating distribution is one with just a single variational parameter a which a separable distribution qx a zq exp xn axn! in which all spins are independent and have the same probability qn of being up. the mean magnetization is tanha and the equation which the minimum of the variational free energy becomes a h where c is the number of couplings that a spin is involved in c in the case of a rectangular two-dimensional ising model. we can solve equations and for numerically in fact it is easiest to vary and solve for and obtain graphs of the free energy minima and maxima as a function of temperature as shown in the solid line shows versus t for the case c j when h there is a pitchfork bifurcation at a critical temperature t mft pitchfork bifurcation is a transition like the one shown by the solid lines in c copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational methods in inference and data modelling from a system with one minimum as a function of a the right to a system the left with two minima and one maximum the maximum is the middle one of the three lines. the solid lines look like a pitchfork. above this temperature there is only one minimum in the variational free energy at a and this minimum corresponds to an approximating distribution that is uniform over all states. below the critical temperature there are two minima corresponding to approximating distributions that are symmetry-broken with all spins more likely to be up or all spins more likely to be down. the state persists as a stationary point of the variational free energy but now it is a local maximum of the variational free energy. when h there is a global variational free energy minimum at any temperature for a positive value of shown by the upper dotted curves in as long as h jc there is also a second local minimum in the free energy if the temperature is small. this second minimum corresponds to a self-preserving state of magnetization in the opposite direction to the applied the temperature at which the second minimum appears is smaller than t mft and when it appears it is accompanied by a saddle point located between the two minima. a name given to this type of bifurcation is a saddle-node bifurcation. c the variational free energy per spin is given by c j h exercise sketch the variational free energy as a function of its one parameter for a variety of values of the temperature t and the applied h. figure reproduces the key properties of the real ising system that for h there is a critical temperature below which the system has longrange order and that it can adopt one of two macroscopic states. however by probing a little more we can reveal some inadequacies of the variational approximation. to start with the critical temperature t mft is which is nearly a factor of greater than the true critical temperature tc also the variational model has equivalent properties in any number of dimensions including d where the true system does not have a phase transition. so the bifurcation at t mft should not be described as a phase transition. c c for the case h we can follow the trajectory of the global minimum as a function of and the entropy heat capacity and of the approximating distribution and compare them with those of a real fragment using the matrix method of chapter as shown in one of the biggest is in the in energy. the real system has large near the critical temperature whereas the approximating distribution has no correlations among its spins and thus has an energy-variance which scales simply linearly with the number of spins. variational methods in inference and data modelling in statistical data modelling we are interested in the posterior probability distribution of a parameter vector w given data d and model assumptions h p j dh. p j dh p j whp jh p jh in traditional approaches to model a single parameter vector w is optimized to the mode of this distribution. what is really of interest is copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational methods free energy energy figure comparison of approximating distribution s properties with those of a real fragment. notice that the variational free energy of the approximating distribution is indeed an upper bound on the free energy of the real system. all quantities are shown per spin mean field theory real system mean field theory real system entropy mean field theory real system fluctuations vare mean field theory real system heat capacity dedt mean field theory real system copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the case of an unknown gaussian the whole distribution. we may also be interested in its normalizing constant p jh if we wish to do model comparison. the probability distribution p j dh is often a complex distribution. in a variational approach to inference we introduce an approximating probability distribution over the parameters qw and optimize this distribution varying its own parameters so that it approximates the posterior distribution of the parameters p j dh well. proximation is the variational free energy one objective function we may choose to measure the quality of the ap dkw qw ln qw p j whp jh the denominator p j whp jh is within a multiplicative constant the posterior probability p j dh p j whp jhp jh so the variational free energy can be viewed as the sum of ln p jh and the relative entropy between qw and p j dh. is bounded below by ln p jh and only attains this value for qw p j dh. for certain models and certain approximating distributions this free energy and its derivatives with respect to the approximating distribution s parameters can be evaluated. the approximation of posterior probability distributions using variational free energy minimization provides a useful approach to approximating bayesian inference in a number of ranging from neural networks to the decoding of error-correcting codes and van camp hinton and zemel dayan et al. neal and hinton mackay the method is sometimes called ensemble learning to contrast it with traditional learning processes in which a single parameter vector is optimized. another name for it is variational bayes. let us examine how ensemble learning works in the simple case of a gaussian distribution. the case of an unknown gaussian approximating the posterior distribution of and we will an approximating ensemble to the posterior distribution that we studied in chapter p jfxngn p j p n p we make the single assumption that the approximating ensemble is separable in the form no restrictions on the functional form of and are made. we write down a variational free energy ln p j we can the optimal separable distribution q by considering separately the optimization of over for and then the optimization of for copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational methods figure optimization of an approximating distribution. the posterior distribution p jfxng which is the same as that in is shown by solid contours. initial condition. the approximating distribution contours is an arbitrary separable distribution. has been updated using equation has been updated using equation updated again. updated again. converged approximation iterations. the arrows point to the peaks of the two distributions which are at p and q. optimization of as a functional of is ln p j lnp z ln where and denote constants that do not depend on the dependence on thus collapses down to a simple dependence on the mean now we can recognize the function as the logarithm of a gaussian identical to the posterior distribution for a particular value of since a relative entropy r q lnqp is minimized by setting q p we can immediately write down the distribution qopt that minimizes for p d qopt where optimization of we represent using the density over as a functional of is additive constants ln p j lnp z ln ln n the prior p transforms to p copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. interlude where the integral over is performed assuming qopt here the expression in square brackets can be recognized as the logarithm of a gamma distribution over see equation giving as the distribution that minimizes for with qopt s and n in these two update rules are applied alternately starting from an arbitrary initial condition. the algorithm converges to the optimal approximating ensemble in a few iterations. direct solution for the joint optimum in this problem we do not need to resort to iterative computation to the optimal approximating ensemble. equations and the optimum implicitly. we must simultaneously have the solution is and sn this is similar to the true posterior distribution of which is a gamma distribution with and equation this true posterior also has a mean value of satisfying sn the only is that the approximating distribution s parameter is too large by the approximations given by variational free energy minimization always tend to be more compact than the true distribution. in conclusion ensemble learning gives an approximation to the posterior that agrees nicely with the conventional estimators. the approximate posterior distribution over is a gamma distribution with mean corresponding to a variance of sn and the approximate posterior distribution over is a gaussian with mean and standard deviation the variational free energy minimization approach has the nice property that it is parameterization-independent it avoids the problem of basisdependence from which map methods and laplace s method a convenient software package for automatic implementation of variational inference in graphical models is vibes et al. it plays the same role for variational inference as bugs plays for monte carlo inference. interlude one of my students asked how do you ever come up with a useful approximating distribution given that the true distribution is so complex you can t compute it directly? let s answer this question in the context of bayesian data modelling. let the true distribution of interest be the posterior probability distribution over a set of parameters x p d. a standard data modelling practice is to a single setting of the parameters for example by the maximum of the likelihood function p j x or of the posterior distribution. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational methods one interpretation of this standard practice is that the full description of our knowledge about x p d is being approximated by a delta-function a probability distribution concentrated on from this perspective any approximating distribution qx no matter how crummy it is has to be an improvement on the spike produced by the standard method! so even if we use only a simple gaussian approximation we are doing well. we now study an application of the variational approach to a realistic example data clustering. k-means clustering and the expectationmaximization algo rithm as a variational method in chapter we introduced the soft k-means clustering algorithm version in chapter we introduced versions and of this algorithm and motivated the algorithm as a maximum likelihood algorithm. k-means clustering is an example of an expectationmaximization algorithm with the two steps which we called assignment and update being known as the e-step and the m-step respectively. we now give a more general view of k-means clustering due to neal and hinton in which the algorithm is shown to optimize a variational objective function. neal and hinton s derivation applies to any em algorithm. the probability of everything let the parameters of the mixture model the means standard deviations and weights be denoted by for each data point there is a missing variable known as a latent variable the class label kn for that point. the probability of everything given our assumed model h is p kngn jh p jh n j kn j the posterior probability of everything given the data is proportional to the probability of everything p jfxngn p kngn p jh jh we now approximate this posterior distribution by a separable distribution and a variational free energy in the usual way qkfkngn q q ln q qkfkngn p kngn jh q xfkngz qkfkngn is bounded below by minus the evidence ln p jh. we can now make an iterative algorithm with an assignment step and an update step. in the assignment step qkfkngn in the update step q is adjusted to reduce for q is adjusted to reduce for qk. if we wish to obtain exactly the soft k-means algorithm we impose a is constrained to be further constraint on our approximating distribution q a delta function centred on a point estimate of q copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational methods other than free energy minimization upper bound lower bound h e where figure illustration of the jaakkolajordan variational method. upper and lower bounds on the logistic function line ga these upper and lower bounds are exponential or gaussian functions of a and so easier to integrate over. the graph shows the sigmoid function and upper and lower bounds with and large integral r q unfortunately this distribution contributes to the variational free energy an so we d better leave that term out of treating it as an additive constant. a delta function q is not a good idea if our aim is to minimize moving on our aim is to derive the soft k-means algorithm. ln q exercise show that given q the optimal qk in the sense of minimizing is a separable distribution in which the probability that kn k is given by the responsibility rn k exercise show that given a separable qk as described above the optimal in the sense of minimizing is obtained by the update step of the soft k-means algorithm. a uniform prior on exercise we can instantly improve on the large value of achieved by soft k-means clustering by allowing q to be a more general distribution than a delta-function. derive an update step in which q is allowed to be a separable distribution a product of and discuss whether this generalized algorithm still from soft k-means s kaboom problem where the algorithm glues an evershrinking gaussian to one data point. sadly while it sounds like a promising generalization of the algorithm to be a non-delta-function and the kaboom problem goes to allow q away other artefacts can arise in this approximate inference method involving local minima of for further reading see mackay variational methods other than free energy minimization there are other strategies for approximating a complicated distribution p in addition to those based on minimizing the relative entropy between an approximating distribution q and p one approach pioneered by jaakkola and jordan is to create adjustable upper and lower bounds qu and ql to p as illustrated in these bounds are unnormalized densities are parameterized by variational parameters which are adjusted in order to obtain the tightest possible the lower bound can be adjusted to maximize qlx xx and the upper bound can be adjusted to minimize qu xx copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. variational methods using the normalized versions of the optimized bounds we then compute approximations to the predictive distributions. further reading on such methods can be found in the references and jordan jaakkola and jordan jaakkola and jordan gibbs and mackay further reading the bethe and kikuchi free energies in chapter we discussed the sumproduct algorithm for functions of the factor-graph form if the factor graph is tree-like the sumproduct algorithm converges and correctly computes the marginal function of any variable xn and can also yield the joint marginal function of subsets of variables that appear in a common factor such as xm. the sumproduct algorithm may also be applied to factor graphs that are not tree-like. if the algorithm converges to a point it has been shown that that point is a stationary point a minimum of a function of the messages called the kikuchi free energy. in the special case where all factors in factor graph are functions of one or two variables the kikuchi free energy is called the bethe free energy. for articles on this idea and new approximate inference algorithms motivated by it see yedidia yedidia et al. welling and teh yuille yedidia et al. yedidia et al. further exercises exercise this exercise explores the assertion made above that the approximations given by variational free energy minimization always tend to be more compact than the true distribution. consider a two dimensional gaussian distribution p with axes aligned with the directions and let the variances in these two directions be what is the optimal variance if this distribution is approximated by a spherical gaussian with variance q optimized by variational free energy minimization? if we instead optimized the objective function and p qx g dx p ln what would be the optimal value of sketch a contour of the true distribution p and the two approximating distributions in the case that in general it is not possible to evaluate the objective function g because integrals under the true distribution p are usually intractable. exercise what do you think of the idea of using a variational method to optimize an approximating distribution q which we then use as a proposal density for importance sampling? exercise the relative entropy or kullbackleibler divergence between two probability distributions p and q and state gibbs inequality. consider the problem of approximating a joint distribution p y by a separable distribution qx y qxxqy show that if the objec copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions tive function for this approximation is gqx qy p y p y qx that the minimal value of g is achieved when qx and qy are equal to the marginal distributions over x and y. now consider the alternative objective function f qy qxxqy qxxqy p y the probability distribution p y shown in the margin is to be approximated by a separable distribution qx y qxxqy state the value of f qy if qx and qy are set to the marginal distributions over x and y. show that f qy has three distinct minima identify those minima and evaluate f at each of them. solutions solution to exercise we need to know the relative entropy between two one-dimensional gaussian distributions p y x y z dx normalx ln z dx normalx p! ln p q q normalx normalx q p! so if we approximate p whose variances are are both q we and by q whose variances f q ln q q ln q q which is zero when q d d q f q q thus we set the approximating distribution s inverse variance to the mean inverse variance of the target distribution p in the case and we obtain which is just a factor of larger than pretty much independent of the value of the larger standard deviation variational free energy minimization typically leads to approximating distributions whose length scales match the shortest length scale of the target distribution. the approximating distribution might be viewed as too compact. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. in contrast if we use the objective function g then we q ln q q ln q q! constant where the constant depends on and only. d d ln q g q q! variational methods figure two separable gaussian approximations lines to a bivariate gaussian distribution line. the approximation that minimizes the variational free energy. the approximation that minimizes the objective function g. in each the lines show the contours at which xtax where a is the inverse covariance matrix of the gaussian. which is zero when q thus we set the approximating distribution s variance to the mean variance of the target distribution p factor of smaller than independent of the value of the two approximations are shown to scale in in the case and we obtain which is just a solution to exercise the best possible variational approximation is of course the target distribution p assuming that this is not possible a good variational approximation is more compact than the true distribution. in contrast a good sampler is more heavy tailed than the true distribution. an over-compact distribution would be a lousy sampler with a large variance. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. sk g yn figure error-correcting codes as latent variable models. the k latent variables are the independent source bits sk these give rise to the observables via the generator matrix g. independent component analysis and latent variable modelling latent variable models many statistical models are generative models is models that specify a full probability density over all variables in the situation that make use of latent variables to describe a probability distribution over observables. examples of latent variable models include chapter s mixture models which model the observables as coming from a superposed mixture of simple probability distributions latent variables are the unknown class labels of the examples hidden markov models and juang durbin et al. and factor analysis. the decoding problem for error-correcting codes can also be viewed in in that case the encoding terms of a latent variable model matrix g is normally known in advance. in latent variable modelling the parameters equivalent to g are usually not known and must be inferred from the data along with the latent variables s. usually the latent variables have a simple distribution often a separable distribution. thus when we a latent variable model we are a description of the data in terms of independent components the independent component analysis algorithm corresponds to perhaps the simplest possible latent variable model with continuous latent variables. the generative model for independent component analysis a set of n observations d fxngn are assumed to be generated as follows. each j-dimensional vector x is a linear mixture of i underlying source signals s x gs where the matrix of mixing g is not known. the simplest algorithm results if we assume that the number of sources is equal to the number of observations i.e. i j. our aim is to recover the source variables s some multiplicative factors and possibly permuted. to put it another way we aim to create the inverse of g a post-multiplicative factor given only a set of examples fxg. we assume that the latent variables are independently distributed with marginal distributions p jh pisi. here h denotes the assumed form of this model and the assumed probability distributions pi of the latent variables. the probability of the observables and the hidden variables given g and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. independent component analysis and latent variable modelling h is p sngn j gh n j sn ghp jhi i a yi j gjisn pisn i n we assume that the vector x is generated without noise. this assumption is not usually made in latent variable modelling since noise-free data are rare but it makes the inference problem far simpler to solve. the likelihood function for learning about g from the data d the relevant quantity is the likelihood function p j gh p j gh which is a product of factors each of which is obtained by marginalizing over the latent variables. when we marginalize over delta functions remember v f we adopt summation convention at this a single factor in the that r ds vsf point such that for example gjisn likelihood is given by i pi gjisn i p j gh z disn p j sn ghp jh z disn yj jdet gjyi j gjisn i pisn i ij xj ln p j gh lnjdet gj ln ij xj to obtain a maximum likelihood algorithm we the gradient of the log if we introduce w the log likelihood contributed by a likelihood. single example may be written ln p j gh lnjdet wj ln piwijxj we ll assume from now on that det w is positive so that we can omit the absolute value sign. we will need the following identities ln det g ij wij lj lm f im gli let us ai wijxj d ln piaidai copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the generative model for independent component analysis algorithm independent component analysis online steepest ascents version. see also algorithm which is to be preferred. repeat for each datapoint x put x through a linear mapping a wx put a through a nonlinear map zi where a popular choice for is tanhai. adjust the weights in accordance with zxt and zi which indicates in which direction ai needs to change to make the probability of the data greater. we may then obtain the gradient with respect to gji using equations and ln p j gh or alternatively the derivative with respect to wij ln p j gh gji xjzi if we choose to change w so as to ascend this gradient we obtain the learning rule zxt the algorithm so far is summarized in algorithm choices of the choice of the function the assumed prior distribution of the latent variable s. let s consider the linear choice which implicitly equation assumes a gaussian distribution on the latent variables. the gaussian distribution on the latent variables is invariant under rotation of the latent variables so there can be no evidence favouring any particular alignment of the latent variable space. the linear algorithm is thus uninteresting in that it will never recover the matrix g or the original sources. our only hope is thus that the sources are non-gaussian. thankfully most real sources have non-gaussian distributions often they have heavier tails than gaussians. we thus move on to the popular tanh nonlinearity. if then implicitly we are assuming tanhai pisi coshsi esi this is a heavier-tailed distribution for the latent variables than the gaussian distribution. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. independent component analysis and latent variable modelling figure illustration of the generative models implicit in the learning algorithm. distributions over two observables generated by cosh distributions on the latent distribution and variables for g g distribution. contours of the generative distributions when the latent variables have cauchy distributions. the learning algorithm this amoeboid object to the empirical data in such a way as to maximize the likelihood. the contour plot in does not adequately represent this heavy-tailed distribution. part of the tails of the cauchy distribution giving the contours times the density at the origin. some data from one of the generative distributions illustrated in and can you tell which? samples were created of which fell in the plotted region. we could also use a tanh nonlinearity with gain that is whose implicit probabilistic model is pisi in the limit of large the nonlinearity becomes a step function and the probability distribution pisi becomes a biexponential distribution pisi in the limit pisi approaches a gaussian with mean zero and variance heavier-tailed distributions than these may also be used. the student and cauchy distributions spring to mind. example distributions figures illustrate typical distributions generated by the independent components model when the components have cosh and cauchy distributions. figure shows some samples from the cauchy model. the cauchy distribution being the more heavy-tailed gives the clearest picture of how the predictive distribution depends on the assumed generative parameters g. a covariant simpler and faster learning algorithm we have thus derived a learning algorithm that performs steepest descents on the likelihood function. the algorithm does not work very quickly even on toy data the algorithm is ill-conditioned and illustrates nicely the general advice that while the gradient of an objective function is a splendid idea ascending the gradient directly may not be. the fact that the algorithm is ill-conditioned can be seen in the fact that it involves a matrix inverse which can be arbitrarily large or even covariant optimization in general the principle of covariance says that a consistent algorithm should give the same results independent of the units in which quantities are measured copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a covariant simpler and faster learning algorithm here n is the number of iterations. a prime example of a non-covariant algorithm is the popular steepest descents rule. a dimensionless objective function lw is its derivative with respect to some parameters w is computed and then w is changed by the rule this popular equation is dimensionally inconsistent the left-hand side of this equation has dimensions of and the right-hand side has dimensions the behaviour of the learning algorithm is not covariant with respect to linear rescaling of the vector w. dimensional inconsistency is not the end of the world as the success of numerous gradient descent algorithms has demonstrated and indeed if decreases with n on-line learning as then the munrorobbins theorem p. shows that the parameters will asymptotically converge to the maximum likelihood parameters. but the non-covariant algorithm may take a very large number of iterations to achieve this convergence indeed many former users of steepest descents algorithms prefer to use algorithms such as conjugate gradients that adaptively out the curvature of the objective function. the defense of equation that points out could be a dimensional constant is untenable if not all the parameters wi have the same dimensions. the algorithm would be covariant if it had the form where m is a matrix whose i element has dimensions from where can we obtain such a matrix? two sources of such matrices are metrics and curvatures. metrics and curvatures if there is a natural metric that distances in our parameter space w then a matrix m can be obtained from the metric. there is often a natural choice. in the special case where there is a known quadratic metric the length of a vector w then the matrix can be obtained from the quadratic form. for example if the length is then the natural matrix is m i and steepest descents is appropriate. another way of a metric is to look at the curvature of the objective function a r then the matrix m will give a covariant algorithm what is more this algorithm is the newton algorithm so we recognize that it will alleviate one of the principal with steepest descents namely its slow convergence to a minimum when the objective function is at all ill-conditioned. the newton algorithm converges to the minimum in a single step if l is quadratic. in some problems it may be that the curvature a consists of both datadependent terms and data-independent terms in this case one might choose to the metric using the data-independent terms only the resulting algorithm will still be covariant but it will not implement an exact newton step. obviously there are many covariant algorithms there is no unique choice. but covariant algorithms are a small subset of the set of all algorithms! copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. independent component analysis and latent variable modelling back to independent component analysis for the present maximum likelihood problem we have evaluated the gradient with respect to g and the gradient with respect to w steepest ascents in w is not covariant. let us construct an alternative covariant algorithm with the help of the curvature of the log likelihood. taking the second derivative of the log likelihood with respect to w we obtain two terms the of which is data-independent and the second of which is data-dependent sum over i where is the derivative of z. it is tempting to drop the data-dependent term and the matrix m by however this matrix is not positive has at least one non-positive eigenvalue so it is a poor approximation to the curvature of the log likelihood which must be positive in the neighbourhood of a maximum likelihood solution. we must therefore consult the data-dependent term for inspiration. the aim is to a convenient approximation to the curvature and to obtain a covariant algorithm not necessarily to implement an exact newton step. what is the average value of if the true value of g is then we now make several severe approximations we replace by the present value of g and replace the correlated average by here is the variancecovariance matrix of the latent variables is assumed to exist and di is the typical value of the curvature ln given that the sources are assumed to be independent and d are both diagonal matrices. these approximations motivate the matrix m given by that is mijkl i for simplicity we further assume that the sources are similar to each other so that and d are both homogeneous and that this will lead us to an algorithm that is covariant with respect to linear rescaling of the data x but not with respect to linear rescaling of the latent variables. we thus use mijkl multiplying this matrix by the gradient in equation we obtain the following covariant learning algorithm notice that this expression does not require any inversion of the matrix w. the only additional computation once z has been computed is a single backward pass through the weights to compute the quantity copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a covariant simpler and faster learning algorithm repeat for each datapoint x put x through a linear mapping a wx put a through a nonlinear map zi where a popular choice for is tanhai. put a back through w wta adjust the weights in accordance with w in terms of which the covariant algorithm reads algorithm independent component analysis covariant version. the quantity on the right-hand side is sometimes called the natural gradient. the covariant independent component analysis algorithm is summarized in algorithm further reading ica was originally derived using an information maximization approach and sejnowski another view of ica in terms of energy functions which motivates more general models is given by hinton et al. another generalization of ica can be found in pearlmutter and parra there is now an enormous literature on applications of ica. a variational free energy minimization approach to ica-like models is given in miskin and mackay miskin and mackay further reading on blind separation including non-ica algorithms can be found in and herault comon et al. hendin et al. amari et al. hojen-sorensen et al. models while latent variable models with a number of latent variables are widely used it is often the case that our beliefs about the situation would be most accurately captured by a very large number of latent variables. consider clustering for example. if we attack speech recognition by modelling words using a cluster model how many clusters should we use? the number of possible words is unbounded so we would really like to use a model in which it s always possible for new clusters to arise. furthermore if we do a careful job of modelling the cluster corresponding to just one english word we will probably that the cluster for one word should itself be modelled as composed of clusters indeed a hierarchy of copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. independent component analysis and latent variable modelling clusters within clusters. the levels of the hierarchy would divide male speakers from female and would separate speakers from regions india britain europe and so forth. within each of those clusters would be subclusters for the accents within each region. the subclusters could have subsubclusters right down to the level of villages streets or families. thus we would often like to have numbers of clusters in some cases the clusters would have a hierarchical structure and in other cases the hierarchy would be so how should such models be implemented in computers? and how should we set up our bayesian models so as to avoid getting silly answers? mixture models for categorical data are presented in neal along with a monte carlo method for simulating inferences and predictions. gaussian mixture models with a hierarchical structure are presented in rasmussen neal shows how to use dirichlet trees to models of hierarchical clusters. most of these ideas build on the dirichlet process this remains an active research area and ghahramani beal et al. exercises exercise repeat the derivation of the algorithm but assume a small amount of noise in x x gs n so the term in the joint probability is replaced by a probability distribution over xn show that if this noise distribution has small standard deviation the identical algorithm results. i j gjisn j with meanpi gjisn i exercise implement the covariant ica algorithm and apply it to toy data. exercise create algorithms appropriate for the situations x includes substantial gaussian noise more measurements than latent variables i fewer measurements than latent variables i. factor analysis assumes that the observations x can be described in terms of independent latent variables fskg and independent additive noise. thus the observable x is given by x gs n where n is a noise vector whose components have a separable probability distribution. in factor analysis it is often assumed that the probability distributions of fskg and fnig are zero-mean gaussians the noise terms may have variances i exercise make a maximum likelihood algorithm for inferring g from data assuming the generative model x gs n is correct and that s and n have independent gaussian distributions. include parameters j to describe the variance of each nj and maximize the likelihood with respect to them too. let the variance of each si be exercise implement the gaussian mixture model of rasmussen copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. random inference topics what do you know if you are ignorant? example a real variable x is measured in an accurate experiment. for example x might be the half-life of the neutron the wavelength of light emitted by a the depth of lake vostok or the mass of jupiter s moon io. what is the probability that the value of x starts with a like the charge of the electron s.i. units e c and the boltzmann constant k j and what is the probability that it starts with a like the faraday constant f c what about the second digit? what is the probability that the mantissa of x starts and what is the probability that x starts solution. an expert on neutrons antarctica or jove might be able to predict the value of x and thus predict the digit with some but what about someone with no knowledge of the topic? what is the probability distribution corresponding to knowing nothing one way to attack this question is to notice that the units of x have not been if the half-life of the neutron were measured in fortnights instead of seconds the number x would be divided by if it were measured in years it would be divided by now is our knowledge about x and in particular our knowledge of its digit by the change in units? for the expert the answer is yes but let us take someone truly ignorant for whom the answer is no their predictions about the digit of x are independent of the units. the arbitrariness of the units corresponds to invariance of the probability distribution when x is multiplied by any number. metres feet inches figure when viewed on a logarithmic scale scales using units are translated relative to each other. if you don t know the units that a quantity is measured in the probability of the digit must be proportional to the length of the corresponding piece of logarithmic scale. the probability that the digit of a number is is thus log log log log log log copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. random inference topics p p now so without needing a calculator we have log log and more generally the probability that the digit is d is logdlog log this observation about initial digits is known as benford s law. does not correspond to a uniform probability distribution over d. ignorance exercise a pin is thrown tumbling in the air. what is the probability distribution of the angle between the pin and the vertical at a moment while it is in the air? the tumbling pin is photographed. what is the probability distribution of the angle between the pin and the vertical as imaged in the photograph? exercise record breaking. consider keeping track of the world record for some quantity x say earthquake magnitude or longjump distances jumped at world championships. if we assume that attempts to break the record take place at a steady rate and if we assume that the underlying probability distribution of the outcome x p is not changing an assumption that i think is unlikely to be true in the case of sports endeavours but an interesting assumption to consider nonetheless and assuming no knowledge at all about p what can be predicted about successive intervals between the dates when records are broken? the distribution exercise in their landmark paper demonstrating that bacteria could mutate from virus sensitivity to virus resistance luria and wanted to estimate the mutation rate in an exponentially-growing population from the total number of mutants found at the end of the experiment. this problem is because the quantity measured number of mutated bacteria has a heavy-tailed probability distribution a mutation occuring early in the experiment can give rise to a huge number of mutants. unfortunately luria and didn t know bayes theorem and their way of coping with the heavy-tailed distribution involves arbitrary hacks leading to two estimators of the mutation rate. one of these estimators on the mean number of mutated bacteria averaging over several experiments has appallingly large variance yet sampling theorists continue to use it and base intervals around it and oprea in this exercise you ll do the inference right. in each culture a single bacterium that is not resistant gives rise after g generations to n descendants all clones except for arising from mutations. the culture is then exposed to a virus and the number of resistant bacteria n is measured. according to the now accepted mutation hypothesis these resistant bacteria got their resistance from random mutations that took place during the growth of the colony. the mutation rate cell per generation a is about one in a hundred million. the total number of n if a bacterium mutates at the ith generation its descendants all inherit the mutation and the number of resistant bacteria contributed by that one ancestor is opportunities to mutate is n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. inferring causation given m separate experiments in each of which a colony of size n is created and where the measured numbers of resistant bacteria are fnmgm what can we infer about the mutation rate a? make the inference given the following dataset from luria and for n fnmg small amount of computation is required to solve this problem. inferring causation exercise in the bayesian graphical model community the task of inferring which way the arrows point that is which nodes are parents and which children is one on which much has been written. inferring causation is tricky because of likelihood equivalence two graphical models are likelihood-equivalent if for any setting of the parameters of either there exists a setting of the parameters of the other such that the two joint probability distributions of all observables are identical. an example of a pair of likelihood-equivalent models are a b and b a. the model a b asserts that a is the parent of b or in very sloppy terminology a causes b an example of a situation where b a is true is the case where b is the variable burglar in house and a is the variable alarm is ringing here it is literally true that b causes a. but this choice of words is confusing if applied to another example r d where r denotes it rained this morning and d denotes the pavement is dry r causes d is confusing. i ll therefore use the words b is a parent of a to denote causation. some statistical methods that use the likelihood alone are unable to use data to distinguish between likelihood-equivalent models. in a bayesian approach on the other hand two likelihood-equivalent models may nevertheless be somewhat distinguished in the light of data since likelihood-equivalence does not force a bayesian to use priors that assign equivalent densities over the two parameter spaces of the models. however many bayesian graphical modelling folks perhaps out of sympathy for their non-bayesian colleagues or from a latent urge not to appear from them deliberately discard this potential advantage of bayesian methods the ability to infer causation from data by skewing their models so that the ability goes away a widespread orthodoxy holds that one should identify the choices of prior for which prior equivalence holds i.e. the priors such that models that are likelihood-equivalent also have identical posterior probabilities and then one should use one of those priors in inference and prediction. this argument motivates the use as the prior over all probability vectors of specially-constructed dirichlet distributions. in my view it is a philosophical error to use only those priors such that causation cannot be inferred. priors should be set to describe one s assumptions when this is done it s likely that interesting inferences about causation can be made from data. in this exercise you ll make an example of such an inference. consider the toy problem where a and b are binary variables. the two models are ha!b and hb!a. ha!b asserts that the marginal probability of a comes from a beta distribution with parameters i.e. the uniform distribution and that the two conditional distributions p a and p a also come independently from beta distributions with parameters the other model assigns similar priors to the marginal probability of b and the conditional distributions of a given b. data are gathered and the copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. random inference topics counts given f outcomes are b b a a what are the posterior probabilities of the two hypotheses? it s a good idea to work this exercise out symbolically in order to spot hint all the that emerge. d dx ln lnx the topic of inferring causation is a complex one. the fact that bayesian inference can sensibly be used to infer the directions of arrows in graphs seems to be a neglected view but it is certainly not the whole story. see pearl for discussion of many other aspects of causality. further exercises exercise photons arriving at a photon detector are believed to be emit ted as a poisson process with a time-varying rate expa b sin!t where the parameters a b and are known. data are collected during the time t t given that n photons arrived at times ftngn reading gregory and discuss the inference of a b and loredo exercise a data consisting of two columns of numbers has been printed in such a way that the boundaries between the columns are unclear. here are the resulting strings. discuss how probable it is given these data that the correct parsing of each item is etc. etc. parsing of a string is a grammatical interpretation of the string. for example punch bores could be parsed as punch bores or punch verb bores noun exercise in an experiment the measured quantities fxng come inde pendently from a biexponential distribution with mean p z copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions where z is the normalizing constant z the mean is not known. an example of this distribution with is shown in assuming the four datapoints are figure the biexponential distribution p fxng what do these data tell us about include detailed sketches in your answer. give a range of plausible values of solutions solution to exercise a population of size n has n opportunities to mutate. the probability of the number of mutations that occurred r is roughly poisson p j a n r! is slightly inaccurate because the descendants of a mutant cannot themselves undergo the same mutation. each mutation gives rise to a number of mutant cells ni that depends on the generation time of the mutation. if multiplication went like clockwork then the probability of ni being would be the probability of would be the probability of would be and p for all ni that are powers of two. but we don t expect the mutant progeny to divide in exact synchrony and we don t know the precise timing of the end of the experiment compared to the division times. a smoothed version of this distribution that permits all integers to occur is p z i where z distribution s moments are all wrong since ni can never exceed n but who cares about moments? only sampling theory statisticians who are barking up the wrong tree constructing unbiased estimators such as log n the error that we introduce in the likelihood function by using the approximation to p is negligible. the observed number of mutants n is the sum n ni r the probability distribution of n given r is the convolution of r identical distributions of the form for example p r z for n the probability distribution of n given a which is what we need for the bayesian inference is given by summing over r. p a n p rp j a n this quantity can t be evaluated analytically but for small a it s easy to evaluate to any desired numerical precision by explicitly summing over r from copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. random inference topics r to some rmax with p r also being found for each r by rmax explicit convolutions for all required values of n if rmax nmax the largest value of n encountered in the data then p a is computed exactly but for this question s data rmax is plenty for an accurate result i used rmax to make the graphs in octave source code is incidentally for data sets like the one in this exercise which have a substantial number of zero counts very little is lost by making luria and delbruck s second approximation which is to retain only the count of how many n were equal to zero and how many were non-zero. the likelihood function found using this weakened data set la is scarcely distinguishable from the likelihood computed using full information. solution to exercise from the six terms of the form p qi fi most factors cancel and all that remains is qi p j data p j data there is modest evidence in favour of ha!b because the three probabilities inferred for that hypothesis and are more typical of the prior than are the three probabilities inferred for the other and this statement sounds absurd if we think of the priors as uniform over the three probabilities surely under a uniform prior any settings of the probabilities are equally probable? but in the natural basis the logit basis the prior is proportional to p and the posterior probability ratio can be estimated by which is not exactly right but it does illustrate where the preference for a b is coming from. figure likelihood of the mutation rate a on a linear scale and log scale given luria and delbruck s data. vertical axis horizontal axis a. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decision theory decision theory is trivial apart from computational details like playing chess!. you have a choice of various actions a. the world may be in one of many states x which one occurs may be by your action. the world s state has a probability distribution p a. finally there is a utility function u a which the you receive when the world is in state x and you chose action a. the task of decision theory is to select the action that maximizes the expected utility eu j a dkx u ap a that s all. the computational problem is to maximize eu j a over a. may prefer to a loss function l instead of a utility function u and minimize the expected loss. is there anything more to be said about decision theory? well in a real problem the choice of an appropriate utility function may be quite furthermore when a sequence of actions is to be taken with each action providing information about x we have to take into account the that this anticipated information may have on our subsequent actions. the resulting mixture of forward probability and inverse probability computations in a decision problem is distinctive. in a realistic problem such as playing a board game the tree of possible cogitations and actions that must be considered becomes enormous and doing the right thing is not simple because the expected utility of an action cannot be computed exactly and wefald baum and smith baum and smith let s explore an example. rational prospecting suppose you have the task of choosing the site for a tanzanite mine. your action will be to select the site from a list of n sites. the nth site has a net value called the return xn which is initially unknown and will be found out exactly only after site n has been chosen. equals the revenue earned from selling the tanzanite from that site minus the costs of buying the site paying the and so forth. at the outset the return xn has a probability distribution p based on the information already available. before you take your action you have the opportunity to do some prospecting. prospecting at the nth site has a cost cn and yields data dn which reduce the uncertainty about xn. ll assume that the returns of copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decision theory the n sites are unrelated to each other and that prospecting at one site only yields information about that site and doesn t the return from that site. your decision problem is given the initial probability distributions p p p decide whether to prospect and at which sites then in the light of your prospecting results choose which site to mine. for simplicity let s make everything in the problem gaussian and focus on the question of whether to prospect once or not. we ll assume our utility function is linear in xn we wish to maximize our expected return. the utility function is the notation p normaly indicates that y has gaussian distribution with mean and variance if no prospecting is done where na is the chosen action site and if prospecting is done the utility is u xna where np is the site at which prospecting took place. u xna the prior distribution of the return of site n is p normalxn n if you prospect at site n the datum dn is a noisy version of xn p j xn normaldn xn exercise given these assumptions show that the prior probability dis tribution of dn is p normaldn n when independent variables add variances add and that the posterior distribution of xn given dn is where p j dn when gaussians multiply precisions add. n n and n to start with let s evaluate the expected utility if we do no prospecting choose the site immediately then we ll evaluate the expected utility if we prospect at one site and then make our choice. from these two results we will be able to decide whether to prospect once or zero times and if we prospect once at which site. so we consider the expected utility without any prospecting. exercise show that the optimal action assuming no prospecting is to select the site with biggest mean and the expected utility of this action is na argmax n eu j optimal n max n your intuition says surely the optimal decision should take into account the uncertainties too? the answer to this question is reasonable if so then the utility function should be nonlinear in x copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further reading now the exciting bit. should we prospect? once we have prospected at site np we will choose the site using the decision rule with the value of mean replaced by the updated value given by what makes the problem exciting is that we don t yet know the value of dn so we don t know what our action na will be indeed the whole value of doing the prospecting comes from the fact that the outcome dn may alter the action from the one that we would have taken in the absence of the experimental information. from the expression for the new mean in terms of dn and the known variance of dn we can compute the probability distribution of the key quantity and can work out the expected utility by integrating over all possible outcomes and their associated actions. exercise show that the probability distribution of the new mean is gaussian with mean and variance n n n consider prospecting at site n. let the biggest mean of the other sites be when we obtain the new value of the mean we will choose site n and get an expected return of if and we will choose site and get an expected return of if so the expected utility of prospecting at site n then picking the best site is eu j prospect at n p the in utility between prospecting and not prospecting is the quantity of interest and it depends on what we would have done without prospecting and that depends on whether is bigger than eu j no prospecting if if so eu j prospect at n eu j no prospecting if if we can plot the change in expected utility due to prospecting cn as a function of the axis and the initial standard deviation axis. in the the noise variance is further reading if the world in which we act is a little more complicated than the prospecting problem for example if multiple iterations of prospecting are possible and the cost of prospecting is uncertain then the optimal balance between exploration and exploitation becomes a much harder computational problem. reinforcement learning addresses approximate methods for this problem and barto figure contour plot of the gain in expected utility due to prospecting. the contours are equally spaced from to in steps of to decide whether it is worth prospecting at site n the contour equal to cn cost of prospecting all points above that contour are worthwhile. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decision theory further exercises exercise the four doors problem. a new game show uses rules similar to those of the three doors but there are four doors and the host explains first you will point to one of the doors and then i will open one of the other doors guaranteeing to choose a non-winner. then you decide whether to stick with your original pick or switch to one of the remaining doors. then i will open another non-winner never the current pick. you will then make your decision by sticking with the door picked on the previous decision or by switching to the only other remaining door. what is the optimal strategy? should you switch on the opportunity? should you switch on the second opportunity? exercise one of the challenges of decision theory is out exactly what the utility function is. the utility of money for example is notoriously nonlinear for most people. in fact the behaviour of many people cannot be captured by a coherent utility function as illustrated by the allais paradox which runs as follows. which of these choices do you most attractive? a. million guaranteed. b. chance of million chance of million chance of nothing. now consider these choices c. d. chance of nothing chance of million. chance of nothing chance of million. many people prefer a to b and at the same time d to c. prove that these preferences are inconsistent with any utility function u for money. exercise optimal stopping. a large queue of n potential partners is waiting at your door all asking to marry you. they have arrived in random order. as you meet each partner you have to decide on the spot based on the information so far whether to marry them or say no. each potential partner has a desirability dn which you out if and when you meet them. you must marry one of them but you are not allowed to go back to anyone you have said no to. there are several ways to the precise problem. assuming your aim is to maximize the desirability dn i.e. your utility function is dn where is the partner selected what strategy should you use? assuming you wish very much to marry the most desirable person your utility function is if you achieve that and zero otherwise what strategy should you use? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises action buy don t buy outcome no win wins table utility in the lottery ticket problem. action buy don t buy outcome no win wins table regret in the lottery ticket problem. assuming you wish very much to marry the most desirable person and that your strategy will be strategy m strategy m meet the m partners and say no to all of them. memorize the maximum desirability dmax among them. then meet the others in sequence waiting until a partner with dn dmax comes along and marry them. if none more desirable comes along marry the n th partner feel miserable. what is the optimal value of m exercise regret as an objective function? the preceding exercise b and c involved a utility function based on regret. if one married the tenth most desirable candidate the utility function asserts that one would feel regret for having not chosen the most desirable. many people working in learning theory and decision theory use minimizing the maximal possible regret as an objective function but does this make sense? imagine that fred has bought a lottery ticket and to sell it to you before it s known whether the ticket is a winner. for simplicity say the probability that the ticket is a winner is and if it is a winner it is worth fred to sell you the ticket for do you buy it? the possible actions are buy and don t buy the utilities of the four possible actionoutcome pairs are shown in table i have assumed that the utility of small amounts of money for you is linear. if you don t buy the ticket then the utility is zero regardless of whether the ticket proves to be a winner. if you do buy the ticket you end up either losing one pound probability or gaining nine probability in the minimax regret community actions are chosen to minimize the maximum possible regret. the four possible regret outcomes are shown in table if you buy the ticket and it doesn t win you have a regret of because if you had not bought it you would have been better if you do not buy the ticket and it wins you have a regret of because if you had bought it you would have been better the action that minimizes the maximum possible regret is thus to buy the ticket. discuss whether this use of regret to choose actions can be philosophically the above problem can be turned into an investment portfolio decision problem by imagining that you have been given one pound to invest in two possible funds for one day fred s lottery fund and the cash fund. if you put into fred s lottery fund fred promises to return to you if the lottery ticket is a winner and otherwise nothing. the remaining is kept as cash. what is the best investment? show that the minimax regret community will invest of their money in the high risk high return lottery fund and only in cash. can this investment method be exercise gambling oddities cover and thomas a horse race involving i horses occurs repeatedly and you are obliged to bet all your money each time. your bet at time t can be represented by copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decision theory a normalized probability vector b multiplied by your money mt. the odds by the bookies are such that if horse i wins then your return is bioimt. assuming the bookies odds are fair that is oi xi and assuming that the probability that horse i wins is pi work out the optimal betting strategy if your aim is cover s aim namely to maximize the expected value of log mt show that the optimal strategy sets b equal to p independent of the bookies odds o. show that when this strategy is used the money is expected to grow exponentially as where w pi log bioi. if you only bet once is the optimal strategy any do you think this optimal strategy makes sense? do you think that it s optimal in common language to ignore the bookies odds? what can you conclude about cover s aim exercise two ordinary dice are thrown repeatedly the outcome of each throw is the sum of the two numbers. joe shark who says that and are his lucky numbers bets even money that a will be thrown before the is thrown. if you were a gambler would you take the bet? what is your probability of winning? joe then bets even money that an will be thrown before the is thrown. would you take the bet? having gained your joe suggests combining the two bets into a single bet he bets a larger sum still at even odds that an and a will be thrown before two have been thrown. would you take the bet? what is your probability of winning? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bayesian inference and sampling theory there are two schools of statistics. sampling theorists concentrate on having methods guaranteed to work most of the time given minimal assumptions. bayesians try to make inferences that take into account all available information and answer the question of interest given the particular data set. as you have probably gathered i strongly recommend the use of bayesian methods. sampling theory is the widely used approach to statistics and most papers in most journals report their experiments using quantities like intervals levels and p-values. a p-value p is the probability given a null hypothesis for the probability distribution of the data that the outcome would be as extreme as or more extreme than the observed outcome. untrained readers and perhaps more worryingly the authors of many papers usually interpret such a p-value as if it is a bayesian probability example the posterior probability of the null hypothesis an interpretation that both sampling theorists and bayesians would agree is incorrect. in this chapter we study a couple of simple inference problems in order to compare these two approaches to statistics. while in some cases the answers from a bayesian approach and from sampling theory are very similar we can also cases where there are we have already seen such an example in exercise where a sampling theorist got a p-value smaller than and viewed this as strong evidence against the null hypothesis whereas the data actually favoured the null hypothesis over the simplest alternative. on another example was given where the p-value was smaller than the mystical value of yet the data again favoured the null hypothesis. thus in some cases sampling theory can be trigger-happy declaring results to be improbable that the null hypothesis should be rejected when those results actually weakly support the null hypothesis. as we will now see there are also inference problems where sampling theory fails to detect evidence where a bayesian approach and everyday intuition agree that the evidence is strong. most telling of all are the inference problems where the assigned by sampling theory changes depending on irrelevant factors concerned with the design of the experiment. this chapter is only provided for those readers who are curious about the sampling theory bayesian methods debate. if you any of this chapter tough to understand please skip it. there is no point trying to understand the debate. just use bayesian methods they are much easier to understand than the debate itself! copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bayesian inference and sampling theory a medical example we are trying to reduce the incidence of an unpleasant disease called microsoftus. two vaccinations a and b are tested on a group of volunteers. vaccination b is a control treatment a placebo treatment with no active ingredients. of the subjects are randomly assigned to have treatment a and the other are given the control treatment b. we observe the subjects for one year after their vaccinations. of the in group a one contracts microsoftus. of the in group b three contract microsoftus. is treatment a better than treatment b? sampling theory has a go the standard sampling theory approach to the question is a better than b? is to construct a statistical test. the test usually compares a hypothesis such as a and b have with a null hypothesis such as a and b have exactly the same as each other a novice might object no no i want to compare the hypothesis is better than b with the alternative is better than a! but such objections are not welcome in sampling theory. once the two hypotheses have been the hypothesis is scarcely mentioned again attention focuses solely on the null hypothesis. it makes me laugh to write this but it s true! the null hypothesis is accepted or rejected purely on the basis of how unexpected the data were to not on how much better predicted the data. one chooses a statistic which measures how much a data set deviates from the null hypothesis. in the example here the standard statistic to use would be one called to compute we take the between each data measurement and its expected value assuming the null hypothesis to be true and divide the square of that by the variance of the measurement assuming the null hypothesis to be true. in the present problem the four data measurements are the integers fa fb and that is the number of subjects given treatment a who contracted microsoftus the number of subjects given treatment a who didn t and so forth. the of is hfii actually in my elementary statistics book i yates s correction is recommended hfiij hfii in this case given the null hypothesis that treatments a and b are equally and have rates f and for the two outcomes the expected counts are hfaifna hfbifnb if you want to know about yates s correction read a sampling theory textbook. the point of this chapter is not to teach sampling theory i merely mention yates s correction because it is what a professional sampling theorist might use. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a medical example the sampling distribution of a statistic is the probability distribution of its value under repetitions of the experiment assuming that the null hypothesis is true. the test accepts or rejects the null hypothesis on the basis of how big is. to make this test precise and give it a level we have to work out what the sampling distribution of is taking into account the fact that the four data points are not independent satisfy the two constraints fa na and fb nb and the fact that the parameters are not known. these three constraints reduce the number of degrees of freedom in the data from four to one. you want to learn more about computing the number of degrees of freedom read a sampling theory book in bayesian methods we don t need to know all that and quantities equivalent to the number of degrees of freedom pop straight out of a bayesian analysis when they are appropriate. these sampling distributions are tabulated by sampling theory gnomes and come accompanied by warnings about the conditions under which they are accurate. for example standard tabulated distributions for are only accurate if the expected numbers fi are about or more. once the data arrive sampling theorists estimate the unknown parameters of the null hypothesis from the data fa fb na nb na nb and evaluate at this point the sampling theory school divides itself into two camps. one camp uses the following protocol before looking at the data pick the level of the test and determine the critical value of above which the null hypothesis will be rejected. level is the fraction of times that the statistic would exceed the critical value if the null hypothesis were true. then evaluate compare with the critical value and declare the outcome of the test and its level was beforehand. the second camp looks at the data then looks in the table of for the level p for which the observed value of would be the critical value. the result of the test is then reported by giving this value of p which is the fraction of times that a result as extreme as the one observed or more extreme would be expected to arise if the null hypothesis were true. let s apply these two methods. first camp cance level. the critical value for with one degree of freedom is the estimated values of are let s pick as our the expected values of the four measurements are f hfai hfbi and in equation is since this value exceeds we reject the null hypothesis that the two treatments are equivalent at the level. however if we use yates s correction we and therefore accept the null hypothesis. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bayesian inference and sampling theory camp two runs a across the table found at the back of any good and interpolating between sampling theory book and camp two reports the p-value is p notice that this answer does not say how much more a is than b it simply says that a is from b. and here means only statistically not practically the man in the street reading the statement that the treatment was from the control might come to the conclusion that there is a chance that the treatments in but what p actually means is if you did this experiment many times and the two treatments had equal then of the time you would a value of more extreme than the one that happened here this has almost nothing to do with what we want to know which is how likely it is that treatment a is better than b. let me through i m a bayesian ok now let s infer what we really want to know. we scrap the hypothesis that the two treatments have exactly equal since we do not believe it. there are two unknown parameters pa and pb which are the probabilities that people given treatments a and b respectively contract the disease. given the data we can infer these two probabilities and we can answer questions of interest by examining the posterior distribution. the posterior distribution is p pb jffig p pa pbp pb p the likelihood function is p pa pb na nb a b what prior distribution should we use? the prior distribution gives us the opportunity to include knowledge from other experiments or a prior belief that the two parameters pa and pb while from each other are expected to have similar values. here we will use the simplest vanilla prior distribution a uniform distri bution over each parameter. p pb we can now plot the posterior distribution. given the assumption of a separable prior on pa and pb the posterior distribution is also separable p pb jffig p j fa j fb the two posterior distributions are shown in the graphs are not normalized and the joint posterior probability is shown in if we want to know the answer to the question how probable is it that pa is smaller than pb? we can answer exactly that question by computing the posterior probability p pb j data copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a medical example figure posterior probabilities of the two treatment a solid line b dotted line. figure joint posterior probability of the two contour plot and surface plot. pb pa which is the integral of the joint posterior probability p pb j data shown in over the region in which pa pb i.e. the shaded triangle in the value of this integral by a straightforward numerical integration of the likelihood function over the relevant region is p pb j data thus there is a chance given the data and our prior assumptions that treatment a is superior to treatment b. in conclusion according to our bayesian model the data out of contracted the disease after vaccination a and out of contracted the disease after vaccination b give very strong evidence about to one that treatment a is superior to treatment b. in the bayesian approach it is also easy to answer other relevant questions. for example if we want to know how likely is it that treatment a is ten times more than treatment b? we can integrate the joint posterior probability p pb j data over the region in which pa pb model comparison pb pa figure the proposition pa pb is true for all points in the shaded triangle. to the probability of this proposition we integrate the joint posterior probability p pb j data over this region. pb if there were a situation in which we really did want to compare the two hypotheses pa pb and pa pb we can of course do this directly with bayesian methods also. as an example consider the data set d one subject given treatment a subsequently contracted microsoftus. one subject given treatment b did not. pa figure the proposition pa pb is true for all points in the shaded triangle. treatment a b got disease did not total treated copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bayesian inference and sampling theory how strongly does this data set favour over we answer this question by computing the evidence for each hypothesis. let s assume uniform priors over the unknown parameters of the models. the hypothesis pa pb has just one unknown parameter let s call it p. we ll use the uniform prior over the two parameters of model that we used before p p p pb pa pb now the probability of the data d under model is the normalizing constant from the inference of p given d p z dp p j pp z dp p the probability of the data d under model is given by a simple twodimensional integral p z z dpa dpb p j pa pbp pb z dpa pa z dpb pb thus the evidence ratio in favour of model which asserts that the two are unequal is p p so if the prior probability over the two hypotheses was the posterior probability is in favour of is it not easy to get sensible answers to well-posed questions using bayesian methods? sampling theory answer to this question would involve the identical test that was used in the preceding problem that test would yield a not result. i think it is greatly preferable to acknowledge what is obvious to the intuition namely that the data d do give weak evidence in favour of bayesian methods quantify how weak the evidence is. dependence of p-values on irrelevant information in an expensive laboratory dr. bloggs tosses a coin labelled a and b twelve times and the outcome is the string aaabaaaabaab which contains three bs and nine as. what evidence do these data give that the coin is biased in favour of a? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. dependence of p-values on irrelevant information dr. bloggs consults his sampling theory friend who says let r be the number of bs and n be the total number of tosses i view r as the random variable and the probability of r taking on the value r or a more extreme value assuming the null hypothesis pa to be true he thus computes p n n and reports at the level of there is not evidence of bias in favour of a or if the friend prefers to report p-values rather than simply compare p with he would report the p-value is which is not conventionally viewed as small if a two-tailed test seemed more appropriate he might compute the two-tailed area which is twice the above probability and report the p-value is which is not small we won t focus on the issue of the choice between the one-tailed and two-tailed tests as we have bigger to catch. dr. bloggs pays careful attention to the calculation and responds no no the random variable in the experiment was not r i decided before running the experiment that i would keep tossing the coin until i saw three bs the random variable is thus n such experimental designs are not unusual. in my experiments on errorcorrecting codes i often simulate the decoding of a code until a chosen number r of block errors has occurred since the error on the inferred value of log pb goes roughly as pr independent of n. exercise find the bayesian inference about the bias pa of the coin given the data and determine whether a bayesian s inferences depend on what stopping rule was in force. according to sampling theory a calculation is required in order to assess the of the result n the probability distribution of n given is the probability that the tosses contain exactly bs and then the nth toss is a b. p r n the sampling theorist thus computes p r he reports back to dr. bloggs the p-value is there is evidence of bias after all! what do you think dr. bloggs should do? should he publish the result with this marvellous p-value in one of the journals that insists that all experimental results have their assessed using sampling theory? or should he boot the sampling theorist out of the door and seek a coherent method of assessing one that does not depend on the stopping rule? at this point the audience divides in two. half the audience intuitively feel that the stopping rule is irrelevant and don t need any convincing that the answer to exercise is the inferences about pa do not depend on the stopping rule the other half perhaps on account of a thorough copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bayesian inference and sampling theory training in sampling theory intuitively feel that dr. bloggs s stopping rule which stopped tossing the moment the third b appeared may have biased the experiment somehow. if you are in the second group i encourage you to on the situation and hope you ll eventually come round to the view that is consistent with the likelihood principle which is that the stopping rule is not relevant to what we have learned about pa. as a thought experiment consider some onlookers who order to save money are spying on dr. bloggs s experiments each time he tosses the coin the spies update the values of r and n. the spies are eager to make inferences from the data as soon as each new result occurs. should the spies beliefs about the bias of the coin depend on dr. bloggs s intentions regarding the continuation of the experiment? the fact that the p-values of sampling theory do depend on the stopping rule whole volumes of the sampling theory literature are concerned with the task of assessing when a complicated stopping rule is required sequential probability ratio tests for example seems to me a compelling argument for having nothing to do with p-values at all. a bayesian solution to this inference problem was given in sections and and exercise would it help clarify this issue if i added one more scene to the story? the janitor who s been eavesdropping on dr. bloggs s conversation comes in and says i happened to notice that just after you stopped doing the experiments on the coin the for whimsical departmental rules ordered the immediate destruction of all such coins. your coin was therefore destroyed by the departmental safety there is no way you could have continued the experiment much beyond n tosses. seems to me you need to recompute your p-value? intervals in an experiment in which data d are obtained from a system with an unknown parameter a standard concept in sampling theory is the idea of a interval for such an interval has associated with it a level such as which is informally interpreted as the probability that lies in the interval let s make precise what the level really means then give an example. a interval is a function of the data set d. the level of the interval is a property that we can compute before the data arrive. we imagine generating many data sets from a particular true value of and calculating the interval and then checking whether the true value of lies in that interval. if averaging over all these imagined repetitions of the experiment the true value of lies in the interval a fraction f of the time and this property holds for all true values of then the level of the interval is f for example if is the mean of a gaussian distribution which is known to have standard deviation and d is a sample from that gaussian then is a interval for let us now look at a simple example where the meaning of the level becomes clearer. let the parameter be an integer and let the data be a pair of points drawn independently from the following distribution p x x for other values of x. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. some compromise positions for example if were then we could expect the following data sets d with probability with probability with probability with probability we now consider the following interval for example if then the interval for would be let s think about this interval. what is its level? by considering the four possibilities shown in we can see that there is a chance that the interval will contain the true value. the interval therefore has a level of by now what if the data we acquire are well we can compute the interval and it is so shall we report this interval and its associated level this would be correct by the rules of sampling theory. but does this make sense? what do we actually know in this case? intuitively or by bayes theorem it is clear that could either be or and both possibilities are equally likely the prior probabilities of and were equal. the posterior probability of is on and on what if the data are in this case the interval is still and its associated level is but in this case by bayes theorem or common sense we are sure that is in neither case is the probability that lies in the interval equal to thus the way in which many people interpret the levels of sampling theory is incorrect given some data what people usually want to know they know it or not is a bayesian posterior probability distribution. are all these examples contrived? am i making a fuss about nothing? if you are sceptical about the dogmatic views i have expressed i encourage you to look at a case study look in depth at exercise and the reference and oprea in which sampling theory estimates and intervals for a mutation rate are constructed. try both methods on simulated data the bayesian approach based on simply computing the likelihood function and the interval from sampling theory and let me know if you don t that the bayesian answer is always better than the sampling theory answer and often much much better. this suboptimality of sampling theory achieved with great is why i am passionate about bayesian methods. bayesian methods are straightforward and they optimally use all the information in the data. some compromise positions let s end on a conciliatory note. many sampling theorists are pragmatic they are happy to choose from a selection of statistical methods choosing whichever has the best long-run properties. in contrast i have no problem copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bayesian inference and sampling theory with the idea that there is only one answer to a well-posed problem but it s not essential to convert sampling theorists to this viewpoint instead we can them bayesian estimators and bayesian intervals and request that the sampling theoretical properties of these methods be evaluated. we don t need to mention that the methods are derived from a bayesian perspective. if the sampling properties are good then the pragmatic sampling theorist will choose to use the bayesian methods. it is indeed the case that many bayesian methods have good sampling-theoretical properties. perhaps it s not surprising that a method that gives the optimal answer for each individual case should also be good in the long run! another piece of common ground can be conceded while i believe that most well-posed inference problems have a unique correct answer which can be found by bayesian methods not all problems are well-posed. a common question arising in data modelling is am i using an appropriate model? model criticism that is hunting for defects in a current model is a task that may be aided by sampling theory tests in which the null hypothesis the current model is correct is well but the alternative model is not one could use sampling theory measures such as p-values to guide one s search for the aspects of the model most in need of scrutiny. further reading my favourite reading on this topic includes gull loredo berger jaynes treatises on bayesian statistics from the statistics community include and tiao o hagan further exercises exercise a survey records on two successive days. on friday morning there are vehicles in one hour. on saturday morning there are vehicles in half an hour. assuming that the vehicles are poisson distributed with rates and vehicles per hour respectively is greater than by what factor is bigger or smaller than exercise write a program to compare treatments a and b given data fa fb as described in section the outputs of the program should be the probability that treatment a is more than treatment b the probability that pa pb the probability that pb pa. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. part v neural networks copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to neural networks in the of neural networks we study the properties of networks of idealized neurons three motivations underlie work in this broad and interdisciplinary biology. the task of understanding how the brain works is one of the outstanding unsolved problems in science. some neural network models are intended to shed light on the way in which computation and memory are performed by brains. engineering. many researchers would like to create machines that can learn perform pattern recognition or discover patterns in data complex systems. a third motivation for being interested in neural networks is that they are complex adaptive systems whose properties are interesting in their own right. i should emphasize several points at the outset. this book gives only a taste of this there are many interesting neural network models which we will not have time to touch on. the models that we discuss are not intended to be faithful models of biological systems. if they are at all relevant to biology their relevance is on an abstract level. i will describe some neural network methods that are widely used in nonlinear data modelling but i will not be able to give a full description of the state of the art. if you wish to solve real problems with neural networks please read the relevant papers. memories in the next few chapters we will meet several neural network models which come with simple learning algorithms which make them function as memories. perhaps we should dwell for a moment on the conventional idea of memory in digital computation. a memory string of bits describing the name of a person and an image of their face say is stored in a digital computer at an address. to retrieve the memory you need to know the address. the address has nothing to do with the memory itself. notice the properties that this scheme does not have address-based memory is not associative. imagine you know half of a memory say someone s face and you would like to recall the rest of the copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. memories memory their name. if your memory is address-based then you can t get at a memory without knowing the address. scientists have devoted to wrapping traditional address-based memories inside cunning software to produce content-addressable memories but contentaddressability does not come naturally. it has to be added on. address-based memory is not robust or fault-tolerant. if a one-bit mistake is made in specifying the address then a completely memory will be retrieved. if one bit of a memory is then whenever that memory is retrieved the error will be present. of course in all modern computers error-correcting codes are used in the memory so that small numbers of errors can be detected and corrected. but this errortolerance is not an intrinsic property of the memory system. if minor damage occurs to certain hardware that implements memory retrieval it is likely that all functionality will be catastrophically lost. address-based memory is not distributed. in a serial computer that is accessing a particular memory only a tiny fraction of the devices participate in the memory recall the cpu and the circuits that are storing the required byte. all the other millions of devices in the machine are sitting idle. are there models of truly parallel computation in which multiple devices participate in all computations? parallel computers scarcely from serial computers from this point of view. memory retrieval works in just the same way and control of the computation process resides in cpus. there are simply a few more cpus. most of the devices sit idle most of the time. biological memory systems are completely biological memory is associative. memory recall is content-addressable. given a person s name we can often recall their face and vice versa. memories are apparently recalled spontaneously not just at the request of some cpu. biological memory recall is error-tolerant and robust. errors in the cues for memory recall can be corrected. an example asks you to recall an american politician who was very intelligent and whose politician father did not like broccoli many people think of president bush even though one of the cues contains an error. hardware faults can also be tolerated. our brains are noisy lumps of meat that are in a continual state of change with cells being damaged by natural processes alcohol and boxing. while the cells in our brains and the proteins in our cells are continually changing many of our memories persist biological memory is parallel and distributed not completely distributed throughout the whole brain there does appear to be some functional specialization but in the parts of the brain where memories are stored it seems that many neurons participate in the storage of multiple memories. these properties of biological memory systems motivate the study of neural networks parallel distributed computational systems consisting copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. introduction to neural networks of many interacting simple elements. the hope is that these model systems might give some hints as to how neural computation is achieved in real biological neural networks. terminology each time we describe a neural network algorithm we will typically specify three things. any of this terminology is hard to understand it s probably best to dive straight into the next chapter. architecture. the architecture what variables are involved in the network and their topological relationships for example the variables involved in a neural net might be the weights of the connections between the neurons along with the activities of the neurons. activity rule. most neural network models have short time-scale dynamics local rules how the activities of the neurons change in response to each other. typically the activity rule depends on the weights parameters in the network. learning rule. the learning rule the way in which the neural network s weights change with time. this learning is usually viewed as taking place on a longer time scale than the time scale of the dynamics under the activity rule. usually the learning rule will depend on the activities of the neurons. it may also depend on the values of target values supplied by a teacher and on the current value of the weights. where do these rules come from? often activity rules and learning rules are invented by imaginative researchers. alternatively activity rules and learning rules may be derived from carefully chosen objective functions. neural network algorithms can be roughly divided into two classes. supervised neural networks are given data in the form of inputs and targets the targets being a teacher s of what the neural network s response to the input should be. unsupervised neural networks are given data in an undivided form simply a set of examples fxg. some learning algorithms are intended simply to memorize these data in such a way that the examples can be recalled in the future. other algorithms are intended to generalize to discover patterns in the data or extract the underlying features from them. some unsupervised algorithms are able to make predictions for example some algorithms can in missing variables in an example x and so can also be viewed as supervised networks. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the single neuron as a the single neuron we will study a single neuron for two reasons. first many neural network models are built out of single neurons so it is good to understand them in detail. and second a single neuron is itself capable of learning indeed various standard statistical methods can be viewed in terms of single neurons so this model will serve as a example of a supervised neural network. of a single neuron we will start by the architecture and the activity rule of a single neuron and we will then derive a learning rule. architecture. a single neuron has a number i of inputs xi and one output which we will here call y. associated with each input is a weight wi i. there may be an additional parameter of the neuron called a bias which we may view as being the weight associated with an input that is permanently set to the single neuron is a feedforward device the connections are directed from the inputs to the output of the neuron. activity rule. the activity rule has two steps. first in response to the imposed inputs x we compute the activa tion of the neuron a wixi where the sum is over i i if there is a bias and i i otherwise. second the output y is set as a function f of the activation. the output is also called the activity of the neuron not to be confused with the activation a. there are several possible activation functions here are the most popular. deterministic activation functions i. linear. ii. sigmoid function. ya a ya b b b b a wi a xi aa ee e a e e y figure a single neuron activation activity a ya copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the single neuron as a iii. sigmoid ya tanha iv. threshold function. ya a a stochastic activation functions y is stochastically selected from i. heat bath. ya with probability otherwise. ii. the metropolis rule produces the output in a way that depends on the previous output state y compute ay if y to the other state else y to the other state with probability basic neural network concepts a neural network implements a function yx w the output of the network y is a nonlinear function of the inputs x this function is parameterized by weights w. we will study a single neuron which produces an output between and as the following function of x yx w exercise in what contexts have we encountered the function yx w already? motivations for the linear logistic function in section we studied the best detection of pulses assuming that one of two signals and had been transmitted over a gaussian channel with variancecovariance matrix we found that the probability that the source signal was s rather than s given the received signal y was p y where ay was a linear function of the received vector ay wty with w the exercises. the linear logistic function can be motivated in several other ways see copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. basic neural network concepts figure output of a simple neural network as a function of its input. w input space and weight space for convenience let us study the case where the input vector x and the parameter vector w are both two-dimensional x w then we can spell out the function performed by the neuron thus yx w figure shows the output of the neuron as a function of the input vector for w the two horizontal axes of this are the inputs and with the output y on the vertical axis. notice that on any line perpendicular to w the output is constant and along a line in the direction of w the output is a sigmoid function. we now introduce the idea of weight space that is the parameter space of the network. in this case there are two parameters and so the weight space is two dimensional. this weight space is shown in for a selection of values of the parameter vector w smaller inset show the function of x performed by the network when w is set to those values. each of these smaller is equivalent to thus each point in w space corresponds to a function of x. notice that the gain of the sigmoid function gradient of the ramp increases as the magnitude of w increases. now the central idea of supervised neural networks is this. given examples of a relationship between an input vector x and a target t we hope to make the neural network learn a model of the relationship between x and t. a successfully trained network will for any given x give an output y that is close some sense to the target value t. training the network involves searching in the weight space of the network for a value of w that produces a function that the provided training data well. typically an objective function or error function is as a function of w to measure how well the network with weights set to w solves the task. the objective function is a sum of terms one for each inputtarget pair fx tg measuring how close the output yx w is to the target t. the training process is an exercise in function minimization i.e. adjusting w in such a way as to a w that minimizes the objective function. many function-minimization algorithms make use not only of the objective function but also its gradient with respect to the parameters w. for general feedforward neural networks the backpropagation algorithm evaluates the gradient of the output y with respect to the parameters w and thence the gradient of the objective function with respect to w. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the single neuron as a w w w w w w w w w w figure weight space. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. training the single neuron as a binary training the single neuron as a binary we assume we have a data set of inputs fxngn and a neuron whose output yx w is bounded between and we can then write down the following error function with binary labels ftngn gw htn ln yxn w tn yxn wi each term in this objective function may be recognized as the information content of one outcome. it may also be described as the relative entropy between the empirical probability distribution tn and the probability distribution implied by the output of the neuron the objective function is bounded below by zero and only attains this value if yxn w tn for all n. we now this objective function with respect to w. exercise the backpropagation algorithm. show that the derivative g is given by gj n ynxn j notice that the quantity en tn yn is the error on example n the between the target and the output. the simplest thing to do with a gradient of an error function is to descend it though this is often dimensionally incorrect since a gradient has dimensions whereas a change in a parameter has dimensions since the derivative is a sum of terms gn by gn j ynxn j for n n we can obtain a simple on-line algorithm by putting each input through the network one at a time and adjusting w a little in a direction opposite to gn. we summarize the whole learning algorithm. the on-line gradient-descent learning algorithm architecture. a single neuron has a number i of inputs xi and one output y. associated with each input is a weight wi i. activity rule. first in response to the received inputs x may be arbitrary real numbers we compute the activation of the neuron a wixi where the sum is over i i if there is a bias and i i otherwise. second the output y is set as a sigmoid function of the activation. ya this output might be viewed as stating the probability according to the neuron that the given input is in class rather than class copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the single neuron as a learning rule. the teacher supplies a target value t which says what the correct answer is for the given input. we compute the error signal e t y then adjust the weights w in a direction that would reduce the magnitude of this error where is the learning rate commonly is set by trial and error to a constant value or to a decreasing function of simulation time such as the activity rule and learning rule are repeated for each inputtarget pair t that is presented. if there is a data set of size n we can cycle through the data multiple times. batch learning versus on-line learning here we have described the on-line learning algorithm in which a change in the weights is made after every example is presented. an alternative paradigm is to go through a batch of examples computing the outputs and errors and accumulating the changes in equation which are then made at the end of the batch. batch learning for the single neuron for each inputtarget pair tn n compute yn yxn w where yx w en tn yn and compute for each weight wi wixi gn i i then let gn i this batch learning algorithm is a gradient descent algorithm whereas the on-line algorithm is a stochastic gradient descent algorithm. source code implementing batch learning is given in algorithm this algorithm is demonstrated in for a neuron with two inputs with weights and and a bias performing the function yx w the bias is included in contrast to where it was omitted. the neuron is trained on a data set of ten labelled examples. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. training the single neuron as a binary gw e_ww figure a single neuron learning to classify by gradient descent. the neuron has two weights and and a bias the learning rate was set to and batch-mode gradient descent was performed using the code displayed in algorithm the training data. evolution of weights and as a function of number of iterations log scale. evolution of weights and in weight space. the objective function gw as a function of number of iterations. the magnitude of the weights ew as a function of time. the function performed by the neuron by three of its contours after and iterations. the contours shown are those corresponding to a namely y and also shown is a vector proportional to the larger the weights are the bigger this vector becomes and the closer together are the contours. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the single neuron as a algorithm octave source code for a gradient descent optimizer of a single neuron batch learning with optional weight decay alpha. octave notation the instruction a x w causes the i matrix x consisting of all the input vectors to be multiplied by the weight vector w giving the vector a listing the activations for all n input vectors x means x-transpose the single command y sigmoida computes the sigmoid function of all elements of the vector a. figure the of weight decay on a single neuron s learning. the objective function is m gw the learning method was as in evolution of weights and evolution of weights and in weight space shown by points contrasted with the trajectory followed in the case of zero weight decay shown by a thin line notice that for this problem weight decay has an very similar to early stopping the objective function m and the error function gw as a function of number of iterations. the function performed by the neuron after iterations. global x global t x is an n i matrix containing all the input vectors t is a vector of length n containing all the targets for l loop l times a x w y sigmoida e t y g x e w w eta g alpha w compute all activations compute outputs compute errors compute the gradient vector make step using learning rate eta and weight decay alpha endfor function f sigmoid v f exp v endfunction gw mw gw mw gw mw copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. beyond descent on the error function regularization beyond descent on the error function regularization if the parameter is set to an appropriate value this algorithm works the algorithm a setting of w that correctly as many of the examples as possible. if the examples are in fact linearly separable then the neuron this linear separation and its weights diverge to ever-larger values as the simulation continues. this can be seen happening in this is an example of where a model the data so well that its generalization performance is likely to be adversely this behaviour may be viewed as undesirable. how can it be an ad hoc solution to is to use early stopping that is use an algorithm originally intended to minimize the error function gw then prevent it from doing so by halting the algorithm at some point. a more principled solution to makes use of regularization. regularization involves modifying the objective function in such a way as to incorporate a bias against the sorts of solution w which we dislike. in the above example what we dislike is the development of a very sharp decision boundary in this sharp boundary is associated with large weight values so we use a regularizer that penalizes large weight values. we modify the objective function to m gw where the simplest choice of regularizer is the weight decay regularizer ew i the regularization constant is called the weight decay rate. this additional term favours small values of w and decreases the tendency of a model to details of the training data. the quantity is known as a hyperparameter. hyperparameters play a role in the learning algorithm but play no role in the activity rule of the network. exercise compute the derivative of m with respect to wi. why is the above regularizer known as the weight decay regularizer? the gradient descent source code of algorithm implements weight decay. this gradient descent algorithm is demonstrated in using weight decay rates and as the weight decay rate is increased the solution becomes biased towards broader sigmoid functions with decision boundaries that are closer to the origin. note gradient descent with a step size is in general not the most way to minimize a function. a of gradient descent known as momentum while improving convergence is also not recommended. most neural network experts use more advanced optimizers such as conjugate gradient algorithms. do not confuse momentum which is sometimes given the symbol with weight decay. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the single neuron as a further exercises more motivations for the linear neuron exercise consider the task of recognizing which of two gaussian distributions a vector z comes from. unlike the case studied in section where the distributions had means but a common variance covariance matrix we will assume that the two distributions have exactly the same mean but variances. let the probability of z given s be p s i normalzi si si is the variance of zi when the source symbol is s. show that where p z can be written in the form p z where xi is an appropriate function of zi xi gzi. exercise the noisy led. consider an led display with elements numbered as shown above. the state of the display is a vector x. when the controller wants the display to show character number s e.g. s each element xj either adopts its intended state cjs with probability or is with probability f let s call the two states of x and assuming that the intended character s is actually a or a what is the probability of s given the state x? show that p x can be written in the form p x and compute the values of the weights w in the case f assuming that s is one of with prior probabilities ps what is the probability of s given the state x? put your answer in the form p x eas where fasg are functions of fcjsg and x. could you make a better alphabet of characters for a noisy led i.e. an alphabet less susceptible to confusion? exercise a error-correcting code consists of the two codewords and a source bit s having probability distribution is used to select one of the two codewords for transmission over a binary symmetric channel with noise level f the table an alternative alphabet for the led display. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises received vector is r. show that the posterior probability of s given r can be written in the form p r and give expressions for the describe with a diagram how this optimal decoder can be expressed in terms of a neuron and the bias copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. problems to look at before chapter exercise what is pn symbol means the combination n exercise if the top row of pascal s triangle contains the single number is denoted row zero what is the sum of all the numbers in the triangle above row n exercise points are selected at random on the surface of a sphere. what is the probability that all of them lie on a single hemisphere? this chapter s material is originally due to polya and cover and the exposition that follows is yaser abu-mostafa s. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. capacity of a single neuron ftngn learning algorithm w fxngn w ftngn figure neural network learning viewed as communication. fxngn neural network learning as communication at given locations fxngn many neural network models involve the adaptation of a set of weights w in response to a set of data points for example a set of n target values dn ftngn the adapted weights are then used to process subsequent input data. this process can be viewed as a communication process in which the sender examines the data dn and creates a message w that depends on those data. the receiver then uses w for example the receiver might use the weights to try to reconstruct what the data dn was. neural network parlance this is using the neuron for memory rather than for generalization generalizing means extrapolating from the observed data to the value of tn at some new location xn just as a disk drive is a communication channel the adapted network weights w therefore play the role of a communication channel conveying information about the training data to a future user of that neural net. the question we now address is what is the capacity of this channel? that is how much information can be stored by training a neural network? if we had a learning algorithm that either produces a network whose response to all inputs is or a network whose response to all inputs is depending on the training data then the weights allow us to distinguish between just two sorts of data set. the maximum information such a learning algorithm could convey about the data is therefore bit this information content being achieved if the two sorts of data set are equiprobable. how much more information can be conveyed if we make full use of a neural network s ability to represent other functions? the capacity of a single neuron we will look at the simplest case that of a single binary threshold neuron. we will that the capacity of such a neuron is two bits per weight. a neuron with k inputs can store bits of information. to obtain this interesting result we lay down some rules to exclude less interesting answers such as the capacity of a neuron is because each copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. capacity of a single neuron of its weights is a real number and so can convey an number of bits we exclude this answer by saying that the receiver is not able to examine the weights directly nor is the receiver allowed to probe the weights by observing the output of the neuron for arbitrarily chosen inputs. we constrain the receiver to observe the output of the neuron at the same set of n points fxng that were in the training set. what matters now is how many distinguishable functions our neuron can produce given that we can observe the function only at these n points. how many binary labellings of n points can a linear threshold function produce? and how does this number compare with the maximum possible number of binary labellings if nearly all of the labellings can be realized by our neuron then it is a communication channel that can convey all n bits target values ftng with small probability of error. we will identify the capacity of the neuron as the maximum value that n can have such that the probability of error is very small. are departing a little from the of capacity in chapter we thus examine the following scenario. the sender is given a neuron with k inputs and a data set dn which is a labelling of n points. the sender uses an adaptive algorithm to try to a w that can reproduce this labelling exactly. we will assume the algorithm such a w if it exists. the receiver then evaluates the threshold function on the n input values. what is the probability that all n bits are correctly reproduced? how large can n become for a given k without this probability becoming substantially less than one? general position one technical detail needs to be pinned down what set of inputs fxng are we considering? our answer might depend on this choice. we will assume that the points are in general position. a set of points fxng in k-dimensional space are in general position if any subset of size k is linearly independent and no k of them lie in a plane. in k dimensions for example a set of points are in general position if no three points are colinear and no four points are coplanar. the intuitive idea is that points in general position are like random points in the space in terms of the linear dependences between points. you don t expect three random points in three dimensions to lie on a straight line. the linear threshold function the neuron we will consider performs the function where y f k f wkxk! a a we will not have a bias the capacity for a neuron with a bias can be obtained by replacing k by k in the result below i.e. considering one of the inputs to be to input points would not then be in general position the derivation still works. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. counting threshold functions figure one data point in a two-dimensional input space and the two regions of weight space that give the two alternative labellings of that point. counting threshold functions let us denote by t k the number of distinct threshold functions on n points in general position in k dimensions. we will derive a formula for t k. to start with let us work out a few cases by hand. in k dimension for any n the n points lie on a line. by changing the sign of the one weight we can label all points on the right side of the origin and the others or vice versa. thus there are two distinct threshold functions. t with n point for any k if there is just one point then we can realize both possible labellings by setting w thus t k in k dimensions in two dimensions with n points we are free to spin the separating line around the origin. each time the line passes over a point we obtain a new function. once we have spun the line through degrees we reproduce the function we started from. because the points are in general position the separating plane crosses only one point at a time. in one revolution every point is passed over twice. there are therefore distinct threshold functions. t comparing with the total number of binary functions we may note that for n not all binary functions can be realized by a linear threshold function. one famous example of an unrealizable function with n and k is the exclusive-or function on the points x points are not in general position but you may that the function remains unrealizable even if the points are perturbed into general position. in k dimensions from the point of view of weight space there is another way of visualizing this problem. instead of visualizing a plane separating points in the two-dimensional input space we can consider the two-dimensional weight space colouring regions in weight space colours if they label the given datapoints we can then count the number of threshold functions by counting how many distinguishable regions there are in weight space. consider the set of weight vectors in weight copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. capacity of a single neuron figure two data points in a two-dimensional input space and the four regions of weight space that give the four alternative labellings. figure three data points in a two-dimensional input space and the six regions of weight space that give alternative labellings of those points. in this case the labellings and cannot be realized. for any three points in general position there are always two labellings that cannot be realized. space that classify a particular example xn as a for example shows a single point in our two-dimensional x-space and shows the two corresponding sets of points in w-space. one set of weight vectors occupy the half space and the others occupy in we have added a second point in the input space. there are now possible labellings and figure shows the two hyperplanes and which separate the sets of weight vectors that produce each of these labellings. when n weight space is divided by three hyperplanes into six regions. not all of the eight conceivable labellings can be realized. thus t in k dimensions we now use this weight space visualization to study the three dimensional case. let us imagine adding one point at a time and count the number of threshold functions as we do so. when n weight space is divided by two hyperplanes and into four regions in any one region all vectors w produce the same function on the input vectors. thus t adding a third point in general position produces a third plane in w space so that there are distinguishable regions. t the three bisecting planes are shown in at this point matters become slightly more tricky. as illustrates the fourth plane in the three-dimensional w space cannot transect all eight of the sets created by the three planes. six of the existing regions are cut in two and the remaining two are so t two copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. counting threshold functions k n figure weight space illustrations for t and t t three hyperplanes to three points in general position divide into regions shown here by colouring the relevant part of the surface of a hollow semi-transparent cube centred on the origin. t four hyperplanes divide into regions of which this shows region is out of view on the right-hand face. compare with all of the regions that are not coloured white have been cut into two. table values of t k deduced by hand. figure illustration of the cutting process going from t to t the eight regions of with one added hyperplane. all of the regions that are not coloured white have been cut into two. here the hollow cube has been made solid so we can see which regions are cut by the fourth plane. the front half of the cube has been cut away. this shows the new two dimensional hyperplane which is divided into six regions by the three one-dimensional hyperplanes which cross it. each of these regions corresponds to one of the three-dimensional regions in which is cut into two by this new hyperplane. this shows that t t figure should be compared with copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. capacity of a single neuron of the binary functions on points in dimensions cannot be realized by a linear threshold function. we have now in the values of t k shown in table can we obtain any insights into our derivation of t in order to in the rest of the table for t k? why was t greater than t by six? six is the number of regions that the new hyperplane bisected in w-space b. equivalently if we look in the k dimensional subspace that is the n th hyperplane that subspace is divided into six regions by the previous hyperplanes now this is a concept we have met before. compare with how many regions are created by n hyperplanes in a dimensional space? why t of course! in the present case n k we can look up t in the previous section. so t t t recurrence relation for any n k generalizing this picture we see that when we add an n th hyperplane in k dimensions it will bisect t of the t k regions that were created by the previous n hyperplanes. therefore the total number of regions obtained after adding the n th hyperplane is t out of t k regions are split in two plus the remaining t k t regions not split by the n th hyperplane which gives the following equation for t k t k t k t now all that remains is to solve this recurrence relation given the boundary conditions t and t k does the recurrence relation look familiar? maybe you remember building pascal s triangle by adding together two adjacent numbers in one row to get the number below. the n k element of pascal s triangle is equal to cn k n k!k! table pascal s triangle. n k satisfy the equation combinations cn k k we are adopting the convention that if k n or k the required recurrence relation this doesn t mean so since many functions can satisfy one recurrence relation. t k for all n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. counting threshold functions n k nk k nk n log tnk log nk nk figure the fraction of functions on n points in k dimensions that are linear threshold functions t shown from various viewpoints. in we see the dependence on k which is approximately an error function passing through at k the fraction reaches at k n in we see the dependence on n which is up to n k and drops sharply at n panel shows the dependence on nk for k there is a sudden drop in the fraction of realizable labellings when n panel shows the values of t k and as a function of n for k these were plotted using the approximation of t by the error function. but perhaps we can express t k as a linear superposition of combination functions of the form k we can see how to satisfy the boundary conditions we simply need to translate pascal s triangle to the right by superpose add multiply by two and drop the whole table by one line. thus by comparing tables and t k k using the fact that the n th row of pascal s triangle sums to that is k we can simplify the cases where t k k n k k n interpretation it is natural to compare t k with the total number of binary functions on n points the ratio t tells us the probability that an arbitrary labelling ftngn can be memorized by our neuron. the two functions are equal for all n k. the line n k is thus a special line the maximum number of points on which any arbitrary labelling can be realized. this number of points is referred to as the vapnikchervonenkis dimension dimension of the class of functions. the vc dimension of a binary threshold function on k dimensions is thus k. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. capacity of a single neuron what is interesting is large k the number of points n such that almost any labelling can be realized. the ratio t is for n still greater than and for large k the ratio is very close to for our purposes the sum in equation is well approximated by the error function k pn k figure shows the realizable fraction where z t as a function of n and k. the take-home message is shown in although the fraction t is less than for n k it is only negligibly less than up to n there there is a catastrophic drop to zero so that for n only a tiny fraction of the binary labellings can be realized by the threshold function. conclusion the capacity of a linear threshold neuron for large k is bits per weight. a single neuron can almost certainly memorize up to n random binary labels perfectly but will almost certainly fail to memorize more. further exercises exercise can a set of distinct points in a two-dimensional space be split in half by a straight line if the points are in general position? if the points are not in general position? can points in a k dimensional space be split in half by a k dimensional hyperplane? exercise four points are selected at random on the surface of a sphere. what is the probability that all of them lie on a single hemisphere? how does this question relate to t k? exercise consider the binary threshold neuron in k dimensions and the set of points fxg find a parameter vector w such that the neuron memorizes the labels ftg ftg find an unrealizable labelling ftg. exercise in this chapter we constrained all our hyperplanes to go through the origin. in this exercise we remove this constraint. how many regions in a plane are created by n lines in general position? exercise estimate in bits the total sensory experience that you have had in your life visual information auditory information etc. estimate how much information you have memorized. estimate the information content of the works of shakespeare. compare these with the capacity of your brain assuming you have neurons each making synaptic connections and that the capacity result for one neuron bits per connection applies. is your brain full yet? figure three lines in a plane create seven regions. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions exercise what is the capacity of the axon of a spiking neuron viewed in bits per second? mackay and as a communication channel mcculloch for an early publication on this topic. multiply by the number of axons in the optic nerve or cochlear nerve per ear to estimate again the rate of acquisition sensory experience. solutions solution to exercise the probability that all four points lie on a single hemisphere is t copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. learning as inference neural network learning as inference in chapter we trained a simple neural network as a by minimizing an objective function m gw made up of an error function gw htn ln yxn w tn yxn wi and a regularizer ew i this neural network learning process can be given the following probabilistic interpretation. we interpret the output yx w of the neuron literally as its parameters w are the probability that an input x belongs to class t rather than the alternative t thus yx w p x w. then each value of w a hypothesis about the probability of class relative to class as a function of x. we the observed data d to be the targets ftg the inputs fxg are assumed to be given and not to be modelled. to infer w given the data we require a likelihood function and a prior probability over w. the likelihood function measures how well the parameters w predict the observed data it is the probability assigned to the observed t values by the model with parameters set to w. now the two equations p w x y p w x y can be rewritten as the single equation p w x expt ln y t y so the error function g can be interpreted as minus the log likelihood p j w similarly the regularizer can be interpreted in terms of a log prior proba bility distribution over the parameters p j zw copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. illustration for a neuron with two weights if ew is quadratic as above then the corresponding prior distribution w and is equal to is a gaussian with variance where k is the number of parameters in the vector w. the objective function m then corresponds to the inference of the parameters w given the data p j d p j wp j p j p j zm so the w found by minimizing m can be interpreted as the most probable parameter vector from now on we will refer to as wmp. why is it natural to interpret the error functions as log probabilities? error functions are usually additive. for example g is a sum of information contents and ew is a sum of squared weights. probabilities on the other hand are multiplicative for independent events x and y the joint probability is p y p the logarithmic mapping maintains this correspondence. the interpretation of m as a log probability has numerous some of which we will discuss in a moment. illustration for a neuron with two weights in the case of a neuron with just two inputs and no bias yx w we can plot the posterior probability of w p j d imagine that we receive some data as shown in the left column of each data point consists of a two-dimensional input vector x and a t value indicated by or the likelihood function is shown as a function of w in the second column. it is a product of functions of the form the product of traditional learning is a point in w-space the estimator which maximizes the posterior probability density. in contrast in the bayesian view the product of learning is an ensemble of plausible parameter values right of we do not choose one particular hypothesis w rather we evaluate their posterior probabilities. the posterior distribution is obtained by multiplying the likelihood by a prior distribution over w space as a broad gaussian at the upper right of the posterior ensemble a multiplicative constant is shown in the third column of and as a contour plot in the fourth column. as the amount of data increases top to bottom the posterior ensemble becomes increasingly concentrated around the most probable value beyond optimization making predictions let us consider the task of making predictions with the neuron which we trained as a in section this was a neuron with two inputs and a bias. yx w copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. data set likelihood probability of parameters learning as inference x n n n x n x figure the bayesian interpretation and generalization of traditional neural network learning. evolution of the probability distribution over parameters as data arrive. a b samples from pwdh wmp w a b figure making predictions. the function performed by an optimized neuron wmp by three of its contours trained with weight decay the contours shown are those corresponding to a namely y and are these predictions more reasonable? shown are for y and the posterior probability of w the bayesian predictions shown in were obtained by averaging together the predictions made by each possible value of the weights w with each value of w receiving a vote proportional to its probability under the posterior ensemble. the method used to create is described in section copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. beyond optimization making predictions when we last played with it we trained it by minimizing the objective function m gw the resulting optimized function for the case is reproduced in we now consider the task of predicting the class corresponding to a new input it is common practice when making predictions simply to use a neural network with its weights to their optimized value wmp but this is not optimal as can be seen intuitively by considering the predictions shown in are these reasonable predictions? consider new data arriving at points a and b. the model assigns both of these examples probability of being in class because they have the same value of w if we really knew that w was equal to wmp then these predictions would be correct. but we do not know w. the parameters are uncertain. intuitively we might be inclined to assign a less probability to at b than at a as shown in since point b is far from the training data. the parameters wmp often give predictions. a non-bayesian approach to this problem is to downweight all predictions uniformly by an empirically determined factor this is not ideal since intuition suggests the strength of the predictions at b should be downweighted more than those at a. a bayesian viewpoint helps us to understand the cause of the problem and provides a straightforward solution. in a nutshell we obtain bayesian predictions by taking into account the whole posterior ensemble shown schematically in the bayesian prediction of a new datum involves marginalizing over the parameters over anything else about which we are uncertain. for simplicity let us assume that the weights w are the only uncertain quantities the weight decay rate and the model h itself are assumed to be then by the sum rule the predictive probability of a new target at a location is p j d dkw p j w j d where k is the dimensionality of w three in the toy problem. thus the predictions are obtained by weighting the prediction for each possible w p w w p w w with a weight given by the posterior probability of w p j d which we most recently wrote down in equation this posterior probability is zm p j d zm dkw where in summary we can get the bayesian predictions if we can a way of computing the integral p d dkw w zm which is the average of the output of the neuron at under the posterior distribution of w. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. learning as inference figure one step of the langevin method in two dimensions contrasted with a traditional dumb metropolis method and with gradient descent the proposal density of the langevin method is given by gradient descent with noise p qx dumb metropolis gradient descent langevin implementation how shall we compute the integral for our toy problem the weight space is three dimensional for a realistic neural network the dimensionality k might be in the thousands. bayesian inference for general data modelling problems may be implemented by exact methods by monte carlo sampling or by deterministic approximate methods for example methods that make gaussian approximations to p j d using laplace s method or variational methods for neural networks there are few exact methods. the two main approaches to implementing bayesian inference for neural networks are the monte carlo methods developed by neal and the gaussian approximation methods developed by mackay monte carlo implementation of a single neuron first we will use a monte carlo approach in which the task of evaluating the integral is solved by treating w as a function f of w whose mean we compute using hf f rxr zm where fwrg are samples from the posterior distribution equation we obtain the samples using a metropolis method as an aside a possible disadvantage of this monte carlo approach is that it is a poor way of estimating the probability of an improbable event i.e. a p dh that is very close to zero if the improbable event is most likely to occur in conjunction with improbable parameter values. how to generate the samples fwrg? radford neal introduced the hamiltonian monte carlo method to neural networks. we met this sophisticated metropolis method which makes use of gradient information in chapter the method we now demonstrate is a simple version of hamiltonian monte carlo called the langevin monte carlo method. the langevin monte carlo method the langevin method may be summarized as gradient descent with added noise as shown pictorially in a noise vector p is generated from a gaussian with unit variance. the gradient g is computed copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo implementation of a single neuron g gradm w m findm w set gradient using initial w set objective function too for l p randn sizew h p p m loop l times initial momentum is evaluate hwp p p epsilon g wnew w epsilon p gnew gradm wnew p p epsilon gnew make half-step in p make step in w find new gradient make half-step in p algorithm octave source code for the langevin monte carlo method. to obtain the hamiltonian monte carlo method we repeat the four lines marked multiple times mnew findm wnew hnew p p mnew dh hnew h if dh accept elseif rand exp-dh accept else accept endif if accept g gnew w wnew find new objective function evaluate new value of h decide whether to accept compare with a uniform variate m mnew endif endfor function gm gradm w a x w y sigmoida e t y g x e gm alpha w g gradient of objective function compute activations compute outputs compute errors compute the gradient of gw endfunction function m findm w objective function g logy log ew w w m g alpha ew endfunction copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gw langevin gw optimizer learning as inference mw langevin mw optimizer figure a single neuron learning under the langevin monte carlo method. evolution of weights and as a function of number of iterations. evolution of weights and in weight space. also shown by a line is the evolution of the weights using the optimizer of the error function gw as a function of number of iterations. also shown is the error function during the optimization of the objective function m as a function of number of iterations. see also and and a step in w is made given by notice that if the term were omitted this would simply be gradient descent with learning rate this step in w is accepted or rejected depending on the change in the value of the objective function m and on the change in gradient with a probability of acceptance such that detailed balance holds. the langevin method has one free parameter which controls the typical step size. if is set to too large a value moves may be rejected. if it is set to a very small value progress around the state space will be slow. demonstration of langevin method the langevin method is demonstrated in and here the objective function is m gw with these include for comparison the results of the previous optimization method using gradient descent on the same objective function it can be seen that the mean evolution of w is similar to the evolution of the parameters under gradient descent. the monte carlo method appears to have converged to the posterior distribution after about iterations. the average acceptance rate during this simulation was only of the proposed moves were rejected. probably faster progress around the state space would have been made if a larger step size had been used but the value was chosen so that the descent rate matched the step size of the earlier simulations. making bayesian predictions from iteration to the weights were sampled every iterations and the corresponding functions of x are plotted in there is a considerable variety of plausible functions. we obtain a monte carlo approximation to the bayesian predictions by averaging these thirty functions of x together. the result is shown in and contrasted with the predictions given by the optimized parameters. the bayesian predictions become satisfyingly moderate as we move away from the region of highest data density. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. monte carlo implementation of a single neuron figure samples obtained by the langevin monte carlo method. the learning rate was set to and the weight decay rate to the step size is given by the function performed by the neuron is shown by three of its contours every iterations from iteration to the contours shown are those corresponding to a namely y and also shown is a vector proportional to figure bayesian predictions found by the langevin monte carlo method compared with the predictions using the optimized parameters. the predictive function obtained by averaging the predictions for samples uniformly spaced between iterations and shown in the contours shown are those corresponding to a namely y and for contrast the predictions given by the most probable setting of the neuron s parameters as given by optimization of m copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. learning as inference algorithm octave source code for the hamiltonian monte carlo method. the algorithm is identical to the langevin method in algorithm except for the replacement of the four lines marked in that algorithm by the fragment shown here. figure comparison of sampling properties of the langevin monte carlo method and the hamiltonian monte carlo method. the horizontal axis is the number of gradient evaluations made. each shows the weights during the iterations. the rejection rate during this hamiltonian monte carlo simulation was wnew w gnew g for tau p p epsilon gnew wnew wnew epsilon p make half-step in p make step in w gnew gradm wnew p p epsilon gnew find new gradient make half-step in p endfor langevin hmc the bayesian is better able to identify the points where the is uncertain. this pleasing behaviour results simply from a mechanical application of the rules of probability. optimization and typicality a observation concerns the behaviour of the functions gw and m during the monte carlo sampling process compared with the values of g and m at the optimum wmp the function gw around the value of gwmp though not in a symmetrical way. the function m also but it does not around m obviously it cannot because m is minimized at wmp so m could not go any smaller furthermore m only rarely drops close to m in the language of information theory the typical set of w has properties from the most probable state w mp. a general message therefore emerges applicable to all data models not just neural networks one should be cautious about making use of optimized parameters as the properties of optimized parameters may be unrepresentative of the properties of typical plausible parameters and the predictions obtained using optimized parameters alone will often be unreasonably reducing random walk behaviour using hamiltonian monte carlo as a study of monte carlo methods we now compare the langevin monte carlo method with its big brother the hamiltonian monte carlo method. the change to hamiltonian monte carlo is simple to implement as shown in algorithm each single proposal makes use of multiple gradient evaluations copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. implementing inference with gaussian approximations along a dynamical trajectory in w p space where p are the extra momentum variables of the langevin and hamiltonian monte carlo methods. the number of steps tau was set at random to a number between and for each trajectory. the step size was kept so as to retain comparability with the simulations that have gone before it is recommended that one randomize the step size in practical applications however. figure compares the sampling properties of the langevin and hamiltonian monte carlo methods. the autocorrelation of the state of the hamiltonian monte carlo simulation falls much more rapidly with simulation time than that of the langevin method. for this toy problem hamiltonian monte carlo is at least ten times more in its use of computer time. implementing inference with gaussian approximations physicists love to take nonlinearities and locally linearize them and they love to approximate probability distributions by gaussians. such approximations an alternative strategy for dealing with the integral p d dkw w zm which we just evaluated using monte carlo methods. we start by making a gaussian approximation to the posterior probability. we go to the minimum of m a gradient-based optimizer and taylorexpand m there m m wmptaw wmp where a is the matrix of second derivatives also known as the hessian by aij m wmptaw we thus our gaussian approximation qw wmp a we can think of the matrix a as error bars on w. to be precise q is a normal distribution whose variancecovariance matrix is exercise show that the second derivative of m with respect to w is given by m n i xn j where is the derivative of f which is and d da f f f an wjxn j having computed the hessian our task is then to perform the integral using our gaussian approximation. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. learning as inference figure the marginalized probability and an approximation to it. the function evaluated numerically. in the functions and in the text are shown as a function of a for from mackay figure the gaussian approximation in weight space and its approximate predictions in input space. a projection of the gaussian approximation onto the plane of weight space. the one- and two-standard-deviation contours are shown. also shown are the trajectory of the optimizer and the monte carlo method s samples. the predictive function obtained from the gaussian approximation and equation a b calculating the marginalized probability the output yx w depends on w only through the scalar ax w so we can reduce the dimensionality of the integral by the probability density of a. we are assuming a locally gaussian posterior probability distribution over w wmp p j d for our single neuron the activation ax w is a linear function of w with x so for any x the activation a is gaussian-distributed. exercise assuming w is gaussian-distributed with mean wmp and variancecovariance matrix show that the probability distribution of ax is p x d normalamp where amp ax wmp and this means that the marginalized output is p x d da f normalamp this is to be contrasted with yx wmp f the output of the most probable network. the integral of a sigmoid times a gaussian can be approximated by f with demonstration figure shows the result of a gaussian approximation at the optimum wmp and the results of using that gaussian approximation and equa copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. implementing inference with gaussian approximations tion to make predictions. comparing these predictions with those of the langevin monte carlo method we observe that whilst qualitatively the same the two are clearly numerically so at least one of the two methods is not completely accurate. exercise is the gaussian approximation to p j d too heavy-tailed or too light-tailed or both? it may help to consider p j d as a function of one parameter wi and to think of the two distributions on a logarithmic scale. discuss the conditions under which the gaussian approximation is most accurate. why marginalize? if the output is immediately used to make a decision and the costs associated with error are symmetrical then the use of marginalized outputs under this gaussian approximation will make no to the performance of the compared with using the outputs given by the most probable parameters since both functions pass through at amp but these bayesian outputs will make a if for example there is an option of saying i don t know in addition to saying i guess and i guess and even if there are just the two choices and if the costs associated with error are unequal then the decision boundary will be some contour other than the contour and the boundary will be by marginalization. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. postscript on supervised neural networks one of my students robert asked maybe i m missing something fundamental but supervised neural networks seem equivalent to a function to some given data then extrapolating what s the i agree with robert. the supervised neural networks we have studied so far are simply parameterized nonlinear functions which can be to data. hopefully you will agree with another comment that robert made unsupervised networks seem much more interesting than their supervised counterparts. i m amazed that it works! copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. networks we have now spent three chapters studying the single neuron. the time has come to connect multiple neurons together making the output of one neuron be the input to another so as to make neural networks. neural networks can be divided into two classes on the basis of their con nectivity. figure a feedforward network. a feedback network. feedforward networks. in a feedforward network all the connections are directed such that the network forms a directed acyclic graph. feedback networks. any network that is not a feedforward network will be called a feedback network. in this chapter we will discuss a fully connected feedback network called the network. the weights in the network are constrained to be symmetric i.e. the weight from neuron i to neuron j is equal to the weight from neuron j to neuron i. networks have two applications. first they can act as associative memories. second they can be used to solve optimization problems. we will discuss the idea of associative memory also known as content-addressable memory. hebbian learning in chapter we discussed the contrast between traditional digital memories and biological memories. perhaps the most striking is the associative nature of biological memory. a simple model due to donald hebb captures the idea of associative memory. imagine that the weights between neurons whose activities are positively correlated are increased dwij dt correlationxi xj now imagine that when stimulus m is present example the smell of a banana the activity of neuron m increases and that neuron n is associated copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. networks with another stimulus n example the sight of a yellow object. if these two stimuli a yellow sight and a banana smell co-occur in the environment then the hebbian learning rule will increase the weights wnm and wmn. this means that when on a later occasion stimulus n occurs in isolation making the activity xn large the positive weight from n to m will cause neuron m also to be activated. thus the response to the sight of a yellow object is an automatic association with the smell of a banana. we could call this pattern completion no teacher is required for this associative memory to work. no signal is needed to indicate that a correlation has been detected or that an association should be made. the unsupervised local learning algorithm and the unsupervised local activity rule spontaneously produce associative memory. this idea seems so simple and so that it must be relevant to how memories work in the brain. of the binary network convention for weights. our convention in general will be that wij denotes the connection from neuron j to neuron i. architecture. a network consists of i neurons. they are fully connected through symmetric bidirectional connections with weights wij wji. there are no self-connections so wii for all i. biases may be included may be viewed as weights from a neuron whose activity is permanently we will denote the activity of neuron i output by xi. activity rule. roughly a network s activity rule is for each neuron to update its state as if it were a single neuron with the threshold activation function xa a a since there is feedback in a network neuron s output is an input to all the other neurons we will have to specify an order for the updates to occur. the updates may be synchronous or asynchronous. synchronous updates all neurons compute their activations ai wijxj then update their states simultaneously to xi asynchronous updates one neuron at a time computes its activation and updates its state. the sequence of selected neurons may be a sequence or a random sequence. the properties of a network may be sensitive to the above choices. learning rule. the learning rule is intended to make a set of desired memories fxng be stable states of the network s activity rule. each memory is a binary pattern with xi copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. of the continuous network moscow------russia lima----------peru london-----england tokyo--------japan edinburgh-scotland ottawa------canada oslo--------norway stockholm---sweden paris-------france moscow--- moscow------russia ottawa------canada otowa-------canada ottawa------canada egindurrh-sxotland edinburgh-scotland the weights are set using the sum of outer products or hebb rule wij xn i xn j where is an unimportant constant. to prevent the largest possible weight from growing with n we might choose to set exercise explain why the value of is not important for the network above. of the continuous network figure associative memory a list of desired memories. the purpose of an associative memory is pattern completion given a partial pattern. the second purpose of a memory is error correction. ai wijxj using the identical architecture and learning rule we can a network whose activities are real numbers between and activity rule. a network s activity rule is for each neuron to update its state as if it were a single neuron with a sigmoid activation function. the updates may be synchronous or asynchronous and involve the equations and xi tanhai the learning rule is the same as in the binary network but the value of becomes relevant. alternatively we may and introduce a gain into the activation function xi exercise where have we encountered equations and before? convergence of the network the hope is that the networks we have will perform associative memory recall as shown schematically in we hope that the activity rule of a network will take a partial memory or a corrupted memory and perform pattern completion or error correction to restore the original memory. but why should we expect any pattern to be stable under the activity rule let alone the desired memories? we address the continuous network since the binary network is a special case of it. we have already encountered the activity rule copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. networks when we discussed variational methods when we approximated the spin system whose energy function was with a separable distribution ex j hnxn zq qx a jmnxmxn anxn! exp xn qx aex j am xn jmn hm! we found that the pair of iterative equations and optimized the latter so as to minimize the variational free energy qx a ln qx a and were guaranteed to decrease the variational free energy tanhan jmn hn h if we simply replace j by w by x and hn by we see that the equations of the network are identical to a set of equations that minimize xtwx h there is a general name for a function that decreases under the dynamical evolution of a system and that is bounded below such a function is a lyapunov function for the system. it is useful to be able to prove the existence of lyapunov functions if a system has a lyapunov function then its dynamics are bound to settle down to a point which is a local minimum of the lyapunov function or a limit cycle along which the lyapunov function is a constant. chaotic behaviour is not possible for a system with a lyapunov function. if a system has a lyapunov function then its state space can be divided into basins of attraction one basin associated with each attractor. so the continuous network s activity rules implemented asynchronously have a lyapunov function. this lyapunov function is a convex function of each parameter ai so a network s dynamics will always converge to a stable point. this convergence proof depends crucially on the fact that the network s connections are symmetric. it also depends on the updates being made asynchronously. exercise show by constructing an example that if a feedback network does not have symmetric connections then its dynamics may fail to converge to a point. exercise show by constructing an example that if a network is updated synchronously that from some initial conditions it may fail to converge to a point. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. convergence of the network figure binary network storing four memories. the four memories and the weight matrix. initial states that by one two three four or even bits from a desired memory are restored to that memory in one or two iterations. some initial conditions that are far from the memories lead to stable states other than the four memories in the stable state looks like a mixture of two memories d and j stable state is like a mixture of j and c in we a corrupted version of the m memory bits distant in a corrupted version of j bits distant and in a state which looks spurious until we recognize that it is the inverse of the stable state copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. networks the associative memory in action figure shows the dynamics of a binary network that has learnt four patterns by hebbian learning. the four patterns are displayed as by binary images in for twelve initial conditions panels show the state of the network iteration by iteration all units being updated asynchronously in each iteration. for an initial condition randomly perturbed from a memory it often only takes one iteration for all the errors to be corrected. the network has more stable states in addition to the four desired memories the inverse of any stable state is also a stable state and there are several stable states that can be interpreted as mixtures of the memories. brain damage the network can be severely damaged and still work as an associative memory. if we take the weights of the network shown in and randomly set or of them to zero we still that the desired memories are attracting stable states. imagine a digital computer that still works even when of its components are destroyed! exercise implement a network and this amazing robust error-correcting capability. more memories we can squash more memories into the network too. figure shows a set of memories. when we train the network with hebbian learning all memories are stable states even when of the weights are randomly deleted shown by the x s in the weight matrix. however the basins of attraction are smaller than before show the dynamics resulting from randomly chosen starting states close to each of the memories bits only three of the memories are recovered correctly. if we try to store too many patterns the associative memory fails catastrophically. when we add a sixth pattern as shown in only one of the patterns is stable the others all into one of two spurious stable states. the continuous-time continuous network the fact that the network s properties are not robust to the minor change from asynchronous to synchronous updates might be a cause for concern can this model be a useful model of biological networks? it turns out that once we move to a continuous-time version of the networks this issue melts away. we assume that each neuron s activity xi is a continuous function of time xit and that the activations ait are computed instantaneously in accordance with wijxjt ait the neuron s response to its activation is assumed to be mediated by the equation d dt xit f copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the continuous-time continuous network x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x figure network storing memories and deletion of of its weights. the memories and the weights of the network with deleted weights shown by x initial states that by three random bits from a memory some are restored but some converge to other states. desired memories figure an overloaded network trained on six memories most of which are not stable. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. desired memories moscow------russia lima----------peru london-----england tokyo--------japan edinburgh-scotland ottawa------canada oslo--------norway stockholm---sweden paris-------france w attracting stable states moscow------russia lima----------peru londog-----englard tonco--------japan edinburgh-scotland oslo--------norway stockholm---sweden paris-------france wzkmhewn--xqwqwpoq paris-------sweden ecnarf-------sirap where f is the activation function for example f tanha. for a steady activation ai the activity xit relaxes exponentially to f with time-constant now here is the nice result as long as the weight matrix is symmetric this system has the variational free energy as its lyapunov function. exercise by computing d dt prove that the variational free energy is a lyapunov function for the continuous-time network. networks figure failure modes of a network schematic. a list of desired memories and the resulting list of attracting stable states. notice some memories that are retained with a small number of errors desired memories that are completely lost is no attracting stable state at the desired memory or near it spurious stable states unrelated to the original list spurious stable states that are confabulations of desired memories. it is particularly easy to prove that a function l is a lyapunov function if the system s dynamics perform steepest descent on l with d l. in the case of the continuous-time continuous network it is not quite so simple but every component of d which means that with an appropriately metric the network dynamics do perform steepest descents on dt xit dt xit does have the same sign as the capacity of the network one way in which we viewed learning in the single neuron was as communication communication of the labels of the training data set from one point in time to a later point in time. we found that the capacity of a linear threshold neuron was bits per weight. similarly we might view the associative memory as a communication channel a list of desired memories is encoded into a set of weights w using the hebb rule of equation or perhaps some other learning rule. the receiver receiving the weights w only the stable states of the network which he interprets as the original memories. this communication system can fail in various ways as illustrated in the individual bits in some memories might be corrupted that is a stable state of the network is displaced a little from the desired memory. entire memories might be absent from the list of attractors of the network or a stable state might be present but have such a small basin of attraction that it is of no use for pattern completion and error correction. spurious additional memories unrelated to the desired memories might be present. spurious additional memories derived from the desired memories by op erations such as mixing and inversion may also be present. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the capacity of the network of these failure modes modes and are clearly undesirable mode especially so. mode might not matter so much as long as each of the desired memories has a large basin of attraction. the fourth failure mode might in some contexts actually be viewed as for example if a network is required to memorize examples of valid sentences such as john loves mary and john gets cake we might be happy to that john loves cake was also a stable state of the network. we might call this behaviour generalization the capacity of a network with i neurons might be to be the number of random patterns n that can be stored without failure-mode having substantial probability. if we also require failure-mode to have tiny probability then the resulting capacity is much smaller. we now study these alternative of the capacity. the capacity of the network stringent we will explore the information storage capabilities of a binary network that learns using the hebb rule by considering the stability of just one bit of one of the desired patterns assuming that the state of the network is set to that desired pattern xn. we will assume that the patterns to be stored are randomly selected binary patterns. the activation of a particular neuron i is wijxn j ai where the weights are for i j wij xn i xn j xm i xm j here we have split w into two terms the of which will contribute signal reinforcing the desired memory and the second noise substituting for wij the activation is ai xn i xn j xn j xm i xm j xn j xm i xm j xn j i the term is times the desired state xn second term is a sum of random quantities xm if this were the only term it would keep the neuron clamped in the desired state. the a moment s that these quantities are independent random binary variables with mean and variance i xm j xn j i thus considering the statistics of ai under the ensemble of random patterns we conclude that ai has mean and variance for brevity we will now assume i and n are large enough that we can neglect the distinction between i and i and between n and n then we can restate our conclusion ai is gaussian-distributed with mean ixn and variance in i i what then is the probability that the selected bit is stable if we put the network into the state xn? the probability that bit i will on the iteration of the network s dynamics is p unstable i pni! pin i ai figure the probability density of the activation ai in the case xn i the probability that bit i becomes is the area of the tail. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. networks figure overlap between a desired memory and the stable state nearest to it as a function of the loading fraction ni. the overlap is to be the scaled i which is when recall is perfect and zero when the stable state has of the bits there is an abrupt transition at ni where the overlap drops from to zero. inner productpi xixn where z dz the important quantity ni is the ratio of the number of patterns stored to the number of neurons. if for example we try to store n patterns in the network then there is a chance of that a bit in a pattern will be unstable on the iteration. we are now in a position to derive our capacity result for the case where no corruption of the desired memories is permitted. exercise assume that we wish all the desired patterns to be completely stable we don t want any of the bits to when the network is put into any desired pattern state and the total probability of any error at all is required to be less than a small number using the approximation to the error function for large z z show that the maximum number of patterns that can be stored nmax is nmax ln i i if however we allow a small amount of corruption of memories to occur the number of patterns that can be stored increases. the statistical physicists capacity the analysis that led to equation tells us that if we try to store n patterns in the network then starting from a desired memory about of the bits will be unstable on the iteration. our analysis does not shed light on what is expected to happen on subsequent iterations. the of these bits might make some of the other bits unstable too causing an increasing number of bits to be this process might lead to an avalanche in which the network s state ends up a long way from the desired memory. in fact when ni is large such avalanches do happen. when ni is small they tend not to there is a stable state near to each desired memory. for the limit of large i amit et al. have used methods from statistical physics to numerically the transition between these two behaviours. there is a sharp discontinuity at ncrit copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. improving on the capacity of the hebb rule below this critical value there is likely to be a stable state near every desired memory in which a small fraction of the bits are when ni exceeds the system has only spurious stable states known as spin glass states none of which is correlated with any of the desired memories. just below the critical value the fraction of bits that are when a desired memory has evolved to its associated stable state is figure shows the overlap between the desired memory and the nearest stable state as a function of ni. some other transitions in properties of the model occur at some additional values of ni as summarized below. for all ni stable spin glass states exist uncorrelated with the desired memories. for ni these spin glass states are the only stable states. for ni there are stable states close to the desired memories. for ni the stable states associated with the desired memories have lower energy than the spurious spin glass states. for ni the spin glass states dominate there are spin glass states that have lower energy than the stable states associated with the desired memories. for ni there are additional mixture states which are combinations of several desired memories. these stable states do not have as low energy as the stable states associated with the desired memories. in conclusion the capacity of the network with i neurons if we the capacity in terms of the abrupt discontinuity discussed above is random binary patterns each of length i each of which is received with of its bits in bits this capacity is i bits since there are i weights in the network we can also express the capacity as bits per weight. improving on the capacity of the hebb rule the capacities discussed in the previous section are the capacities of the network whose weights are set using the hebbian learning rule. we can do better than the hebb rule by an objective function that measures how well the network stores all the memories and minimizing it. for an associative memory to be useful it must be able to correct at least one bit. let s make an objective function that measures whether bits tend to be restored correctly. our intention is that for every neuron i in the network the weights to that neuron should satisfy this rule this expression for the capacity omits a smaller negative term of order n n bits associated with the arbitrary order of the memories. for every pattern xn if the neurons other than i are set correctly to xj xn then the activation of neuron i should be such that its preferred output is xi xn j i is this rule a familiar idea? yes it is precisely what we wanted the single neuron of chapter to do. each pattern xn an input target pair for the single neuron i. and it an input target pair for all the other neurons too. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. networks algorithm octave source code for optimizing the weights of a network so that it works as an associative memory. cf. algorithm the data matrix x has i columns and n rows. the matrix t is identical to x except that are replaced by w x x initialize the weights using hebb rule for l loop l times for wii end ensure the self-weights are zero. a y e gw x e gw gw gw x w sigmoida t y compute all activations compute all outputs compute all errors compute the gradients symmetrize gradients w w eta gw alpha w make step endfor so just as we an objective function for the training of a single neuron as a we can gw xn tn i ln yn i tn i yn i where and tn i xn i xn i yn i i where an i wijxn j we can then steal the algorithm which we wrote for the single neuron to write an algorithm for optimizing a network algorithm the convenient syntax of octave requires very few changes the extra lines enforce the constraints that the self-weights wii should all be zero and that the weight matrix should be symmetrical wji. as expected this learning algorithm does a better job than the one-shot hebbian learning rule. when the six patterns of which cannot be memorized by the hebb rule are learned using algorithm all six patterns become stable states. exercise implement this learning rule and investigate empirically its capacity for memorizing random patterns also compare its avalanche properties with those of the hebb rule. networks for optimization problems since a network s dynamics minimize an energy function it is natural to ask whether we can map interesting optimization problems onto networks. biological data processing problems often involve an element of constraint satisfaction in scene interpretation for example one might wish to infer the spatial location orientation brightness and texture of each visible element and which visible elements are connected together in objects. these inferences are constrained by the given data and by prior knowledge about continuity of objects. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. networks for optimization problems place in tour place in tour city a b c d a c city a b c d a c b d b d figure network for solving a travelling salesman problem with k cities. two solution states of the network with activites represented by black white and the tours corresponding to these network states. the negative weights between node and other nodes these weights enforce validity of a tour. the negative weights that embody the distance objective function. a b c d a b c d and tank suggested that one might take an interesting constraint satisfaction problem and design the weights of a binary or continuous network such that the settling process of the network would minimize the objective function of the problem. the travelling salesman problem a classic constraint satisfaction problem to which networks have been applied is the travelling salesman problem. a set of k cities is given and a matrix of the distances between those cities. the task is to a closed tour of the cities visiting each city once that has the smallest total distance. the travelling salesman problem is equivalent in to an np-complete problem. the method suggested by and tank is to represent a tentative solution to the problem by the state of a network with i k neurons arranged in a square with each neuron representing the hypothesis that a particular city comes at a particular point in the tour. it will be convenient to consider the states of the neurons as being between and rather than and two solution states for a four-city travelling salesman problem are shown in the weights in the network play two roles. first they must an energy function which is minimized only when the state of the network represents a valid tour. a valid state is one that looks like a permutation matrix having exactly one in every row and one in every column. this rule can be enforced by putting large negative weights between any pair of neurons that are in the same row or the same column and setting a positive bias for all neurons to ensure that k neurons do turn on. figure shows the negative weights that are connected to one neuron which represents the statement city b comes second in the tour second the weights must encode the objective function that we want to minimize the total distance. this can be done by putting negative weights proportional to the appropriate distances between the nodes in adjacent columns. for example between the b and d nodes in adjacent columns the weight would be the negative weights that are connected to neuron are shown in the result is that when the network is in a valid state its total energy will be the total distance of the corresponding copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. networks figure evolution of the state of a continuous network solving a travelling salesman problem using aiyer s graduated non-convexity method the state of the network is projected into the two-dimensional space in which the cities are located by the centre of mass for each point in the tour using the neuron activities as the mass function. the travelling scholar problem. the shortest tour linking the cambridge colleges the engineering department the university library and sree aiyer s house. from aiyer tour plus a constant given by the energy associated with the biases. now since a network minimizes its energy it is hoped that the binary or continuous network s dynamics will take the state to a minimum that is a valid tour and which might be an optimal tour. this hope is not for large travelling salesman problems however without some careful we have not the size of the weights that enforce the tour s validity relative to the size of the distance weights and setting this scale factor poses if large validity-enforcing weights are used the network s dynamics will rattle into a valid state with little regard for the distances. if small validity-enforcing weights are used it is possible that the distance weights will cause the network to adopt an invalid state that has lower energy than any valid state. our original formulation of the energy function puts the objective function and the solution s validity in potential with each other. this has been resolved by the work of sree aiyer who showed how to modify the distance weights so that they would not interfere with the solution s validity and how to a continuous network whose dynamics are at all times to a valid subspace aiyer used a graduated non-convexity or deterministic annealing approach to good solutions using these networks. the deterministic annealing approach involves gradually increasing the gain of the neurons in the network from to at which point the state of the network corresponds to a valid tour. a sequence of trajectories generated by applying this method to a thirtycity travelling salesman problem is shown in a solution to the travelling scholar problem found by aiyer using a con tinuous network is shown in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises further exercises exercise storing two memories. two binary memories m and n ni are stored by hebbian learning in a network using wij mimj ninj for i j for i j. the biases bi are set to zero. the network is put in the state x m. evaluate the activation ai of neuron i and show that in can be written in the form ai by comparing the signal strength with the magnitude of the noise strength show that x m is a stable state of the dynamics of the network. the network is put in a state x in d places from m x m where the perturbation d di d is the number of components of d that are non-zero and for each di that is non-zero di the overlap between m and n to be omn mini i evaluate the activation ai of neuron i again and show that the dynamics of the network will restore x to m if the number of bits d jomnj how does this number compare with the maximum number of bits that can be corrected by the optimal decoder assuming the vector x is either a noisy version of m or of n? exercise network as a collection of binary this exercise explores the link between unsupervised networks and supervised networks. if a network s desired memories are all attracting stable states then every neuron in the network has weights going to it that solve a problem personal to that neuron. take the set of memories and write them in the form xn where denotes all the components for all i and let denote the vector of weights for i. using what we know about the capacity of the single neuron show that it is almost certainly impossible to store more than random memories in a network of i neurons. i copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. lyapunov functions exercise erik s puzzle. in a stripped-down version of conway s game of life cells are arranged on a square grid. each cell is either alive or dead. live cells do not die. dead cells become alive if two or more of their immediate neighbours are alive. to north south east and west. what is the smallest number of live cells needed in order that these rules lead to an entire n n square being alive? in a d-dimensional version of the same game the rule is that if d neighbours are alive then you come to life. what is the smallest number of live cells needed in order that an entire n n n hypercube becomes alive? how should those live cells be arranged? networks figure erik s dynamics. the southeast puzzle u u? u? u? u u uu u eee ee e eeee the southeast puzzle is played on a chess board starting at its northwest left corner. there are three rules figure the southeast puzzle. in the starting position one piece is placed in the northwest-most square it is not permitted for more than one piece to be on any given square. at each step you remove one piece from the board and replace it with two pieces one in the square immediately to the east and one in the the square immediately to the south as illustrated in every such step increases the number of pieces on the board by one. after move has been made either piece may be selected for the next move. figure shows the outcome of moving the lower piece. at the next move either the lowest piece or the middle piece of the three may be selected the uppermost piece may not be selected since that would violate rule at move we have selected the middle piece. now any of the pieces may be moved except for the leftmost piece. now here is the puzzle exercise is it possible to obtain a position in which all the ten squares closest to the northwest corner marked in are empty? this puzzle has a connection to data compression. solutions solution to exercise take a binary feedback network with neurons and let and then whenever neuron is updated it will match neuron and whenever neuron is updated it will to the opposite state from neuron there is no stable state. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions solution to exercise take a binary network with neurons and let and let the initial condition be then if the dynamics are synchronous on every iteration both neurons will their state. the dynamics do not converge to a point. solution to exercise the key to this problem is to notice its similarity to the construction of a binary symbol code. starting from the empty string we can build a binary tree by repeatedly splitting a codeword into two. every codeword has an implicit probability where l is the depth of the codeword in the binary tree. whenever we split a codeword in two and create two new codewords whose length is increased by one the two new codewords each have implicit probability equal to half that of the old codeword. for a complete binary code the kraft equality that the sum of these implicit probabilities is similarly in southeast we can associate a weight with each piece on the board. if we assign a weight of to any piece sitting on the top left square a weight of to any piece on a square whose distance from the top left is one a weight of to any piece whose distance from the top left is two and so forth with distance being the city-block distance then every legal move in southeast leaves unchanged the total weight of all pieces on the board. lyapunov functions come in two the function may be a function of state whose value is known to stay constant or it may be a function of state that is bounded below and whose value always decreases or stays constant. the total weight is a lyapunov function of the second type. the starting weight is so now we have a powerful tool a conserved function of the state. is it possible to a position in which the ten highestweight squares are vacant and the total weight is what is the total weight if all the other squares on the board are occupied the total weight would be which is equal to so it is impossible to empty all ten of those squares. uuuuu uu uu uuuuuu figure a possible position for the southeast puzzle? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. boltzmann machines from networks to boltzmann machines we have noticed that the binary network minimizes an energy function xtwx ex and that the continuous network with activation function xn tanhan can be viewed as approximating the probability distribution associated with that energy function p w zw zw these observations motivate the idea of working with a neural network model that actually implements the above probability distribution. the stochastic network or boltzmann machine and se jnowski has the following activity rule activity rule of boltzmann machine after computing the activa tion ai set xi with probability else set xi this rule implements gibbs sampling for the probability distribution boltzmann machine learning given a set of examples fxngn in adjusting the weights w such that the generative model from the real world we might be interested p w zw is well matched to those examples. we can derive a learning algorithm by writing down bayes theorem to obtain the posterior probability of the weights given the data p jfxngn g n p j w p p g copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. from networks to boltzmann machines we concentrate on the term in the numerator the likelihood and derive a maximum likelihood algorithm there might be advantages in pursuing a full bayesian approach as we did in the case of the single neuron. we the logarithm of the likelihood ln n p j w n xnt wxn ln with respect to wij bearing in mind that w is to be symmetric with wji wij. exercise show that the derivative of ln zw with respect to wij is ln zw xixjp w hxixjip w exercise is similar to exercise the derivative of the log likelihood is therefore ln p gj w n i xn j hxixjip j wi nhhxixjidata hxixjip wi this gradient is proportional to the of two terms. the term is the empirical correlation between xi and xj hxixjidata n n i xn j i and the second term is the correlation between xi and xj under the current model hxixjip j w xixjp w the correlation hxixjidata is readily evaluated it is just the empirical correlation between the activities in the real world. the second correlation hxixjip j w is not so easy to evaluate but it can be estimated by monte carlo methods that is by observing the average value of xixj while the activity rule of the boltzmann machine equation is iterated. in the special case w we can evaluate the gradient exactly because by symmetry the correlation hxixjip j w must be zero. if the weights are adjusted by gradient descent with learning rate then after one iteration the weights will be wij n i xn j i precisely the value of the weights given by the hebb rule equation with which we trained the network. interpretation of boltzmann machine learning one way of viewing the two terms in the gradient is as waking and sleeping rules. while the network is awake it measures the correlation between xi and xj in the real world and weights are increased in proportion. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. boltzmann machines figure the shifter ensembles. four samples from the plain shifter ensemble. four corresponding samples from the labelled shifter ensemble. while the network is asleep it dreams about the world using the generative model and measures the correlations between xi and xj in the model world these correlations determine a proportional decrease in the weights. if the second-order correlations in the dream world match the correlations in the real world then the two terms balance and the weights do not change. criticism of networks and simple boltzmann machines up to this point we have discussed networks and boltzmann machines in which all of the neurons correspond to visible variables xi. the result is a probabilistic model that when optimized can capture the second-order statistics of the environment. second-order statistics of an ensemble p are the expected values hxixji of all the pairwise products xixj. the real world however often has higher-order correlations that must be included if our description of it is to be often the second-order correlations in themselves may carry little or no useful information. consider for example the ensemble of binary images of chairs. we can imagine images of chairs with various designs four-legged chairs comfy chairs chairs with legs and wheels wooden chairs cushioned chairs chairs with rockers instead of legs. a child can easily learn to distinguish these images from images of carrots and parrots. but i expect the second-order statistics of the raw data are useless for describing the ensemble. second-order statistics only capture whether two pixels are likely to be in the same state as each other. higher-order concepts are needed to make a good generative model of images of chairs. a simpler ensemble of images in which high-order statistics are important is the shifter ensemble which comes in two figure shows a few samples from the plain shifter ensemble in each image the bottom eight pixels are a copy of the top eight pixels either shifted one pixel to the left or unshifted or shifted one pixel to the right. top eight pixels are set at random. this ensemble is a simple model of the visual signals from the two eyes arriving at early levels of the brain. the signals from the two eyes are similar to each other but may by small translations because of the varying depth of the visual world. this ensemble is simple to describe but its second-order statistics convey no useful information. the correlation between one pixel and any of the three pixels above it is the correlation between any other two pixels is zero. figure shows a few samples from the labelled shifter ensemble here the problem has been made easier by including an extra three neurons that label the visual image as being an instance of either the shift left no shift or shift right sub-ensemble. but with this extra information the ensemble is still not learnable using second-order statistics alone. the secondorder correlation between any label neuron and any image neuron is zero. we need models that can capture higher-order statistics of an environment. so how can we develop such models? one idea might be to create models that directly capture higher-order correlations such as p w v wijxixj vijkxixjxk a such higher-order boltzmann machines are equally easy to simulate using stochastic updates and the learning rule for the higher-order parameters vijk is equivalent to the learning rule for wij. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. boltzmann machine with hidden units exercise derive the gradient of the log likelihood with respect to vijk. it is possible that the spines found on biological neurons are responsible for detecting correlations between small numbers of incoming signals. however to capture statistics of high enough order to describe the ensemble of images of chairs well would require an unimaginable number of terms. to capture merely the fourth-order statistics in a pixel image we need more than parameters. so measuring moments of images is not a good way to describe their underlying structure. perhaps what we need instead or in addition are hidden variables also known to statisticians as latent variables. this is the important innovation introduced by hinton and sejnowski the idea is that the high-order correlations among the visible variables are described by including extra hidden variables and sticking to a model that has only second-order interactions between its variables the hidden variables induce higher-order correlations between the visible variables. boltzmann machine with hidden units we now add hidden neurons to our stochastic model. these are neurons that do not correspond to observed variables they are free to play any role in the probabilistic model by equation they might actually take on interpretable roles performing feature extraction learning in boltzmann machines with hidden units the activity rule of a boltzmann machine with hidden units is identical to that of the original boltzmann machine. the learning rule can again be derived by maximum likelihood but now we need to take into account the fact that the states of the hidden units are unknown. we will denote the states of the visible units by x the states of the hidden units by h and the generic state of a neuron visible or hidden by yi with y h. the state of the network when the visible neurons are clamped in state xn is yn h. the likelihood of w given a single data example xn is p j w where p hj w zw equation may also be written zw where p j w zxnw zxn zw the likelihood as before we that the derivative with respect to any weight wij is again the between a waking term and a sleeping term ln p j w nhyiyjip j xnw hyiyjip wo copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. boltzmann machines the term hyiyjip j xnw is the correlation between yi and yj if the boltzmann machine is simulated with the visible variables clamped to xn and the hidden variables freely sampling from their conditional distribution. the second term hyiyjip w is the correlation between yi and yj when hinton and sejnowski demonstrated that non-trivial ensembles such as the labelled shifter ensemble can be learned using a boltzmann machine with hidden units. the hidden units take on the role of feature detectors that spot patterns likely to be associated with one of the three shifts. the boltzmann machine generates samples from its model distribution. the boltzmann machine is time-consuming to simulate because the computation of the gradient of the log likelihood depends on taking the of two gradients both found by monte carlo methods. so boltzmann machines are not in widespread use. it is an area of active research to create models that embody the same capabilities using more computations et al. dayan et al. hinton and ghahramani hinton hinton and teh exercise exercise can the bars and stripes ensemble be learned by a boltzmann machine with no hidden units? may be surprised! figure four samples from the bars and stripes ensemble. each sample is generated by picking an orientation horizontal or vertical then for each row of spins in that orientation bar or stripe respectively switching all spins on with probability copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. supervised learning in multilayer networks multilayer perceptrons no course on neural networks could be complete without a discussion of supervised multilayer networks also known as backpropagation networks. the multilayer perceptron is a feedforward network. it has input neurons hidden neurons and output neurons. the hidden neurons may be arranged in a sequence of layers. the most common multilayer perceptrons have a single hidden layer and are known as two-layer networks the number two counting the number of layers of neurons not including the inputs. such a feedforward network a nonlinear parameterized mapping from an input x to an output y yx wa. the output is a continuous function of the input and of the parameters w the architecture of the net i.e. the functional form of the mapping is denoted by a. feedforward networks can be trained to perform regression and tasks. regression networks in the case of a regression problem the mapping for a network with one hidden layer may have the form hidden layer output layer j i jl xl j hj f j ij hj i yi f i where for example f tanha and f a. here l runs over the inputs xl j runs over the hidden units and i runs over the outputs. the weights w and biases together make up the parameter vector w. the nonlinear sigmoid function f at the hidden layer gives the neural network greater computational than a standard linear regression model. graphically we can represent the neural network as a set of layers of connected neurons what sorts of functions can these networks implement? just as we explored the weight space of the single neuron in chapter examining the functions it could produce let us explore the weight space of a multilayer network. in and i take a network with one input and one output and a large number h of hidden units set the biases outputs hiddens inputs figure a typical two-layer network with six inputs seven hidden units and three outputs. each line represents one weight. figure samples from the prior over functions of a one-input network. for each of a sequence of values of and is shown. the other hyperparameters of the network were h bias one random function out copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. output ty ttttt t tx hidden layer input supervised learning in multilayer networks t u p t u o input figure properties of a function produced by a random network. the vertical scale of a typical function produced by the network with random weights is of order the horizontal range in which the function varies is of order and the shortest horizontal length scale is of order the function shown was produced by making a random network with h hidden units and gaussian weights with and i j jl and ij to random values and plot the resulting i set the hidden units biases and weights j function yx. to random values from a gaussian with zero mean and standard deviation the input-to-hidden weights to random values with standard deviation and the bias and jl output weights to random values with standard deviation the sort of functions that we obtain depend on the values of and as the weights and biases are made bigger we obtain more complex functions with more features and a greater sensitivity to the input variable. the vertical scale of a typical function produced by the network with random weights is of order the horizontal range in which the function varies is of order and the shortest horizontal length scale is of order and ij i radford neal has also shown that in the limit as h the statistical properties of the functions generated by randomizing the weights are independent of the number of hidden units so interestingly the complexity of the functions becomes independent of the number of parameters in the model. what determines the complexity of the typical functions is the characteristic magnitude of the weights. thus we anticipate that when we these models to real data an important way of controlling the complexity of the function will be to control the characteristic magnitude of the weights. figure shows one typical function produced by a network with two inputs and one output. this should be contrasted with the function produced by a traditional linear regression model which is a plane. neural networks can create functions with more complexity than a linear regression. how a regression network is traditionally trained this network is trained using a data set d fxn tng by adjusting w so as to minimize an error function e.g. edw xi i yixn this objective function is a sum of terms one for each inputtarget pair fx tg measuring how close the output yx w is to the target t. this minimization is based on repeated evaluation of the gradient of ed. this gradient can be computed using the backpropagation algorithm et al. which uses the chain rule to the derivatives. figure one sample from the prior of a two-input network with fh outg bias in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. neural network learning as inference often regularization known as weight decay is included modifying the objective function to m where for example ew i this additional term favours small values of w and decreases the tendency of a model to noise in the training data. rumelhart et al. showed that multilayer perceptrons can be trained by gradient descent on m to discover solutions to non-trivial problems such as deciding whether an image is symmetric or not. these networks have been successfully applied to real-world tasks as varied as pronouncing english text and rosenberg and focussing multiple-mirror telescopes et al. neural network learning as inference the neural network learning process above can be given the following probabilistic interpretation. we repeat and generalize the discussion of chapter the error function is interpreted as a noise model. is the negative log likelihood p j w thus the use of the sum-squared error ed corresponds to an assumption of gaussian noise on the target variables and the parameter a noise level similarly the regularizer is interpreted in terms of a log prior probability distribution over the parameters p j zw if ew is quadratic as above then the corresponding prior distribution is a gaussian with variance w the probabilistic model h the architecture a of the network the likelihood and the prior the objective function m then corresponds to the inference of the parameters w given the data p j d p j w j p j zm the w found by minimizing m is then interpreted as the most probable parameter vector wmp. the interpretation of m as a log probability adds little new at this stage. but new tools will emerge when we proceed to other inferences. first though let us establish the probabilistic interpretation of networks to which the same tools apply. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. supervised learning in multilayer networks binary networks if the targets t in a data set are binary labels it is natural to use a neural network whose output yx wa is bounded between and and is interpreted as a probability p j x wa. for example a network with one hidden layer could be described by the feedforward equations and with f the error function is replaced by the negative log likelihood gw tn ln yxn w tn yxn w the total objective function is then m g note that this includes no parameter there is no gaussian noise. multi-class networks for a multi-class problem we can represent the targets by a vector t in which a single element is set to indicating the correct class and all other elements are set to in this case it is appropriate to use a softmax network having coupled outputs which sum to one and are interpreted as class probabilities yi p j x wa. the last part of equation is replaced by eai yi the negative log likelihood in this case is gw xi tn i ln yixn w as in the case of the regression network the minimization of the objective function m g corresponds to an inference of the form a variety of useful results can be built on this interpretation. of the bayesian approach to supervised feedforward neural networks from the statistical perspective supervised neural networks are nothing more than nonlinear devices. curve is not a trivial task however. the complexity of an interpolating model is of crucial importance as illustrated in consider a control parameter that the complexity of a model for example a regularization constant decay parameter. as the control parameter is varied to increase the complexity of the model from and going from left to right across the best to the training data that the model can achieve becomes increasingly good. however the empirical performance of the model the test error decreases then increases again. an over-complex model the data and generalizes poorly. this problem may also complicate the choice of architecture in a multilayer perceptron the radius of the basis functions in a radial basis function network and the choice of the input variables themselves in any multidimensional regression problem. finding values for model control parameters that are appropriate for the data is therefore an important and non-trivial problem. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. of the bayesian approach to supervised feedforward neural networks figure optimization of model complexity. panels show a radial basis function model interpolating a simple data set with one input variable and one output variable. as the regularization constant is varied to increase the complexity of the model to the interpolant is able to the training data increasingly well but beyond a certain point the generalization ability error of the model deteriorates. probability theory allows us to optimize the control parameters without needing a test set. test error training error model control parameters log probabilitytraining data control parameters model control parameters the problem can be solved by using a bayesian approach to control model complexity. if we give a probabilistic interpretation to the model then we can evaluate the evidence for alternative values of the control parameters. as was explained in chapter over-complex models turn out to be less probable and the evidence p j control parameters can be used as an objective function for optimization of model control parameters the setting of that maximizes the evidence is displayed in bayesian optimization of model control parameters has four important advantages. no test set or validation set is involved so all available training data can be devoted to both model and model comparison. regularization constants can be optimized on-line i.e. simultaneously with the optimization of ordinary model parameters. the bayesian objective function is not noisy in contrast to a cross-validation measure. the gradient of the evidence with respect to the control parameters can be evaluated making it possible to simultaneously optimize a large number of control parameters. probabilistic modelling also handles uncertainty in a natural manner. it a unique prescription marginalization for incorporating uncertainty about parameters into predictions this procedure yields better predictions as we saw in chapter figure shows error bars on the predictions of a trained neural network. implementation of bayesian inference as was mentioned in chapter bayesian inference for multilayer networks may be implemented by monte carlo sampling or by deterministic methods employing gaussian approximations mackay figure error bars on the predictions of a trained regression network. the solid line gives the predictions of the parameters of a multilayer perceptron trained on the data points. the error bars lines are those produced by the uncertainty of the parameters w. notice that the error bars become larger where the data are sparse. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. supervised learning in multilayer networks within the bayesian framework for data modelling it is easy to improve our probabilistic models. for example if we believe that some input variables in a problem may be irrelevant to the predicted quantity but we don t know which we can a new model with multiple hyperparameters that captures the idea of uncertain input variable relevance neal mackay these models then infer automatically from the data which are the relevant input variables for a problem. exercises exercise how to measure a s quality. you ve just written a new algorithm and want to measure how well it performs on a test set and compare it with other what performance measure should you use? there are several standard answers. let s assume the gives an output yx where x is the input which we won t discuss further and that the true target value is t. in the simplest discussions of both y and t are binary variables but you might care to consider cases where y and t are more general objects also. the most widely used measure of performance on a test set is the error rate the fraction of made by the this measure forces the to give a output and ignores any additional information that the might be able to for example an indication of the of a prediction. unfortunately the error rate does not necessarily measure how informative a s output is. consider frequency tables showing the joint frequency of the output of a axis and the true variable axis. the numbers that we ll show are percentages. the error rate e is the sum of the two numbers which we could call the false positive rate e and the false negative rate of the following three a and b have the same error rate of and c has a greater error rate of a y t b y c y t t but clearly a which simply guesses that the outcome is for all cases is conveying no information at all about t whereas b has an informative output if y then we are sure that t really is zero and if y then there is a chance that t as compared to the prior probability p c is slightly less informative than b but it is still much more useful than the information-free a. one way to improve on the error rate as a performance measure is to report the pair the false positive error rate and the false negative error rate which are and for a and b. it is especially important to distinguish between these two error probabilities in applications where the two sorts of error have associated costs. however there are a couple of problems with the error rate pair first if i simply told you that a has error rates and b has error rates it would not be immediately evident that a is actually utterly worthless. surely we should have a performance measure that gives the worst possible score to a! how common sense ranks the b c a how error rate ranks the a b c copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. exercises second if we turn to a multiple-class problem such as digit recognition then the number of types of error increases from two to one for each possible confusion of class t with it would be nice to have some sensible way of collapsing these numbers into a single rankable number that makes more sense than the error rate. another reason for not liking the error rate is that it doesn t give a credit for accurately specifying its uncertainty. consider that have three outputs available and a rejection class which indicates that the is not sure. consider d and e with the following frequency tables in percentages d y t e y t both of these have r but are they equally good compare e with c. the two are equivalent. e is just c in disguise we could make e by taking the output of c and tossing a coin when c says in order to decide whether to give output or so e is equal to c and thus inferior to b. now compare d with b. can you justify the suggestion that d is a more informative than b and thus is superior to e? yet d and e have the same r scores. people often plot error-reject curves known as roc curves roc stands for receiver operating characteristic which show the total e versus r as r is allowed to vary from to and use these curves to compare the special case of binary problems e may be plotted versus instead. but as we have seen error rates can be undiscerning performance measures. does plotting one error rate as a function of another make this weakness of error rates go away? for this exercise either construct an explicit example demonstrating that the error-reject curve and the area under it are not necessarily good ways to compare or prove that they are. as a suggested alternative method for comparing consider the mutual information between the output and the target it y ht ht j y p y log p p y which measures how many bits the s output conveys about the target. evaluate the mutual information for ae above. investigate this performance measure and discuss whether it is a useful one. does it have practical drawbacks? error rate rejection rate figure an error-reject curve. some people use the area under this curve as a measure of quality. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter feedforward neural networks such as multilayer perceptrons are popular tools for nonlinear regression and problems. from a bayesian perspective a choice of a neural network model can be viewed as a prior probability distribution over nonlinear functions and the neural network s learning process can be interpreted in terms of the posterior probability distribution over the unknown function. learning algorithms search for the function with maximum posterior probability and other monte carlo methods draw samples from this posterior probability. in the limit of large but otherwise standard networks neal has shown that the prior distribution over nonlinear functions implied by the bayesian neural network falls in a class of probability distributions known as gaussian processes. the hyperparameters of the neural network model determine the characteristic lengthscales of the gaussian process. neal s observation motivates the idea of discarding parameterized networks and working directly with gaussian processes. computations in which the parameters of the network are optimized are then replaced by simple matrix operations using the covariance matrix of the gaussian process. in this chapter i will review work on this idea by williams and rasmussen neal barber and williams and gibbs and mackay and will assess whether for supervised regression and tasks the feedforward network has been superceded. exercise i regret that this chapter is rather dry. there s no simple explanatory examples in it and few pictures. this exercise asks you to create interesting pictures to explain to yourself this chapter s ideas. source code for computer demonstrations written in the free language octave is available at httpwww.inference.phy.cam.ac.ukmackayitprnnsoftware.html. radford neal s software for gaussian processes is available at httpwww.cs.toronto.eduradford. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gaussian processes after the publication of rumelhart hinton and williams s paper on supervised learning in neural networks there was a surge of interest in the empirical modelling of relationships in high-dimensional data using nonlinear parametric models such as multilayer perceptrons and radial basis functions. in the bayesian interpretation of these modelling methods a nonlinear function yx parameterized by parameters w is assumed to underlie the data fxn tngn and the adaptation of the model to the data corresponds to an inference of the function given the data. we will denote the set of input vectors by xn fxngn and the set of corresponding target values by the vector tn ftngn the inference of yx is described by the posterior probability distribution p tn xn p j yx xn p j xn of the two terms on the right-hand side the p j yx xn is the probability of the target values given the function yx which in the case of regression problems is often assumed to be a separable gaussian distribution and the second term p is the prior distribution on functions assumed by the model. this prior is implicit in the choice of parametric model and the choice of regularizers used during the model the prior typically that the function yx is expected to be continuous and smooth and has less high frequency power than low frequency power but the precise meaning of the prior is somewhat obscured by the use of the parametric model. now for the prediction of future values of t all that matters is the assumed prior p and the assumed noise model p j yx xn the parameterization of the function yx w is irrelevant. the idea of gaussian process modelling is to place a prior p directly on the space of functions without parameterizing yx. the simplest type of prior over functions is called a gaussian process. it can be thought of as the generalization of a gaussian distribution over a vector space to a function space of dimension. just as a gaussian distribution is fully by its mean and covariance matrix a gaussian process is by a mean and a covariance function. here the mean is a function of x we will often take to be the zero function and the covariance is a function cx that expresses the expected covariance between the values of the function y at the points x and the function yx in any one data modelling problem is assumed to be a single sample from this gaussian distribution. gaussian processes are already well established models for various spatial and temporal problems for example brownian motion langevin processes and wiener processes are all examples of gaussian processes kalman widely used copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gaussian processes to model speech waveforms also correspond to gaussian process models the method of kriging in geostatistics is a gaussian process regression method. reservations about gaussian processes it might be thought that it is not possible to reproduce the interesting properties of neural network interpolation methods with something so simple as a gaussian distribution but as we shall now see many popular nonlinear interpolation methods are equivalent to particular gaussian processes. use the term interpolation to cover both the problem of regression a curve through noisy data and the task of an interpolant that passes exactly through the given data points. it might also be thought that the computational complexity of inference when we work with priors over function spaces might be large. but by concentrating on the joint probability distribution of the observed data and the quantities we wish to predict it is possible to make predictions with resources that scale as polynomial functions of n the number of data points. standard methods for nonlinear regression the problem we are given n data points xn tn fxn tngn the inputs x are vectors of some input dimension i. the targets t are either real numbers in which case the task will be a regression or interpolation task or they are categorical variables for example t in which case the task is a task. we will concentrate on the case of regression for the time being. assuming that a function yx underlies the observed data the task is to infer the function from the given data and predict the function s value or the value of the observation tn at a new point xn parametric approaches to the problem in a parametric approach to regression we express the unknown function yx in terms of a nonlinear function yx w parameterized by parameters w. example fixed basis functions. using a set of basis functions we can write h yx w if the basis functions are nonlinear functions of x such as radial basis functions centred at points fchgh then yx w is a nonlinear function of x however since the dependence of y on the parameters w is linear we might sometimes refer to this as a linear model. in neural network terms this model is like a multilayer network whose connections from the input layer to the nonlinear hidden layer are only the output weights w are adaptive. other possible sets of basis functions include polynomials such as xp j where p and q are integer powers that depend on h. i xq copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. standard methods for nonlinear regression example adaptive basis functions. alternatively we might make a function yx from basis functions that depend on additional parameters included in the vector w. in a two-layer feedforward neural network with nonlinear hidden units and a linear output the function can be written yx w h h tanh i hi xi where i is the dimensionality of the input space and the weight vector hi g the hidden unit biases g in this model the w consists of the input weights the output weights h g and the output bias dependence of y on w is nonlinear. having chosen the parameterization we then infer the function yx w by inferring the parameters w. the posterior probability of the parameters is p j tn xn p j w xn p j xn the factor p j w xn states the probability of the observed data points when the parameters w hence the function y are known. this probability distribution is often taken to be a separable gaussian each data point tn from the underlying value yxn w by additive noise. the factor p the prior probability distribution of the parameters. this too is often taken to be a separable gaussian distribution. if the dependence of y on w is nonlinear the posterior distribution p j tn xn is in general not a gaussian distribution. the inference can be implemented in various ways. in the laplace method we minimize an objective function m ln j w xn with respect to w locating the locally most probable parameters then use the curvature of m to error bars on w. alternatively we can use more general markov chain monte carlo techniques to create samples from the posterior distribution p j tn xn having obtained one of these representations of the inference of w given the data predictions are then made by marginalizing over the parameters p j tn xn dhw p j w xn j tn xn if we have a gaussian representation of the posterior p j tn xn then this integral can typically be evaluated directly. in the alternative monte carlo approach which generates r samples wr that are intended to be samples from the posterior distribution p j tn xn we approximate the predictive distribution by p j tn xn r r p j wr xn copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gaussian processes nonparametric approaches. in nonparametric methods predictions are obtained without explicitly parameterizing the unknown function yx yx lives in the space of all continuous functions of x. one well known nonparametric approach to the regression problem is the spline smoothing method and wahba a spline solution to a one-dimensional regression problem can be described as follows we the estimator of yx to be the function that minimizes the functional m n dx where yp is the pth derivative of y and p is a positive number. if p is set to then the resulting function is a cubic spline that is a piecewise cubic function that has knots discontinuities in its second derivative at the data points fxng. tifying the prior for the function yx as this estimation method can be interpreted as a bayesian method by iden ln p dx const and the probability of the data measurements tn ftngn pendent gaussian noise as assuming inde ln p tn j yx n const constants in equations and are functions of and respectively. strictly the prior is improper since addition of an arbitrary polynomial of degree to yx is not constrained. this impropriety is easily by the addition of appropriate terms to given this interpretation of the functions in equation m is equal to minus the log of the posterior probability p tn within an additive constant and the splines estimation procedure can be interpreted as yielding a bayesian map estimate. the bayesian perspective allows us additionally to put error bars on the splines estimate and to draw typical samples from the posterior distribution and it gives an automatic method for inferring the hyperparameters and comments splines priors are gaussian processes the prior distribution in equation is our example of a gaussian process. throwing mathematical precision to the winds a gaussian process can be as a probability distribution on a space of functions yx that can be written in the form p a z where is the mean function and a is a linear operator and where the inner product of two functions yxtzx is by for example r dx yxzx. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. standard methods for nonlinear regression here if we denote by d the linear operator that maps yx to the derivative of yx we can write equation as ln p dx const yxtayx const which has the same form as equation with and a ptdp. in order for the prior in equation to be a proper prior a must be a positive operator i.e. one satisfying yxtayx for all functions yx other than yx splines can be written as parametric models splines may be written in terms of an set of basis functions as in equation as follows. first rescale the x axis so that the interval is much wider than the range of x values of interest. let the basis functions be a fourier set fcos hx sin hx h so the function is whcos coshx whsin sinhx yx use the regularizer ew h to a gaussian prior on w p hcos h p hsin p j zw if p then we have the cubic splines regularizer ew dx as if p we have the regularizer ew dx in equation make the prior proper we must add an extra regularizer on the etc. term thus in terms of the prior p there is no fundamental between the nonparametric splines approach and other parametric approaches. representation is irrelevant for prediction from the point of view of prediction at least there are two objects of interest. the is the conditional distribution p j tn xn in equation the other object of interest should we wish to compare one model with others is the joint probability of all the observed data given the model the evidence p j xn which appeared as the normalizing constant in equation neither of these quantities makes any reference to the representation of the unknown function yx. so at the end of the day our choice of representation is irrelevant. the question we now address is in the case of popular parametric models what form do these two quantities take? we will see that for standard models with basis functions and gaussian distributions on the unknown parameters the joint probability of all the observed data given the model p j xn is a multivariate gaussian distribution with mean zero and with a covariance matrix determined by the basis functions this implies that the conditional distribution p j tn xn is also a gaussian distribution whose mean depends linearly on the values of the targets tn standard parametric models are simple examples of gaussian processes. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gaussian processes from parametric models to gaussian processes linear models let us consider a regression problem using h basis functions for example one-dimensional radial basis functions as in equation let us assume that a list of n input points fxng has been and the n h matrix r to be the matrix of values of the basis functions at the points fxng rnh we the vector yn to be the vector of values of yx at the n points yn rnhwh if the prior distribution of w is gaussian with zero mean p normalw wi then y being a linear function of w is also gaussian distributed with mean zero. the covariance matrix of y is q hyyti hrwwtrti rhwwti rt wrrt so the prior distribution of y is p normaly q normaly wrrt this result that the vector of n function values y has a gaussian distribution is true for any selected points xn this is the property of a gaussian process. the probability distribution of a function yx is a gaussian process if for any selection of points xn the density p yxn is a gaussian. now if the number of basis functions h is smaller than the number of data points n then the matrix q will not have full rank. in this case the probability distribution of y might be thought of as a elliptical pancake to an h-dimensional subspace in the n space in which y lives. what about the target values? if each target tn is assumed to by from the corresponding function value additive gaussian noise of variance yn then t also has a gaussian prior distribution p normalt q we will denote the covariance matrix of t by c c q wrrt whether or not q has full rank the covariance matrix c has full rank since is full rank. what does the covariance matrix q look like? in general the entry of q is wxh copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. from parametric models to gaussian processes and the entry of c is wxh where if n and otherwise. example let s take as an example a one-dimensional case with radial basis functions. the expression for becomes simplest if we assume we have uniformly-spaced basis functions with the basis function labelled h centred on the point x h and take the limit h so that the sum over h becomes an integral to avoid having a covariance that diverges with h we had better make w scale as where is the number of basis functions per unit length of the x-axis and s is a constant then sz hmax sz hmax hmin hmin if we let the limits of integration be we can solve this integral dh dh s we are arriving at a new perspective on the interpolation problem. instead of specifying the prior distribution on functions in terms of basis functions and priors on parameters the prior can be summarized simply by a covariance function cxn where we have given a new name to the constant out front. generalizing from this particular case a vista of interpolation methods opens up. given any valid covariance function cx we ll discuss in a moment what valid means we can the covariance matrix for n function values at locations xn to be the matrix q given by cxn and the covariance matrix for n corresponding target values assuming gaussian noise to be the matrix c given by cxn in conclusion the prior probability of the n target values t in the data set is p normalt c z samples from this gaussian process and a few other simple gaussian processes are displayed in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. t t x t t x gaussian processes figure samples drawn from gaussian process priors. each panel shows two functions drawn from a gaussian process prior. the four corresponding covariance functions are given below each plot. the decrease in lengthscale from to produces more rapidly functions. the periodic properties of the covariance function in can be seen. the covariance function in contains the non-stationary term corresponding to the covariance of a straight line so that typical functions include linear trends. from gibbs x x multilayer neural networks and gaussian processes figures and show some random samples from the prior distribution over functions by a selection of standard multilayer perceptrons with large numbers of hidden units. those samples don t seem a million miles away from the gaussian process samples of and indeed neal showed that the properties of a neural network with one hidden layer in equation converge to those of a gaussian process as the number of hidden neurons tends to if standard weight decay priors are assumed. the covariance function of this gaussian process depends on the details of the priors assumed for the weights in the network and the activation functions of the hidden units. using a given gaussian process model in regression we have spent some time talking about priors. we now return to our data and the problem of prediction. how do we make predictions with a gaussian process? having formed the covariance matrix c in equation our task is to infer tn given the observed vector tn the joint density p tn is a gaussian so the conditional distribution p j tn p tn p is also a gaussian. we now distinguish between sizes of covariance matrix c with a subscript such that cn is the covariance copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. examples of covariance functions matrix for the vector tn tn we submatrices of cn as follows cn cn kt tn n tn tn p j tn the posterior distribution is given by we can evaluate the mean and standard deviation of the posterior distribution of tn by brute-force inversion of cn there is a more elegant expression for the predictive distribution however which is useful whenever predictions are to be made at a number of new points on the basis of the data set of size n we can write n using the partitioned inverse equations n in terms of cn and n m m mt m where m kt m n k m n m n mmt when we substitute this matrix into equation we p j tn z where kt n tn kt n k the predictive mean at the new point is given by and the error bars on this prediction. notice that we do not need to invert cn in order to make predictions at xn only cn needs to be inverted. thus gaussian processes allow one to implement a model with a number of basis functions h much larger than the number of data points n with the computational requirement being of order n independent of h. ll discuss ways of reducing this cost later. the predictions produced by a gaussian process depend entirely on the covariance matrix c. we now discuss the sorts of covariance functions one might choose to c and how we can automate the selection of the covariance function in response to data. examples of covariance functions the only constraint on our choice of covariance function is that it must generate a covariance matrix for any set of points fxngn copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gaussian processes we will denote the parameters of a covariance function by the covariance matrix of t has entries given by cmn cxm xn where c is the covariance function and n is a noise model which might be stationary or spatially varying for example n for input-dependent noise. the continuity properties of c determine the continuity properties of typical samples from the gaussian process prior. an encyclopaedic paper on gaussian processes giving many valid covariance functions has been written by abrahamsen for input-independent noise stationary covariance functions a stationary covariance function is one that is translation invariant in that it cx dx for some function d i.e. the covariance is a function of separation only also known as the autocovariance function. if additionally c depends only on the magnitude of the distance between x and then the covariance function is said to be homogeneous. stationary covariance functions may also be described in terms of the fourier transform of the function d which is known as the power spectrum of the gaussian process. this fourier transform is necessarily a positive function of frequency. one way of constructing a valid stationary covariance function is to invent a positive function of frequency and d to be its inverse fourier transform. example let the power spectrum be a gaussian function of frequency. since the fourier transform of a gaussian is a gaussian the autocovariance function corresponding to this power spectrum is a gaussian function of separation. this argument rederives the covariance function we derived at equation generalizing slightly a popular form for c with hyperparameters is cx i i x is an i-dimensional vector and ri is a lengthscale associated with input xi the lengthscale in direction i on which y is expected to vary a very large lengthscale means that y is expected to be essentially a constant function of that input. such an input could be said to be irrelevant as in the automatic relevance determination method for neural networks neal the hyperparameter the vertical scale of variations of a typical function. the hyperparameter allows the whole function to be away from zero by some unknown constant to understand this term examine equation and consider the basis function another stationary covariance function is cx copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. adaptation of gaussian process models figure multimodal likelihood functions for gaussian processes. a data set of points is modelled with the simple covariance function with one hyperparameter controlling the noise variance. panels a and b show the most probable interpolant and its error bars when the hyperparameters are set to two values that maximize the likelihood p j xn panel c shows a contour plot of the likelihood as a function of and with the two maxima shown by crosses. from gibbs for this is a special case of the previous covariance function. for the typical functions from this prior are smooth but not analytic functions. for typical functions are continuous but not smooth. a covariance function that models a function that is periodic with known period in the ith input direction is cx ri a figure shows some random samples drawn from gaussian processes with a variety of covariance functions. nonstationary covariance functions the simplest nonstationary covariance function is the one corresponding to a linear trend. consider the plane yx pi wixi c. if the fwig and c have gaussian distributions with zero mean and variances then the plane has a covariance function w and c respectively clinx i c an example of random sample functions incorporating the linear term can be seen in adaptation of gaussian process models let us assume that a form of covariance function has been chosen but that it depends on undetermined hyperparameters we would like to learn these copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gaussian processes hyperparameters from the data. this learning process is equivalent to the inference of the hyperparameters of a neural network for example weight decay hyperparameters. it is a complexity-control problem one that is solved nicely by the bayesian occam s razor. ideally we would like to a prior distribution on the hyperparameters and integrate over them in order to make our predictions i.e. we would like to p j xn p j xn jd but this integral is usually intractable. there are two approaches we can take. we can approximate the integral by using the most probable values of hyperparameters. p j xn p j xn mp or we can perform the integration over numerically using monte carlo methods and rasmussen neal either of these approaches is implemented most if the gradient of the posterior probability of can be evaluated. gradient the posterior probability of is p jd p j xn the log of the term evidence for the hyperparameters is ln p j xn n tn and its derivative with respect to a hyperparameter is ln det cn n tt n ln ln p j xn trace n n tt n n tn comments assuming that the derivatives of the priors is straightforward we can now search for mp. however there are two problems that we need to be aware of. firstly as illustrated in the evidence may be multimodal. suitable priors and sensible optimization strategies often eliminate poor optima. secondly and perhaps most importantly the evaluation of the gradient of the log likelihood requires the evaluation of n any exact inversion method as cholesky decomposition lu decomposition or gaussjordan elimination has an associated computational cost that is of order n and so calculating gradients becomes time consuming for large training data sets. approximate methods for implementing the predictions and and gradient computation are an active research area. one approach based on the ideas of skilling makes approximations to and trace using iterative methods with cost on and mackay gibbs further references on this topic are given at the end of the chapter. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gaussian processes can be integrated into modelling once we identify a variable that can sensibly be given a gaussian process prior. in a binary problem we can a quantity an axn such that the probability that the class is rather than is p an large positive values of a correspond to probabilities close to one large negative values of a probabilities that are close to zero. in a problem we typically intend that the probability p should be a smoothly varying function of x. we can embody this prior belief by ax to have a gaussian process prior. implementation it is not so easy to perform inferences and adapt the gaussian process model to data in a model as in regression problems because the likelihood function is not a gaussian function of an. so the posterior distribution of a given some observations t is not gaussian and the normalization constant p j xn cannot be written down analytically. barber and williams have implemented based on gaussian process priors using laplace approximations neal has implemented a monte carlo approach to implementing a gaussian process gibbs and mackay have implemented another cheap and cheerful approach based on the methods of jaakkola and jordan in this variational gaussian process we obtain tractable upper and lower bounds for the unnormalized posterior density over a p j ap these bounds are parameterized by variational parameters which are adjusted in order to obtain the tightest possible using normalized versions of the optimized bounds we then compute approximations to the predictive distributions. multi-class problems can also be solved with monte carlo methods and variational methods discussion gaussian processes are moderately simple to implement and use. because very few parameters of the model need to be determined by hand only the priors on the hyperparameters gaussian processes are useful tools for automated tasks where tuning for each problem is not possible. we do not appear to any performance for this simplicity. it is easy to construct gaussian processes that have particular desired properties for example we can make a straightforward automatic relevance determination model. one obvious problem with gaussian processes is the computational cost associated with inverting an n n matrix. the cost of direct methods of inversion becomes prohibitive when the number of data points n is greater than about have we thrown the baby out with the bath water? according to the hype of neural networks were meant to be intelligent models that discovered features and patterns in data. gaussian processes in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gaussian processes contrast are simply smoothing devices. how can gaussian processes possibly replace neural networks? were neural networks over-hyped or have we underestimated the power of smoothing methods? i think both these propositions are true. the success of gaussian processes shows that many real-world data modelling problems are perfectly well solved by sensible smoothing methods. the most interesting problems the task of feature discovery for example are not ones that gaussian processes will solve. but maybe multilayer perceptrons can t solve them either. perhaps a fresh start is needed approaching the problem of machine learning from a paradigm from the supervised feedforward mapping. further reading the study of gaussian processes for regression is far from new. time series analysis was being performed by the astronomer t.n. thiele using gaussian processes in in the wienerkolmogorov prediction theory was introduced for prediction of trajectories of military targets within the geostatistics matheron proposed a framework for regression using optimal linear estimators which he called kriging after d.g. krige a south african mining engineer. this framework is identical to the gaussian process approach to regression. kriging has been developed considerably in the last thirty years cressie for a review including several bayesian treatments kitanidis however the geostatistics approach to the gaussian process model has concentrated mainly on low-dimensional problems and has largely ignored any probabilistic interpretation of the model. kalman are widely used to implement inferences for stationary one-dimensional gaussian processes and are popular models for speech and music modelling and fortmann generalized radial basis functions and girosi arma models and variable metric kernel methods are all closely related to gaussian processes. see also o hagan the idea of replacing supervised neural networks by gaussian processes was explored by williams and rasmussen and neal a thorough comparison of gaussian processes with other methods such as neural networks and mars was made by rasmussen methods for reducing the complexity of data modelling with gaussian processes remain an active research area and girosi luo and wahba tresp williams and seeger smola and bartlett rasmussen seeger et al. opper and winther a longer review of gaussian processes is in a review paper on regression with complexity control using hierarchical bayesian models is gaussian processes and support vector learning machines et al. vapnik have a lot in common. both are kernel-based predictors the kernel being another name for the covariance function. a bayesian version of support vectors exploiting this connection can be found in et al. chu et al. chu et al. chu et al. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. deconvolution traditional image reconstruction methods optimal linear in many imaging problems the data measurements fdng are linearly related to the underlying image f dn rnkfk nn the vector n denotes the inevitable noise that corrupts real data. in the case of a camera which produces a blurred picture the vector f denotes the true image d denotes the blurred and noisy picture and the linear operator r is a convolution by the point spread function of the camera. in this special case the true image and the data vector reside in the same space but it is important to maintain a distinction between them. we will use the subscript n n to run over data measurements and the subscripts k k to run over image pixels. one might speculate that since the blur was created by a linear operation then perhaps it might be deblurred by another linear operation. we can derive the optimal linear in two ways. bayesian derivation we assume that the linear operator r is known and that the noise n is gaussian and independent with a known standard deviation p f exp we assume that the prior probability of the image is also gaussian with a scale parameter p j c f f a if we assume no correlations among the pixels then the symmetric full rank matrix c is equal to the identity matrix i. the more sophisticated intrinsic correlation function model uses c where g is a convolution that takes us from an imaginary hidden image which is uncorrelated to the real correlated image. the intrinsic correlation function should not be confused with the point spread function r which the image-to-data mapping. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. deconvolution a zero-mean gaussian prior is clearly a poor assumption if it is known that all elements of the image f are positive but let us proceed. we can now write down the posterior probability of an image f given the data d. in words p j d p f j p posterior likelihood prior evidence the evidence p is the normalizing constant for this posterior distribution. here it is unimportant but it is used in a more sophisticated analysis to compare for example values of and or point spread functions r. since the posterior distribution is the product of two gaussian functions of f it is also a gaussian and can therefore be summarized by its mean which is also the most probable image fmp and its covariance matrix log p j d f fmp rt is called the optimal linear when which the joint error bars on f in this equation the symbol r denotes with respect to the image parameters f we can fmp by the log of the posterior and solving for the derivative being zero. we obtain rtd the operator the term f rt. the term f f c can be neglected the optimal linear is the pseudoinverse c regularizes this ill-conditioned inverse. the optimal linear can also be manipulated into the form optimal linear f minimum square error derivation the non-bayesian derivation of the optimal linear starts by assuming that we will estimate the true image f by a linear function of the data wd the linear operator w is then optimized by minimizing the expected sumsquared error between and the unknown true image in the following equations summations over repeated indices k n are implicit. the expectation is over both the statistics of the random variables fnng and the ensemble of images f which we expect to bump into. we assume that the noise is zero mean and uncorrelated to second order with itself and everything else with hei copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. traditional image reconstruction methods with respect to w and introducing f the bayesian derivation above we that the optimal linear is f in wopt f we obtain the optimal linear of the if we identify f bayesian derivation. the ad hoc assumptions made in this derivation were the choice of a quadratic error measure and the decision to use a linear estimator. it is interesting that without explicit assumptions of gaussian distributions this derivation has reproduced the same estimator as the bayesian posterior mode fmp. the advantage of a bayesian approach is that we can criticize these as sumptions and modify them in order to make better reconstructions. other image models the better matched our model of images p jh is to the real world the better our image reconstructions will be and the less data we will need to answer any given question. the gaussian models which lead to the optimal linear are spectacularly poorly matched to the real world. for example the gaussian prior fails to specify that all pixel intensities in an image are positive. this omission leads to the most pronounced artefacts where the image under observation has high contrast or large black patches. optimal linear applied to astronomical data give reconstructions with negative areas in them corresponding to patches of sky that suck energy out of telescopes! the maximum entropy model for image deconvolution and daniell was a great success principally because this model forced the reconstructed image to be positive. the spurious negative areas and complementary spurious positive areas are eliminated and the quality of the reconstruction is greatly enhanced. the classic maximum entropy model assigns an entropic prior p j mhclassic mz sf m lnmifi fi mi where this model enforces positivity the parameter a characteristic dynamic range by which the pixel values are expected to from the default image m. the intrinsic-correlation-function maximum-entropy model introduces an expectation of spatial correlations into the prior on f by writing f gh where g is a convolution with an intrinsic correlation function and putting a classic maxent prior on the underlying hidden image h. probabilistic movies having found not only the most probable image fmp but also error bars on it one task is to visualize those error bars. whether or not we use monte carlo methods to infer f a correlated random walk around the posterior distribution can be used to visualize the uncertainties and correlations. for a gaussian posterior distribution we can create a correlated sequence of unit normal random vectors n using cnt sz copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. deconvolution where z is a unit normal random vector and controls how persistent the memory of the sequence is. we then render the image sequence by f fmp fjd nt fjd is the cholesky decomposition of where supervised neural networks for image deconvolution neural network researchers often exploit the following strategy. given a problem currently solved with a standard algorithm interpret the computations performed by the algorithm as a parameterized mapping from an input to an output and call this mapping a neural network then adapt the parameters to data so as to produce another mapping that solves the task better. by construction the neural network can reproduce the standard algorithm so this data-driven adaptation can only make the performance better. there are several reasons why standard algorithms can be bettered in this way. algorithms are often not designed to optimize the real objective function. for example in speech recognition a hidden markov model is designed to model the speech signal and is so as to to maximize the generative probability given the known string of words in the training data but the real objective is to discriminate between words. if an inadequate model is being used the neural-net-style training of the model will focus the limited resources of the model on the aspects relevant to the discrimination task. discriminative training of hidden markov models for speech recognition does improve their performance. the neural network can be more than the standard model some of the adaptive parameters might have been viewed as features by the original designers. a network can properties in the data that were not included in the original model. deconvolution in humans a huge fraction of our brain is devoted to vision. one of the neglected features of our visual system is that the raw image falling on the retina is severely blurred while most people can see with a resolution of about arcminute sixtieth of a degree under any daylight conditions bright or dim the image on our retina is blurred through a point spread function of width as large as arcminutes and howarth and bradley it is amazing that we are able to resolve pixels that are times smaller in area than the blob produced on our retina by any point source. isaac newton was aware of this conundrum. it s hard to make a lens that does not have chromatic aberration and our cornea and lens like a lens made of ordinary glass refract blue light more strongly than red. typically our eyes focus correctly for the middle of the visible spectrum so if we look at a single white dot made of red green and blue light the image on our retina consists of a sharply focussed green dot surrounded by a broader red blob superposed on an even broader blue blob. the width of the red and blue blobs is proportional to the diameter of the pupil which is largest under dim lighting conditions. blobs are roughly concentric though most people have a slight bias such that in one eye the red blob is centred a tiny distance copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. deconvolution in humans to the left and the blue is centred a tiny distance to the right and in the other eye it s the other way round. this slight bias explains why when we look at blue and red writing on a dark background most people perceive the blue writing to be at a slightly greater depth than the red. in a minority of people this small bias is the other way round and the redblue depth perception is reversed. but this many people are aware of having noticed it in cinemas for example is tiny compared with the chromatic aberration we are discussing. you can vividly demonstrate to yourself how enormous the chromatic aberration in your eye is with the help of a sheet of card and a colour computer screen. for the most impressive results i guarantee you will be amazed use a dim room with no light apart from the computer screen a pretty strong will still be seen even if the room has daylight coming into it as long as it is not bright sunshine. cut a slit about mm wide in the card. on the screen display a few small coloured objects on a black background. i especially recommend thin vertical objects coloured pure red pure blue magenta red plus blue and white plus blue plus include a little blackand-white text on the screen too. stand or sit far away that you can only just read the text perhaps a distance of four metres or so if you have normal vision. now hold the slit vertically in front of one of your eyes and close the other eye. hold the slit near to your eye brushing your eyelashes and look through it. waggle the slit slowly to the left and to the right so that the slit is alternately in front of the left and right sides of your pupil. what do you see? i see the red objects waggling to and fro and the blue objects waggling to and fro through huge distances and in opposite directions while white objects appear to stay still and are negligibly distorted. thin magenta objects can be seen splitting into their constituent red and blue parts. measure how large the motion of the red and blue objects is it s more than minutes of arc for me in a dim room. then check how sharply you can see under these conditions look at the text on the screen for example is it not the case that you can see your whole pupil features far smaller than the distance through which the red and blue components were waggling? yet when you are using the whole pupil what is falling on your retina must be an image blurred with a blurring diameter equal to the waggling amplitude. one of the main functions of early visual processing must be to deconvolve this chromatic aberration. neuroscientists sometimes conjecture that the reason why retinal ganglion cells and cells in the lateral geniculate nucleus main brain area to which retinal ganglion cells project have centre-surround receptive with colour opponency wavelength in the centre and medium wavelength in the surround for example is in order to perform feature extraction or edge detection but i think this view is mistaken. the reason we have centre-surround at the stage of visual processing the fovea at least is for the huge task of deconvolution of chromatic aberration. i speculate that the mccollough an extremely long-lasting association of colours with orientation mackay and mackay is produced by the adaptation mechanism that tunes our chromaticaberration-deconvolution circuits. our deconvolution circuits need to be rapidly tuneable because the point spread function of our eye changes with our pupil diameter which can change within seconds and indeed the mccollough can be induced within seconds. at the same time the is long-lasting copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. deconvolution when an eye is covered because it s in our interests that our deconvolution circuits should stay well-tuned while we sleep so that we can see sharply the instant we wake up. i also wonder whether the main reason that we evolved colour vision was not in order to see fruit better but so as to be able to see black and white sharper deconvolving chromatic aberration is easier even in an entirely black and white world if one has access to chromatic information in the image. and a speculation why do our eyes make micro-saccades when we look at things? these miniature eye-movements are of an angular size bigger than the spacing between the cones in the fovea are spaced at roughly minute of arc the perceived resolution of the eye. the typical size of a microsaccade is minutes of arc and riggs is it a coincidence that this is the same as the size of chromatic aberration? surely micro-saccades must play an essential role in the deconvolution mechanism that delivers our high-resolution vision. exercises exercise blur an image with a circular hat point spread function and add noise. then deconvolve the blurry noisy image using the optimal linear find error bars and visualize them by making a probabilistic movie. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. part vi sparse graph codes copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about part vi the central problem of communication theory is to construct an encoding and a decoding system that make it possible to communicate reliably over a noisy channel. during the remarkable progress was made towards the shannon limit using codes that are in terms of sparse random graphs and which are decoded by a simple probability-based message-passing algorithm. in a sparse-graph code the nodes in the graph represent the transmitted bits and the constraints they satisfy. for a linear code with a codeword length n and rate r kn the number of constraints is of order m n k. any linear code can be described by a graph but what makes a sparse-graph code special is that each constraint involves only a small number of variables in the graph so the number of edges in the graph scales roughly linearly with n rather than quadratically. in the following four chapters we will look at four families of sparse-graph codes three families that are excellent for error-correction low-density paritycheck codes turbo codes and repeataccumulate codes and the family of digital fountain codes which are outstanding for erasure-correction. all these codes can be decoded by a local message-passing algorithm on the graph the sumproduct algorithm and while this algorithm is not a perfect maximum likelihood decoder the empirical results are record-breaking. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes h figure a low-density parity-check matrix and the corresponding graph of a low-density parity-check code with blocklength n and m constraints. each white circle represents a transmitted bit. each bit participates in j constraints represented by squares. each constraint forces the sum of the k bits to which it is connected to be even. a low-density parity-check code gallager code is a block code that has a parity-check matrix h every row and column of which is sparse a regular gallager code is a low-density parity-check code in which every column of h has the same weight j and every row has the same weight k regular gallager codes are constructed at random subject to these constraints. a low-density parity-check code with j and k is illustrated in theoretical properties low-density parity-check codes lend themselves to theoretical study. the following results are proved in gallager and mackay low-density parity-check codes in spite of their simple construction are good codes given an optimal decoder codes in the sense of section furthermore they have good distance the sense of section these two results hold for any column weight j furthermore there are sequences of low-density parity-check codes in which j increases gradually with n in such a way that the ratio jn still goes to zero that are very good and that have very good distance. however we don t have an optimal decoder and decoding low-density parity-check codes is an np-complete problem. so what can we do in practice? practical decoding given a channel output r we wish to the codeword t whose likelihood p t is biggest. all the decoding strategies for low-density paritycheck codes are message-passing algorithms. the best algorithm known is the sumproduct algorithm also known as iterative probabilistic decoding or belief propagation. we ll assume that the channel is a memoryless channel more complex channels can easily be handled by running the sumproduct algorithm on a more complex graph that represents the expected correlations among the errors and stark for any memoryless channel there are two approaches to the decoding problem both of which lead to the generic problem the x that maximizes p p z where p is a separable distribution on a binary vector x and z is another binary vector. each of these two approaches represents the decoding problem in terms of a factor graph copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes tn the prior distribution over codewords p p j tn tn p nn zm the variable nodes are the transmitted bits ftng. each node represents the factor tn mod the posterior distribution over codewords p r p t each upper function node represents a likelihood factor p j tn. the joint probability of the noise n and syndrome z p z p hn the top variable nodes are now the noise bits fnng. the added variable nodes at the base are the syndrome values fzmg. each zm hmnnn mod is enforced by a factor. figure factor graphs associated with a low-density parity-check code. the codeword decoding viewpoint first we note that the prior distribution over codewords p mod can be represented by a factor graph with the factorization being tn mod p ym ll omit the mod s from now on. the posterior distribution over codewords is given by multiplying this prior by the likelihood which introduces another n factors one for each received bit. p r p t ym tn yn p j tn the factor graph corresponding to this function is shown in it is the same as the graph for the prior except for the addition of likelihood dongles to the transmitted bits. in this viewpoint the received signal rn can live in any alphabet all that matters are the values of p j tn. the syndrome decoding viewpoint alternatively we can view the channel output in terms of a binary received vector r and a noise vector n with a probability distribution p that can be derived from the channel properties and whatever additional information is available at the channel outputs. for example with a binary symmetric channel we the noise by r t n the syndrome z hr and noise model p f for other channels such as the gaussian channel with output y we may a received copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decoding with the sumproduct algorithm binary vector r however we wish and obtain an binary noise model p from y and the joint probability of the noise n and syndrome z hn can be factored as p z p hn p ym yn nn the factor graph of this function is shown in the variables n and z can also be drawn in a belief network known as a bayesian network causal network or diagram similar to but with arrows on the edges from the upper circular nodes represent the variables n to the lower square nodes now represent the variables z. we can say that every bit xn is the parent of j checks zm and each check zm is the child of k bits. both decoding viewpoints involve essentially the same graph. either version of the decoding problem can be expressed as the generic decoding problem the x that maximizes p p z in the codeword decoding viewpoint x is the codeword t and z is in the syndrome decoding viewpoint x is the noise n and z is the syndrome. it doesn t matter which viewpoint we take when we apply the sumproduct algorithm. the two decoding algorithms are isomorphic and will give equivalent outcomes numerical errors intervene. i tend to use the syndrome decoding viewpoint because it has one advantage one does not need to implement an encoder for a code in order to be able to simulate a decoding problem realistically. we ll now talk in terms of the generic decoding problem. decoding with the sumproduct algorithm we aim given the observed checks to compute the marginal posterior probabilities p z h for each n. it is hard to compute these exactly because the graph contains many cycles. however it is interesting to implement the decoding algorithm that would be appropriate if there were no cycles on the assumption that the errors introduced might be relatively small. this approach of ignoring cycles has been used in the intelligence literature but is now frowned upon because it produces inaccurate probabilities. however if we are decoding a good error-correcting code we don t care about accurate marginal probabilities we just want the correct codeword. also the posterior probability in the case of a good code communicating at an achievable rate is expected typically to be hugely concentrated on the most probable decoding so we are dealing with a distinctive probability distribution to which experience gained in other may not apply. the sumproduct algorithm was presented in chapter we now write out explicitly how it works for solving the decoding problem hx z for brevity we reabsorb the dongles hanging the x and z nodes in and modify the sumproduct algorithm accordingly. the graph in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes which x and z live is then the original graph whose edges are by the in h. the graph contains nodes of two types which we ll call checks and bits. the graph connecting the checks and bits is a bipartite graph bits connect only to checks and vice versa. on each iteration a probability ratio is propagated along each edge in the graph and each bit node xn updates its probability that it should be in state we denote the set of bits n that participate in check m by n fn hmn similarly we the set of checks in which bit n participates mn fm hmn we denote a set n with bit n excluded by n the algorithm has two alternating parts in which quantities qmn and rmn associated with each edge in the graph are iteratively updated. the quantity qx mn is meant to be the probability that bit n of x has the value x given the information obtained via checks other than check m. the quantity rx mn is meant to be the probability of check m being if bit n of x is considered at x and the other bits have a separable distribution given by the probabilities n the algorithm would produce the exact posterior probabilities of all the bits after a number of iterations if the bipartite graph by the matrix h contained no cycles. n p initialization. let n p prior probability that bit xn is and let n. if we are taking the syndrome decoding viewpoint and the channel is a binary symmetric channel then n will equal f if the noise level varies in a known way example if the channel is a binary-input gaussian channel with a real output then n is initialized to the appropriate normalized likelihood. for every m such that hmn the variables mn are initialized to the values n respectively. mn and n and horizontal step. in the horizontal step of the algorithm from the point of view of the matrix h we run through the checks m and compute for each n n two probabilities mn the probability of the observed value of zm arising when xn given that the other bits ng have a separable distribution given by the probabilities fq mn j xn n mn the probability of the observed value of zm arising when by and second xn by mn j xn n the conditional probabilities in these summations are either zero or one depending on whether the observed zm matches the hypothesized values for xn and the these probabilities can be computed in various obvious ways based on equation and the computations may be done most jn is large by regarding zm as the state of a markov chain with states and this chain being started in state and undergoing transitions corresponding to additions of the various with transition probabilities given by the corresponding the probabilities for zm having its observed value given either xn or xn can then be found by use of the forwardbackward algorithm and copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decoding with the sumproduct algorithm a particularly convenient implementation of this method uses forward and mn are backward passes in which products of the computed. we obtain mn from the identity mn mn this identity is derived by iterating the following observation mod and and have probabilities then p p we recover and p mn and and mn using if of being and thus p mn mn the transformations into and back from to frg may be viewed as a fourier transform and an inverse fourier transformation. vertical step. the vertical step takes the computed values of r and updates the values of the probabilities compute mn and mn mn. for each n we mn and mn mn n n where is chosen such that computed in a downward pass and an upward pass. mn these products can be we can also compute the pseudoposterior probabilities q n and n at this iteration given by n n n n mn mn these quantities are used to create a tentative decoding the consistency of which is used to decide whether the decoding algorithm can halt. if hx z. at this point the algorithm repeats from the horizontal step. the stop-when-it s-done decoding method. the recommended decoding procedure is to set to if n and see if the checks hx z mod are all halting when they are and declaring a failure if some maximum number of iterations or occurs without successful decoding. in the event of a failure we may still report but we the whole block as a failure. we note in passing the between this decoding procedure and the widespread practice in the turbo code community where the decoding algorithm is run for a number of iterations of whether the decoder a consistent state at some earlier time. this practice is wasteful of computer time and it blurs the distinction between undetected and detected errors. in our procedure undetected errors occur if the decoder an copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes parity bits figure demonstration of encoding with a gallager code. the encoder is derived from a very sparse parity-check matrix with three per column the code creates transmitted vectors consisting of source bits and paritycheck bits. here the source sequence has been altered by changing the bit. notice that many of the parity-check bits are changed. each parity bit depends on about half of the source bits. the transmission for the case s this vector is the between transmissions and image copyright united feature syndicate inc. used with permission. satisfying hx z mod that is not equal to the true x. detected errors occur if the algorithm runs for the maximum number of iterations without a valid decoding. undetected errors are of interest because they reveal distance properties of a code. and in engineering practice it would seem preferable for the blocks that are known to contain detected errors to be so labelled if practically possible. cost. in a brute-force approach the time to create the generator matrix scales as n where n is the block size. the encoding time scales as n but encoding involves only binary arithmetic so for the block lengths studied here it takes considerably less time than the simulation of the gaussian channel. decoding involves approximately j multiplies per iteration so the total number of operations per decoded bit iterations is about independent of blocklength. for the codes presented in the next section this is about operations. the encoding complexity can be reduced by clever encoding tricks invented by richardson and urbanke or by specially constructing the paritycheck matrix et al. the decoding complexity can be reduced with only a small loss in performance by passing low-precision messages in place of real numbers and urbanke pictorial demonstration of gallager codes figures illustrate visually the conditions under which low-density parity-check codes can give reliable communication over binary symmetric channels and gaussian channels. these demonstrations may be viewed as animations on the world wide copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. pictorial demonstration of gallager codes h figure a low-density parity-check matrix with n columns of weight j and m rows of weight k encoding figure illustrates the encoding operation for the case of a gallager code whose parity-check matrix is a matrix with three per column the high density of the generator matrix is illustrated in and c by showing the change in the transmitted vector when one of the source bits is altered. of course the source images shown here are highly redundant and such images should really be compressed before encoding. redundant images are chosen in these demonstrations to make it easier to see the correction process during the iterative decoding. the decoding algorithm does not take advantage of the redundancy of the source vector and it would work in exactly the same way irrespective of the choice of source vector. iterative decoding the transmission is sent over a channel with noise level f and the received vector is shown in the upper left of the subsequent pictures in show the iterative probabilistic decoding process. the sequence of shows the best guess bit by bit given by the iterative decoder after and iterations. the decoder halts after the iteration when the best guess violates no parity checks. this decoding is error free. in the case of an unusually noisy transmission the decoding algorithm fails to a valid decoding. for this code and a channel with f such failures happen about once in every transmissions. figure shows this error rate compared with the block error rates of classical error-correcting codes. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes received decoded figure iterative probabilistic decoding of a low-density parity-check code for a transmission received over a channel with noise level f the sequence of shows the best guess bit by bit given by the iterative decoder after and iterations. the decoder halts after the iteration when the best guess violates no parity checks. this decoding is error free. r o r r e r e d o c e d f o y t i l i b a b o r p low-density parity-check code shannon limit gv c rate figure error probability of the low-density parity-check code error bars for binary symmetric channel with f compared with algebraic codes. squares repetition codes and hamming code other points reedmuller and bch codes. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. pictorial demonstration of gallager codes figure demonstration of a gallager code for a gaussian channel. the received vector after transmission over a gaussian channel with db. the greyscale represents the value of the normalized likelihood. this transmission can be perfectly decoded by the sumproduct decoder. the empirical probability of decoding failure is about the probability distribution of the output y of the channel with for each of the two possible inputs. the received transmission over a gaussian channel with which corresponds to the shannon limit. the probability distribution of the output y of the channel with for each of the two possible inputs. figure performance of gallager codes on the gaussian channel. vertical axis block error probability. horizontal axis signal-to-noise ratio dependence on blocklength n for k codes. from left to right n n n n the dashed lines show the frequency of undetected errors which is measurable only when the blocklength is as small as n or n dependence on column weight j for codes of blocklength n py py gaussian channel py py in the left picture shows the received vector after transmission over a gaussian channel with the greyscale represents the value this signal-to-noise ratio of the normalized likelihood is a noise level at which this gallager code communicates reliably probability of error is to show how close we are to the shannon limit the right panel shows the received vector when the signal-tonoise ratio is reduced to which corresponds to the shannon limit for codes of rate p j t j t p j t variation of performance with code parameters figure shows how the parameters n and j the performance of low-density parity-check codes. as shannon would predict increasing the blocklength leads to improved performance. the dependence on j follows a pattern. given an optimal decoder the best performance would be obtained for the codes closest to random codes that is the codes with largest j. however the sumproduct decoder makes poor progress in dense graphs so the best performance is obtained for a small value of j. among the values copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes f f figure schematic illustration of constructions of a completely regular gallager code with j k and r of a nearly-regular gallager code with rate notation an integer represents a number of permutation matrices superposed on the surrounding square. a diagonal line represents an identity matrix. figure monte carlo simulation of density evolution following the decoding process for j k each curve shows the average entropy of a bit as a function of number of iterations as estimated by a monte carlo algorithm using samples per iteration. the noise level of the binary symmetric channel f increases by steps of from bottom graph to top graph there is evidently a threshold at about f above which the algorithm cannot determine x. from mackay of j shown in the j is the best for a blocklength of down to a block error probability of this observation motivates construction of gallager codes with some columns of weight a construction with columns of weight is shown in too many columns of weight and the code becomes a much poorer code. as we ll discuss later we can do even better by making the code even more irregular. density evolution one way to study the decoding algorithm is to imagine it running on an tree-like graph with the same local topology as the gallager code s graph. the larger the matrix h the closer its decoding properties should approach those of the graph. imagine an belief network with no loops in which every bit xn connects to j checks and every check zm connects to k bits we consider the iterative of information in this network and examine the average entropy of one bit as a function of number of iterations. at each iteration a bit has accumulated information from its local network out to a radius equal to the number of iterations. successful decoding will occur only if the average entropy of a bit decreases to zero as the number of iterations increases. the iterations of an belief network can be simulated by monte carlo methods a technique used by gallager imagine a network of radius i total number of iterations centred on one bit. our aim is to compute the conditional entropy of the central bit x given the state z of all checks out to radius i. to evaluate the probability that the central bit is given a particular syndrome z involves an i-step propagation from the outside of the network into the centre. at the ith iteration probabilities r at figure local topology of the graph of a gallager code with column weight j and row weight k white nodes represent bits xl black nodes represent checks zm each edge corresponds to a in h. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. improving gallager codes radius i i are transformed into qs and then into rs at radius i i in a way that depends on the states x of the unknown bits at radius i i. in the monte carlo method rather than simulating this network exactly which would take a time that grows exponentially with i we create for each iteration a representative sample size say of the values of fr xg. in the case of a regular network with parameters j k each new pair fr xg in the list at the ith iteration is created by drawing the new x from its distribution and drawing at random with replacement pairs fr xg from the list at the iteration these are assembled into a tree fragment and the sumproduct algorithm is run from top to bottom to the new r value associated with the new node. as an example the results of runs with j k and noise densities f between and using samples at each iteration are shown in runs with low enough noise level show a collapse to zero entropy after a small number of iterations and those with high noise level decrease to a non-zero entropy corresponding to a failure to decode. the boundary between these two behaviours is called the threshold of the decoding algorithm for the binary symmetric channel. figure shows by monte carlo simulation that the threshold for regular k codes is about richardson and urbanke have derived thresholds for regular codes by a tour de force of direct analytic methods. some of these thresholds are shown in table approximate density evolution for practical purposes the computational cost of density evolution can be reduced by making gaussian approximations to the probability distributions over the messages in density evolution and updating only the parameters of these approximations. for further information about these techniques which produce diagrams known as exit charts see brink chung et al. ten brink et al. improving gallager codes since the rediscovery of gallager codes two methods have been found for enhancing their performance. clump bits and checks together first we can make gallager codes in which the variable nodes are grouped together into metavariables consisting of say binary variables and the check nodes are similarly grouped together into metachecks. as before a sparse graph can be constructed connecting metavariables to metachecks with a lot of freedom about the details of how the variables and checks within are wired up. one way to set the wiring is to work in a gf such as gf or gf low-density parity-check matrices using elements of gf and translate our binary messages into gf using a mapping such as the one for gf given in table now when messages are passed during decoding those messages are probabilities and likelihoods over conjunctions of binary variables. for example if each clump contains three binary variables then the likelihoods will describe the likelihoods of the eight alternative states of those bits. with carefully optimized constructions the resulting codes over gf rf f f x iteration f f f x r iteration i figure a tree-fragment constructed during monte carlo simulation of density evolution. this fragment is appropriate for a regular j k gallager code. k fmax table thresholds fmax for regular low-density parity-check codes assuming sumproduct decoding algorithm from richardson and urbanke the shannon limit for codes is fmax gf binary a b table translation between gf and binary for message symbols. gf binary a b table translation between gf and binary for matrix entries. an m n parity-check matrix over gf can be turned into a binary parity-check matrix in this way. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes f f a f b f f a f b f a f a f b f b f a f b luby reg irreg irreg reg gallileo turbo y t i l i b a b o r p r o r r e t i b l a c i r i p m e algorithm the fourier transform over gf the fourier transform f of a function f over gf is given by f f f f f f transforms over gf can be viewed as a sequence of binary transforms in each of k dimensions. the inverse transform is identical to the fourier transform except that we also divide by signal to noise ratio figure comparison of regular binary gallager codes with irregular codes codes over gf and other outstanding codes of rate from left performance to right irregular low-density parity-check code over gf blocklength bits jpl turbo code blocklength regular low-density parity-check over gf blocklength bits and mackay irregular binary low-density paritycheck code blocklength bits luby et al. irregular binary lowdensity parity-check code blocklength bits jpl code for galileo this was the best known code of rate regular binary low-density parity-check code blocklength bits the shannon limit is at about db. as of even better sparse-graph codes have been constructed. gf and gf perform nearly one decibel better than comparable binary gallager codes. the computational cost for decoding in gf scales as q log q if the appropriate fourier transform is used in the check nodes the update rule for the check-to-variable message ra mn xxxna qxj mj is a convolution of the quantities qa mj so the summation can be replaced by a product of the fourier transforms of qa mj for j n followed by an inverse fourier transform. the fourier transform for gf is shown in algorithm make the graph irregular the second way of improving gallager codes introduced by luby et al. is to make their graphs irregular. instead of giving all variable nodes the same degree j we can have some variable nodes with degree some some and a few with degree check nodes can also be given unequal degrees this helps improve performance on erasure channels but it turns out that for the gaussian channel the best graphs have regular check degrees. figure illustrates the by these two methods for improving gallager codes focussing on codes of rate making the binary code irregular gives a win of about db switching from gf to gf gives copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. fast encoding of low-density parity-check codes figure an algebraically constructed low-density parity-check code satisfying many redundant constraints outperforms an equivalent random gallager code. the table shows the n m k distance d and row weight k of some cyclic codes highlighting the codes that have large dn small k and large nm in the comparison the gallager code had k and rate identical to the n cyclic code. vertical axis block error probability. horizontal axis signal-to-noise ratio difference set cyclic codes n m k d k about db and matthew davey s code that combines both these features it s irregular over gf gives a win of about db over the regular binary gallager code. methods for optimizing the of a gallager code is its number of rows and columns of each degree have been developed by richardson et al. and have led to low-density parity-check codes whose performance when decoded by the sumproduct algorithm is within a hair s breadth of the shannon limit. algebraic constructions of gallager codes the performance of regular gallager codes can be enhanced in a third manner by designing the code to have redundant sparse constraints. there is a cyclic code for example that has n and k but the code not m but n i.e. low-weight constraints it is impossible to make random gallager codes that have anywhere near this much redundancy among their checks. the cyclic code performs about db better than an equivalent random gallager code. an open problem is to discover codes sharing the remarkable properties of the cyclic codes but with blocklengths and rates. i call this task the tanner challenge. fast encoding of low-density parity-check codes we now discuss methods for fast encoding of low-density parity-check codes faster than the standard method in which a generator matrix g is found by gaussian elimination a cost of order m and then each block is encoded by multiplying it by g a cost of order m staircase codes certain low-density parity-check matrices with m columns of weight or less can be encoded easily in linear time. for example if the matrix has a staircase structure as illustrated by the right-hand side of h copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes and if the data s are loaded into the k bits then the m parity bits p can be computed from left to right in linear time. pk pk pk pm pk hm nsn if we call two parts of the h matrix we can describe the encoding operation in two steps compute an intermediate parity vector v hss then pass v through an accumulator to create p. figure the parity-check matrix in approximate lower-triangular form. the cost of this encoding method is linear if the sparsity of h is exploited when computing the sums in fast encoding of general low-density parity-check codes richardson and urbanke demonstrated an elegant method by which the encoding cost of any low-density parity-check code can be reduced from the straightforward method s m to a cost of n where g the gap is hopefully a small constant and in the worst cases scales as a small fraction of n m a c b d n t e m g in the step the parity-check matrix is rearranged by row-interchange and column-interchange into the approximate lower-triangular form shown in the original matrix h was very sparse so the six matrices a b t c d and e are also very sparse. the matrix t is lower triangular and has everywhere on the diagonal. h a b t c d e the source vector s of length k n m is encoded into a transmission t as follows. compute the upper syndrome of the source vector za as this can be done in linear time. find a setting of the second parity bits pa such that the upper syn drome is zero. pa this vector can be found in linear time by back-substitution i.e. computing the bit of pa then the second then the third and so forth. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further reading compute the lower syndrome of the vector pa zb cs epa this can be done in linear time. now we get to the clever bit. the matrix f d and its inverse this computation needs to be done once only and its cost is of order this inverse is a dense g matrix. f is not invertible then either h is not of full rank or else further column permutations of h can produce an f that is invertible. set the parity bits to this operation has a cost of order claim at this point we have found the correct setting of the parity bits discard the tentative parity bits pa and the new upper syndrome zc za this can be done in linear time. find a setting of the second parity bits such that the upper syndrome is zero this vector can be found in linear time by back-substitution. further reading low-density parity-check codes codes were studied in by gallager then were generally forgotten by the coding theory community. tanner generalized gallager s work by introducing more general constraint nodes the codes that are now called turbo product codes should in fact be called tanner product codes since tanner proposed them and his colleagues and krit implemented them in hardware. publications on gallager codes contributing to their rebirth include et al. mackay and neal mackay and neal wiberg mackay spielman sipser and spielman low-precision decoding algorithms and fast encoding algorithms for gallager codes are discussed in and urbanke richardson and urbanke mackay and davey showed that low-density parity-check codes can outperform reedsolomon codes even on the reedsolomon codes home turf high rate and short blocklengths. other important papers include et al. luby et al. luby et al. davey and mackay richardson et al. chung et al. useful tools for the design of irregular low-density paritycheck codes include et al. urbanke see frey mceliece et al. for further discussion of the sumproduct algorithm. for a view of low-density parity-check code decoding in terms of group theory and coding theory see and soljanin copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. low-density parity-check codes and soljanin and for background reading on this topic see and rudolph terras there is a growing literature on the practical design of low-density parity-check codes and banihashemi mao and banihashemi ten brink et al. they are now being adopted for applications from hard drives to satellite communications. for low-density parity-check codes applicable to quantum error-correction see mackay et al. exercises exercise the hyperbolic tangent version of the decoding algorithm. in section the sumproduct decoding algorithm for low-density paritycheck codes was presented in terms of quantities q mn then in terms of quantities and there is a third description in which the fqg are replaced by log probability-ratios mn and lmn ln mn mn show that mn mn derive the update rules for frg and flg. exercise i am sometimes asked why not decode other linear codes for example algebraic codes by transforming their parity-check matrices so that they are low-density and applying the sumproduct algorithm? that any linear combination of rows of h ph is a valid parity-check matrix for a code as long as the matrix p is invertible so there are many parity check matrices for any one code. explain why a random linear code does not have a low-density paritycheck matrix. low-density means having row-weight at most k where k is some small constant n exercise show that if a low-density parity-check code has more than m columns of weight say columns where then the code will have words with weight of order log m exercise in section we found the expected value of the weight enumerator function aw averaging over the ensemble of all random linear codes. this calculation can also be carried out for the ensemble of low-density parity-check codes mackay litsyn and shevelev it is plausible however that the mean value of aw is not always a good indicator of the typical value of aw in the ensemble. for example if at a particular value of w of codes have aw and have aw then while we might say the typical value of aw is zero the mean is found to be find the typical weight enumerator function of low-density parity-check codes. solutions solution to exercise consider codes of rate r and blocklength n having k rn source bits and m parity-check bits. let all copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. solutions the codes have their bits ordered so that the k bits are independent so that we could if we wish put the code in systematic form g h the number of distinct linear codes is the number of matrices p which is k can these all be expressed as distinct low-density parity-check codes? the number of low-density parity-check matrices with row-weight k is log n r and the number of distinct codes that they is at most m which is much smaller than so by the pigeon-hole principle it is not possible for every random linear code to map on to a low-density h. log n k log n copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. convolutional codes and turbo codes this chapter follows tightly on from chapter it makes use of the ideas of codes and trellises and the forwardbackward algorithm. introduction to convolutional codes when we studied linear block codes we described them in three ways the generator matrix describes how to turn a string of k arbitrary source bits into a transmission of n bits. the parity-check matrix the m n k parity-check con straints that a valid codeword the trellis of the code describes its valid codewords in terms of paths through a trellis with labelled edges. a fourth way of describing some block codes the algebraic approach is not covered in this book because it has been well covered by numerous other books in coding theory because as this part of the book discusses the state-of-the-art in error-correcting codes makes little use of algebraic coding theory and because i am not competent to teach this subject. we will now describe convolutional codes in two ways in terms of mechanisms for generating transmissions t from source bits s and second in terms of trellises that describe the constraints by valid transmissions. linear-feedback shift-registers we generate a transmission with a convolutional code by putting a source stream through a linear this makes use of a shift register linear output functions and possibly linear feedback. i will draw the shift-register in a right-to-left orientation bits roll from right to left as time goes on. figure shows three linear-feedback shift-registers which could be used to convolutional codes. the rectangular box surrounding the bits indicates the memory of the also known as its state. all three have one input and two outputs. on each clock cycle the source supplies one bit and the outputs two bits ta and tb. by concatenating together these bits we can obtain from our source stream a transmission stream ta because there are two transmitted bits for every source bit the codes shown in have rate because tb ta tb ta tb copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. linear-feedback shift-registers figure linear-feedback shift-registers for generating convolutional codes with rate the symbol hd indicates a copying with a delay of one clock cycle. the symbol denotes linear addition modulo with no delay. the are systematic and nonrecursive nonsystematic and nonrecursive systematic and recursive. table how taps in the delay line are converted to octal. hd octal name ta s tb tb hd s ta hd tb ta s these require k bits of memory the codes they are known as a constraint-length codes. convolutional codes come in three corresponding to the three types of in systematic nonrecursive the shown in has no feedback. it also has the property that one of the output bits ta is identical to the source bit s. this encoder is thus called systematic because the source bits are reproduced transparently in the transmitted stream and nonrecursive because it has no feedback. the other transmitted bit tb is a linear function of the state of the one way of describing that function is as a dot product between two binary vectors of length k a binary vector gb and the state vector z we include in the state vector the bit that will be put into the bit of the memory on the next cycle. the vector gb has gb for every where there is a tap downward pointing arrow from state bit into the transmitted bit tb. a convenient way to describe these binary tap vectors is in octal. thus i have drawn the delay lines from this makes use of the tap vector right to left to make it easy to relate the diagrams to these octal numbers. nonsystematic nonrecursive the shown in also has no feedback but it is not systematic. it makes use of two tap vectors ga and gb to create its two transmitted bits. this encoder is thus nonsystematic and nonrecursive. because of their added complexity nonsystematic codes can have error-correcting abilities superior to those of systematic nonrecursive codes with the same constraint length. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. systematic recursive convolutional codes and turbo codes the shown in is similar to the nonsystematic nonrecursive shown in but it uses the taps that formerly made up ga to make a linear signal that is fed back into the shift register along with the source bit. the output tb is a linear function of the state vector as before. the other output is ta s so this is systematic. a recursive code is conventionally by an octal ratio e.g. ure s code is denoted by equivalence of systematic recursive and nonsystematic nonrecursive codes the two in are code-equivalent in that the sets of codewords that they are identical. for every codeword of the nonsystematic nonrecursive code we can choose a source stream for the other encoder such that its output is identical vice versa. ga to prove this we denote by p the quantity pk as shown in and b which shows a pair of smaller but otherwise equivalent if the two transmissions are to be equivalent that is the tas are equal in both and so are the tbs then on every cycle the source bit in the systematic code must be s ta. so now we must simply that for this choice of s the systematic code s shift register will follow the same state sequence as that of the nonsystematic code assuming that the states match initially. in we have tb hd s ta tb hd ta s figure two convolutional codes with constraint length k non-recursive recursive. the two codes are equivalent. ta p znonrecursive whereas in we have zrecursive ta p substituting for ta and using p p we immediately zrecursive znonrecursive thus any codeword of a nonsystematic nonrecursive code is a codeword of a systematic recursive code with the same taps the same taps in the sense that there are vertical arrows in all the same places in and though one of the arrows points up instead of down in now while these two codes are equivalent the two encoders behave differently. the nonrecursive encoder has a impulse response that is if one puts in a string that is all zeroes except for a single one the resulting output stream contains a number of ones. once the one bit has passed through all the states of the memory the delay line returns to the all-zero state. figure shows the state sequence resulting from the source string s figure shows the trellis of the recursive code of and the response of this to the same source string s the has an impulse response. the response settles into a periodic state with period equal to three clock cycles. exercise what is the input to the recursive such that its state sequence and the transmission are the same as those of the nonrecursive see copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. linear-feedback shift-registers transmit source transmit source transmit source figure trellises of the convolutional codes of it is assumed that the initial state of the is time is on the horizontal axis and the state of the at each time step is the vertical coordinate. on the line segments are shown the emitted symbols ta and tb with stars for and boxes for the paths taken through the trellises when the source sequence is are highlighted with a solid line. the light dotted lines show the state trajectories that are possible for other source sequences. figure the source sequence for the systematic recursive code produces the same path through the trellis as does in the nonsystematic nonrecursive case. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. convolutional codes and turbo codes hd tb ta s figure the trellis for a k code painted with the likelihood function when the received vector is equal to a codeword with just one bit there are three line styles depending on the value of the likelihood thick solid lines show the edges in the trellis that match the corresponding two bits of the received string exactly thick dotted lines show edges that match one bit but mismatch the other and thin dotted lines show the edges that mismatch both bits. received in general a linear-feedback shift-register with k bits of memory has an impulse response that is periodic with a period that is at most corresponding to the visiting every non-zero state in its state space. incidentally cheap pseudorandom number generators and cheap cryptographic products make use of exactly these periodic sequences though with larger values of k than the random number seed or cryptographic key selects the initial state of the memory. there is thus a close connection between certain cryptanalysis problems and the decoding of convolutional codes. decoding convolutional codes the receiver receives a bit stream and wishes to infer the state sequence and thence the source stream. the posterior probability of each bit can be found by the sumproduct algorithm known as the forwardbackward or bcjr algorithm which was introduced in section the most probable state sequence can be found using the minsum algorithm of section known as the viterbi algorithm. the nature of this task is illustrated in which shows the cost associated with each edge in the trellis for the case of a sixteen-state code the channel is assumed to be a binary symmetric channel and the received vector is equal to a codeword except that one bit has been there are three line styles depending on the value of the likelihood thick solid lines show the edges in the trellis that match the corresponding two bits of the received string exactly thick dotted lines show edges that match one bit but mismatch the other and thin dotted lines show the edges that mismatch both bits. the minsum algorithm seeks the path through the trellis that uses as many solid lines as possible more precisely it minimizes the cost of the path where the cost is zero for a solid line one for a thick dotted line and two for a thin dotted line. exercise can you spot the most probable path and the bit? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. figure two paths that in two transmitted bits only. figure a terminated trellis. when any codeword is completed the state is turbo codes transmit source transmit source unequal protection a defect of the convolutional codes presented thus far is that they unequal protection to the source bits. figure shows two paths through the trellis that in only two transmitted bits. the last source bit is less well protected than the other source bits. this unequal protection of bits motivates the termination of the trellis. a terminated trellis is shown in termination slightly reduces the number of source bits used per codeword. here four source bits are turned into parity bits because the k memory bits must be returned to zero. turbo codes an k turbo code is by a number of constituent convolutional encoders two and an equal number of interleavers which are k k permutation matrices. without loss of generality we take the interleaver to be the identity matrix. a string of k source bits is encoded by feeding them into each constituent encoder in the order by the associated interleaver and transmitting the bits that come out of each constituent encoder. often the constituent encoder is chosen to be a systematic encoder just like the recursive shown in and the second is a non-systematic one of rate that emits parity bits only. the transmitted codeword then consists of figure the encoder of a turbo code. each box contains a convolutional code. the source bits are reordered using a permutation before they are fed to the transmitted codeword is obtained by concatenating or interleaving the outputs of the two convolutional codes. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. convolutional codes and turbo codes figure and turbo codes represented as factor graphs. the circles represent the codeword bits. the two rectangles represent trellises of convolutional codes with the systematic bits occupying the left half of the rectangle and the parity bits occupying the right half. the puncturing of these constituent codes in the turbo code is represented by the lack of connections to half of the parity bits in each trellis. k source bits followed by parity bits generated by the convolutional code and parity bits from the second. the resulting turbo code has rate the turbo code can be represented by a factor graph in which the two trellises are represented by two large rectangular nodes the k source bits and the parity bits participate in the trellis and the k source bits and the last parity bits participate in the second trellis. each codeword bit participates in either one or two trellises depending on whether it is a parity bit or a source bit. each trellis node contains a trellis exactly like the terminated trellis shown in except one thousand times as long. are other factor graph representations for turbo codes that make use of more elementary nodes but the factor graph given here yields the standard version of the sumproduct algorithm used for turbo codes. if a turbo code of smaller rate such as is required a standard to the code is to puncture some of the parity bits turbo codes are decoded using the sumproduct algorithm described in chapter on the iteration each trellis receives the channel likelihoods and runs the forwardbackward algorithm to compute for each bit the relative likelihood of its being or given the information about the other bits. these likelihoods are then passed across from each trellis to the other and multiplied by the channel likelihoods on the way. we are then ready for the second iteration the forwardbackward algorithm is run again in each trellis using the updated probabilities. after about ten or twenty such iterations it s hoped that the correct decoding will be found. it is common practice to stop after some number of iterations but we can do better. as a stopping criterion the following procedure can be used at every iteration. for each time-step in each trellis we identify the most probable edge according to the local messages. if these most probable edges join up into two valid paths one in each trellis and if these two paths are consistent with each other it is reasonable to stop as subsequent iterations are unlikely to take the decoder away from this codeword. if a maximum number of iterations is reached without this stopping criterion being a decoding error can be reported. this stopping procedure is recommended for several reasons it allows a big saving in decoding time with no loss in error probability it allows decoding failures that are detected by the decoder to be so knowing that a particular block is corrupted is surely useful information for the receiver! and when we distinguish between detected and undetected errors the undetected errors give helpful insights into the low-weight codewords copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. parity-check matrices of convolutional codes and turbo codes of the code which may improve the process of code design. turbo codes as described here have excellent performance down to decoded error probabilities of about but randomly-constructed turbo codes tend to have an error starting at that level. this error is caused by lowweight codewords. to reduce the height of the error one can attempt to modify the random construction to increase the weight of these low-weight codewords. the tweaking of turbo codes is a black art and it never succeeds in totalling eliminating low-weight codewords more precisely the low-weight codewords can be eliminated only by the turbo code s excellent performance. in contrast low-density parity-check codes rarely have error as long as their number of columns is not too large exercise figure schematic pictures of the parity-check matrices of a convolutional code rate and a turbo code rate notation a diagonal line represents an identity matrix. a band of diagonal lines represent a band of diagonal a circle inside a square represents the random permutation of all the columns in that square. a number inside a square represents the number of random permutation matrices superposed in that square. horizontal and vertical lines indicate the boundaries of the blocks within the matrix. parity-check matrices of convolutional codes and turbo codes we close by discussing the parity-check matrix of a convolutional code viewed as a linear block code. we adopt the convention that the n bits of one block are made up of the bits ta followed by the bits tb. exercise prove that a convolutional code has a low-density parity check matrix as shown schematically in hint it s easiest to out the parity constraints by a convolutional code by thinking about the nonsystematic nonrecursive encoder consider putting through a a stream that s been through convolutional b and vice versa compare the two resulting streams. ignore termination of the trellises. the parity-check matrix of a turbo code can be written down by listing the constraints by the two constituent trellises so turbo codes are also special cases of low-density parity-check codes. if a turbo code is punctured it no longer necessarily has a low-density parity-check matrix but it always has a generalized parity-check matrix that is sparse as explained in the next chapter. further reading for further reading about convolutional codes johannesson and zigangirov is highly recommended. one topic i would have liked to include is sequential decoding. sequential decoding explores only the most promising paths in the trellis and backtracks when evidence accumulates that a wrong turning has been taken. sequential decoding is used when the trellis is too big for us to be able to apply the maximum likelihood algorithm the min sum algorithm. you can read about sequential decoding in johannesson and zigangirov for further information about the use of the sumproduct algorithm in turbo codes and the rarely-used but highly recommended stopping criteria for halting their decoding frey is essential reading. there s lots more good in the same book! solutions solution to exercise the bit was the most probable path is the upper one in copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. repeataccumulate codes in chapter we discussed a very simple and not very method for communicating over a noisy channel the repetition code. we now discuss a code that is almost as simple and whose performance is outstandingly good. repeataccumulate codes were studied by divsalar et al. for theoretical purposes as simple turbo-like codes that might be more amenable to analysis than messy turbo codes. their practical performance turned out to be just as good as other sparse-graph codes. the encoder take k source bits. sk repeat each bit three times giving n bits. sksksk permute these n bits using a random permutation random permutation the same one for every codeword. call the permuted string u. un transmit the accumulated sum. tn un tn un that s it! graph figure shows the graph of a repeataccumulate code using four types of node equality constraints intermediate binary variables circles parity constraints and the transmitted bits circles. the source sets the values of the black bits at the bottom three at a time and the accumulator computes the transmitted bits along the top. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. decoding total undetected figure factor graphs for a repeataccumulate code with rate using elementary nodes. each white circle represents a transmitted bit. each constraint forces the sum of the bits to which it is connected to be even. each black circle represents an intermediate binary variable. each variables to which it is connected to be equal. factor graph normally used for decoding. the top rectangle represents the trellis of the accumulator shown in the inset. constraint forces the three figure performance of six repeataccumulate codes on the gaussian channel. the blocklengths range from n to n vertical axis block error probability horizontal axis the dotted lines show the frequency of undetected errors. this graph is a factor graph for the prior probability over codewords with the circles being binary variable nodes and the squares representing contributes a factor of the form two types of factor nodes. as usual each contributes a factor of the form x mod each decoding the repeataccumulate code is normally decoded using the sumproduct algorithm on the factor graph depicted in the top box represents the trellis of the accumulator including the channel likelihoods. in the half of each iteration the top trellis receives likelihoods for every transition in the trellis and runs the forwardbackward algorithm so as to produce likelihoods for each variable node. in the second half of the iteration these likelihoods nodes to produce new likelihood messages to are multiplied together at the send back to the trellis. as with gallager codes and turbo codes the stop-when-it s-done decoding method can be applied so it is possible to distinguish between undetected errors are caused by low-weight codewords in the code and detected errors the decoder gets stuck and knows that it has failed to a valid answer. figure shows the performance of six randomly-constructed repeat accumulate codes on the gaussian channel. if one does not mind the error which kicks in at about a block error probability of the performance is staggeringly good for such a simple code copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. total detected undetected repeataccumulate codes db db empirical distribution of decoding times it is interesting to study the number of iterations of the sumproduct algorithm required to decode a sparse-graph code. given one code and a set of channel conditions the decoding time varies randomly from trial to trial. we that the histogram of decoding times follows a power law p for large the power p depends on the signal-to-noise ratio and becomes smaller that the distribution is more heavy-tailed as the signal-to-noise ratio decreases. we have observed power laws in repeataccumulate codes and in irregular and regular gallager codes. figures and show the distribution of decoding times of a repeataccumulate code at two signal-to-noise ratios. the power laws extend over several orders of magnitude. exercise investigate these power laws. does density evolution predict them? can the design of a code be used to manipulate the power law in a useful way? generalized parity-check matrices figure histograms of number of iterations to a valid decoding for a repeataccumulate code with source block length k and transmitted blocklength n block error probability versus signal-to-noise ratio for the ra code. histogram for db. db. iii.c fits of power laws to and and node to a i that it is helpful when relating sparse-graph codes to each other to use a common representation for them all. forney introduced the idea of and all variable nodes a normal graph in which the only nodes are have degree one or two variable nodes with degree two can be represented on node. the generalized parity-check matrix edges that connect a is a graphical way of representing normal graphs. in a parity-check matrix the columns are transmitted bits and the rows are linear constraints. in a generalized parity-check matrix additional columns may be included which represent state variables that are not transmitted. one way of thinking of these state variables is that they are punctured from the code before transmission. state variables are indicated by a horizontal line above the corresponding columns. the other pieces of diagrammatic notation for generalized parity copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. generalized parity-check matrices gt h figure the generator matrix parity-check matrix and a generalized parity-check matrix of a repetition code with rate fa pg check matrices are as in mackay et al. a diagonal line in a square indicates that that part of the matrix contains an identity matrix. two or more parallel diagonal lines indicate a band-diagonal matrix with a corresponding number of per row. a horizontal ellipse with an arrow on it indicates that the corresponding columns in a block are randomly permuted. a vertical ellipse with an arrow on it indicates that the corresponding rows in a block are randomly permuted. an integer surrounded by a circle represents that number of superposed random permutation matrices. a generalized parity-check matrix is a pair fa pg where a is a binary matrix and p is a list of the punctured bits. the matrix a set of valid vectors x satisfying ax for each valid vector there is a codeword tx that is obtained by puncturing from x the bits indicated by p. for any one code there are many generalized parity-check matrices. the rate of a code with generalized parity-check matrix fa pg can be estimated as follows. if a is l m and p punctures s bits and selects n bits for transmission n s then the number of constraints on the codeword m is the number of source bits is m s k n m l and the rate is greater than or equal to r m n s l s copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. gt gt h a p repeataccumulate codes figure the generator matrix and parity-check matrix of a systematic low-density generator-matrix code. the code has rate figure the generator matrix and generalized parity-check matrix of a non-systematic low-density generator-matrix code. the code has rate examples repetition code. the generator matrix parity-check matrix and generalized parity-check matrix of a simple repetition code are shown in in an k systematic lowsystematic low-density generator-matrix code. density generator-matrix code there are no state variables. a transmitted codeword t of length n is given by where t gts gt ik p with ik denoting the identity matrix and p being a very sparse m matrix where m n k. the parity-check matrix of this code is h in the case of a code this parity-check matrix might be represented as shown in non-systematic low-density generator-matrix code. in an k non-systematic low-density generator-matrix code a transmitted codeword t of length n is given by where gt is a very sparse n k matrix. the generalized parity-check matrix of this code is t gts and the corresponding generalized parity-check equation is a ax where x s t whereas the parity-check matrix of this simple code is typically a complex dense matrix the generalized parity-check matrix retains the underlying simplicity of the code. in the case of a code this generalized parity-check matrix might be represented as shown in low-density parity-check codes and linear mn codes. the parity-check matrix of a low-density parity-check code is shown in figure the generalized parity-check matrices of a gallager code with columns of weight a linear mn code. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. generalized parity-check matrices figure the generalized parity-check matrices of a convolutional code with rate a turbo code built by parallel concatenation of two convolutional codes. a linear mn code is a non-systematic low-density parity-check code. the k state bits of an mn code are the source bits. figure shows the generalized parity-check matrix of a linear mn code. in a non-systematic non-recursive convolutional code convolutional codes. the source bits which play the role of state bits are fed into a delay-line and two linear functions of the delay-line are transmitted. in these two parity streams are shown as two successive vectors of length k. is common to interleave these two parity streams a bit-reordering that is not relevant here and is not illustrated. concatenation. parallel concatenation of two codes is represented in one of these diagrams by aligning the matrices of two codes in such a way that the source bits line up and by adding blocks of zero-entries to the matrix such that the state bits and parity bits of the two codes occupy separate columns. an example is given by the turbo code that follows. in serial concatenation the columns corresponding to the transmitted bits of the code are aligned with the columns corresponding to the source bits of the second code. turbo codes. a turbo code is the parallel concatenation of two convolutional codes. the generalized parity-check matrix of a turbo code is shown in repeataccumulate codes. the generalized parity-check matrices of a repeataccumulate code is shown in repeat-accumulate codes are equivalent to staircase codes intersection. the generalized parity-check matrix of the intersection of two codes is made by stacking their generalized parity-check matrices on top of each other in such a way that all the transmitted bits columns are correctly aligned and any punctured bits associated with the two component codes occupy separate columns. figure the generalized parity-check matrix of a repeataccumulate code with rate copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about chapter the following exercise provides a helpful background for digital fountain codes. exercise an author proofreads his k book by inspecting random pages. he makes n page-inspections and does not take any precautions to avoid inspecting the same page twice. after n k page-inspections what fraction of pages do you expect have never been inspected? after n k page-inspections what is the probability that one or more pages have never been inspected? show that in order for the probability that all k pages have been inspected to be we require n k page-inspections. problem is commonly presented in terms of throwing n balls at random into k bins what s the probability that every bin gets at least one ball? copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. digital fountain codes digital fountain codes are record-breaking sparse-graph codes for channels with erasures. channels with erasures are of great importance. for example sent over the internet are chopped into packets and each packet is either received without error or not received. a simple channel model describing this situation is a q-ary erasure channel which has all inputs in the input alphabet a probability f of transmitting the input without error and probability f of delivering the output the alphabet size q is where l is the number of bits in a packet. common methods for communicating over such channels employ a feedback channel from receiver to sender that is used to control the retransmission of erased packets. for example the receiver might send back messages that identify the missing packets which are then retransmitted. alternatively the receiver might send back messages that acknowledge each received packet the sender keeps track of which packets have been acknowledged and retransmits the others until all packets have been acknowledged. these simple retransmission protocols have the advantage that they will work regardless of the erasure probability f but purists who have learned their shannon theory will feel that these retransmission protocols are wasteful. if the erasure probability f is large the number of feedback messages sent by the protocol will be large. under the second protocol it s likely that the receiver will end up receiving multiple redundant copies of some packets and heavy use is made of the feedback channel. according to shannon there is no need for the feedback channel the capacity of the forward channel is f bits whether or not we have feedback. the wastefulness of the simple retransmission protocols is especially evident in the case of a broadcast channel with erasures channels where one sender broadcasts to many receivers and each receiver receives a random fraction f of the packets. if every packet that is missed by one or more receivers has to be retransmitted those retransmissions will be terribly redundant. every receiver will have already received most of the retransmitted packets. so we would like to make erasure-correcting codes that require no feedback or almost no feedback. the classic block codes for erasure correction are called reedsolomon codes. an k reedsolomon code an alphabet of size q has the ideal property that if any k of the n transmitted symbols are received then the original k source symbols can be recovered. berlekamp or lin and costello for further information reedsolomon codes exist for n q. but reedsolomon codes have the disadvantage that they are practical only for small k n and q standard im copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. digital fountain codes lt stands for luby transform plementations of encoding and decoding have a cost of order log n packet operations. furthermore with a reedsolomon code as with any block code one must estimate the erasure probability f and choose the code rate r kn before transmission. if we are unlucky and f is larger than expected and the receiver receives fewer than k symbols what are we to do? we d like a simple way to extend the code on the to create a lower-rate k code. for reedsolomon codes no such method exists. there is a better way pioneered by michael luby at his company digital fountain the company whose business is based on sparse-graph codes. the digital fountain codes i describe here lt codes were invented by luby in the idea of a digital fountain code is as follows. the encoder is a fountain that produces an endless supply of water drops packets let s say the original source has a size of kl bits and each drop contains l encoded bits. now anyone who wishes to receive the encoded holds a bucket under the fountain and collects drops until the number of drops in the bucket is a little larger than k. they can then recover the original digital fountain codes are rateless in the sense that the number of encoded packets that can be generated from the source message is potentially limitless and the number of encoded packets generated can be determined on the regardless of the statistics of the erasure events on the channel we can send as many encoded packets as are needed in order for the decoder to recover the source data. the source data can be decoded from any set of k encoded packets for slightly larger than k practice about larger. digital fountain codes also have fantastically small encoding and decoding complexities. with probability k packets can be communicated with average encoding and decoding costs both of order k packet operations. luby calls these codes universal because they are simultaneously nearoptimal for every erasure channel and they are very as the length k grows. the overhead k is of order a digital fountain s encoder each encoded packet tn is produced from the source sk as follows randomly choose the degree dn of the packet from a degree distribution the appropriate choice of depends on the source size k as we ll discuss later. choose uniformly at random dn distinct input packets and set tn equal to the bitwise sum modulo of those dn packets. this sum can be done by successively exclusive-or-ing the packets together. this encoding operation a graph connecting encoded packets to source packets. if the mean degree is smaller than k then the graph is sparse. we can think of the resulting code as an irregular low-density generator-matrix code. the decoder needs to know the degree of each packet that is received and which source packets it is connected to in the graph. this information can be communicated to the decoder in various ways. for example if the sender and receiver have synchronized clocks they could use identical pseudo-random copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. the decoder a b c d e f figure example decoding for a digital fountain code with k source bits and n encoded bits. number generators seeded by the clock to choose each random degree and each set of connections. alternatively the sender could pick a random key given which the degree and the connections are determined by a pseudorandom process and send that key in the header of the packet. as long as the packet size l is much bigger than the key size need only be bits or so this key introduces only a small overhead cost. the decoder decoding a sparse-graph code is especially easy in the case of an erasure channel. the decoder s task is to recover s from t gs where g is the matrix associated with the graph. the simple way to attempt to solve this problem is by message-passing. we can think of the decoding algorithm as the sumproduct algorithm if we wish but all messages are either completely uncertain messages or completely certain messages. uncertain messages assert that a message packet sk could have any value with equal probability certain messages assert that sk has a particular value with probability one. this simplicity of the messages allows a simple description of the decoding process. we ll call the encoded packets ftng check nodes. find a check node tn that is connected to only one source packet sk. there is no such check node this decoding algorithm halts at this point and fails to recover all the source packets. set sk tn. add sk to all checks that are connected to sk sk for all such that remove all the edges connected to the source packet sk. repeat until all fskg are determined. this decoding process is illustrated in for a toy case where each packet is just one bit. there are three source packets by the upper circles and four received packets by the lower check symbols which have the values at the start of the algorithm. at the iteration the only check node that is connected to a sole source bit is the check node a. we set that source bit accordingly b discard the check node then add the value of to the checks to which it is connected c disconnecting from the graph. at the start of the second iteration c the fourth check node is connected to a sole source bit we set to in panel d and add to the two checks it is connected to e. finally we that two check nodes are both connected to and they agree about the value of we would hope! which is restored in panel f. designing the degree distribution the probability distribution of the degree is a critical part of the design occasional encoded packets must have high degree d similar to k in order to ensure that there are not some source packets that are connected to no-one. many packets must have low degree so that the decoding process copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. digital fountain codes can get started and keep going and so that the total number of addition operations involved in the encoding and decoding is kept small. for a given degree distribution the statistics of the decoding process can be predicted by an appropriate version of density evolution. ideally to avoid redundancy we d like the received graph to have the property that just one check node has degree one at each iteration. at each iteration when this check node is processed the degrees in the graph are reduced in such a way that one new degree-one check node appears. in expectation this ideal behaviour is achieved by the ideal soliton distribution for d k. the expected degree under this distribution is roughly ln k. exercise derive the ideal soliton distribution. at the iteration let the number of packets of degree d be show that d the expected number of packets of degree d that have their degree reduced to d is and at the tth iteration when t of the k packets have been recovered and the number of packets of degree d is htd the expected number of packets of degree d that have their degree reduced to d is htddk t. hence show that in order to have the expected number of packets of degree satisfy for all t k we must to start with have and and more generally then by recursion solve for for d upwards. this degree distribution works poorly in practice because around the expected behaviour make it very likely that at some point in the decoding process there will be no degree-one check nodes and furthermore a few source nodes will receive no connections at all. a small these problems. the robust soliton distribution has two extra parameters c and it is designed to ensure that the expected number of degree-one checks is about s c rather than throughout the decoding process. the parameter is a bound on the probability that the decoding fails to run to completion after a certain number of packets have been received. the parameter c is a constant of order if our aim is to prove luby s main theorem about lt codes in practice however it can be viewed as a free parameter with a value somewhat smaller than giving good results. we a positive function d s k s k for d for d ks for d ks and exercise then add the ideal soliton distribution to and normalize to obtain the robust soliton distribution z where z pd the number of encoded packets required at the receiving end to ensure that the decoding can run to completion with probability at least is kz. rho tau figure the distributions and for the case k c which gives s ks and z the distribution is largest at d and d ks. c figure the number of degree-one checks s and the quantity k as a function of the two parameters c and for k luby s main theorem proves that there exists a value of c such that given k received packets the decoding algorithm will recover the k source packets with probability copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. applications figure histograms of the actual number of packets n required in order to recover a of size k packets. the parameters were as follows top histogram c ks and z middle c ks and z bottom c ks and z luby s analysis explains how the small-d end of has the role of ensuring that the decoding process gets started and the spike in at d ks is included to ensure that every source packet is likely to be connected to a check at least once. luby s key result is that an appropriate value of the constant c receiving k checks ensures that all packets can be recovered with probability at least in the illustrative i have set the allowable decoder failure probability quite large because the actual failure probability is much smaller than is suggested by luby s conservative analysis. in practice lt codes can be tuned so that a of original size k packets is recovered with an overhead of about figure shows histograms of the actual number of packets required for a couple of settings of the parameters achieving mean overheads smaller than and respectively. applications digital fountain codes are an excellent solution in a wide variety of situations. let s mention two. storage you wish to make a backup of a large but you are aware that your magnetic tapes and hard drives are all unreliable in the sense that catastrophic failures in which some stored packets are permanently lost within one device occur at a rate of something like per day. how should you store your a digital fountain can be used to spray encoded packets all over the place on every storage device available. then to recover the backup whose size was k packets one simply needs to k packets from anywhere. corrupted packets do not matter we simply skip over them and more packets elsewhere. this method of storage also has advantages in terms of speed of recovery. in a hard drive it is standard practice to store a in successive sectors of a hard drive to allow rapid reading of the but if as occasionally happens a packet is lost to the reading head being track for a moment giving a burst of errors that cannot be corrected by the packet s error-correcting code a whole revolution of the drive must be performed to bring back the packet to the head for a second read. the time taken for one revolution produces an undesirable delay in the system. if were instead stored using the digital fountain principle with the digital drops stored in one or more consecutive sectors on the drive then one would never need to endure the delay of re-reading a packet packet loss would become less important and the hard drive could consequently be operated faster with higher noise level and with fewer resources devoted to noisychannel coding. exercise compare the digital fountain method of robust storage on multiple hard drives with raid redundant array of independent disks. broadcast imagine that ten thousand subscribers in an area wish to receive a digital movie from a broadcaster. the broadcaster can send the movie in packets copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. digital fountain codes over a broadcast network for example by a wide-bandwidth phone line or by satellite. imagine that not all packets are received at all the houses. let s say f of them are lost at each house. in a standard approach in which the is transmitted as a plain sequence of packets with no encoding each house would have to notify the broadcaster of the f k missing packets and request that they be retransmitted. and with ten thousand subscribers all requesting such retransmissions there would be a retransmission request for almost every packet. thus the broadcaster would have to repeat the entire broadcast twice in order to ensure that most subscribers have received the whole movie and most users would have to wait roughly twice as long as the ideal time before the download was complete. if the broadcaster uses a digital fountain to encode the movie each subscriber can recover the movie from any k packets. so the broadcast needs to last for only say packets and every house is very likely to have successfully recovered the whole another application is broadcasting data to cars. imagine that we want to send updates to in-car navigation databases by satellite. there are hundreds of thousands of vehicles and they can receive data only when they are out on the open road there are no feedback channels. a standard method for sending the data is to put it in a carousel broadcasting the packets in a periodic sequence. yes a car may go through a tunnel and miss out on a few hundred packets but it will be able to collect those missed packets an hour later when the carousel has gone through a full revolution hope or maybe the following day if instead the satellite uses a digital fountain each car needs to receive only an amount of data equal to the original size further reading the encoders and decoders sold by digital fountain have even higher than the lt codes described here and they work well for all blocklengths not only large lengths such as k shokrollahi presents raptor codes which are an extension of lt codes with linear-time encoding and decoding. further exercises exercise understanding the robust soliton distribution. repeat the analysis of exercise but now aim to have the expected number of packets of degree be s for all t instead of show that the initial required number of packets is k dd s d for d the reason for truncating the second term beyond d ks and replacing it by the spike at d ks equation is to ensure that the decoding complexity does not grow larger than ok ln k. estimate the expected number of packets pd and the expected number of edges in the sparse graph pd determines the decoding complexity if the histogram of packets is as given in compare with the expected numbers of packets and edges when the robust soliton distribution is used. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. further exercises exercise show that the spike at d ks is an ade quate replacement for the tail of high-weight packets in exercise investigate experimentally how necessary the spike at d ks is for successful decoding. investigate also whether the tail of beyond d ks is necessary. what happens if all highweight degrees are removed both the spike at d ks and the tail of beyond d ks? exercise fill in the details in the proof of luby s main theorem that receiving k checks ensures that all the source packets can be recovered with probability at least exercise optimize the degree distribution of a digital fountain code for a of k packets. pick a sensible objective function for your optimization such as minimizing the mean of n the number of packets required for complete decoding or the percentile of the histogram of n exercise make a model of the situation where a data stream is broadcast to cars and quantify the advantage that the digital fountain has over the carousel method. exercise construct a simple example to illustrate the fact that the digital fountain decoder of section is suboptimal it sometimes gives up even though the information available is to decode the whole how does the cost of the optimal decoder compare? exercise if every transmitted packet were created by adding together source packets at random with probability of each source packet s being included show that the probability that k received packets for the optimal decoder to be able to recover the k source packets is just a little below put it another way what is the probability that a random k k matrix has full rank? show that if k packets are received the probability that they will not for the optimal decoder is roughly exercise implement an optimal digital fountain decoder that uses the method of richardson and urbanke derived for fast encoding of sparse-graph codes to handle the matrix inversion required for optimal decoding. now that you have changed the decoder you can reoptimize the degree distribution using higher-weight packets. by how much can you reduce the overhead? the assertion that this approach makes digital fountain codes viable as erasure-correcting codes for all blocklengths not just the large blocklengths for which lt codes are excellent. exercise digital fountain codes are excellent rateless codes for erasure channels. make a rateless code for a channel that has both erasures and noise. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. digital fountain codes summary of sparse-graph codes a simple method for designing error-correcting codes for noisy channels pioneered by gallager has recently been rediscovered and generalized and communication theory has been transformed. the practical performance of gallager s low-density parity-check codes and their modern cousins is vastly better than the performance of the codes with which textbooks have been in the intervening years. which sparse-graph code is best for a noisy channel depends on the chosen rate and blocklength the permitted encoding and decoding complexity and the question of whether occasional undetected errors are acceptable. lowdensity parity-check codes are the most versatile it s easy to make a competitive low-density parity-check code with almost any rate and blocklength and low-density parity-check codes virtually never make undetected errors. for the special case of the erasure channel the sparse-graph codes that are best are digital fountain codes. conclusion the best solution to the communication problem is combine a simple pseudo-random code with a message-passing decoder. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. part vii appendices copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a notation what does p b c mean? p b c is pronounced the probability that a is true given that b is true and c is true or more the probability of a given b and c chapter what do log and ln mean? in this book log x means the base-two loga rithm x ln x means the natural logarithm loge x. what does mean? usually a hat over a variable denotes a guess or es timator. so is a guess at the value of s. integrals. there is no between r f du andr du f the inte grand is f in both cases. what does but it denotes a product. it s pronounced product over n from to n so for example n n mean? this is like the summation pn n n n exp n ln n i like to choose the name of the free variable in a sum or a product here n to be the lower case version of the range of the sum. so n usually runs from to n and m usually runs from to m this is a habit i learnt from yaser abu-mostafa and i think it makes formulae easier to understand. what does mean? this is pronounced n choose n and it is the number of ways of selecting an unordered set of n objects from a set of size n n n! n! this function is known as the combination function. what is the gamma function is by r du for x the gamma function is an extension of the factorial function to real number arguments. in general and for integer arguments x!. the digamma function is by d dx ln for large x practical purposes x ln lnx x ln copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a notation and for small x practical purposes x ln ln where is euler s constant. x what does rc mean? just as denotes the inverse function to s sinx so there is potential confusion when people use x to denote since then we might expect s to denote sins i therefore like to avoid using the notation x. is the inverse function to h what does mean? the answer depends on the context. often a prime is used to denote d dx f similarly a dot denotes with respect to time t d dt x however the prime is also a useful indicator for another variable for example a new value for a variable so for example might denote the new value of x also if there are two integers that both range from to n i will often name those integers n and so my rule is if a prime occurs in an expression that could be a function such as or then it denotes otherwise it indicates another variable what is the error function? of this function vary. i it to be the cumulative probability of a standard normal distribution z dz what does er mean? er is pronounced the expected value of r or the expectation of r and it is the mean value of r. another symbol for expected value is the pair of angle-brackets hri what does jxj mean? the vertical bars j j have two meanings. if a is a set then jaj denotes the number of elements in the set if x is a number then jxj is the absolute value of x. what does mean? here a and p are matrices with the same number of rows. denotes the double-width matrix obtained by putting a alongside p. the vertical bar is used to avoid confusion with the product ap. what does xt mean? the superscript t is pronounced transpose trans posing a row-vector turns it into a column vector a and vice versa. are column vectors. my vectors indicated by bold face type similarly matrices can be transposed. if mij is the entry in row i and column j of matrix m and n mt then nji mij. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. a notation what are trace m and det m? the trace of a matrix is the sum of its di agonal elements trace m mii the determinant of m is denoted det m. what does mean? the matrix is the identity matrix. if m n if m n. another name for the identity matrix is i or sometimes i include a subscript on this symbol which indicates the size of the matrix k. what does mean? the delta function has the property z dx f f another possible meaning for is the truth function which is if the proposition s is true but i have adopted another notation for that. after all the symbol is quite busy already with the two roles mentioned above in addition to its role as a small real number and an increment operator in what does mean? is the truth function which is if the proposition s is true and otherwise. for example the number of positive numbers in the set t can be written what is the between and in an algorithm x y means that the variable x is updated by assigning it the value of y. in contrast x y is a proposition a statement that x is equal to y. see chapters and for further and notation relating to probability distributions. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. b some physics about phase transitions a system with states x in contact with a heat bath at temperature t has probability distribution p the partition function is the inverse temperature can be interpreted as an exchange rate between entropy and energy. is the amount of energy that must be given to a heat bath to increase its entropy by one nat. often the system will be by some other parameters such as the volume of the box it is in v in which case z is a function of v too v for any system with a number of states the function is evidently a continuous function of since it is simply a sum of exponentials. moreover all the derivatives of with respect to are continuous too. what phase transitions are all about however is this phase transitions correspond to values of and v critical points at which the derivatives of z have discontinuities or divergences. immediately we can deduce only systems with an number of states can show phase transitions. often we include a parameter n describing the size of the system. phase transitions may appear in the limit n real systems may have a value of n like if we make the system large by simply grouping together n independent systems whose partition function is then nothing interesting happens. the partition function for n independent identical systems is simply zn now while this function zn may be a very rapidly varying function of that doesn t mean it is showing phase transitions. the natural way to look at the partition function is in the logarithm ln zn n ln copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. b some physics duplicating the original system n times simply scales up all properties like the energy and heat capacity of the system by a factor of n so if the original system showed no phase transitions then the scaled up system won t have any either. only systems with long-range correlations show phase transitions. long-range correlations do not require long-range energetic couplings for example a magnet has only short-range couplings adjacent spins but these are to create long-range order. why are points at which derivatives diverge interesting? the derivatives of ln z describe properties like the heat capacity of the system s the second derivative or its in energy. if the second derivative of ln z diverges at a temperature then the heat capacity of the system diverges there which means it can absorb or release energy without changing temperature of ice melting in ice water when the system is at equilibrium at that temperature its energy a lot in contrast to the normal law-of-large-numbers behaviour where the energy only varies by one part in pn a toy system that shows a phase transition imagine a collection of n coupled spins that have the following energy as a function of their state x ex x otherwise. this energy function describes a ground state in which all the spins are aligned in the zero direction the energy per spin in this state is if any spin changes state then the energy is zero. this model is like an extreme version of a magnetic interaction which encourages pairs of spins to be aligned. we can contrast it with an ordinary system of n independent spins whose energy is like the system the system of independent spins has a single ground state with energy and it has roughly states with energy very close to so the low-temperature and high-temperature properties of the independent-spin system and the coupled-spin system are virtually identical. the partition function of the coupled-spin system is the function is sketched in along with its low temperature behaviour ln ln n and its high temperature behaviour ln n ln copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. about phase transitions log z n beta epsilon n log vare vare beta beta figure partition function of toy system which shows a phase transition for large n the arrow marks the point log the same for larger n the variance of the energy of the system as a function of for two system sizes. as n increases the variance has an increasingly sharp peak at the critical point contrast with figure the partition function and energy-variance of a system consisting of n independent spins. the partition function changes gradually from one asymptote to the other regardless of how large n is the variance of the energy does not have a peak. the are largest at high temperature and scale linearly with system size n log z n beta epsilon n log beta log z n beta epsilon n log vare vare beta beta the arrow marks the point ln at which these two asymptotes intersect. in the limit n the graph of ln becomes more and more sharply bent at this point the second derivative of ln z which describes the variance of the energy of the system has a peak value at ln roughly equal to n which corresponds to the system spending half of its time in the ground state and half its time in the other states. at this critical point the heat capacity of this system is thus proportional to n the heat capacity per spin is proportional to n which for n is in contrast to the behaviour of systems away from phase transitions whose capacity per atom is a number. for comparison shows the partition function and energy-variance of the ordinary independent-spin system. more generally phase transitions can be categorized into and continuous transitions. in a phase transition there is a discontinuous change of one copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. b some physics or more order-parameters in a continuous transition all order-parameters change continuously. s an order-parameter? a scalar function of the state of the system or to be precise the expectation of such a function. in the vicinity of a critical point the concept of typicality in chapter does not hold. for example our toy system at its critical point has a chance of being in a state with energy and roughly a chance of being in each of the other states that have energy zero. it is thus not the case that ln is very likely to be close to the entropy of the system at this point unlike a system with n i.i.d. components. remember that information content and energy are very closely related. if typicality holds then the system s energy has negligible and vice versa. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. c some mathematics finite theory most linear codes are expressed in the language of galois theory why are galois an appropriate language for linear codes? first a and some examples. a f is a set f f such that f forms an abelian group under an addition operation with being the identity means all elements commute i.e. satisfy a b b a. f forms an abelian group under a multiplication operation multiplication of any element by yields these operations satisfy the distributive rule c c c. for example the real numbers form a with and denoting ordinary addition and multiplication. a galois gf is a with a number of elements q. a unique galois exists for any q pm where p is a prime number and m is a positive integer there are no other gf the addition and multiplication tables for gf are shown in ta ble these are the rules of addition and multiplication modulo gf for any prime number p the addition and multiplication rules are those for ordinary addition and multiplication modulo p. gf the rules for gf with m are not those of ordinary addition and multiplication. for example the tables for gf are not the rules of addition and multiplication modulo notice that for example. so how can gf be described? it turns out that the elements can be related to polynomials. consider polynomial functions of x of degree and with that are elements of gf the polynomials shown in table obey the addition and multiplication rules of gf if addition and multiplication are modulo the polynomial x and the of the polynomials are from gf for example b b x a. each element may also be represented as a bit pattern as shown in table with addition being bitwise modulo and multiplication with an appropriate carry operation. table addition and multiplication tables for gf a b a b b a a a b b b a a b a b a a b b b a table addition and multiplication tables for gf element polynomial bit pattern a b x x table representations of the elements of gf copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. c some mathematics gf we can denote the elements of gf by a b c d e fg. each element can be mapped onto a polynomial over gf the multiplication and addition operations are given by multiplication and addition of the polynomials modulo x the multiplication table is given below. element polynomial binary representation a b c d e f x x x x a b c d e f a b c d e f a a c e b f d b b e d f c a c c b f e a d d d c a f b e e e f d b a c f f d a e c b why are galois relevant to linear codes? imagine generalizing a binary generator matrix g and binary vector s to a matrix and vector with elements from a larger set and generalizing the addition and multiplication operations that the product gs. in order to produce an appropriate input for a symmetric channel it would be convenient if for random s the product gs produced all elements in the enlarged set with equal probability. this uniform distribution is easiest to guarantee if these elements form a group under both addition and multiplication because then these operations do not break the symmetry among the elements. when two random elements of a multiplicative group are multiplied together all elements are produced with equal probability. this is not true of other sets such as the integers for which the multiplication operation is more likely to give rise to some elements composite numbers than others. galois by their avoid such symmetry-breaking eigenvectors and eigenvalues a right-eigenvector of a square matrix a is a non-zero vector er that aer where is the eigenvalue associated with that eigenvector. the eigenvalue may be a real number or complex number and it may be zero. eigenvectors may be real or complex. a left-eigenvector of a matrix a is a vector el that la et l the following statements for right-eigenvectors also apply to left-eigenvectors. if a matrix has two or more linearly independent right-eigenvectors with the same eigenvalue then that eigenvalue is called a degenerate eigenvalue of the matrix or a repeated eigenvalue. any linear combination of those eigenvectors is another right-eigenvector with the same eigenvalue. the principal right-eigenvector of a matrix is by the right eigenvector with the largest associated eigenvalue. if a real matrix has a right-eigenvector with complex eigenvalue x yi then it also has a right-eigenvector with the conjugate eigenvalue x yi. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. eigenvectors and eigenvalues symmetric matrices if a is a real symmetric n n matrix then all the eigenvalues and eigenvectors of a are real every left-eigenvector of a is also a right-eigenvector of a with the same eigenvalue and vice versa a set of n eigenvectors and eigenvalues fea are orthonormal that is can be found that the matrix can be expressed as a weighted sum of outer products of the eigenvectors a n i often use i and n as indices for sets of size i and n i will use the indices a and b to run over eigenvectors even if there are n of them. this is to avoid confusion with the components of the eigenvectors which are indexed by n e.g. ea n general square matrices an n n matrix can have up to n distinct eigenvalues. generically there are n eigenvalues all distinct and each has one left-eigenvector and one righteigenvector. in cases where two or more eigenvalues coincide for each distinct eigenvalue that is non-zero there is at least one left-eigenvector and one righteigenvector. left- and right-eigenvectors that have eigenvalue are orthogonal that is if then ea l r non-negative matrices if all the elements of a non-zero matrix c satisfy cmn then c is a non-negative matrix. similarly if all the elements of a non-zero vector c satisfy cn then c is a non-negative vector. properties. a non-negative matrix has a principal eigenvector that is nonnegative. it may also have other eigenvectors with the same eigenvalue that are not non-negative. but if the principal eigenvalue of a non-negative matrix is not degenerate then the matrix has only one principal eigenvector and it is non-negative. generically all the other eigenvalues are smaller in absolute magnitude. can be several eigenvalues of identical magnitude in special cases. transition probability matrices an important example of a non-negative matrix is a transition probability matrix q. a transition probability matrix q has columns that are probability vectors that is it q and qij for all j xi copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. matrix eigenvalues and eigenvectors el er c some mathematics table some matrices and their eigenvectors. matrix eigenvalues and eigenvectors el er table transition probability matrices for generating random paths through trellises. this property can be rewritten in terms of the all-ones vector n so n is the principal left-eigenvector of q with eigenvalue ntq nt l n because it is a non-negative matrix q has a principal right-eigenvector that is non-negative r generically for markov processes that are ergodic this eigenvector is the only right-eigenvector with eigenvalue of magnitude table for illustrative exceptions. this vector if we normalize it such that r is called the invariant distribution of the transition probability matrix. it is the probability density that is left unchanged under q. unlike the principal left-eigenvector which we explicitly above we can t usually identify the principal right-eigenvector without computation. the matrix may have up to n other right-eigenvectors all of which are orthogonal to the left-eigenvector n that is they are zero-sum vectors. perturbation theory perturbation theory is not used in this book but it is useful in this book s in this section we derive perturbation theory for the eigenvectors and eigenvalues of square not necessarily symmetric matrices. most presentations of perturbation theory focus on symmetric matrices but nonsymmetric matrices as transition matrices also deserve to be perturbed! copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. perturbation theory matrix eigenvalues and eigenvectors el er table illustrative transition probability matrices and their eigenvectors showing the two ways of being non-ergodic. more than one principal eigenvector with eigenvalue because the state space falls into two unconnected pieces. a small perturbation breaks the degeneracy of the principal eigenvectors. under this chain the density may oscillate between two parts of the state space. in addition to the invariant distribution there is another right-eigenvector with eigenvalue in general such circulating densities correspond to complex eigenvalues with magnitude we assume that we have an n n matrix h that is a function of a real parameter with being our starting point. we assume that a taylor expansion of is appropriate where v we assume that for all of interest has a complete set of n righteigenvectors and left-eigenvectors and that these eigenvectors and their eigenvalues are continuous functions of this last assumption is not necessarily a good one if has degenerate eigenvalues then it is possible for the eigenvectors to be discontinuous in in such cases degenerate perturbation theory is needed. that s a fun topic but let s stick with the non-degenerate case here. we write the eigenvectors and eigenvalues as follows r r and we taylor-expand with and ea r ea r r copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. with c some mathematics r f r l and f and similar for ea l we these left-vectors to be row vectors so that the transpose operation is not needed and can be banished. we are free to constrain the magnitudes of the eigenvectors in whatever way we please. each left-eigenvector and each right-eigenvector has an arbitrary magnitude. the natural constraints to use are as follows. first we constrain the inner products with ea l r for all a expanding the eigenvectors in equation implies l l r r from which we can extract the terms in which say ea l r f l ea r we are now free to choose the two constraints l ea r l ea f r which in the special case of a symmetric matrix correspond to constraining the eigenvectors to be of constant length as by the euclidean norm. ok now that we have our cast of characters what do the equations and tell us about our taylor expansions and we expand equation in r r r r identifying the terms of order we have r vea r r r we can extract interesting results from this equation by hitting it with eb l l eb r eb l r eb l r l r l r eb l r l r setting b a we obtain ea l r alternatively choosing b a we obtain eb l r eb l r eb l r l r eb now assuming that the right-eigenvectors feb we must be able to write r form a complete basis f r wbeb r copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. perturbation theory where wb eb l r so comparing and we have f r eb l r eb r equations and are the solution to the perturbation theory problem giving respectively the derivative of the eigenvalue and the eigenvectors. second-order perturbation theory if we expand the eigenvector equation to second order in and assume that the equation is exact that is h is a purely linear function of then we have r r r r r r and are the second derivatives of the eigenvector and eigenvalue. where ga equating the second-order terms in in equation r vf r r r r r hitting this equation on the left with ea l we obtain l ea r l r the term ea r l l r l r l r is equal to zero because of our constraints so so the second derivative of the eigenvalue with respect to is given by ea l r ea l eb l r r eb r l l r this is as far as we will take the perturbation expansion. summary if we introduce the abbreviation vba for eb eigenvectors of to order as l r we can write the ea r ea r vba eb r and the eigenvalues to second order as vbavab copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. some numbers c some mathematics number of distinct number of states of a ising model with spins number of binary strings of length number of binary strings of length having and number of electrons in universe number of electrons in solar system number of electrons in the earth age of universepicoseconds age of universeseconds number of neurons in human brain number of bits stored on a dvd number of bits in the wheat genome number of bits in the human genome population of earth number of bits in c. elegans worm genome number of bits in arabidopsis thaliana plant related to broccoli genome one yearseconds number of bits in the compressed postscript that is this book number of bits in unix kernel number of bits in the e. coli genome or in a disk number of years since humanchimpanzee divergence number of in the corpus callosum number of base pairs in a gene number of generations since humanchimpanzee divergence number of genes in human genome number of genes in arabidopsis thaliana genome lifetime probability of dying from smoking one pack of cigarettes per day. lifetime probability of dying in a motor vehicle accident lifetime probability of developing cancer because of drinking litres per day of water containing p.p.b. benzene probability of error in transmission of coding dna per nucleotide per generation probability of undetected error in a hard disk drive after error correction copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bibliography abrahamsen p. a review of gaussian random and correlation functions. technical report norwegian computing center blindern oslo norway. edition. abramson n. information theory and coding. mcgraw hill. adler s. l. over-relaxation method for the monte-carlo evaluation of the partition function for multiquadratic actions. physical review d particles and fields aiyer s. v. b. solving combinatorial optimization problems using neural networks. cambridge univ. engineering dept. phd dissertation. cuedf-infengtr aji s. jin h. khandekar a. mceliece r. j. and mackay d. j. c. bsc thresholds for code ensembles based on typical pairs decoding. in codes systems and graphical models ed. by b. marcus and j. rosenthal volume of ima volumes in mathematics and its applications pp. springer. amari s. cichocki a. and yang h. h. a new learning algorithm for blind signal separation. in advances in neural information processing systems ed. by d. s. touretzky m. c. mozer and m. e. hasselmo volume pp. mit press. storing numbers of patterns in a spin glass model of neural networks. phys. rev. lett. amit d. j. gutfreund h. and sompolinsky h. angel j. r. p. wizinowich p. lloyd-hart m. and sandler d. adaptive optics for array telescopes using neural-network techniques. nature bahl l. r. cocke j. jelinek f. and raviv j. optimal decoding of linear codes for minimizing symbol error rate. ieee trans. info. theory baldwin j. a new factor in evolution. american natu ralist bar-shalom y. and fortmann t. tracking and data association. academic press. barber d. and williams c. k. i. gaussian processes for bayesian via hybrid monte carlo. in neural information processing systems ed. by m. c. mozer m. i. jordan and t. petsche pp. mit press. barnett s. matrix methods for engineers and scientists. mcgraw-hill. battail g. we can think of good codes and even decode them. in eurocode udine italy october ed. by p. camion p. charpin and s. harari number in cism courses and lectures pp. springer. baum e. boneh d. and garrett c. on genetic in proc. eighth annual conf. on computational algorithms. learning theory pp. acm. baum e. b. and smith w. d. best play for imperfect players and game tree search. technical report nec princeton nj. baum e. b. and smith w. d. a bayesian approach to relevance in game playing. intelligence baum l. e. and petrie t. inference for probabilistic functions of markov chains. ann. math. stat. statistical beal m. j. ghahramani z. and rasmussen c. e. the hidden markov model. in advances in neural information processing systems mit press. bell a. j. and sejnowski t. j. an information maximization approach to blind separation and blind deconvolution. neural computation bentley j. programming pearls. addison-wesley sec ond edition. berger j. statistical decision theory and bayesian anal ysis. springer. berlekamp e. r. algebraic coding theory. mcgraw hill. berlekamp e. r. the technology of error-correcting codes. ieee trans. info. theory berlekamp e. r. mceliece r. j. and van tilborg h. c. a. on the intractability of certain coding problems. ieee trans. info. theory berrou c. and glavieux a. near optimum error corieee trans. on recting coding and decoding turbo-codes. communications berrou c. glavieux a. and thitimajshima p. near shannon limit error-correcting coding and decoding turbocodes. in proc. ieee international conf. on communications geneva switzerland pp. berzuini c. best n. g. gilks w. r. and larizza c. dynamic conditional independence models and markov chain monte carlo methods. j. american statistical assoc. berzuini c. and gilks w. r. following a moving target monte carlo inference for dynamic bayesian models. j. royal statistical society series b statistical methodology bhattacharyya a. on a measure of divergence between two statistical populations by their probability distributions. bull. calcutta math. soc. bishop c. m. exact calculation of the hessian matrix for the multilayer perceptron. neural computation bishop c. m. neural networks for pattern recognition. oxford univ. press. bishop c. m. winn j. m. and spiegelhalter d. vibes a variational inference engine for bayesian networks. in advances in neural information processing systems xv ed. by s. becker s. thrun and k. obermayer. blahut r. e. principles and practice of information theory. addison-wesley. bottou l. howard p. g. and bengio y. the zcoder adaptive binary coder. in proc. data compression conf. snowbird utah march pp. box g. e. p. and tiao g. c. bayesian inference in statistical analysis. addisonwesley. braunstein a. m. and zecchina r. survey propagation an algorithm for copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bibliography bretthorst g. bayesian spectrum analysis and parameter estimation. springer. also available at bayes.wustl.edu. bridle j. s. probabilistic interpretation of feedforward network outputs with relationships to statistical pattern recognition. in neuro-computing algorithms architectures and applications ed. by f. fougelman-soulie and j. springerverlag. bulmer m. the mathematical theory of quantitative genetics. oxford univ. press. burrows m. and wheeler d. j. a block-sorting lossless data compression algorithm. technical report digital src. byers j. luby m. mitzenmacher m. and rege a. a digital fountain approach to reliable distribution of bulk data. in proc. acm sigcomm september cairns-smith a. g. seven clues to the origin of life. cambridge univ. press. calderbank a. r. and shor p. w. good quantum error-correcting codes exist. phys. rev. a quant-ph carroll l. alice s adventures in wonderland and and what alice found there. through the looking-glass macmillan children s books. childs a. m. patterson r. b. and mackay d. j. c. exact sampling from non-attractive distributions using summary states. physical review e chu w. keerthi s. s. and ong c. j. a loss function in bayesian framework for support vector regression. in proc. international conf. on machine learning pp. chu w. keerthi s. s. and ong c. j. a new bayesian design method for support vector in special section on support vector machines of the international conf. on neural information processing. chu w. keerthi s. s. and ong c. j. bayesian support vector regression using a loss function. ieee trans. on neural networks. submitted. chu w. keerthi s. s. and ong c. j. bayesian trigonometric support vector neural computation. chung s.-y. richardson t. j. and urbanke r. l. analysis of sum-product decoding of low-density parity-check codes using a gaussian approximation. ieee trans. info. theory chung s.-y. urbanke r. l. and richardson t. j. ldpc code design applet. lids.mit.edusychung gaopt.html. comon p. jutten c. and herault j. blind separation of sources. problems statement. signal processing copas j. b. regression prediction and shrinkage discussion. j. r. statist. soc. b cover t. m. geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. ieee trans. on electronic computers cover t. m. and thomas j. a. elements of informa tion theory. wiley. cowles m. k. and carlin b. p. markov-chain montecarlo convergence diagnostics a comparative review. j. american statistical assoc. cox r. probability frequency and reasonable expecta tion. am. j. physics cressie n. statistics for spatial data. wiley. davey m. c. error-correction using low-density parity check codes. univ. of cambridge phd dissertation. davey m. c. and mackay d. j. c. low density parity check codes over gfq. ieee communications letters davey m. c. and mackay d. j. c. watermark codes reliable communication over insertiondeletion channels. in proc. ieee international symposium on info. theory p. davey m. c. and mackay d. j. c. reliable communication over channels with insertions deletions and substitutions. ieee trans. info. theory dawid a. stone m. and zidek j. critique of e.t jaynes s paradoxes of probability theory technical report dept. of statistical science univ. college london. dayan p. hinton g. e. neal r. m. and zemel r. s. the helmholtz machine. neural computation divsalar d. jin h. and mceliece r. j. coding theorems for turbo-like codes. in proc. allerton conf. on communication control and computing sept. pp. allerton house. doucet a. de freitas j. and gordon n. eds. se quential monte carlo methods in practice. springer. duane s. kennedy a. d. pendleton b. j. and roweth d. hybrid monte carlo. physics letters b durbin r. eddy s. r. krogh a. and mitchison g. biological sequence analysis. probabilistic models of proteins and nucleic acids. cambridge univ. press. dyson f. j. origins of life. cambridge univ. press. elias p. universal codeword sets and representations of the integers. ieee trans. info. theory felsenstein j. eyre-walker a. and keightley p. high genomic deleterious mutation rates in hominids. nature is maynard smith necessary? in evolution. essays in honour of john maynard smith ed. by p. j. greenwood p. h. harvey and m. slatkin pp. cambridge univ. press. recombination and sex ferreira h. clarke w. helberg a. abdel-ghaffar k. s. and vinck a. h. insertiondeletion correction with spectral nulls. ieee trans. info. theory feynman r. p. statistical mechanics. addisonwesley. forney jr. g. d. concatenated codes. mit press. forney jr. g. d. codes on graphs normal realiza tions. ieee trans. info. theory frey b. j. graphical models for machine learning and digital communication. mit press. gallager r. g. low density parity check codes. ire trans. info. theory gallager r. g. low density parity check codes. number in mit research monograph series. mit press. available from www.inference.phy.cam.ac.ukmackaygallager papers. gallager r. g. information theory and reliable com munication. wiley. gallager r. g. variations on a theme by ieee trans. info. theory gibbs m. n. bayesian gaussian processes for regression and cambridge univ. phd dissertation. gibbs m. n. and mackay d. j. c. effor interpowww.inference.phy.cam.ac.ukmackayabstracts implementation of gaussian processes lation. gpros.html. gibbs m. n. and mackay d. j. c. variational gaussian process ieee trans. on neural networks gilks w. roberts g. and george e. adaptive direction sampling. statistician copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bibliography gilks w. and wild p. adaptive rejection sampling for gibbs sampling. applied statistics gilks w. r. richardson s. and spiegelhalter d. j. markov chain monte carlo in practice. chapman and hall. goldie c. m. and pinch r. g. e. communication theory. cambridge univ. press. golomb s. w. peile r. e. and scholtz r. a. basic concepts in information theory and coding the adventures of secret agent plenum press. good i. j. studies in the history of probability and statistics. xxxvii. a.m. turing s statistical work in world war ii. biometrika graham r. l. on partitions of a set. journal of combinatorial theory graham r. l. and knowlton k. c. method of identifying conductors in a cable by establishing conductor connection groupings at both ends of the cable. u.s. patent green p. j. reversible jump markov chain monte carlo computation and bayesian model determination. biometrika gregory p. c. and loredo t. j. a new method for the detection of a periodic signal of unknown shape and period. in maximum entropy and bayesian methods ed. by g. erickson and c. smith. kluwer. also in astrophysical journal pp. oct gull s. f. bayesian inductive inference and maximum entropy. in maximum entropy and bayesian methods in science and engineering vol. foundations ed. by g. erickson and c. smith pp. kluwer. gull s. f. developments in maximum entropy data analysis. in maximum entropy and bayesian methods cambridge ed. by j. skilling pp. kluwer. gull s. f. and daniell g. image reconstruction from incomplete and noisy data. nature hamilton w. d. narrow roads of gene land volume evolution of sex oxford univ. press. hanson r. stutz j. and cheeseman p. bayesian theory. technical report nasa ames. hanson r. stutz j. and cheeseman p. bayesian with correlation and inheritance. in proc. intern. joint conf. on intelligence sydney australia volume pp. morgan kaufmann. hartmann c. r. p. and rudolph l. d. an optimum symbol by symbol decoding rule for linear codes. ieee trans. info. theory harvey m. and neal r. m. inference for belief networks using coupling from the past. in uncertainty in intelligence proc. sixteenth conf. pp. hebb d. o. the organization of behavior wiley. hendin o. horn d. and hopfield j. j. decomposition of a mixture of signals in a model of the olfactory bulb. proc. natl. acad. sci. usa hertz j. krogh a. and palmer r. g. introduction to the theory of neural computation. addison-wesley. hinton g. training products of experts by minimizing contrastive divergence. technical report gatsby computational neuroscience unit univ. college london. hinton g. and nowlan s. how learning can guide evolution. complex systems hinton g. e. dayan p. frey b. j. and neal r. m. the wake-sleep algorithm for unsupervised neural networks. science hinton g. e. and sejnowski t. j. learning and relearning in boltzmann machines. in parallel distributed processing ed. by d. e. rumelhart and j. e. mcclelland pp. mit press. hinton g. e. and teh y. w. discovering multiple constraints that are frequently approximately in uncertainty in intelligence proc. seventeenth conf. pp. morgan kaufmann. hinton g. e. and van camp d. keeping neural networks simple by minimizing the description length of the weights. in proc. annual workshop on comput. learning theory pp. acm press new york ny. hinton g. e. welling m. teh y. w. and osindero s. a new view of ica. in proc. international conf. on independent component analysis and blind signal separation volume hinton g. e. and zemel r. s. autoencoders minimum description length and helmholtz free energy. in advances in neural information processing systems ed. by j. d. cowan g. tesauro and j. alspector. morgan kaufmann. hodges a. alan turing the enigma. simon and schus ter. hojen-sorensen p. a. winther o. and hansen l. k. mean approaches to independent component analysis. neural computation holmes c. and denison d. perfect sampling for wavelet reconstruction of signals. ieee trans. signal processing holmes c. and mallick b. perfect simulation for orthogonal model mixing. technical report imperial college london. hopfield j. j. kinetic proofreading a new mechanism for reducing errors in biosynthetic processes requiring high proc. natl. acad. sci. usa hopfield j. j. origin of the genetic code a testable hypothesis based on trna structure sequence and kinetic proofreading. proc. natl. acad. sci. usa hopfield j. j. the energy relay a proofreading scheme based on dynamic cooperativity and lacking all characteristic symptoms of kinetic proofreading in dna replication and protein synthesis. proc. natl. acad. sci. usa hopfield j. j. neural networks and physical systems with emergent collective computational abilities. proc. natl. acad. sci. usa hopfield j. j. neurons with graded response properties have collective computational properties like those of two-state neurons. proc. natl. acad. sci. usa hopfield j. j. learning algorithms and probability distributions in feed-forward and feed-back networks. proc. natl. acad. sci. usa hopfield j. j. and brody c. d. what is a moment? sensory integration over a brief interval. proc. natl. acad. sci hopfield j. j. and brody c. d. what is a moment? transient synchrony as a collective mechanism for spatiotemporal integration. proc. natl. acad. sci hopfield j. j. and tank d. w. neural computation of decisions in optimization problems. biol. cybernetics howarth p. and bradley a. the longitudinal aberration of the human eye and its correction. vision res. hinton g. e. and ghahramani z. generative models for discovering sparse distributed representations. philosophical trans. royal society b huber m. exact sampling and approximate counting techniques. in proc. acm symposium on the theory of computing pp. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bibliography huffman d. a method for construction of minimum redundancy codes. proc. of ire ichikawa k. bhadeshia h. k. d. h. and mackay d. j. c. model for hot cracking in low-alloy steel weld metals. science and technology of welding and joining levenshtein v. i. binary codes capable of correcting deletions insertions and reversals. soviet physics doklady lin s. and costello jr. d. j. error control coding fundamentals and applications. prentice-hall. isard m. and blake a. visual tracking by stochastic propagation of conditional density. in proc. fourth european conf. computer vision pp. litsyn s. and shevelev v. on ensembles of lowdensity parity-check codes asymptotic distance distributions. ieee trans. info. theory isard m. and blake a. condensation conditional density propagation for visual tracking. international journal of computer vision jaakkola t. s. and jordan m. i. computing upper and lower bounds on likelihoods in intractable networks. in proc. twelfth conf. on uncertainty in ai morgan kaufman. jaakkola t. s. and jordan m. i. bayesian logistic regression a variational approach. statistics and computing jaakkola t. s. and jordan m. i. bayesian parameter estimation via variational methods. statistics and computing jaynes e. t. bayesian intervals versus intervals. in e.t. jaynes. papers on probability statistics and statistical physics ed. by r. d. rosenkrantz p. kluwer. jaynes e. t. probability theory the logic of science. cambridge univ. press. edited by g. larry bretthorst. jensen f. v. an introduction to bayesian networks. ucl press. johannesson r. and zigangirov k. s. fundamentals of convolutional coding. ieee press. jordan m. i. ed. learning in graphical models. nato science series. kluwer academic publishers. jpl turbo codes performance. available from jutten c. and herault j. blind separation of sources. an adaptive algorithm based on neuromimetic architecture. signal processing karplus k. and krit h. a semi-systolic decoder for the error-correcting code. discrete applied mathematics kepler t. and oprea m. improved inference of mutation rates i. an integral representation of the distribution. theoretical population biology kimeldorf g. s. and wahba g. a correspondence between bayesian estimation of stochastic processes and smoothing by splines. annals of math. statistics kitanidis p. k. parameter uncertainty in estimation of spatial functions bayesian analysis. water resources research loredo t. j. from laplace to supernova sn bayesian inference in astrophysics. in maximum entropy and bayesian methods dartmouth u.s.a. ed. by p. fougere pp. kluwer. lowe d. g. similarity metric learning for a variable kernel neural computation luby m. lt codes. in proc. the annual ieee symposium on foundations of computer science november pp. luby m. g. mitzenmacher m. shokrollahi m. a. and spielman d. a. improved low-density parity-check codes using irregular graphs and belief propagation. in proc. ieee international symposium on info. theory p. luby m. g. mitzenmacher m. shokrollahi m. a. and spielman d. a. erasure correcting codes. ieee trans. info. theory luby m. g. mitzenmacher m. shokrollahi m. a. and spielman d. a. improved low-density parity-check codes using irregular graphs and belief propagation. ieee trans. info. theory luby m. g. mitzenmacher m. shokrollahi m. a. spielman d. a. and stemann v. practical loss-resilient codes. in proc. twenty-ninth annual acm symposium on theory of computing luo z. and wahba g. hybrid adaptive splines. j. amer. statist. assoc. luria s. e. and m. mutations of bacteria from virus sensitivity to virus resistance. genetics reprinted in microbiology a centenary perspective wolfgang k. joklik ed. asm press and available from www.esp.org. luttrell s. p. hierarchical vector quantisation. proc. iee part i luttrell s. p. derivation of a class of training algo rithms. ieee trans. on neural networks mackay d. j. c. bayesian methods for adaptive models. california institute of technology phd dissertation. mackay d. j. c. bayesian interpolation. neural com putation mackay d. j. c. the evidence framework applied to knuth d. e. the art of computer programming. ad networks. neural computation dison wesley. kondrashov a. s. deleterious mutations and the evo lution of sexual reproduction. nature kschischang f. r. frey b. j. and loeliger h.-a. ieee trans. factor graphs and the sum-product algorithm. info. theory kschischang f. r. and sorokine v. on the trelieee trans. info. theory lis structure of block codes. lauritzen s. l. time series analysis in a discussion of contributions made by t. n. thiele. isi review lauritzen s. l. graphical models. number in oxford statistical science series. clarendon press. mackay d. j. c. a practical bayesian framework for backpropagation networks. neural computation bayesian methods for backpropmackay d. j. c. in models of neural networks iii ed. by agation networks. e. domany j. l. van hemmen and k. schulten chapter pp. springer. mackay d. j. c. bayesian non-linear modelling for the prediction competition. in ashrae trans. pp. american society of heating refrigeration and air-conditioning engineers. mackay d. j. c. free energy minimization algorithm for decoding and cryptanalysis. electronics letters lauritzen s. l. and spiegelhalter d. j. local computations with probabilities on graphical structures and their application to expert systems. j. royal statistical society b mackay d. j. c. probable networks and plausible predictions a review of practical bayesian methods for supervised neural networks. network computation in neural systems copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bibliography mackay d. j. c. ensemble learning for hidden markov www.inference.phy.cam.ac.ukmackayabstracts models. ensemblepaper.html. mackay d. j. c. iterative probabilistic decoding of low density parity check codes. animations available on world wide web. www.inference.phy.cam.ac.ukmackaycodesgifs. mackay d. j. c. choice of basis for laplace approxi matheron g. principles of geostatistics. economic ge ology maynard smith j. haldane s dilemma and the rate of evolution. nature maynard smith j. the evolution of sex cambridge univ. press. maynard smith j. games sex and evolution. mation. machine learning harvesterwheatsheaf. mackay d. j. c. introduction to gaussian processes. in neural networks and machine learning ed. by c. m. bishop nato asi series pp. kluwer. mackay d. j. c. comparison of approximate methods for handling hyperparameters. neural computation mackay d. j. c. good error correcting codes based on very sparse matrices. ieee trans. info. theory mackay d. j. c. an alternative to runlength-limiting codes turn timing errors into substitution errors. available from www.inference.phy.cam.ac.ukmackay. mackay d. j. c. a problem with variational free energy minimization. www.inference.phy.cam.ac.ukmackay abstractsminima.html. mackay d. j. c. and davey m. c. evaluation of gallager codes for short block length and high rate applications. in codes systems and graphical models ed. by b. marcus and j. rosenthal volume of ima volumes in mathematics and its applications pp. springer. mackay d. j. c. mitchison g. j. and mcfadden p. l. sparse-graph codes for quantum error-correction. ieee trans. info. theory mackay d. j. c. and neal r. m. good codes based on very sparse matrices. in cryptography and coding. ima conf. lncs ed. by c. boyd pp. springer. mackay d. j. c. and neal r. m. near shannon limit performance of low density parity check codes. electronics letters reprinted electronics letters march mackay d. j. c. and peto l. a hierarchical dirichlet language model. natural language engineering mackay d. j. c. wilson s. t. and davey m. c. comparison of constructions of irregular gallager codes. in proc. allerton conf. on communication control and computing sept. pp. allerton house. mackay d. j. c. wilson s. t. and davey m. c. comparison of constructions of irregular gallager codes. ieee trans. on communications mackay d. m. and mackay v. the time course of the mccollough and its physiological implications. j. physiol. mackay d. m. and mcculloch w. s. the limiting information capacity of a neuronal link. bull. math. biophys. macwilliams f. j. and sloane n. j. a. the theory of error-correcting codes. north-holland. mandelbrot b. the fractal geometry of nature. w.h. freeman. mao y. and banihashemi a. design of good ldpc codes using girth distribution. in ieee international symposium on info. theory italy june mao y. and banihashemi a. a heuristic search for in ieee interna good ldpc codes at short block lengths. tional conf. on communications. marinari e. and parisi g. simulated tempering a new monte-carlo scheme. europhysics letters maynard smith j. and e. the major transitions in evolution. freeman. maynard smith j. and e. the origins of life. oxford univ. press. mccollough c. color adaptation of edge-detectors in the human visual system. science mceliece r. j. the theory of information and coding. cambridge univ. press second edition. mceliece r. j. mackay d. j. c. and cheng j.-f. turbo decoding as an instance of pearl s belief propagation algorithm. ieee journal on selected areas in communications mcmillan b. two inequalities implied by unique deci pherability. ire trans. inform. theory minka t. a family of algorithms for approximate bayesian inference. mit phd dissertation. miskin j. w. ensemble learning for independent component analysis. dept. of physics univ. of cambridge phd dissertation. miskin j. w. and mackay d. j. c. ensemble learning for blind image separation and deconvolution. in advances in independent component analysis ed. by m. girolami. springer. miskin j. w. and mackay d. j. c. ensemble learning for blind source separation. in ica principles and practice ed. by s. roberts and r. everson. cambridge univ. press. mosteller f. and wallace d. l. applied bayesian and classical inference. the case of the federalist papers. springer. neal r. m. bayesian mixture modelling by monte carlo simulation. technical report computer science univ. of toronto. neal r. m. bayesian learning via stochastic dynamics. in advances in neural information processing systems ed. by c. l. giles s. j. hanson and j. d. cowan pp. morgan kaufmann. neal r. m. probabilistic inference using markov chain monte carlo methods. technical report dept. of computer science univ. of toronto. neal r. m. suppressing random walks in markov chain monte carlo using ordered overrelaxation. technical report dept. of statistics univ. of toronto. neal r. m. bayesian learning for neural networks. springer. neal r. m. markov chain monte carlo methods based on slicing the density function. technical report dept. of statistics univ. of toronto. neal r. m. monte carlo implementation of gaussian process models for bayesian regression and technical report dept. of computer science univ. of toronto. neal r. m. annealed importance sampling. technical report dept. of statistics univ. of toronto. neal r. m. priors for distributions using dirichlet trees. technical report dept. of statistics univ. of toronto. neal r. m. slice sampling. annals of statistics copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bibliography neal r. m. and hinton g. e. a new view of the em algorithm that incremental sparse and other variants. in learning in graphical models ed. by m. i. jordan nato science series pp. kluwer. nielsen m. and chuang i. quantum computation and quantum information. cambridge univ. press. offer e. and soljanin e. an algebraic description of iterative decoding schemes. in codes systems and graphical models ed. by b. marcus and j. rosenthal volume of ima volumes in mathematics and its applications pp. springer. offer e. and soljanin e. ldpc codes a group algebra formulation. in proc. internat. workshop on coding and cryptography wcc jan. paris. o hagan a. on curve and optimal design for regression. j. royal statistical society b o hagan a. monte carlo is fundamentally unsound. the statistician o hagan a. bayesian inference volume of kendall s advanced theory of statistics. edward arnold. omre h. bayesian kriging merging observations and guesses in kriging. mathematical geology gaussian processes for algorithms. neural computation opper m. and winther o. rasmussen c. e. and ghahramani z. bayesian monte carlo. in advances in neural information processing systems xv ed. by s. becker s. thrun and k. obermayer. ratliff f. and riggs l. a. involuntary motions of the eye during monocular j. exptl. psychol. ratzer e. a. and mackay d. j. c. sparse low-density parity-check codes for channels with cross-talk. in proc. ieee info. theory workshop paris. reif f. fundamentals of statistical and thermal physics. mcgrawhill. richardson t. shokrollahi m. a. and urbanke r. design of capacity-approaching irregular low-density parity check codes. ieee trans. info. theory the capacity of low-density parity check codes under message-passing decoding. ieee trans. info. theory richardson t. and urbanke r. richardson t. and urbanke r. encoding of low-density parity-check codes. ieee trans. info. theory ridley m. mendel s demon gene justice and the com plexity of life. phoenix. ripley b. d. statistical inference for spatial processes. cambridge univ. press. ripley b. d. pattern recognition and neural networks. cambridge univ. press. patrick j. d. and wallace c. s. stone circle geometries an information theory approach. in archaeoastronomy in the old world ed. by d. c. heggie pp. cambridge univ. press. rumelhart d. e. hinton g. e. and williams r. j. learning representations by back-propagating errors. nature russell s. and wefald e. do the right thing studies pearl j. probabilistic reasoning in intelligent systems in limited rationality. mit press. networks of plausible inference. morgan kaufmann. pearl j. causality. cambridge univ. press. pearlmutter b. a. and parra l. c. a contextsensitive generalization of ica. in international conf. on neural information processing hong kong pp. pearlmutter b. a. and parra l. c. maximum likelihood blind source separation a context-sensitive generalization of ica. in advances in neural information processing systems ed. by m. c. mozer m. i. jordan and t. petsche volume p. mit press. pinto r. l. and neal r. m. improving markov chain monte carlo estimators by coupling to an approximating chain. technical report dept. of statistics univ. of toronto. poggio t. and girosi f. a theory of networks for approximation and learning. technical report a.i. mit. poggio t. and girosi f. networks for approximation and learning. proc. ieee polya g. induction and analogy in mathematics. prince ton univ. press. schneier b. applied cryptography. wiley. scholkopf b. burges c. and vapnik v. extracting support data for a given task. in proc. first international conf. on knowledge discovery and data mining ed. by u. m. fayyad and r. uthurusamy. aaai press. scholtz r. a. the origins of spread-spectrum commu nications. ieee trans. on communications seeger m. williams c. k. i. and lawrence n. fast forward selection to speed up sparse gaussian process regression. in proc. ninth international workshop on intelligence and statistics ed. by c. bishop and b. j. frey. society for intelligence and statistics. sejnowski t. j. higher order boltzmann machines. in neural networks for computing ed. by j. denker pp. american institute of physics. sejnowski t. j. and rosenberg c. r. parallel networks that learn to pronounce english text. journal of complex systems shannon c. e. a mathematical theory of communica propp j. g. and wilson d. b. exact sampling with coupled markov chains and applications to statistical mechanics. random structures and algorithms rabiner l. r. and juang b. h. an introduction to tion. bell sys. tech. j. shannon c. e. the best detection of pulses. in collected papers of claude shannon ed. by n. j. a. sloane and a. d. wyner pp. ieee press. hidden markov models. ieee assp magazine pp. shannon c. e. and weaver w. the mathematical rasmussen c. e. evaluation of gaussian processes and other methods for non-linear regression. univ. of toronto phd dissertation. rasmussen c. e. the gaussian mixture model. in advances in neural information processing systems ed. by s. solla t. leen and k.-r. pp. mit press. rasmussen c. e. reduced rank gaussian process learn ing. unpublished manuscript. rasmussen c. e. and ghahramani z. mixtures of gaussian process experts. in advances in neural information processing systems ed. by t. g. diettrich s. becker and z. ghahramani. mit press. theory of communication. univ. of illinois press. shokrollahi a. raptor codes. technical report laboratoire d algorithmique polytechnique de lausanne lausanne switzerland. available from algo.epfl.ch. sipser m. and spielman d. a. expander codes. ieee trans. info. theory skilling j. classic maximum entropy. in maximum entropy and bayesian methods cambridge ed. by j. skilling. kluwer. skilling j. bayesian numerical analysis. in physics and probability ed. by w. t. grandy jr. and p. milonni. cambridge univ. press. copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. bibliography skilling j. and mackay d. j. c. slice sampling a binary implementation. annals of statistics discussion of slice sampling by radford m. neal. slepian d. and wolf j. noiseless coding of correlated information sources. ieee trans. info. theory smola a. j. and bartlett p. sparse greedy gaussian process regression. in advances in neural information processing systems ed. by t. k. leen t. g. diettrich and v. tresp pp. mit press. spiegel m. r. statistics. schaum s outline series. mcgraw-hill edition. spielman d. a. able error-correcting codes. linear-time encodable and decodieee trans. info. theory sutton r. s. and barto a. g. reinforcement learn ing an introduction. mit press. swanson l. a new code for galileo. in proc. ieee international symposium info. theory pp. tanner m. a. tools for statistical inference methods for the exploration of posterior distributions and likelihood functions. springer series in statistics. springer edition. tanner r. m. a recursive approach to low complexity codes. ieee trans. info. theory teahan w. j. probability estimation for ppm. in proc. nzcsrsc available from citeseer.nj.nec.com ten brink s. convergence of iterative decoding. elec tronics letters ten brink s. kramer g. and ashikhmin a. design of low-density parity-check codes for multi-antenna modulation and detection. submitted to ieee trans. on communications. terras a. fourier analysis on finite groups and ap plications. cambridge univ. press. thomas a. spiegelhalter d. j. and gilks w. r. bugs a program to perform bayesian inference using gibbs sampling. in bayesian statistics ed. by j. m. bernardo j. o. berger a. p. dawid and a. f. m. smith pp. clarendon press. tresp v. a bayesian committee machine. neural com putation urbanke r. ldpcopt a fast and accurate degree distribution optimizer for ldpc code ensembles. lthcwww.epfl.ch researchldpcopt. vapnik v. the nature of statistical learning theory. springer. viterbi a. j. error bounds for convolutional codes and an asymptotically optimum decoding algorithm. ieee trans. info. theory wahba g. spline models for observational data. society for industrial and applied mathematics. cbms-nsf regional conf. series in applied mathematics. wainwright m. j. jaakkola t. and willsky a. s. tree-based reparameterization framework for analysis of sumproduct and related algorithms. ieee trans. info. theory wald g. and griffin d. the change in refractive power of the eye in bright and dim light. j. opt. soc. am. wallace c. and boulton d. an information measure for comput. j. wallace c. s. and freeman p. r. estimation and inference by compact coding. j. r. statist. soc. b ward d. j. blackwell a. f. and mackay d. j. c. dasher a data entry interface using continuous gestures and language models. in proc. user interface software and technology pp. ward d. j. and mackay d. j. c. fast hands-free writing by gaze direction. nature welch t. a. a technique for high-performance data compression. ieee computer welling m. and teh y. w. belief optimization for binary networks a stable alternative to loopy belief propagation. in uncertainty in intelligence proc. seventeenth conf. pp. morgan kaufmann. wiberg n. codes and decoding on general graphs. dept. of elec. eng. sweden phd dissertation. studies in science and technology no. wiberg n. loeliger h.-a. and r. codes and iterative decoding on general graphs. european trans. on telecommunications wiener n. cybernetics. wiley. williams c. k. i. and rasmussen c. e. gaussian processes for regression. in advances in neural information processing systems ed. by d. s. touretzky m. c. mozer and m. e. hasselmo. mit press. williams c. k. i. and seeger m. using the method to speed up kernel machines. in advances in neural information processing systems ed. by t. k. leen t. g. diettrich and v. tresp pp. mit press. witten i. h. neal r. m. and cleary j. g. arithmetic coding for data compression. communications of the acm wolf j. k. and siegel p. on two-dimensional arrays and crossword puzzles. in proc. allerton conf. on communication control and computing sept. pp. allerton house. worthen a. p. and stark w. e. low-density parity check codes for fading channels with memory. in proc. allerton conf. on communication control and computing sept. pp. yedidia j. s. an idiosyncratic journey beyond mean theory. technical report mitsubishi electric res. labs. yedidia j. s. freeman w. t. and weiss y. generalized belief propagation. technical report mitsubishi electric res. labs. yedidia j. s. freeman w. t. and weiss y. bethe free energy kikuchi approximations and belief propagation algorithms. technical report mitsubishi electric res. labs. yedidia j. s. freeman w. t. and weiss y. characterization of belief propagation and its generalizations. technical report mitsubishi electric res. labs. yedidia j. s. freeman w. t. and weiss y. constructing free energy approximations and generalized belief propagation algorithms. technical report mitsubishi electric res. labs. yeung r. w. a new outlook on shannon-information measures. ieee trans. info. theory yuille a. l. a double-loop algorithm to minimize the bethe and kikuchi free energies. in energy minimization methods in computer vision and pattern recognition ed. by m. figueiredo j. zerubia and a. jain number in lncs pp. springer. zipf g. k. human behavior and the principle of least addison-wesley. ziv j. and lempel a. a universal algorithm for sequential data compression. ieee trans. info. theory ziv j. and lempel a. compression of individual sequences via variable-rate coding. ieee trans. info. theory copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. and abu-mostafa yaser acceptance rate acceptance ratio method accumulator activation function activity rule adaptive direction sampling adaptive models adaptive rejection sampling address aiyer sree alberto alchemists algebraic coding theory algorithm see learning algorithms bcjr belief propagation covariant em exact sampling expectationmaximization function minimization genetic hamiltonian monte carlo independent component analysis langevin monte carlo leapfrog maxproduct message-passing minsum monte carlo see monte carlo methods newtonraphson perfect simulation sumproduct viterbi alice allais paradox alphabetical ordering america american amino acid anagram index annealing deterministic importance sampling antiferromagnetic ape approximation by gaussian laplace of complex distribution of density evolution saddle-point stirling variational arabic architecture arithmetic coding decoder software uses beyond compression arithmetic progression arms race intelligence associative memory assumptions astronomy asymptotic equipartition why it is a misleading term atlantic autoclass automatic relevance determination automobile data reception average see expectation awgn background rate backpropagation backward pass bad see error-correcting code balakrishnan sree balance baldwin ban banburismus band-limited signal bandwidth bar-code base transitions base-pairing basis dependence bat battleships bayes theorem bayes rev. thomas bayesian belief networks bayesian inference bch codes bcjr algorithm bearing belarusian belief belief propagation see message passing and sumproduct algorithm benford s law bent coin berlekamp elwyn bernoulli distribution berrou c. bet beta distribution beta function beta integral bethe free energy bhattacharyya parameter bias in neural net in statistics biexponential bifurcation binary entropy function binary erasure channel binary images binary representations binary symmetric channel binding dna binomial distribution bipartite graph birthday bit bit bits back bivariate gaussian black bletchley park blind watchmaker block code see source code or error-correcting code block-sorting blood group blow up blur copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. index bob boltzmann entropy boltzmann machine bombes book isbn bookies bottou leon bound union bounded-distance decoder bounding chain box boyish matters brain bridge british broadcast channel brody carlos brownian motion bsc see channel binary symmetric budget s needle bugs buoy burglar alarm and earthquake burrowswheeler transform burst errors bus-stop paradox byte cable labelling calculator camera canonical capacity channel with synchronization errors constrained channel gaussian channel network neural network neuron symmetry argument car data reception card casting out nines cauchy distribution caution see sermon equipartition gaussian distribution importance sampling sampling theory cave caveat see caution and sermon cellphone see mobile phone cellular automaton central-limit theorem see law of large numbers centre of gravity chain rule challenges tanner channel awgn binary erasure binary symmetric broadcast bursty capacity connection with physics coding theorem see noisy-channel coding theorem complex constrained continuous discrete memoryless erasure extended fading gaussian input ensemble multiple access multiterminal noiseless noisy noisy typewriter symmetric two-dimensional unknown noise level variable symbol durations with dependent sources with memory z channel cheat chebyshev inequality checkerboard bound chess chess board chi-squared cholesky decomposition chromatic aberration cinema circle classical statistics criticisms clockville clustering coalescence cocked hat code see error-correcting code source code data compression symbol code arithmetic coding linear code random code or hash code dual see error-correcting code dual for constrained channel variable-length code-equivalent codebreakers codeword see source code symbol code or error-correcting code coding theory coin coincidence collective collision coloured noise combination commander communication v broadcast of dependent information over noiseless channels perspective on learning competitive learning complexity complexity control compress compression see source code future methods lossless lossy of already-compressed of any universal computer concatenation error-correcting codes in compression in markov chains concave conditional entropy cones interval level confused gameshow host conjugate gradient conjugate prior conjuror connection between channel capacity and physics error correcting code and latent variable model pattern recognition and error-correction supervised and unsupervised learning vector quantization and error-correction connection matrix constrained channel variable-length code constraint satisfaction content-addressable memory continuous channel control treatment conventions see notation convex hull convex convexity convolution convolutional code equivalence copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. conway john h. copernicus correlated sources correlations among errors and phase transitions high-order in images cost function cost of males counting counting argument coupling from the past covariance covariance function covariance matrix covariant algorithm cover thomas cox axioms crib critical critical path cross-validation crossover crossword cryptanalysis cryptography digital signatures tamper detection cumulative probability function cycles in graphs cyclic dasher data compression see source code and compression data entry data modelling see modelling data set davey matthew c. death penalty deciban decibel decision theory decoder bitwise bounded-distance codeword maximum a posteriori probability of error deconvolution degree degree sequence see degrees of belief degrees of freedom vu delay line max deletions delta function density evolution density modelling dependent sources depth of lake design theory detailed balance detection of forgery deterministic annealing dictionary die rolling cyclic code digamma function digital cinema digital fountain digital signature digital video broadcast dimensions dimer directory dirichlet distribution dirichlet model discriminant function discriminative training disease disk drive distance dkl bad distance distribution entropy distance gilbertvarshamov good hamming isn t everything of code goodbad of concatenated code of product code relative entropy very bad distribution beta biexponential binomial cauchy dirichlet exponential gamma gaussian sample from inverse-cosh log-normal normal over periodic variables poisson student-t von mises divergence djvu dna replication do the right thing dodecahedron code dongle doors on game show dr. bloggs draw straws dream index dsc see cyclic code dual dumb metropolis earthquake and burglar alarm earthquake during game show ebert todd edge eigenvalue elias peter em algorithm email empty string encoder energy english enigma ensemble extended ensemble learning entropic distribution entropy boltzmann conditional gibbs joint marginal mutual information of continuous variable relative entropy distance epicycles equipartition erasure channel erasure correction erf see error function ergodic error bars error correction see error-correcting code in dna replication in protein synthesis error detection error error function error probability and distance block in compression error-correcting code bad block code concatenated convolutional cyclic decoding density evolution cyclic distance see distance dodecahedron dual equivalence erasure channel error probability copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. index fountain code gallager golay good hamming in dna replication in protein synthesis interleaving linear coding theorem low-density generator-matrix low-density parity-check fast encoding staircase lt code maximum distance separable nonlinear parity-check code pentagonful perfect practical product code quantum random random linear raptor code rate rateless rectangular reedsolomon code repeataccumulate repetition simple parity sparse graph density evolution syndrome decoding variable rate very bad very good weight enumerator with varying level of protection error-reject curves errors see channel estimator eugenics euro evidence typical behaviour of evolution as learning baldwin colour vision of the genetic code evolutionary computing exact sampling exchange rate exchangeability exclusive or exit chart expectation expectation propagation expectationmaximization algorithm experimental design experimental skill explaining away exploit explore exponential distribution on integers exponential-family expurgation extended channel extended code extended ensemble extra bit extreme value eye movements factor analysis factor graph factorial fading channel feedback female ferromagnetic feynman richard fibonacci see galois storage theory see galois point florida analysis focus football pools forensic forgery forward pass forward probability forwardbackward algorithm fotheringtonthomas fountain code fourier transform fovea free energy see partition function minimization variational frequency frequentist see sampling theory frey brendan j. frobeniusperron theorem frustration full probabilistic model function minimization functions gain galileo code gallager code gallager robert g. galois gambling game see puzzle bridge chess guess that tune guessing life sixty-three submarine three doors twenty questions game show game-playing gamma distribution gamma function ganglion cells gaussian channel gaussian distribution n approximation parameters sample from gaussian processes variational general position generalization generalized parity-check matrix generating function generative model generator matrix genes genetic algorithm genetic code genome geometric progression geostatistics gfq see galois gibbs entropy gibbs sampling see monte carlo methods gibbs inequality gilbertvarshamov conjecture gilbertvarshamov distance gilbertvarshamov rate gilks wally r. girlie glauber dynamics glavieux a. golay code golden ratio good see error-correcting code good jack gradient descent natural graduated non-convexity graham ronald l. grain size graph factor graph of code graphs and cycles guerilla copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. guessing decoder guessing game gull steve gzip haldane j.b.s. hamilton william d. hamiltonian monte carlo hamming code graph hamming distance handwritten digits hard drive hash code hash function linear one-way hat puzzle heat bath heat capacity hebb donald hebbian learning hertz hessian hidden markov model hidden neurons hierarchical clustering hierarchical model high dimensions life in hint for computing mutual information hinton e. hitchhiker homogeneous hooke robert network capacity john j. horse race hot-spot code optimality disadvantages general alphabet human humanmachine interfaces hybrid monte carlo see hamiltonian monte carlo hydrogen bond hyperparameter hypersphere hypothesis testing see model comparison sampling theory i.i.d. ica see independent component analysis icf correlation function identical twin identity matrix ignorance ill-posed problem image integral image analysis image compression image models image processing image reconstruction implicit assumptions implicit probabilities importance sampling weakness of improper in-car navigation independence independent component analysis indicator function inequality inference and learning information content how to measure shannon information maximization information retrieval information theory inner code inquisition insertions instantaneous integral image interleaving internet intersection intrinsic correlation function invariance invariant distribution inverse probability inverse-arithmetic-coder inverse-cosh distribution inverse-gamma distribution inversion of hash function investment portfolio irregular isbn ising model iterative probabilistic decoding jaakkola tommi s. jacobian janitor prior jensen s inequality jet propulsion laboratory johnson noise joint ensemble joint entropy joint typicality joint typicality theorem jordan michael i. journal publication policy judge juggling junction tree algorithm jury index k-means clustering derivation soft kaboom kalman kernel key points communication how much data needed likelihood principle model comparison monte carlo solving probability problems keyboard kikuchi free energy kl distance knowltongraham partitions knuth donald xii kolmogorov andrei nikolaevich kraft inequality kraft l.g. kriging kullbackleibler divergence see relative entropy lagrange multiplier lake langevin method langevin process language model laplace approximation see laplace s method laplace model laplace prior laplace s method laplace s rule latent variable latent variable model compression law of large numbers lawyer le cun yann leaf leapfrog algorithm learning as communication as inference hebbian in evolution learning algorithms see algorithm backpropagation boltzmann machine competitive learning network k-means clustering multilayer perceptron single neuron learning rule lempelziv coding criticisms life copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. index life in high dimensions likelihood contrasted with probability subjectivity likelihood equivalence likelihood principle limit cycle linear block code coding theorem decoding linear regression linear-feedback shift-register litsyn simon little n large data set log-normal logarithms logit long thin strip loopy belief propagation loopy message-passing loss function lossy compression low-density generator-matrix code low-density parity-check code see error-correcting code lt code luby michael g. luria salvador lyapunov function machine learning macho mackay david j.c. magician magnet magnetic recording majority vote male mandelbrot benoit map see maximum a posteriori mapping marginal entropy marginal likelihood see evidence marginal probability marginalization markov chain construction markov chain monte carlo see monte carlo methods markov model see markov chain marriage matrix matrix identities maxproduct maxent see maximum entropy maximum distance separable maximum entropy maximum likelihood maximum a posteriori mccollough mcmc chain monte carlo see monte carlo methods mcmillan b. mdl see minimum description length mds mean mean theory melody memory address-based associative content-addressable memsys message passing bcjr belief propagation forwardbackward in graphs with cycles loopy sumproduct algorithm viterbi metacode metric metropolis method see monte carlo methods marc micro-saccades microcanonical microsoftus microwave oven minsum algorithm mine in ground minimax minimization see optimization minimum description length minimum distance see distance minka thomas mirror mitzenmacher michael mixing mixture modelling mixture of gaussians mixtures in markov chains ml see maximum likelihood mlp see multilayer perceptron mml see minimum description length mobile phone model model comparison typical evidence modelling density modelling images latent variable models nonparametric moderation see marginalization molecules molesworth momentum monte carlo methods acceptance rate acceptance ratio method and communication annealed importance sampling coalescence dependence on dimension exact sampling for visualization gibbs sampling hamiltonian monte carlo hybrid monte carlo see hamiltonian monte carlo importance sampling weakness of langevin method markov chain monte carlo metropolis method dumb metropolis metropolishastings multi-state overrelaxation perfect simulation random walk suppression random-walk metropolis rejection sampling adaptive reversible jump simulated annealing slice sampling thermodynamic integration umbrella sampling monty hall problem morse motorcycle movie multilayer perceptron multiple access channel multiterminal networks multivariate gaussian munrorobbins theorem murder music mutation rate mutual information how to compute myth compression nat natural gradient natural selection navigation neal radford m. needle s network neural network capacity learning as communication learning as inference copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. neuron capacity newton isaac newtonraphson method nines noise see channel coloured spectral density white noisy channel see channel noisy typewriter noisy-channel coding theorem gaussian channel linear codes poor man s version noisy-or non-confusable inputs noninformative nonlinear nonlinear code nonparametric data modelling nonrecursive noodle s normal see gaussian normal graph normalizing constant see partition function not-sum notation absolute value conventions of this book convexconcave entropy error function expectation intervals logarithms matrices probability set size transition probability vectors np-complete nucleotide nuisance parameters numerology nyquist sampling theorem objective function occam factor occam s razor octal octave odds ode to joy for whimsical departmental rules oliver one-way hash function optic nerve optimal decoder optimal input distribution optimal linear optimal stopping optimization gradient descent newton algorithm of model complexity order parameter ordered overrelaxation orthodox statistics see sampling theory outer code overrelaxation p-value packet paradox allais bus-stop heat capacity simpson s waiting for a six paranormal parasite parent parity parity-check bits parity-check code parity-check constraints parity-check matrix generalized parity-check nodes parse parsons code parthenogenesis partial order partial partition functions particle partition partition function analogy with lake partial partitioned inverse path-counting pattern recognition pentagonful code perfect code perfect simulation periodic variable permutation petersen graph phase transition philosophy phone cellular see mobile phone phone directory phone number photon counter physics pigeon-hole principle pitchfork bifurcation plaintext plankton point estimate index point spread function pointer poisoned glass poisson distribution poisson process poissonville polymer poor man s coding theorem porridge portfolio positive positivity posterior probability power cost power law practical see error-correcting code precision precisions add prediction predictive distribution code prior assigning improper subjectivity prior equivalence priority of bits in a message prize on game show probabilistic model probabilistic movie probability bayesian contrasted with likelihood density probability distributions see distribution probability of block error probability propagation see sumproduct algorithm product code of random graph pronunciation proper proposal density propp jim g. prosecutor s fallacy prospecting protein regulatory synthesis protocol pseudoinverse punch puncturing pupil puzzle see game cable labelling chessboard of dna replication hat life magic trick copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. index poisoned glass secretary southeast transatlantic cable weighing balls quantum error-correction queue qwerty see repetition code race radial basis function radio radix raid random random cluster model random code for compression random number generator random variable random walk suppression random-coding exponent random-walk metropolis method rant see sermon raptor codes rate rate-distortion theory rateless code reading aloud receiver operating characteristic recognition record breaking rectangular code reducible redundancy in channel code redundant array of independent disks redundant constraints in code reedsolomon code regression regret regular regularization regularization constant reinforcement learning rejection rejection sampling adaptive relative entropy reliability function repeataccumulate code repetition code responsibility retransmission reverse reversible reversible jump richardson thomas j. rissanen jorma roc rolling die roman rule of thumb runlength runlength-limited channel saccades saddle-point approximation sailor sample from gaussian sampler density sampling distribution sampling theory criticisms sandwiching method satellite communications scaling schottky anomaly scientists secret secretary problem security seek time sejnowski terry j. self-delimiting self-dual self-orthogonal self-punctuating separation sequence sequential decoding sequential probability ratio test sermon see caution classical statistics level dimensions gradient descent illegal integral importance sampling interleaving map method maximum entropy maximum likelihood most probable is atypical p-value sampling theory sphere-packing stopping rule turbo codes unbiased estimator worst-case-ism set shannon shannon claude see noisy-channel coding theorem source coding theorem information content shattering shifter ensemble shokrollahi m. amin shortening siegel paul sigmoid signal-to-noise ratio level simplex simpson s paradox simpson o.j. see wife-beaters simulated annealing see annealing six waiting for skilling john sleep slepianwolf see dependent sources slice sampling multi-dimensional soft k-means clustering softmax softmin software xi arithmetic coding bugs dasher free xii gaussian processes hash function vibes solar system soldier soliton distribution sound source code see compression symbol code arithmetic coding lempelziv algorithms block code block-sorting compression burrowswheeler transform for complex sources for constrained channel for integers see code implicit probabilities optimal lengths code software stream codes supermarket symbol code uniquely decodeable variable symbol durations source coding theorem southeast puzzle span sparse-graph code density evolution species spell sphere packing sphere-packing exponent spielman daniel a. spin system spines spline copyright cambridge university press on-screen viewing permitted. printing not permitted. you can buy this book for pounds or see httpwww.inference.phy.cam.ac.ukmackayitila for links. spread spectrum spring spy square staircase stalactite standard deviation stars state diagram statistic statistical physics see physics statistical test steepest descents stereoscopic vision stirling s approximation stochastic stochastic dynamics see hamiltonian monte carlo stochastic gradient stop-when-it s-done stopping rule straws drawing stream codes student student-t distribution subjective probability submarine subscriber subset substring statistics sum rule sumproduct algorithm summary summary state summation convention super-channel supermarket support vector surprise value survey propagation suspicious coincidences symbol code budget codeword disadvantages optimal self-delimiting supermarket symmetric channel symmetry argument synchronization synchronization errors syndrome syndrome decoding systematic t-distribution see student-t tail tamper detection tank david w. tanner challenge tanner product code tanner michael tanzanite tap telephone see phone telescope temperature termination terminology see notation monte carlo methods test statistical text entry thermal distribution thermodynamic integration thermodynamics third law thiele t.n. thin shell third law of thermodynamics thitimajshima p. three cards three doors threshold tiling time-division timing training data transatlantic transfer matrix method transition transition probability translation-invariant travelling salesman problem tree trellis section termination triangle truth function tube turbo code turbo product code turing alan twenty questions twin twos typical evidence typical set for compression for noisy channel typical-set decoder typicality umbrella sampling unbiased estimator uncompression union union bound uniquely decodeable units universal universality in physics urbanke urn user interfaces utility index vaccination vapnikchervonenkis dimension variable-length code variable-rate error-correcting codes variance variancecovariance matrix variances add variational bayes variational free energy variational methods typical properties variational gaussian process vc dimension vector quantization very good see error-correcting code vibes virtakallio juhani vision visualization viterbi algorithm volume von mises distribution wainwright martin waiting for a bus warning see caution and sermon watsoncrick base pairing weather collator weighing babies weighing problem weight importance sampling in neural net of binary vector weight decay weight enumerator typical weight space wenglish what number comes next? white white noise wiberg niclas widget wiener process wiener norbert wife-beater wilson david b. window winfree erik wodge wolf jack word-english world record worst-case-ism writing yedidia jonathan z channel zipf plot zipf s law zipf george k.